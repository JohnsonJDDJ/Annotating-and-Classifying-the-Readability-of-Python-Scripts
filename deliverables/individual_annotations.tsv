301	jackson	4	"# This file is distributed under the same license as the Django package.
#
# The *_FORMAT strings use the Django date format syntax,
# see https://docs.djangoproject.com/en/dev/ref/templates/builtins/#date
DATE_FORMAT = r'Y. \g\a\d\a j. F'
TIME_FORMAT = 'H:i'
DATETIME_FORMAT = r'Y. \g\a\d\a j. F, H:i'
YEAR_MONTH_FORMAT = r'Y. \g. F'
MONTH_DAY_FORMAT = 'j. F'
SHORT_DATE_FORMAT = r'j.m.Y'
SHORT_DATETIME_FORMAT = 'j.m.Y H:i'
FIRST_DAY_OF_WEEK = 1  # Monday

# The *_INPUT_FORMATS strings use the Python strftime format syntax,
# see https://docs.python.org/library/datetime.html#strftime-strptime-behavior
# Kept ISO formats as they are in first position
DATE_INPUT_FORMATS = [
    '%Y-%m-%d', '%d.%m.%Y', '%d.%m.%y',  # '2006-10-25', '25.10.2006', '25.10.06'
]
TIME_INPUT_FORMATS = [
    '%H:%M:%S',     # '14:30:59'
    '%H:%M:%S.%f',  # '14:30:59.000200'
    '%H:%M',        # '14:30'
    '%H.%M.%S',     # '14.30.59'
    '%H.%M.%S.%f',  # '14.30.59.000200'
    '%H.%M',        # '14.30'
]
DATETIME_INPUT_FORMATS = [
    '%Y-%m-%d %H:%M:%S',     # '2006-10-25 14:30:59'
    '%Y-%m-%d %H:%M:%S.%f',  # '2006-10-25 14:30:59.000200'
    '%Y-%m-%d %H:%M',        # '2006-10-25 14:30'
    '%d.%m.%Y %H:%M:%S',     # '25.10.2006 14:30:59'
    '%d.%m.%Y %H:%M:%S.%f',  # '25.10.2006 14:30:59.000200'
    '%d.%m.%Y %H:%M',        # '25.10.2006 14:30'
    '%d.%m.%y %H:%M:%S',     # '25.10.06 14:30:59'
    '%d.%m.%y %H:%M:%S.%f',  # '25.10.06 14:30:59.000200'
    '%d.%m.%y %H:%M',        # '25.10.06 14:30'
    '%d.%m.%y %H.%M.%S',     # '25.10.06 14.30.59'
    '%d.%m.%y %H.%M.%S.%f',  # '25.10.06 14.30.59.000200'
    '%d.%m.%y %H.%M',        # '25.10.06 14.30'
]
DECIMAL_SEPARATOR = ','
THOUSAND_SEPARATOR = ' '  # Non-breaking space
NUMBER_GROUPING = 3"
301	donghui	1	"# This file is distributed under the same license as the Django package.
#
# The *_FORMAT strings use the Django date format syntax,
# see https://docs.djangoproject.com/en/dev/ref/templates/builtins/#date
DATE_FORMAT = r'Y. \g\a\d\a j. F'
TIME_FORMAT = 'H:i'
DATETIME_FORMAT = r'Y. \g\a\d\a j. F, H:i'
YEAR_MONTH_FORMAT = r'Y. \g. F'
MONTH_DAY_FORMAT = 'j. F'
SHORT_DATE_FORMAT = r'j.m.Y'
SHORT_DATETIME_FORMAT = 'j.m.Y H:i'
FIRST_DAY_OF_WEEK = 1  # Monday

# The *_INPUT_FORMATS strings use the Python strftime format syntax,
# see https://docs.python.org/library/datetime.html#strftime-strptime-behavior
# Kept ISO formats as they are in first position
DATE_INPUT_FORMATS = [
    '%Y-%m-%d', '%d.%m.%Y', '%d.%m.%y',  # '2006-10-25', '25.10.2006', '25.10.06'
]
TIME_INPUT_FORMATS = [
    '%H:%M:%S',     # '14:30:59'
    '%H:%M:%S.%f',  # '14:30:59.000200'
    '%H:%M',        # '14:30'
    '%H.%M.%S',     # '14.30.59'
    '%H.%M.%S.%f',  # '14.30.59.000200'
    '%H.%M',        # '14.30'
]
DATETIME_INPUT_FORMATS = [
    '%Y-%m-%d %H:%M:%S',     # '2006-10-25 14:30:59'
    '%Y-%m-%d %H:%M:%S.%f',  # '2006-10-25 14:30:59.000200'
    '%Y-%m-%d %H:%M',        # '2006-10-25 14:30'
    '%d.%m.%Y %H:%M:%S',     # '25.10.2006 14:30:59'
    '%d.%m.%Y %H:%M:%S.%f',  # '25.10.2006 14:30:59.000200'
    '%d.%m.%Y %H:%M',        # '25.10.2006 14:30'
    '%d.%m.%y %H:%M:%S',     # '25.10.06 14:30:59'
    '%d.%m.%y %H:%M:%S.%f',  # '25.10.06 14:30:59.000200'
    '%d.%m.%y %H:%M',        # '25.10.06 14:30'
    '%d.%m.%y %H.%M.%S',     # '25.10.06 14.30.59'
    '%d.%m.%y %H.%M.%S.%f',  # '25.10.06 14.30.59.000200'
    '%d.%m.%y %H.%M',        # '25.10.06 14.30'
]
DECIMAL_SEPARATOR = ','
THOUSAND_SEPARATOR = ' '  # Non-breaking space
NUMBER_GROUPING = 3"
350	jackson	1	"import os
import signal
import subprocess

from django.db.backends.base.client import BaseDatabaseClient


class DatabaseClient(BaseDatabaseClient):
    executable_name = 'psql'

    @classmethod
    def runshell_db(cls, conn_params, parameters):
        args = [cls.executable_name]

        host = conn_params.get('host', '')
        port = conn_params.get('port', '')
        dbname = conn_params.get('database', '')
        user = conn_params.get('user', '')
        passwd = conn_params.get('password', '')
        sslmode = conn_params.get('sslmode', '')
        sslrootcert = conn_params.get('sslrootcert', '')
        sslcert = conn_params.get('sslcert', '')
        sslkey = conn_params.get('sslkey', '')

        if user:
            args += ['-U', user]
        if host:
            args += ['-h', host]
        if port:
            args += ['-p', str(port)]
        args += [dbname]
        args.extend(parameters)

        sigint_handler = signal.getsignal(signal.SIGINT)
        subprocess_env = os.environ.copy()
        if passwd:
            subprocess_env['PGPASSWORD'] = str(passwd)
        if sslmode:
            subprocess_env['PGSSLMODE'] = str(sslmode)
        if sslrootcert:
            subprocess_env['PGSSLROOTCERT'] = str(sslrootcert)
        if sslcert:
            subprocess_env['PGSSLCERT'] = str(sslcert)
        if sslkey:
            subprocess_env['PGSSLKEY'] = str(sslkey)
        try:
            # Allow SIGINT to pass to psql to abort queries.
            signal.signal(signal.SIGINT, signal.SIG_IGN)
            subprocess.run(args, check=True, env=subprocess_env)
        finally:
            # Restore the original SIGINT handler.
            signal.signal(signal.SIGINT, sigint_handler)

    def runshell(self, parameters):
        self.runshell_db(self.connection.get_connection_params(), parameters)"
350	donghui	1	"import os
import signal
import subprocess

from django.db.backends.base.client import BaseDatabaseClient


class DatabaseClient(BaseDatabaseClient):
    executable_name = 'psql'

    @classmethod
    def runshell_db(cls, conn_params, parameters):
        args = [cls.executable_name]

        host = conn_params.get('host', '')
        port = conn_params.get('port', '')
        dbname = conn_params.get('database', '')
        user = conn_params.get('user', '')
        passwd = conn_params.get('password', '')
        sslmode = conn_params.get('sslmode', '')
        sslrootcert = conn_params.get('sslrootcert', '')
        sslcert = conn_params.get('sslcert', '')
        sslkey = conn_params.get('sslkey', '')

        if user:
            args += ['-U', user]
        if host:
            args += ['-h', host]
        if port:
            args += ['-p', str(port)]
        args += [dbname]
        args.extend(parameters)

        sigint_handler = signal.getsignal(signal.SIGINT)
        subprocess_env = os.environ.copy()
        if passwd:
            subprocess_env['PGPASSWORD'] = str(passwd)
        if sslmode:
            subprocess_env['PGSSLMODE'] = str(sslmode)
        if sslrootcert:
            subprocess_env['PGSSLROOTCERT'] = str(sslrootcert)
        if sslcert:
            subprocess_env['PGSSLCERT'] = str(sslcert)
        if sslkey:
            subprocess_env['PGSSLKEY'] = str(sslkey)
        try:
            # Allow SIGINT to pass to psql to abort queries.
            signal.signal(signal.SIGINT, signal.SIG_IGN)
            subprocess.run(args, check=True, env=subprocess_env)
        finally:
            # Restore the original SIGINT handler.
            signal.signal(signal.SIGINT, sigint_handler)

    def runshell(self, parameters):
        self.runshell_db(self.connection.get_connection_params(), parameters)"
364	jackson	1	"import numpy as np
import numba as nb

from numpy.random import PCG64
from timeit import timeit

bit_gen = PCG64()
next_d = bit_gen.cffi.next_double
state_addr = bit_gen.cffi.state_address

def normals(n, state):
    out = np.empty(n)
    for i in range((n + 1) // 2):
        x1 = 2.0 * next_d(state) - 1.0
        x2 = 2.0 * next_d(state) - 1.0
        r2 = x1 * x1 + x2 * x2
        while r2 >= 1.0 or r2 == 0.0:
            x1 = 2.0 * next_d(state) - 1.0
            x2 = 2.0 * next_d(state) - 1.0
            r2 = x1 * x1 + x2 * x2
        f = np.sqrt(-2.0 * np.log(r2) / r2)
        out[2 * i] = f * x1
        if 2 * i + 1 < n:
            out[2 * i + 1] = f * x2
    return out

# Compile using Numba
normalsj = nb.jit(normals, nopython=True)
# Must use state address not state with numba
n = 10000

def numbacall():
    return normalsj(n, state_addr)

rg = np.random.Generator(PCG64())

def numpycall():
    return rg.normal(size=n)

# Check that the functions work
r1 = numbacall()
r2 = numpycall()
assert r1.shape == (n,)
assert r1.shape == r2.shape

t1 = timeit(numbacall, number=1000)
print(f'{t1:.2f} secs for {n} PCG64 (Numba/PCG64) gaussian randoms')
t2 = timeit(numpycall, number=1000)
print(f'{t2:.2f} secs for {n} PCG64 (NumPy/PCG64) gaussian randoms')

# example 2

next_u32 = bit_gen.ctypes.next_uint32
ctypes_state = bit_gen.ctypes.state

@nb.jit(nopython=True)
def bounded_uint(lb, ub, state):
    mask = delta = ub - lb
    mask |= mask >> 1
    mask |= mask >> 2
    mask |= mask >> 4
    mask |= mask >> 8
    mask |= mask >> 16

    val = next_u32(state) & mask
    while val > delta:
        val = next_u32(state) & mask

    return lb + val


print(bounded_uint(323, 2394691, ctypes_state.value))


@nb.jit(nopython=True)
def bounded_uints(lb, ub, n, state):
    out = np.empty(n, dtype=np.uint32)
    for i in range(n):
        out[i] = bounded_uint(lb, ub, state)


bounded_uints(323, 2394691, 10000000, ctypes_state.value)

"
364	donghui	1	"import numpy as np
import numba as nb

from numpy.random import PCG64
from timeit import timeit

bit_gen = PCG64()
next_d = bit_gen.cffi.next_double
state_addr = bit_gen.cffi.state_address

def normals(n, state):
    out = np.empty(n)
    for i in range((n + 1) // 2):
        x1 = 2.0 * next_d(state) - 1.0
        x2 = 2.0 * next_d(state) - 1.0
        r2 = x1 * x1 + x2 * x2
        while r2 >= 1.0 or r2 == 0.0:
            x1 = 2.0 * next_d(state) - 1.0
            x2 = 2.0 * next_d(state) - 1.0
            r2 = x1 * x1 + x2 * x2
        f = np.sqrt(-2.0 * np.log(r2) / r2)
        out[2 * i] = f * x1
        if 2 * i + 1 < n:
            out[2 * i + 1] = f * x2
    return out

# Compile using Numba
normalsj = nb.jit(normals, nopython=True)
# Must use state address not state with numba
n = 10000

def numbacall():
    return normalsj(n, state_addr)

rg = np.random.Generator(PCG64())

def numpycall():
    return rg.normal(size=n)

# Check that the functions work
r1 = numbacall()
r2 = numpycall()
assert r1.shape == (n,)
assert r1.shape == r2.shape

t1 = timeit(numbacall, number=1000)
print(f'{t1:.2f} secs for {n} PCG64 (Numba/PCG64) gaussian randoms')
t2 = timeit(numpycall, number=1000)
print(f'{t2:.2f} secs for {n} PCG64 (NumPy/PCG64) gaussian randoms')

# example 2

next_u32 = bit_gen.ctypes.next_uint32
ctypes_state = bit_gen.ctypes.state

@nb.jit(nopython=True)
def bounded_uint(lb, ub, state):
    mask = delta = ub - lb
    mask |= mask >> 1
    mask |= mask >> 2
    mask |= mask >> 4
    mask |= mask >> 8
    mask |= mask >> 16

    val = next_u32(state) & mask
    while val > delta:
        val = next_u32(state) & mask

    return lb + val


print(bounded_uint(323, 2394691, ctypes_state.value))


@nb.jit(nopython=True)
def bounded_uints(lb, ub, n, state):
    out = np.empty(n, dtype=np.uint32)
    for i in range(n):
        out[i] = bounded_uint(lb, ub, state)


bounded_uints(323, 2394691, 10000000, ctypes_state.value)

"
335	jackson	3	"import functools
import operator
import itertools

from .extern.jaraco.text import yield_lines
from .extern.jaraco.functools import pass_none
from ._importlib import metadata
from ._itertools import ensure_unique
from .extern.more_itertools import consume


def ensure_valid(ep):
    """"""
    Exercise one of the dynamic properties to trigger
    the pattern match.
    """"""
    ep.extras


def load_group(value, group):
    """"""
    Given a value of an entry point or series of entry points,
    return each as an EntryPoint.
    """"""
    # normalize to a single sequence of lines
    lines = yield_lines(value)
    text = f'[{group}]\n' + '\n'.join(lines)
    return metadata.EntryPoints._from_text(text)


def by_group_and_name(ep):
    return ep.group, ep.name


def validate(eps: metadata.EntryPoints):
    """"""
    Ensure entry points are unique by group and name and validate each.
    """"""
    consume(map(ensure_valid, ensure_unique(eps, key=by_group_and_name)))
    return eps


@functools.singledispatch
def load(eps):
    """"""
    Given a Distribution.entry_points, produce EntryPoints.
    """"""
    groups = itertools.chain.from_iterable(
        load_group(value, group)
        for group, value in eps.items())
    return validate(metadata.EntryPoints(groups))


@load.register(str)
def _(eps):
    r""""""
    >>> ep, = load('[console_scripts]\nfoo=bar')
    >>> ep.group
    'console_scripts'
    >>> ep.name
    'foo'
    >>> ep.value
    'bar'
    """"""
    return validate(metadata.EntryPoints(metadata.EntryPoints._from_text(eps)))


load.register(type(None), lambda x: x)


@pass_none
def render(eps: metadata.EntryPoints):
    by_group = operator.attrgetter('group')
    groups = itertools.groupby(sorted(eps, key=by_group), by_group)

    return '\n'.join(
        f'[{group}]\n{render_items(items)}\n'
        for group, items in groups
    )


def render_items(eps):
    return '\n'.join(
        f'{ep.name} = {ep.value}'
        for ep in sorted(eps)
    )"
335	donghui	3	"import functools
import operator
import itertools

from .extern.jaraco.text import yield_lines
from .extern.jaraco.functools import pass_none
from ._importlib import metadata
from ._itertools import ensure_unique
from .extern.more_itertools import consume


def ensure_valid(ep):
    """"""
    Exercise one of the dynamic properties to trigger
    the pattern match.
    """"""
    ep.extras


def load_group(value, group):
    """"""
    Given a value of an entry point or series of entry points,
    return each as an EntryPoint.
    """"""
    # normalize to a single sequence of lines
    lines = yield_lines(value)
    text = f'[{group}]\n' + '\n'.join(lines)
    return metadata.EntryPoints._from_text(text)


def by_group_and_name(ep):
    return ep.group, ep.name


def validate(eps: metadata.EntryPoints):
    """"""
    Ensure entry points are unique by group and name and validate each.
    """"""
    consume(map(ensure_valid, ensure_unique(eps, key=by_group_and_name)))
    return eps


@functools.singledispatch
def load(eps):
    """"""
    Given a Distribution.entry_points, produce EntryPoints.
    """"""
    groups = itertools.chain.from_iterable(
        load_group(value, group)
        for group, value in eps.items())
    return validate(metadata.EntryPoints(groups))


@load.register(str)
def _(eps):
    r""""""
    >>> ep, = load('[console_scripts]\nfoo=bar')
    >>> ep.group
    'console_scripts'
    >>> ep.name
    'foo'
    >>> ep.value
    'bar'
    """"""
    return validate(metadata.EntryPoints(metadata.EntryPoints._from_text(eps)))


load.register(type(None), lambda x: x)


@pass_none
def render(eps: metadata.EntryPoints):
    by_group = operator.attrgetter('group')
    groups = itertools.groupby(sorted(eps, key=by_group), by_group)

    return '\n'.join(
        f'[{group}]\n{render_items(items)}\n'
        for group, items in groups
    )


def render_items(eps):
    return '\n'.join(
        f'{ep.name} = {ep.value}'
        for ep in sorted(eps)
    )"
275	jackson	1	"import pytest
from traitlets import HasTraits, TraitError
from traitlets.utils.importstring import import_item

from notebook.traittypes import (
    InstanceFromClasses,
    TypeFromClasses
)
from notebook.services.contents.largefilemanager import LargeFileManager


class DummyClass:
    """"""Dummy class for testing Instance""""""


class DummyInt(int):
    """"""Dummy class for testing types.""""""


class Thing(HasTraits):

    a = InstanceFromClasses(
        default_value=2,
        klasses=[
            int,
            str,
            DummyClass,
        ]
    )

    b = TypeFromClasses(
        default_value=None,
        allow_none=True,
        klasses=[
            DummyClass,
            int,
            'notebook.services.contents.manager.ContentsManager'
        ]
    )


class TestInstanceFromClasses:

    @pytest.mark.parametrize(
        'value',
        [1, 'test', DummyClass()]
    )
    def test_good_values(self, value):
        thing = Thing(a=value)
        assert thing.a == value

    @pytest.mark.parametrize(
        'value',
        [2.4, object()]
    )
    def test_bad_values(self, value):
        with pytest.raises(TraitError) as e:
            thing = Thing(a=value)


class TestTypeFromClasses:

    @pytest.mark.parametrize(
        'value',
        [DummyClass, DummyInt, LargeFileManager,
            'notebook.services.contents.manager.ContentsManager']
    )
    def test_good_values(self, value):
        thing = Thing(b=value)
        if isinstance(value, str):
            value = import_item(value)
        assert thing.b == value

    @pytest.mark.parametrize(
        'value',
        [float, object]
    )
    def test_bad_values(self, value):
        with pytest.raises(TraitError) as e:
            thing = Thing(b=value)"
275	donghui	1	"import pytest
from traitlets import HasTraits, TraitError
from traitlets.utils.importstring import import_item

from notebook.traittypes import (
    InstanceFromClasses,
    TypeFromClasses
)
from notebook.services.contents.largefilemanager import LargeFileManager


class DummyClass:
    """"""Dummy class for testing Instance""""""


class DummyInt(int):
    """"""Dummy class for testing types.""""""


class Thing(HasTraits):

    a = InstanceFromClasses(
        default_value=2,
        klasses=[
            int,
            str,
            DummyClass,
        ]
    )

    b = TypeFromClasses(
        default_value=None,
        allow_none=True,
        klasses=[
            DummyClass,
            int,
            'notebook.services.contents.manager.ContentsManager'
        ]
    )


class TestInstanceFromClasses:

    @pytest.mark.parametrize(
        'value',
        [1, 'test', DummyClass()]
    )
    def test_good_values(self, value):
        thing = Thing(a=value)
        assert thing.a == value

    @pytest.mark.parametrize(
        'value',
        [2.4, object()]
    )
    def test_bad_values(self, value):
        with pytest.raises(TraitError) as e:
            thing = Thing(a=value)


class TestTypeFromClasses:

    @pytest.mark.parametrize(
        'value',
        [DummyClass, DummyInt, LargeFileManager,
            'notebook.services.contents.manager.ContentsManager']
    )
    def test_good_values(self, value):
        thing = Thing(b=value)
        if isinstance(value, str):
            value = import_item(value)
        assert thing.b == value

    @pytest.mark.parametrize(
        'value',
        [float, object]
    )
    def test_bad_values(self, value):
        with pytest.raises(TraitError) as e:
            thing = Thing(b=value)"
286	jackson	4	"""""""
Behavioral based tests for offsets and date_range.

This file is adapted from https://github.com/pandas-dev/pandas/pull/18761 -
which was more ambitious but less idiomatic in its use of Hypothesis.

You may wish to consult the previous version for inspiration on further
tests, or when trying to pin down the bugs exposed by the tests below.
""""""
from hypothesis import (
    assume,
    given,
)
import pytest
import pytz

import pandas as pd
from pandas._testing._hypothesis import (
    DATETIME_JAN_1_1900_OPTIONAL_TZ,
    YQM_OFFSET,
)

# ----------------------------------------------------------------
# Offset-specific behaviour tests


@pytest.mark.arm_slow
@given(DATETIME_JAN_1_1900_OPTIONAL_TZ, YQM_OFFSET)
def test_on_offset_implementations(dt, offset):
    assume(not offset.normalize)
    # check that the class-specific implementations of is_on_offset match
    # the general case definition:
    #   (dt + offset) - offset == dt
    try:
        compare = (dt + offset) - offset
    except (pytz.NonExistentTimeError, pytz.AmbiguousTimeError):
        # When dt + offset does not exist or is DST-ambiguous, assume(False) to
        # indicate to hypothesis that this is not a valid test case
        # DST-ambiguous example (GH41906):
        # dt = datetime.datetime(1900, 1, 1, tzinfo=pytz.timezone('Africa/Kinshasa'))
        # offset = MonthBegin(66)
        assume(False)

    assert offset.is_on_offset(dt) == (compare == dt)


@given(YQM_OFFSET)
def test_shift_across_dst(offset):
    # GH#18319 check that 1) timezone is correctly normalized and
    # 2) that hour is not incorrectly changed by this normalization
    assume(not offset.normalize)

    # Note that dti includes a transition across DST boundary
    dti = pd.date_range(
        start=""2017-10-30 12:00:00"", end=""2017-11-06"", freq=""D"", tz=""US/Eastern""
    )
    assert (dti.hour == 12).all()  # we haven't screwed up yet

    res = dti + offset
    assert (res.hour == 12).all()"
286	donghui	4	"""""""
Behavioral based tests for offsets and date_range.

This file is adapted from https://github.com/pandas-dev/pandas/pull/18761 -
which was more ambitious but less idiomatic in its use of Hypothesis.

You may wish to consult the previous version for inspiration on further
tests, or when trying to pin down the bugs exposed by the tests below.
""""""
from hypothesis import (
    assume,
    given,
)
import pytest
import pytz

import pandas as pd
from pandas._testing._hypothesis import (
    DATETIME_JAN_1_1900_OPTIONAL_TZ,
    YQM_OFFSET,
)

# ----------------------------------------------------------------
# Offset-specific behaviour tests


@pytest.mark.arm_slow
@given(DATETIME_JAN_1_1900_OPTIONAL_TZ, YQM_OFFSET)
def test_on_offset_implementations(dt, offset):
    assume(not offset.normalize)
    # check that the class-specific implementations of is_on_offset match
    # the general case definition:
    #   (dt + offset) - offset == dt
    try:
        compare = (dt + offset) - offset
    except (pytz.NonExistentTimeError, pytz.AmbiguousTimeError):
        # When dt + offset does not exist or is DST-ambiguous, assume(False) to
        # indicate to hypothesis that this is not a valid test case
        # DST-ambiguous example (GH41906):
        # dt = datetime.datetime(1900, 1, 1, tzinfo=pytz.timezone('Africa/Kinshasa'))
        # offset = MonthBegin(66)
        assume(False)

    assert offset.is_on_offset(dt) == (compare == dt)


@given(YQM_OFFSET)
def test_shift_across_dst(offset):
    # GH#18319 check that 1) timezone is correctly normalized and
    # 2) that hour is not incorrectly changed by this normalization
    assume(not offset.normalize)

    # Note that dti includes a transition across DST boundary
    dti = pd.date_range(
        start=""2017-10-30 12:00:00"", end=""2017-11-06"", freq=""D"", tz=""US/Eastern""
    )
    assert (dti.hour == 12).all()  # we haven't screwed up yet

    res = dti + offset
    assert (res.hour == 12).all()"
397	jackson	2	"import tempfile, os
from pathlib import Path

from traitlets.config.loader import Config


def setup_module():
    ip.magic('load_ext storemagic')

def test_store_restore():
    assert 'bar' not in ip.user_ns, ""Error: some other test leaked `bar` in user_ns""
    assert 'foo' not in ip.user_ns, ""Error: some other test leaked `foo` in user_ns""
    assert 'foobar' not in ip.user_ns, ""Error: some other test leaked `foobar` in user_ns""
    assert 'foobaz' not in ip.user_ns, ""Error: some other test leaked `foobaz` in user_ns""
    ip.user_ns['foo'] = 78
    ip.magic('alias bar echo ""hello""')
    ip.user_ns['foobar'] = 79
    ip.user_ns['foobaz'] = '80'
    tmpd = tempfile.mkdtemp()
    ip.magic('cd ' + tmpd)
    ip.magic('store foo')
    ip.magic('store bar')
    ip.magic('store foobar foobaz')

    # Check storing
    assert ip.db[""autorestore/foo""] == 78
    assert ""bar"" in ip.db[""stored_aliases""]
    assert ip.db[""autorestore/foobar""] == 79
    assert ip.db[""autorestore/foobaz""] == ""80""

    # Remove those items
    ip.user_ns.pop('foo', None)
    ip.user_ns.pop('foobar', None)
    ip.user_ns.pop('foobaz', None)
    ip.alias_manager.undefine_alias('bar')
    ip.magic('cd -')
    ip.user_ns['_dh'][:] = []

    # Check restoring
    ip.magic(""store -r foo bar foobar foobaz"")
    assert ip.user_ns[""foo""] == 78
    assert ip.alias_manager.is_alias(""bar"")
    assert ip.user_ns[""foobar""] == 79
    assert ip.user_ns[""foobaz""] == ""80""

    ip.magic(""store -r"")  # restores _dh too
    assert any(Path(tmpd).samefile(p) for p in ip.user_ns[""_dh""])

    os.rmdir(tmpd)

def test_autorestore():
    ip.user_ns['foo'] = 95
    ip.magic('store foo')
    del ip.user_ns['foo']
    c = Config()
    c.StoreMagics.autorestore = False
    orig_config = ip.config
    try:
        ip.config = c
        ip.extension_manager.reload_extension(""storemagic"")
        assert ""foo"" not in ip.user_ns
        c.StoreMagics.autorestore = True
        ip.extension_manager.reload_extension(""storemagic"")
        assert ip.user_ns[""foo""] == 95
    finally:
        ip.config = orig_config"
397	donghui	0	"import tempfile, os
from pathlib import Path

from traitlets.config.loader import Config


def setup_module():
    ip.magic('load_ext storemagic')

def test_store_restore():
    assert 'bar' not in ip.user_ns, ""Error: some other test leaked `bar` in user_ns""
    assert 'foo' not in ip.user_ns, ""Error: some other test leaked `foo` in user_ns""
    assert 'foobar' not in ip.user_ns, ""Error: some other test leaked `foobar` in user_ns""
    assert 'foobaz' not in ip.user_ns, ""Error: some other test leaked `foobaz` in user_ns""
    ip.user_ns['foo'] = 78
    ip.magic('alias bar echo ""hello""')
    ip.user_ns['foobar'] = 79
    ip.user_ns['foobaz'] = '80'
    tmpd = tempfile.mkdtemp()
    ip.magic('cd ' + tmpd)
    ip.magic('store foo')
    ip.magic('store bar')
    ip.magic('store foobar foobaz')

    # Check storing
    assert ip.db[""autorestore/foo""] == 78
    assert ""bar"" in ip.db[""stored_aliases""]
    assert ip.db[""autorestore/foobar""] == 79
    assert ip.db[""autorestore/foobaz""] == ""80""

    # Remove those items
    ip.user_ns.pop('foo', None)
    ip.user_ns.pop('foobar', None)
    ip.user_ns.pop('foobaz', None)
    ip.alias_manager.undefine_alias('bar')
    ip.magic('cd -')
    ip.user_ns['_dh'][:] = []

    # Check restoring
    ip.magic(""store -r foo bar foobar foobaz"")
    assert ip.user_ns[""foo""] == 78
    assert ip.alias_manager.is_alias(""bar"")
    assert ip.user_ns[""foobar""] == 79
    assert ip.user_ns[""foobaz""] == ""80""

    ip.magic(""store -r"")  # restores _dh too
    assert any(Path(tmpd).samefile(p) for p in ip.user_ns[""_dh""])

    os.rmdir(tmpd)

def test_autorestore():
    ip.user_ns['foo'] = 95
    ip.magic('store foo')
    del ip.user_ns['foo']
    c = Config()
    c.StoreMagics.autorestore = False
    orig_config = ip.config
    try:
        ip.config = c
        ip.extension_manager.reload_extension(""storemagic"")
        assert ""foo"" not in ip.user_ns
        c.StoreMagics.autorestore = True
        ip.extension_manager.reload_extension(""storemagic"")
        assert ip.user_ns[""foo""] == 95
    finally:
        ip.config = orig_config"
488	jackson	0	"# coding: utf8
from __future__ import unicode_literals

from ...attrs import LIKE_NUM


_num_words = [
    ""zero"",
    ""um"",
    ""dois"",
    ""três"",
    ""tres"",
    ""quatro"",
    ""cinco"",
    ""seis"",
    ""sete"",
    ""oito"",
    ""nove"",
    ""dez"",
    ""onze"",
    ""doze"",
    ""dúzia"",
    ""dúzias"",
    ""duzia"",
    ""duzias"",
    ""treze"",
    ""catorze"",
    ""quinze"",
    ""dezasseis"",
    ""dezassete"",
    ""dezoito"",
    ""dezanove"",
    ""vinte"",
    ""trinta"",
    ""quarenta"",
    ""cinquenta"",
    ""sessenta"",
    ""setenta"",
    ""oitenta"",
    ""noventa"",
    ""cem"",
    ""cento"",
    ""duzentos"",
    ""trezentos"",
    ""quatrocentos"",
    ""quinhentos"",
    ""seicentos"",
    ""setecentos"",
    ""oitocentos"",
    ""novecentos"",
    ""mil"",
    ""milhão"",
    ""milhao"",
    ""milhões"",
    ""milhoes"",
    ""bilhão"",
    ""bilhao"",
    ""bilhões"",
    ""bilhoes"",
    ""trilhão"",
    ""trilhao"",
    ""trilhões"",
    ""trilhoes"",
    ""quadrilhão"",
    ""quadrilhao"",
    ""quadrilhões"",
    ""quadrilhoes"",
]


_ordinal_words = [
    ""primeiro"",
    ""segundo"",
    ""terceiro"",
    ""quarto"",
    ""quinto"",
    ""sexto"",
    ""sétimo"",
    ""oitavo"",
    ""nono"",
    ""décimo"",
    ""vigésimo"",
    ""trigésimo"",
    ""quadragésimo"",
    ""quinquagésimo"",
    ""sexagésimo"",
    ""septuagésimo"",
    ""octogésimo"",
    ""nonagésimo"",
    ""centésimo"",
    ""ducentésimo"",
    ""trecentésimo"",
    ""quadringentésimo"",
    ""quingentésimo"",
    ""sexcentésimo"",
    ""septingentésimo"",
    ""octingentésimo"",
    ""nongentésimo"",
    ""milésimo"",
    ""milionésimo"",
    ""bilionésimo"",
]


def like_num(text):
    if text.startswith((""+"", ""-"", ""±"", ""~"")):
        text = text[1:]
    text = text.replace("","", """").replace(""."", """").replace(""º"", """").replace(""ª"", """")
    if text.isdigit():
        return True
    if text.count(""/"") == 1:
        num, denom = text.split(""/"")
        if num.isdigit() and denom.isdigit():
            return True
    if text.lower() in _num_words:
        return True
    if text.lower() in _ordinal_words:
        return True
    return False


LEX_ATTRS = {LIKE_NUM: like_num}"
488	donghui	0	"# coding: utf8
from __future__ import unicode_literals

from ...attrs import LIKE_NUM


_num_words = [
    ""zero"",
    ""um"",
    ""dois"",
    ""três"",
    ""tres"",
    ""quatro"",
    ""cinco"",
    ""seis"",
    ""sete"",
    ""oito"",
    ""nove"",
    ""dez"",
    ""onze"",
    ""doze"",
    ""dúzia"",
    ""dúzias"",
    ""duzia"",
    ""duzias"",
    ""treze"",
    ""catorze"",
    ""quinze"",
    ""dezasseis"",
    ""dezassete"",
    ""dezoito"",
    ""dezanove"",
    ""vinte"",
    ""trinta"",
    ""quarenta"",
    ""cinquenta"",
    ""sessenta"",
    ""setenta"",
    ""oitenta"",
    ""noventa"",
    ""cem"",
    ""cento"",
    ""duzentos"",
    ""trezentos"",
    ""quatrocentos"",
    ""quinhentos"",
    ""seicentos"",
    ""setecentos"",
    ""oitocentos"",
    ""novecentos"",
    ""mil"",
    ""milhão"",
    ""milhao"",
    ""milhões"",
    ""milhoes"",
    ""bilhão"",
    ""bilhao"",
    ""bilhões"",
    ""bilhoes"",
    ""trilhão"",
    ""trilhao"",
    ""trilhões"",
    ""trilhoes"",
    ""quadrilhão"",
    ""quadrilhao"",
    ""quadrilhões"",
    ""quadrilhoes"",
]


_ordinal_words = [
    ""primeiro"",
    ""segundo"",
    ""terceiro"",
    ""quarto"",
    ""quinto"",
    ""sexto"",
    ""sétimo"",
    ""oitavo"",
    ""nono"",
    ""décimo"",
    ""vigésimo"",
    ""trigésimo"",
    ""quadragésimo"",
    ""quinquagésimo"",
    ""sexagésimo"",
    ""septuagésimo"",
    ""octogésimo"",
    ""nonagésimo"",
    ""centésimo"",
    ""ducentésimo"",
    ""trecentésimo"",
    ""quadringentésimo"",
    ""quingentésimo"",
    ""sexcentésimo"",
    ""septingentésimo"",
    ""octingentésimo"",
    ""nongentésimo"",
    ""milésimo"",
    ""milionésimo"",
    ""bilionésimo"",
]


def like_num(text):
    if text.startswith((""+"", ""-"", ""±"", ""~"")):
        text = text[1:]
    text = text.replace("","", """").replace(""."", """").replace(""º"", """").replace(""ª"", """")
    if text.isdigit():
        return True
    if text.count(""/"") == 1:
        num, denom = text.split(""/"")
        if num.isdigit() and denom.isdigit():
            return True
    if text.lower() in _num_words:
        return True
    if text.lower() in _ordinal_words:
        return True
    return False


LEX_ATTRS = {LIKE_NUM: like_num}"
498	jackson	2	"from django.conf import settings
from django.core import checks
from django.core.exceptions import FieldDoesNotExist
from django.db import models


class CurrentSiteManager(models.Manager):
    ""Use this to limit objects to those associated with the current site.""

    use_in_migrations = True

    def __init__(self, field_name=None):
        super().__init__()
        self.__field_name = field_name

    def check(self, **kwargs):
        errors = super().check(**kwargs)
        errors.extend(self._check_field_name())
        return errors

    def _check_field_name(self):
        field_name = self._get_field_name()
        try:
            field = self.model._meta.get_field(field_name)
        except FieldDoesNotExist:
            return [
                checks.Error(
                    ""CurrentSiteManager could not find a field named '%s'."" % field_name,
                    obj=self,
                    id='sites.E001',
                )
            ]

        if not field.many_to_many and not isinstance(field, (models.ForeignKey)):
            return [
                checks.Error(
                    ""CurrentSiteManager cannot use '%s.%s' as it is not a foreign key or a many-to-many field."" % (
                        self.model._meta.object_name, field_name
                    ),
                    obj=self,
                    id='sites.E002',
                )
            ]

        return []

    def _get_field_name(self):
        """""" Return self.__field_name or 'site' or 'sites'. """"""

        if not self.__field_name:
            try:
                self.model._meta.get_field('site')
            except FieldDoesNotExist:
                self.__field_name = 'sites'
            else:
                self.__field_name = 'site'
        return self.__field_name

    def get_queryset(self):
        return super().get_queryset().filter(**{self._get_field_name() + '__id': settings.SITE_ID})"
498	donghui	2	"from django.conf import settings
from django.core import checks
from django.core.exceptions import FieldDoesNotExist
from django.db import models


class CurrentSiteManager(models.Manager):
    ""Use this to limit objects to those associated with the current site.""

    use_in_migrations = True

    def __init__(self, field_name=None):
        super().__init__()
        self.__field_name = field_name

    def check(self, **kwargs):
        errors = super().check(**kwargs)
        errors.extend(self._check_field_name())
        return errors

    def _check_field_name(self):
        field_name = self._get_field_name()
        try:
            field = self.model._meta.get_field(field_name)
        except FieldDoesNotExist:
            return [
                checks.Error(
                    ""CurrentSiteManager could not find a field named '%s'."" % field_name,
                    obj=self,
                    id='sites.E001',
                )
            ]

        if not field.many_to_many and not isinstance(field, (models.ForeignKey)):
            return [
                checks.Error(
                    ""CurrentSiteManager cannot use '%s.%s' as it is not a foreign key or a many-to-many field."" % (
                        self.model._meta.object_name, field_name
                    ),
                    obj=self,
                    id='sites.E002',
                )
            ]

        return []

    def _get_field_name(self):
        """""" Return self.__field_name or 'site' or 'sites'. """"""

        if not self.__field_name:
            try:
                self.model._meta.get_field('site')
            except FieldDoesNotExist:
                self.__field_name = 'sites'
            else:
                self.__field_name = 'site'
        return self.__field_name

    def get_queryset(self):
        return super().get_queryset().filter(**{self._get_field_name() + '__id': settings.SITE_ID})"
387	jackson	2	"# Copyright 2022 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.


# [START speech_quickstart_v2]
import io

from google.cloud.speech_v2 import SpeechClient
from google.cloud.speech_v2.types import cloud_speech


def quickstart_v2(project_id, recognizer_id, audio_file):
    # Instantiates a client
    client = SpeechClient()

    request = cloud_speech.CreateRecognizerRequest(
        parent=f""projects/{project_id}/locations/global"",
        recognizer_id=recognizer_id,
        recognizer=cloud_speech.Recognizer(
            language_codes=[""en-US""], model=""latest_long""
        ),
    )

    # Creates a Recognizer
    operation = client.create_recognizer(request=request)
    recognizer = operation.result()

    # Reads a file as bytes
    with io.open(audio_file, ""rb"") as f:
        content = f.read()

    config = cloud_speech.RecognitionConfig(auto_decoding_config={})

    request = cloud_speech.RecognizeRequest(
        recognizer=recognizer.name, config=config, content=content
    )

    # Transcribes the audio into text
    response = client.recognize(request=request)

    for result in response.results:
        print(""Transcript: {}"".format(result.alternatives[0].transcript))

    return response


# [END speech_quickstart_v2]


if __name__ == ""__main__"":
    quickstart_v2()"
387	donghui	2	"# Copyright 2022 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.


# [START speech_quickstart_v2]
import io

from google.cloud.speech_v2 import SpeechClient
from google.cloud.speech_v2.types import cloud_speech


def quickstart_v2(project_id, recognizer_id, audio_file):
    # Instantiates a client
    client = SpeechClient()

    request = cloud_speech.CreateRecognizerRequest(
        parent=f""projects/{project_id}/locations/global"",
        recognizer_id=recognizer_id,
        recognizer=cloud_speech.Recognizer(
            language_codes=[""en-US""], model=""latest_long""
        ),
    )

    # Creates a Recognizer
    operation = client.create_recognizer(request=request)
    recognizer = operation.result()

    # Reads a file as bytes
    with io.open(audio_file, ""rb"") as f:
        content = f.read()

    config = cloud_speech.RecognitionConfig(auto_decoding_config={})

    request = cloud_speech.RecognizeRequest(
        recognizer=recognizer.name, config=config, content=content
    )

    # Transcribes the audio into text
    response = client.recognize(request=request)

    for result in response.results:
        print(""Transcript: {}"".format(result.alternatives[0].transcript))

    return response


# [END speech_quickstart_v2]


if __name__ == ""__main__"":
    quickstart_v2()"
296	jackson	2	"import _plotly_utils.basevalidators


class HoverlabelValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(
        self, plotly_name=""hoverlabel"", parent_name=""scatterternary"", **kwargs
    ):
        super(HoverlabelValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Hoverlabel""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            align
                Sets the horizontal alignment of the text
                content within hover label box. Has an effect
                only if the hover label text spans more two or
                more lines
            alignsrc
                Sets the source reference on Chart Studio Cloud
                for `align`.
            bgcolor
                Sets the background color of the hover labels
                for this trace
            bgcolorsrc
                Sets the source reference on Chart Studio Cloud
                for `bgcolor`.
            bordercolor
                Sets the border color of the hover labels for
                this trace.
            bordercolorsrc
                Sets the source reference on Chart Studio Cloud
                for `bordercolor`.
            font
                Sets the font used in hover labels.
            namelength
                Sets the default length (in number of
                characters) of the trace name in the hover
                labels for all traces. -1 shows the whole name
                regardless of length. 0-3 shows the first 0-3
                characters, and an integer >3 will show the
                whole name if it is less than that many
                characters, but if it is longer, will truncate
                to `namelength - 3` characters and add an
                ellipsis.
            namelengthsrc
                Sets the source reference on Chart Studio Cloud
                for `namelength`.
"""""",
            ),
            **kwargs,
        )"
296	donghui	1	"import _plotly_utils.basevalidators


class HoverlabelValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(
        self, plotly_name=""hoverlabel"", parent_name=""scatterternary"", **kwargs
    ):
        super(HoverlabelValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Hoverlabel""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            align
                Sets the horizontal alignment of the text
                content within hover label box. Has an effect
                only if the hover label text spans more two or
                more lines
            alignsrc
                Sets the source reference on Chart Studio Cloud
                for `align`.
            bgcolor
                Sets the background color of the hover labels
                for this trace
            bgcolorsrc
                Sets the source reference on Chart Studio Cloud
                for `bgcolor`.
            bordercolor
                Sets the border color of the hover labels for
                this trace.
            bordercolorsrc
                Sets the source reference on Chart Studio Cloud
                for `bordercolor`.
            font
                Sets the font used in hover labels.
            namelength
                Sets the default length (in number of
                characters) of the trace name in the hover
                labels for all traces. -1 shows the whole name
                regardless of length. 0-3 shows the first 0-3
                characters, and an integer >3 will show the
                whole name if it is less than that many
                characters, but if it is longer, will truncate
                to `namelength - 3` characters and add an
                ellipsis.
            namelengthsrc
                Sets the source reference on Chart Studio Cloud
                for `namelength`.
"""""",
            ),
            **kwargs,
        )"
265	jackson	0	"# Copyright 2019 Google, LLC.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# NOTE:
# These tests are unit tests that mock Pub/Sub.
import base64
import json
import uuid

import mock
import pytest

import main


@pytest.fixture
def client():
    main.app.testing = True
    return main.app.test_client()


def test_empty_payload(client):
    r = client.post(""/"", json="""")
    assert r.status_code == 400


def test_invalid_payload(client):
    r = client.post(""/"", json={""nomessage"": ""invalid""})
    assert r.status_code == 400


def test_invalid_mimetype(client):
    r = client.post(""/"", json=""{ message: true }"")
    assert r.status_code == 400


@mock.patch(""image.blur_offensive_images"", mock.MagicMock(return_value=204))
def test_minimally_valid_message(client):
    data_json = json.dumps({""name"": True, ""bucket"": True})
    data = base64.b64encode(data_json.encode()).decode()

    r = client.post(""/"", json={""message"": {""data"": data}})
    assert r.status_code == 204


def test_call_to_blur_image(client, capsys):
    filename = str(uuid.uuid4())
    blur_bucket = ""blurred-bucket-"" + str(uuid.uuid4())

    data_json = json.dumps({""name"": filename, ""bucket"": blur_bucket})
    data = base64.b64encode(data_json.encode()).decode()

    r = client.post(""/"", json={""message"": {""data"": data}})
    assert r.status_code == 204

    out, _ = capsys.readouterr()
    assert f""The image {filename} was detected as OK"" in out"
265	donghui	0	"# Copyright 2019 Google, LLC.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# NOTE:
# These tests are unit tests that mock Pub/Sub.
import base64
import json
import uuid

import mock
import pytest

import main


@pytest.fixture
def client():
    main.app.testing = True
    return main.app.test_client()


def test_empty_payload(client):
    r = client.post(""/"", json="""")
    assert r.status_code == 400


def test_invalid_payload(client):
    r = client.post(""/"", json={""nomessage"": ""invalid""})
    assert r.status_code == 400


def test_invalid_mimetype(client):
    r = client.post(""/"", json=""{ message: true }"")
    assert r.status_code == 400


@mock.patch(""image.blur_offensive_images"", mock.MagicMock(return_value=204))
def test_minimally_valid_message(client):
    data_json = json.dumps({""name"": True, ""bucket"": True})
    data = base64.b64encode(data_json.encode()).decode()

    r = client.post(""/"", json={""message"": {""data"": data}})
    assert r.status_code == 204


def test_call_to_blur_image(client, capsys):
    filename = str(uuid.uuid4())
    blur_bucket = ""blurred-bucket-"" + str(uuid.uuid4())

    data_json = json.dumps({""name"": filename, ""bucket"": blur_bucket})
    data = base64.b64encode(data_json.encode()).decode()

    r = client.post(""/"", json={""message"": {""data"": data}})
    assert r.status_code == 204

    out, _ = capsys.readouterr()
    assert f""The image {filename} was detected as OK"" in out"
325	jackson	1	"import pytest

import pandas as pd
import pandas._testing as tm


class TestDatetimeIndexFillNA:
    @pytest.mark.parametrize(""tz"", [""US/Eastern"", ""Asia/Tokyo""])
    def test_fillna_datetime64(self, tz):
        # GH 11343
        idx = pd.DatetimeIndex([""2011-01-01 09:00"", pd.NaT, ""2011-01-01 11:00""])

        exp = pd.DatetimeIndex(
            [""2011-01-01 09:00"", ""2011-01-01 10:00"", ""2011-01-01 11:00""]
        )
        tm.assert_index_equal(idx.fillna(pd.Timestamp(""2011-01-01 10:00"")), exp)

        # tz mismatch
        exp = pd.Index(
            [
                pd.Timestamp(""2011-01-01 09:00""),
                pd.Timestamp(""2011-01-01 10:00"", tz=tz),
                pd.Timestamp(""2011-01-01 11:00""),
            ],
            dtype=object,
        )
        tm.assert_index_equal(idx.fillna(pd.Timestamp(""2011-01-01 10:00"", tz=tz)), exp)

        # object
        exp = pd.Index(
            [pd.Timestamp(""2011-01-01 09:00""), ""x"", pd.Timestamp(""2011-01-01 11:00"")],
            dtype=object,
        )
        tm.assert_index_equal(idx.fillna(""x""), exp)

        idx = pd.DatetimeIndex([""2011-01-01 09:00"", pd.NaT, ""2011-01-01 11:00""], tz=tz)

        exp = pd.DatetimeIndex(
            [""2011-01-01 09:00"", ""2011-01-01 10:00"", ""2011-01-01 11:00""], tz=tz
        )
        tm.assert_index_equal(idx.fillna(pd.Timestamp(""2011-01-01 10:00"", tz=tz)), exp)

        exp = pd.Index(
            [
                pd.Timestamp(""2011-01-01 09:00"", tz=tz),
                pd.Timestamp(""2011-01-01 10:00""),
                pd.Timestamp(""2011-01-01 11:00"", tz=tz),
            ],
            dtype=object,
        )
        tm.assert_index_equal(idx.fillna(pd.Timestamp(""2011-01-01 10:00"")), exp)

        # object
        exp = pd.Index(
            [
                pd.Timestamp(""2011-01-01 09:00"", tz=tz),
                ""x"",
                pd.Timestamp(""2011-01-01 11:00"", tz=tz),
            ],
            dtype=object,
        )
        tm.assert_index_equal(idx.fillna(""x""), exp)"
325	donghui	1	"import pytest

import pandas as pd
import pandas._testing as tm


class TestDatetimeIndexFillNA:
    @pytest.mark.parametrize(""tz"", [""US/Eastern"", ""Asia/Tokyo""])
    def test_fillna_datetime64(self, tz):
        # GH 11343
        idx = pd.DatetimeIndex([""2011-01-01 09:00"", pd.NaT, ""2011-01-01 11:00""])

        exp = pd.DatetimeIndex(
            [""2011-01-01 09:00"", ""2011-01-01 10:00"", ""2011-01-01 11:00""]
        )
        tm.assert_index_equal(idx.fillna(pd.Timestamp(""2011-01-01 10:00"")), exp)

        # tz mismatch
        exp = pd.Index(
            [
                pd.Timestamp(""2011-01-01 09:00""),
                pd.Timestamp(""2011-01-01 10:00"", tz=tz),
                pd.Timestamp(""2011-01-01 11:00""),
            ],
            dtype=object,
        )
        tm.assert_index_equal(idx.fillna(pd.Timestamp(""2011-01-01 10:00"", tz=tz)), exp)

        # object
        exp = pd.Index(
            [pd.Timestamp(""2011-01-01 09:00""), ""x"", pd.Timestamp(""2011-01-01 11:00"")],
            dtype=object,
        )
        tm.assert_index_equal(idx.fillna(""x""), exp)

        idx = pd.DatetimeIndex([""2011-01-01 09:00"", pd.NaT, ""2011-01-01 11:00""], tz=tz)

        exp = pd.DatetimeIndex(
            [""2011-01-01 09:00"", ""2011-01-01 10:00"", ""2011-01-01 11:00""], tz=tz
        )
        tm.assert_index_equal(idx.fillna(pd.Timestamp(""2011-01-01 10:00"", tz=tz)), exp)

        exp = pd.Index(
            [
                pd.Timestamp(""2011-01-01 09:00"", tz=tz),
                pd.Timestamp(""2011-01-01 10:00""),
                pd.Timestamp(""2011-01-01 11:00"", tz=tz),
            ],
            dtype=object,
        )
        tm.assert_index_equal(idx.fillna(pd.Timestamp(""2011-01-01 10:00"")), exp)

        # object
        exp = pd.Index(
            [
                pd.Timestamp(""2011-01-01 09:00"", tz=tz),
                ""x"",
                pd.Timestamp(""2011-01-01 11:00"", tz=tz),
            ],
            dtype=object,
        )
        tm.assert_index_equal(idx.fillna(""x""), exp)"
374	jackson	3	"# A version of the ActiveScripting engine that enables rexec support
# This version supports hosting by IE - however, due to Python's
# rexec module being neither completely trusted nor private, it is
# *not* enabled by default.
# As of Python 2.2, rexec is simply not available - thus, if you use this,
# a HTML page can do almost *anything* at all on your machine.

# You almost certainly do NOT want to use thus!

import pythoncom
from win32com.axscript import axscript
import winerror
from . import pyscript

INTERFACE_USES_DISPEX = 0x00000004  # Object knows to use IDispatchEx
INTERFACE_USES_SECURITY_MANAGER = (
    0x00000008  # Object knows to use IInternetHostSecurityManager
)


class PyScriptRExec(pyscript.PyScript):
    # Setup the auto-registration stuff...
    _reg_verprogid_ = ""Python.AXScript-rexec.2""
    _reg_progid_ = ""Python""  # Same ProgID as the standard engine.
    # 	_reg_policy_spec_ = default
    _reg_catids_ = [axscript.CATID_ActiveScript, axscript.CATID_ActiveScriptParse]
    _reg_desc_ = ""Python ActiveX Scripting Engine (with rexec support)""
    _reg_clsid_ = ""{69c2454b-efa2-455b-988c-c3651c4a2f69}""
    _reg_class_spec_ = ""win32com.axscript.client.pyscript_rexec.PyScriptRExec""
    _reg_remove_keys_ = [("".pys"",), (""pysFile"",)]
    _reg_threading_ = ""Apartment""

    def _GetSupportedInterfaceSafetyOptions(self):
        # print ""**** calling"", pyscript.PyScript._GetSupportedInterfaceSafetyOptions, ""**->"", pyscript.PyScript._GetSupportedInterfaceSafetyOptions(self)
        return (
            INTERFACE_USES_DISPEX
            | INTERFACE_USES_SECURITY_MANAGER
            | axscript.INTERFACESAFE_FOR_UNTRUSTED_DATA
            | axscript.INTERFACESAFE_FOR_UNTRUSTED_CALLER
        )


if __name__ == ""__main__"":
    print(""WARNING: By registering this engine, you are giving remote HTML code"")
    print(""the ability to execute *any* code on your system."")
    print()
    print(""You almost certainly do NOT want to do this."")
    print(""You have been warned, and are doing this at your own (significant) risk"")
    pyscript.Register(PyScriptRExec)"
374	donghui	2	"# A version of the ActiveScripting engine that enables rexec support
# This version supports hosting by IE - however, due to Python's
# rexec module being neither completely trusted nor private, it is
# *not* enabled by default.
# As of Python 2.2, rexec is simply not available - thus, if you use this,
# a HTML page can do almost *anything* at all on your machine.

# You almost certainly do NOT want to use thus!

import pythoncom
from win32com.axscript import axscript
import winerror
from . import pyscript

INTERFACE_USES_DISPEX = 0x00000004  # Object knows to use IDispatchEx
INTERFACE_USES_SECURITY_MANAGER = (
    0x00000008  # Object knows to use IInternetHostSecurityManager
)


class PyScriptRExec(pyscript.PyScript):
    # Setup the auto-registration stuff...
    _reg_verprogid_ = ""Python.AXScript-rexec.2""
    _reg_progid_ = ""Python""  # Same ProgID as the standard engine.
    # 	_reg_policy_spec_ = default
    _reg_catids_ = [axscript.CATID_ActiveScript, axscript.CATID_ActiveScriptParse]
    _reg_desc_ = ""Python ActiveX Scripting Engine (with rexec support)""
    _reg_clsid_ = ""{69c2454b-efa2-455b-988c-c3651c4a2f69}""
    _reg_class_spec_ = ""win32com.axscript.client.pyscript_rexec.PyScriptRExec""
    _reg_remove_keys_ = [("".pys"",), (""pysFile"",)]
    _reg_threading_ = ""Apartment""

    def _GetSupportedInterfaceSafetyOptions(self):
        # print ""**** calling"", pyscript.PyScript._GetSupportedInterfaceSafetyOptions, ""**->"", pyscript.PyScript._GetSupportedInterfaceSafetyOptions(self)
        return (
            INTERFACE_USES_DISPEX
            | INTERFACE_USES_SECURITY_MANAGER
            | axscript.INTERFACESAFE_FOR_UNTRUSTED_DATA
            | axscript.INTERFACESAFE_FOR_UNTRUSTED_CALLER
        )


if __name__ == ""__main__"":
    print(""WARNING: By registering this engine, you are giving remote HTML code"")
    print(""the ability to execute *any* code on your system."")
    print()
    print(""You almost certainly do NOT want to do this."")
    print(""You have been warned, and are doing this at your own (significant) risk"")
    pyscript.Register(PyScriptRExec)"
340	jackson	2	"# Copyright 2020 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the 'License');
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an 'AS IS' BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# [START functions_pubsub_integration_test]
import base64
import os
import subprocess
import uuid

import requests
from requests.packages.urllib3.util.retry import Retry


def test_print_name():
    name = str(uuid.uuid4())
    port = 8088  # Each running framework instance needs a unique port

    encoded_name = base64.b64encode(name.encode('utf-8')).decode('utf-8')
    pubsub_message = {
        'data': {'data': encoded_name}
    }

    process = subprocess.Popen(
      [
        'functions-framework',
        '--target', 'hello_pubsub',
        '--signature-type', 'event',
        '--port', str(port)
      ],
      cwd=os.path.dirname(__file__),
      stdout=subprocess.PIPE
    )

    # Send HTTP request simulating Pub/Sub message
    # (GCF translates Pub/Sub messages to HTTP requests internally)
    url = f'http://localhost:{port}/'

    retry_policy = Retry(total=6, backoff_factor=1)
    retry_adapter = requests.adapters.HTTPAdapter(
      max_retries=retry_policy)

    session = requests.Session()
    session.mount(url, retry_adapter)

    response = session.post(url, json=pubsub_message)

    assert response.status_code == 200

    # Stop the functions framework process
    process.kill()
    process.wait()
    out, err = process.communicate()

    print(out, err, response.content)

    assert f'Hello {name}!' in str(out)
# [END functions_pubsub_integration_test]"
340	donghui	1	"# Copyright 2020 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the 'License');
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an 'AS IS' BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# [START functions_pubsub_integration_test]
import base64
import os
import subprocess
import uuid

import requests
from requests.packages.urllib3.util.retry import Retry


def test_print_name():
    name = str(uuid.uuid4())
    port = 8088  # Each running framework instance needs a unique port

    encoded_name = base64.b64encode(name.encode('utf-8')).decode('utf-8')
    pubsub_message = {
        'data': {'data': encoded_name}
    }

    process = subprocess.Popen(
      [
        'functions-framework',
        '--target', 'hello_pubsub',
        '--signature-type', 'event',
        '--port', str(port)
      ],
      cwd=os.path.dirname(__file__),
      stdout=subprocess.PIPE
    )

    # Send HTTP request simulating Pub/Sub message
    # (GCF translates Pub/Sub messages to HTTP requests internally)
    url = f'http://localhost:{port}/'

    retry_policy = Retry(total=6, backoff_factor=1)
    retry_adapter = requests.adapters.HTTPAdapter(
      max_retries=retry_policy)

    session = requests.Session()
    session.mount(url, retry_adapter)

    response = session.post(url, json=pubsub_message)

    assert response.status_code == 200

    # Stop the functions framework process
    process.kill()
    process.wait()
    out, err = process.communicate()

    print(out, err, response.content)

    assert f'Hello {name}!' in str(out)
# [END functions_pubsub_integration_test]"
311	jackson	0	"#!/usr/bin/env python
#
# Copyright 2017 the original author or authors.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

from lxml import etree
import structlog
from netconf.nc_rpc.rpc import Rpc
import netconf.nc_common.error as ncerror

log = structlog.get_logger()


class CloseSession(Rpc):
    def __init__(self, request, request_xml, grpc_client, session,
                 capabilities):
        super(CloseSession, self).__init__(request, request_xml, grpc_client,
                                           session, capabilities)
        self._validate_parameters()

    def execute(self):
        log.info('close-session-request', session=self.session.session_id)
        if self.rpc_response.is_error:
            return self.rpc_response

        self.rpc_response.node = etree.Element(""ok"")

        # Set the close session flag
        self.rpc_response.close_session = True
        return self.rpc_response

    def _validate_parameters(self):

        if self.request:
            try:
                if self.request['command'] != 'close-session':
                    self.rpc_response.is_error = True
                    self.rpc_response.node = ncerror.BadMsg(self.request_xml)
                    return

            except Exception as e:
                self.rpc_response.is_error = True
                self.rpc_response.node = ncerror.ServerException(
                    self.request_xml)
                return"
311	donghui	0	"#!/usr/bin/env python
#
# Copyright 2017 the original author or authors.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

from lxml import etree
import structlog
from netconf.nc_rpc.rpc import Rpc
import netconf.nc_common.error as ncerror

log = structlog.get_logger()


class CloseSession(Rpc):
    def __init__(self, request, request_xml, grpc_client, session,
                 capabilities):
        super(CloseSession, self).__init__(request, request_xml, grpc_client,
                                           session, capabilities)
        self._validate_parameters()

    def execute(self):
        log.info('close-session-request', session=self.session.session_id)
        if self.rpc_response.is_error:
            return self.rpc_response

        self.rpc_response.node = etree.Element(""ok"")

        # Set the close session flag
        self.rpc_response.close_session = True
        return self.rpc_response

    def _validate_parameters(self):

        if self.request:
            try:
                if self.request['command'] != 'close-session':
                    self.rpc_response.is_error = True
                    self.rpc_response.node = ncerror.BadMsg(self.request_xml)
                    return

            except Exception as e:
                self.rpc_response.is_error = True
                self.rpc_response.node = ncerror.ServerException(
                    self.request_xml)
                return"
251	jackson	4	"""""""
Provide urlresolver functions that return fully qualified URLs or view names
""""""
from django.urls import NoReverseMatch
from django.urls import reverse as django_reverse
from django.utils.functional import lazy

from rest_framework.settings import api_settings
from rest_framework.utils.urls import replace_query_param


def preserve_builtin_query_params(url, request=None):
    """"""
    Given an incoming request, and an outgoing URL representation,
    append the value of any built-in query parameters.
    """"""
    if request is None:
        return url

    overrides = [
        api_settings.URL_FORMAT_OVERRIDE,
    ]

    for param in overrides:
        if param and (param in request.GET):
            value = request.GET[param]
            url = replace_query_param(url, param, value)

    return url


def reverse(viewname, args=None, kwargs=None, request=None, format=None, **extra):
    """"""
    If versioning is being used then we pass any `reverse` calls through
    to the versioning scheme instance, so that the resulting URL
    can be modified if needed.
    """"""
    scheme = getattr(request, 'versioning_scheme', None)
    if scheme is not None:
        try:
            url = scheme.reverse(viewname, args, kwargs, request, format, **extra)
        except NoReverseMatch:
            # In case the versioning scheme reversal fails, fallback to the
            # default implementation
            url = _reverse(viewname, args, kwargs, request, format, **extra)
    else:
        url = _reverse(viewname, args, kwargs, request, format, **extra)

    return preserve_builtin_query_params(url, request)


def _reverse(viewname, args=None, kwargs=None, request=None, format=None, **extra):
    """"""
    Same as `django.urls.reverse`, but optionally takes a request
    and returns a fully qualified URL, using the request to get the base URL.
    """"""
    if format is not None:
        kwargs = kwargs or {}
        kwargs['format'] = format
    url = django_reverse(viewname, args=args, kwargs=kwargs, **extra)
    if request:
        return request.build_absolute_uri(url)
    return url


reverse_lazy = lazy(reverse, str)"
251	donghui	2	"""""""
Provide urlresolver functions that return fully qualified URLs or view names
""""""
from django.urls import NoReverseMatch
from django.urls import reverse as django_reverse
from django.utils.functional import lazy

from rest_framework.settings import api_settings
from rest_framework.utils.urls import replace_query_param


def preserve_builtin_query_params(url, request=None):
    """"""
    Given an incoming request, and an outgoing URL representation,
    append the value of any built-in query parameters.
    """"""
    if request is None:
        return url

    overrides = [
        api_settings.URL_FORMAT_OVERRIDE,
    ]

    for param in overrides:
        if param and (param in request.GET):
            value = request.GET[param]
            url = replace_query_param(url, param, value)

    return url


def reverse(viewname, args=None, kwargs=None, request=None, format=None, **extra):
    """"""
    If versioning is being used then we pass any `reverse` calls through
    to the versioning scheme instance, so that the resulting URL
    can be modified if needed.
    """"""
    scheme = getattr(request, 'versioning_scheme', None)
    if scheme is not None:
        try:
            url = scheme.reverse(viewname, args, kwargs, request, format, **extra)
        except NoReverseMatch:
            # In case the versioning scheme reversal fails, fallback to the
            # default implementation
            url = _reverse(viewname, args, kwargs, request, format, **extra)
    else:
        url = _reverse(viewname, args, kwargs, request, format, **extra)

    return preserve_builtin_query_params(url, request)


def _reverse(viewname, args=None, kwargs=None, request=None, format=None, **extra):
    """"""
    Same as `django.urls.reverse`, but optionally takes a request
    and returns a fully qualified URL, using the request to get the base URL.
    """"""
    if format is not None:
        kwargs = kwargs or {}
        kwargs['format'] = format
    url = django_reverse(viewname, args=args, kwargs=kwargs, **extra)
    if request:
        return request.build_absolute_uri(url)
    return url


reverse_lazy = lazy(reverse, str)"
331	jackson	3	"""""""
    pygments.styles.native
    ~~~~~~~~~~~~~~~~~~~~~~

    pygments version of my ""native"" vim theme.

    :copyright: Copyright 2006-2022 by the Pygments team, see AUTHORS.
    :license: BSD, see LICENSE for details.
""""""

from pygments.style import Style
from pygments.token import Keyword, Name, Comment, String, Error, \
     Number, Operator, Generic, Token, Whitespace


class NativeStyle(Style):
    """"""
    Pygments version of the ""native"" vim theme.
    """"""

    background_color = '#202020'
    highlight_color = '#404040'
    line_number_color = '#aaaaaa'

    styles = {
        Token:              '#d0d0d0',
        Whitespace:         '#666666',

        Comment:            'italic #ababab',
        Comment.Preproc:    'noitalic bold #cd2828',
        Comment.Special:    'noitalic bold #e50808 bg:#520000',

        Keyword:            'bold #6ebf26',
        Keyword.Pseudo:     'nobold',
        Operator.Word:      'bold #6ebf26',

        String:             '#ed9d13',
        String.Other:       '#ffa500',

        Number:             '#51b2fd',

        Name.Builtin:       '#2fbccd',
        Name.Variable:      '#40ffff',
        Name.Constant:      '#40ffff',
        Name.Class:         'underline #71adff',
        Name.Function:      '#71adff',
        Name.Namespace:     'underline #71adff',
        Name.Exception:     '#bbbbbb',
        Name.Tag:           'bold #6ebf26',
        Name.Attribute:     '#bbbbbb',
        Name.Decorator:     '#ffa500',

        Generic.Heading:    'bold #ffffff',
        Generic.Subheading: 'underline #ffffff',
        Generic.Deleted:    '#d22323',
        Generic.Inserted:   '#589819',
        Generic.Error:      '#d22323',
        Generic.Emph:       'italic',
        Generic.Strong:     'bold',
        Generic.Prompt:     '#aaaaaa',
        Generic.Output:     '#cccccc',
        Generic.Traceback:  '#d22323',

        Error:              'bg:#e3d2d2 #a61717'
    }"
331	donghui	1	"""""""
    pygments.styles.native
    ~~~~~~~~~~~~~~~~~~~~~~

    pygments version of my ""native"" vim theme.

    :copyright: Copyright 2006-2022 by the Pygments team, see AUTHORS.
    :license: BSD, see LICENSE for details.
""""""

from pygments.style import Style
from pygments.token import Keyword, Name, Comment, String, Error, \
     Number, Operator, Generic, Token, Whitespace


class NativeStyle(Style):
    """"""
    Pygments version of the ""native"" vim theme.
    """"""

    background_color = '#202020'
    highlight_color = '#404040'
    line_number_color = '#aaaaaa'

    styles = {
        Token:              '#d0d0d0',
        Whitespace:         '#666666',

        Comment:            'italic #ababab',
        Comment.Preproc:    'noitalic bold #cd2828',
        Comment.Special:    'noitalic bold #e50808 bg:#520000',

        Keyword:            'bold #6ebf26',
        Keyword.Pseudo:     'nobold',
        Operator.Word:      'bold #6ebf26',

        String:             '#ed9d13',
        String.Other:       '#ffa500',

        Number:             '#51b2fd',

        Name.Builtin:       '#2fbccd',
        Name.Variable:      '#40ffff',
        Name.Constant:      '#40ffff',
        Name.Class:         'underline #71adff',
        Name.Function:      '#71adff',
        Name.Namespace:     'underline #71adff',
        Name.Exception:     '#bbbbbb',
        Name.Tag:           'bold #6ebf26',
        Name.Attribute:     '#bbbbbb',
        Name.Decorator:     '#ffa500',

        Generic.Heading:    'bold #ffffff',
        Generic.Subheading: 'underline #ffffff',
        Generic.Deleted:    '#d22323',
        Generic.Inserted:   '#589819',
        Generic.Error:      '#d22323',
        Generic.Emph:       'italic',
        Generic.Strong:     'bold',
        Generic.Prompt:     '#aaaaaa',
        Generic.Output:     '#cccccc',
        Generic.Traceback:  '#d22323',

        Error:              'bg:#e3d2d2 #a61717'
    }"
271	jackson	2	"import _plotly_utils.basevalidators


class InsidetextfontValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(
        self, plotly_name=""insidetextfont"", parent_name=""funnelarea"", **kwargs
    ):
        super(InsidetextfontValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Insidetextfont""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            color

            colorsrc
                Sets the source reference on Chart Studio Cloud
                for `color`.
            family
                HTML font family - the typeface that will be
                applied by the web browser. The web browser
                will only be able to apply a font if it is
                available on the system which it operates.
                Provide multiple font families, separated by
                commas, to indicate the preference in which to
                apply fonts if they aren't available on the
                system. The Chart Studio Cloud (at
                https://chart-studio.plotly.com or on-premise)
                generates images on a server, where only a
                select number of fonts are installed and
                supported. These include ""Arial"", ""Balto"",
                ""Courier New"", ""Droid Sans"",, ""Droid Serif"",
                ""Droid Sans Mono"", ""Gravitas One"", ""Old
                Standard TT"", ""Open Sans"", ""Overpass"", ""PT Sans
                Narrow"", ""Raleway"", ""Times New Roman"".
            familysrc
                Sets the source reference on Chart Studio Cloud
                for `family`.
            size

            sizesrc
                Sets the source reference on Chart Studio Cloud
                for `size`.
"""""",
            ),
            **kwargs,
        )"
271	donghui	1	"import _plotly_utils.basevalidators


class InsidetextfontValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(
        self, plotly_name=""insidetextfont"", parent_name=""funnelarea"", **kwargs
    ):
        super(InsidetextfontValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Insidetextfont""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            color

            colorsrc
                Sets the source reference on Chart Studio Cloud
                for `color`.
            family
                HTML font family - the typeface that will be
                applied by the web browser. The web browser
                will only be able to apply a font if it is
                available on the system which it operates.
                Provide multiple font families, separated by
                commas, to indicate the preference in which to
                apply fonts if they aren't available on the
                system. The Chart Studio Cloud (at
                https://chart-studio.plotly.com or on-premise)
                generates images on a server, where only a
                select number of fonts are installed and
                supported. These include ""Arial"", ""Balto"",
                ""Courier New"", ""Droid Sans"",, ""Droid Serif"",
                ""Droid Sans Mono"", ""Gravitas One"", ""Old
                Standard TT"", ""Open Sans"", ""Overpass"", ""PT Sans
                Narrow"", ""Raleway"", ""Times New Roman"".
            familysrc
                Sets the source reference on Chart Studio Cloud
                for `family`.
            size

            sizesrc
                Sets the source reference on Chart Studio Cloud
                for `size`.
"""""",
            ),
            **kwargs,
        )"
360	jackson	2	"""""""Manager to read and modify frontend config data in JSON files.
""""""
# Copyright (c) Jupyter Development Team.
# Distributed under the terms of the Modified BSD License.

import os.path

from notebook.config_manager import BaseJSONConfigManager, recursive_update
from jupyter_core.paths import jupyter_config_dir, jupyter_config_path
from traitlets import Unicode, Instance, List, observe, default
from traitlets.config import LoggingConfigurable


class ConfigManager(LoggingConfigurable):
    """"""Config Manager used for storing notebook frontend config""""""

    # Public API

    def get(self, section_name):
        """"""Get the config from all config sections.""""""
        config = {}
        # step through back to front, to ensure front of the list is top priority
        for p in self.read_config_path[::-1]:
            cm = BaseJSONConfigManager(config_dir=p)
            recursive_update(config, cm.get(section_name))
        return config

    def set(self, section_name, data):
        """"""Set the config only to the user's config.""""""
        return self.write_config_manager.set(section_name, data)

    def update(self, section_name, new_data):
        """"""Update the config only to the user's config.""""""
        return self.write_config_manager.update(section_name, new_data)

    # Private API

    read_config_path = List(Unicode())

    @default('read_config_path')
    def _default_read_config_path(self):
        return [os.path.join(p, 'nbconfig') for p in jupyter_config_path()]

    write_config_dir = Unicode()

    @default('write_config_dir')
    def _default_write_config_dir(self):
        return os.path.join(jupyter_config_dir(), 'nbconfig')

    write_config_manager = Instance(BaseJSONConfigManager)

    @default('write_config_manager')
    def _default_write_config_manager(self):
        return BaseJSONConfigManager(config_dir=self.write_config_dir)

    @observe('write_config_dir')
    def _update_write_config_dir(self, change):
        self.write_config_manager = BaseJSONConfigManager(config_dir=self.write_config_dir)"
360	donghui	2	"""""""Manager to read and modify frontend config data in JSON files.
""""""
# Copyright (c) Jupyter Development Team.
# Distributed under the terms of the Modified BSD License.

import os.path

from notebook.config_manager import BaseJSONConfigManager, recursive_update
from jupyter_core.paths import jupyter_config_dir, jupyter_config_path
from traitlets import Unicode, Instance, List, observe, default
from traitlets.config import LoggingConfigurable


class ConfigManager(LoggingConfigurable):
    """"""Config Manager used for storing notebook frontend config""""""

    # Public API

    def get(self, section_name):
        """"""Get the config from all config sections.""""""
        config = {}
        # step through back to front, to ensure front of the list is top priority
        for p in self.read_config_path[::-1]:
            cm = BaseJSONConfigManager(config_dir=p)
            recursive_update(config, cm.get(section_name))
        return config

    def set(self, section_name, data):
        """"""Set the config only to the user's config.""""""
        return self.write_config_manager.set(section_name, data)

    def update(self, section_name, new_data):
        """"""Update the config only to the user's config.""""""
        return self.write_config_manager.update(section_name, new_data)

    # Private API

    read_config_path = List(Unicode())

    @default('read_config_path')
    def _default_read_config_path(self):
        return [os.path.join(p, 'nbconfig') for p in jupyter_config_path()]

    write_config_dir = Unicode()

    @default('write_config_dir')
    def _default_write_config_dir(self):
        return os.path.join(jupyter_config_dir(), 'nbconfig')

    write_config_manager = Instance(BaseJSONConfigManager)

    @default('write_config_manager')
    def _default_write_config_manager(self):
        return BaseJSONConfigManager(config_dir=self.write_config_dir)

    @observe('write_config_dir')
    def _update_write_config_dir(self, change):
        self.write_config_manager = BaseJSONConfigManager(config_dir=self.write_config_dir)"
393	jackson	3	"# Copyright 2017-present Open Networking Foundation
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# Copyright (c) 2008 The Board of Trustees of The Leland Stanford Junior University
# Copyright (c) 2011, 2012 Open Networking Foundation
# Copyright (c) 2012, 2013 Big Switch Networks, Inc.
# See the file LICENSE.pyloxi which should have been included in the source distribution
# Automatically generated by LOXI from template toplevel_init.py
# Do not modify

version_names = {
    1: ""1.0"",
    2: ""1.1"",
    3: ""1.2"",
    4: ""1.3"",
    5: ""1.4"",
}

def protocol(ver):
    """"""
    Import and return the protocol module for the given wire version.
    """"""
    if ver == 1:
        import of10
        return of10

    if ver == 2:
        import of11
        return of11

    if ver == 3:
        import of12
        return of12

    if ver == 4:
        import of13
        return of13

    if ver == 5:
        import of14
        return of14

    raise ValueError

class ProtocolError(Exception):
    """"""
    Raised when failing to deserialize an invalid OpenFlow message.
    """"""
    pass

class Unimplemented(Exception):
    """"""
    Raised when an OpenFlow feature is not yet implemented in PyLoxi.
    """"""
    pass

def unimplemented(msg):
    raise Unimplemented(msg)

class OFObject(object):
    """"""
    Superclass of all OpenFlow classes
    """"""
    def __init__(self, *args):
        raise NotImplementedError(""cannot instantiate abstract class"")

    def __ne__(self, other):
        return not self.__eq__(other)

    def show(self):
        import loxi.pp
        return loxi.pp.pp(self)"
393	donghui	2	"# Copyright 2017-present Open Networking Foundation
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# Copyright (c) 2008 The Board of Trustees of The Leland Stanford Junior University
# Copyright (c) 2011, 2012 Open Networking Foundation
# Copyright (c) 2012, 2013 Big Switch Networks, Inc.
# See the file LICENSE.pyloxi which should have been included in the source distribution
# Automatically generated by LOXI from template toplevel_init.py
# Do not modify

version_names = {
    1: ""1.0"",
    2: ""1.1"",
    3: ""1.2"",
    4: ""1.3"",
    5: ""1.4"",
}

def protocol(ver):
    """"""
    Import and return the protocol module for the given wire version.
    """"""
    if ver == 1:
        import of10
        return of10

    if ver == 2:
        import of11
        return of11

    if ver == 3:
        import of12
        return of12

    if ver == 4:
        import of13
        return of13

    if ver == 5:
        import of14
        return of14

    raise ValueError

class ProtocolError(Exception):
    """"""
    Raised when failing to deserialize an invalid OpenFlow message.
    """"""
    pass

class Unimplemented(Exception):
    """"""
    Raised when an OpenFlow feature is not yet implemented in PyLoxi.
    """"""
    pass

def unimplemented(msg):
    raise Unimplemented(msg)

class OFObject(object):
    """"""
    Superclass of all OpenFlow classes
    """"""
    def __init__(self, *args):
        raise NotImplementedError(""cannot instantiate abstract class"")

    def __ne__(self, other):
        return not self.__eq__(other)

    def show(self):
        import loxi.pp
        return loxi.pp.pp(self)"
282	jackson	0	"#!/usr/bin/env python

# Copyright 2020 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import os

from google.cloud.automl_v1beta1 import Model

import pytest

import automl_tables_model
import automl_tables_predict
import model_test


PROJECT = os.environ[""GOOGLE_CLOUD_PROJECT""]
REGION = ""us-central1""
STATIC_MODEL = model_test.STATIC_MODEL
GCS_INPUT = ""gs://{}-automl-tables-test/bank-marketing.csv"".format(PROJECT)
GCS_OUTPUT = ""gs://{}-automl-tables-test/TABLE_TEST_OUTPUT/"".format(PROJECT)
BQ_INPUT = ""bq://{}.automl_test.bank_marketing"".format(PROJECT)
BQ_OUTPUT = ""bq://{}"".format(PROJECT)
PARAMS = {}


@pytest.mark.slow
def test_batch_predict(capsys):
    ensure_model_online()

    automl_tables_predict.batch_predict(
        PROJECT, REGION, STATIC_MODEL, GCS_INPUT, GCS_OUTPUT, PARAMS
    )
    out, _ = capsys.readouterr()
    assert ""Batch prediction complete"" in out


@pytest.mark.slow
def test_batch_predict_bq(capsys):
    ensure_model_online()
    automl_tables_predict.batch_predict_bq(
        PROJECT, REGION, STATIC_MODEL, BQ_INPUT, BQ_OUTPUT, PARAMS
    )
    out, _ = capsys.readouterr()
    assert ""Batch prediction complete"" in out


def ensure_model_online():
    model = model_test.ensure_model_ready()
    if model.deployment_state != Model.DeploymentState.DEPLOYED:
        automl_tables_model.deploy_model(PROJECT, REGION, model.display_name)

    return automl_tables_model.get_model(PROJECT, REGION, model.display_name)"
282	donghui	0	"#!/usr/bin/env python

# Copyright 2020 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import os

from google.cloud.automl_v1beta1 import Model

import pytest

import automl_tables_model
import automl_tables_predict
import model_test


PROJECT = os.environ[""GOOGLE_CLOUD_PROJECT""]
REGION = ""us-central1""
STATIC_MODEL = model_test.STATIC_MODEL
GCS_INPUT = ""gs://{}-automl-tables-test/bank-marketing.csv"".format(PROJECT)
GCS_OUTPUT = ""gs://{}-automl-tables-test/TABLE_TEST_OUTPUT/"".format(PROJECT)
BQ_INPUT = ""bq://{}.automl_test.bank_marketing"".format(PROJECT)
BQ_OUTPUT = ""bq://{}"".format(PROJECT)
PARAMS = {}


@pytest.mark.slow
def test_batch_predict(capsys):
    ensure_model_online()

    automl_tables_predict.batch_predict(
        PROJECT, REGION, STATIC_MODEL, GCS_INPUT, GCS_OUTPUT, PARAMS
    )
    out, _ = capsys.readouterr()
    assert ""Batch prediction complete"" in out


@pytest.mark.slow
def test_batch_predict_bq(capsys):
    ensure_model_online()
    automl_tables_predict.batch_predict_bq(
        PROJECT, REGION, STATIC_MODEL, BQ_INPUT, BQ_OUTPUT, PARAMS
    )
    out, _ = capsys.readouterr()
    assert ""Batch prediction complete"" in out


def ensure_model_online():
    model = model_test.ensure_model_ready()
    if model.deployment_state != Model.DeploymentState.DEPLOYED:
        automl_tables_model.deploy_model(PROJECT, REGION, model.display_name)

    return automl_tables_model.get_model(PROJECT, REGION, model.display_name)"
354	jackson	1	"import builtins
import logging
import signal
import threading
import traceback
import warnings

import trio


class TrioRunner:
    def __init__(self):
        self._cell_cancel_scope = None
        self._trio_token = None

    def initialize(self, kernel, io_loop):
        kernel.shell.set_trio_runner(self)
        kernel.shell.run_line_magic(""autoawait"", ""trio"")
        kernel.shell.magics_manager.magics[""line""][""autoawait""] = lambda _: warnings.warn(
            ""Autoawait isn't allowed in Trio background loop mode.""
        )
        bg_thread = threading.Thread(target=io_loop.start, daemon=True, name=""TornadoBackground"")
        bg_thread.start()

    def interrupt(self, signum, frame):
        if self._cell_cancel_scope:
            self._cell_cancel_scope.cancel()
        else:
            raise Exception(""Kernel interrupted but no cell is running"")

    def run(self):
        old_sig = signal.signal(signal.SIGINT, self.interrupt)

        def log_nursery_exc(exc):
            exc = ""\n"".join(traceback.format_exception(type(exc), exc, exc.__traceback__))
            logging.error(""An exception occurred in a global nursery task.\n%s"", exc)

        async def trio_main():
            self._trio_token = trio.lowlevel.current_trio_token()
            async with trio.open_nursery() as nursery:
                # TODO This hack prevents the nursery from cancelling all child
                # tasks when an uncaught exception occurs, but it's ugly.
                nursery._add_exc = log_nursery_exc
                builtins.GLOBAL_NURSERY = nursery  # type:ignore[attr-defined]
                await trio.sleep_forever()

        trio.run(trio_main)
        signal.signal(signal.SIGINT, old_sig)

    def __call__(self, async_fn):
        async def loc(coro):
            self._cell_cancel_scope = trio.CancelScope()
            with self._cell_cancel_scope:
                return await coro
            self._cell_cancel_scope = None

        return trio.from_thread.run(loc, async_fn, trio_token=self._trio_token)"
354	donghui	1	"import builtins
import logging
import signal
import threading
import traceback
import warnings

import trio


class TrioRunner:
    def __init__(self):
        self._cell_cancel_scope = None
        self._trio_token = None

    def initialize(self, kernel, io_loop):
        kernel.shell.set_trio_runner(self)
        kernel.shell.run_line_magic(""autoawait"", ""trio"")
        kernel.shell.magics_manager.magics[""line""][""autoawait""] = lambda _: warnings.warn(
            ""Autoawait isn't allowed in Trio background loop mode.""
        )
        bg_thread = threading.Thread(target=io_loop.start, daemon=True, name=""TornadoBackground"")
        bg_thread.start()

    def interrupt(self, signum, frame):
        if self._cell_cancel_scope:
            self._cell_cancel_scope.cancel()
        else:
            raise Exception(""Kernel interrupted but no cell is running"")

    def run(self):
        old_sig = signal.signal(signal.SIGINT, self.interrupt)

        def log_nursery_exc(exc):
            exc = ""\n"".join(traceback.format_exception(type(exc), exc, exc.__traceback__))
            logging.error(""An exception occurred in a global nursery task.\n%s"", exc)

        async def trio_main():
            self._trio_token = trio.lowlevel.current_trio_token()
            async with trio.open_nursery() as nursery:
                # TODO This hack prevents the nursery from cancelling all child
                # tasks when an uncaught exception occurs, but it's ugly.
                nursery._add_exc = log_nursery_exc
                builtins.GLOBAL_NURSERY = nursery  # type:ignore[attr-defined]
                await trio.sleep_forever()

        trio.run(trio_main)
        signal.signal(signal.SIGINT, old_sig)

    def __call__(self, async_fn):
        async def loc(coro):
            self._cell_cancel_scope = trio.CancelScope()
            with self._cell_cancel_scope:
                return await coro
            self._cell_cancel_scope = None

        return trio.from_thread.run(loc, async_fn, trio_token=self._trio_token)"
315	jackson	0	"# This file is dual licensed under the terms of the Apache License, Version
# 2.0, and the BSD License. See the LICENSE file in the root of this repository
# for complete details.

import typing

from cryptography import utils
from cryptography.exceptions import (
    AlreadyFinalized,
    UnsupportedAlgorithm,
    _Reasons,
)
from cryptography.hazmat.backends.openssl.poly1305 import _Poly1305Context


class Poly1305:
    _ctx: typing.Optional[_Poly1305Context]

    def __init__(self, key: bytes):
        from cryptography.hazmat.backends.openssl.backend import backend

        if not backend.poly1305_supported():
            raise UnsupportedAlgorithm(
                ""poly1305 is not supported by this version of OpenSSL."",
                _Reasons.UNSUPPORTED_MAC,
            )
        self._ctx = backend.create_poly1305_ctx(key)

    def update(self, data: bytes) -> None:
        if self._ctx is None:
            raise AlreadyFinalized(""Context was already finalized."")
        utils._check_byteslike(""data"", data)
        self._ctx.update(data)

    def finalize(self) -> bytes:
        if self._ctx is None:
            raise AlreadyFinalized(""Context was already finalized."")
        mac = self._ctx.finalize()
        self._ctx = None
        return mac

    def verify(self, tag: bytes) -> None:
        utils._check_bytes(""tag"", tag)
        if self._ctx is None:
            raise AlreadyFinalized(""Context was already finalized."")

        ctx, self._ctx = self._ctx, None
        ctx.verify(tag)

    @classmethod
    def generate_tag(cls, key: bytes, data: bytes) -> bytes:
        p = Poly1305(key)
        p.update(data)
        return p.finalize()

    @classmethod
    def verify_tag(cls, key: bytes, data: bytes, tag: bytes) -> None:
        p = Poly1305(key)
        p.update(data)
        p.verify(tag)"
315	donghui	0	"# This file is dual licensed under the terms of the Apache License, Version
# 2.0, and the BSD License. See the LICENSE file in the root of this repository
# for complete details.

import typing

from cryptography import utils
from cryptography.exceptions import (
    AlreadyFinalized,
    UnsupportedAlgorithm,
    _Reasons,
)
from cryptography.hazmat.backends.openssl.poly1305 import _Poly1305Context


class Poly1305:
    _ctx: typing.Optional[_Poly1305Context]

    def __init__(self, key: bytes):
        from cryptography.hazmat.backends.openssl.backend import backend

        if not backend.poly1305_supported():
            raise UnsupportedAlgorithm(
                ""poly1305 is not supported by this version of OpenSSL."",
                _Reasons.UNSUPPORTED_MAC,
            )
        self._ctx = backend.create_poly1305_ctx(key)

    def update(self, data: bytes) -> None:
        if self._ctx is None:
            raise AlreadyFinalized(""Context was already finalized."")
        utils._check_byteslike(""data"", data)
        self._ctx.update(data)

    def finalize(self) -> bytes:
        if self._ctx is None:
            raise AlreadyFinalized(""Context was already finalized."")
        mac = self._ctx.finalize()
        self._ctx = None
        return mac

    def verify(self, tag: bytes) -> None:
        utils._check_bytes(""tag"", tag)
        if self._ctx is None:
            raise AlreadyFinalized(""Context was already finalized."")

        ctx, self._ctx = self._ctx, None
        ctx.verify(tag)

    @classmethod
    def generate_tag(cls, key: bytes, data: bytes) -> bytes:
        p = Poly1305(key)
        p.update(data)
        return p.finalize()

    @classmethod
    def verify_tag(cls, key: bytes, data: bytes, tag: bytes) -> None:
        p = Poly1305(key)
        p.update(data)
        p.verify(tag)"
255	jackson	2	"import _plotly_utils.basevalidators


class HoverlabelValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(self, plotly_name=""hoverlabel"", parent_name=""densitymapbox"", **kwargs):
        super(HoverlabelValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Hoverlabel""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            align
                Sets the horizontal alignment of the text
                content within hover label box. Has an effect
                only if the hover label text spans more two or
                more lines
            alignsrc
                Sets the source reference on Chart Studio Cloud
                for `align`.
            bgcolor
                Sets the background color of the hover labels
                for this trace
            bgcolorsrc
                Sets the source reference on Chart Studio Cloud
                for `bgcolor`.
            bordercolor
                Sets the border color of the hover labels for
                this trace.
            bordercolorsrc
                Sets the source reference on Chart Studio Cloud
                for `bordercolor`.
            font
                Sets the font used in hover labels.
            namelength
                Sets the default length (in number of
                characters) of the trace name in the hover
                labels for all traces. -1 shows the whole name
                regardless of length. 0-3 shows the first 0-3
                characters, and an integer >3 will show the
                whole name if it is less than that many
                characters, but if it is longer, will truncate
                to `namelength - 3` characters and add an
                ellipsis.
            namelengthsrc
                Sets the source reference on Chart Studio Cloud
                for `namelength`.
"""""",
            ),
            **kwargs,
        )"
255	donghui	1	"import _plotly_utils.basevalidators


class HoverlabelValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(self, plotly_name=""hoverlabel"", parent_name=""densitymapbox"", **kwargs):
        super(HoverlabelValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Hoverlabel""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            align
                Sets the horizontal alignment of the text
                content within hover label box. Has an effect
                only if the hover label text spans more two or
                more lines
            alignsrc
                Sets the source reference on Chart Studio Cloud
                for `align`.
            bgcolor
                Sets the background color of the hover labels
                for this trace
            bgcolorsrc
                Sets the source reference on Chart Studio Cloud
                for `bgcolor`.
            bordercolor
                Sets the border color of the hover labels for
                this trace.
            bordercolorsrc
                Sets the source reference on Chart Studio Cloud
                for `bordercolor`.
            font
                Sets the font used in hover labels.
            namelength
                Sets the default length (in number of
                characters) of the trace name in the hover
                labels for all traces. -1 shows the whole name
                regardless of length. 0-3 shows the first 0-3
                characters, and an integer >3 will show the
                whole name if it is less than that many
                characters, but if it is longer, will truncate
                to `namelength - 3` characters and add an
                ellipsis.
            namelengthsrc
                Sets the source reference on Chart Studio Cloud
                for `namelength`.
"""""",
            ),
            **kwargs,
        )"
344	jackson	1	"#
# The Python Imaging Library.
# $Id$
#
# DCX file handling
#
# DCX is a container file format defined by Intel, commonly used
# for fax applications.  Each DCX file consists of a directory
# (a list of file offsets) followed by a set of (usually 1-bit)
# PCX files.
#
# History:
# 1995-09-09 fl   Created
# 1996-03-20 fl   Properly derived from PcxImageFile.
# 1998-07-15 fl   Renamed offset attribute to avoid name clash
# 2002-07-30 fl   Fixed file handling
#
# Copyright (c) 1997-98 by Secret Labs AB.
# Copyright (c) 1995-96 by Fredrik Lundh.
#
# See the README file for information on usage and redistribution.
#

from . import Image
from ._binary import i32le as i32
from .PcxImagePlugin import PcxImageFile

MAGIC = 0x3ADE68B1  # QUIZ: what's this value, then?


def _accept(prefix):
    return len(prefix) >= 4 and i32(prefix) == MAGIC


##
# Image plugin for the Intel DCX format.


class DcxImageFile(PcxImageFile):

    format = ""DCX""
    format_description = ""Intel DCX""
    _close_exclusive_fp_after_loading = False

    def _open(self):

        # Header
        s = self.fp.read(4)
        if not _accept(s):
            msg = ""not a DCX file""
            raise SyntaxError(msg)

        # Component directory
        self._offset = []
        for i in range(1024):
            offset = i32(self.fp.read(4))
            if not offset:
                break
            self._offset.append(offset)

        self._fp = self.fp
        self.frame = None
        self.n_frames = len(self._offset)
        self.is_animated = self.n_frames > 1
        self.seek(0)

    def seek(self, frame):
        if not self._seek_check(frame):
            return
        self.frame = frame
        self.fp = self._fp
        self.fp.seek(self._offset[frame])
        PcxImageFile._open(self)

    def tell(self):
        return self.frame


Image.register_open(DcxImageFile.format, DcxImageFile, _accept)

Image.register_extension(DcxImageFile.format, "".dcx"")"
344	donghui	2	"#
# The Python Imaging Library.
# $Id$
#
# DCX file handling
#
# DCX is a container file format defined by Intel, commonly used
# for fax applications.  Each DCX file consists of a directory
# (a list of file offsets) followed by a set of (usually 1-bit)
# PCX files.
#
# History:
# 1995-09-09 fl   Created
# 1996-03-20 fl   Properly derived from PcxImageFile.
# 1998-07-15 fl   Renamed offset attribute to avoid name clash
# 2002-07-30 fl   Fixed file handling
#
# Copyright (c) 1997-98 by Secret Labs AB.
# Copyright (c) 1995-96 by Fredrik Lundh.
#
# See the README file for information on usage and redistribution.
#

from . import Image
from ._binary import i32le as i32
from .PcxImagePlugin import PcxImageFile

MAGIC = 0x3ADE68B1  # QUIZ: what's this value, then?


def _accept(prefix):
    return len(prefix) >= 4 and i32(prefix) == MAGIC


##
# Image plugin for the Intel DCX format.


class DcxImageFile(PcxImageFile):

    format = ""DCX""
    format_description = ""Intel DCX""
    _close_exclusive_fp_after_loading = False

    def _open(self):

        # Header
        s = self.fp.read(4)
        if not _accept(s):
            msg = ""not a DCX file""
            raise SyntaxError(msg)

        # Component directory
        self._offset = []
        for i in range(1024):
            offset = i32(self.fp.read(4))
            if not offset:
                break
            self._offset.append(offset)

        self._fp = self.fp
        self.frame = None
        self.n_frames = len(self._offset)
        self.is_animated = self.n_frames > 1
        self.seek(0)

    def seek(self, frame):
        if not self._seek_check(frame):
            return
        self.frame = frame
        self.fp = self._fp
        self.fp.seek(self._offset[frame])
        PcxImageFile._open(self)

    def tell(self):
        return self.frame


Image.register_open(DcxImageFile.format, DcxImageFile, _accept)

Image.register_extension(DcxImageFile.format, "".dcx"")"
292	jackson	0	"# This file is dual licensed under the terms of the Apache License, Version
# 2.0, and the BSD License. See the LICENSE file in the root of this repository
# for complete details.


import typing

from cryptography import utils
from cryptography.exceptions import AlreadyFinalized, InvalidKey
from cryptography.hazmat.primitives import constant_time, hashes
from cryptography.hazmat.primitives.kdf import KeyDerivationFunction


def _int_to_u32be(n: int) -> bytes:
    return n.to_bytes(length=4, byteorder=""big"")


class X963KDF(KeyDerivationFunction):
    def __init__(
        self,
        algorithm: hashes.HashAlgorithm,
        length: int,
        sharedinfo: typing.Optional[bytes],
        backend: typing.Any = None,
    ):
        max_len = algorithm.digest_size * (2**32 - 1)
        if length > max_len:
            raise ValueError(
                ""Cannot derive keys larger than {} bits."".format(max_len)
            )
        if sharedinfo is not None:
            utils._check_bytes(""sharedinfo"", sharedinfo)

        self._algorithm = algorithm
        self._length = length
        self._sharedinfo = sharedinfo
        self._used = False

    def derive(self, key_material: bytes) -> bytes:
        if self._used:
            raise AlreadyFinalized
        self._used = True
        utils._check_byteslike(""key_material"", key_material)
        output = [b""""]
        outlen = 0
        counter = 1

        while self._length > outlen:
            h = hashes.Hash(self._algorithm)
            h.update(key_material)
            h.update(_int_to_u32be(counter))
            if self._sharedinfo is not None:
                h.update(self._sharedinfo)
            output.append(h.finalize())
            outlen += len(output[-1])
            counter += 1

        return b"""".join(output)[: self._length]

    def verify(self, key_material: bytes, expected_key: bytes) -> None:
        if not constant_time.bytes_eq(self.derive(key_material), expected_key):
            raise InvalidKey"
292	donghui	0	"# This file is dual licensed under the terms of the Apache License, Version
# 2.0, and the BSD License. See the LICENSE file in the root of this repository
# for complete details.


import typing

from cryptography import utils
from cryptography.exceptions import AlreadyFinalized, InvalidKey
from cryptography.hazmat.primitives import constant_time, hashes
from cryptography.hazmat.primitives.kdf import KeyDerivationFunction


def _int_to_u32be(n: int) -> bytes:
    return n.to_bytes(length=4, byteorder=""big"")


class X963KDF(KeyDerivationFunction):
    def __init__(
        self,
        algorithm: hashes.HashAlgorithm,
        length: int,
        sharedinfo: typing.Optional[bytes],
        backend: typing.Any = None,
    ):
        max_len = algorithm.digest_size * (2**32 - 1)
        if length > max_len:
            raise ValueError(
                ""Cannot derive keys larger than {} bits."".format(max_len)
            )
        if sharedinfo is not None:
            utils._check_bytes(""sharedinfo"", sharedinfo)

        self._algorithm = algorithm
        self._length = length
        self._sharedinfo = sharedinfo
        self._used = False

    def derive(self, key_material: bytes) -> bytes:
        if self._used:
            raise AlreadyFinalized
        self._used = True
        utils._check_byteslike(""key_material"", key_material)
        output = [b""""]
        outlen = 0
        counter = 1

        while self._length > outlen:
            h = hashes.Hash(self._algorithm)
            h.update(key_material)
            h.update(_int_to_u32be(counter))
            if self._sharedinfo is not None:
                h.update(self._sharedinfo)
            output.append(h.finalize())
            outlen += len(output[-1])
            counter += 1

        return b"""".join(output)[: self._length]

    def verify(self, key_material: bytes, expected_key: bytes) -> None:
        if not constant_time.bytes_eq(self.derive(key_material), expected_key):
            raise InvalidKey"
383	jackson	1	"""""""distutils.command.install_scripts

Implements the Distutils 'install_scripts' command, for installing
Python scripts.""""""

# contributed by Bastian Kleineidam

import os
from distutils.core import Command
from distutils import log
from stat import ST_MODE


class install_scripts(Command):

    description = ""install scripts (Python or otherwise)""

    user_options = [
        ('install-dir=', 'd', ""directory to install scripts to""),
        ('build-dir=', 'b', ""build directory (where to install from)""),
        ('force', 'f', ""force installation (overwrite existing files)""),
        ('skip-build', None, ""skip the build steps""),
    ]

    boolean_options = ['force', 'skip-build']

    def initialize_options(self):
        self.install_dir = None
        self.force = 0
        self.build_dir = None
        self.skip_build = None

    def finalize_options(self):
        self.set_undefined_options('build', ('build_scripts', 'build_dir'))
        self.set_undefined_options(
            'install',
            ('install_scripts', 'install_dir'),
            ('force', 'force'),
            ('skip_build', 'skip_build'),
        )

    def run(self):
        if not self.skip_build:
            self.run_command('build_scripts')
        self.outfiles = self.copy_tree(self.build_dir, self.install_dir)
        if os.name == 'posix':
            # Set the executable bits (owner, group, and world) on
            # all the scripts we just installed.
            for file in self.get_outputs():
                if self.dry_run:
                    log.info(""changing mode of %s"", file)
                else:
                    mode = ((os.stat(file)[ST_MODE]) | 0o555) & 0o7777
                    log.info(""changing mode of %s to %o"", file, mode)
                    os.chmod(file, mode)

    def get_inputs(self):
        return self.distribution.scripts or []

    def get_outputs(self):
        return self.outfiles or []"
383	donghui	1	"""""""distutils.command.install_scripts

Implements the Distutils 'install_scripts' command, for installing
Python scripts.""""""

# contributed by Bastian Kleineidam

import os
from distutils.core import Command
from distutils import log
from stat import ST_MODE


class install_scripts(Command):

    description = ""install scripts (Python or otherwise)""

    user_options = [
        ('install-dir=', 'd', ""directory to install scripts to""),
        ('build-dir=', 'b', ""build directory (where to install from)""),
        ('force', 'f', ""force installation (overwrite existing files)""),
        ('skip-build', None, ""skip the build steps""),
    ]

    boolean_options = ['force', 'skip-build']

    def initialize_options(self):
        self.install_dir = None
        self.force = 0
        self.build_dir = None
        self.skip_build = None

    def finalize_options(self):
        self.set_undefined_options('build', ('build_scripts', 'build_dir'))
        self.set_undefined_options(
            'install',
            ('install_scripts', 'install_dir'),
            ('force', 'force'),
            ('skip_build', 'skip_build'),
        )

    def run(self):
        if not self.skip_build:
            self.run_command('build_scripts')
        self.outfiles = self.copy_tree(self.build_dir, self.install_dir)
        if os.name == 'posix':
            # Set the executable bits (owner, group, and world) on
            # all the scripts we just installed.
            for file in self.get_outputs():
                if self.dry_run:
                    log.info(""changing mode of %s"", file)
                else:
                    mode = ((os.stat(file)[ST_MODE]) | 0o555) & 0o7777
                    log.info(""changing mode of %s to %o"", file, mode)
                    os.chmod(file, mode)

    def get_inputs(self):
        return self.distribution.scripts or []

    def get_outputs(self):
        return self.outfiles or []"
370	jackson	2	"# Copyright 2022 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import os
import re
import uuid

from google.cloud import iam_v2
from google.cloud.iam_v2 import types
import pytest
from snippets.create_deny_policy import create_deny_policy
from snippets.delete_deny_policy import delete_deny_policy

PROJECT_ID = os.environ[""IAM_PROJECT_ID""]
GOOGLE_APPLICATION_CREDENTIALS = os.environ[""IAM_CREDENTIALS""]


@pytest.fixture
def deny_policy(capsys: ""pytest.CaptureFixture[str]"") -> None:
    policy_id = f""test-deny-policy-{uuid.uuid4()}""

    # Delete any existing policies. Otherwise it might throw quota issue.
    delete_existing_deny_policies(PROJECT_ID, ""test-deny-policy"")

    # Create the Deny policy.
    create_deny_policy(PROJECT_ID, policy_id)

    yield policy_id

    # Delete the Deny policy and assert if deleted.
    delete_deny_policy(PROJECT_ID, policy_id)
    out, _ = capsys.readouterr()
    assert re.search(f""Deleted the deny policy: {policy_id}"", out)


def delete_existing_deny_policies(project_id: str, delete_name_prefix: str) -> None:
    policies_client = iam_v2.PoliciesClient()

    attachment_point = f""cloudresourcemanager.googleapis.com%2Fprojects%2F{project_id}""

    request = types.ListPoliciesRequest()
    request.parent = f""policies/{attachment_point}/denypolicies""
    for policy in policies_client.list_policies(request=request):
        if delete_name_prefix in policy.name:
            delete_deny_policy(PROJECT_ID, str(policy.name).rsplit(""/"", 1)[-1])"
370	donghui	2	"# Copyright 2022 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import os
import re
import uuid

from google.cloud import iam_v2
from google.cloud.iam_v2 import types
import pytest
from snippets.create_deny_policy import create_deny_policy
from snippets.delete_deny_policy import delete_deny_policy

PROJECT_ID = os.environ[""IAM_PROJECT_ID""]
GOOGLE_APPLICATION_CREDENTIALS = os.environ[""IAM_CREDENTIALS""]


@pytest.fixture
def deny_policy(capsys: ""pytest.CaptureFixture[str]"") -> None:
    policy_id = f""test-deny-policy-{uuid.uuid4()}""

    # Delete any existing policies. Otherwise it might throw quota issue.
    delete_existing_deny_policies(PROJECT_ID, ""test-deny-policy"")

    # Create the Deny policy.
    create_deny_policy(PROJECT_ID, policy_id)

    yield policy_id

    # Delete the Deny policy and assert if deleted.
    delete_deny_policy(PROJECT_ID, policy_id)
    out, _ = capsys.readouterr()
    assert re.search(f""Deleted the deny policy: {policy_id}"", out)


def delete_existing_deny_policies(project_id: str, delete_name_prefix: str) -> None:
    policies_client = iam_v2.PoliciesClient()

    attachment_point = f""cloudresourcemanager.googleapis.com%2Fprojects%2F{project_id}""

    request = types.ListPoliciesRequest()
    request.parent = f""policies/{attachment_point}/denypolicies""
    for policy in policies_client.list_policies(request=request):
        if delete_name_prefix in policy.name:
            delete_deny_policy(PROJECT_ID, str(policy.name).rsplit(""/"", 1)[-1])"
261	jackson	0	"from django.contrib.auth.forms import UserCreationForm
from django import forms
from django.contrib.auth.models import User
from .models import Post

class RegisterForm(UserCreationForm):
    email = forms.EmailField(label = ""Email"")
    firstname = forms.CharField(label = ""First name"")
    lastname = forms.CharField(label = ""Last name"")

    class Meta:
        model = User
        fields = (""username"", ""firstname"", ""lastname"", ""email"", )

    def save(self, commit=True):
        user = super(RegisterForm, self).save(commit=False)
        firstname = self.cleaned_data[""firstname""]
        lastname = self.cleaned_data[""lastname""]
        user.first_name = firstname
        user.last_name = lastname
        user.email = self.cleaned_data[""email""]
        if commit:
            user.save()
        return user
    
class PostForm(forms.ModelForm):
    text = forms.CharField(max_length=1000, widget=forms.Textarea(attrs={'placeholder': 'What\'s on your mind?', 'onchange': 'character_count()', 'onkeypress': 'character_count()', 'onfocus': 'character_count()' ,'oninput': 'character_count()', 'onkeyup':'character_count()','onpaste':'character_count()'}))
    images = forms.ImageField(required=False,widget=forms.ClearableFileInput(attrs={'multiple': True, 'onchange': 'previewImages(this)'}))
    class Meta:
        model = Post
        fields = (""text"", )

class SearchForm(forms.Form):
    search = forms.CharField(max_length=100, widget=forms.TextInput(attrs={'placeholder': 'Type something or someone to search for ...'}))

class UpdateProfileForm(forms.Form):
    first_name = forms.CharField(max_length=100, required=False)
    last_name = forms.CharField(max_length=100, required=False)
    profile_image = forms.ImageField(required=False)
    remove_profile_image = forms.BooleanField(required=False)
    profile_cover_photo = forms.ImageField(required=False)
    remove_cover_photo = forms.BooleanField(required=False)
    profile_bio = forms.CharField(max_length=500, required=False, widget=forms.Textarea(attrs={'placeholder': 'Write something about yourself ...'}))"
261	donghui	0	"from django.contrib.auth.forms import UserCreationForm
from django import forms
from django.contrib.auth.models import User
from .models import Post

class RegisterForm(UserCreationForm):
    email = forms.EmailField(label = ""Email"")
    firstname = forms.CharField(label = ""First name"")
    lastname = forms.CharField(label = ""Last name"")

    class Meta:
        model = User
        fields = (""username"", ""firstname"", ""lastname"", ""email"", )

    def save(self, commit=True):
        user = super(RegisterForm, self).save(commit=False)
        firstname = self.cleaned_data[""firstname""]
        lastname = self.cleaned_data[""lastname""]
        user.first_name = firstname
        user.last_name = lastname
        user.email = self.cleaned_data[""email""]
        if commit:
            user.save()
        return user
    
class PostForm(forms.ModelForm):
    text = forms.CharField(max_length=1000, widget=forms.Textarea(attrs={'placeholder': 'What\'s on your mind?', 'onchange': 'character_count()', 'onkeypress': 'character_count()', 'onfocus': 'character_count()' ,'oninput': 'character_count()', 'onkeyup':'character_count()','onpaste':'character_count()'}))
    images = forms.ImageField(required=False,widget=forms.ClearableFileInput(attrs={'multiple': True, 'onchange': 'previewImages(this)'}))
    class Meta:
        model = Post
        fields = (""text"", )

class SearchForm(forms.Form):
    search = forms.CharField(max_length=100, widget=forms.TextInput(attrs={'placeholder': 'Type something or someone to search for ...'}))

class UpdateProfileForm(forms.Form):
    first_name = forms.CharField(max_length=100, required=False)
    last_name = forms.CharField(max_length=100, required=False)
    profile_image = forms.ImageField(required=False)
    remove_profile_image = forms.BooleanField(required=False)
    profile_cover_photo = forms.ImageField(required=False)
    remove_cover_photo = forms.BooleanField(required=False)
    profile_bio = forms.CharField(max_length=500, required=False, widget=forms.Textarea(attrs={'placeholder': 'Write something about yourself ...'}))"
321	jackson	3	"# Copyright (c) Facebook, Inc. and its affiliates.
#
# This source code is licensed under the MIT license found in the
# LICENSE file in the root directory of this source tree.

from fairseq.optim import LegacyFairseqOptimizer, register_optimizer


@register_optimizer(""lamb"")
class FairseqLAMB(LegacyFairseqOptimizer):
    """"""LAMB optimizer.""""""

    def __init__(self, args, params):
        super().__init__(args)
        try:
            from apex.optimizers import FusedLAMB

            self._optimizer = FusedLAMB(params, **self.optimizer_config)
        except ImportError:
            raise ImportError(""Please install apex to use LAMB optimizer"")

    @staticmethod
    def add_args(parser):
        """"""Add optimizer-specific arguments to the parser.""""""
        # fmt: off
        parser.add_argument('--lamb-betas', default='(0.9, 0.999)', metavar='B',
                            help='betas for LAMB optimizer')
        parser.add_argument('--lamb-eps', type=float, default=1e-8, metavar='D',
                            help='epsilon for LAMB optimizer')
        parser.add_argument('--weight-decay', '--wd', default=0.0, type=float, metavar='WD',
                            help='weight decay')
        # fmt: on

    @property
    def optimizer_config(self):
        """"""
        Return a kwarg dictionary that will be used to override optimizer
        args stored in checkpoints. This allows us to load a checkpoint and
        resume training using a different set of optimizer args, e.g., with a
        different learning rate.
        """"""
        return {
            ""lr"": self.args.lr[0],
            ""betas"": eval(self.args.lamb_betas),
            ""eps"": self.args.lamb_eps,
            ""weight_decay"": self.args.weight_decay,
        }

    @property
    def supports_flat_params(self):
        return False"
321	donghui	2	"# Copyright (c) Facebook, Inc. and its affiliates.
#
# This source code is licensed under the MIT license found in the
# LICENSE file in the root directory of this source tree.

from fairseq.optim import LegacyFairseqOptimizer, register_optimizer


@register_optimizer(""lamb"")
class FairseqLAMB(LegacyFairseqOptimizer):
    """"""LAMB optimizer.""""""

    def __init__(self, args, params):
        super().__init__(args)
        try:
            from apex.optimizers import FusedLAMB

            self._optimizer = FusedLAMB(params, **self.optimizer_config)
        except ImportError:
            raise ImportError(""Please install apex to use LAMB optimizer"")

    @staticmethod
    def add_args(parser):
        """"""Add optimizer-specific arguments to the parser.""""""
        # fmt: off
        parser.add_argument('--lamb-betas', default='(0.9, 0.999)', metavar='B',
                            help='betas for LAMB optimizer')
        parser.add_argument('--lamb-eps', type=float, default=1e-8, metavar='D',
                            help='epsilon for LAMB optimizer')
        parser.add_argument('--weight-decay', '--wd', default=0.0, type=float, metavar='WD',
                            help='weight decay')
        # fmt: on

    @property
    def optimizer_config(self):
        """"""
        Return a kwarg dictionary that will be used to override optimizer
        args stored in checkpoints. This allows us to load a checkpoint and
        resume training using a different set of optimizer args, e.g., with a
        different learning rate.
        """"""
        return {
            ""lr"": self.args.lr[0],
            ""betas"": eval(self.args.lamb_betas),
            ""eps"": self.args.lamb_eps,
            ""weight_decay"": self.args.weight_decay,
        }

    @property
    def supports_flat_params(self):
        return False"
330	jackson	1	"""""""Tests for the NumpyVersion class.

""""""
from numpy.testing import assert_, assert_raises
from numpy.lib import NumpyVersion


def test_main_versions():
    assert_(NumpyVersion('1.8.0') == '1.8.0')
    for ver in ['1.9.0', '2.0.0', '1.8.1', '10.0.1']:
        assert_(NumpyVersion('1.8.0') < ver)

    for ver in ['1.7.0', '1.7.1', '0.9.9']:
        assert_(NumpyVersion('1.8.0') > ver)


def test_version_1_point_10():
    # regression test for gh-2998.
    assert_(NumpyVersion('1.9.0') < '1.10.0')
    assert_(NumpyVersion('1.11.0') < '1.11.1')
    assert_(NumpyVersion('1.11.0') == '1.11.0')
    assert_(NumpyVersion('1.99.11') < '1.99.12')


def test_alpha_beta_rc():
    assert_(NumpyVersion('1.8.0rc1') == '1.8.0rc1')
    for ver in ['1.8.0', '1.8.0rc2']:
        assert_(NumpyVersion('1.8.0rc1') < ver)

    for ver in ['1.8.0a2', '1.8.0b3', '1.7.2rc4']:
        assert_(NumpyVersion('1.8.0rc1') > ver)

    assert_(NumpyVersion('1.8.0b1') > '1.8.0a2')


def test_dev_version():
    assert_(NumpyVersion('1.9.0.dev-Unknown') < '1.9.0')
    for ver in ['1.9.0', '1.9.0a1', '1.9.0b2', '1.9.0b2.dev-ffffffff']:
        assert_(NumpyVersion('1.9.0.dev-f16acvda') < ver)

    assert_(NumpyVersion('1.9.0.dev-f16acvda') == '1.9.0.dev-11111111')


def test_dev_a_b_rc_mixed():
    assert_(NumpyVersion('1.9.0a2.dev-f16acvda') == '1.9.0a2.dev-11111111')
    assert_(NumpyVersion('1.9.0a2.dev-6acvda54') < '1.9.0a2')


def test_dev0_version():
    assert_(NumpyVersion('1.9.0.dev0+Unknown') < '1.9.0')
    for ver in ['1.9.0', '1.9.0a1', '1.9.0b2', '1.9.0b2.dev0+ffffffff']:
        assert_(NumpyVersion('1.9.0.dev0+f16acvda') < ver)

    assert_(NumpyVersion('1.9.0.dev0+f16acvda') == '1.9.0.dev0+11111111')


def test_dev0_a_b_rc_mixed():
    assert_(NumpyVersion('1.9.0a2.dev0+f16acvda') == '1.9.0a2.dev0+11111111')
    assert_(NumpyVersion('1.9.0a2.dev0+6acvda54') < '1.9.0a2')


def test_raises():
    for ver in ['1.9', '1,9.0', '1.7.x']:
        assert_raises(ValueError, NumpyVersion, ver)"
330	donghui	0	"""""""Tests for the NumpyVersion class.

""""""
from numpy.testing import assert_, assert_raises
from numpy.lib import NumpyVersion


def test_main_versions():
    assert_(NumpyVersion('1.8.0') == '1.8.0')
    for ver in ['1.9.0', '2.0.0', '1.8.1', '10.0.1']:
        assert_(NumpyVersion('1.8.0') < ver)

    for ver in ['1.7.0', '1.7.1', '0.9.9']:
        assert_(NumpyVersion('1.8.0') > ver)


def test_version_1_point_10():
    # regression test for gh-2998.
    assert_(NumpyVersion('1.9.0') < '1.10.0')
    assert_(NumpyVersion('1.11.0') < '1.11.1')
    assert_(NumpyVersion('1.11.0') == '1.11.0')
    assert_(NumpyVersion('1.99.11') < '1.99.12')


def test_alpha_beta_rc():
    assert_(NumpyVersion('1.8.0rc1') == '1.8.0rc1')
    for ver in ['1.8.0', '1.8.0rc2']:
        assert_(NumpyVersion('1.8.0rc1') < ver)

    for ver in ['1.8.0a2', '1.8.0b3', '1.7.2rc4']:
        assert_(NumpyVersion('1.8.0rc1') > ver)

    assert_(NumpyVersion('1.8.0b1') > '1.8.0a2')


def test_dev_version():
    assert_(NumpyVersion('1.9.0.dev-Unknown') < '1.9.0')
    for ver in ['1.9.0', '1.9.0a1', '1.9.0b2', '1.9.0b2.dev-ffffffff']:
        assert_(NumpyVersion('1.9.0.dev-f16acvda') < ver)

    assert_(NumpyVersion('1.9.0.dev-f16acvda') == '1.9.0.dev-11111111')


def test_dev_a_b_rc_mixed():
    assert_(NumpyVersion('1.9.0a2.dev-f16acvda') == '1.9.0a2.dev-11111111')
    assert_(NumpyVersion('1.9.0a2.dev-6acvda54') < '1.9.0a2')


def test_dev0_version():
    assert_(NumpyVersion('1.9.0.dev0+Unknown') < '1.9.0')
    for ver in ['1.9.0', '1.9.0a1', '1.9.0b2', '1.9.0b2.dev0+ffffffff']:
        assert_(NumpyVersion('1.9.0.dev0+f16acvda') < ver)

    assert_(NumpyVersion('1.9.0.dev0+f16acvda') == '1.9.0.dev0+11111111')


def test_dev0_a_b_rc_mixed():
    assert_(NumpyVersion('1.9.0a2.dev0+f16acvda') == '1.9.0a2.dev0+11111111')
    assert_(NumpyVersion('1.9.0a2.dev0+6acvda54') < '1.9.0a2')


def test_raises():
    for ver in ['1.9', '1,9.0', '1.7.x']:
        assert_raises(ValueError, NumpyVersion, ver)"
270	jackson	2	"import _plotly_utils.basevalidators


class FontValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(
        self, plotly_name=""font"", parent_name=""funnelarea.hoverlabel"", **kwargs
    ):
        super(FontValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Font""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            color

            colorsrc
                Sets the source reference on Chart Studio Cloud
                for `color`.
            family
                HTML font family - the typeface that will be
                applied by the web browser. The web browser
                will only be able to apply a font if it is
                available on the system which it operates.
                Provide multiple font families, separated by
                commas, to indicate the preference in which to
                apply fonts if they aren't available on the
                system. The Chart Studio Cloud (at
                https://chart-studio.plotly.com or on-premise)
                generates images on a server, where only a
                select number of fonts are installed and
                supported. These include ""Arial"", ""Balto"",
                ""Courier New"", ""Droid Sans"",, ""Droid Serif"",
                ""Droid Sans Mono"", ""Gravitas One"", ""Old
                Standard TT"", ""Open Sans"", ""Overpass"", ""PT Sans
                Narrow"", ""Raleway"", ""Times New Roman"".
            familysrc
                Sets the source reference on Chart Studio Cloud
                for `family`.
            size

            sizesrc
                Sets the source reference on Chart Studio Cloud
                for `size`.
"""""",
            ),
            **kwargs,
        )"
270	donghui	1	"import _plotly_utils.basevalidators


class FontValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(
        self, plotly_name=""font"", parent_name=""funnelarea.hoverlabel"", **kwargs
    ):
        super(FontValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Font""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            color

            colorsrc
                Sets the source reference on Chart Studio Cloud
                for `color`.
            family
                HTML font family - the typeface that will be
                applied by the web browser. The web browser
                will only be able to apply a font if it is
                available on the system which it operates.
                Provide multiple font families, separated by
                commas, to indicate the preference in which to
                apply fonts if they aren't available on the
                system. The Chart Studio Cloud (at
                https://chart-studio.plotly.com or on-premise)
                generates images on a server, where only a
                select number of fonts are installed and
                supported. These include ""Arial"", ""Balto"",
                ""Courier New"", ""Droid Sans"",, ""Droid Serif"",
                ""Droid Sans Mono"", ""Gravitas One"", ""Old
                Standard TT"", ""Open Sans"", ""Overpass"", ""PT Sans
                Narrow"", ""Raleway"", ""Times New Roman"".
            familysrc
                Sets the source reference on Chart Studio Cloud
                for `family`.
            size

            sizesrc
                Sets the source reference on Chart Studio Cloud
                for `size`.
"""""",
            ),
            **kwargs,
        )"
361	jackson	3	"#!/usr/bin/env python

# Copyright 2021 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# [START privateca_disable_ca]
import google.cloud.security.privateca_v1 as privateca_v1


def disable_certificate_authority(
    project_id: str, location: str, ca_pool_name: str, ca_name: str
) -> None:
    """"""
    Disable a Certificate Authority which is present in the given CA pool.

    Args:
        project_id: project ID or project number of the Cloud project you want to use.
        location: location you want to use. For a list of locations, see: https://cloud.google.com/certificate-authority-service/docs/locations.
        ca_pool_name: the name of the CA pool under which the CA is present.
        ca_name: the name of the CA to be disabled.
    """"""

    caServiceClient = privateca_v1.CertificateAuthorityServiceClient()
    ca_path = caServiceClient.certificate_authority_path(
        project_id, location, ca_pool_name, ca_name
    )

    # Create the Disable Certificate Authority Request.
    request = privateca_v1.DisableCertificateAuthorityRequest(name=ca_path)

    # Disable the Certificate Authority.
    operation = caServiceClient.disable_certificate_authority(request=request)
    result = operation.result()

    print(""Operation result:"", result)

    # Get the current CA state.
    ca_state = caServiceClient.get_certificate_authority(name=ca_path).state

    # Check if the CA is disabled.
    if ca_state == privateca_v1.CertificateAuthority.State.DISABLED:
        print(""Disabled Certificate Authority:"", ca_name)
    else:
        print(""Cannot disable the Certificate Authority ! Current CA State:"", ca_state)


# [END privateca_disable_ca]"
361	donghui	3	"#!/usr/bin/env python

# Copyright 2021 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# [START privateca_disable_ca]
import google.cloud.security.privateca_v1 as privateca_v1


def disable_certificate_authority(
    project_id: str, location: str, ca_pool_name: str, ca_name: str
) -> None:
    """"""
    Disable a Certificate Authority which is present in the given CA pool.

    Args:
        project_id: project ID or project number of the Cloud project you want to use.
        location: location you want to use. For a list of locations, see: https://cloud.google.com/certificate-authority-service/docs/locations.
        ca_pool_name: the name of the CA pool under which the CA is present.
        ca_name: the name of the CA to be disabled.
    """"""

    caServiceClient = privateca_v1.CertificateAuthorityServiceClient()
    ca_path = caServiceClient.certificate_authority_path(
        project_id, location, ca_pool_name, ca_name
    )

    # Create the Disable Certificate Authority Request.
    request = privateca_v1.DisableCertificateAuthorityRequest(name=ca_path)

    # Disable the Certificate Authority.
    operation = caServiceClient.disable_certificate_authority(request=request)
    result = operation.result()

    print(""Operation result:"", result)

    # Get the current CA state.
    ca_state = caServiceClient.get_certificate_authority(name=ca_path).state

    # Check if the CA is disabled.
    if ca_state == privateca_v1.CertificateAuthority.State.DISABLED:
        print(""Disabled Certificate Authority:"", ca_name)
    else:
        print(""Cannot disable the Certificate Authority ! Current CA State:"", ca_state)


# [END privateca_disable_ca]"
392	jackson	1	"from django.core import checks

NOT_PROVIDED = object()


class FieldCacheMixin:
    """"""Provide an API for working with the model's fields value cache.""""""

    def get_cache_name(self):
        raise NotImplementedError

    def get_cached_value(self, instance, default=NOT_PROVIDED):
        cache_name = self.get_cache_name()
        try:
            return instance._state.fields_cache[cache_name]
        except KeyError:
            if default is NOT_PROVIDED:
                raise
            return default

    def is_cached(self, instance):
        return self.get_cache_name() in instance._state.fields_cache

    def set_cached_value(self, instance, value):
        instance._state.fields_cache[self.get_cache_name()] = value

    def delete_cached_value(self, instance):
        del instance._state.fields_cache[self.get_cache_name()]


class CheckFieldDefaultMixin:
    _default_hint = (""<valid default>"", ""<invalid default>"")

    def _check_default(self):
        if (
            self.has_default()
            and self.default is not None
            and not callable(self.default)
        ):
            return [
                checks.Warning(
                    ""%s default should be a callable instead of an instance ""
                    ""so that it's not shared between all field instances.""
                    % (self.__class__.__name__,),
                    hint=(
                        ""Use a callable instead, e.g., use `%s` instead of ""
                        ""`%s`."" % self._default_hint
                    ),
                    obj=self,
                    id=""fields.E010"",
                )
            ]
        else:
            return []

    def check(self, **kwargs):
        errors = super().check(**kwargs)
        errors.extend(self._check_default())
        return errors"
392	donghui	1	"from django.core import checks

NOT_PROVIDED = object()


class FieldCacheMixin:
    """"""Provide an API for working with the model's fields value cache.""""""

    def get_cache_name(self):
        raise NotImplementedError

    def get_cached_value(self, instance, default=NOT_PROVIDED):
        cache_name = self.get_cache_name()
        try:
            return instance._state.fields_cache[cache_name]
        except KeyError:
            if default is NOT_PROVIDED:
                raise
            return default

    def is_cached(self, instance):
        return self.get_cache_name() in instance._state.fields_cache

    def set_cached_value(self, instance, value):
        instance._state.fields_cache[self.get_cache_name()] = value

    def delete_cached_value(self, instance):
        del instance._state.fields_cache[self.get_cache_name()]


class CheckFieldDefaultMixin:
    _default_hint = (""<valid default>"", ""<invalid default>"")

    def _check_default(self):
        if (
            self.has_default()
            and self.default is not None
            and not callable(self.default)
        ):
            return [
                checks.Warning(
                    ""%s default should be a callable instead of an instance ""
                    ""so that it's not shared between all field instances.""
                    % (self.__class__.__name__,),
                    hint=(
                        ""Use a callable instead, e.g., use `%s` instead of ""
                        ""`%s`."" % self._default_hint
                    ),
                    obj=self,
                    id=""fields.E010"",
                )
            ]
        else:
            return []

    def check(self, **kwargs):
        errors = super().check(**kwargs)
        errors.extend(self._check_default())
        return errors"
283	jackson	2	"alphabet = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']

def caesar(start_text, shift_amount, cipher_direction):
  end_text = """"
  if cipher_direction == ""decode"":
    shift_amount *= -1
  for char in start_text:
    #TODO-3: What happens if the user enters a number/symbol/space?
    #Can you fix the code to keep the number/symbol/space when the text is encoded/decoded?
    #e.g. start_text = ""meet me at 3""
    #end_text = ""•••• •• •• 3""
    if char in alphabet:
      position = alphabet.index(char)
      new_position = position + shift_amount
      end_text += alphabet[new_position]
    else:
      end_text += char
  print(f""Here's the {cipher_direction}d result: {end_text}"")

#TODO-1: Import and print the logo from art.py when the program starts.
from art import logo
print(logo)

#TODO-4: Can you figure out a way to ask the user if they want to restart the cipher program?
#e.g. Type 'yes' if you want to go again. Otherwise type 'no'.
#If they type 'yes' then ask them for the direction/text/shift again and call the caesar() function again?
#Hint: Try creating a while loop that continues to execute the program if the user types 'yes'.
should_end = False
while not should_end:

  direction = input(""Type 'encode' to encrypt, type 'decode' to decrypt:\n"")
  text = input(""Type your message:\n"").lower()
  shift = int(input(""Type the shift number:\n""))
  #TODO-2: What if the user enters a shift that is greater than the number of letters in the alphabet?
  #Try running the program and entering a shift number of 45.
  #Add some code so that the program continues to work even if the user enters a shift number greater than 26. 
  #Hint: Think about how you can use the modulus (%).
  shift = shift % 26

  caesar(start_text=text, shift_amount=shift, cipher_direction=direction)

  restart = input(""Type 'yes' if you want to go again. Otherwise type 'no'.\n"")
  if restart == ""no"":
    should_end = True
    print(""Goodbye"")
    "
283	donghui	3	"alphabet = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']

def caesar(start_text, shift_amount, cipher_direction):
  end_text = """"
  if cipher_direction == ""decode"":
    shift_amount *= -1
  for char in start_text:
    #TODO-3: What happens if the user enters a number/symbol/space?
    #Can you fix the code to keep the number/symbol/space when the text is encoded/decoded?
    #e.g. start_text = ""meet me at 3""
    #end_text = ""•••• •• •• 3""
    if char in alphabet:
      position = alphabet.index(char)
      new_position = position + shift_amount
      end_text += alphabet[new_position]
    else:
      end_text += char
  print(f""Here's the {cipher_direction}d result: {end_text}"")

#TODO-1: Import and print the logo from art.py when the program starts.
from art import logo
print(logo)

#TODO-4: Can you figure out a way to ask the user if they want to restart the cipher program?
#e.g. Type 'yes' if you want to go again. Otherwise type 'no'.
#If they type 'yes' then ask them for the direction/text/shift again and call the caesar() function again?
#Hint: Try creating a while loop that continues to execute the program if the user types 'yes'.
should_end = False
while not should_end:

  direction = input(""Type 'encode' to encrypt, type 'decode' to decrypt:\n"")
  text = input(""Type your message:\n"").lower()
  shift = int(input(""Type the shift number:\n""))
  #TODO-2: What if the user enters a shift that is greater than the number of letters in the alphabet?
  #Try running the program and entering a shift number of 45.
  #Add some code so that the program continues to work even if the user enters a shift number greater than 26. 
  #Hint: Think about how you can use the modulus (%).
  shift = shift % 26

  caesar(start_text=text, shift_amount=shift, cipher_direction=direction)

  restart = input(""Type 'yes' if you want to go again. Otherwise type 'no'.\n"")
  if restart == ""no"":
    should_end = True
    print(""Goodbye"")
    "
355	jackson	2	"import lightly.data as data

# the collate function applies random transforms to the input images
collate_fn = data.ImageCollateFunction(input_size=32, cj_prob=0.5)
import torch

# create a dataset from your image folder
dataset = data.LightlyDataset(input_dir='./my/cute/cats/dataset/')

# build a PyTorch dataloader
dataloader = torch.utils.data.DataLoader(
    dataset,                # pass the dataset to the dataloader
    batch_size=128,         # a large batch size helps with the learning
    shuffle=True,           # shuffling is important!
    collate_fn=collate_fn)  # apply transformations to the input images
import torchvision

from lightly.loss import NTXentLoss
from lightly.models.modules.heads import SimCLRProjectionHead

# use a resnet backbone
resnet = torchvision.models.resnext101_64x4d()  # or efficientnet0_b0
resnet = torch.nn.Sequential(*list(resnet.children())[:-1])

# build a SimCLR model
class SimCLR(torch.nn.Module):
    def __init__(self, backbone, hidden_dim, out_dim):
        super().__init__()
        self.backbone = backbone
        self.projection_head = SimCLRProjectionHead(hidden_dim, hidden_dim, out_dim)

    def forward(self, x):
        h = self.backbone(x).flatten(start_dim=1)
        z = self.projection_head(h)
        return z

model = SimCLR(resnet, hidden_dim=512, out_dim=128)

# use a criterion for self-supervised learning
# (normalized temperature-scaled cross entropy loss)
criterion = NTXentLoss(temperature=0.5)

# get a PyTorch optimizer
optimizer = torch.optim.SGD(model.parameters(), lr=1e-0, weight_decay=1e-5)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
max_epochs = 10
for epoch in range(max_epochs):
    for (x0, x1), _, _ in dataloader:

        x0 = x0.to(device)
        x1 = x1.to(device)

        z0 = model(x0)
        z1 = model(x1)

        loss = criterion(z0, z1)
        loss.backward()

        optimizer.step()
        optimizer.zero_grad()"
355	donghui	2	"import lightly.data as data

# the collate function applies random transforms to the input images
collate_fn = data.ImageCollateFunction(input_size=32, cj_prob=0.5)
import torch

# create a dataset from your image folder
dataset = data.LightlyDataset(input_dir='./my/cute/cats/dataset/')

# build a PyTorch dataloader
dataloader = torch.utils.data.DataLoader(
    dataset,                # pass the dataset to the dataloader
    batch_size=128,         # a large batch size helps with the learning
    shuffle=True,           # shuffling is important!
    collate_fn=collate_fn)  # apply transformations to the input images
import torchvision

from lightly.loss import NTXentLoss
from lightly.models.modules.heads import SimCLRProjectionHead

# use a resnet backbone
resnet = torchvision.models.resnext101_64x4d()  # or efficientnet0_b0
resnet = torch.nn.Sequential(*list(resnet.children())[:-1])

# build a SimCLR model
class SimCLR(torch.nn.Module):
    def __init__(self, backbone, hidden_dim, out_dim):
        super().__init__()
        self.backbone = backbone
        self.projection_head = SimCLRProjectionHead(hidden_dim, hidden_dim, out_dim)

    def forward(self, x):
        h = self.backbone(x).flatten(start_dim=1)
        z = self.projection_head(h)
        return z

model = SimCLR(resnet, hidden_dim=512, out_dim=128)

# use a criterion for self-supervised learning
# (normalized temperature-scaled cross entropy loss)
criterion = NTXentLoss(temperature=0.5)

# get a PyTorch optimizer
optimizer = torch.optim.SGD(model.parameters(), lr=1e-0, weight_decay=1e-5)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
max_epochs = 10
for epoch in range(max_epochs):
    for (x0, x1), _, _ in dataloader:

        x0 = x0.to(device)
        x1 = x1.to(device)

        z0 = model(x0)
        z1 = model(x1)

        loss = criterion(z0, z1)
        loss.backward()

        optimizer.step()
        optimizer.zero_grad()"
304	jackson	3	"# Copyright (c) Facebook, Inc. and its affiliates.
#
# This source code is licensed under the MIT license found in the
# LICENSE file in the root directory of this source tree.
""""""isort:skip_file""""""

import functools
import importlib


dependencies = [
    ""dataclasses"",
    ""hydra"",
    ""numpy"",
    ""omegaconf"",
    ""regex"",
    ""requests"",
    ""torch"",
]


# Check for required dependencies and raise a RuntimeError if any are missing.
missing_deps = []
for dep in dependencies:
    try:
        importlib.import_module(dep)
    except ImportError:
        # Hack: the hydra package is provided under the ""hydra-core"" name in
        # pypi. We don't want the user mistakenly calling `pip install hydra`
        # since that will install an unrelated package.
        if dep == ""hydra"":
            dep = ""hydra-core""
        missing_deps.append(dep)
if len(missing_deps) > 0:
    raise RuntimeError(""Missing dependencies: {}"".format("", "".join(missing_deps)))


# only do fairseq imports after checking for dependencies
from fairseq.hub_utils import (  # noqa; noqa
    BPEHubInterface as bpe,
    TokenizerHubInterface as tokenizer,
)
from fairseq.models import MODEL_REGISTRY  # noqa


# torch.hub doesn't build Cython components, so if they are not found then try
# to build them here
try:
    import fairseq.data.token_block_utils_fast  # noqa
except ImportError:
    try:
        import cython  # noqa
        import os
        from setuptools import sandbox

        sandbox.run_setup(
            os.path.join(os.path.dirname(__file__), ""setup.py""),
            [""build_ext"", ""--inplace""],
        )
    except ImportError:
        print(
            ""Unable to build Cython components. Please make sure Cython is ""
            ""installed if the torch.hub model you are loading depends on it.""
        )


# automatically expose models defined in FairseqModel::hub_models
for _model_type, _cls in MODEL_REGISTRY.items():
    for model_name in _cls.hub_models().keys():
        globals()[model_name] = functools.partial(
            _cls.from_pretrained,
            model_name,
        )"
304	donghui	3	"# Copyright (c) Facebook, Inc. and its affiliates.
#
# This source code is licensed under the MIT license found in the
# LICENSE file in the root directory of this source tree.
""""""isort:skip_file""""""

import functools
import importlib


dependencies = [
    ""dataclasses"",
    ""hydra"",
    ""numpy"",
    ""omegaconf"",
    ""regex"",
    ""requests"",
    ""torch"",
]


# Check for required dependencies and raise a RuntimeError if any are missing.
missing_deps = []
for dep in dependencies:
    try:
        importlib.import_module(dep)
    except ImportError:
        # Hack: the hydra package is provided under the ""hydra-core"" name in
        # pypi. We don't want the user mistakenly calling `pip install hydra`
        # since that will install an unrelated package.
        if dep == ""hydra"":
            dep = ""hydra-core""
        missing_deps.append(dep)
if len(missing_deps) > 0:
    raise RuntimeError(""Missing dependencies: {}"".format("", "".join(missing_deps)))


# only do fairseq imports after checking for dependencies
from fairseq.hub_utils import (  # noqa; noqa
    BPEHubInterface as bpe,
    TokenizerHubInterface as tokenizer,
)
from fairseq.models import MODEL_REGISTRY  # noqa


# torch.hub doesn't build Cython components, so if they are not found then try
# to build them here
try:
    import fairseq.data.token_block_utils_fast  # noqa
except ImportError:
    try:
        import cython  # noqa
        import os
        from setuptools import sandbox

        sandbox.run_setup(
            os.path.join(os.path.dirname(__file__), ""setup.py""),
            [""build_ext"", ""--inplace""],
        )
    except ImportError:
        print(
            ""Unable to build Cython components. Please make sure Cython is ""
            ""installed if the torch.hub model you are loading depends on it.""
        )


# automatically expose models defined in FairseqModel::hub_models
for _model_type, _cls in MODEL_REGISTRY.items():
    for model_name in _cls.hub_models().keys():
        globals()[model_name] = functools.partial(
            _cls.from_pretrained,
            model_name,
        )"
314	jackson	3	"from abc import ABCMeta, abstractmethod


class CacheAdapter:
    """"""
    CacheAdapter Abstract Base Class
    """"""
    __metaclass__ = ABCMeta

    @abstractmethod
    def get(self, public_id, type, resource_type, transformation, format):
        """"""
        Gets value specified by parameters

        :param public_id:       The public ID of the resource
        :param type:            The storage type
        :param resource_type:   The type of the resource
        :param transformation:  The transformation string
        :param format:          The format of the resource

        :return: None|mixed value, None if not found
        """"""
        raise NotImplementedError

    @abstractmethod
    def set(self, public_id, type, resource_type, transformation, format, value):
        """"""
        Sets value specified by parameters

        :param public_id:       The public ID of the resource
        :param type:            The storage type
        :param resource_type:   The type of the resource
        :param transformation:  The transformation string
        :param format:          The format of the resource
        :param value:           The value to set

        :return: bool True on success or False on failure
        """"""
        raise NotImplementedError

    @abstractmethod
    def delete(self, public_id, type, resource_type, transformation, format):
        """"""
        Deletes entry specified by parameters

        :param public_id:       The public ID of the resource
        :param type:            The storage type
        :param resource_type:   The type of the resource
        :param transformation:  The transformation string
        :param format:          The format of the resource

        :return: bool True on success or False on failure
        """"""
        raise NotImplementedError

    @abstractmethod
    def flush_all(self):
        """"""
        Flushes all entries from cache

        :return: bool True on success or False on failure
        """"""
        raise NotImplementedError"
314	donghui	3	"from abc import ABCMeta, abstractmethod


class CacheAdapter:
    """"""
    CacheAdapter Abstract Base Class
    """"""
    __metaclass__ = ABCMeta

    @abstractmethod
    def get(self, public_id, type, resource_type, transformation, format):
        """"""
        Gets value specified by parameters

        :param public_id:       The public ID of the resource
        :param type:            The storage type
        :param resource_type:   The type of the resource
        :param transformation:  The transformation string
        :param format:          The format of the resource

        :return: None|mixed value, None if not found
        """"""
        raise NotImplementedError

    @abstractmethod
    def set(self, public_id, type, resource_type, transformation, format, value):
        """"""
        Sets value specified by parameters

        :param public_id:       The public ID of the resource
        :param type:            The storage type
        :param resource_type:   The type of the resource
        :param transformation:  The transformation string
        :param format:          The format of the resource
        :param value:           The value to set

        :return: bool True on success or False on failure
        """"""
        raise NotImplementedError

    @abstractmethod
    def delete(self, public_id, type, resource_type, transformation, format):
        """"""
        Deletes entry specified by parameters

        :param public_id:       The public ID of the resource
        :param type:            The storage type
        :param resource_type:   The type of the resource
        :param transformation:  The transformation string
        :param format:          The format of the resource

        :return: bool True on success or False on failure
        """"""
        raise NotImplementedError

    @abstractmethod
    def flush_all(self):
        """"""
        Flushes all entries from cache

        :return: bool True on success or False on failure
        """"""
        raise NotImplementedError"
254	jackson	3	"import pytest

from pandas import (
    CategoricalIndex,
    Index,
)
import pandas._testing as tm


class TestAppend:
    @pytest.fixture
    def ci(self):
        categories = list(""cab"")
        return CategoricalIndex(list(""aabbca""), categories=categories, ordered=False)

    def test_append(self, ci):
        # append cats with the same categories
        result = ci[:3].append(ci[3:])
        tm.assert_index_equal(result, ci, exact=True)

        foos = [ci[:1], ci[1:3], ci[3:]]
        result = foos[0].append(foos[1:])
        tm.assert_index_equal(result, ci, exact=True)

    def test_append_empty(self, ci):
        # empty
        result = ci.append([])
        tm.assert_index_equal(result, ci, exact=True)

    def test_append_mismatched_categories(self, ci):
        # appending with different categories or reordered is not ok
        msg = ""all inputs must be Index""
        with pytest.raises(TypeError, match=msg):
            ci.append(ci.values.set_categories(list(""abcd"")))
        with pytest.raises(TypeError, match=msg):
            ci.append(ci.values.reorder_categories(list(""abc"")))

    def test_append_category_objects(self, ci):
        # with objects
        result = ci.append(Index([""c"", ""a""]))
        expected = CategoricalIndex(list(""aabbcaca""), categories=ci.categories)
        tm.assert_index_equal(result, expected, exact=True)

    def test_append_non_categories(self, ci):
        # invalid objects -> cast to object via concat_compat
        result = ci.append(Index([""a"", ""d""]))
        expected = Index([""a"", ""a"", ""b"", ""b"", ""c"", ""a"", ""a"", ""d""])
        tm.assert_index_equal(result, expected, exact=True)

    def test_append_object(self, ci):
        # GH#14298 - if base object is not categorical -> coerce to object
        result = Index([""c"", ""a""]).append(ci)
        expected = Index(list(""caaabbca""))
        tm.assert_index_equal(result, expected, exact=True)

    def test_append_to_another(self):
        # hits Index._concat
        fst = Index([""a"", ""b""])
        snd = CategoricalIndex([""d"", ""e""])
        result = fst.append(snd)
        expected = Index([""a"", ""b"", ""d"", ""e""])
        tm.assert_index_equal(result, expected)"
254	donghui	2	"import pytest

from pandas import (
    CategoricalIndex,
    Index,
)
import pandas._testing as tm


class TestAppend:
    @pytest.fixture
    def ci(self):
        categories = list(""cab"")
        return CategoricalIndex(list(""aabbca""), categories=categories, ordered=False)

    def test_append(self, ci):
        # append cats with the same categories
        result = ci[:3].append(ci[3:])
        tm.assert_index_equal(result, ci, exact=True)

        foos = [ci[:1], ci[1:3], ci[3:]]
        result = foos[0].append(foos[1:])
        tm.assert_index_equal(result, ci, exact=True)

    def test_append_empty(self, ci):
        # empty
        result = ci.append([])
        tm.assert_index_equal(result, ci, exact=True)

    def test_append_mismatched_categories(self, ci):
        # appending with different categories or reordered is not ok
        msg = ""all inputs must be Index""
        with pytest.raises(TypeError, match=msg):
            ci.append(ci.values.set_categories(list(""abcd"")))
        with pytest.raises(TypeError, match=msg):
            ci.append(ci.values.reorder_categories(list(""abc"")))

    def test_append_category_objects(self, ci):
        # with objects
        result = ci.append(Index([""c"", ""a""]))
        expected = CategoricalIndex(list(""aabbcaca""), categories=ci.categories)
        tm.assert_index_equal(result, expected, exact=True)

    def test_append_non_categories(self, ci):
        # invalid objects -> cast to object via concat_compat
        result = ci.append(Index([""a"", ""d""]))
        expected = Index([""a"", ""a"", ""b"", ""b"", ""c"", ""a"", ""a"", ""d""])
        tm.assert_index_equal(result, expected, exact=True)

    def test_append_object(self, ci):
        # GH#14298 - if base object is not categorical -> coerce to object
        result = Index([""c"", ""a""]).append(ci)
        expected = Index(list(""caaabbca""))
        tm.assert_index_equal(result, expected, exact=True)

    def test_append_to_another(self):
        # hits Index._concat
        fst = Index([""a"", ""b""])
        snd = CategoricalIndex([""d"", ""e""])
        result = fst.append(snd)
        expected = Index([""a"", ""b"", ""d"", ""e""])
        tm.assert_index_equal(result, expected)"
345	jackson	0	"import pygame as pg

class Launch:

    def __init__(self, game):
        self.game = game
        self.screen = game.screen
        self.settings = game.settings
        self.screen_rect = self.screen.get_rect()
        self.images = []
        self.default_color = (255, 255, 255)
        self.prep_strings()
        self.prep_aliens()

    
    def prep_strings(self):
        self.prep_Text(""SPACE"", 170, offsetY=40)
        self.prep_Text(""INVADERS"", 90, color=(0,210,0), offsetY=140)
        self.prep_Text(""= 10 PTS"", 40, offsetX=600, offsetY=300)
        self.prep_Text(""= 20 PTS"", 40, offsetX=600, offsetY=350)
        self.prep_Text(""= 40 PTS"", 40, offsetX=600, offsetY=400)
        self.prep_Text(""= ???"", 40, offsetX=610, offsetY=450)
    
    def prep_aliens(self):
        alien1 = pg.transform.rotozoom(pg.image.load(f'images/alien_03-0.png'), 0, 1.5)
        self.images.append((alien1, (540, 290)))
        alien1 = pg.transform.rotozoom(pg.image.load(f'images/alien__10.png'), 0, .5)
        self.images.append((alien1, (525, 340)))
        alien1 = pg.transform.rotozoom(pg.image.load(f'images/alien__20.png'), 0, .5)
        self.images.append((alien1, (525, 390)))
        alien1 = pg.transform.rotozoom(pg.image.load(f'images/ufo.png'), 0, 1.2)
        self.images.append((alien1, (500, 410)))

    def prep_Text(self, msg, size, color=(255,255,255), offsetX=0, offsetY=0):
        font = pg.font.SysFont(None, size)
        text_image = font.render(msg, True, color, self.settings.bg_color)
        rect = text_image.get_rect()
        if offsetY == 0:
            rect.centery = self.screen_rect.centery
        else:
            rect.top = offsetY
        if offsetX == 0:
            rect.centerx = self.screen_rect.centerx
        else:
            rect.left = offsetX

        self.images.append((text_image,rect))

    def draw(self):
        for image in self.images:
            self.screen.blit(image[0], image[1])"
345	donghui	0	"import pygame as pg

class Launch:

    def __init__(self, game):
        self.game = game
        self.screen = game.screen
        self.settings = game.settings
        self.screen_rect = self.screen.get_rect()
        self.images = []
        self.default_color = (255, 255, 255)
        self.prep_strings()
        self.prep_aliens()

    
    def prep_strings(self):
        self.prep_Text(""SPACE"", 170, offsetY=40)
        self.prep_Text(""INVADERS"", 90, color=(0,210,0), offsetY=140)
        self.prep_Text(""= 10 PTS"", 40, offsetX=600, offsetY=300)
        self.prep_Text(""= 20 PTS"", 40, offsetX=600, offsetY=350)
        self.prep_Text(""= 40 PTS"", 40, offsetX=600, offsetY=400)
        self.prep_Text(""= ???"", 40, offsetX=610, offsetY=450)
    
    def prep_aliens(self):
        alien1 = pg.transform.rotozoom(pg.image.load(f'images/alien_03-0.png'), 0, 1.5)
        self.images.append((alien1, (540, 290)))
        alien1 = pg.transform.rotozoom(pg.image.load(f'images/alien__10.png'), 0, .5)
        self.images.append((alien1, (525, 340)))
        alien1 = pg.transform.rotozoom(pg.image.load(f'images/alien__20.png'), 0, .5)
        self.images.append((alien1, (525, 390)))
        alien1 = pg.transform.rotozoom(pg.image.load(f'images/ufo.png'), 0, 1.2)
        self.images.append((alien1, (500, 410)))

    def prep_Text(self, msg, size, color=(255,255,255), offsetX=0, offsetY=0):
        font = pg.font.SysFont(None, size)
        text_image = font.render(msg, True, color, self.settings.bg_color)
        rect = text_image.get_rect()
        if offsetY == 0:
            rect.centery = self.screen_rect.centery
        else:
            rect.top = offsetY
        if offsetX == 0:
            rect.centerx = self.screen_rect.centerx
        else:
            rect.left = offsetX

        self.images.append((text_image,rect))

    def draw(self):
        for image in self.images:
            self.screen.blit(image[0], image[1])"
293	jackson	4	"""""""eCommerce URL Configuration

The `urlpatterns` list routes URLs to views. For more information please see:
    https://docs.djangoproject.com/en/4.1/topics/http/urls/
Examples:
Function views
    1. Add an import:  from my_app import views
    2. Add a URL to urlpatterns:  path('', views.home, name='home')
Class-based views
    1. Add an import:  from other_app.views import Home
    2. Add a URL to urlpatterns:  path('', Home.as_view(), name='home')
Including another URLconf
    1. Import the include() function: from django.urls import include, path
    2. Add a URL to urlpatterns:  path('blog/', include('blog.urls'))
""""""
from django.contrib import admin
from django.urls import path,include
from crud import views

from django.conf import settings
from django.conf.urls.static import static

urlpatterns = [
    path('ckeditor/',include('ckeditor_uploader.urls')),
    path('admin/', admin.site.urls),
    path('',views.homepage,name=""homepage""),
    path('home/',views.homepage,name=""homepage""),
    path('about/',views.about,name=""about""),
    path('contact/',views.contact,name=""contact""),
    path('product/',views.product,name=""product""),
    path('signup/',views.user_signup,name=""user_signup""),
    path('signin/',views.user_login,name=""user_login""),
    path('dashboard/',views.dashboard,name=""dashboard""),
    path('cart/',views.cart,name=""cart""),
    path('addpost/',views.addpost,name=""addpost""),
    path('addcart/<str:title>',views.addcart,name=""addcart""),
    path('updatepost/<int:id>',views.updatepost,name=""updatepost""),
    path('deletepost/<int:id>',views.deletepost,name=""deletepost""),
    path('logout/',views.user_logout,name=""logout""),
    path('oauth/', include('social_django.urls', namespace='social')),
]

if settings.DEBUG:
    urlpatterns += static(settings.MEDIA_URL,document_root=settings.MEDIA_ROOT)"
293	donghui	3	"""""""eCommerce URL Configuration

The `urlpatterns` list routes URLs to views. For more information please see:
    https://docs.djangoproject.com/en/4.1/topics/http/urls/
Examples:
Function views
    1. Add an import:  from my_app import views
    2. Add a URL to urlpatterns:  path('', views.home, name='home')
Class-based views
    1. Add an import:  from other_app.views import Home
    2. Add a URL to urlpatterns:  path('', Home.as_view(), name='home')
Including another URLconf
    1. Import the include() function: from django.urls import include, path
    2. Add a URL to urlpatterns:  path('blog/', include('blog.urls'))
""""""
from django.contrib import admin
from django.urls import path,include
from crud import views

from django.conf import settings
from django.conf.urls.static import static

urlpatterns = [
    path('ckeditor/',include('ckeditor_uploader.urls')),
    path('admin/', admin.site.urls),
    path('',views.homepage,name=""homepage""),
    path('home/',views.homepage,name=""homepage""),
    path('about/',views.about,name=""about""),
    path('contact/',views.contact,name=""contact""),
    path('product/',views.product,name=""product""),
    path('signup/',views.user_signup,name=""user_signup""),
    path('signin/',views.user_login,name=""user_login""),
    path('dashboard/',views.dashboard,name=""dashboard""),
    path('cart/',views.cart,name=""cart""),
    path('addpost/',views.addpost,name=""addpost""),
    path('addcart/<str:title>',views.addcart,name=""addcart""),
    path('updatepost/<int:id>',views.updatepost,name=""updatepost""),
    path('deletepost/<int:id>',views.deletepost,name=""deletepost""),
    path('logout/',views.user_logout,name=""logout""),
    path('oauth/', include('social_django.urls', namespace='social')),
]

if settings.DEBUG:
    urlpatterns += static(settings.MEDIA_URL,document_root=settings.MEDIA_ROOT)"
382	jackson	1	"def _fix_contents(filename, contents):
    import re

    contents = re.sub(
        r""from bytecode"", r'from _pydevd_frame_eval.vendored.bytecode', contents, flags=re.MULTILINE
    )

    contents = re.sub(
        r""import bytecode"", r'from _pydevd_frame_eval.vendored import bytecode', contents, flags=re.MULTILINE
    )

    # This test will import the wrong setup (we're not interested in it).
    contents = re.sub(
        r""def test_version\(self\):"", r'def skip_test_version(self):', contents, flags=re.MULTILINE
    )

    if filename.startswith('test_'):
        if 'pytestmark' not in contents:
            pytest_mark = '''
import pytest
from tests_python.debugger_unittest import IS_PY36_OR_GREATER, IS_CPYTHON
from tests_python.debug_constants import TEST_CYTHON
pytestmark = pytest.mark.skipif(not IS_PY36_OR_GREATER or not IS_CPYTHON or not TEST_CYTHON, reason='Requires CPython >= 3.6')
'''
            contents = pytest_mark + contents
    return contents


def main():
    import os

    # traverse root directory, and list directories as dirs and files as files
    for root, dirs, files in os.walk(os.path.dirname(__file__)):
        path = root.split(os.sep)
        for filename in files:
            if filename.endswith('.py') and filename != 'pydevd_fix_code.py':
                with open(os.path.join(root, filename), 'r') as stream:
                    contents = stream.read()

                new_contents = _fix_contents(filename, contents)
                if contents != new_contents:
                    print('fixed ', os.path.join(root, filename))
                    with open(os.path.join(root, filename), 'w') as stream:
                        stream.write(new_contents)

#             print(len(path) * '---', filename)


if __name__ == '__main__':
    main()"
382	donghui	2	"def _fix_contents(filename, contents):
    import re

    contents = re.sub(
        r""from bytecode"", r'from _pydevd_frame_eval.vendored.bytecode', contents, flags=re.MULTILINE
    )

    contents = re.sub(
        r""import bytecode"", r'from _pydevd_frame_eval.vendored import bytecode', contents, flags=re.MULTILINE
    )

    # This test will import the wrong setup (we're not interested in it).
    contents = re.sub(
        r""def test_version\(self\):"", r'def skip_test_version(self):', contents, flags=re.MULTILINE
    )

    if filename.startswith('test_'):
        if 'pytestmark' not in contents:
            pytest_mark = '''
import pytest
from tests_python.debugger_unittest import IS_PY36_OR_GREATER, IS_CPYTHON
from tests_python.debug_constants import TEST_CYTHON
pytestmark = pytest.mark.skipif(not IS_PY36_OR_GREATER or not IS_CPYTHON or not TEST_CYTHON, reason='Requires CPython >= 3.6')
'''
            contents = pytest_mark + contents
    return contents


def main():
    import os

    # traverse root directory, and list directories as dirs and files as files
    for root, dirs, files in os.walk(os.path.dirname(__file__)):
        path = root.split(os.sep)
        for filename in files:
            if filename.endswith('.py') and filename != 'pydevd_fix_code.py':
                with open(os.path.join(root, filename), 'r') as stream:
                    contents = stream.read()

                new_contents = _fix_contents(filename, contents)
                if contents != new_contents:
                    print('fixed ', os.path.join(root, filename))
                    with open(os.path.join(root, filename), 'w') as stream:
                        stream.write(new_contents)

#             print(len(path) * '---', filename)


if __name__ == '__main__':
    main()"
371	jackson	3	"from functools import wraps

from django.middleware.csrf import CsrfViewMiddleware, get_token
from django.utils.decorators import decorator_from_middleware

csrf_protect = decorator_from_middleware(CsrfViewMiddleware)
csrf_protect.__name__ = ""csrf_protect""
csrf_protect.__doc__ = """"""
This decorator adds CSRF protection in exactly the same way as
CsrfViewMiddleware, but it can be used on a per view basis.  Using both, or
using the decorator multiple times, is harmless and efficient.
""""""


class _EnsureCsrfToken(CsrfViewMiddleware):
    # Behave like CsrfViewMiddleware but don't reject requests or log warnings.
    def _reject(self, request, reason):
        return None


requires_csrf_token = decorator_from_middleware(_EnsureCsrfToken)
requires_csrf_token.__name__ = 'requires_csrf_token'
requires_csrf_token.__doc__ = """"""
Use this decorator on views that need a correct csrf_token available to
RequestContext, but without the CSRF protection that csrf_protect
enforces.
""""""


class _EnsureCsrfCookie(CsrfViewMiddleware):
    def _reject(self, request, reason):
        return None

    def process_view(self, request, callback, callback_args, callback_kwargs):
        retval = super().process_view(request, callback, callback_args, callback_kwargs)
        # Force process_response to send the cookie
        get_token(request)
        return retval


ensure_csrf_cookie = decorator_from_middleware(_EnsureCsrfCookie)
ensure_csrf_cookie.__name__ = 'ensure_csrf_cookie'
ensure_csrf_cookie.__doc__ = """"""
Use this decorator to ensure that a view sets a CSRF cookie, whether or not it
uses the csrf_token template tag, or the CsrfViewMiddleware is used.
""""""


def csrf_exempt(view_func):
    """"""Mark a view function as being exempt from the CSRF view protection.""""""
    # view_func.csrf_exempt = True would also work, but decorators are nicer
    # if they don't have side effects, so return a new function.
    def wrapped_view(*args, **kwargs):
        return view_func(*args, **kwargs)
    wrapped_view.csrf_exempt = True
    return wraps(view_func)(wrapped_view)"
371	donghui	2	"from functools import wraps

from django.middleware.csrf import CsrfViewMiddleware, get_token
from django.utils.decorators import decorator_from_middleware

csrf_protect = decorator_from_middleware(CsrfViewMiddleware)
csrf_protect.__name__ = ""csrf_protect""
csrf_protect.__doc__ = """"""
This decorator adds CSRF protection in exactly the same way as
CsrfViewMiddleware, but it can be used on a per view basis.  Using both, or
using the decorator multiple times, is harmless and efficient.
""""""


class _EnsureCsrfToken(CsrfViewMiddleware):
    # Behave like CsrfViewMiddleware but don't reject requests or log warnings.
    def _reject(self, request, reason):
        return None


requires_csrf_token = decorator_from_middleware(_EnsureCsrfToken)
requires_csrf_token.__name__ = 'requires_csrf_token'
requires_csrf_token.__doc__ = """"""
Use this decorator on views that need a correct csrf_token available to
RequestContext, but without the CSRF protection that csrf_protect
enforces.
""""""


class _EnsureCsrfCookie(CsrfViewMiddleware):
    def _reject(self, request, reason):
        return None

    def process_view(self, request, callback, callback_args, callback_kwargs):
        retval = super().process_view(request, callback, callback_args, callback_kwargs)
        # Force process_response to send the cookie
        get_token(request)
        return retval


ensure_csrf_cookie = decorator_from_middleware(_EnsureCsrfCookie)
ensure_csrf_cookie.__name__ = 'ensure_csrf_cookie'
ensure_csrf_cookie.__doc__ = """"""
Use this decorator to ensure that a view sets a CSRF cookie, whether or not it
uses the csrf_token template tag, or the CsrfViewMiddleware is used.
""""""


def csrf_exempt(view_func):
    """"""Mark a view function as being exempt from the CSRF view protection.""""""
    # view_func.csrf_exempt = True would also work, but decorators are nicer
    # if they don't have side effects, so return a new function.
    def wrapped_view(*args, **kwargs):
        return view_func(*args, **kwargs)
    wrapped_view.csrf_exempt = True
    return wraps(view_func)(wrapped_view)"
260	jackson	1	"# -*- coding: utf-8 -*-
# Generated by the protocol buffer compiler.  DO NOT EDIT!
# source: streamlit/proto/PageNotFound.proto

from google.protobuf import descriptor as _descriptor
from google.protobuf import message as _message
from google.protobuf import reflection as _reflection
from google.protobuf import symbol_database as _symbol_database
# @@protoc_insertion_point(imports)

_sym_db = _symbol_database.Default()




DESCRIPTOR = _descriptor.FileDescriptor(
  name='streamlit/proto/PageNotFound.proto',
  package='',
  syntax='proto3',
  serialized_options=None,
  create_key=_descriptor._internal_create_key,
  serialized_pb=b'\n\""streamlit/proto/PageNotFound.proto\""!\n\x0cPageNotFound\x12\x11\n\tpage_name\x18\x01 \x01(\tb\x06proto3'
)




_PAGENOTFOUND = _descriptor.Descriptor(
  name='PageNotFound',
  full_name='PageNotFound',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  create_key=_descriptor._internal_create_key,
  fields=[
    _descriptor.FieldDescriptor(
      name='page_name', full_name='PageNotFound.page_name', index=0,
      number=1, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=b"""".decode('utf-8'),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR,  create_key=_descriptor._internal_create_key),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  serialized_options=None,
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=38,
  serialized_end=71,
)

DESCRIPTOR.message_types_by_name['PageNotFound'] = _PAGENOTFOUND
_sym_db.RegisterFileDescriptor(DESCRIPTOR)

PageNotFound = _reflection.GeneratedProtocolMessageType('PageNotFound', (_message.Message,), {
  'DESCRIPTOR' : _PAGENOTFOUND,
  '__module__' : 'streamlit.proto.PageNotFound_pb2'
  # @@protoc_insertion_point(class_scope:PageNotFound)
  })
_sym_db.RegisterMessage(PageNotFound)


# @@protoc_insertion_point(module_scope)"
260	donghui	0	"# -*- coding: utf-8 -*-
# Generated by the protocol buffer compiler.  DO NOT EDIT!
# source: streamlit/proto/PageNotFound.proto

from google.protobuf import descriptor as _descriptor
from google.protobuf import message as _message
from google.protobuf import reflection as _reflection
from google.protobuf import symbol_database as _symbol_database
# @@protoc_insertion_point(imports)

_sym_db = _symbol_database.Default()




DESCRIPTOR = _descriptor.FileDescriptor(
  name='streamlit/proto/PageNotFound.proto',
  package='',
  syntax='proto3',
  serialized_options=None,
  create_key=_descriptor._internal_create_key,
  serialized_pb=b'\n\""streamlit/proto/PageNotFound.proto\""!\n\x0cPageNotFound\x12\x11\n\tpage_name\x18\x01 \x01(\tb\x06proto3'
)




_PAGENOTFOUND = _descriptor.Descriptor(
  name='PageNotFound',
  full_name='PageNotFound',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  create_key=_descriptor._internal_create_key,
  fields=[
    _descriptor.FieldDescriptor(
      name='page_name', full_name='PageNotFound.page_name', index=0,
      number=1, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=b"""".decode('utf-8'),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR,  create_key=_descriptor._internal_create_key),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  serialized_options=None,
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=38,
  serialized_end=71,
)

DESCRIPTOR.message_types_by_name['PageNotFound'] = _PAGENOTFOUND
_sym_db.RegisterFileDescriptor(DESCRIPTOR)

PageNotFound = _reflection.GeneratedProtocolMessageType('PageNotFound', (_message.Message,), {
  'DESCRIPTOR' : _PAGENOTFOUND,
  '__module__' : 'streamlit.proto.PageNotFound_pb2'
  # @@protoc_insertion_point(class_scope:PageNotFound)
  })
_sym_db.RegisterMessage(PageNotFound)


# @@protoc_insertion_point(module_scope)"
300	jackson	1	"# Copyright 2020 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.


# [START automl_batch_predict_beta]
from google.cloud import automl_v1beta1 as automl


def batch_predict(
    project_id=""YOUR_PROJECT_ID"",
    model_id=""YOUR_MODEL_ID"",
    input_uri=""gs://YOUR_BUCKET_ID/path/to/your/input/csv_or_jsonl"",
    output_uri=""gs://YOUR_BUCKET_ID/path/to/save/results/"",
):
    """"""Batch predict""""""
    prediction_client = automl.PredictionServiceClient()

    # Get the full path of the model.
    model_full_id = automl.AutoMlClient.model_path(
        project_id, ""us-central1"", model_id
    )

    gcs_source = automl.GcsSource(input_uris=[input_uri])

    input_config = automl.BatchPredictInputConfig(gcs_source=gcs_source)
    gcs_destination = automl.GcsDestination(output_uri_prefix=output_uri)
    output_config = automl.BatchPredictOutputConfig(
        gcs_destination=gcs_destination
    )
    params = {}

    request = automl.BatchPredictRequest(
        name=model_full_id,
        input_config=input_config,
        output_config=output_config,
        params=params
    )
    response = prediction_client.batch_predict(
        request=request
    )

    print(""Waiting for operation to complete..."")
    print(
        ""Batch Prediction results saved to Cloud Storage bucket. {}"".format(
            response.result()
        )
    )
# [END automl_batch_predict_beta]"
300	donghui	1	"# Copyright 2020 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.


# [START automl_batch_predict_beta]
from google.cloud import automl_v1beta1 as automl


def batch_predict(
    project_id=""YOUR_PROJECT_ID"",
    model_id=""YOUR_MODEL_ID"",
    input_uri=""gs://YOUR_BUCKET_ID/path/to/your/input/csv_or_jsonl"",
    output_uri=""gs://YOUR_BUCKET_ID/path/to/save/results/"",
):
    """"""Batch predict""""""
    prediction_client = automl.PredictionServiceClient()

    # Get the full path of the model.
    model_full_id = automl.AutoMlClient.model_path(
        project_id, ""us-central1"", model_id
    )

    gcs_source = automl.GcsSource(input_uris=[input_uri])

    input_config = automl.BatchPredictInputConfig(gcs_source=gcs_source)
    gcs_destination = automl.GcsDestination(output_uri_prefix=output_uri)
    output_config = automl.BatchPredictOutputConfig(
        gcs_destination=gcs_destination
    )
    params = {}

    request = automl.BatchPredictRequest(
        name=model_full_id,
        input_config=input_config,
        output_config=output_config,
        params=params
    )
    response = prediction_client.batch_predict(
        request=request
    )

    print(""Waiting for operation to complete..."")
    print(
        ""Batch Prediction results saved to Cloud Storage bucket. {}"".format(
            response.result()
        )
    )
# [END automl_batch_predict_beta]"
351	jackson	3	"from django.contrib.messages.storage.base import BaseStorage
from django.contrib.messages.storage.cookie import CookieStorage
from django.contrib.messages.storage.session import SessionStorage


class FallbackStorage(BaseStorage):
    """"""
    Try to store all messages in the first backend. Store any unstored
    messages in each subsequent backend.
    """"""

    storage_classes = (CookieStorage, SessionStorage)

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.storages = [
            storage_class(*args, **kwargs) for storage_class in self.storage_classes
        ]
        self._used_storages = set()

    def _get(self, *args, **kwargs):
        """"""
        Get a single list of messages from all storage backends.
        """"""
        all_messages = []
        for storage in self.storages:
            messages, all_retrieved = storage._get()
            # If the backend hasn't been used, no more retrieval is necessary.
            if messages is None:
                break
            if messages:
                self._used_storages.add(storage)
            all_messages.extend(messages)
            # If this storage class contained all the messages, no further
            # retrieval is necessary
            if all_retrieved:
                break
        return all_messages, all_retrieved

    def _store(self, messages, response, *args, **kwargs):
        """"""
        Store the messages and return any unstored messages after trying all
        backends.

        For each storage backend, any messages not stored are passed on to the
        next backend.
        """"""
        for storage in self.storages:
            if messages:
                messages = storage._store(messages, response, remove_oldest=False)
            # Even if there are no more messages, continue iterating to ensure
            # storages which contained messages are flushed.
            elif storage in self._used_storages:
                storage._store([], response)
                self._used_storages.remove(storage)
        return messages"
351	donghui	2	"from django.contrib.messages.storage.base import BaseStorage
from django.contrib.messages.storage.cookie import CookieStorage
from django.contrib.messages.storage.session import SessionStorage


class FallbackStorage(BaseStorage):
    """"""
    Try to store all messages in the first backend. Store any unstored
    messages in each subsequent backend.
    """"""

    storage_classes = (CookieStorage, SessionStorage)

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.storages = [
            storage_class(*args, **kwargs) for storage_class in self.storage_classes
        ]
        self._used_storages = set()

    def _get(self, *args, **kwargs):
        """"""
        Get a single list of messages from all storage backends.
        """"""
        all_messages = []
        for storage in self.storages:
            messages, all_retrieved = storage._get()
            # If the backend hasn't been used, no more retrieval is necessary.
            if messages is None:
                break
            if messages:
                self._used_storages.add(storage)
            all_messages.extend(messages)
            # If this storage class contained all the messages, no further
            # retrieval is necessary
            if all_retrieved:
                break
        return all_messages, all_retrieved

    def _store(self, messages, response, *args, **kwargs):
        """"""
        Store the messages and return any unstored messages after trying all
        backends.

        For each storage backend, any messages not stored are passed on to the
        next backend.
        """"""
        for storage in self.storages:
            if messages:
                messages = storage._store(messages, response, remove_oldest=False)
            # Even if there are no more messages, continue iterating to ensure
            # storages which contained messages are flushed.
            elif storage in self._used_storages:
                storage._store([], response)
                self._used_storages.remove(storage)
        return messages"
365	jackson	0	"from sqlalchemy import create_engine
import pandas as pd

class Manager:
    __instance = None

    def __init__(self, engine=None, username=None, password=None, database=None, host=None, port=None):
        if Manager.__instance is None:
            self.engine_type = engine
            self.username = username
            self.password = password
            self.database = database
            self.host = host
            self.port = port
            self.url = self._generate_url()
            self.engine = create_engine(self.url)
            Manager.__instance = self
        else:
            raise Exception(""Cannot create multiple instances of Database class"")

    @staticmethod
    def get_instance(engine=None, username=None, password=None, database=None, host=None, port=None):
        if Manager.__instance is None:
            Manager(engine, username, password, database, host, port)
        return Manager.__instance

    def _generate_url(self):
        if self.engine_type == 'postgresql':
            return f""postgresql://{self.username}:{self.password}@{self.host}:{self.port}/{self.database}""
        elif self.engine_type == 'mysql':
            return f""mysql+pymysql://{self.username}:{self.password}@{self.host}:{self.port}/{self.database}""
        elif self.engine_type == 'sqlite':
            return f""sqlite:///{self.database}""
        elif self.engine_type == 'oracle':
            return f""oracle://{self.username}:{self.password}@{self.host}:{self.port}/{self.database}""
        elif self.engine_type == 'mssql':
            return f""mssql+pymssql://{self.username}:{self.password}@{self.host}:{self.port}/{self.database}""
        else:
            raise Exception(""Unsupported engine type"")

    def execute_query(self, query):
        with self.engine.connect() as conn:
            result = conn.execute(query)
            data = pd.DataFrame(result.fetchall(), columns=result.keys())
        return data"
365	donghui	0	"from sqlalchemy import create_engine
import pandas as pd

class Manager:
    __instance = None

    def __init__(self, engine=None, username=None, password=None, database=None, host=None, port=None):
        if Manager.__instance is None:
            self.engine_type = engine
            self.username = username
            self.password = password
            self.database = database
            self.host = host
            self.port = port
            self.url = self._generate_url()
            self.engine = create_engine(self.url)
            Manager.__instance = self
        else:
            raise Exception(""Cannot create multiple instances of Database class"")

    @staticmethod
    def get_instance(engine=None, username=None, password=None, database=None, host=None, port=None):
        if Manager.__instance is None:
            Manager(engine, username, password, database, host, port)
        return Manager.__instance

    def _generate_url(self):
        if self.engine_type == 'postgresql':
            return f""postgresql://{self.username}:{self.password}@{self.host}:{self.port}/{self.database}""
        elif self.engine_type == 'mysql':
            return f""mysql+pymysql://{self.username}:{self.password}@{self.host}:{self.port}/{self.database}""
        elif self.engine_type == 'sqlite':
            return f""sqlite:///{self.database}""
        elif self.engine_type == 'oracle':
            return f""oracle://{self.username}:{self.password}@{self.host}:{self.port}/{self.database}""
        elif self.engine_type == 'mssql':
            return f""mssql+pymssql://{self.username}:{self.password}@{self.host}:{self.port}/{self.database}""
        else:
            raise Exception(""Unsupported engine type"")

    def execute_query(self, query):
        with self.engine.connect() as conn:
            result = conn.execute(query)
            data = pd.DataFrame(result.fetchall(), columns=result.keys())
        return data"
334	jackson	0	"# This file is dual licensed under the terms of the Apache License, Version
# 2.0, and the BSD License. See the LICENSE file in the root of this repository
# for complete details.

import typing

from cryptography.hazmat.primitives import hashes
from cryptography.hazmat.primitives.asymmetric.utils import Prehashed

if typing.TYPE_CHECKING:
    from cryptography.hazmat.backends.openssl.backend import Backend


def _evp_pkey_derive(backend: ""Backend"", evp_pkey, peer_public_key) -> bytes:
    ctx = backend._lib.EVP_PKEY_CTX_new(evp_pkey, backend._ffi.NULL)
    backend.openssl_assert(ctx != backend._ffi.NULL)
    ctx = backend._ffi.gc(ctx, backend._lib.EVP_PKEY_CTX_free)
    res = backend._lib.EVP_PKEY_derive_init(ctx)
    backend.openssl_assert(res == 1)
    res = backend._lib.EVP_PKEY_derive_set_peer(ctx, peer_public_key._evp_pkey)
    if res != 1:
        errors_with_text = backend._consume_errors_with_text()
        raise ValueError(""Error computing shared key."", errors_with_text)

    keylen = backend._ffi.new(""size_t *"")
    res = backend._lib.EVP_PKEY_derive(ctx, backend._ffi.NULL, keylen)
    backend.openssl_assert(res == 1)
    backend.openssl_assert(keylen[0] > 0)
    buf = backend._ffi.new(""unsigned char[]"", keylen[0])
    res = backend._lib.EVP_PKEY_derive(ctx, buf, keylen)
    if res != 1:
        errors_with_text = backend._consume_errors_with_text()
        raise ValueError(""Error computing shared key."", errors_with_text)

    return backend._ffi.buffer(buf, keylen[0])[:]


def _calculate_digest_and_algorithm(
    data: bytes,
    algorithm: typing.Union[Prehashed, hashes.HashAlgorithm],
) -> typing.Tuple[bytes, hashes.HashAlgorithm]:
    if not isinstance(algorithm, Prehashed):
        hash_ctx = hashes.Hash(algorithm)
        hash_ctx.update(data)
        data = hash_ctx.finalize()
    else:
        algorithm = algorithm._algorithm

    if len(data) != algorithm.digest_size:
        raise ValueError(
            ""The provided data must be the same length as the hash ""
            ""algorithm's digest size.""
        )

    return (data, algorithm)"
334	donghui	0	"# This file is dual licensed under the terms of the Apache License, Version
# 2.0, and the BSD License. See the LICENSE file in the root of this repository
# for complete details.

import typing

from cryptography.hazmat.primitives import hashes
from cryptography.hazmat.primitives.asymmetric.utils import Prehashed

if typing.TYPE_CHECKING:
    from cryptography.hazmat.backends.openssl.backend import Backend


def _evp_pkey_derive(backend: ""Backend"", evp_pkey, peer_public_key) -> bytes:
    ctx = backend._lib.EVP_PKEY_CTX_new(evp_pkey, backend._ffi.NULL)
    backend.openssl_assert(ctx != backend._ffi.NULL)
    ctx = backend._ffi.gc(ctx, backend._lib.EVP_PKEY_CTX_free)
    res = backend._lib.EVP_PKEY_derive_init(ctx)
    backend.openssl_assert(res == 1)
    res = backend._lib.EVP_PKEY_derive_set_peer(ctx, peer_public_key._evp_pkey)
    if res != 1:
        errors_with_text = backend._consume_errors_with_text()
        raise ValueError(""Error computing shared key."", errors_with_text)

    keylen = backend._ffi.new(""size_t *"")
    res = backend._lib.EVP_PKEY_derive(ctx, backend._ffi.NULL, keylen)
    backend.openssl_assert(res == 1)
    backend.openssl_assert(keylen[0] > 0)
    buf = backend._ffi.new(""unsigned char[]"", keylen[0])
    res = backend._lib.EVP_PKEY_derive(ctx, buf, keylen)
    if res != 1:
        errors_with_text = backend._consume_errors_with_text()
        raise ValueError(""Error computing shared key."", errors_with_text)

    return backend._ffi.buffer(buf, keylen[0])[:]


def _calculate_digest_and_algorithm(
    data: bytes,
    algorithm: typing.Union[Prehashed, hashes.HashAlgorithm],
) -> typing.Tuple[bytes, hashes.HashAlgorithm]:
    if not isinstance(algorithm, Prehashed):
        hash_ctx = hashes.Hash(algorithm)
        hash_ctx.update(data)
        data = hash_ctx.finalize()
    else:
        algorithm = algorithm._algorithm

    if len(data) != algorithm.digest_size:
        raise ValueError(
            ""The provided data must be the same length as the hash ""
            ""algorithm's digest size.""
        )

    return (data, algorithm)"
274	jackson	0	"import threading
from dataclasses import dataclass

from prowler.lib.logger import logger
from prowler.providers.aws.aws_provider import generate_regional_clients


################## Macie
class Macie:
    def __init__(self, audit_info):
        self.service = ""macie2""
        self.session = audit_info.audit_session
        self.audited_account = audit_info.audited_account
        self.regional_clients = generate_regional_clients(self.service, audit_info)
        self.sessions = []
        self.__threading_call__(self.__get_macie_session__)

    def __get_session__(self):
        return self.session

    def __threading_call__(self, call):
        threads = []
        for regional_client in self.regional_clients.values():
            threads.append(threading.Thread(target=call, args=(regional_client,)))
        for t in threads:
            t.start()
        for t in threads:
            t.join()

    def __get_macie_session__(self, regional_client):
        logger.info(""Macie - Get Macie Session..."")
        try:
            self.sessions.append(
                Session(
                    regional_client.get_macie_session()[""status""],
                    regional_client.region,
                )
            )

        except Exception as error:
            if ""Macie is not enabled"" in str(error):
                self.sessions.append(
                    Session(
                        ""DISABLED"",
                        regional_client.region,
                    )
                )
            else:
                logger.error(
                    f""{regional_client.region} -- {error.__class__.__name__}[{error.__traceback__.tb_lineno}]: {error}""
                )


@dataclass
class Session:
    status: str
    region: str

    def __init__(
        self,
        status,
        region,
    ):
        self.status = status
        self.region = region"
274	donghui	0	"import threading
from dataclasses import dataclass

from prowler.lib.logger import logger
from prowler.providers.aws.aws_provider import generate_regional_clients


################## Macie
class Macie:
    def __init__(self, audit_info):
        self.service = ""macie2""
        self.session = audit_info.audit_session
        self.audited_account = audit_info.audited_account
        self.regional_clients = generate_regional_clients(self.service, audit_info)
        self.sessions = []
        self.__threading_call__(self.__get_macie_session__)

    def __get_session__(self):
        return self.session

    def __threading_call__(self, call):
        threads = []
        for regional_client in self.regional_clients.values():
            threads.append(threading.Thread(target=call, args=(regional_client,)))
        for t in threads:
            t.start()
        for t in threads:
            t.join()

    def __get_macie_session__(self, regional_client):
        logger.info(""Macie - Get Macie Session..."")
        try:
            self.sessions.append(
                Session(
                    regional_client.get_macie_session()[""status""],
                    regional_client.region,
                )
            )

        except Exception as error:
            if ""Macie is not enabled"" in str(error):
                self.sessions.append(
                    Session(
                        ""DISABLED"",
                        regional_client.region,
                    )
                )
            else:
                logger.error(
                    f""{regional_client.region} -- {error.__class__.__name__}[{error.__traceback__.tb_lineno}]: {error}""
                )


@dataclass
class Session:
    status: str
    region: str

    def __init__(
        self,
        status,
        region,
    ):
        self.status = status
        self.region = region"
287	jackson	0	"_base_ = [
    '../_base_/datasets/coco_detection.py',
    '../_base_/schedules/schedule_1x.py', '../_base_/default_runtime.py'
]
model = dict(
    type='ATSS',
    backbone=dict(
        type='ResNet',
        depth=50,
        num_stages=4,
        out_indices=(0, 1, 2, 3),
        frozen_stages=1,
        norm_cfg=dict(type='BN', requires_grad=True),
        norm_eval=True,
        style='pytorch',
        init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet50')),
    neck=[
        dict(
            type='FPN',
            in_channels=[256, 512, 1024, 2048],
            out_channels=256,
            start_level=1,
            add_extra_convs='on_output',
            num_outs=5),
        dict(type='DyHead', in_channels=256, out_channels=256, num_blocks=6)
    ],
    bbox_head=dict(
        type='ATSSHead',
        num_classes=80,
        in_channels=256,
        stacked_convs=0,
        feat_channels=256,
        anchor_generator=dict(
            type='AnchorGenerator',
            ratios=[1.0],
            octave_base_scale=8,
            scales_per_octave=1,
            strides=[8, 16, 32, 64, 128]),
        bbox_coder=dict(
            type='DeltaXYWHBBoxCoder',
            target_means=[.0, .0, .0, .0],
            target_stds=[0.1, 0.1, 0.2, 0.2]),
        loss_cls=dict(
            type='FocalLoss',
            use_sigmoid=True,
            gamma=2.0,
            alpha=0.25,
            loss_weight=1.0),
        loss_bbox=dict(type='GIoULoss', loss_weight=2.0),
        loss_centerness=dict(
            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0)),
    # training and testing settings
    train_cfg=dict(
        assigner=dict(type='ATSSAssigner', topk=9),
        allowed_border=-1,
        pos_weight=-1,
        debug=False),
    test_cfg=dict(
        nms_pre=1000,
        min_bbox_size=0,
        score_thr=0.05,
        nms=dict(type='nms', iou_threshold=0.6),
        max_per_img=100))
# optimizer
optimizer = dict(type='SGD', lr=0.01, momentum=0.9, weight_decay=0.0001)"
287	donghui	0	"_base_ = [
    '../_base_/datasets/coco_detection.py',
    '../_base_/schedules/schedule_1x.py', '../_base_/default_runtime.py'
]
model = dict(
    type='ATSS',
    backbone=dict(
        type='ResNet',
        depth=50,
        num_stages=4,
        out_indices=(0, 1, 2, 3),
        frozen_stages=1,
        norm_cfg=dict(type='BN', requires_grad=True),
        norm_eval=True,
        style='pytorch',
        init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet50')),
    neck=[
        dict(
            type='FPN',
            in_channels=[256, 512, 1024, 2048],
            out_channels=256,
            start_level=1,
            add_extra_convs='on_output',
            num_outs=5),
        dict(type='DyHead', in_channels=256, out_channels=256, num_blocks=6)
    ],
    bbox_head=dict(
        type='ATSSHead',
        num_classes=80,
        in_channels=256,
        stacked_convs=0,
        feat_channels=256,
        anchor_generator=dict(
            type='AnchorGenerator',
            ratios=[1.0],
            octave_base_scale=8,
            scales_per_octave=1,
            strides=[8, 16, 32, 64, 128]),
        bbox_coder=dict(
            type='DeltaXYWHBBoxCoder',
            target_means=[.0, .0, .0, .0],
            target_stds=[0.1, 0.1, 0.2, 0.2]),
        loss_cls=dict(
            type='FocalLoss',
            use_sigmoid=True,
            gamma=2.0,
            alpha=0.25,
            loss_weight=1.0),
        loss_bbox=dict(type='GIoULoss', loss_weight=2.0),
        loss_centerness=dict(
            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0)),
    # training and testing settings
    train_cfg=dict(
        assigner=dict(type='ATSSAssigner', topk=9),
        allowed_border=-1,
        pos_weight=-1,
        debug=False),
    test_cfg=dict(
        nms_pre=1000,
        min_bbox_size=0,
        score_thr=0.05,
        nms=dict(type='nms', iou_threshold=0.6),
        max_per_img=100))
# optimizer
optimizer = dict(type='SGD', lr=0.01, momentum=0.9, weight_decay=0.0001)"
396	jackson	0	"# Generated by Django 3.2.18 on 2023-03-09 11:26

from django.db import migrations, models
import django.db.models.deletion
import multiselectfield.db.fields


class Migration(migrations.Migration):

    initial = True

    dependencies = [
    ]

    operations = [
        migrations.CreateModel(
            name='Event',
            fields=[
                ('id', models.BigAutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),
                ('event_name', models.CharField(max_length=200, unique=True)),
                ('event_date', models.DateField()),
                ('event_time', models.TimeField()),
                ('created_on', models.DateTimeField(auto_now_add=True)),
            ],
        ),
        migrations.CreateModel(
            name='Guest',
            fields=[
                ('id', models.BigAutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),
                ('guest_name', models.CharField(max_length=200, unique=True)),
                ('slug', models.SlugField(max_length=200, unique=True)),
                ('email', models.EmailField(max_length=254)),
                ('is_attending', models.BooleanField(choices=[(False, 'No'), (True, 'Yes')], default='', verbose_name='Attending?')),
                ('message', models.TextField(blank=True)),
                ('dietary_requirements', multiselectfield.db.fields.MultiSelectField(choices=[(1, 'none'), (2, 'coeliac'), (3, 'food allergy'), (4, 'food intolerance'), (5, 'vegetarian'), (6, 'vegan'), (7, 'pescatarian'), (8, 'teetotal')], max_length=15)),
                ('plus_one_attending', models.BooleanField(choices=[(False, 'No'), (True, 'Yes')], default='', verbose_name='Attending?')),
                ('invited', models.IntegerField(choices=[(0, 'Draft'), (1, 'Invited')], default=0)),
                ('event', models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, related_name='guests', to='weddingapp.event')),
            ],
            options={
                'ordering': ['-guest_name'],
            },
        ),
    ]"
396	donghui	0	"# Generated by Django 3.2.18 on 2023-03-09 11:26

from django.db import migrations, models
import django.db.models.deletion
import multiselectfield.db.fields


class Migration(migrations.Migration):

    initial = True

    dependencies = [
    ]

    operations = [
        migrations.CreateModel(
            name='Event',
            fields=[
                ('id', models.BigAutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),
                ('event_name', models.CharField(max_length=200, unique=True)),
                ('event_date', models.DateField()),
                ('event_time', models.TimeField()),
                ('created_on', models.DateTimeField(auto_now_add=True)),
            ],
        ),
        migrations.CreateModel(
            name='Guest',
            fields=[
                ('id', models.BigAutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),
                ('guest_name', models.CharField(max_length=200, unique=True)),
                ('slug', models.SlugField(max_length=200, unique=True)),
                ('email', models.EmailField(max_length=254)),
                ('is_attending', models.BooleanField(choices=[(False, 'No'), (True, 'Yes')], default='', verbose_name='Attending?')),
                ('message', models.TextField(blank=True)),
                ('dietary_requirements', multiselectfield.db.fields.MultiSelectField(choices=[(1, 'none'), (2, 'coeliac'), (3, 'food allergy'), (4, 'food intolerance'), (5, 'vegetarian'), (6, 'vegan'), (7, 'pescatarian'), (8, 'teetotal')], max_length=15)),
                ('plus_one_attending', models.BooleanField(choices=[(False, 'No'), (True, 'Yes')], default='', verbose_name='Attending?')),
                ('invited', models.IntegerField(choices=[(0, 'Draft'), (1, 'Invited')], default=0)),
                ('event', models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, related_name='guests', to='weddingapp.event')),
            ],
            options={
                'ordering': ['-guest_name'],
            },
        ),
    ]"
489	jackson	3	"from django.contrib.messages.storage.base import BaseStorage
from django.contrib.messages.storage.cookie import CookieStorage
from django.contrib.messages.storage.session import SessionStorage


class FallbackStorage(BaseStorage):
    """"""
    Try to store all messages in the first backend. Store any unstored
    messages in each subsequent backend.
    """"""
    storage_classes = (CookieStorage, SessionStorage)

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.storages = [storage_class(*args, **kwargs)
                         for storage_class in self.storage_classes]
        self._used_storages = set()

    def _get(self, *args, **kwargs):
        """"""
        Get a single list of messages from all storage backends.
        """"""
        all_messages = []
        for storage in self.storages:
            messages, all_retrieved = storage._get()
            # If the backend hasn't been used, no more retrieval is necessary.
            if messages is None:
                break
            if messages:
                self._used_storages.add(storage)
            all_messages.extend(messages)
            # If this storage class contained all the messages, no further
            # retrieval is necessary
            if all_retrieved:
                break
        return all_messages, all_retrieved

    def _store(self, messages, response, *args, **kwargs):
        """"""
        Store the messages and return any unstored messages after trying all
        backends.

        For each storage backend, any messages not stored are passed on to the
        next backend.
        """"""
        for storage in self.storages:
            if messages:
                messages = storage._store(messages, response, remove_oldest=False)
            # Even if there are no more messages, continue iterating to ensure
            # storages which contained messages are flushed.
            elif storage in self._used_storages:
                storage._store([], response)
                self._used_storages.remove(storage)
        return messages"
489	donghui	3	"from django.contrib.messages.storage.base import BaseStorage
from django.contrib.messages.storage.cookie import CookieStorage
from django.contrib.messages.storage.session import SessionStorage


class FallbackStorage(BaseStorage):
    """"""
    Try to store all messages in the first backend. Store any unstored
    messages in each subsequent backend.
    """"""
    storage_classes = (CookieStorage, SessionStorage)

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.storages = [storage_class(*args, **kwargs)
                         for storage_class in self.storage_classes]
        self._used_storages = set()

    def _get(self, *args, **kwargs):
        """"""
        Get a single list of messages from all storage backends.
        """"""
        all_messages = []
        for storage in self.storages:
            messages, all_retrieved = storage._get()
            # If the backend hasn't been used, no more retrieval is necessary.
            if messages is None:
                break
            if messages:
                self._used_storages.add(storage)
            all_messages.extend(messages)
            # If this storage class contained all the messages, no further
            # retrieval is necessary
            if all_retrieved:
                break
        return all_messages, all_retrieved

    def _store(self, messages, response, *args, **kwargs):
        """"""
        Store the messages and return any unstored messages after trying all
        backends.

        For each storage backend, any messages not stored are passed on to the
        next backend.
        """"""
        for storage in self.storages:
            if messages:
                messages = storage._store(messages, response, remove_oldest=False)
            # Even if there are no more messages, continue iterating to ensure
            # storages which contained messages are flushed.
            elif storage in self._used_storages:
                storage._store([], response)
                self._used_storages.remove(storage)
        return messages"
499	jackson	4	"""""""Object Utilities.""""""
from __future__ import absolute_import, unicode_literals


class cached_property(object):
    """"""Cached property descriptor.

    Caches the return value of the get method on first call.

    Examples:
        .. code-block:: python

            @cached_property
            def connection(self):
                return Connection()

            @connection.setter  # Prepares stored value
            def connection(self, value):
                if value is None:
                    raise TypeError('Connection must be a connection')
                return value

            @connection.deleter
            def connection(self, value):
                # Additional action to do at del(self.attr)
                if value is not None:
                    print('Connection {0!r} deleted'.format(value)
    """"""

    def __init__(self, fget=None, fset=None, fdel=None, doc=None):
        self.__get = fget
        self.__set = fset
        self.__del = fdel
        self.__doc__ = doc or fget.__doc__
        self.__name__ = fget.__name__
        self.__module__ = fget.__module__

    def __get__(self, obj, type=None):
        if obj is None:
            return self
        try:
            return obj.__dict__[self.__name__]
        except KeyError:
            value = obj.__dict__[self.__name__] = self.__get(obj)
            return value

    def __set__(self, obj, value):
        if obj is None:
            return self
        if self.__set is not None:
            value = self.__set(obj, value)
        obj.__dict__[self.__name__] = value

    def __delete__(self, obj, _sentinel=object()):
        if obj is None:
            return self
        value = obj.__dict__.pop(self.__name__, _sentinel)
        if self.__del is not None and value is not _sentinel:
            self.__del(obj, value)

    def setter(self, fset):
        return self.__class__(self.__get, fset, self.__del)

    def deleter(self, fdel):
        return self.__class__(self.__get, self.__set, fdel)"
499	donghui	4	"""""""Object Utilities.""""""
from __future__ import absolute_import, unicode_literals


class cached_property(object):
    """"""Cached property descriptor.

    Caches the return value of the get method on first call.

    Examples:
        .. code-block:: python

            @cached_property
            def connection(self):
                return Connection()

            @connection.setter  # Prepares stored value
            def connection(self, value):
                if value is None:
                    raise TypeError('Connection must be a connection')
                return value

            @connection.deleter
            def connection(self, value):
                # Additional action to do at del(self.attr)
                if value is not None:
                    print('Connection {0!r} deleted'.format(value)
    """"""

    def __init__(self, fget=None, fset=None, fdel=None, doc=None):
        self.__get = fget
        self.__set = fset
        self.__del = fdel
        self.__doc__ = doc or fget.__doc__
        self.__name__ = fget.__name__
        self.__module__ = fget.__module__

    def __get__(self, obj, type=None):
        if obj is None:
            return self
        try:
            return obj.__dict__[self.__name__]
        except KeyError:
            value = obj.__dict__[self.__name__] = self.__get(obj)
            return value

    def __set__(self, obj, value):
        if obj is None:
            return self
        if self.__set is not None:
            value = self.__set(obj, value)
        obj.__dict__[self.__name__] = value

    def __delete__(self, obj, _sentinel=object()):
        if obj is None:
            return self
        value = obj.__dict__.pop(self.__name__, _sentinel)
        if self.__del is not None and value is not _sentinel:
            self.__del(obj, value)

    def setter(self, fset):
        return self.__class__(self.__get, fset, self.__del)

    def deleter(self, fdel):
        return self.__class__(self.__get, self.__set, fdel)"
386	jackson	2	"""""""
rest_framework.schemas

schemas:
    __init__.py
    generators.py   # Top-down schema generation
    inspectors.py   # Per-endpoint view introspection
    utils.py        # Shared helper functions
    views.py        # Houses `SchemaView`, `APIView` subclass.

We expose a minimal ""public"" API directly from `schemas`. This covers the
basic use-cases:

    from rest_framework.schemas import (
        AutoSchema,
        ManualSchema,
        get_schema_view,
        SchemaGenerator,
    )

Other access should target the submodules directly
""""""
from rest_framework.settings import api_settings

from . import coreapi, openapi
from .coreapi import AutoSchema, ManualSchema, SchemaGenerator  # noqa
from .inspectors import DefaultSchema  # noqa


def get_schema_view(
    title=None,
    url=None,
    description=None,
    urlconf=None,
    renderer_classes=None,
    public=False,
    patterns=None,
    generator_class=None,
    authentication_classes=api_settings.DEFAULT_AUTHENTICATION_CLASSES,
    permission_classes=api_settings.DEFAULT_PERMISSION_CLASSES,
    version=None,
):
    """"""
    Return a schema view.
    """"""
    if generator_class is None:
        if coreapi.is_enabled():
            generator_class = coreapi.SchemaGenerator
        else:
            generator_class = openapi.SchemaGenerator

    generator = generator_class(
        title=title,
        url=url,
        description=description,
        urlconf=urlconf,
        patterns=patterns,
        version=version,
    )

    # Avoid import cycle on APIView
    from .views import SchemaView

    return SchemaView.as_view(
        renderer_classes=renderer_classes,
        schema_generator=generator,
        public=public,
        authentication_classes=authentication_classes,
        permission_classes=permission_classes,
    )"
386	donghui	2	"""""""
rest_framework.schemas

schemas:
    __init__.py
    generators.py   # Top-down schema generation
    inspectors.py   # Per-endpoint view introspection
    utils.py        # Shared helper functions
    views.py        # Houses `SchemaView`, `APIView` subclass.

We expose a minimal ""public"" API directly from `schemas`. This covers the
basic use-cases:

    from rest_framework.schemas import (
        AutoSchema,
        ManualSchema,
        get_schema_view,
        SchemaGenerator,
    )

Other access should target the submodules directly
""""""
from rest_framework.settings import api_settings

from . import coreapi, openapi
from .coreapi import AutoSchema, ManualSchema, SchemaGenerator  # noqa
from .inspectors import DefaultSchema  # noqa


def get_schema_view(
    title=None,
    url=None,
    description=None,
    urlconf=None,
    renderer_classes=None,
    public=False,
    patterns=None,
    generator_class=None,
    authentication_classes=api_settings.DEFAULT_AUTHENTICATION_CLASSES,
    permission_classes=api_settings.DEFAULT_PERMISSION_CLASSES,
    version=None,
):
    """"""
    Return a schema view.
    """"""
    if generator_class is None:
        if coreapi.is_enabled():
            generator_class = coreapi.SchemaGenerator
        else:
            generator_class = openapi.SchemaGenerator

    generator = generator_class(
        title=title,
        url=url,
        description=description,
        urlconf=urlconf,
        patterns=patterns,
        version=version,
    )

    # Avoid import cycle on APIView
    from .views import SchemaView

    return SchemaView.as_view(
        renderer_classes=renderer_classes,
        schema_generator=generator,
        public=public,
        authentication_classes=authentication_classes,
        permission_classes=permission_classes,
    )"
297	jackson	0	"#  Copyright 2022 Google LLC
#
#  Licensed under the Apache License, Version 2.0 (the ""License"");
#  you may not use this file except in compliance with the License.
#  You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an ""AS IS"" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
#  limitations under the License.
import uuid


import google.auth
from google.cloud import batch_v1
from google.cloud import storage
import pytest

from .test_basics import _test_body
from ..create.create_with_mounted_bucket import create_script_job_with_bucket

PROJECT = google.auth.default()[1]
REGION = 'europe-north1'

TIMEOUT = 600  # 10 minutes

WAIT_STATES = {
    batch_v1.JobStatus.State.STATE_UNSPECIFIED,
    batch_v1.JobStatus.State.QUEUED,
    batch_v1.JobStatus.State.RUNNING,
    batch_v1.JobStatus.State.SCHEDULED,
}


@pytest.fixture
def job_name():
    return f""test-job-{uuid.uuid4().hex[:10]}""


@pytest.fixture()
def test_bucket():
    bucket_name = f""test-bucket-{uuid.uuid4().hex[:8]}""
    client = storage.Client()
    client.create_bucket(bucket_name, location=""eu"")

    yield bucket_name

    bucket = client.get_bucket(bucket_name)
    bucket.delete(force=True)


def _test_bucket_content(test_bucket):
    client = storage.Client()
    bucket = client.get_bucket(test_bucket)

    file_name_template = ""output_task_{task_number}.txt""
    file_content_template = ""Hello world from task {task_number}.\n""

    for i in range(4):
        blob = bucket.blob(file_name_template.format(task_number=i))
        content = blob.download_as_bytes().decode()
        assert content == file_content_template.format(task_number=i)


def test_bucket_job(job_name, test_bucket):
    job = create_script_job_with_bucket(PROJECT, REGION, job_name, test_bucket)
    _test_body(job, lambda: _test_bucket_content(test_bucket))"
297	donghui	0	"#  Copyright 2022 Google LLC
#
#  Licensed under the Apache License, Version 2.0 (the ""License"");
#  you may not use this file except in compliance with the License.
#  You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an ""AS IS"" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
#  limitations under the License.
import uuid


import google.auth
from google.cloud import batch_v1
from google.cloud import storage
import pytest

from .test_basics import _test_body
from ..create.create_with_mounted_bucket import create_script_job_with_bucket

PROJECT = google.auth.default()[1]
REGION = 'europe-north1'

TIMEOUT = 600  # 10 minutes

WAIT_STATES = {
    batch_v1.JobStatus.State.STATE_UNSPECIFIED,
    batch_v1.JobStatus.State.QUEUED,
    batch_v1.JobStatus.State.RUNNING,
    batch_v1.JobStatus.State.SCHEDULED,
}


@pytest.fixture
def job_name():
    return f""test-job-{uuid.uuid4().hex[:10]}""


@pytest.fixture()
def test_bucket():
    bucket_name = f""test-bucket-{uuid.uuid4().hex[:8]}""
    client = storage.Client()
    client.create_bucket(bucket_name, location=""eu"")

    yield bucket_name

    bucket = client.get_bucket(bucket_name)
    bucket.delete(force=True)


def _test_bucket_content(test_bucket):
    client = storage.Client()
    bucket = client.get_bucket(test_bucket)

    file_name_template = ""output_task_{task_number}.txt""
    file_content_template = ""Hello world from task {task_number}.\n""

    for i in range(4):
        blob = bucket.blob(file_name_template.format(task_number=i))
        content = blob.download_as_bytes().decode()
        assert content == file_content_template.format(task_number=i)


def test_bucket_job(job_name, test_bucket):
    job = create_script_job_with_bucket(PROJECT, REGION, job_name, test_bucket)
    _test_body(job, lambda: _test_bucket_content(test_bucket))"
264	jackson	1	"# automatically generated by the FlatBuffers compiler, do not modify

# namespace: proto

import flatbuffers
from flatbuffers.compat import import_numpy
np = import_numpy()

class RouterRoles(object):
    __slots__ = ['_tab']

    @classmethod
    def GetRootAs(cls, buf, offset=0):
        n = flatbuffers.encode.Get(flatbuffers.packer.uoffset, buf, offset)
        x = RouterRoles()
        x.Init(buf, n + offset)
        return x

    @classmethod
    def GetRootAsRouterRoles(cls, buf, offset=0):
        """"""This method is deprecated. Please switch to GetRootAs.""""""
        return cls.GetRootAs(buf, offset)
    # RouterRoles
    def Init(self, buf, pos):
        self._tab = flatbuffers.table.Table(buf, pos)

    # RouterRoles
    def Broker(self):
        o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(4))
        if o != 0:
            x = self._tab.Indirect(o + self._tab.Pos)
            from wamp.proto.BrokerFeatures import BrokerFeatures
            obj = BrokerFeatures()
            obj.Init(self._tab.Bytes, x)
            return obj
        return None

    # RouterRoles
    def Dealer(self):
        o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(6))
        if o != 0:
            x = self._tab.Indirect(o + self._tab.Pos)
            from wamp.proto.DealerFeatures import DealerFeatures
            obj = DealerFeatures()
            obj.Init(self._tab.Bytes, x)
            return obj
        return None

def RouterRolesStart(builder): builder.StartObject(2)
def Start(builder):
    return RouterRolesStart(builder)
def RouterRolesAddBroker(builder, broker): builder.PrependUOffsetTRelativeSlot(0, flatbuffers.number_types.UOffsetTFlags.py_type(broker), 0)
def AddBroker(builder, broker):
    return RouterRolesAddBroker(builder, broker)
def RouterRolesAddDealer(builder, dealer): builder.PrependUOffsetTRelativeSlot(1, flatbuffers.number_types.UOffsetTFlags.py_type(dealer), 0)
def AddDealer(builder, dealer):
    return RouterRolesAddDealer(builder, dealer)
def RouterRolesEnd(builder): return builder.EndObject()
def End(builder):
    return RouterRolesEnd(builder)"
264	donghui	1	"# automatically generated by the FlatBuffers compiler, do not modify

# namespace: proto

import flatbuffers
from flatbuffers.compat import import_numpy
np = import_numpy()

class RouterRoles(object):
    __slots__ = ['_tab']

    @classmethod
    def GetRootAs(cls, buf, offset=0):
        n = flatbuffers.encode.Get(flatbuffers.packer.uoffset, buf, offset)
        x = RouterRoles()
        x.Init(buf, n + offset)
        return x

    @classmethod
    def GetRootAsRouterRoles(cls, buf, offset=0):
        """"""This method is deprecated. Please switch to GetRootAs.""""""
        return cls.GetRootAs(buf, offset)
    # RouterRoles
    def Init(self, buf, pos):
        self._tab = flatbuffers.table.Table(buf, pos)

    # RouterRoles
    def Broker(self):
        o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(4))
        if o != 0:
            x = self._tab.Indirect(o + self._tab.Pos)
            from wamp.proto.BrokerFeatures import BrokerFeatures
            obj = BrokerFeatures()
            obj.Init(self._tab.Bytes, x)
            return obj
        return None

    # RouterRoles
    def Dealer(self):
        o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(6))
        if o != 0:
            x = self._tab.Indirect(o + self._tab.Pos)
            from wamp.proto.DealerFeatures import DealerFeatures
            obj = DealerFeatures()
            obj.Init(self._tab.Bytes, x)
            return obj
        return None

def RouterRolesStart(builder): builder.StartObject(2)
def Start(builder):
    return RouterRolesStart(builder)
def RouterRolesAddBroker(builder, broker): builder.PrependUOffsetTRelativeSlot(0, flatbuffers.number_types.UOffsetTFlags.py_type(broker), 0)
def AddBroker(builder, broker):
    return RouterRolesAddBroker(builder, broker)
def RouterRolesAddDealer(builder, dealer): builder.PrependUOffsetTRelativeSlot(1, flatbuffers.number_types.UOffsetTFlags.py_type(dealer), 0)
def AddDealer(builder, dealer):
    return RouterRolesAddDealer(builder, dealer)
def RouterRolesEnd(builder): return builder.EndObject()
def End(builder):
    return RouterRolesEnd(builder)"
324	jackson	0	"from urllib.parse import parse_qsl, unquote, urlparse, urlunparse

from django import template
from django.contrib.admin.utils import quote
from django.urls import Resolver404, get_script_prefix, resolve
from django.utils.http import urlencode

register = template.Library()


@register.filter
def admin_urlname(value, arg):
    return 'admin:%s_%s_%s' % (value.app_label, value.model_name, arg)


@register.filter
def admin_urlquote(value):
    return quote(value)


@register.simple_tag(takes_context=True)
def add_preserved_filters(context, url, popup=False, to_field=None):
    opts = context.get('opts')
    preserved_filters = context.get('preserved_filters')

    parsed_url = list(urlparse(url))
    parsed_qs = dict(parse_qsl(parsed_url[4]))
    merged_qs = {}

    if opts and preserved_filters:
        preserved_filters = dict(parse_qsl(preserved_filters))

        match_url = '/%s' % unquote(url).partition(get_script_prefix())[2]
        try:
            match = resolve(match_url)
        except Resolver404:
            pass
        else:
            current_url = '%s:%s' % (match.app_name, match.url_name)
            changelist_url = 'admin:%s_%s_changelist' % (opts.app_label, opts.model_name)
            if changelist_url == current_url and '_changelist_filters' in preserved_filters:
                preserved_filters = dict(parse_qsl(preserved_filters['_changelist_filters']))

        merged_qs.update(preserved_filters)

    if popup:
        from django.contrib.admin.options import IS_POPUP_VAR
        merged_qs[IS_POPUP_VAR] = 1
    if to_field:
        from django.contrib.admin.options import TO_FIELD_VAR
        merged_qs[TO_FIELD_VAR] = to_field

    merged_qs.update(parsed_qs)

    parsed_url[4] = urlencode(merged_qs)
    return urlunparse(parsed_url)"
324	donghui	0	"from urllib.parse import parse_qsl, unquote, urlparse, urlunparse

from django import template
from django.contrib.admin.utils import quote
from django.urls import Resolver404, get_script_prefix, resolve
from django.utils.http import urlencode

register = template.Library()


@register.filter
def admin_urlname(value, arg):
    return 'admin:%s_%s_%s' % (value.app_label, value.model_name, arg)


@register.filter
def admin_urlquote(value):
    return quote(value)


@register.simple_tag(takes_context=True)
def add_preserved_filters(context, url, popup=False, to_field=None):
    opts = context.get('opts')
    preserved_filters = context.get('preserved_filters')

    parsed_url = list(urlparse(url))
    parsed_qs = dict(parse_qsl(parsed_url[4]))
    merged_qs = {}

    if opts and preserved_filters:
        preserved_filters = dict(parse_qsl(preserved_filters))

        match_url = '/%s' % unquote(url).partition(get_script_prefix())[2]
        try:
            match = resolve(match_url)
        except Resolver404:
            pass
        else:
            current_url = '%s:%s' % (match.app_name, match.url_name)
            changelist_url = 'admin:%s_%s_changelist' % (opts.app_label, opts.model_name)
            if changelist_url == current_url and '_changelist_filters' in preserved_filters:
                preserved_filters = dict(parse_qsl(preserved_filters['_changelist_filters']))

        merged_qs.update(preserved_filters)

    if popup:
        from django.contrib.admin.options import IS_POPUP_VAR
        merged_qs[IS_POPUP_VAR] = 1
    if to_field:
        from django.contrib.admin.options import TO_FIELD_VAR
        merged_qs[TO_FIELD_VAR] = to_field

    merged_qs.update(parsed_qs)

    parsed_url[4] = urlencode(merged_qs)
    return urlunparse(parsed_url)"
375	jackson	3	"#
# The Python Imaging Library.
# $Id$
#
# Binary input/output support routines.
#
# Copyright (c) 1997-2003 by Secret Labs AB
# Copyright (c) 1995-2003 by Fredrik Lundh
# Copyright (c) 2012 by Brian Crowell
#
# See the README file for information on usage and redistribution.
#


""""""Binary input/output support routines.""""""


from struct import pack, unpack_from


def i8(c):
    return c if c.__class__ is int else c[0]


def o8(i):
    return bytes((i & 255,))


# Input, le = little endian, be = big endian
def i16le(c, o=0):
    """"""
    Converts a 2-bytes (16 bits) string to an unsigned integer.

    :param c: string containing bytes to convert
    :param o: offset of bytes to convert in string
    """"""
    return unpack_from(""<H"", c, o)[0]


def si16le(c, o=0):
    """"""
    Converts a 2-bytes (16 bits) string to a signed integer.

    :param c: string containing bytes to convert
    :param o: offset of bytes to convert in string
    """"""
    return unpack_from(""<h"", c, o)[0]


def si16be(c, o=0):
    """"""
    Converts a 2-bytes (16 bits) string to a signed integer, big endian.

    :param c: string containing bytes to convert
    :param o: offset of bytes to convert in string
    """"""
    return unpack_from("">h"", c, o)[0]


def i32le(c, o=0):
    """"""
    Converts a 4-bytes (32 bits) string to an unsigned integer.

    :param c: string containing bytes to convert
    :param o: offset of bytes to convert in string
    """"""
    return unpack_from(""<I"", c, o)[0]


def si32le(c, o=0):
    """"""
    Converts a 4-bytes (32 bits) string to a signed integer.

    :param c: string containing bytes to convert
    :param o: offset of bytes to convert in string
    """"""
    return unpack_from(""<i"", c, o)[0]


def i16be(c, o=0):
    return unpack_from("">H"", c, o)[0]


def i32be(c, o=0):
    return unpack_from("">I"", c, o)[0]


# Output, le = little endian, be = big endian
def o16le(i):
    return pack(""<H"", i)


def o32le(i):
    return pack(""<I"", i)


def o16be(i):
    return pack("">H"", i)


def o32be(i):
    return pack("">I"", i)"
375	donghui	2	"#
# The Python Imaging Library.
# $Id$
#
# Binary input/output support routines.
#
# Copyright (c) 1997-2003 by Secret Labs AB
# Copyright (c) 1995-2003 by Fredrik Lundh
# Copyright (c) 2012 by Brian Crowell
#
# See the README file for information on usage and redistribution.
#


""""""Binary input/output support routines.""""""


from struct import pack, unpack_from


def i8(c):
    return c if c.__class__ is int else c[0]


def o8(i):
    return bytes((i & 255,))


# Input, le = little endian, be = big endian
def i16le(c, o=0):
    """"""
    Converts a 2-bytes (16 bits) string to an unsigned integer.

    :param c: string containing bytes to convert
    :param o: offset of bytes to convert in string
    """"""
    return unpack_from(""<H"", c, o)[0]


def si16le(c, o=0):
    """"""
    Converts a 2-bytes (16 bits) string to a signed integer.

    :param c: string containing bytes to convert
    :param o: offset of bytes to convert in string
    """"""
    return unpack_from(""<h"", c, o)[0]


def si16be(c, o=0):
    """"""
    Converts a 2-bytes (16 bits) string to a signed integer, big endian.

    :param c: string containing bytes to convert
    :param o: offset of bytes to convert in string
    """"""
    return unpack_from("">h"", c, o)[0]


def i32le(c, o=0):
    """"""
    Converts a 4-bytes (32 bits) string to an unsigned integer.

    :param c: string containing bytes to convert
    :param o: offset of bytes to convert in string
    """"""
    return unpack_from(""<I"", c, o)[0]


def si32le(c, o=0):
    """"""
    Converts a 4-bytes (32 bits) string to a signed integer.

    :param c: string containing bytes to convert
    :param o: offset of bytes to convert in string
    """"""
    return unpack_from(""<i"", c, o)[0]


def i16be(c, o=0):
    return unpack_from("">H"", c, o)[0]


def i32be(c, o=0):
    return unpack_from("">I"", c, o)[0]


# Output, le = little endian, be = big endian
def o16le(i):
    return pack(""<H"", i)


def o32le(i):
    return pack(""<I"", i)


def o16be(i):
    return pack("">H"", i)


def o32be(i):
    return pack("">I"", i)"
341	jackson	1	"from django.db import models
from django.contrib.auth.models import User
from io import BytesIO
import sys
print(sys.path)
from PIL import Image
from django.core.files.uploadedfile import InMemoryUploadedFile


# Create your models here.

class Blog_Post(models.Model, object):
    image = models.ImageField(blank= True, upload_to='get_upload_file_name')
    title = models.CharField(blank=True, max_length = 100)
    summary = models.TextField(blank= True, max_length =30)
    body = models.TextField(blank=True)
    slug = models.SlugField( unique=True)
    writer = models.ForeignKey(User,on_delete= models.CASCADE)
    created_on = models.DateTimeField(auto_now_add=True)

    def __str__(self) -> str:
        return self.title
    def save(self):
        # Opening the uploaded image
        im = Image.open(self.image)

        output = BytesIO()

        # Resize/modify the image
        im = im.resize((1024, 720))

        # after modifications, save it to the output
        im.save(output, format='png', quality=300)
        output.seek(0)

        # change the imagefield value to be the newley modifed image value
        self.image = InMemoryUploadedFile(output, 'ImageField', ""%s.webp"" % self.image.name.split('.')[0], 'image/webp',
                                        sys.getsizeof(output), None)

        super(Blog_Post, self).save()

class Comment(models.Model):
        commenter = models.CharField(max_length=15)
        body = models.TextField(max_length=30, blank=True)
        post = models.ForeignKey(Blog_Post, on_delete=models.CASCADE, related_name='comments')
        date = models.DateField(auto_now_add=True)
        like = models.BooleanField(default=True)
        def __str__(self) -> str:
             return self.commenter



class Meta:
    ordering = ('-created_at',)

   "
341	donghui	2	"from django.db import models
from django.contrib.auth.models import User
from io import BytesIO
import sys
print(sys.path)
from PIL import Image
from django.core.files.uploadedfile import InMemoryUploadedFile


# Create your models here.

class Blog_Post(models.Model, object):
    image = models.ImageField(blank= True, upload_to='get_upload_file_name')
    title = models.CharField(blank=True, max_length = 100)
    summary = models.TextField(blank= True, max_length =30)
    body = models.TextField(blank=True)
    slug = models.SlugField( unique=True)
    writer = models.ForeignKey(User,on_delete= models.CASCADE)
    created_on = models.DateTimeField(auto_now_add=True)

    def __str__(self) -> str:
        return self.title
    def save(self):
        # Opening the uploaded image
        im = Image.open(self.image)

        output = BytesIO()

        # Resize/modify the image
        im = im.resize((1024, 720))

        # after modifications, save it to the output
        im.save(output, format='png', quality=300)
        output.seek(0)

        # change the imagefield value to be the newley modifed image value
        self.image = InMemoryUploadedFile(output, 'ImageField', ""%s.webp"" % self.image.name.split('.')[0], 'image/webp',
                                        sys.getsizeof(output), None)

        super(Blog_Post, self).save()

class Comment(models.Model):
        commenter = models.CharField(max_length=15)
        body = models.TextField(max_length=30, blank=True)
        post = models.ForeignKey(Blog_Post, on_delete=models.CASCADE, related_name='comments')
        date = models.DateField(auto_now_add=True)
        like = models.BooleanField(default=True)
        def __str__(self) -> str:
             return self.commenter



class Meta:
    ordering = ('-created_at',)

   "
310	jackson	1	"from datetime import datetime

import pytest

from pandas._libs import tslib


@pytest.mark.parametrize(
    ""date_str, exp"",
    [
        (""2011-01-02"", datetime(2011, 1, 2)),
        (""2011-1-2"", datetime(2011, 1, 2)),
        (""2011-01"", datetime(2011, 1, 1)),
        (""2011-1"", datetime(2011, 1, 1)),
        (""2011 01 02"", datetime(2011, 1, 2)),
        (""2011.01.02"", datetime(2011, 1, 2)),
        (""2011/01/02"", datetime(2011, 1, 2)),
        (""2011\\01\\02"", datetime(2011, 1, 2)),
        (""2013-01-01 05:30:00"", datetime(2013, 1, 1, 5, 30)),
        (""2013-1-1 5:30:00"", datetime(2013, 1, 1, 5, 30)),
    ],
)
def test_parsers_iso8601(date_str, exp):
    # see gh-12060
    #
    # Test only the ISO parser - flexibility to
    # different separators and leading zero's.
    actual = tslib._test_parse_iso8601(date_str)
    assert actual == exp


@pytest.mark.parametrize(
    ""date_str"",
    [
        ""2011-01/02"",
        ""2011=11=11"",
        ""201401"",
        ""201111"",
        ""200101"",
        # Mixed separated and unseparated.
        ""2005-0101"",
        ""200501-01"",
        ""20010101 12:3456"",
        ""20010101 1234:56"",
        # HHMMSS must have two digits in
        # each component if unseparated.
        ""20010101 1"",
        ""20010101 123"",
        ""20010101 12345"",
        ""20010101 12345Z"",
    ],
)
def test_parsers_iso8601_invalid(date_str):
    msg = f'Error parsing datetime string ""{date_str}""'

    with pytest.raises(ValueError, match=msg):
        tslib._test_parse_iso8601(date_str)


def test_parsers_iso8601_invalid_offset_invalid():
    date_str = ""2001-01-01 12-34-56""
    msg = f'Timezone hours offset out of range in datetime string ""{date_str}""'

    with pytest.raises(ValueError, match=msg):
        tslib._test_parse_iso8601(date_str)


def test_parsers_iso8601_leading_space():
    # GH#25895 make sure isoparser doesn't overflow with long input
    date_str, expected = (""2013-1-1 5:30:00"", datetime(2013, 1, 1, 5, 30))
    actual = tslib._test_parse_iso8601("" "" * 200 + date_str)
    assert actual == expected"
310	donghui	1	"from datetime import datetime

import pytest

from pandas._libs import tslib


@pytest.mark.parametrize(
    ""date_str, exp"",
    [
        (""2011-01-02"", datetime(2011, 1, 2)),
        (""2011-1-2"", datetime(2011, 1, 2)),
        (""2011-01"", datetime(2011, 1, 1)),
        (""2011-1"", datetime(2011, 1, 1)),
        (""2011 01 02"", datetime(2011, 1, 2)),
        (""2011.01.02"", datetime(2011, 1, 2)),
        (""2011/01/02"", datetime(2011, 1, 2)),
        (""2011\\01\\02"", datetime(2011, 1, 2)),
        (""2013-01-01 05:30:00"", datetime(2013, 1, 1, 5, 30)),
        (""2013-1-1 5:30:00"", datetime(2013, 1, 1, 5, 30)),
    ],
)
def test_parsers_iso8601(date_str, exp):
    # see gh-12060
    #
    # Test only the ISO parser - flexibility to
    # different separators and leading zero's.
    actual = tslib._test_parse_iso8601(date_str)
    assert actual == exp


@pytest.mark.parametrize(
    ""date_str"",
    [
        ""2011-01/02"",
        ""2011=11=11"",
        ""201401"",
        ""201111"",
        ""200101"",
        # Mixed separated and unseparated.
        ""2005-0101"",
        ""200501-01"",
        ""20010101 12:3456"",
        ""20010101 1234:56"",
        # HHMMSS must have two digits in
        # each component if unseparated.
        ""20010101 1"",
        ""20010101 123"",
        ""20010101 12345"",
        ""20010101 12345Z"",
    ],
)
def test_parsers_iso8601_invalid(date_str):
    msg = f'Error parsing datetime string ""{date_str}""'

    with pytest.raises(ValueError, match=msg):
        tslib._test_parse_iso8601(date_str)


def test_parsers_iso8601_invalid_offset_invalid():
    date_str = ""2001-01-01 12-34-56""
    msg = f'Timezone hours offset out of range in datetime string ""{date_str}""'

    with pytest.raises(ValueError, match=msg):
        tslib._test_parse_iso8601(date_str)


def test_parsers_iso8601_leading_space():
    # GH#25895 make sure isoparser doesn't overflow with long input
    date_str, expected = (""2013-1-1 5:30:00"", datetime(2013, 1, 1, 5, 30))
    actual = tslib._test_parse_iso8601("" "" * 200 + date_str)
    assert actual == expected"
471	jackson	3	"""""""Metadata generation logic for legacy source distributions.
""""""

import logging
import os

from pip._internal.build_env import BuildEnvironment
from pip._internal.exceptions import InstallationError
from pip._internal.utils.setuptools_build import make_setuptools_egg_info_args
from pip._internal.utils.subprocess import call_subprocess
from pip._internal.utils.temp_dir import TempDirectory

logger = logging.getLogger(__name__)


def _find_egg_info(directory):
    # type: (str) -> str
    """"""Find an .egg-info subdirectory in `directory`.
    """"""
    filenames = [
        f for f in os.listdir(directory) if f.endswith("".egg-info"")
    ]

    if not filenames:
        raise InstallationError(
            f""No .egg-info directory found in {directory}""
        )

    if len(filenames) > 1:
        raise InstallationError(
            ""More than one .egg-info directory found in {}"".format(
                directory
            )
        )

    return os.path.join(directory, filenames[0])


def generate_metadata(
    build_env,  # type: BuildEnvironment
    setup_py_path,  # type: str
    source_dir,  # type: str
    isolated,  # type: bool
    details,  # type: str
):
    # type: (...) -> str
    """"""Generate metadata using setup.py-based defacto mechanisms.

    Returns the generated metadata directory.
    """"""
    logger.debug(
        'Running setup.py (path:%s) egg_info for package %s',
        setup_py_path, details,
    )

    egg_info_dir = TempDirectory(
        kind=""pip-egg-info"", globally_managed=True
    ).path

    args = make_setuptools_egg_info_args(
        setup_py_path,
        egg_info_dir=egg_info_dir,
        no_user_config=isolated,
    )

    with build_env:
        call_subprocess(
            args,
            cwd=source_dir,
            command_desc='python setup.py egg_info',
        )

    # Return the .egg-info directory.
    return _find_egg_info(egg_info_dir)"
471	donghui	2	"""""""Metadata generation logic for legacy source distributions.
""""""

import logging
import os

from pip._internal.build_env import BuildEnvironment
from pip._internal.exceptions import InstallationError
from pip._internal.utils.setuptools_build import make_setuptools_egg_info_args
from pip._internal.utils.subprocess import call_subprocess
from pip._internal.utils.temp_dir import TempDirectory

logger = logging.getLogger(__name__)


def _find_egg_info(directory):
    # type: (str) -> str
    """"""Find an .egg-info subdirectory in `directory`.
    """"""
    filenames = [
        f for f in os.listdir(directory) if f.endswith("".egg-info"")
    ]

    if not filenames:
        raise InstallationError(
            f""No .egg-info directory found in {directory}""
        )

    if len(filenames) > 1:
        raise InstallationError(
            ""More than one .egg-info directory found in {}"".format(
                directory
            )
        )

    return os.path.join(directory, filenames[0])


def generate_metadata(
    build_env,  # type: BuildEnvironment
    setup_py_path,  # type: str
    source_dir,  # type: str
    isolated,  # type: bool
    details,  # type: str
):
    # type: (...) -> str
    """"""Generate metadata using setup.py-based defacto mechanisms.

    Returns the generated metadata directory.
    """"""
    logger.debug(
        'Running setup.py (path:%s) egg_info for package %s',
        setup_py_path, details,
    )

    egg_info_dir = TempDirectory(
        kind=""pip-egg-info"", globally_managed=True
    ).path

    args = make_setuptools_egg_info_args(
        setup_py_path,
        egg_info_dir=egg_info_dir,
        no_user_config=isolated,
    )

    with build_env:
        call_subprocess(
            args,
            cwd=source_dir,
            command_desc='python setup.py egg_info',
        )

    # Return the .egg-info directory.
    return _find_egg_info(egg_info_dir)"
420	jackson	1	"from distutils.util import convert_path
from distutils import log
from distutils.errors import DistutilsOptionError
import os
import shutil

from setuptools import Command


class rotate(Command):
    """"""Delete older distributions""""""

    description = ""delete older distributions, keeping N newest files""
    user_options = [
        ('match=', 'm', ""patterns to match (required)""),
        ('dist-dir=', 'd', ""directory where the distributions are""),
        ('keep=', 'k', ""number of matching distributions to keep""),
    ]

    boolean_options = []

    def initialize_options(self):
        self.match = None
        self.dist_dir = None
        self.keep = None

    def finalize_options(self):
        if self.match is None:
            raise DistutilsOptionError(
                ""Must specify one or more (comma-separated) match patterns ""
                ""(e.g. '.zip' or '.egg')""
            )
        if self.keep is None:
            raise DistutilsOptionError(""Must specify number of files to keep"")
        try:
            self.keep = int(self.keep)
        except ValueError as e:
            raise DistutilsOptionError(""--keep must be an integer"") from e
        if isinstance(self.match, str):
            self.match = [
                convert_path(p.strip()) for p in self.match.split(',')
            ]
        self.set_undefined_options('bdist', ('dist_dir', 'dist_dir'))

    def run(self):
        self.run_command(""egg_info"")
        from glob import glob

        for pattern in self.match:
            pattern = self.distribution.get_name() + '*' + pattern
            files = glob(os.path.join(self.dist_dir, pattern))
            files = [(os.path.getmtime(f), f) for f in files]
            files.sort()
            files.reverse()

            log.info(""%d file(s) matching %s"", len(files), pattern)
            files = files[self.keep:]
            for (t, f) in files:
                log.info(""Deleting %s"", f)
                if not self.dry_run:
                    if os.path.isdir(f):
                        shutil.rmtree(f)
                    else:
                        os.unlink(f)"
420	donghui	1	"from distutils.util import convert_path
from distutils import log
from distutils.errors import DistutilsOptionError
import os
import shutil

from setuptools import Command


class rotate(Command):
    """"""Delete older distributions""""""

    description = ""delete older distributions, keeping N newest files""
    user_options = [
        ('match=', 'm', ""patterns to match (required)""),
        ('dist-dir=', 'd', ""directory where the distributions are""),
        ('keep=', 'k', ""number of matching distributions to keep""),
    ]

    boolean_options = []

    def initialize_options(self):
        self.match = None
        self.dist_dir = None
        self.keep = None

    def finalize_options(self):
        if self.match is None:
            raise DistutilsOptionError(
                ""Must specify one or more (comma-separated) match patterns ""
                ""(e.g. '.zip' or '.egg')""
            )
        if self.keep is None:
            raise DistutilsOptionError(""Must specify number of files to keep"")
        try:
            self.keep = int(self.keep)
        except ValueError as e:
            raise DistutilsOptionError(""--keep must be an integer"") from e
        if isinstance(self.match, str):
            self.match = [
                convert_path(p.strip()) for p in self.match.split(',')
            ]
        self.set_undefined_options('bdist', ('dist_dir', 'dist_dir'))

    def run(self):
        self.run_command(""egg_info"")
        from glob import glob

        for pattern in self.match:
            pattern = self.distribution.get_name() + '*' + pattern
            files = glob(os.path.join(self.dist_dir, pattern))
            files = [(os.path.getmtime(f), f) for f in files]
            files.sort()
            files.reverse()

            log.info(""%d file(s) matching %s"", len(files), pattern)
            files = files[self.keep:]
            for (t, f) in files:
                log.info(""Deleting %s"", f)
                if not self.dry_run:
                    if os.path.isdir(f):
                        shutil.rmtree(f)
                    else:
                        os.unlink(f)"
482	jackson	3	"from urllib.parse import unquote, urlparse

from asgiref.testing import ApplicationCommunicator


class HttpCommunicator(ApplicationCommunicator):
    """"""
    ApplicationCommunicator subclass that has HTTP shortcut methods.

    It will construct the scope for you, so you need to pass the application
    (uninstantiated) along with HTTP parameters.

    This does not support full chunking - for that, just use ApplicationCommunicator
    directly.
    """"""

    def __init__(self, application, method, path, body=b"""", headers=None):
        parsed = urlparse(path)
        self.scope = {
            ""type"": ""http"",
            ""http_version"": ""1.1"",
            ""method"": method.upper(),
            ""path"": unquote(parsed.path),
            ""query_string"": parsed.query.encode(""utf-8""),
            ""headers"": headers or [],
        }
        assert isinstance(body, bytes)
        self.body = body
        self.sent_request = False
        super().__init__(application, self.scope)

    async def get_response(self, timeout=1):
        """"""
        Get the application's response. Returns a dict with keys of
        ""body"", ""headers"" and ""status"".
        """"""
        # If we've not sent the request yet, do so
        if not self.sent_request:
            self.sent_request = True
            await self.send_input({""type"": ""http.request"", ""body"": self.body})
        # Get the response start
        response_start = await self.receive_output(timeout)
        assert response_start[""type""] == ""http.response.start""
        # Get all body parts
        response_start[""body""] = b""""
        while True:
            chunk = await self.receive_output(timeout)
            assert chunk[""type""] == ""http.response.body""
            assert isinstance(chunk[""body""], bytes)
            response_start[""body""] += chunk[""body""]
            if not chunk.get(""more_body"", False):
                break
        # Return structured info
        del response_start[""type""]
        response_start.setdefault(""headers"", [])
        return response_start"
482	donghui	3	"from urllib.parse import unquote, urlparse

from asgiref.testing import ApplicationCommunicator


class HttpCommunicator(ApplicationCommunicator):
    """"""
    ApplicationCommunicator subclass that has HTTP shortcut methods.

    It will construct the scope for you, so you need to pass the application
    (uninstantiated) along with HTTP parameters.

    This does not support full chunking - for that, just use ApplicationCommunicator
    directly.
    """"""

    def __init__(self, application, method, path, body=b"""", headers=None):
        parsed = urlparse(path)
        self.scope = {
            ""type"": ""http"",
            ""http_version"": ""1.1"",
            ""method"": method.upper(),
            ""path"": unquote(parsed.path),
            ""query_string"": parsed.query.encode(""utf-8""),
            ""headers"": headers or [],
        }
        assert isinstance(body, bytes)
        self.body = body
        self.sent_request = False
        super().__init__(application, self.scope)

    async def get_response(self, timeout=1):
        """"""
        Get the application's response. Returns a dict with keys of
        ""body"", ""headers"" and ""status"".
        """"""
        # If we've not sent the request yet, do so
        if not self.sent_request:
            self.sent_request = True
            await self.send_input({""type"": ""http.request"", ""body"": self.body})
        # Get the response start
        response_start = await self.receive_output(timeout)
        assert response_start[""type""] == ""http.response.start""
        # Get all body parts
        response_start[""body""] = b""""
        while True:
            chunk = await self.receive_output(timeout)
            assert chunk[""type""] == ""http.response.body""
            assert isinstance(chunk[""body""], bytes)
            response_start[""body""] += chunk[""body""]
            if not chunk.get(""more_body"", False):
                break
        # Return structured info
        del response_start[""type""]
        response_start.setdefault(""headers"", [])
        return response_start"
414	jackson	3	"# -*- coding: utf-8 -*-
#
# Copyright (C) 2019 Radim Rehurek <me@radimrehurek.com>
#
# This code is distributed under the terms and conditions
# from the MIT License (MIT).
#

""""""
Utilities for streaming to/from several file-like data storages: S3 / HDFS / local
filesystem / compressed files, and many more, using a simple, Pythonic API.

The streaming makes heavy use of generators and pipes, to avoid loading
full file contents into memory, allowing work with arbitrarily large files.

The main functions are:

* `open()`, which opens the given file for reading/writing
* `parse_uri()`
* `s3_iter_bucket()`, which goes over all keys in an S3 bucket in parallel
* `register_compressor()`, which registers callbacks for transparent compressor handling

""""""

import logging

#
# Prevent regression of #474 and #475
#
logger = logging.getLogger(__name__)
logger.addHandler(logging.NullHandler())

from smart_open import version  # noqa: E402
from .smart_open_lib import open, parse_uri, smart_open, register_compressor  # noqa: E402

_WARNING = """"""smart_open.s3_iter_bucket is deprecated and will stop functioning
in a future version. Please import iter_bucket from the smart_open.s3 module instead:

    from smart_open.s3 import iter_bucket as s3_iter_bucket

""""""
_WARNED = False


def s3_iter_bucket(
        bucket_name,
        prefix='',
        accept_key=None,
        key_limit=None,
        workers=16,
        retries=3,
        **session_kwargs
):
    """"""Deprecated.  Use smart_open.s3.iter_bucket instead.""""""
    global _WARNED
    from .s3 import iter_bucket
    if not _WARNED:
        logger.warning(_WARNING)
        _WARNED = True
    return iter_bucket(
        bucket_name=bucket_name,
        prefix=prefix,
        accept_key=accept_key,
        key_limit=key_limit,
        workers=workers,
        retries=retries,
        session_kwargs=session_kwargs
    )


__all__ = [
    'open',
    'parse_uri',
    'register_compressor',
    's3_iter_bucket',
    'smart_open',
]

__version__ = version.__version__"
414	donghui	2	"# -*- coding: utf-8 -*-
#
# Copyright (C) 2019 Radim Rehurek <me@radimrehurek.com>
#
# This code is distributed under the terms and conditions
# from the MIT License (MIT).
#

""""""
Utilities for streaming to/from several file-like data storages: S3 / HDFS / local
filesystem / compressed files, and many more, using a simple, Pythonic API.

The streaming makes heavy use of generators and pipes, to avoid loading
full file contents into memory, allowing work with arbitrarily large files.

The main functions are:

* `open()`, which opens the given file for reading/writing
* `parse_uri()`
* `s3_iter_bucket()`, which goes over all keys in an S3 bucket in parallel
* `register_compressor()`, which registers callbacks for transparent compressor handling

""""""

import logging

#
# Prevent regression of #474 and #475
#
logger = logging.getLogger(__name__)
logger.addHandler(logging.NullHandler())

from smart_open import version  # noqa: E402
from .smart_open_lib import open, parse_uri, smart_open, register_compressor  # noqa: E402

_WARNING = """"""smart_open.s3_iter_bucket is deprecated and will stop functioning
in a future version. Please import iter_bucket from the smart_open.s3 module instead:

    from smart_open.s3 import iter_bucket as s3_iter_bucket

""""""
_WARNED = False


def s3_iter_bucket(
        bucket_name,
        prefix='',
        accept_key=None,
        key_limit=None,
        workers=16,
        retries=3,
        **session_kwargs
):
    """"""Deprecated.  Use smart_open.s3.iter_bucket instead.""""""
    global _WARNED
    from .s3 import iter_bucket
    if not _WARNED:
        logger.warning(_WARNING)
        _WARNED = True
    return iter_bucket(
        bucket_name=bucket_name,
        prefix=prefix,
        accept_key=accept_key,
        key_limit=key_limit,
        workers=workers,
        retries=retries,
        session_kwargs=session_kwargs
    )


__all__ = [
    'open',
    'parse_uri',
    'register_compressor',
    's3_iter_bucket',
    'smart_open',
]

__version__ = version.__version__"
505	jackson	3	"""""""
    pygments.styles.vim
    ~~~~~~~~~~~~~~~~~~~

    A highlighting style for Pygments, inspired by vim.

    :copyright: Copyright 2006-2022 by the Pygments team, see AUTHORS.
    :license: BSD, see LICENSE for details.
""""""

from pygments.style import Style
from pygments.token import Keyword, Name, Comment, String, Error, \
     Number, Operator, Generic, Whitespace, Token


class VimStyle(Style):
    """"""
    Styles somewhat like vim 7.0
    """"""

    background_color = ""#000000""
    highlight_color = ""#222222""

    styles = {
        Token:                     ""#cccccc"",
        Whitespace:                """",
        Comment:                   ""#000080"",
        Comment.Preproc:           """",
        Comment.Special:           ""bold #cd0000"",

        Keyword:                   ""#cdcd00"",
        Keyword.Declaration:       ""#00cd00"",
        Keyword.Namespace:         ""#cd00cd"",
        Keyword.Pseudo:            """",
        Keyword.Type:              ""#00cd00"",

        Operator:                  ""#3399cc"",
        Operator.Word:             ""#cdcd00"",

        Name:                      """",
        Name.Class:                ""#00cdcd"",
        Name.Builtin:              ""#cd00cd"",
        Name.Exception:            ""bold #666699"",
        Name.Variable:             ""#00cdcd"",

        String:                    ""#cd0000"",
        Number:                    ""#cd00cd"",

        Generic.Heading:           ""bold #000080"",
        Generic.Subheading:        ""bold #800080"",
        Generic.Deleted:           ""#cd0000"",
        Generic.Inserted:          ""#00cd00"",
        Generic.Error:             ""#FF0000"",
        Generic.Emph:              ""italic"",
        Generic.Strong:            ""bold"",
        Generic.Prompt:            ""bold #000080"",
        Generic.Output:            ""#888"",
        Generic.Traceback:         ""#04D"",

        Error:                     ""border:#FF0000""
    }"
505	donghui	1	"""""""
    pygments.styles.vim
    ~~~~~~~~~~~~~~~~~~~

    A highlighting style for Pygments, inspired by vim.

    :copyright: Copyright 2006-2022 by the Pygments team, see AUTHORS.
    :license: BSD, see LICENSE for details.
""""""

from pygments.style import Style
from pygments.token import Keyword, Name, Comment, String, Error, \
     Number, Operator, Generic, Whitespace, Token


class VimStyle(Style):
    """"""
    Styles somewhat like vim 7.0
    """"""

    background_color = ""#000000""
    highlight_color = ""#222222""

    styles = {
        Token:                     ""#cccccc"",
        Whitespace:                """",
        Comment:                   ""#000080"",
        Comment.Preproc:           """",
        Comment.Special:           ""bold #cd0000"",

        Keyword:                   ""#cdcd00"",
        Keyword.Declaration:       ""#00cd00"",
        Keyword.Namespace:         ""#cd00cd"",
        Keyword.Pseudo:            """",
        Keyword.Type:              ""#00cd00"",

        Operator:                  ""#3399cc"",
        Operator.Word:             ""#cdcd00"",

        Name:                      """",
        Name.Class:                ""#00cdcd"",
        Name.Builtin:              ""#cd00cd"",
        Name.Exception:            ""bold #666699"",
        Name.Variable:             ""#00cdcd"",

        String:                    ""#cd0000"",
        Number:                    ""#cd00cd"",

        Generic.Heading:           ""bold #000080"",
        Generic.Subheading:        ""bold #800080"",
        Generic.Deleted:           ""#cd0000"",
        Generic.Inserted:          ""#00cd00"",
        Generic.Error:             ""#FF0000"",
        Generic.Emph:              ""italic"",
        Generic.Strong:            ""bold"",
        Generic.Prompt:            ""bold #000080"",
        Generic.Output:            ""#888"",
        Generic.Traceback:         ""#04D"",

        Error:                     ""border:#FF0000""
    }"
445	jackson	0	"from rest_framework.views import APIView
from rest_framework.response import Response
from rest_framework.exceptions import AuthenticationFailed
from rest_framework.parsers import JSONParser
from .serializers import UserSerializer
from .models import User
import jwt
import datetime
from jwt import decode
from bson.objectid import ObjectId

import sys

from helpers.permissions import isUser

class RegisterView(APIView):
    def post(self, request):
        serializer = UserSerializer(data=request.data)
        serializer.is_valid(raise_exception=True)
        serializer.save()
        return Response(serializer.data)


class LoginView(APIView):
    def post(self, request):
        email = request.data['email']
        password = request.data['password']

        user = User.objects.filter(email__exact=email).first()

        if user is None:
            raise AuthenticationFailed('User not found!')

        if not user.check_password(password):
            raise AuthenticationFailed('Incorrect password!')

        payload = {
            'id': str(user._id),
            'admin': user.is_superAdmin,
            'exp': datetime.datetime.utcnow() + datetime.timedelta(minutes=60),
            'iat': datetime.datetime.utcnow()
        }
        

        token = jwt.encode(payload, 'secret',
                           algorithm='HS256').decode('utf-8')

        response = Response()

        response.set_cookie(key='jwt', value=token, httponly=True)
        response.data = {
            'jwt': token
        }

        return response


class UserView(APIView):
    
    permission_classes = [isUser]

    def get(self, request):
        user = User.objects.filter(_id=ObjectId(request.account['id'])).first()
        serializer = UserSerializer(user)
        
        return Response(serializer.data)



class LogoutView(APIView):
    def post(self, request):
        response = Response()
        response.delete_cookie('jwt')
        response.data = {
            'message': 'success'
        }
        return response"
445	donghui	0	"from rest_framework.views import APIView
from rest_framework.response import Response
from rest_framework.exceptions import AuthenticationFailed
from rest_framework.parsers import JSONParser
from .serializers import UserSerializer
from .models import User
import jwt
import datetime
from jwt import decode
from bson.objectid import ObjectId

import sys

from helpers.permissions import isUser

class RegisterView(APIView):
    def post(self, request):
        serializer = UserSerializer(data=request.data)
        serializer.is_valid(raise_exception=True)
        serializer.save()
        return Response(serializer.data)


class LoginView(APIView):
    def post(self, request):
        email = request.data['email']
        password = request.data['password']

        user = User.objects.filter(email__exact=email).first()

        if user is None:
            raise AuthenticationFailed('User not found!')

        if not user.check_password(password):
            raise AuthenticationFailed('Incorrect password!')

        payload = {
            'id': str(user._id),
            'admin': user.is_superAdmin,
            'exp': datetime.datetime.utcnow() + datetime.timedelta(minutes=60),
            'iat': datetime.datetime.utcnow()
        }
        

        token = jwt.encode(payload, 'secret',
                           algorithm='HS256').decode('utf-8')

        response = Response()

        response.set_cookie(key='jwt', value=token, httponly=True)
        response.data = {
            'jwt': token
        }

        return response


class UserView(APIView):
    
    permission_classes = [isUser]

    def get(self, request):
        user = User.objects.filter(_id=ObjectId(request.account['id'])).first()
        serializer = UserSerializer(user)
        
        return Response(serializer.data)



class LogoutView(APIView):
    def post(self, request):
        response = Response()
        response.delete_cookie('jwt')
        response.data = {
            'message': 'success'
        }
        return response"
455	jackson	3	"# Copyright 2016 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from flask import Flask
import requests

import services_config

app = Flask(__name__)
services_config.init_app(app)


@app.route('/')
def root():
    """"""Gets index.html from the static file server""""""
    res = requests.get(app.config['SERVICE_MAP']['static'])
    return res.content


@app.route('/hello/<service>')
def say_hello(service):
    """"""Recieves requests from buttons on the front end and resopnds
    or sends request to the static file server""""""
    # If 'gateway' is specified return immediate
    if service == 'gateway':
        return 'Gateway says hello'

    # Otherwise send request to service indicated by URL param
    responses = []
    url = app.config['SERVICE_MAP'][service]
    res = requests.get(url + '/hello')
    responses.append(res.content)
    return '\n'.encode().join(responses)


@app.route('/<path>')
def static_file(path):
    """"""Gets static files required by index.html to static file server""""""
    url = app.config['SERVICE_MAP']['static']
    res = requests.get(url + '/' + path)
    return res.content, 200, {'Content-Type': res.headers['Content-Type']}


if __name__ == '__main__':
    # This is used when running locally. Gunicorn is used to run the
    # application on Google App Engine. See entrypoint in app.yaml.
    app.run(host='127.0.0.1', port=8000, debug=True)"
455	donghui	2	"# Copyright 2016 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from flask import Flask
import requests

import services_config

app = Flask(__name__)
services_config.init_app(app)


@app.route('/')
def root():
    """"""Gets index.html from the static file server""""""
    res = requests.get(app.config['SERVICE_MAP']['static'])
    return res.content


@app.route('/hello/<service>')
def say_hello(service):
    """"""Recieves requests from buttons on the front end and resopnds
    or sends request to the static file server""""""
    # If 'gateway' is specified return immediate
    if service == 'gateway':
        return 'Gateway says hello'

    # Otherwise send request to service indicated by URL param
    responses = []
    url = app.config['SERVICE_MAP'][service]
    res = requests.get(url + '/hello')
    responses.append(res.content)
    return '\n'.encode().join(responses)


@app.route('/<path>')
def static_file(path):
    """"""Gets static files required by index.html to static file server""""""
    url = app.config['SERVICE_MAP']['static']
    res = requests.get(url + '/' + path)
    return res.content, 200, {'Content-Type': res.headers['Content-Type']}


if __name__ == '__main__':
    # This is used when running locally. Gunicorn is used to run the
    # application on Google App Engine. See entrypoint in app.yaml.
    app.run(host='127.0.0.1', port=8000, debug=True)"
515	jackson	0	"import sys
from typing import TYPE_CHECKING

if sys.version_info < (3, 7) or TYPE_CHECKING:
    from ._visible import VisibleValidator
    from ._type import TypeValidator
    from ._templateitemname import TemplateitemnameValidator
    from ._symbol import SymbolValidator
    from ._sourcetype import SourcetypeValidator
    from ._sourcelayer import SourcelayerValidator
    from ._sourceattribution import SourceattributionValidator
    from ._source import SourceValidator
    from ._opacity import OpacityValidator
    from ._name import NameValidator
    from ._minzoom import MinzoomValidator
    from ._maxzoom import MaxzoomValidator
    from ._line import LineValidator
    from ._fill import FillValidator
    from ._coordinates import CoordinatesValidator
    from ._color import ColorValidator
    from ._circle import CircleValidator
    from ._below import BelowValidator
else:
    from _plotly_utils.importers import relative_import

    __all__, __getattr__, __dir__ = relative_import(
        __name__,
        [],
        [
            ""._visible.VisibleValidator"",
            ""._type.TypeValidator"",
            ""._templateitemname.TemplateitemnameValidator"",
            ""._symbol.SymbolValidator"",
            ""._sourcetype.SourcetypeValidator"",
            ""._sourcelayer.SourcelayerValidator"",
            ""._sourceattribution.SourceattributionValidator"",
            ""._source.SourceValidator"",
            ""._opacity.OpacityValidator"",
            ""._name.NameValidator"",
            ""._minzoom.MinzoomValidator"",
            ""._maxzoom.MaxzoomValidator"",
            ""._line.LineValidator"",
            ""._fill.FillValidator"",
            ""._coordinates.CoordinatesValidator"",
            ""._color.ColorValidator"",
            ""._circle.CircleValidator"",
            ""._below.BelowValidator"",
        ],
    )"
515	donghui	0	"import sys
from typing import TYPE_CHECKING

if sys.version_info < (3, 7) or TYPE_CHECKING:
    from ._visible import VisibleValidator
    from ._type import TypeValidator
    from ._templateitemname import TemplateitemnameValidator
    from ._symbol import SymbolValidator
    from ._sourcetype import SourcetypeValidator
    from ._sourcelayer import SourcelayerValidator
    from ._sourceattribution import SourceattributionValidator
    from ._source import SourceValidator
    from ._opacity import OpacityValidator
    from ._name import NameValidator
    from ._minzoom import MinzoomValidator
    from ._maxzoom import MaxzoomValidator
    from ._line import LineValidator
    from ._fill import FillValidator
    from ._coordinates import CoordinatesValidator
    from ._color import ColorValidator
    from ._circle import CircleValidator
    from ._below import BelowValidator
else:
    from _plotly_utils.importers import relative_import

    __all__, __getattr__, __dir__ = relative_import(
        __name__,
        [],
        [
            ""._visible.VisibleValidator"",
            ""._type.TypeValidator"",
            ""._templateitemname.TemplateitemnameValidator"",
            ""._symbol.SymbolValidator"",
            ""._sourcetype.SourcetypeValidator"",
            ""._sourcelayer.SourcelayerValidator"",
            ""._sourceattribution.SourceattributionValidator"",
            ""._source.SourceValidator"",
            ""._opacity.OpacityValidator"",
            ""._name.NameValidator"",
            ""._minzoom.MinzoomValidator"",
            ""._maxzoom.MaxzoomValidator"",
            ""._line.LineValidator"",
            ""._fill.FillValidator"",
            ""._coordinates.CoordinatesValidator"",
            ""._color.ColorValidator"",
            ""._circle.CircleValidator"",
            ""._below.BelowValidator"",
        ],
    )"
404	jackson	3	"#
# The Python Imaging Library.
# $Id$
#
# sequence support classes
#
# history:
# 1997-02-20 fl     Created
#
# Copyright (c) 1997 by Secret Labs AB.
# Copyright (c) 1997 by Fredrik Lundh.
#
# See the README file for information on usage and redistribution.
#

##


class Iterator:
    """"""
    This class implements an iterator object that can be used to loop
    over an image sequence.

    You can use the ``[]`` operator to access elements by index. This operator
    will raise an :py:exc:`IndexError` if you try to access a nonexistent
    frame.

    :param im: An image object.
    """"""

    def __init__(self, im):
        if not hasattr(im, ""seek""):
            msg = ""im must have seek method""
            raise AttributeError(msg)
        self.im = im
        self.position = getattr(self.im, ""_min_frame"", 0)

    def __getitem__(self, ix):
        try:
            self.im.seek(ix)
            return self.im
        except EOFError as e:
            raise IndexError from e  # end of sequence

    def __iter__(self):
        return self

    def __next__(self):
        try:
            self.im.seek(self.position)
            self.position += 1
            return self.im
        except EOFError as e:
            raise StopIteration from e


def all_frames(im, func=None):
    """"""
    Applies a given function to all frames in an image or a list of images.
    The frames are returned as a list of separate images.

    :param im: An image, or a list of images.
    :param func: The function to apply to all of the image frames.
    :returns: A list of images.
    """"""
    if not isinstance(im, list):
        im = [im]

    ims = []
    for imSequence in im:
        current = imSequence.tell()

        ims += [im_frame.copy() for im_frame in Iterator(imSequence)]

        imSequence.seek(current)
    return [func(im) for im in ims] if func else ims"
404	donghui	3	"#
# The Python Imaging Library.
# $Id$
#
# sequence support classes
#
# history:
# 1997-02-20 fl     Created
#
# Copyright (c) 1997 by Secret Labs AB.
# Copyright (c) 1997 by Fredrik Lundh.
#
# See the README file for information on usage and redistribution.
#

##


class Iterator:
    """"""
    This class implements an iterator object that can be used to loop
    over an image sequence.

    You can use the ``[]`` operator to access elements by index. This operator
    will raise an :py:exc:`IndexError` if you try to access a nonexistent
    frame.

    :param im: An image object.
    """"""

    def __init__(self, im):
        if not hasattr(im, ""seek""):
            msg = ""im must have seek method""
            raise AttributeError(msg)
        self.im = im
        self.position = getattr(self.im, ""_min_frame"", 0)

    def __getitem__(self, ix):
        try:
            self.im.seek(ix)
            return self.im
        except EOFError as e:
            raise IndexError from e  # end of sequence

    def __iter__(self):
        return self

    def __next__(self):
        try:
            self.im.seek(self.position)
            self.position += 1
            return self.im
        except EOFError as e:
            raise StopIteration from e


def all_frames(im, func=None):
    """"""
    Applies a given function to all frames in an image or a list of images.
    The frames are returned as a list of separate images.

    :param im: An image, or a list of images.
    :param func: The function to apply to all of the image frames.
    :returns: A list of images.
    """"""
    if not isinstance(im, list):
        im = [im]

    ims = []
    for imSequence in im:
        current = imSequence.tell()

        ims += [im_frame.copy() for im_frame in Iterator(imSequence)]

        imSequence.seek(current)
    return [func(im) for im in ims] if func else ims"
492	jackson	1	"#
# download_mks_assets.py
# Added by HAS_TFT_LVGL_UI to download assets from Makerbase repo
#
import pioutil
if pioutil.is_pio_build():
    Import(""env"")
    import requests,zipfile,tempfile,shutil
    from pathlib import Path

    url = ""https://github.com/makerbase-mks/Mks-Robin-Nano-Marlin2.0-Firmware/archive/0263cdaccf.zip""
    deps_path = Path(env.Dictionary(""PROJECT_LIBDEPS_DIR""))
    zip_path = deps_path / ""mks-assets.zip""
    assets_path = Path(env.Dictionary(""PROJECT_BUILD_DIR""), env.Dictionary(""PIOENV""), ""assets"")

    def download_mks_assets():
        print(""Downloading MKS Assets"")
        r = requests.get(url, stream=True)
        # the user may have a very clean workspace,
        # so create the PROJECT_LIBDEPS_DIR directory if not exits
        if not deps_path.exists():
            deps_path.mkdir()
        with zip_path.open('wb') as fd:
            for chunk in r.iter_content(chunk_size=128):
                fd.write(chunk)

    def copy_mks_assets():
        print(""Copying MKS Assets"")
        output_path = Path(tempfile.mkdtemp())
        zip_obj = zipfile.ZipFile(zip_path, 'r')
        zip_obj.extractall(output_path)
        zip_obj.close()
        if assets_path.exists() and not assets_path.is_dir():
            assets_path.unlink()
        if not assets_path.exists():
            assets_path.mkdir()
        base_path = ''
        for filename in output_path.iterdir():
            base_path = filename
        fw_path = (output_path / base_path / 'Firmware')
        font_path = fw_path / 'mks_font'
        for filename in font_path.iterdir():
            shutil.copy(font_path / filename, assets_path)
        pic_path = fw_path / 'mks_pic'
        for filename in pic_path.iterdir():
            shutil.copy(pic_path / filename, assets_path)
        shutil.rmtree(output_path, ignore_errors=True)

    if not zip_path.exists():
        download_mks_assets()

    if not assets_path.exists():
        copy_mks_assets()"
492	donghui	1	"#
# download_mks_assets.py
# Added by HAS_TFT_LVGL_UI to download assets from Makerbase repo
#
import pioutil
if pioutil.is_pio_build():
    Import(""env"")
    import requests,zipfile,tempfile,shutil
    from pathlib import Path

    url = ""https://github.com/makerbase-mks/Mks-Robin-Nano-Marlin2.0-Firmware/archive/0263cdaccf.zip""
    deps_path = Path(env.Dictionary(""PROJECT_LIBDEPS_DIR""))
    zip_path = deps_path / ""mks-assets.zip""
    assets_path = Path(env.Dictionary(""PROJECT_BUILD_DIR""), env.Dictionary(""PIOENV""), ""assets"")

    def download_mks_assets():
        print(""Downloading MKS Assets"")
        r = requests.get(url, stream=True)
        # the user may have a very clean workspace,
        # so create the PROJECT_LIBDEPS_DIR directory if not exits
        if not deps_path.exists():
            deps_path.mkdir()
        with zip_path.open('wb') as fd:
            for chunk in r.iter_content(chunk_size=128):
                fd.write(chunk)

    def copy_mks_assets():
        print(""Copying MKS Assets"")
        output_path = Path(tempfile.mkdtemp())
        zip_obj = zipfile.ZipFile(zip_path, 'r')
        zip_obj.extractall(output_path)
        zip_obj.close()
        if assets_path.exists() and not assets_path.is_dir():
            assets_path.unlink()
        if not assets_path.exists():
            assets_path.mkdir()
        base_path = ''
        for filename in output_path.iterdir():
            base_path = filename
        fw_path = (output_path / base_path / 'Firmware')
        font_path = fw_path / 'mks_font'
        for filename in font_path.iterdir():
            shutil.copy(font_path / filename, assets_path)
        pic_path = fw_path / 'mks_pic'
        for filename in pic_path.iterdir():
            shutil.copy(pic_path / filename, assets_path)
        shutil.rmtree(output_path, ignore_errors=True)

    if not zip_path.exists():
        download_mks_assets()

    if not assets_path.exists():
        copy_mks_assets()"
430	jackson	4	"import os

from _pydev_bundle import pydev_log
from _pydevd_bundle.pydevd_trace_dispatch import USING_CYTHON
from _pydevd_bundle.pydevd_constants import USE_CYTHON_FLAG, ENV_FALSE_LOWER_VALUES, \
    ENV_TRUE_LOWER_VALUES, IS_PY36_OR_GREATER, IS_PY38_OR_GREATER, SUPPORT_GEVENT, IS_PYTHON_STACKLESS, \
    PYDEVD_USE_FRAME_EVAL, PYDEVD_IPYTHON_COMPATIBLE_DEBUGGING

frame_eval_func = None
stop_frame_eval = None
dummy_trace_dispatch = None
clear_thread_local_info = None

# ""NO"" means we should not use frame evaluation, 'YES' we should use it (and fail if not there) and unspecified uses if possible.
if (
        PYDEVD_USE_FRAME_EVAL in ENV_FALSE_LOWER_VALUES or
        USE_CYTHON_FLAG in ENV_FALSE_LOWER_VALUES or
        not USING_CYTHON or

        # Frame eval mode does not work with ipython compatible debugging (this happens because the
        # way that frame eval works is run untraced and set tracing only for the frames with
        # breakpoints, but ipython compatible debugging creates separate frames for what's logically
        # the same frame).
        PYDEVD_IPYTHON_COMPATIBLE_DEBUGGING
    ):
    USING_FRAME_EVAL = False

elif SUPPORT_GEVENT or (IS_PYTHON_STACKLESS and not IS_PY38_OR_GREATER):
    USING_FRAME_EVAL = False
    # i.e gevent and frame eval mode don't get along very well.
    # https://github.com/microsoft/debugpy/issues/189
    # Same problem with Stackless.
    # https://github.com/stackless-dev/stackless/issues/240

elif PYDEVD_USE_FRAME_EVAL in ENV_TRUE_LOWER_VALUES:
    # Fail if unable to use
    from _pydevd_frame_eval.pydevd_frame_eval_cython_wrapper import frame_eval_func, stop_frame_eval, dummy_trace_dispatch, clear_thread_local_info
    USING_FRAME_EVAL = True

else:
    USING_FRAME_EVAL = False
    # Try to use if possible
    if IS_PY36_OR_GREATER:
        try:
            from _pydevd_frame_eval.pydevd_frame_eval_cython_wrapper import frame_eval_func, stop_frame_eval, dummy_trace_dispatch, clear_thread_local_info
            USING_FRAME_EVAL = True
        except ImportError:
            pydev_log.show_compile_cython_command_line()"
430	donghui	2	"import os

from _pydev_bundle import pydev_log
from _pydevd_bundle.pydevd_trace_dispatch import USING_CYTHON
from _pydevd_bundle.pydevd_constants import USE_CYTHON_FLAG, ENV_FALSE_LOWER_VALUES, \
    ENV_TRUE_LOWER_VALUES, IS_PY36_OR_GREATER, IS_PY38_OR_GREATER, SUPPORT_GEVENT, IS_PYTHON_STACKLESS, \
    PYDEVD_USE_FRAME_EVAL, PYDEVD_IPYTHON_COMPATIBLE_DEBUGGING

frame_eval_func = None
stop_frame_eval = None
dummy_trace_dispatch = None
clear_thread_local_info = None

# ""NO"" means we should not use frame evaluation, 'YES' we should use it (and fail if not there) and unspecified uses if possible.
if (
        PYDEVD_USE_FRAME_EVAL in ENV_FALSE_LOWER_VALUES or
        USE_CYTHON_FLAG in ENV_FALSE_LOWER_VALUES or
        not USING_CYTHON or

        # Frame eval mode does not work with ipython compatible debugging (this happens because the
        # way that frame eval works is run untraced and set tracing only for the frames with
        # breakpoints, but ipython compatible debugging creates separate frames for what's logically
        # the same frame).
        PYDEVD_IPYTHON_COMPATIBLE_DEBUGGING
    ):
    USING_FRAME_EVAL = False

elif SUPPORT_GEVENT or (IS_PYTHON_STACKLESS and not IS_PY38_OR_GREATER):
    USING_FRAME_EVAL = False
    # i.e gevent and frame eval mode don't get along very well.
    # https://github.com/microsoft/debugpy/issues/189
    # Same problem with Stackless.
    # https://github.com/stackless-dev/stackless/issues/240

elif PYDEVD_USE_FRAME_EVAL in ENV_TRUE_LOWER_VALUES:
    # Fail if unable to use
    from _pydevd_frame_eval.pydevd_frame_eval_cython_wrapper import frame_eval_func, stop_frame_eval, dummy_trace_dispatch, clear_thread_local_info
    USING_FRAME_EVAL = True

else:
    USING_FRAME_EVAL = False
    # Try to use if possible
    if IS_PY36_OR_GREATER:
        try:
            from _pydevd_frame_eval.pydevd_frame_eval_cython_wrapper import frame_eval_func, stop_frame_eval, dummy_trace_dispatch, clear_thread_local_info
            USING_FRAME_EVAL = True
        except ImportError:
            pydev_log.show_compile_cython_command_line()"
461	jackson	1	"#!/usr/bin/env python3

# Copyright (c) 2012 Google Inc. All rights reserved.
# Use of this source code is governed by a BSD-style license that can be
# found in the LICENSE file.

"""""" Unit tests for the ninja.py file. """"""

import sys
import unittest

import gyp.generator.ninja as ninja


class TestPrefixesAndSuffixes(unittest.TestCase):
    def test_BinaryNamesWindows(self):
        # These cannot run on non-Windows as they require a VS installation to
        # correctly handle variable expansion.
        if sys.platform.startswith(""win""):
            writer = ninja.NinjaWriter(
                ""foo"", ""wee"", ""."", ""."", ""build.ninja"", ""."", ""build.ninja"", ""win""
            )
            spec = {""target_name"": ""wee""}
            self.assertTrue(
                writer.ComputeOutputFileName(spec, ""executable"").endswith("".exe"")
            )
            self.assertTrue(
                writer.ComputeOutputFileName(spec, ""shared_library"").endswith("".dll"")
            )
            self.assertTrue(
                writer.ComputeOutputFileName(spec, ""static_library"").endswith("".lib"")
            )

    def test_BinaryNamesLinux(self):
        writer = ninja.NinjaWriter(
            ""foo"", ""wee"", ""."", ""."", ""build.ninja"", ""."", ""build.ninja"", ""linux""
        )
        spec = {""target_name"": ""wee""}
        self.assertTrue(""."" not in writer.ComputeOutputFileName(spec, ""executable""))
        self.assertTrue(
            writer.ComputeOutputFileName(spec, ""shared_library"").startswith(""lib"")
        )
        self.assertTrue(
            writer.ComputeOutputFileName(spec, ""static_library"").startswith(""lib"")
        )
        self.assertTrue(
            writer.ComputeOutputFileName(spec, ""shared_library"").endswith("".so"")
        )
        self.assertTrue(
            writer.ComputeOutputFileName(spec, ""static_library"").endswith("".a"")
        )


if __name__ == ""__main__"":
    unittest.main()"
461	donghui	2	"#!/usr/bin/env python3

# Copyright (c) 2012 Google Inc. All rights reserved.
# Use of this source code is governed by a BSD-style license that can be
# found in the LICENSE file.

"""""" Unit tests for the ninja.py file. """"""

import sys
import unittest

import gyp.generator.ninja as ninja


class TestPrefixesAndSuffixes(unittest.TestCase):
    def test_BinaryNamesWindows(self):
        # These cannot run on non-Windows as they require a VS installation to
        # correctly handle variable expansion.
        if sys.platform.startswith(""win""):
            writer = ninja.NinjaWriter(
                ""foo"", ""wee"", ""."", ""."", ""build.ninja"", ""."", ""build.ninja"", ""win""
            )
            spec = {""target_name"": ""wee""}
            self.assertTrue(
                writer.ComputeOutputFileName(spec, ""executable"").endswith("".exe"")
            )
            self.assertTrue(
                writer.ComputeOutputFileName(spec, ""shared_library"").endswith("".dll"")
            )
            self.assertTrue(
                writer.ComputeOutputFileName(spec, ""static_library"").endswith("".lib"")
            )

    def test_BinaryNamesLinux(self):
        writer = ninja.NinjaWriter(
            ""foo"", ""wee"", ""."", ""."", ""build.ninja"", ""."", ""build.ninja"", ""linux""
        )
        spec = {""target_name"": ""wee""}
        self.assertTrue(""."" not in writer.ComputeOutputFileName(spec, ""executable""))
        self.assertTrue(
            writer.ComputeOutputFileName(spec, ""shared_library"").startswith(""lib"")
        )
        self.assertTrue(
            writer.ComputeOutputFileName(spec, ""static_library"").startswith(""lib"")
        )
        self.assertTrue(
            writer.ComputeOutputFileName(spec, ""shared_library"").endswith("".so"")
        )
        self.assertTrue(
            writer.ComputeOutputFileName(spec, ""static_library"").endswith("".a"")
        )


if __name__ == ""__main__"":
    unittest.main()"
441	jackson	4	"""""""
PostGIS to GDAL conversion constant definitions
""""""
# Lookup to convert pixel type values from GDAL to PostGIS
GDAL_TO_POSTGIS = [None, 4, 6, 5, 8, 7, 10, 11, None, None, None, None]

# Lookup to convert pixel type values from PostGIS to GDAL
POSTGIS_TO_GDAL = [1, 1, 1, 3, 1, 3, 2, 5, 4, None, 6, 7, None, None]

# Struct pack structure for raster header, the raster header has the
# following structure:
#
# Endianness, PostGIS raster version, number of bands, scale, origin,
# skew, srid, width, and height.
#
# Scale, origin, and skew have x and y values. PostGIS currently uses
# a fixed endianness (1) and there is only one version (0).
POSTGIS_HEADER_STRUCTURE = ""B H H d d d d d d i H H""

# Lookup values to convert GDAL pixel types to struct characters. This is
# used to pack and unpack the pixel values of PostGIS raster bands.
GDAL_TO_STRUCT = [
    None,
    ""B"",
    ""H"",
    ""h"",
    ""L"",
    ""l"",
    ""f"",
    ""d"",
    None,
    None,
    None,
    None,
]

# Size of the packed value in bytes for different numerical types.
# This is needed to cut chunks of band data out of PostGIS raster strings
# when decomposing them into GDALRasters.
# See https://docs.python.org/library/struct.html#format-characters
STRUCT_SIZE = {
    ""b"": 1,  # Signed char
    ""B"": 1,  # Unsigned char
    ""?"": 1,  # _Bool
    ""h"": 2,  # Short
    ""H"": 2,  # Unsigned short
    ""i"": 4,  # Integer
    ""I"": 4,  # Unsigned Integer
    ""l"": 4,  # Long
    ""L"": 4,  # Unsigned Long
    ""f"": 4,  # Float
    ""d"": 8,  # Double
}

# Pixel type specifies type of pixel values in a band. Storage flag specifies
# whether the band data is stored as part of the datum or is to be found on the
# server's filesystem. There are currently 11 supported pixel value types, so 4
# bits are enough to account for all. Reserve the upper 4 bits for generic
# flags. See
# https://trac.osgeo.org/postgis/wiki/WKTRaster/RFC/RFC1_V0SerialFormat#Pixeltypeandstorageflag
BANDTYPE_PIXTYPE_MASK = 0x0F
BANDTYPE_FLAG_HASNODATA = 1 << 6"
441	donghui	3	"""""""
PostGIS to GDAL conversion constant definitions
""""""
# Lookup to convert pixel type values from GDAL to PostGIS
GDAL_TO_POSTGIS = [None, 4, 6, 5, 8, 7, 10, 11, None, None, None, None]

# Lookup to convert pixel type values from PostGIS to GDAL
POSTGIS_TO_GDAL = [1, 1, 1, 3, 1, 3, 2, 5, 4, None, 6, 7, None, None]

# Struct pack structure for raster header, the raster header has the
# following structure:
#
# Endianness, PostGIS raster version, number of bands, scale, origin,
# skew, srid, width, and height.
#
# Scale, origin, and skew have x and y values. PostGIS currently uses
# a fixed endianness (1) and there is only one version (0).
POSTGIS_HEADER_STRUCTURE = ""B H H d d d d d d i H H""

# Lookup values to convert GDAL pixel types to struct characters. This is
# used to pack and unpack the pixel values of PostGIS raster bands.
GDAL_TO_STRUCT = [
    None,
    ""B"",
    ""H"",
    ""h"",
    ""L"",
    ""l"",
    ""f"",
    ""d"",
    None,
    None,
    None,
    None,
]

# Size of the packed value in bytes for different numerical types.
# This is needed to cut chunks of band data out of PostGIS raster strings
# when decomposing them into GDALRasters.
# See https://docs.python.org/library/struct.html#format-characters
STRUCT_SIZE = {
    ""b"": 1,  # Signed char
    ""B"": 1,  # Unsigned char
    ""?"": 1,  # _Bool
    ""h"": 2,  # Short
    ""H"": 2,  # Unsigned short
    ""i"": 4,  # Integer
    ""I"": 4,  # Unsigned Integer
    ""l"": 4,  # Long
    ""L"": 4,  # Unsigned Long
    ""f"": 4,  # Float
    ""d"": 8,  # Double
}

# Pixel type specifies type of pixel values in a band. Storage flag specifies
# whether the band data is stored as part of the datum or is to be found on the
# server's filesystem. There are currently 11 supported pixel value types, so 4
# bits are enough to account for all. Reserve the upper 4 bits for generic
# flags. See
# https://trac.osgeo.org/postgis/wiki/WKTRaster/RFC/RFC1_V0SerialFormat#Pixeltypeandstorageflag
BANDTYPE_PIXTYPE_MASK = 0x0F
BANDTYPE_FLAG_HASNODATA = 1 << 6"
410	jackson	1	"# -*- coding: utf-8 -*-
# Generated by the protocol buffer compiler.  DO NOT EDIT!
# source: streamlit/proto/Spinner.proto

from google.protobuf import descriptor as _descriptor
from google.protobuf import message as _message
from google.protobuf import reflection as _reflection
from google.protobuf import symbol_database as _symbol_database
# @@protoc_insertion_point(imports)

_sym_db = _symbol_database.Default()




DESCRIPTOR = _descriptor.FileDescriptor(
  name='streamlit/proto/Spinner.proto',
  package='',
  syntax='proto3',
  serialized_options=None,
  create_key=_descriptor._internal_create_key,
  serialized_pb=b'\n\x1dstreamlit/proto/Spinner.proto\""\x17\n\x07Spinner\x12\x0c\n\x04text\x18\x01 \x01(\tb\x06proto3'
)




_SPINNER = _descriptor.Descriptor(
  name='Spinner',
  full_name='Spinner',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  create_key=_descriptor._internal_create_key,
  fields=[
    _descriptor.FieldDescriptor(
      name='text', full_name='Spinner.text', index=0,
      number=1, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=b"""".decode('utf-8'),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR,  create_key=_descriptor._internal_create_key),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  serialized_options=None,
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=33,
  serialized_end=56,
)

DESCRIPTOR.message_types_by_name['Spinner'] = _SPINNER
_sym_db.RegisterFileDescriptor(DESCRIPTOR)

Spinner = _reflection.GeneratedProtocolMessageType('Spinner', (_message.Message,), {
  'DESCRIPTOR' : _SPINNER,
  '__module__' : 'streamlit.proto.Spinner_pb2'
  # @@protoc_insertion_point(class_scope:Spinner)
  })
_sym_db.RegisterMessage(Spinner)


# @@protoc_insertion_point(module_scope)"
410	donghui	1	"# -*- coding: utf-8 -*-
# Generated by the protocol buffer compiler.  DO NOT EDIT!
# source: streamlit/proto/Spinner.proto

from google.protobuf import descriptor as _descriptor
from google.protobuf import message as _message
from google.protobuf import reflection as _reflection
from google.protobuf import symbol_database as _symbol_database
# @@protoc_insertion_point(imports)

_sym_db = _symbol_database.Default()




DESCRIPTOR = _descriptor.FileDescriptor(
  name='streamlit/proto/Spinner.proto',
  package='',
  syntax='proto3',
  serialized_options=None,
  create_key=_descriptor._internal_create_key,
  serialized_pb=b'\n\x1dstreamlit/proto/Spinner.proto\""\x17\n\x07Spinner\x12\x0c\n\x04text\x18\x01 \x01(\tb\x06proto3'
)




_SPINNER = _descriptor.Descriptor(
  name='Spinner',
  full_name='Spinner',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  create_key=_descriptor._internal_create_key,
  fields=[
    _descriptor.FieldDescriptor(
      name='text', full_name='Spinner.text', index=0,
      number=1, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=b"""".decode('utf-8'),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR,  create_key=_descriptor._internal_create_key),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  serialized_options=None,
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=33,
  serialized_end=56,
)

DESCRIPTOR.message_types_by_name['Spinner'] = _SPINNER
_sym_db.RegisterFileDescriptor(DESCRIPTOR)

Spinner = _reflection.GeneratedProtocolMessageType('Spinner', (_message.Message,), {
  'DESCRIPTOR' : _SPINNER,
  '__module__' : 'streamlit.proto.Spinner_pb2'
  # @@protoc_insertion_point(class_scope:Spinner)
  })
_sym_db.RegisterMessage(Spinner)


# @@protoc_insertion_point(module_scope)"
424	jackson	1	"""""""Test markdown rendering""""""


from nbformat.v4 import new_markdown_cell

from .utils import EDITOR_PAGE


def get_rendered_contents(nb):
    # TODO: Encapsulate element access/refactor so we're not accessing playwright element objects
    cl = [""text_cell"", ""render""]
    rendered_cells = [cell.locate("".text_cell_render"")
                      for cell in nb.cells
                      if all([c in cell.get_attribute(""class"") for c in cl])]
    return [x.get_inner_html().strip()
            for x in rendered_cells
            if x is not None]


def test_markdown_cell(prefill_notebook):
    notebook_frontend = prefill_notebook([new_markdown_cell(md) for md in [
        '# Foo', '**Bar**', '*Baz*', '```\nx = 1\n```', '```aaaa\nx = 1\n```',
        '```python\ns = ""$""\nt = ""$""\n```'
    ]])

    assert get_rendered_contents(notebook_frontend) == [
        '<h1 id=""Foo"">Foo<a class=""anchor-link"" href=""#Foo"">¶</a></h1>',
        '<p><strong>Bar</strong></p>',
        '<p><em>Baz</em></p>',
        '<pre><code>x = 1</code></pre>',
        '<pre><code class=""cm-s-ipython language-aaaa"">x = 1</code></pre>',
        '<pre><code class=""cm-s-ipython language-python"">' +
        '<span class=""cm-variable"">s</span> <span class=""cm-operator"">=</span> <span class=""cm-string"">""$""</span>\n' +
        '<span class=""cm-variable"">t</span> <span class=""cm-operator"">=</span> <span class=""cm-string"">""$""</span></code></pre>'
    ]


def test_markdown_headings(notebook_frontend):
    for i in [1, 2, 3, 4, 5, 6, 2, 1]:
        notebook_frontend.add_markdown_cell()
        cell_text = notebook_frontend.evaluate(f""""""
            var cell = IPython.notebook.get_cell(1);
            cell.set_heading_level({i});
            cell.get_text();
        """""", page=EDITOR_PAGE)
        assert notebook_frontend.get_cell_contents(1) == ""#"" * i + "" ""
        notebook_frontend.delete_cell(1)"
424	donghui	1	"""""""Test markdown rendering""""""


from nbformat.v4 import new_markdown_cell

from .utils import EDITOR_PAGE


def get_rendered_contents(nb):
    # TODO: Encapsulate element access/refactor so we're not accessing playwright element objects
    cl = [""text_cell"", ""render""]
    rendered_cells = [cell.locate("".text_cell_render"")
                      for cell in nb.cells
                      if all([c in cell.get_attribute(""class"") for c in cl])]
    return [x.get_inner_html().strip()
            for x in rendered_cells
            if x is not None]


def test_markdown_cell(prefill_notebook):
    notebook_frontend = prefill_notebook([new_markdown_cell(md) for md in [
        '# Foo', '**Bar**', '*Baz*', '```\nx = 1\n```', '```aaaa\nx = 1\n```',
        '```python\ns = ""$""\nt = ""$""\n```'
    ]])

    assert get_rendered_contents(notebook_frontend) == [
        '<h1 id=""Foo"">Foo<a class=""anchor-link"" href=""#Foo"">¶</a></h1>',
        '<p><strong>Bar</strong></p>',
        '<p><em>Baz</em></p>',
        '<pre><code>x = 1</code></pre>',
        '<pre><code class=""cm-s-ipython language-aaaa"">x = 1</code></pre>',
        '<pre><code class=""cm-s-ipython language-python"">' +
        '<span class=""cm-variable"">s</span> <span class=""cm-operator"">=</span> <span class=""cm-string"">""$""</span>\n' +
        '<span class=""cm-variable"">t</span> <span class=""cm-operator"">=</span> <span class=""cm-string"">""$""</span></code></pre>'
    ]


def test_markdown_headings(notebook_frontend):
    for i in [1, 2, 3, 4, 5, 6, 2, 1]:
        notebook_frontend.add_markdown_cell()
        cell_text = notebook_frontend.evaluate(f""""""
            var cell = IPython.notebook.get_cell(1);
            cell.set_heading_level({i});
            cell.get_text();
        """""", page=EDITOR_PAGE)
        assert notebook_frontend.get_cell_contents(1) == ""#"" * i + "" ""
        notebook_frontend.delete_cell(1)"
475	jackson	1	"from rx.core import ObservableBase, Observer, AnonymousObserver, Disposable
from rx.disposables import CompositeDisposable

from .subscription import Subscription
from .reactive_assert import AssertList


class ColdObservable(ObservableBase):
    def __init__(self, scheduler, messages):
        super(ColdObservable, self).__init__()

        self.scheduler = scheduler
        self.messages = messages
        self.subscriptions = AssertList()

    def subscribe(self, on_next=None, on_error=None, on_completed=None, observer=None):
        # Be forgiving and accept an un-named observer as first parameter
        if isinstance(on_next, Observer):
            observer = on_next
        elif not observer:
            observer = AnonymousObserver(on_next, on_error, on_completed)

        return self._subscribe_core(observer)

    def _subscribe_core(self, observer):
        clock = self.scheduler.to_relative(self.scheduler.now)
        self.subscriptions.append(Subscription(clock))
        index = len(self.subscriptions) - 1
        disposable = CompositeDisposable()

        def get_action(notification):
            def action(scheduler, state):
                notification.accept(observer)
                return Disposable.empty()
            return action

        for message in self.messages:
            notification = message.value

            # Don't make closures within a loop
            action = get_action(notification)
            disposable.add(self.scheduler.schedule_relative(message.time, action))

        def dispose():
            start = self.subscriptions[index].subscribe
            end = self.scheduler.to_relative(self.scheduler.now)
            self.subscriptions[index] = Subscription(start, end)
            disposable.dispose()

        return Disposable.create(dispose)"
475	donghui	0	"from rx.core import ObservableBase, Observer, AnonymousObserver, Disposable
from rx.disposables import CompositeDisposable

from .subscription import Subscription
from .reactive_assert import AssertList


class ColdObservable(ObservableBase):
    def __init__(self, scheduler, messages):
        super(ColdObservable, self).__init__()

        self.scheduler = scheduler
        self.messages = messages
        self.subscriptions = AssertList()

    def subscribe(self, on_next=None, on_error=None, on_completed=None, observer=None):
        # Be forgiving and accept an un-named observer as first parameter
        if isinstance(on_next, Observer):
            observer = on_next
        elif not observer:
            observer = AnonymousObserver(on_next, on_error, on_completed)

        return self._subscribe_core(observer)

    def _subscribe_core(self, observer):
        clock = self.scheduler.to_relative(self.scheduler.now)
        self.subscriptions.append(Subscription(clock))
        index = len(self.subscriptions) - 1
        disposable = CompositeDisposable()

        def get_action(notification):
            def action(scheduler, state):
                notification.accept(observer)
                return Disposable.empty()
            return action

        for message in self.messages:
            notification = message.value

            # Don't make closures within a loop
            action = get_action(notification)
            disposable.add(self.scheduler.schedule_relative(message.time, action))

        def dispose():
            start = self.subscriptions[index].subscribe
            end = self.scheduler.to_relative(self.scheduler.now)
            self.subscriptions[index] = Subscription(start, end)
            disposable.dispose()

        return Disposable.create(dispose)"
486	jackson	1	"import socket
import typing

from tornado.http1connection import HTTP1Connection
from tornado.httputil import HTTPMessageDelegate
from tornado.iostream import IOStream
from tornado.locks import Event
from tornado.netutil import add_accept_handler
from tornado.testing import AsyncTestCase, bind_unused_port, gen_test


class HTTP1ConnectionTest(AsyncTestCase):
    code = None  # type: typing.Optional[int]

    def setUp(self):
        super().setUp()
        self.asyncSetUp()

    @gen_test
    def asyncSetUp(self):
        listener, port = bind_unused_port()
        event = Event()

        def accept_callback(conn, addr):
            self.server_stream = IOStream(conn)
            self.addCleanup(self.server_stream.close)
            event.set()

        add_accept_handler(listener, accept_callback)
        self.client_stream = IOStream(socket.socket())
        self.addCleanup(self.client_stream.close)
        yield [self.client_stream.connect((""127.0.0.1"", port)), event.wait()]
        self.io_loop.remove_handler(listener)
        listener.close()

    @gen_test
    def test_http10_no_content_length(self):
        # Regression test for a bug in which can_keep_alive would crash
        # for an HTTP/1.0 (not 1.1) response with no content-length.
        conn = HTTP1Connection(self.client_stream, True)
        self.server_stream.write(b""HTTP/1.0 200 Not Modified\r\n\r\nhello"")
        self.server_stream.close()

        event = Event()
        test = self
        body = []

        class Delegate(HTTPMessageDelegate):
            def headers_received(self, start_line, headers):
                test.code = start_line.code

            def data_received(self, data):
                body.append(data)

            def finish(self):
                event.set()

        yield conn.read_response(Delegate())
        yield event.wait()
        self.assertEqual(self.code, 200)
        self.assertEqual(b"""".join(body), b""hello"")"
486	donghui	1	"import socket
import typing

from tornado.http1connection import HTTP1Connection
from tornado.httputil import HTTPMessageDelegate
from tornado.iostream import IOStream
from tornado.locks import Event
from tornado.netutil import add_accept_handler
from tornado.testing import AsyncTestCase, bind_unused_port, gen_test


class HTTP1ConnectionTest(AsyncTestCase):
    code = None  # type: typing.Optional[int]

    def setUp(self):
        super().setUp()
        self.asyncSetUp()

    @gen_test
    def asyncSetUp(self):
        listener, port = bind_unused_port()
        event = Event()

        def accept_callback(conn, addr):
            self.server_stream = IOStream(conn)
            self.addCleanup(self.server_stream.close)
            event.set()

        add_accept_handler(listener, accept_callback)
        self.client_stream = IOStream(socket.socket())
        self.addCleanup(self.client_stream.close)
        yield [self.client_stream.connect((""127.0.0.1"", port)), event.wait()]
        self.io_loop.remove_handler(listener)
        listener.close()

    @gen_test
    def test_http10_no_content_length(self):
        # Regression test for a bug in which can_keep_alive would crash
        # for an HTTP/1.0 (not 1.1) response with no content-length.
        conn = HTTP1Connection(self.client_stream, True)
        self.server_stream.write(b""HTTP/1.0 200 Not Modified\r\n\r\nhello"")
        self.server_stream.close()

        event = Event()
        test = self
        body = []

        class Delegate(HTTPMessageDelegate):
            def headers_received(self, start_line, headers):
                test.code = start_line.code

            def data_received(self, data):
                body.append(data)

            def finish(self):
                event.set()

        yield conn.read_response(Delegate())
        yield event.wait()
        self.assertEqual(self.code, 200)
        self.assertEqual(b"""".join(body), b""hello"")"
399	jackson	3	"# PermWrapper and PermLookupDict proxy the permissions system into objects that
# the template system can understand.


class PermLookupDict:
    def __init__(self, user, app_label):
        self.user, self.app_label = user, app_label

    def __repr__(self):
        return str(self.user.get_all_permissions())

    def __getitem__(self, perm_name):
        return self.user.has_perm(""%s.%s"" % (self.app_label, perm_name))

    def __iter__(self):
        # To fix 'item in perms.someapp' and __getitem__ interaction we need to
        # define __iter__. See #18979 for details.
        raise TypeError(""PermLookupDict is not iterable."")

    def __bool__(self):
        return self.user.has_module_perms(self.app_label)


class PermWrapper:
    def __init__(self, user):
        self.user = user

    def __getitem__(self, app_label):
        return PermLookupDict(self.user, app_label)

    def __iter__(self):
        # I am large, I contain multitudes.
        raise TypeError(""PermWrapper is not iterable."")

    def __contains__(self, perm_name):
        """"""
        Lookup by ""someapp"" or ""someapp.someperm"" in perms.
        """"""
        if '.' not in perm_name:
            # The name refers to module.
            return bool(self[perm_name])
        app_label, perm_name = perm_name.split('.', 1)
        return self[app_label][perm_name]


def auth(request):
    """"""
    Return context variables required by apps that use Django's authentication
    system.

    If there is no 'user' attribute in the request, use AnonymousUser (from
    django.contrib.auth).
    """"""
    if hasattr(request, 'user'):
        user = request.user
    else:
        from django.contrib.auth.models import AnonymousUser
        user = AnonymousUser()

    return {
        'user': user,
        'perms': PermWrapper(user),
    }"
399	donghui	2	"# PermWrapper and PermLookupDict proxy the permissions system into objects that
# the template system can understand.


class PermLookupDict:
    def __init__(self, user, app_label):
        self.user, self.app_label = user, app_label

    def __repr__(self):
        return str(self.user.get_all_permissions())

    def __getitem__(self, perm_name):
        return self.user.has_perm(""%s.%s"" % (self.app_label, perm_name))

    def __iter__(self):
        # To fix 'item in perms.someapp' and __getitem__ interaction we need to
        # define __iter__. See #18979 for details.
        raise TypeError(""PermLookupDict is not iterable."")

    def __bool__(self):
        return self.user.has_module_perms(self.app_label)


class PermWrapper:
    def __init__(self, user):
        self.user = user

    def __getitem__(self, app_label):
        return PermLookupDict(self.user, app_label)

    def __iter__(self):
        # I am large, I contain multitudes.
        raise TypeError(""PermWrapper is not iterable."")

    def __contains__(self, perm_name):
        """"""
        Lookup by ""someapp"" or ""someapp.someperm"" in perms.
        """"""
        if '.' not in perm_name:
            # The name refers to module.
            return bool(self[perm_name])
        app_label, perm_name = perm_name.split('.', 1)
        return self[app_label][perm_name]


def auth(request):
    """"""
    Return context variables required by apps that use Django's authentication
    system.

    If there is no 'user' attribute in the request, use AnonymousUser (from
    django.contrib.auth).
    """"""
    if hasattr(request, 'user'):
        user = request.user
    else:
        from django.contrib.auth.models import AnonymousUser
        user = AnonymousUser()

    return {
        'user': user,
        'perms': PermWrapper(user),
    }"
288	jackson	0	"import esphome.codegen as cg
import esphome.config_validation as cv
from esphome import pins
from esphome.components import display
from esphome.const import (
    CONF_CLK_PIN,
    CONF_DIO_PIN,
    CONF_ID,
    CONF_LAMBDA,
    CONF_INTENSITY,
    CONF_INVERTED,
    CONF_LENGTH,
)

CODEOWNERS = [""@glmnet""]

tm1637_ns = cg.esphome_ns.namespace(""tm1637"")
TM1637Display = tm1637_ns.class_(""TM1637Display"", cg.PollingComponent)
TM1637DisplayRef = TM1637Display.operator(""ref"")

CONFIG_SCHEMA = display.BASIC_DISPLAY_SCHEMA.extend(
    {
        cv.GenerateID(): cv.declare_id(TM1637Display),
        cv.Optional(CONF_INTENSITY, default=7): cv.All(
            cv.uint8_t, cv.Range(min=0, max=7)
        ),
        cv.Optional(CONF_INVERTED, default=False): cv.boolean,
        cv.Optional(CONF_LENGTH, default=6): cv.All(cv.uint8_t, cv.Range(min=1, max=6)),
        cv.Required(CONF_CLK_PIN): pins.gpio_output_pin_schema,
        cv.Required(CONF_DIO_PIN): pins.gpio_output_pin_schema,
    }
).extend(cv.polling_component_schema(""1s""))


async def to_code(config):
    var = cg.new_Pvariable(config[CONF_ID])
    await cg.register_component(var, config)
    await display.register_display(var, config)

    clk = await cg.gpio_pin_expression(config[CONF_CLK_PIN])
    cg.add(var.set_clk_pin(clk))
    dio = await cg.gpio_pin_expression(config[CONF_DIO_PIN])
    cg.add(var.set_dio_pin(dio))

    cg.add(var.set_intensity(config[CONF_INTENSITY]))
    cg.add(var.set_inverted(config[CONF_INVERTED]))
    cg.add(var.set_length(config[CONF_LENGTH]))

    if CONF_LAMBDA in config:
        lambda_ = await cg.process_lambda(
            config[CONF_LAMBDA], [(TM1637DisplayRef, ""it"")], return_type=cg.void
        )
        cg.add(var.set_writer(lambda_))"
288	donghui	0	"import esphome.codegen as cg
import esphome.config_validation as cv
from esphome import pins
from esphome.components import display
from esphome.const import (
    CONF_CLK_PIN,
    CONF_DIO_PIN,
    CONF_ID,
    CONF_LAMBDA,
    CONF_INTENSITY,
    CONF_INVERTED,
    CONF_LENGTH,
)

CODEOWNERS = [""@glmnet""]

tm1637_ns = cg.esphome_ns.namespace(""tm1637"")
TM1637Display = tm1637_ns.class_(""TM1637Display"", cg.PollingComponent)
TM1637DisplayRef = TM1637Display.operator(""ref"")

CONFIG_SCHEMA = display.BASIC_DISPLAY_SCHEMA.extend(
    {
        cv.GenerateID(): cv.declare_id(TM1637Display),
        cv.Optional(CONF_INTENSITY, default=7): cv.All(
            cv.uint8_t, cv.Range(min=0, max=7)
        ),
        cv.Optional(CONF_INVERTED, default=False): cv.boolean,
        cv.Optional(CONF_LENGTH, default=6): cv.All(cv.uint8_t, cv.Range(min=1, max=6)),
        cv.Required(CONF_CLK_PIN): pins.gpio_output_pin_schema,
        cv.Required(CONF_DIO_PIN): pins.gpio_output_pin_schema,
    }
).extend(cv.polling_component_schema(""1s""))


async def to_code(config):
    var = cg.new_Pvariable(config[CONF_ID])
    await cg.register_component(var, config)
    await display.register_display(var, config)

    clk = await cg.gpio_pin_expression(config[CONF_CLK_PIN])
    cg.add(var.set_clk_pin(clk))
    dio = await cg.gpio_pin_expression(config[CONF_DIO_PIN])
    cg.add(var.set_dio_pin(dio))

    cg.add(var.set_intensity(config[CONF_INTENSITY]))
    cg.add(var.set_inverted(config[CONF_INVERTED]))
    cg.add(var.set_length(config[CONF_LENGTH]))

    if CONF_LAMBDA in config:
        lambda_ = await cg.process_lambda(
            config[CONF_LAMBDA], [(TM1637DisplayRef, ""it"")], return_type=cg.void
        )
        cg.add(var.set_writer(lambda_))"
298	jackson	2	"import _plotly_utils.basevalidators


class InsidetextfontValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(self, plotly_name=""insidetextfont"", parent_name=""bar"", **kwargs):
        super(InsidetextfontValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Insidetextfont""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            color

            colorsrc
                Sets the source reference on Chart Studio Cloud
                for `color`.
            family
                HTML font family - the typeface that will be
                applied by the web browser. The web browser
                will only be able to apply a font if it is
                available on the system which it operates.
                Provide multiple font families, separated by
                commas, to indicate the preference in which to
                apply fonts if they aren't available on the
                system. The Chart Studio Cloud (at
                https://chart-studio.plotly.com or on-premise)
                generates images on a server, where only a
                select number of fonts are installed and
                supported. These include ""Arial"", ""Balto"",
                ""Courier New"", ""Droid Sans"",, ""Droid Serif"",
                ""Droid Sans Mono"", ""Gravitas One"", ""Old
                Standard TT"", ""Open Sans"", ""Overpass"", ""PT Sans
                Narrow"", ""Raleway"", ""Times New Roman"".
            familysrc
                Sets the source reference on Chart Studio Cloud
                for `family`.
            size

            sizesrc
                Sets the source reference on Chart Studio Cloud
                for `size`.
"""""",
            ),
            **kwargs,
        )"
298	donghui	1	"import _plotly_utils.basevalidators


class InsidetextfontValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(self, plotly_name=""insidetextfont"", parent_name=""bar"", **kwargs):
        super(InsidetextfontValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Insidetextfont""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            color

            colorsrc
                Sets the source reference on Chart Studio Cloud
                for `color`.
            family
                HTML font family - the typeface that will be
                applied by the web browser. The web browser
                will only be able to apply a font if it is
                available on the system which it operates.
                Provide multiple font families, separated by
                commas, to indicate the preference in which to
                apply fonts if they aren't available on the
                system. The Chart Studio Cloud (at
                https://chart-studio.plotly.com or on-premise)
                generates images on a server, where only a
                select number of fonts are installed and
                supported. These include ""Arial"", ""Balto"",
                ""Courier New"", ""Droid Sans"",, ""Droid Serif"",
                ""Droid Sans Mono"", ""Gravitas One"", ""Old
                Standard TT"", ""Open Sans"", ""Overpass"", ""PT Sans
                Narrow"", ""Raleway"", ""Times New Roman"".
            familysrc
                Sets the source reference on Chart Studio Cloud
                for `family`.
            size

            sizesrc
                Sets the source reference on Chart Studio Cloud
                for `size`.
"""""",
            ),
            **kwargs,
        )"
389	jackson	0	"import cv2
from cvzone.HandTrackingModule import HandDetector
from cvzone.ClassificationModule import Classifier
import numpy as np
import math
cap = cv2.VideoCapture(0)
detector = HandDetector(maxHands=1)
classifier = Classifier(""keras_model.h5"", ""labels.txt"")

offset = 20
imgSize = 300

folder = ""Data/C""
counter = 0

labels = [""A"", ""B"", ""C""]

while True:
    success, img = cap.read()
    imgOutput = img.copy()
    hands, img = detector.findHands(img)
    if hands:
        hand = hands[0]
        x, y, w, h = hand['bbox']

        imgWhite = np.ones((imgSize, imgSize, 3), np.uint8) * 255
        imgCrop = img[y - offset:y + h + offset, x - offset:x + w + offset]

        imgCropShape = imgCrop.shape

        aspectRatio = h / w

        if aspectRatio > 1:
            k = imgSize / h
            wCal = math.ceil(k * w)
            imgResize = cv2.resize(imgCrop, (wCal, imgSize))
            imgResizeShape = imgResize.shape
            wGap = math.ceil((imgSize - wCal) / 2)
            imgWhite[:, wGap:wCal + wGap] = imgResize
            prediction, index = classifier.getPrediction(imgWhite, draw=False)
            print(prediction, index)

        else:
            k = imgSize / w
            hCal = math.ceil(k * h)
            imgResize = cv2.resize(imgCrop, (imgSize, hCal))
            imgResizeShape = imgResize.shape
            hGap = math.ceil((imgSize - hCal) / 2)
            imgWhite[hGap:hCal + hGap, :] = imgResize
            prediction, index = classifier.getPrediction(imgWhite, draw=False)

        cv2.rectangle(imgOutput, (x - offset, y - offset-50),
                      (x - offset+90, y - offset-50+50), (255, 0, 255), cv2.FILLED)
        cv2.putText(imgOutput, labels[index], (x, y -26), cv2.FONT_HERSHEY_COMPLEX, 1.7, (255, 255, 255), 2)
        cv2.rectangle(imgOutput, (x-offset, y-offset),
                      (x + w+offset, y + h+offset), (255, 0, 255), 4)


        cv2.imshow(""ImageCrop"", imgCrop)
        cv2.imshow(""ImageWhite"", imgWhite)

    cv2.imshow(""Image"", imgOutput)
    cv2.waitKey(1)"
389	donghui	0	"import cv2
from cvzone.HandTrackingModule import HandDetector
from cvzone.ClassificationModule import Classifier
import numpy as np
import math
cap = cv2.VideoCapture(0)
detector = HandDetector(maxHands=1)
classifier = Classifier(""keras_model.h5"", ""labels.txt"")

offset = 20
imgSize = 300

folder = ""Data/C""
counter = 0

labels = [""A"", ""B"", ""C""]

while True:
    success, img = cap.read()
    imgOutput = img.copy()
    hands, img = detector.findHands(img)
    if hands:
        hand = hands[0]
        x, y, w, h = hand['bbox']

        imgWhite = np.ones((imgSize, imgSize, 3), np.uint8) * 255
        imgCrop = img[y - offset:y + h + offset, x - offset:x + w + offset]

        imgCropShape = imgCrop.shape

        aspectRatio = h / w

        if aspectRatio > 1:
            k = imgSize / h
            wCal = math.ceil(k * w)
            imgResize = cv2.resize(imgCrop, (wCal, imgSize))
            imgResizeShape = imgResize.shape
            wGap = math.ceil((imgSize - wCal) / 2)
            imgWhite[:, wGap:wCal + wGap] = imgResize
            prediction, index = classifier.getPrediction(imgWhite, draw=False)
            print(prediction, index)

        else:
            k = imgSize / w
            hCal = math.ceil(k * h)
            imgResize = cv2.resize(imgCrop, (imgSize, hCal))
            imgResizeShape = imgResize.shape
            hGap = math.ceil((imgSize - hCal) / 2)
            imgWhite[hGap:hCal + hGap, :] = imgResize
            prediction, index = classifier.getPrediction(imgWhite, draw=False)

        cv2.rectangle(imgOutput, (x - offset, y - offset-50),
                      (x - offset+90, y - offset-50+50), (255, 0, 255), cv2.FILLED)
        cv2.putText(imgOutput, labels[index], (x, y -26), cv2.FONT_HERSHEY_COMPLEX, 1.7, (255, 255, 255), 2)
        cv2.rectangle(imgOutput, (x-offset, y-offset),
                      (x + w+offset, y + h+offset), (255, 0, 255), 4)


        cv2.imshow(""ImageCrop"", imgCrop)
        cv2.imshow(""ImageWhite"", imgWhite)

    cv2.imshow(""Image"", imgOutput)
    cv2.waitKey(1)"
496	jackson	4	"# Copyright 2021 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# Default TEST_CONFIG_OVERRIDE for python repos.

# You can copy this file into your directory, then it will be imported from
# the noxfile.py.

# The source of truth:
# https://github.com/GoogleCloudPlatform/python-docs-samples/blob/main/noxfile_config.py

TEST_CONFIG_OVERRIDE = {
    # You can opt out from the test for specific Python versions.
    ""ignored_versions"": [""2.7"", ""3.6"", ""3.9"", ""3.10"", ""3.11""],
    # Old samples are opted out of enforcing Python type hints
    # All new samples should feature them
    ""enforce_type_hints"": False,
    # An envvar key for determining the project id to use. Change it
    # to 'BUILD_SPECIFIC_GCLOUD_PROJECT' if you want to opt in using a
    # build specific Cloud project. You can also use your own string
    # to use your own Cloud project.
    ""gcloud_project_env"": ""GOOGLE_CLOUD_PROJECT"",
    # 'gcloud_project_env': 'BUILD_SPECIFIC_GCLOUD_PROJECT',
    # If you need to use a specific version of pip,
    # change pip_version_override to the string representation
    # of the version number, for example, ""20.2.4""
    ""pip_version_override"": None,
    # A dictionary you want to inject into your test. Don't put any
    # secrets here. These values will override predefined values.
    ""envs"": {},
}"
496	donghui	3	"# Copyright 2021 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# Default TEST_CONFIG_OVERRIDE for python repos.

# You can copy this file into your directory, then it will be imported from
# the noxfile.py.

# The source of truth:
# https://github.com/GoogleCloudPlatform/python-docs-samples/blob/main/noxfile_config.py

TEST_CONFIG_OVERRIDE = {
    # You can opt out from the test for specific Python versions.
    ""ignored_versions"": [""2.7"", ""3.6"", ""3.9"", ""3.10"", ""3.11""],
    # Old samples are opted out of enforcing Python type hints
    # All new samples should feature them
    ""enforce_type_hints"": False,
    # An envvar key for determining the project id to use. Change it
    # to 'BUILD_SPECIFIC_GCLOUD_PROJECT' if you want to opt in using a
    # build specific Cloud project. You can also use your own string
    # to use your own Cloud project.
    ""gcloud_project_env"": ""GOOGLE_CLOUD_PROJECT"",
    # 'gcloud_project_env': 'BUILD_SPECIFIC_GCLOUD_PROJECT',
    # If you need to use a specific version of pip,
    # change pip_version_override to the string representation
    # of the version number, for example, ""20.2.4""
    ""pip_version_override"": None,
    # A dictionary you want to inject into your test. Don't put any
    # secrets here. These values will override predefined values.
    ""envs"": {},
}"
465	jackson	0	"# Copyright (c) Meta Platforms, Inc. and affiliates.
#
# This source code is licensed under the MIT license found in the
# LICENSE file in the root directory of this source tree.

import dataclasses

from textwrap import dedent

import viktor._vendor.libcst as cst
from viktor._vendor.libcst.metadata import AccessorProvider, MetadataWrapper
from viktor._vendor.libcst.testing.utils import data_provider, UnitTest


class DependentVisitor(cst.CSTVisitor):
    METADATA_DEPENDENCIES = (AccessorProvider,)

    def __init__(self, *, test: UnitTest) -> None:
        self.test = test

    def on_visit(self, node: cst.CSTNode) -> bool:
        for f in dataclasses.fields(node):
            child = getattr(node, f.name)
            if type(child) is cst.CSTNode:
                accessor = self.get_metadata(AccessorProvider, child)
                self.test.assertEqual(accessor, f.name)

        return True


class AccessorProviderTest(UnitTest):
    @data_provider(
        (
            (
                """"""
                foo = 'toplevel'
                fn1(foo)
                fn2(foo)
                def fn_def():
                    foo = 'shadow'
                    fn3(foo)
                """""",
            ),
            (
                """"""
                global_var = None
                @cls_attr
                class Cls(cls_attr, kwarg=cls_attr):
                    cls_attr = 5
                    def f():
                        pass
                """""",
            ),
            (
                """"""
                iterator = None
                condition = None
                [elt for target in iterator if condition]
                {elt for target in iterator if condition}
                {elt: target for target in iterator if condition}
                (elt for target in iterator if condition)
                """""",
            ),
        )
    )
    def test_accessor_provier(self, code: str) -> None:
        wrapper = MetadataWrapper(cst.parse_module(dedent(code)))
        wrapper.visit(DependentVisitor(test=self))"
465	donghui	0	"# Copyright (c) Meta Platforms, Inc. and affiliates.
#
# This source code is licensed under the MIT license found in the
# LICENSE file in the root directory of this source tree.

import dataclasses

from textwrap import dedent

import viktor._vendor.libcst as cst
from viktor._vendor.libcst.metadata import AccessorProvider, MetadataWrapper
from viktor._vendor.libcst.testing.utils import data_provider, UnitTest


class DependentVisitor(cst.CSTVisitor):
    METADATA_DEPENDENCIES = (AccessorProvider,)

    def __init__(self, *, test: UnitTest) -> None:
        self.test = test

    def on_visit(self, node: cst.CSTNode) -> bool:
        for f in dataclasses.fields(node):
            child = getattr(node, f.name)
            if type(child) is cst.CSTNode:
                accessor = self.get_metadata(AccessorProvider, child)
                self.test.assertEqual(accessor, f.name)

        return True


class AccessorProviderTest(UnitTest):
    @data_provider(
        (
            (
                """"""
                foo = 'toplevel'
                fn1(foo)
                fn2(foo)
                def fn_def():
                    foo = 'shadow'
                    fn3(foo)
                """""",
            ),
            (
                """"""
                global_var = None
                @cls_attr
                class Cls(cls_attr, kwarg=cls_attr):
                    cls_attr = 5
                    def f():
                        pass
                """""",
            ),
            (
                """"""
                iterator = None
                condition = None
                [elt for target in iterator if condition]
                {elt for target in iterator if condition}
                {elt: target for target in iterator if condition}
                (elt for target in iterator if condition)
                """""",
            ),
        )
    )
    def test_accessor_provier(self, code: str) -> None:
        wrapper = MetadataWrapper(cst.parse_module(dedent(code)))
        wrapper.visit(DependentVisitor(test=self))"
434	jackson	4	"# This file is distributed under the same license as the Django package.
#
# The *_FORMAT strings use the Django date format syntax,
# see https://docs.djangoproject.com/en/dev/ref/templates/builtins/#date
DATE_FORMAT = ""d F Y""  # 25 Ottobre 2006
TIME_FORMAT = ""H:i""  # 14:30
DATETIME_FORMAT = ""l d F Y H:i""  # Mercoledì 25 Ottobre 2006 14:30
YEAR_MONTH_FORMAT = ""F Y""  # Ottobre 2006
MONTH_DAY_FORMAT = ""j F""  # 25 Ottobre
SHORT_DATE_FORMAT = ""d/m/Y""  # 25/12/2009
SHORT_DATETIME_FORMAT = ""d/m/Y H:i""  # 25/10/2009 14:30
FIRST_DAY_OF_WEEK = 1  # Lunedì

# The *_INPUT_FORMATS strings use the Python strftime format syntax,
# see https://docs.python.org/library/datetime.html#strftime-strptime-behavior
DATE_INPUT_FORMATS = [
    ""%d/%m/%Y"",  # '25/10/2006'
    ""%Y/%m/%d"",  # '2006/10/25'
    ""%d-%m-%Y"",  # '25-10-2006'
    ""%Y-%m-%d"",  # '2006-10-25'
    ""%d-%m-%y"",  # '25-10-06'
    ""%d/%m/%y"",  # '25/10/06'
]
DATETIME_INPUT_FORMATS = [
    ""%d/%m/%Y %H:%M:%S"",  # '25/10/2006 14:30:59'
    ""%d/%m/%Y %H:%M:%S.%f"",  # '25/10/2006 14:30:59.000200'
    ""%d/%m/%Y %H:%M"",  # '25/10/2006 14:30'
    ""%d/%m/%y %H:%M:%S"",  # '25/10/06 14:30:59'
    ""%d/%m/%y %H:%M:%S.%f"",  # '25/10/06 14:30:59.000200'
    ""%d/%m/%y %H:%M"",  # '25/10/06 14:30'
    ""%Y-%m-%d %H:%M:%S"",  # '2006-10-25 14:30:59'
    ""%Y-%m-%d %H:%M:%S.%f"",  # '2006-10-25 14:30:59.000200'
    ""%Y-%m-%d %H:%M"",  # '2006-10-25 14:30'
    ""%d-%m-%Y %H:%M:%S"",  # '25-10-2006 14:30:59'
    ""%d-%m-%Y %H:%M:%S.%f"",  # '25-10-2006 14:30:59.000200'
    ""%d-%m-%Y %H:%M"",  # '25-10-2006 14:30'
    ""%d-%m-%y %H:%M:%S"",  # '25-10-06 14:30:59'
    ""%d-%m-%y %H:%M:%S.%f"",  # '25-10-06 14:30:59.000200'
    ""%d-%m-%y %H:%M"",  # '25-10-06 14:30'
]
DECIMAL_SEPARATOR = "",""
THOUSAND_SEPARATOR = "".""
NUMBER_GROUPING = 3"
434	donghui	1	"# This file is distributed under the same license as the Django package.
#
# The *_FORMAT strings use the Django date format syntax,
# see https://docs.djangoproject.com/en/dev/ref/templates/builtins/#date
DATE_FORMAT = ""d F Y""  # 25 Ottobre 2006
TIME_FORMAT = ""H:i""  # 14:30
DATETIME_FORMAT = ""l d F Y H:i""  # Mercoledì 25 Ottobre 2006 14:30
YEAR_MONTH_FORMAT = ""F Y""  # Ottobre 2006
MONTH_DAY_FORMAT = ""j F""  # 25 Ottobre
SHORT_DATE_FORMAT = ""d/m/Y""  # 25/12/2009
SHORT_DATETIME_FORMAT = ""d/m/Y H:i""  # 25/10/2009 14:30
FIRST_DAY_OF_WEEK = 1  # Lunedì

# The *_INPUT_FORMATS strings use the Python strftime format syntax,
# see https://docs.python.org/library/datetime.html#strftime-strptime-behavior
DATE_INPUT_FORMATS = [
    ""%d/%m/%Y"",  # '25/10/2006'
    ""%Y/%m/%d"",  # '2006/10/25'
    ""%d-%m-%Y"",  # '25-10-2006'
    ""%Y-%m-%d"",  # '2006-10-25'
    ""%d-%m-%y"",  # '25-10-06'
    ""%d/%m/%y"",  # '25/10/06'
]
DATETIME_INPUT_FORMATS = [
    ""%d/%m/%Y %H:%M:%S"",  # '25/10/2006 14:30:59'
    ""%d/%m/%Y %H:%M:%S.%f"",  # '25/10/2006 14:30:59.000200'
    ""%d/%m/%Y %H:%M"",  # '25/10/2006 14:30'
    ""%d/%m/%y %H:%M:%S"",  # '25/10/06 14:30:59'
    ""%d/%m/%y %H:%M:%S.%f"",  # '25/10/06 14:30:59.000200'
    ""%d/%m/%y %H:%M"",  # '25/10/06 14:30'
    ""%Y-%m-%d %H:%M:%S"",  # '2006-10-25 14:30:59'
    ""%Y-%m-%d %H:%M:%S.%f"",  # '2006-10-25 14:30:59.000200'
    ""%Y-%m-%d %H:%M"",  # '2006-10-25 14:30'
    ""%d-%m-%Y %H:%M:%S"",  # '25-10-2006 14:30:59'
    ""%d-%m-%Y %H:%M:%S.%f"",  # '25-10-2006 14:30:59.000200'
    ""%d-%m-%Y %H:%M"",  # '25-10-2006 14:30'
    ""%d-%m-%y %H:%M:%S"",  # '25-10-06 14:30:59'
    ""%d-%m-%y %H:%M:%S.%f"",  # '25-10-06 14:30:59.000200'
    ""%d-%m-%y %H:%M"",  # '25-10-06 14:30'
]
DECIMAL_SEPARATOR = "",""
THOUSAND_SEPARATOR = "".""
NUMBER_GROUPING = 3"
400	jackson	4	"""""""Stuff that differs in different Python versions and platform
distributions.""""""

import logging
import os
import sys

__all__ = [""get_path_uid"", ""stdlib_pkgs"", ""WINDOWS""]


logger = logging.getLogger(__name__)


def has_tls() -> bool:
    try:
        import _ssl  # noqa: F401  # ignore unused

        return True
    except ImportError:
        pass

    from pip._vendor.urllib3.util import IS_PYOPENSSL

    return IS_PYOPENSSL


def get_path_uid(path: str) -> int:
    """"""
    Return path's uid.

    Does not follow symlinks:
        https://github.com/pypa/pip/pull/935#discussion_r5307003

    Placed this function in compat due to differences on AIX and
    Jython, that should eventually go away.

    :raises OSError: When path is a symlink or can't be read.
    """"""
    if hasattr(os, ""O_NOFOLLOW""):
        fd = os.open(path, os.O_RDONLY | os.O_NOFOLLOW)
        file_uid = os.fstat(fd).st_uid
        os.close(fd)
    else:  # AIX and Jython
        # WARNING: time of check vulnerability, but best we can do w/o NOFOLLOW
        if not os.path.islink(path):
            # older versions of Jython don't have `os.fstat`
            file_uid = os.stat(path).st_uid
        else:
            # raise OSError for parity with os.O_NOFOLLOW above
            raise OSError(f""{path} is a symlink; Will not return uid for symlinks"")
    return file_uid


# packages in the stdlib that may have installation metadata, but should not be
# considered 'installed'.  this theoretically could be determined based on
# dist.location (py27:`sysconfig.get_paths()['stdlib']`,
# py26:sysconfig.get_config_vars('LIBDEST')), but fear platform variation may
# make this ineffective, so hard-coding
stdlib_pkgs = {""python"", ""wsgiref"", ""argparse""}


# windows detection, covers cpython and ironpython
WINDOWS = sys.platform.startswith(""win"") or (sys.platform == ""cli"" and os.name == ""nt"")"
400	donghui	3	"""""""Stuff that differs in different Python versions and platform
distributions.""""""

import logging
import os
import sys

__all__ = [""get_path_uid"", ""stdlib_pkgs"", ""WINDOWS""]


logger = logging.getLogger(__name__)


def has_tls() -> bool:
    try:
        import _ssl  # noqa: F401  # ignore unused

        return True
    except ImportError:
        pass

    from pip._vendor.urllib3.util import IS_PYOPENSSL

    return IS_PYOPENSSL


def get_path_uid(path: str) -> int:
    """"""
    Return path's uid.

    Does not follow symlinks:
        https://github.com/pypa/pip/pull/935#discussion_r5307003

    Placed this function in compat due to differences on AIX and
    Jython, that should eventually go away.

    :raises OSError: When path is a symlink or can't be read.
    """"""
    if hasattr(os, ""O_NOFOLLOW""):
        fd = os.open(path, os.O_RDONLY | os.O_NOFOLLOW)
        file_uid = os.fstat(fd).st_uid
        os.close(fd)
    else:  # AIX and Jython
        # WARNING: time of check vulnerability, but best we can do w/o NOFOLLOW
        if not os.path.islink(path):
            # older versions of Jython don't have `os.fstat`
            file_uid = os.stat(path).st_uid
        else:
            # raise OSError for parity with os.O_NOFOLLOW above
            raise OSError(f""{path} is a symlink; Will not return uid for symlinks"")
    return file_uid


# packages in the stdlib that may have installation metadata, but should not be
# considered 'installed'.  this theoretically could be determined based on
# dist.location (py27:`sysconfig.get_paths()['stdlib']`,
# py26:sysconfig.get_config_vars('LIBDEST')), but fear platform variation may
# make this ineffective, so hard-coding
stdlib_pkgs = {""python"", ""wsgiref"", ""argparse""}


# windows detection, covers cpython and ironpython
WINDOWS = sys.platform.startswith(""win"") or (sys.platform == ""cli"" and os.name == ""nt"")"
451	jackson	2	"import _plotly_utils.basevalidators


class HoverlabelValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(self, plotly_name=""hoverlabel"", parent_name=""contour"", **kwargs):
        super(HoverlabelValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Hoverlabel""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            align
                Sets the horizontal alignment of the text
                content within hover label box. Has an effect
                only if the hover label text spans more two or
                more lines
            alignsrc
                Sets the source reference on Chart Studio Cloud
                for `align`.
            bgcolor
                Sets the background color of the hover labels
                for this trace
            bgcolorsrc
                Sets the source reference on Chart Studio Cloud
                for `bgcolor`.
            bordercolor
                Sets the border color of the hover labels for
                this trace.
            bordercolorsrc
                Sets the source reference on Chart Studio Cloud
                for `bordercolor`.
            font
                Sets the font used in hover labels.
            namelength
                Sets the default length (in number of
                characters) of the trace name in the hover
                labels for all traces. -1 shows the whole name
                regardless of length. 0-3 shows the first 0-3
                characters, and an integer >3 will show the
                whole name if it is less than that many
                characters, but if it is longer, will truncate
                to `namelength - 3` characters and add an
                ellipsis.
            namelengthsrc
                Sets the source reference on Chart Studio Cloud
                for `namelength`.
"""""",
            ),
            **kwargs,
        )"
451	donghui	1	"import _plotly_utils.basevalidators


class HoverlabelValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(self, plotly_name=""hoverlabel"", parent_name=""contour"", **kwargs):
        super(HoverlabelValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Hoverlabel""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            align
                Sets the horizontal alignment of the text
                content within hover label box. Has an effect
                only if the hover label text spans more two or
                more lines
            alignsrc
                Sets the source reference on Chart Studio Cloud
                for `align`.
            bgcolor
                Sets the background color of the hover labels
                for this trace
            bgcolorsrc
                Sets the source reference on Chart Studio Cloud
                for `bgcolor`.
            bordercolor
                Sets the border color of the hover labels for
                this trace.
            bordercolorsrc
                Sets the source reference on Chart Studio Cloud
                for `bordercolor`.
            font
                Sets the font used in hover labels.
            namelength
                Sets the default length (in number of
                characters) of the trace name in the hover
                labels for all traces. -1 shows the whole name
                regardless of length. 0-3 shows the first 0-3
                characters, and an integer >3 will show the
                whole name if it is less than that many
                characters, but if it is longer, will truncate
                to `namelength - 3` characters and add an
                ellipsis.
            namelengthsrc
                Sets the source reference on Chart Studio Cloud
                for `namelength`.
"""""",
            ),
            **kwargs,
        )"
511	jackson	3	"#!/usr/bin/env python3
""""""Session authentication with expiration
and storage support module for the API.
""""""
from flask import request
from datetime import datetime, timedelta

from models.user_session import UserSession
from .session_exp_auth import SessionExpAuth


class SessionDBAuth(SessionExpAuth):
    """"""Session authentication class with expiration and storage support.
    """"""

    def create_session(self, user_id=None) -> str:
        """"""Creates and stores a session id for the user.
        """"""
        session_id = super().create_session(user_id)
        if type(session_id) == str:
            kwargs = {
                'user_id': user_id,
                'session_id': session_id,
            }
            user_session = UserSession(**kwargs)
            user_session.save()
            return session_id

    def user_id_for_session_id(self, session_id=None):
        """"""Retrieves the user id of the user associated with
        a given session id.
        """"""
        try:
            sessions = UserSession.search({'session_id': session_id})
        except Exception:
            return None
        if len(sessions) <= 0:
            return None
        cur_time = datetime.now()
        time_span = timedelta(seconds=self.session_duration)
        exp_time = sessions[0].created_at + time_span
        if exp_time < cur_time:
            return None
        return sessions[0].user_id

    def destroy_session(self, request=None) -> bool:
        """"""Destroys an authenticated session.
        """"""
        session_id = self.session_cookie(request)
        try:
            sessions = UserSession.search({'session_id': session_id})
        except Exception:
            return False
        if len(sessions) <= 0:
            return False
        sessions[0].remove()
        return True"
511	donghui	3	"#!/usr/bin/env python3
""""""Session authentication with expiration
and storage support module for the API.
""""""
from flask import request
from datetime import datetime, timedelta

from models.user_session import UserSession
from .session_exp_auth import SessionExpAuth


class SessionDBAuth(SessionExpAuth):
    """"""Session authentication class with expiration and storage support.
    """"""

    def create_session(self, user_id=None) -> str:
        """"""Creates and stores a session id for the user.
        """"""
        session_id = super().create_session(user_id)
        if type(session_id) == str:
            kwargs = {
                'user_id': user_id,
                'session_id': session_id,
            }
            user_session = UserSession(**kwargs)
            user_session.save()
            return session_id

    def user_id_for_session_id(self, session_id=None):
        """"""Retrieves the user id of the user associated with
        a given session id.
        """"""
        try:
            sessions = UserSession.search({'session_id': session_id})
        except Exception:
            return None
        if len(sessions) <= 0:
            return None
        cur_time = datetime.now()
        time_span = timedelta(seconds=self.session_duration)
        exp_time = sessions[0].created_at + time_span
        if exp_time < cur_time:
            return None
        return sessions[0].user_id

    def destroy_session(self, request=None) -> bool:
        """"""Destroys an authenticated session.
        """"""
        session_id = self.session_cookie(request)
        try:
            sessions = UserSession.search({'session_id': session_id})
        except Exception:
            return False
        if len(sessions) <= 0:
            return False
        sessions[0].remove()
        return True"
500	jackson	3	"from MySQLdb.constants import FIELD_TYPE

from django.contrib.gis.gdal import OGRGeomType
from django.db.backends.mysql.introspection import DatabaseIntrospection


class MySQLIntrospection(DatabaseIntrospection):
    # Updating the data_types_reverse dictionary with the appropriate
    # type for Geometry fields.
    data_types_reverse = DatabaseIntrospection.data_types_reverse.copy()
    data_types_reverse[FIELD_TYPE.GEOMETRY] = 'GeometryField'

    def get_geometry_type(self, table_name, description):
        with self.connection.cursor() as cursor:
            # In order to get the specific geometry type of the field,
            # we introspect on the table definition using `DESCRIBE`.
            cursor.execute('DESCRIBE %s' %
                           self.connection.ops.quote_name(table_name))
            # Increment over description info until we get to the geometry
            # column.
            for column, typ, null, key, default, extra in cursor.fetchall():
                if column == description.name:
                    # Using OGRGeomType to convert from OGC name to Django field.
                    # MySQL does not support 3D or SRIDs, so the field params
                    # are empty.
                    field_type = OGRGeomType(typ).django
                    field_params = {}
                    break
        return field_type, field_params

    def supports_spatial_index(self, cursor, table_name):
        # Supported with MyISAM/Aria, or InnoDB on MySQL 5.7.5+/MariaDB 10.2.2+
        storage_engine = self.get_storage_engine(cursor, table_name)
        if storage_engine == 'InnoDB':
            return self.connection.mysql_version >= (
                (10, 2, 2) if self.connection.mysql_is_mariadb else (5, 7, 5)
            )
        return storage_engine in ('MyISAM', 'Aria')"
500	donghui	3	"from MySQLdb.constants import FIELD_TYPE

from django.contrib.gis.gdal import OGRGeomType
from django.db.backends.mysql.introspection import DatabaseIntrospection


class MySQLIntrospection(DatabaseIntrospection):
    # Updating the data_types_reverse dictionary with the appropriate
    # type for Geometry fields.
    data_types_reverse = DatabaseIntrospection.data_types_reverse.copy()
    data_types_reverse[FIELD_TYPE.GEOMETRY] = 'GeometryField'

    def get_geometry_type(self, table_name, description):
        with self.connection.cursor() as cursor:
            # In order to get the specific geometry type of the field,
            # we introspect on the table definition using `DESCRIBE`.
            cursor.execute('DESCRIBE %s' %
                           self.connection.ops.quote_name(table_name))
            # Increment over description info until we get to the geometry
            # column.
            for column, typ, null, key, default, extra in cursor.fetchall():
                if column == description.name:
                    # Using OGRGeomType to convert from OGC name to Django field.
                    # MySQL does not support 3D or SRIDs, so the field params
                    # are empty.
                    field_type = OGRGeomType(typ).django
                    field_params = {}
                    break
        return field_type, field_params

    def supports_spatial_index(self, cursor, table_name):
        # Supported with MyISAM/Aria, or InnoDB on MySQL 5.7.5+/MariaDB 10.2.2+
        storage_engine = self.get_storage_engine(cursor, table_name)
        if storage_engine == 'InnoDB':
            return self.connection.mysql_version >= (
                (10, 2, 2) if self.connection.mysql_is_mariadb else (5, 7, 5)
            )
        return storage_engine in ('MyISAM', 'Aria')"
440	jackson	3	"from typing import List, Optional

from .base import BaseDistribution, BaseEnvironment, FilesystemWheel, MemoryWheel, Wheel

__all__ = [
    ""BaseDistribution"",
    ""BaseEnvironment"",
    ""FilesystemWheel"",
    ""MemoryWheel"",
    ""Wheel"",
    ""get_default_environment"",
    ""get_environment"",
    ""get_wheel_distribution"",
]


def get_default_environment() -> BaseEnvironment:
    """"""Get the default representation for the current environment.

    This returns an Environment instance from the chosen backend. The default
    Environment instance should be built from ``sys.path`` and may use caching
    to share instance state accorss calls.
    """"""
    from .pkg_resources import Environment

    return Environment.default()


def get_environment(paths: Optional[List[str]]) -> BaseEnvironment:
    """"""Get a representation of the environment specified by ``paths``.

    This returns an Environment instance from the chosen backend based on the
    given import paths. The backend must build a fresh instance representing
    the state of installed distributions when this function is called.
    """"""
    from .pkg_resources import Environment

    return Environment.from_paths(paths)


def get_directory_distribution(directory: str) -> BaseDistribution:
    """"""Get the distribution metadata representation in the specified directory.

    This returns a Distribution instance from the chosen backend based on
    the given on-disk ``.dist-info`` directory.
    """"""
    from .pkg_resources import Distribution

    return Distribution.from_directory(directory)


def get_wheel_distribution(wheel: Wheel, canonical_name: str) -> BaseDistribution:
    """"""Get the representation of the specified wheel's distribution metadata.

    This returns a Distribution instance from the chosen backend based on
    the given wheel's ``.dist-info`` directory.

    :param canonical_name: Normalized project name of the given wheel.
    """"""
    from .pkg_resources import Distribution

    return Distribution.from_wheel(wheel, canonical_name)"
440	donghui	3	"from typing import List, Optional

from .base import BaseDistribution, BaseEnvironment, FilesystemWheel, MemoryWheel, Wheel

__all__ = [
    ""BaseDistribution"",
    ""BaseEnvironment"",
    ""FilesystemWheel"",
    ""MemoryWheel"",
    ""Wheel"",
    ""get_default_environment"",
    ""get_environment"",
    ""get_wheel_distribution"",
]


def get_default_environment() -> BaseEnvironment:
    """"""Get the default representation for the current environment.

    This returns an Environment instance from the chosen backend. The default
    Environment instance should be built from ``sys.path`` and may use caching
    to share instance state accorss calls.
    """"""
    from .pkg_resources import Environment

    return Environment.default()


def get_environment(paths: Optional[List[str]]) -> BaseEnvironment:
    """"""Get a representation of the environment specified by ``paths``.

    This returns an Environment instance from the chosen backend based on the
    given import paths. The backend must build a fresh instance representing
    the state of installed distributions when this function is called.
    """"""
    from .pkg_resources import Environment

    return Environment.from_paths(paths)


def get_directory_distribution(directory: str) -> BaseDistribution:
    """"""Get the distribution metadata representation in the specified directory.

    This returns a Distribution instance from the chosen backend based on
    the given on-disk ``.dist-info`` directory.
    """"""
    from .pkg_resources import Distribution

    return Distribution.from_directory(directory)


def get_wheel_distribution(wheel: Wheel, canonical_name: str) -> BaseDistribution:
    """"""Get the representation of the specified wheel's distribution metadata.

    This returns a Distribution instance from the chosen backend based on
    the given wheel's ``.dist-info`` directory.

    :param canonical_name: Normalized project name of the given wheel.
    """"""
    from .pkg_resources import Distribution

    return Distribution.from_wheel(wheel, canonical_name)"
411	jackson	0	"from . import DefaultTable
import sys
import array
import logging


log = logging.getLogger(__name__)


class table__l_o_c_a(DefaultTable.DefaultTable):

    dependencies = [""glyf""]

    def decompile(self, data, ttFont):
        longFormat = ttFont[""head""].indexToLocFormat
        if longFormat:
            format = ""I""
        else:
            format = ""H""
        locations = array.array(format)
        locations.frombytes(data)
        if sys.byteorder != ""big"":
            locations.byteswap()
        if not longFormat:
            l = array.array(""I"")
            for i in range(len(locations)):
                l.append(locations[i] * 2)
            locations = l
        if len(locations) < (ttFont[""maxp""].numGlyphs + 1):
            log.warning(
                ""corrupt 'loca' table, or wrong numGlyphs in 'maxp': %d %d"",
                len(locations) - 1,
                ttFont[""maxp""].numGlyphs,
            )
        self.locations = locations

    def compile(self, ttFont):
        try:
            max_location = max(self.locations)
        except AttributeError:
            self.set([])
            max_location = 0
        if max_location < 0x20000 and all(l % 2 == 0 for l in self.locations):
            locations = array.array(""H"")
            for i in range(len(self.locations)):
                locations.append(self.locations[i] // 2)
            ttFont[""head""].indexToLocFormat = 0
        else:
            locations = array.array(""I"", self.locations)
            ttFont[""head""].indexToLocFormat = 1
        if sys.byteorder != ""big"":
            locations.byteswap()
        return locations.tobytes()

    def set(self, locations):
        self.locations = array.array(""I"", locations)

    def toXML(self, writer, ttFont):
        writer.comment(""The 'loca' table will be calculated by the compiler"")
        writer.newline()

    def __getitem__(self, index):
        return self.locations[index]

    def __len__(self):
        return len(self.locations)"
411	donghui	0	"from . import DefaultTable
import sys
import array
import logging


log = logging.getLogger(__name__)


class table__l_o_c_a(DefaultTable.DefaultTable):

    dependencies = [""glyf""]

    def decompile(self, data, ttFont):
        longFormat = ttFont[""head""].indexToLocFormat
        if longFormat:
            format = ""I""
        else:
            format = ""H""
        locations = array.array(format)
        locations.frombytes(data)
        if sys.byteorder != ""big"":
            locations.byteswap()
        if not longFormat:
            l = array.array(""I"")
            for i in range(len(locations)):
                l.append(locations[i] * 2)
            locations = l
        if len(locations) < (ttFont[""maxp""].numGlyphs + 1):
            log.warning(
                ""corrupt 'loca' table, or wrong numGlyphs in 'maxp': %d %d"",
                len(locations) - 1,
                ttFont[""maxp""].numGlyphs,
            )
        self.locations = locations

    def compile(self, ttFont):
        try:
            max_location = max(self.locations)
        except AttributeError:
            self.set([])
            max_location = 0
        if max_location < 0x20000 and all(l % 2 == 0 for l in self.locations):
            locations = array.array(""H"")
            for i in range(len(self.locations)):
                locations.append(self.locations[i] // 2)
            ttFont[""head""].indexToLocFormat = 0
        else:
            locations = array.array(""I"", self.locations)
            ttFont[""head""].indexToLocFormat = 1
        if sys.byteorder != ""big"":
            locations.byteswap()
        return locations.tobytes()

    def set(self, locations):
        self.locations = array.array(""I"", locations)

    def toXML(self, writer, ttFont):
        writer.comment(""The 'loca' table will be calculated by the compiler"")
        writer.newline()

    def __getitem__(self, index):
        return self.locations[index]

    def __len__(self):
        return len(self.locations)"
425	jackson	0	"from datetime import (
    datetime,
    timedelta,
)

from pandas import (
    DatetimeIndex,
    NaT,
    Timestamp,
)
import pandas._testing as tm


def test_unique(tz_naive_fixture):

    idx = DatetimeIndex([""2017""] * 2, tz=tz_naive_fixture)
    expected = idx[:1]

    result = idx.unique()
    tm.assert_index_equal(result, expected)
    # GH#21737
    # Ensure the underlying data is consistent
    assert result[0] == expected[0]


def test_index_unique(rand_series_with_duplicate_datetimeindex):
    dups = rand_series_with_duplicate_datetimeindex
    index = dups.index

    uniques = index.unique()
    expected = DatetimeIndex(
        [
            datetime(2000, 1, 2),
            datetime(2000, 1, 3),
            datetime(2000, 1, 4),
            datetime(2000, 1, 5),
        ]
    )
    assert uniques.dtype == ""M8[ns]""  # sanity
    tm.assert_index_equal(uniques, expected)
    assert index.nunique() == 4

    # GH#2563
    assert isinstance(uniques, DatetimeIndex)

    dups_local = index.tz_localize(""US/Eastern"")
    dups_local.name = ""foo""
    result = dups_local.unique()
    expected = DatetimeIndex(expected, name=""foo"")
    expected = expected.tz_localize(""US/Eastern"")
    assert result.tz is not None
    assert result.name == ""foo""
    tm.assert_index_equal(result, expected)


def test_index_unique2():
    # NaT, note this is excluded
    arr = [1370745748 + t for t in range(20)] + [NaT.value]
    idx = DatetimeIndex(arr * 3)
    tm.assert_index_equal(idx.unique(), DatetimeIndex(arr))
    assert idx.nunique() == 20
    assert idx.nunique(dropna=False) == 21


def test_index_unique3():
    arr = [
        Timestamp(""2013-06-09 02:42:28"") + timedelta(seconds=t) for t in range(20)
    ] + [NaT]
    idx = DatetimeIndex(arr * 3)
    tm.assert_index_equal(idx.unique(), DatetimeIndex(arr))
    assert idx.nunique() == 20
    assert idx.nunique(dropna=False) == 21


def test_is_unique_monotonic(rand_series_with_duplicate_datetimeindex):
    index = rand_series_with_duplicate_datetimeindex.index
    assert not index.is_unique"
425	donghui	1	"from datetime import (
    datetime,
    timedelta,
)

from pandas import (
    DatetimeIndex,
    NaT,
    Timestamp,
)
import pandas._testing as tm


def test_unique(tz_naive_fixture):

    idx = DatetimeIndex([""2017""] * 2, tz=tz_naive_fixture)
    expected = idx[:1]

    result = idx.unique()
    tm.assert_index_equal(result, expected)
    # GH#21737
    # Ensure the underlying data is consistent
    assert result[0] == expected[0]


def test_index_unique(rand_series_with_duplicate_datetimeindex):
    dups = rand_series_with_duplicate_datetimeindex
    index = dups.index

    uniques = index.unique()
    expected = DatetimeIndex(
        [
            datetime(2000, 1, 2),
            datetime(2000, 1, 3),
            datetime(2000, 1, 4),
            datetime(2000, 1, 5),
        ]
    )
    assert uniques.dtype == ""M8[ns]""  # sanity
    tm.assert_index_equal(uniques, expected)
    assert index.nunique() == 4

    # GH#2563
    assert isinstance(uniques, DatetimeIndex)

    dups_local = index.tz_localize(""US/Eastern"")
    dups_local.name = ""foo""
    result = dups_local.unique()
    expected = DatetimeIndex(expected, name=""foo"")
    expected = expected.tz_localize(""US/Eastern"")
    assert result.tz is not None
    assert result.name == ""foo""
    tm.assert_index_equal(result, expected)


def test_index_unique2():
    # NaT, note this is excluded
    arr = [1370745748 + t for t in range(20)] + [NaT.value]
    idx = DatetimeIndex(arr * 3)
    tm.assert_index_equal(idx.unique(), DatetimeIndex(arr))
    assert idx.nunique() == 20
    assert idx.nunique(dropna=False) == 21


def test_index_unique3():
    arr = [
        Timestamp(""2013-06-09 02:42:28"") + timedelta(seconds=t) for t in range(20)
    ] + [NaT]
    idx = DatetimeIndex(arr * 3)
    tm.assert_index_equal(idx.unique(), DatetimeIndex(arr))
    assert idx.nunique() == 20
    assert idx.nunique(dropna=False) == 21


def test_is_unique_monotonic(rand_series_with_duplicate_datetimeindex):
    index = rand_series_with_duplicate_datetimeindex.index
    assert not index.is_unique"
474	jackson	1	"# Copyright 2018-present Tellabs, Inc.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
"""""" Tellabs vendor-specific OMCI Entities""""""

import inspect
import structlog
import sys

from scapy.fields import ShortField, IntField, ByteField, StrFixedLenField
from voltha.extensions.omci.omci_entities import EntityClassAttribute, \
    AttributeAccess, OmciNullPointer, EntityOperations, EntityClass

log = structlog.get_logger()

# abbreviations
ECA = EntityClassAttribute
AA = AttributeAccess
OP = EntityOperations

#################################################################################
# entity class lookup table from entity_class values
_onu_entity_classes_name_map = dict(
    inspect.getmembers(sys.modules[__name__], lambda o:
    inspect.isclass(o) and issubclass(o, EntityClass) and o is not EntityClass)
)
_onu_custom_entity_classes = [c for c in _onu_entity_classes_name_map.itervalues()]
_onu_custom_entity_id_to_class_map = dict()


def onu_custom_me_entities():
    log.info('onu_custom_me_entities')

    if len(_onu_custom_entity_id_to_class_map) == 0:
        for entity_class in _onu_custom_entity_classes:
            log.info('adding-custom-me', class_id=entity_class.class_id)
            assert entity_class.class_id not in _onu_custom_entity_id_to_class_map, \
                ""Class ID '{}' already exists in the class map"".format(entity_class.class_id)
            _onu_custom_entity_id_to_class_map[entity_class.class_id] = entity_class

    log.info('onu_custom_me_entities', map=_onu_custom_entity_id_to_class_map)
    return _onu_custom_entity_id_to_class_map
"
474	donghui	1	"# Copyright 2018-present Tellabs, Inc.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
"""""" Tellabs vendor-specific OMCI Entities""""""

import inspect
import structlog
import sys

from scapy.fields import ShortField, IntField, ByteField, StrFixedLenField
from voltha.extensions.omci.omci_entities import EntityClassAttribute, \
    AttributeAccess, OmciNullPointer, EntityOperations, EntityClass

log = structlog.get_logger()

# abbreviations
ECA = EntityClassAttribute
AA = AttributeAccess
OP = EntityOperations

#################################################################################
# entity class lookup table from entity_class values
_onu_entity_classes_name_map = dict(
    inspect.getmembers(sys.modules[__name__], lambda o:
    inspect.isclass(o) and issubclass(o, EntityClass) and o is not EntityClass)
)
_onu_custom_entity_classes = [c for c in _onu_entity_classes_name_map.itervalues()]
_onu_custom_entity_id_to_class_map = dict()


def onu_custom_me_entities():
    log.info('onu_custom_me_entities')

    if len(_onu_custom_entity_id_to_class_map) == 0:
        for entity_class in _onu_custom_entity_classes:
            log.info('adding-custom-me', class_id=entity_class.class_id)
            assert entity_class.class_id not in _onu_custom_entity_id_to_class_map, \
                ""Class ID '{}' already exists in the class map"".format(entity_class.class_id)
            _onu_custom_entity_id_to_class_map[entity_class.class_id] = entity_class

    log.info('onu_custom_me_entities', map=_onu_custom_entity_id_to_class_map)
    return _onu_custom_entity_id_to_class_map
"
487	jackson	0	"import numpy as np

from pandas import (
    Series,
    Timestamp,
    date_range,
)
import pandas._testing as tm
from pandas.api.types import is_scalar


class TestSeriesSearchSorted:
    def test_searchsorted(self):
        ser = Series([1, 2, 3])

        result = ser.searchsorted(1, side=""left"")
        assert is_scalar(result)
        assert result == 0

        result = ser.searchsorted(1, side=""right"")
        assert is_scalar(result)
        assert result == 1

    def test_searchsorted_numeric_dtypes_scalar(self):
        ser = Series([1, 2, 90, 1000, 3e9])
        res = ser.searchsorted(30)
        assert is_scalar(res)
        assert res == 2

        res = ser.searchsorted([30])
        exp = np.array([2], dtype=np.intp)
        tm.assert_numpy_array_equal(res, exp)

    def test_searchsorted_numeric_dtypes_vector(self):
        ser = Series([1, 2, 90, 1000, 3e9])
        res = ser.searchsorted([91, 2e6])
        exp = np.array([3, 4], dtype=np.intp)
        tm.assert_numpy_array_equal(res, exp)

    def test_searchsorted_datetime64_scalar(self):
        ser = Series(date_range(""20120101"", periods=10, freq=""2D""))
        val = Timestamp(""20120102"")
        res = ser.searchsorted(val)
        assert is_scalar(res)
        assert res == 1

    def test_searchsorted_datetime64_scalar_mixed_timezones(self):
        # GH 30086
        ser = Series(date_range(""20120101"", periods=10, freq=""2D"", tz=""UTC""))
        val = Timestamp(""20120102"", tz=""America/New_York"")
        res = ser.searchsorted(val)
        assert is_scalar(res)
        assert res == 1

    def test_searchsorted_datetime64_list(self):
        ser = Series(date_range(""20120101"", periods=10, freq=""2D""))
        vals = [Timestamp(""20120102""), Timestamp(""20120104"")]
        res = ser.searchsorted(vals)
        exp = np.array([1, 2], dtype=np.intp)
        tm.assert_numpy_array_equal(res, exp)

    def test_searchsorted_sorter(self):
        # GH8490
        ser = Series([3, 1, 2])
        res = ser.searchsorted([0, 3], sorter=np.argsort(ser))
        exp = np.array([0, 2], dtype=np.intp)
        tm.assert_numpy_array_equal(res, exp)"
487	donghui	0	"import numpy as np

from pandas import (
    Series,
    Timestamp,
    date_range,
)
import pandas._testing as tm
from pandas.api.types import is_scalar


class TestSeriesSearchSorted:
    def test_searchsorted(self):
        ser = Series([1, 2, 3])

        result = ser.searchsorted(1, side=""left"")
        assert is_scalar(result)
        assert result == 0

        result = ser.searchsorted(1, side=""right"")
        assert is_scalar(result)
        assert result == 1

    def test_searchsorted_numeric_dtypes_scalar(self):
        ser = Series([1, 2, 90, 1000, 3e9])
        res = ser.searchsorted(30)
        assert is_scalar(res)
        assert res == 2

        res = ser.searchsorted([30])
        exp = np.array([2], dtype=np.intp)
        tm.assert_numpy_array_equal(res, exp)

    def test_searchsorted_numeric_dtypes_vector(self):
        ser = Series([1, 2, 90, 1000, 3e9])
        res = ser.searchsorted([91, 2e6])
        exp = np.array([3, 4], dtype=np.intp)
        tm.assert_numpy_array_equal(res, exp)

    def test_searchsorted_datetime64_scalar(self):
        ser = Series(date_range(""20120101"", periods=10, freq=""2D""))
        val = Timestamp(""20120102"")
        res = ser.searchsorted(val)
        assert is_scalar(res)
        assert res == 1

    def test_searchsorted_datetime64_scalar_mixed_timezones(self):
        # GH 30086
        ser = Series(date_range(""20120101"", periods=10, freq=""2D"", tz=""UTC""))
        val = Timestamp(""20120102"", tz=""America/New_York"")
        res = ser.searchsorted(val)
        assert is_scalar(res)
        assert res == 1

    def test_searchsorted_datetime64_list(self):
        ser = Series(date_range(""20120101"", periods=10, freq=""2D""))
        vals = [Timestamp(""20120102""), Timestamp(""20120104"")]
        res = ser.searchsorted(vals)
        exp = np.array([1, 2], dtype=np.intp)
        tm.assert_numpy_array_equal(res, exp)

    def test_searchsorted_sorter(self):
        # GH8490
        ser = Series([3, 1, 2])
        res = ser.searchsorted([0, 3], sorter=np.argsort(ser))
        exp = np.array([0, 2], dtype=np.intp)
        tm.assert_numpy_array_equal(res, exp)"
398	jackson	3	"""""""
    pygments.styles.vim
    ~~~~~~~~~~~~~~~~~~~

    A highlighting style for Pygments, inspired by vim.

    :copyright: Copyright 2006-2022 by the Pygments team, see AUTHORS.
    :license: BSD, see LICENSE for details.
""""""

from pygments.style import Style
from pygments.token import Keyword, Name, Comment, String, Error, \
     Number, Operator, Generic, Whitespace, Token


class VimStyle(Style):
    """"""
    Styles somewhat like vim 7.0
    """"""

    background_color = ""#000000""
    highlight_color = ""#222222""

    styles = {
        Token:                     ""#cccccc"",
        Whitespace:                """",
        Comment:                   ""#000080"",
        Comment.Preproc:           """",
        Comment.Special:           ""bold #cd0000"",

        Keyword:                   ""#cdcd00"",
        Keyword.Declaration:       ""#00cd00"",
        Keyword.Namespace:         ""#cd00cd"",
        Keyword.Pseudo:            """",
        Keyword.Type:              ""#00cd00"",

        Operator:                  ""#3399cc"",
        Operator.Word:             ""#cdcd00"",

        Name:                      """",
        Name.Class:                ""#00cdcd"",
        Name.Builtin:              ""#cd00cd"",
        Name.Exception:            ""bold #666699"",
        Name.Variable:             ""#00cdcd"",

        String:                    ""#cd0000"",
        Number:                    ""#cd00cd"",

        Generic.Heading:           ""bold #000080"",
        Generic.Subheading:        ""bold #800080"",
        Generic.Deleted:           ""#cd0000"",
        Generic.Inserted:          ""#00cd00"",
        Generic.Error:             ""#FF0000"",
        Generic.Emph:              ""italic"",
        Generic.Strong:            ""bold"",
        Generic.Prompt:            ""bold #000080"",
        Generic.Output:            ""#888"",
        Generic.Traceback:         ""#04D"",

        Error:                     ""border:#FF0000""
    }"
398	donghui	1	"""""""
    pygments.styles.vim
    ~~~~~~~~~~~~~~~~~~~

    A highlighting style for Pygments, inspired by vim.

    :copyright: Copyright 2006-2022 by the Pygments team, see AUTHORS.
    :license: BSD, see LICENSE for details.
""""""

from pygments.style import Style
from pygments.token import Keyword, Name, Comment, String, Error, \
     Number, Operator, Generic, Whitespace, Token


class VimStyle(Style):
    """"""
    Styles somewhat like vim 7.0
    """"""

    background_color = ""#000000""
    highlight_color = ""#222222""

    styles = {
        Token:                     ""#cccccc"",
        Whitespace:                """",
        Comment:                   ""#000080"",
        Comment.Preproc:           """",
        Comment.Special:           ""bold #cd0000"",

        Keyword:                   ""#cdcd00"",
        Keyword.Declaration:       ""#00cd00"",
        Keyword.Namespace:         ""#cd00cd"",
        Keyword.Pseudo:            """",
        Keyword.Type:              ""#00cd00"",

        Operator:                  ""#3399cc"",
        Operator.Word:             ""#cdcd00"",

        Name:                      """",
        Name.Class:                ""#00cdcd"",
        Name.Builtin:              ""#cd00cd"",
        Name.Exception:            ""bold #666699"",
        Name.Variable:             ""#00cdcd"",

        String:                    ""#cd0000"",
        Number:                    ""#cd00cd"",

        Generic.Heading:           ""bold #000080"",
        Generic.Subheading:        ""bold #800080"",
        Generic.Deleted:           ""#cd0000"",
        Generic.Inserted:          ""#00cd00"",
        Generic.Error:             ""#FF0000"",
        Generic.Emph:              ""italic"",
        Generic.Strong:            ""bold"",
        Generic.Prompt:            ""bold #000080"",
        Generic.Output:            ""#888"",
        Generic.Traceback:         ""#04D"",

        Error:                     ""border:#FF0000""
    }"
388	jackson	3	"class BaseInstanceLoader:
    """"""
    Base abstract implementation of instance loader.
    """"""

    def __init__(self, resource, dataset=None):
        self.resource = resource
        self.dataset = dataset

    def get_instance(self, row):
        raise NotImplementedError


class ModelInstanceLoader(BaseInstanceLoader):
    """"""
    Instance loader for Django model.

    Lookup for model instance by ``import_id_fields``.
    """"""

    def get_queryset(self):
        return self.resource.get_queryset()

    def get_instance(self, row):
        try:
            params = {}
            for key in self.resource.get_import_id_fields():
                field = self.resource.fields[key]
                params[field.attribute] = field.clean(row)
            if params:
                return self.get_queryset().get(**params)
            else:
                return None
        except self.resource._meta.model.DoesNotExist:
            return None


class CachedInstanceLoader(ModelInstanceLoader):
    """"""
    Loads all possible model instances in dataset avoid hitting database for
    every ``get_instance`` call.

    This instance loader work only when there is one ``import_id_fields``
    field.
    """"""

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)

        pk_field_name = self.resource.get_import_id_fields()[0]
        self.pk_field = self.resource.fields[pk_field_name]

        ids = [self.pk_field.clean(row) for row in self.dataset.dict]
        qs = self.get_queryset().filter(**{
            ""%s__in"" % self.pk_field.attribute: ids
            })

        self.all_instances = {
            self.pk_field.get_value(instance): instance
            for instance in qs
        }

    def get_instance(self, row):
        return self.all_instances.get(self.pk_field.clean(row))"
388	donghui	2	"class BaseInstanceLoader:
    """"""
    Base abstract implementation of instance loader.
    """"""

    def __init__(self, resource, dataset=None):
        self.resource = resource
        self.dataset = dataset

    def get_instance(self, row):
        raise NotImplementedError


class ModelInstanceLoader(BaseInstanceLoader):
    """"""
    Instance loader for Django model.

    Lookup for model instance by ``import_id_fields``.
    """"""

    def get_queryset(self):
        return self.resource.get_queryset()

    def get_instance(self, row):
        try:
            params = {}
            for key in self.resource.get_import_id_fields():
                field = self.resource.fields[key]
                params[field.attribute] = field.clean(row)
            if params:
                return self.get_queryset().get(**params)
            else:
                return None
        except self.resource._meta.model.DoesNotExist:
            return None


class CachedInstanceLoader(ModelInstanceLoader):
    """"""
    Loads all possible model instances in dataset avoid hitting database for
    every ``get_instance`` call.

    This instance loader work only when there is one ``import_id_fields``
    field.
    """"""

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)

        pk_field_name = self.resource.get_import_id_fields()[0]
        self.pk_field = self.resource.fields[pk_field_name]

        ids = [self.pk_field.clean(row) for row in self.dataset.dict]
        qs = self.get_queryset().filter(**{
            ""%s__in"" % self.pk_field.attribute: ids
            })

        self.all_instances = {
            self.pk_field.get_value(instance): instance
            for instance in qs
        }

    def get_instance(self, row):
        return self.all_instances.get(self.pk_field.clean(row))"
497	jackson	3	"""""""miscellaneous zmq_utils wrapping""""""

# Copyright (C) PyZMQ Developers
# Distributed under the terms of the Modified BSD License.

from zmq.error import InterruptedSystemCall, _check_rc, _check_version

from ._cffi import ffi
from ._cffi import lib as C


def has(capability):
    """"""Check for zmq capability by name (e.g. 'ipc', 'curve')

    .. versionadded:: libzmq-4.1
    .. versionadded:: 14.1
    """"""
    _check_version((4, 1), 'zmq.has')
    if isinstance(capability, str):
        capability = capability.encode('utf8')
    return bool(C.zmq_has(capability))


def curve_keypair():
    """"""generate a Z85 key pair for use with zmq.CURVE security

    Requires libzmq (≥ 4.0) to have been built with CURVE support.

    Returns
    -------
    (public, secret) : two bytestrings
        The public and private key pair as 40 byte z85-encoded bytestrings.
    """"""
    _check_version((3, 2), ""curve_keypair"")
    public = ffi.new('char[64]')
    private = ffi.new('char[64]')
    rc = C.zmq_curve_keypair(public, private)
    _check_rc(rc)
    return ffi.buffer(public)[:40], ffi.buffer(private)[:40]


def curve_public(private):
    """"""Compute the public key corresponding to a private key for use
    with zmq.CURVE security

    Requires libzmq (≥ 4.2) to have been built with CURVE support.

    Parameters
    ----------
    private
        The private key as a 40 byte z85-encoded bytestring
    Returns
    -------
    bytestring
        The public key as a 40 byte z85-encoded bytestring.
    """"""
    if isinstance(private, str):
        private = private.encode('utf8')
    _check_version((4, 2), ""curve_public"")
    public = ffi.new('char[64]')
    rc = C.zmq_curve_public(public, private)
    _check_rc(rc)
    return ffi.buffer(public)[:40]


def _retry_sys_call(f, *args, **kwargs):
    """"""make a call, retrying if interrupted with EINTR""""""
    while True:
        rc = f(*args)
        try:
            _check_rc(rc)
        except InterruptedSystemCall:
            continue
        else:
            break


__all__ = ['has', 'curve_keypair', 'curve_public']"
497	donghui	3	"""""""miscellaneous zmq_utils wrapping""""""

# Copyright (C) PyZMQ Developers
# Distributed under the terms of the Modified BSD License.

from zmq.error import InterruptedSystemCall, _check_rc, _check_version

from ._cffi import ffi
from ._cffi import lib as C


def has(capability):
    """"""Check for zmq capability by name (e.g. 'ipc', 'curve')

    .. versionadded:: libzmq-4.1
    .. versionadded:: 14.1
    """"""
    _check_version((4, 1), 'zmq.has')
    if isinstance(capability, str):
        capability = capability.encode('utf8')
    return bool(C.zmq_has(capability))


def curve_keypair():
    """"""generate a Z85 key pair for use with zmq.CURVE security

    Requires libzmq (≥ 4.0) to have been built with CURVE support.

    Returns
    -------
    (public, secret) : two bytestrings
        The public and private key pair as 40 byte z85-encoded bytestrings.
    """"""
    _check_version((3, 2), ""curve_keypair"")
    public = ffi.new('char[64]')
    private = ffi.new('char[64]')
    rc = C.zmq_curve_keypair(public, private)
    _check_rc(rc)
    return ffi.buffer(public)[:40], ffi.buffer(private)[:40]


def curve_public(private):
    """"""Compute the public key corresponding to a private key for use
    with zmq.CURVE security

    Requires libzmq (≥ 4.2) to have been built with CURVE support.

    Parameters
    ----------
    private
        The private key as a 40 byte z85-encoded bytestring
    Returns
    -------
    bytestring
        The public key as a 40 byte z85-encoded bytestring.
    """"""
    if isinstance(private, str):
        private = private.encode('utf8')
    _check_version((4, 2), ""curve_public"")
    public = ffi.new('char[64]')
    rc = C.zmq_curve_public(public, private)
    _check_rc(rc)
    return ffi.buffer(public)[:40]


def _retry_sys_call(f, *args, **kwargs):
    """"""make a call, retrying if interrupted with EINTR""""""
    while True:
        rc = f(*args)
        try:
            _check_rc(rc)
        except InterruptedSystemCall:
            continue
        else:
            break


__all__ = ['has', 'curve_keypair', 'curve_public']"
464	jackson	1	"import os

import pytest

import pandas.compat as compat

import pandas._testing as tm


def test_rands():
    r = tm.rands(10)
    assert len(r) == 10


def test_rands_array_1d():
    arr = tm.rands_array(5, size=10)
    assert arr.shape == (10,)
    assert len(arr[0]) == 5


def test_rands_array_2d():
    arr = tm.rands_array(7, size=(10, 10))
    assert arr.shape == (10, 10)
    assert len(arr[1, 1]) == 7


def test_numpy_err_state_is_default():
    expected = {""over"": ""warn"", ""divide"": ""warn"", ""invalid"": ""warn"", ""under"": ""ignore""}
    import numpy as np

    # The error state should be unchanged after that import.
    assert np.geterr() == expected


def test_convert_rows_list_to_csv_str():
    rows_list = [""aaa"", ""bbb"", ""ccc""]
    ret = tm.convert_rows_list_to_csv_str(rows_list)

    if compat.is_platform_windows():
        expected = ""aaa\r\nbbb\r\nccc\r\n""
    else:
        expected = ""aaa\nbbb\nccc\n""

    assert ret == expected


def test_create_temp_directory():
    with tm.ensure_clean_dir() as path:
        assert os.path.exists(path)
        assert os.path.isdir(path)
    assert not os.path.exists(path)


@pytest.mark.parametrize(""strict_data_files"", [True, False])
def test_datapath_missing(datapath):
    with pytest.raises(ValueError, match=""Could not find file""):
        datapath(""not_a_file"")


def test_datapath(datapath):
    args = (""io"", ""data"", ""csv"", ""iris.csv"")

    result = datapath(*args)
    expected = os.path.join(os.path.dirname(os.path.dirname(__file__)), *args)

    assert result == expected


def test_rng_context():
    import numpy as np

    expected0 = 1.764052345967664
    expected1 = 1.6243453636632417

    with tm.RNGContext(0):
        with tm.RNGContext(1):
            assert np.random.randn() == expected1
        assert np.random.randn() == expected0


def test_external_error_raised():
    with tm.external_error_raised(TypeError):
        raise TypeError(""Should not check this error message, so it will pass"")"
464	donghui	1	"import os

import pytest

import pandas.compat as compat

import pandas._testing as tm


def test_rands():
    r = tm.rands(10)
    assert len(r) == 10


def test_rands_array_1d():
    arr = tm.rands_array(5, size=10)
    assert arr.shape == (10,)
    assert len(arr[0]) == 5


def test_rands_array_2d():
    arr = tm.rands_array(7, size=(10, 10))
    assert arr.shape == (10, 10)
    assert len(arr[1, 1]) == 7


def test_numpy_err_state_is_default():
    expected = {""over"": ""warn"", ""divide"": ""warn"", ""invalid"": ""warn"", ""under"": ""ignore""}
    import numpy as np

    # The error state should be unchanged after that import.
    assert np.geterr() == expected


def test_convert_rows_list_to_csv_str():
    rows_list = [""aaa"", ""bbb"", ""ccc""]
    ret = tm.convert_rows_list_to_csv_str(rows_list)

    if compat.is_platform_windows():
        expected = ""aaa\r\nbbb\r\nccc\r\n""
    else:
        expected = ""aaa\nbbb\nccc\n""

    assert ret == expected


def test_create_temp_directory():
    with tm.ensure_clean_dir() as path:
        assert os.path.exists(path)
        assert os.path.isdir(path)
    assert not os.path.exists(path)


@pytest.mark.parametrize(""strict_data_files"", [True, False])
def test_datapath_missing(datapath):
    with pytest.raises(ValueError, match=""Could not find file""):
        datapath(""not_a_file"")


def test_datapath(datapath):
    args = (""io"", ""data"", ""csv"", ""iris.csv"")

    result = datapath(*args)
    expected = os.path.join(os.path.dirname(os.path.dirname(__file__)), *args)

    assert result == expected


def test_rng_context():
    import numpy as np

    expected0 = 1.764052345967664
    expected1 = 1.6243453636632417

    with tm.RNGContext(0):
        with tm.RNGContext(1):
            assert np.random.randn() == expected1
        assert np.random.randn() == expected0


def test_external_error_raised():
    with tm.external_error_raised(TypeError):
        raise TypeError(""Should not check this error message, so it will pass"")"
435	jackson	3	"""""""
    pygments.lexers.sgf
    ~~~~~~~~~~~~~~~~~~~

    Lexer for Smart Game Format (sgf) file format.

    :copyright: Copyright 2006-2022 by the Pygments team, see AUTHORS.
    :license: BSD, see LICENSE for details.
""""""

from pygments.lexer import RegexLexer, bygroups
from pygments.token import Name, Literal, String, Text, Punctuation, Whitespace

__all__ = [""SmartGameFormatLexer""]


class SmartGameFormatLexer(RegexLexer):
    """"""
    Lexer for Smart Game Format (sgf) file format.

    The format is used to store game records of board games for two players
    (mainly Go game).

    .. versionadded:: 2.4
    """"""
    name = 'SmartGameFormat'
    url = 'https://www.red-bean.com/sgf/'
    aliases = ['sgf']
    filenames = ['*.sgf']

    tokens = {
        'root': [
            (r'[():;]+', Punctuation),
            # tokens:
            (r'(A[BW]|AE|AN|AP|AR|AS|[BW]L|BM|[BW]R|[BW]S|[BW]T|CA|CH|CP|CR|'
             r'DD|DM|DO|DT|EL|EV|EX|FF|FG|G[BW]|GC|GM|GN|HA|HO|ID|IP|IT|IY|KM|'
             r'KO|LB|LN|LT|L|MA|MN|M|N|OB|OM|ON|OP|OT|OV|P[BW]|PC|PL|PM|RE|RG|'
             r'RO|RU|SO|SC|SE|SI|SL|SO|SQ|ST|SU|SZ|T[BW]|TC|TE|TM|TR|UC|US|VW|'
             r'V|[BW]|C)',
             Name.Builtin),
            # number:
            (r'(\[)([0-9.]+)(\])',
             bygroups(Punctuation, Literal.Number, Punctuation)),
            # date:
            (r'(\[)([0-9]{4}-[0-9]{2}-[0-9]{2})(\])',
             bygroups(Punctuation, Literal.Date, Punctuation)),
            # point:
            (r'(\[)([a-z]{2})(\])',
             bygroups(Punctuation, String, Punctuation)),
            # double points:
            (r'(\[)([a-z]{2})(:)([a-z]{2})(\])',
             bygroups(Punctuation, String, Punctuation, String, Punctuation)),

            (r'(\[)([\w\s#()+,\-.:?]+)(\])',
             bygroups(Punctuation, String, Punctuation)),
            (r'(\[)(\s.*)(\])',
             bygroups(Punctuation, Whitespace, Punctuation)),
            (r'\s+', Whitespace)
        ],
    }"
435	donghui	2	"""""""
    pygments.lexers.sgf
    ~~~~~~~~~~~~~~~~~~~

    Lexer for Smart Game Format (sgf) file format.

    :copyright: Copyright 2006-2022 by the Pygments team, see AUTHORS.
    :license: BSD, see LICENSE for details.
""""""

from pygments.lexer import RegexLexer, bygroups
from pygments.token import Name, Literal, String, Text, Punctuation, Whitespace

__all__ = [""SmartGameFormatLexer""]


class SmartGameFormatLexer(RegexLexer):
    """"""
    Lexer for Smart Game Format (sgf) file format.

    The format is used to store game records of board games for two players
    (mainly Go game).

    .. versionadded:: 2.4
    """"""
    name = 'SmartGameFormat'
    url = 'https://www.red-bean.com/sgf/'
    aliases = ['sgf']
    filenames = ['*.sgf']

    tokens = {
        'root': [
            (r'[():;]+', Punctuation),
            # tokens:
            (r'(A[BW]|AE|AN|AP|AR|AS|[BW]L|BM|[BW]R|[BW]S|[BW]T|CA|CH|CP|CR|'
             r'DD|DM|DO|DT|EL|EV|EX|FF|FG|G[BW]|GC|GM|GN|HA|HO|ID|IP|IT|IY|KM|'
             r'KO|LB|LN|LT|L|MA|MN|M|N|OB|OM|ON|OP|OT|OV|P[BW]|PC|PL|PM|RE|RG|'
             r'RO|RU|SO|SC|SE|SI|SL|SO|SQ|ST|SU|SZ|T[BW]|TC|TE|TM|TR|UC|US|VW|'
             r'V|[BW]|C)',
             Name.Builtin),
            # number:
            (r'(\[)([0-9.]+)(\])',
             bygroups(Punctuation, Literal.Number, Punctuation)),
            # date:
            (r'(\[)([0-9]{4}-[0-9]{2}-[0-9]{2})(\])',
             bygroups(Punctuation, Literal.Date, Punctuation)),
            # point:
            (r'(\[)([a-z]{2})(\])',
             bygroups(Punctuation, String, Punctuation)),
            # double points:
            (r'(\[)([a-z]{2})(:)([a-z]{2})(\])',
             bygroups(Punctuation, String, Punctuation, String, Punctuation)),

            (r'(\[)([\w\s#()+,\-.:?]+)(\])',
             bygroups(Punctuation, String, Punctuation)),
            (r'(\[)(\s.*)(\])',
             bygroups(Punctuation, Whitespace, Punctuation)),
            (r'\s+', Whitespace)
        ],
    }"
401	jackson	4	"import hashlib
import hmac
from operator import itemgetter
from typing import Callable, Any, Dict
from urllib.parse import parse_qsl


def check_webapp_signature(token: str, init_data: str) -> bool:
    """"""
    Check incoming WebApp init data signature

    Source: https://core.telegram.org/bots/webapps#validating-data-received-via-the-web-app

    :param token:
    :param init_data:
    :return:
    """"""
    try:
        parsed_data = dict(parse_qsl(init_data))
    except ValueError:
        # Init data is not a valid query string
        return False
    if ""hash"" not in parsed_data:
        # Hash is not present in init data
        return False

    hash_ = parsed_data.pop('hash')
    data_check_string = ""\n"".join(
        f""{k}={v}"" for k, v in sorted(parsed_data.items(), key=itemgetter(0))
    )
    secret_key = hmac.new(
        key=b""WebAppData"", msg=token.encode(), digestmod=hashlib.sha256
    )
    calculated_hash = hmac.new(
        key=secret_key.digest(), msg=data_check_string.encode(), digestmod=hashlib.sha256
    ).hexdigest()
    return calculated_hash == hash_


def parse_init_data(init_data: str, _loads: Callable[..., Any]) -> Dict[str, Any]:
    """"""
    Parse WebApp init data and return it as dict

    :param init_data:
    :param _loads:
    :return:
    """"""
    result = {}
    for key, value in parse_qsl(init_data):
        if (value.startswith('[') and value.endswith(']')) or (value.startswith('{') and value.endswith('}')):
            value = _loads(value)
        result[key] = value
    return result


def safe_parse_webapp_init_data(token: str, init_data: str, _loads: Callable[..., Any]) -> Dict[str, Any]:
    """"""
    Validate WebApp init data and return it as dict

    :param token:
    :param init_data:
    :param _loads:
    :return:
    """"""
    if check_webapp_signature(token, init_data):
        return parse_init_data(init_data, _loads)
    raise ValueError(""Invalid init data signature"")"
401	donghui	3	"import hashlib
import hmac
from operator import itemgetter
from typing import Callable, Any, Dict
from urllib.parse import parse_qsl


def check_webapp_signature(token: str, init_data: str) -> bool:
    """"""
    Check incoming WebApp init data signature

    Source: https://core.telegram.org/bots/webapps#validating-data-received-via-the-web-app

    :param token:
    :param init_data:
    :return:
    """"""
    try:
        parsed_data = dict(parse_qsl(init_data))
    except ValueError:
        # Init data is not a valid query string
        return False
    if ""hash"" not in parsed_data:
        # Hash is not present in init data
        return False

    hash_ = parsed_data.pop('hash')
    data_check_string = ""\n"".join(
        f""{k}={v}"" for k, v in sorted(parsed_data.items(), key=itemgetter(0))
    )
    secret_key = hmac.new(
        key=b""WebAppData"", msg=token.encode(), digestmod=hashlib.sha256
    )
    calculated_hash = hmac.new(
        key=secret_key.digest(), msg=data_check_string.encode(), digestmod=hashlib.sha256
    ).hexdigest()
    return calculated_hash == hash_


def parse_init_data(init_data: str, _loads: Callable[..., Any]) -> Dict[str, Any]:
    """"""
    Parse WebApp init data and return it as dict

    :param init_data:
    :param _loads:
    :return:
    """"""
    result = {}
    for key, value in parse_qsl(init_data):
        if (value.startswith('[') and value.endswith(']')) or (value.startswith('{') and value.endswith('}')):
            value = _loads(value)
        result[key] = value
    return result


def safe_parse_webapp_init_data(token: str, init_data: str, _loads: Callable[..., Any]) -> Dict[str, Any]:
    """"""
    Validate WebApp init data and return it as dict

    :param token:
    :param init_data:
    :param _loads:
    :return:
    """"""
    if check_webapp_signature(token, init_data):
        return parse_init_data(init_data, _loads)
    raise ValueError(""Invalid init data signature"")"
450	jackson	3	"# flake8: noqa
import subprocess
import sys
import unittest

_import_everything = b""""""
# The event loop is not fork-safe, and it's easy to initialize an asyncio.Future
# at startup, which in turn creates the default event loop and prevents forking.
# Explicitly disallow the default event loop so that an error will be raised
# if something tries to touch it.
import asyncio
asyncio.set_event_loop(None)

import tornado.auth
import tornado.autoreload
import tornado.concurrent
import tornado.escape
import tornado.gen
import tornado.http1connection
import tornado.httpclient
import tornado.httpserver
import tornado.httputil
import tornado.ioloop
import tornado.iostream
import tornado.locale
import tornado.log
import tornado.netutil
import tornado.options
import tornado.process
import tornado.simple_httpclient
import tornado.tcpserver
import tornado.tcpclient
import tornado.template
import tornado.testing
import tornado.util
import tornado.web
import tornado.websocket
import tornado.wsgi

try:
    import pycurl
except ImportError:
    pass
else:
    import tornado.curl_httpclient
""""""


class ImportTest(unittest.TestCase):
    def test_import_everything(self):
        # Test that all Tornado modules can be imported without side effects,
        # specifically without initializing the default asyncio event loop.
        # Since we can't tell which modules may have already beein imported
        # in our process, do it in a subprocess for a clean slate.
        proc = subprocess.Popen([sys.executable], stdin=subprocess.PIPE)
        proc.communicate(_import_everything)
        self.assertEqual(proc.returncode, 0)

    def test_import_aliases(self):
        # Ensure we don't delete formerly-documented aliases accidentally.
        import tornado.ioloop
        import tornado.gen
        import tornado.util
        import asyncio

        self.assertIs(tornado.ioloop.TimeoutError, tornado.util.TimeoutError)
        self.assertIs(tornado.gen.TimeoutError, tornado.util.TimeoutError)
        self.assertIs(tornado.util.TimeoutError, asyncio.TimeoutError)"
450	donghui	2	"# flake8: noqa
import subprocess
import sys
import unittest

_import_everything = b""""""
# The event loop is not fork-safe, and it's easy to initialize an asyncio.Future
# at startup, which in turn creates the default event loop and prevents forking.
# Explicitly disallow the default event loop so that an error will be raised
# if something tries to touch it.
import asyncio
asyncio.set_event_loop(None)

import tornado.auth
import tornado.autoreload
import tornado.concurrent
import tornado.escape
import tornado.gen
import tornado.http1connection
import tornado.httpclient
import tornado.httpserver
import tornado.httputil
import tornado.ioloop
import tornado.iostream
import tornado.locale
import tornado.log
import tornado.netutil
import tornado.options
import tornado.process
import tornado.simple_httpclient
import tornado.tcpserver
import tornado.tcpclient
import tornado.template
import tornado.testing
import tornado.util
import tornado.web
import tornado.websocket
import tornado.wsgi

try:
    import pycurl
except ImportError:
    pass
else:
    import tornado.curl_httpclient
""""""


class ImportTest(unittest.TestCase):
    def test_import_everything(self):
        # Test that all Tornado modules can be imported without side effects,
        # specifically without initializing the default asyncio event loop.
        # Since we can't tell which modules may have already beein imported
        # in our process, do it in a subprocess for a clean slate.
        proc = subprocess.Popen([sys.executable], stdin=subprocess.PIPE)
        proc.communicate(_import_everything)
        self.assertEqual(proc.returncode, 0)

    def test_import_aliases(self):
        # Ensure we don't delete formerly-documented aliases accidentally.
        import tornado.ioloop
        import tornado.gen
        import tornado.util
        import asyncio

        self.assertIs(tornado.ioloop.TimeoutError, tornado.util.TimeoutError)
        self.assertIs(tornado.gen.TimeoutError, tornado.util.TimeoutError)
        self.assertIs(tornado.util.TimeoutError, asyncio.TimeoutError)"
510	jackson	1	"import argparse
from typing import Tuple


def get_next_version(release_type) -> Tuple[Tuple[int, int, int], str, str]:
    current_ver = find_version(""fairseq/version.txt"")
    version_list = [int(x) for x in current_ver.strip(""'"").split(""."")]
    major, minor, patch = version_list[0], version_list[1], version_list[2]
    if release_type == ""patch"":
        patch += 1
    elif release_type == ""minor"":
        minor += 1
        patch = 0
    elif release_type == ""major"":
        major += 1
        minor = patch = 0
    else:
        raise ValueError(
            ""Incorrect release type specified. Acceptable types are major, minor and patch.""
        )

    new_version_tuple = (major, minor, patch)
    new_version_str = ""."".join([str(x) for x in new_version_tuple])
    new_tag_str = ""v"" + new_version_str
    return new_version_tuple, new_version_str, new_tag_str


def find_version(version_file_path) -> str:
    with open(version_file_path) as f:
        version = f.read().strip()
        return version


def update_version(new_version_str) -> None:
    """"""
    given the current version, update the version to the
    next version depending on the type of release.
    """"""

    with open(""fairseq/version.txt"", ""w"") as writer:
        writer.write(new_version_str)


def main(args):
    if args.release_type in [""major"", ""minor"", ""patch""]:
        new_version_tuple, new_version, new_tag = get_next_version(args.release_type)
    else:
        raise ValueError(""Incorrect release type specified"")

    if args.update_version:
        update_version(new_version)

    print(new_version, new_tag)


if __name__ == ""__main__"":
    parser = argparse.ArgumentParser(description=""Versioning utils"")
    parser.add_argument(
        ""--release-type"",
        type=str,
        required=True,
        help=""type of release = major/minor/patch"",
    )
    parser.add_argument(
        ""--update-version"",
        action=""store_true"",
        required=False,
        help=""updates the version in fairseq/version.txt"",
    )

    args = parser.parse_args()
    main(args)"
510	donghui	1	"import argparse
from typing import Tuple


def get_next_version(release_type) -> Tuple[Tuple[int, int, int], str, str]:
    current_ver = find_version(""fairseq/version.txt"")
    version_list = [int(x) for x in current_ver.strip(""'"").split(""."")]
    major, minor, patch = version_list[0], version_list[1], version_list[2]
    if release_type == ""patch"":
        patch += 1
    elif release_type == ""minor"":
        minor += 1
        patch = 0
    elif release_type == ""major"":
        major += 1
        minor = patch = 0
    else:
        raise ValueError(
            ""Incorrect release type specified. Acceptable types are major, minor and patch.""
        )

    new_version_tuple = (major, minor, patch)
    new_version_str = ""."".join([str(x) for x in new_version_tuple])
    new_tag_str = ""v"" + new_version_str
    return new_version_tuple, new_version_str, new_tag_str


def find_version(version_file_path) -> str:
    with open(version_file_path) as f:
        version = f.read().strip()
        return version


def update_version(new_version_str) -> None:
    """"""
    given the current version, update the version to the
    next version depending on the type of release.
    """"""

    with open(""fairseq/version.txt"", ""w"") as writer:
        writer.write(new_version_str)


def main(args):
    if args.release_type in [""major"", ""minor"", ""patch""]:
        new_version_tuple, new_version, new_tag = get_next_version(args.release_type)
    else:
        raise ValueError(""Incorrect release type specified"")

    if args.update_version:
        update_version(new_version)

    print(new_version, new_tag)


if __name__ == ""__main__"":
    parser = argparse.ArgumentParser(description=""Versioning utils"")
    parser.add_argument(
        ""--release-type"",
        type=str,
        required=True,
        help=""type of release = major/minor/patch"",
    )
    parser.add_argument(
        ""--update-version"",
        action=""store_true"",
        required=False,
        help=""updates the version in fairseq/version.txt"",
    )

    args = parser.parse_args()
    main(args)"
470	jackson	1	"from selenium.webdriver.common.by import By


class BasePageLocators():
    LOGIN_LINK = (By.CSS_SELECTOR, ""#login_link"")
    LOGIN_LINK_INVALID = (By.CSS_SELECTOR, ""#login_link_inc"")  # for checking the correct error message


class MainPageLocators():
    pass


class LoginPageLocators():
    LOGIN_EMAIL = (By.CSS_SELECTOR, ""[name='login-username']"")
    LOGIN_PASSWORD = (By.CSS_SELECTOR, ""[name='login-password']"")
    FORGOT_PASSWORD_BUTTON = (By.CSS_SELECTOR, 'a[href*=""password-reset""]')
    LOGIN_BUTTON = (By.CSS_SELECTOR, ""[name='login_submit']"")
    SIGN_UP_EMAIL = (By.CSS_SELECTOR, ""[name='registration-email']"")
    SIGN_UP_PASSWORD = (By.CSS_SELECTOR, ""[name='registration-password1']"")
    SIGN_UP_PASSWORD_REPETITION = (By.CSS_SELECTOR, ""[name='registration-password2']"")
    SIGN_UP_BUTTON = (By.CSS_SELECTOR, ""[name='registration_submit']"")


class ProductPageLocators():
    PRODUCT_NAME = (By.CSS_SELECTOR, "".product_main > h1"")
    ADD_TO_BASKET_BUTTON = (By. CSS_SELECTOR, "".btn-add-to-basket"")
    ADD_TO_WISHLIST_BUTTON = (By.CSS_SELECTOR, "".btn-wishlist"")
    PRODUCT_GALLERY = (By.CSS_SELECTOR, ""#product_gallery"")
    PRODUCT_DESCRIPTION = (By.CSS_SELECTOR, ""#product_description"")
    PRICE = (By.CSS_SELECTOR, "".product_main > .price_color"")
    AVAILABILITY = (By.CSS_SELECTOR, "".product_main > .availability"")
    WRITE_REVIEW = (By.CSS_SELECTOR, ""#write_review"")
    PRODUCT_INFO_TABLE = (By.CSS_SELECTOR, "".table-striped"")
    SUCCESS_MESSAGE = (By.CSS_SELECTOR, ""#messages > .alert-success:nth-child(1)"")
    NAME_OF_ADDED_PRODUCT = (By.CSS_SELECTOR, ""div.alert:nth-child(1) strong"")
    TOTAL_PRICE = (By.CSS_SELECTOR, "".alertinner p strong"")


class BasketPageLocators():
    VIEW_BASKET = (By.CSS_SELECTOR, "".btn-group > a[href*='basket']"")
    VIEW_BASKET_INVALID = (By.CSS_SELECTOR, "".btn-group > a[href*='basket']"")  # for checking the correct error message
    EMPTY_BASKET_MESSAGE = (By.CSS_SELECTOR, ""#content_inner > p"")
    FILLED_BASKET = (By.CSS_SELECTOR, "".basket-items"")"
470	donghui	1	"from selenium.webdriver.common.by import By


class BasePageLocators():
    LOGIN_LINK = (By.CSS_SELECTOR, ""#login_link"")
    LOGIN_LINK_INVALID = (By.CSS_SELECTOR, ""#login_link_inc"")  # for checking the correct error message


class MainPageLocators():
    pass


class LoginPageLocators():
    LOGIN_EMAIL = (By.CSS_SELECTOR, ""[name='login-username']"")
    LOGIN_PASSWORD = (By.CSS_SELECTOR, ""[name='login-password']"")
    FORGOT_PASSWORD_BUTTON = (By.CSS_SELECTOR, 'a[href*=""password-reset""]')
    LOGIN_BUTTON = (By.CSS_SELECTOR, ""[name='login_submit']"")
    SIGN_UP_EMAIL = (By.CSS_SELECTOR, ""[name='registration-email']"")
    SIGN_UP_PASSWORD = (By.CSS_SELECTOR, ""[name='registration-password1']"")
    SIGN_UP_PASSWORD_REPETITION = (By.CSS_SELECTOR, ""[name='registration-password2']"")
    SIGN_UP_BUTTON = (By.CSS_SELECTOR, ""[name='registration_submit']"")


class ProductPageLocators():
    PRODUCT_NAME = (By.CSS_SELECTOR, "".product_main > h1"")
    ADD_TO_BASKET_BUTTON = (By. CSS_SELECTOR, "".btn-add-to-basket"")
    ADD_TO_WISHLIST_BUTTON = (By.CSS_SELECTOR, "".btn-wishlist"")
    PRODUCT_GALLERY = (By.CSS_SELECTOR, ""#product_gallery"")
    PRODUCT_DESCRIPTION = (By.CSS_SELECTOR, ""#product_description"")
    PRICE = (By.CSS_SELECTOR, "".product_main > .price_color"")
    AVAILABILITY = (By.CSS_SELECTOR, "".product_main > .availability"")
    WRITE_REVIEW = (By.CSS_SELECTOR, ""#write_review"")
    PRODUCT_INFO_TABLE = (By.CSS_SELECTOR, "".table-striped"")
    SUCCESS_MESSAGE = (By.CSS_SELECTOR, ""#messages > .alert-success:nth-child(1)"")
    NAME_OF_ADDED_PRODUCT = (By.CSS_SELECTOR, ""div.alert:nth-child(1) strong"")
    TOTAL_PRICE = (By.CSS_SELECTOR, "".alertinner p strong"")


class BasketPageLocators():
    VIEW_BASKET = (By.CSS_SELECTOR, "".btn-group > a[href*='basket']"")
    VIEW_BASKET_INVALID = (By.CSS_SELECTOR, "".btn-group > a[href*='basket']"")  # for checking the correct error message
    EMPTY_BASKET_MESSAGE = (By.CSS_SELECTOR, ""#content_inner > p"")
    FILLED_BASKET = (By.CSS_SELECTOR, "".basket-items"")"
421	jackson	0	"import asyncio
import json

from openpyxl import load_workbook
from rest_framework.response import Response
from rest_framework.views import APIView

from wildberries.models import Product
from wildberries.pydantic import CardPydantic
from wildberries.utils import make_request


class CardView(APIView):
    @staticmethod
    def get_card_info(value):
        page = asyncio.run(make_request(value))
        return CardView.get_objects(page, value)

    @staticmethod
    def get_cards_info(file):
        values = []
        wb = load_workbook(file)
        for sheet in wb.sheetnames:
            for row in wb[sheet].iter_rows(values_only=True):
                values.append(row[0])
        cards_info = [CardView.get_card_info(i) for i in values]
        return cards_info

    @staticmethod
    def get_objects(page, value):
        card = None
        try:
            products = json.dumps(page['data']['products'][0])
            card = CardPydantic.parse_raw(products)
            Product.objects.create(**card.dict())
        except IndexError:
            print(f'id {value} отсутствует на сайте wildberries.ru')
        if card:
            return card.dict()
        else:
            return {'error': f'id {value} отсутствует на сайте wildberries.ru'}

    def post(self, request, *args, **kwargs):
        data = None
        if 'file' in request.data and 'value' in request.data:
            return Response({'error': 'Одновременно отправлять поля '
                                      'file и value запрещено!'})
        elif 'file' in request.data:
            file = request.data['file']
            data = CardView.get_cards_info(file)
        elif 'value' in request.data:
            value = request.data['value']
            data = CardView.get_card_info(value)
        return Response(data)"
421	donghui	0	"import asyncio
import json

from openpyxl import load_workbook
from rest_framework.response import Response
from rest_framework.views import APIView

from wildberries.models import Product
from wildberries.pydantic import CardPydantic
from wildberries.utils import make_request


class CardView(APIView):
    @staticmethod
    def get_card_info(value):
        page = asyncio.run(make_request(value))
        return CardView.get_objects(page, value)

    @staticmethod
    def get_cards_info(file):
        values = []
        wb = load_workbook(file)
        for sheet in wb.sheetnames:
            for row in wb[sheet].iter_rows(values_only=True):
                values.append(row[0])
        cards_info = [CardView.get_card_info(i) for i in values]
        return cards_info

    @staticmethod
    def get_objects(page, value):
        card = None
        try:
            products = json.dumps(page['data']['products'][0])
            card = CardPydantic.parse_raw(products)
            Product.objects.create(**card.dict())
        except IndexError:
            print(f'id {value} отсутствует на сайте wildberries.ru')
        if card:
            return card.dict()
        else:
            return {'error': f'id {value} отсутствует на сайте wildberries.ru'}

    def post(self, request, *args, **kwargs):
        data = None
        if 'file' in request.data and 'value' in request.data:
            return Response({'error': 'Одновременно отправлять поля '
                                      'file и value запрещено!'})
        elif 'file' in request.data:
            file = request.data['file']
            data = CardView.get_cards_info(file)
        elif 'value' in request.data:
            value = request.data['value']
            data = CardView.get_card_info(value)
        return Response(data)"
483	jackson	4	"from . import base
from . import fields
from . import mixins
from .mask_position import MaskPosition
from .photo_size import PhotoSize
from .file import File


class Sticker(base.TelegramObject, mixins.Downloadable):
    """"""
    This object represents a sticker.

    https://core.telegram.org/bots/api#sticker
    """"""
    file_id: base.String = fields.Field()
    file_unique_id: base.String = fields.Field()
    type: base.String = fields.Field()
    width: base.Integer = fields.Field()
    height: base.Integer = fields.Field()
    is_animated: base.Boolean = fields.Field()
    is_video: base.Boolean = fields.Field()
    thumb: PhotoSize = fields.Field(base=PhotoSize)
    emoji: base.String = fields.Field()
    set_name: base.String = fields.Field()
    premium_animation: File = fields.Field(base=File)
    mask_position: MaskPosition = fields.Field(base=MaskPosition)
    custom_emoji_id: base.String = fields.Field()
    file_size: base.Integer = fields.Field()

    async def set_position_in_set(self, position: base.Integer) -> base.Boolean:
        """"""
        Use this method to move a sticker in a set created by the bot to a specific position.

        Source: https://core.telegram.org/bots/api#setstickerpositioninset

        :param position: New sticker position in the set, zero-based
        :type position: :obj:`base.Integer`
        :return: Returns True on success
        :rtype: :obj:`base.Boolean`
        """"""
        return await self.bot.set_sticker_position_in_set(self.file_id, position=position)

    async def delete_from_set(self) -> base.Boolean:
        """"""
        Use this method to delete a sticker from a set created by the bot.

        Source: https://core.telegram.org/bots/api#deletestickerfromset

        :return: Returns True on success
        :rtype: :obj:`base.Boolean`
        """"""
        return await self.bot.delete_sticker_from_set(self.file_id)"
483	donghui	3	"from . import base
from . import fields
from . import mixins
from .mask_position import MaskPosition
from .photo_size import PhotoSize
from .file import File


class Sticker(base.TelegramObject, mixins.Downloadable):
    """"""
    This object represents a sticker.

    https://core.telegram.org/bots/api#sticker
    """"""
    file_id: base.String = fields.Field()
    file_unique_id: base.String = fields.Field()
    type: base.String = fields.Field()
    width: base.Integer = fields.Field()
    height: base.Integer = fields.Field()
    is_animated: base.Boolean = fields.Field()
    is_video: base.Boolean = fields.Field()
    thumb: PhotoSize = fields.Field(base=PhotoSize)
    emoji: base.String = fields.Field()
    set_name: base.String = fields.Field()
    premium_animation: File = fields.Field(base=File)
    mask_position: MaskPosition = fields.Field(base=MaskPosition)
    custom_emoji_id: base.String = fields.Field()
    file_size: base.Integer = fields.Field()

    async def set_position_in_set(self, position: base.Integer) -> base.Boolean:
        """"""
        Use this method to move a sticker in a set created by the bot to a specific position.

        Source: https://core.telegram.org/bots/api#setstickerpositioninset

        :param position: New sticker position in the set, zero-based
        :type position: :obj:`base.Integer`
        :return: Returns True on success
        :rtype: :obj:`base.Boolean`
        """"""
        return await self.bot.set_sticker_position_in_set(self.file_id, position=position)

    async def delete_from_set(self) -> base.Boolean:
        """"""
        Use this method to delete a sticker from a set created by the bot.

        Source: https://core.telegram.org/bots/api#deletestickerfromset

        :return: Returns True on success
        :rtype: :obj:`base.Boolean`
        """"""
        return await self.bot.delete_sticker_from_set(self.file_id)"
415	jackson	0	"import pandas as pd
import numpy as np
import random

import matplotlib.pyplot as plt
import seaborn as sns

from plotly import graph_objs as go
from plotly import express as px
from plotly.subplots import make_subplots

import pickle
import lightgbm as lgb



colorarr = ['#0592D0','#Cd7f32', '#E97451', '#Bdb76b', '#954535', '#C2b280', '#808000','#C2b280', '#E4d008', '#9acd32', '#Eedc82', '#E4d96f',
           '#32cd32','#39ff14','#00ff7f', '#008080', '#36454f', '#F88379', '#Ff4500', '#Ffb347', '#A94064', '#E75480', '#Ffb6c1', '#E5e4e2',
           '#Faf0e6', '#8c92ac', '#Dbd7d2','#A7a6ba', '#B38b6d']


cropdf = pd.read_csv(""D:/Suyash College Files/Semester 6/22060 - Capstone Project Execution and Report Writing/Datasets/Crop_recommendation.csv"")

X = cropdf.drop('label', axis=1)
y = cropdf['label']

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3,
                                                    shuffle = True, random_state = 0)

model = lgb.LGBMClassifier()
model.fit(X_train, y_train)

y_pred=model.predict(X_test)


from sklearn.metrics import accuracy_score

accuracy=accuracy_score(y_pred, y_test)
print('LightGBM Model accuracy score: {0:0.4f}'.format(accuracy_score(y_test, y_pred)))


y_pred_train = model.predict(X_train)
print('Training-set accuracy score: {0:0.4f}'. format(accuracy_score(y_train, y_pred_train)))

print('Training set score: {:.4f}'.format(model.score(X_train, y_train)))
print('Test set score: {:.4f}'.format(model.score(X_test, y_test)))


model.booster_.save_model('crop_predict1.h5')

        #        my_model.booster_.save_model('mode.txt')
        #        #load from model:
        #        #bst = lgb.Booster(model_file='mode.txt')



filename = ""trained_model.pkl""
pickle.dump(model,open(filename,'wb'))

print(""done with all"")

"
415	donghui	1	"import pandas as pd
import numpy as np
import random

import matplotlib.pyplot as plt
import seaborn as sns

from plotly import graph_objs as go
from plotly import express as px
from plotly.subplots import make_subplots

import pickle
import lightgbm as lgb



colorarr = ['#0592D0','#Cd7f32', '#E97451', '#Bdb76b', '#954535', '#C2b280', '#808000','#C2b280', '#E4d008', '#9acd32', '#Eedc82', '#E4d96f',
           '#32cd32','#39ff14','#00ff7f', '#008080', '#36454f', '#F88379', '#Ff4500', '#Ffb347', '#A94064', '#E75480', '#Ffb6c1', '#E5e4e2',
           '#Faf0e6', '#8c92ac', '#Dbd7d2','#A7a6ba', '#B38b6d']


cropdf = pd.read_csv(""D:/Suyash College Files/Semester 6/22060 - Capstone Project Execution and Report Writing/Datasets/Crop_recommendation.csv"")

X = cropdf.drop('label', axis=1)
y = cropdf['label']

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3,
                                                    shuffle = True, random_state = 0)

model = lgb.LGBMClassifier()
model.fit(X_train, y_train)

y_pred=model.predict(X_test)


from sklearn.metrics import accuracy_score

accuracy=accuracy_score(y_pred, y_test)
print('LightGBM Model accuracy score: {0:0.4f}'.format(accuracy_score(y_test, y_pred)))


y_pred_train = model.predict(X_train)
print('Training-set accuracy score: {0:0.4f}'. format(accuracy_score(y_train, y_pred_train)))

print('Training set score: {:.4f}'.format(model.score(X_train, y_train)))
print('Test set score: {:.4f}'.format(model.score(X_test, y_test)))


model.booster_.save_model('crop_predict1.h5')

        #        my_model.booster_.save_model('mode.txt')
        #        #load from model:
        #        #bst = lgb.Booster(model_file='mode.txt')



filename = ""trained_model.pkl""
pickle.dump(model,open(filename,'wb'))

print(""done with all"")

"
444	jackson	0	"# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# ""License""); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.

from __future__ import absolute_import

import cffi

c_source = """"""
    struct ArrowSchema {
      // Array type description
      const char* format;
      const char* name;
      const char* metadata;
      int64_t flags;
      int64_t n_children;
      struct ArrowSchema** children;
      struct ArrowSchema* dictionary;

      // Release callback
      void (*release)(struct ArrowSchema*);
      // Opaque producer-specific data
      void* private_data;
    };

    struct ArrowArray {
      // Array data description
      int64_t length;
      int64_t null_count;
      int64_t offset;
      int64_t n_buffers;
      int64_t n_children;
      const void** buffers;
      struct ArrowArray** children;
      struct ArrowArray* dictionary;

      // Release callback
      void (*release)(struct ArrowArray*);
      // Opaque producer-specific data
      void* private_data;
    };

    struct ArrowArrayStream {
      int (*get_schema)(struct ArrowArrayStream*, struct ArrowSchema* out);
      int (*get_next)(struct ArrowArrayStream*, struct ArrowArray* out);

      const char* (*get_last_error)(struct ArrowArrayStream*);

      // Release callback
      void (*release)(struct ArrowArrayStream*);
      // Opaque producer-specific data
      void* private_data;
    };
    """"""

# TODO use out-of-line mode for faster import and avoid C parsing
ffi = cffi.FFI()
ffi.cdef(c_source)"
444	donghui	1	"# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# ""License""); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.

from __future__ import absolute_import

import cffi

c_source = """"""
    struct ArrowSchema {
      // Array type description
      const char* format;
      const char* name;
      const char* metadata;
      int64_t flags;
      int64_t n_children;
      struct ArrowSchema** children;
      struct ArrowSchema* dictionary;

      // Release callback
      void (*release)(struct ArrowSchema*);
      // Opaque producer-specific data
      void* private_data;
    };

    struct ArrowArray {
      // Array data description
      int64_t length;
      int64_t null_count;
      int64_t offset;
      int64_t n_buffers;
      int64_t n_children;
      const void** buffers;
      struct ArrowArray** children;
      struct ArrowArray* dictionary;

      // Release callback
      void (*release)(struct ArrowArray*);
      // Opaque producer-specific data
      void* private_data;
    };

    struct ArrowArrayStream {
      int (*get_schema)(struct ArrowArrayStream*, struct ArrowSchema* out);
      int (*get_next)(struct ArrowArrayStream*, struct ArrowArray* out);

      const char* (*get_last_error)(struct ArrowArrayStream*);

      // Release callback
      void (*release)(struct ArrowArrayStream*);
      // Opaque producer-specific data
      void* private_data;
    };
    """"""

# TODO use out-of-line mode for faster import and avoid C parsing
ffi = cffi.FFI()
ffi.cdef(c_source)"
454	jackson	1	"""""""
views.py        # Houses `SchemaView`, `APIView` subclass.

See schemas.__init__.py for package overview.
""""""
from rest_framework import exceptions, renderers
from rest_framework.response import Response
from rest_framework.schemas import coreapi
from rest_framework.settings import api_settings
from rest_framework.views import APIView


class SchemaView(APIView):
    _ignore_model_permissions = True
    schema = None  # exclude from schema
    renderer_classes = None
    schema_generator = None
    public = False

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        if self.renderer_classes is None:
            if coreapi.is_enabled():
                self.renderer_classes = [
                    renderers.CoreAPIOpenAPIRenderer,
                    renderers.CoreJSONRenderer
                ]
            else:
                self.renderer_classes = [
                    renderers.OpenAPIRenderer,
                    renderers.JSONOpenAPIRenderer,
                ]
            if renderers.BrowsableAPIRenderer in api_settings.DEFAULT_RENDERER_CLASSES:
                self.renderer_classes += [renderers.BrowsableAPIRenderer]

    def get(self, request, *args, **kwargs):
        schema = self.schema_generator.get_schema(request, self.public)
        if schema is None:
            raise exceptions.PermissionDenied()
        return Response(schema)

    def handle_exception(self, exc):
        # Schema renderers do not render exceptions, so re-perform content
        # negotiation with default renderers.
        self.renderer_classes = api_settings.DEFAULT_RENDERER_CLASSES
        neg = self.perform_content_negotiation(self.request, force=True)
        self.request.accepted_renderer, self.request.accepted_media_type = neg
        return super().handle_exception(exc)"
454	donghui	1	"""""""
views.py        # Houses `SchemaView`, `APIView` subclass.

See schemas.__init__.py for package overview.
""""""
from rest_framework import exceptions, renderers
from rest_framework.response import Response
from rest_framework.schemas import coreapi
from rest_framework.settings import api_settings
from rest_framework.views import APIView


class SchemaView(APIView):
    _ignore_model_permissions = True
    schema = None  # exclude from schema
    renderer_classes = None
    schema_generator = None
    public = False

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        if self.renderer_classes is None:
            if coreapi.is_enabled():
                self.renderer_classes = [
                    renderers.CoreAPIOpenAPIRenderer,
                    renderers.CoreJSONRenderer
                ]
            else:
                self.renderer_classes = [
                    renderers.OpenAPIRenderer,
                    renderers.JSONOpenAPIRenderer,
                ]
            if renderers.BrowsableAPIRenderer in api_settings.DEFAULT_RENDERER_CLASSES:
                self.renderer_classes += [renderers.BrowsableAPIRenderer]

    def get(self, request, *args, **kwargs):
        schema = self.schema_generator.get_schema(request, self.public)
        if schema is None:
            raise exceptions.PermissionDenied()
        return Response(schema)

    def handle_exception(self, exc):
        # Schema renderers do not render exceptions, so re-perform content
        # negotiation with default renderers.
        self.renderer_classes = api_settings.DEFAULT_RENDERER_CLASSES
        neg = self.perform_content_negotiation(self.request, force=True)
        self.request.accepted_renderer, self.request.accepted_media_type = neg
        return super().handle_exception(exc)"
514	jackson	3	"""""""
 The GeometryColumns and SpatialRefSys models for the SpatiaLite backend.
""""""
from django.contrib.gis.db.backends.base.models import SpatialRefSysMixin
from django.db import models


class SpatialiteGeometryColumns(models.Model):
    """"""
    The 'geometry_columns' table from SpatiaLite.
    """"""
    f_table_name = models.CharField(max_length=256)
    f_geometry_column = models.CharField(max_length=256)
    coord_dimension = models.IntegerField()
    srid = models.IntegerField(primary_key=True)
    spatial_index_enabled = models.IntegerField()
    type = models.IntegerField(db_column='geometry_type')

    class Meta:
        app_label = 'gis'
        db_table = 'geometry_columns'
        managed = False

    def __str__(self):
        return '%s.%s - %dD %s field (SRID: %d)' % (
            self.f_table_name,
            self.f_geometry_column,
            self.coord_dimension,
            self.type,
            self.srid,
        )

    @classmethod
    def table_name_col(cls):
        """"""
        Return the name of the metadata column used to store the feature table
        name.
        """"""
        return 'f_table_name'

    @classmethod
    def geom_col_name(cls):
        """"""
        Return the name of the metadata column used to store the feature
        geometry column.
        """"""
        return 'f_geometry_column'


class SpatialiteSpatialRefSys(models.Model, SpatialRefSysMixin):
    """"""
    The 'spatial_ref_sys' table from SpatiaLite.
    """"""
    srid = models.IntegerField(primary_key=True)
    auth_name = models.CharField(max_length=256)
    auth_srid = models.IntegerField()
    ref_sys_name = models.CharField(max_length=256)
    proj4text = models.CharField(max_length=2048)
    srtext = models.CharField(max_length=2048)

    class Meta:
        app_label = 'gis'
        db_table = 'spatial_ref_sys'
        managed = False

    @property
    def wkt(self):
        return self.srtext"
514	donghui	3	"""""""
 The GeometryColumns and SpatialRefSys models for the SpatiaLite backend.
""""""
from django.contrib.gis.db.backends.base.models import SpatialRefSysMixin
from django.db import models


class SpatialiteGeometryColumns(models.Model):
    """"""
    The 'geometry_columns' table from SpatiaLite.
    """"""
    f_table_name = models.CharField(max_length=256)
    f_geometry_column = models.CharField(max_length=256)
    coord_dimension = models.IntegerField()
    srid = models.IntegerField(primary_key=True)
    spatial_index_enabled = models.IntegerField()
    type = models.IntegerField(db_column='geometry_type')

    class Meta:
        app_label = 'gis'
        db_table = 'geometry_columns'
        managed = False

    def __str__(self):
        return '%s.%s - %dD %s field (SRID: %d)' % (
            self.f_table_name,
            self.f_geometry_column,
            self.coord_dimension,
            self.type,
            self.srid,
        )

    @classmethod
    def table_name_col(cls):
        """"""
        Return the name of the metadata column used to store the feature table
        name.
        """"""
        return 'f_table_name'

    @classmethod
    def geom_col_name(cls):
        """"""
        Return the name of the metadata column used to store the feature
        geometry column.
        """"""
        return 'f_geometry_column'


class SpatialiteSpatialRefSys(models.Model, SpatialRefSysMixin):
    """"""
    The 'spatial_ref_sys' table from SpatiaLite.
    """"""
    srid = models.IntegerField(primary_key=True)
    auth_name = models.CharField(max_length=256)
    auth_srid = models.IntegerField()
    ref_sys_name = models.CharField(max_length=256)
    proj4text = models.CharField(max_length=2048)
    srtext = models.CharField(max_length=2048)

    class Meta:
        app_label = 'gis'
        db_table = 'spatial_ref_sys'
        managed = False

    @property
    def wkt(self):
        return self.srtext"
405	jackson	4	"from itertools import filterfalse


def unique_everseen(iterable, key=None):
    ""List unique elements, preserving order. Remember all elements ever seen.""
    # unique_everseen('AAAABBBCCDAABBB') --> A B C D
    # unique_everseen('ABBCcAD', str.lower) --> A B C D
    seen = set()
    seen_add = seen.add
    if key is None:
        for element in filterfalse(seen.__contains__, iterable):
            seen_add(element)
            yield element
    else:
        for element in iterable:
            k = key(element)
            if k not in seen:
                seen_add(k)
                yield element


# copied from more_itertools 8.8
def always_iterable(obj, base_type=(str, bytes)):
    """"""If *obj* is iterable, return an iterator over its items::

        >>> obj = (1, 2, 3)
        >>> list(always_iterable(obj))
        [1, 2, 3]

    If *obj* is not iterable, return a one-item iterable containing *obj*::

        >>> obj = 1
        >>> list(always_iterable(obj))
        [1]

    If *obj* is ``None``, return an empty iterable:

        >>> obj = None
        >>> list(always_iterable(None))
        []

    By default, binary and text strings are not considered iterable::

        >>> obj = 'foo'
        >>> list(always_iterable(obj))
        ['foo']

    If *base_type* is set, objects for which ``isinstance(obj, base_type)``
    returns ``True`` won't be considered iterable.

        >>> obj = {'a': 1}
        >>> list(always_iterable(obj))  # Iterate over the dict's keys
        ['a']
        >>> list(always_iterable(obj, base_type=dict))  # Treat dicts as a unit
        [{'a': 1}]

    Set *base_type* to ``None`` to avoid any special handling and treat objects
    Python considers iterable as iterable:

        >>> obj = 'foo'
        >>> list(always_iterable(obj, base_type=None))
        ['f', 'o', 'o']
    """"""
    if obj is None:
        return iter(())

    if (base_type is not None) and isinstance(obj, base_type):
        return iter((obj,))

    try:
        return iter(obj)
    except TypeError:
        return iter((obj,))"
405	donghui	4	"from itertools import filterfalse


def unique_everseen(iterable, key=None):
    ""List unique elements, preserving order. Remember all elements ever seen.""
    # unique_everseen('AAAABBBCCDAABBB') --> A B C D
    # unique_everseen('ABBCcAD', str.lower) --> A B C D
    seen = set()
    seen_add = seen.add
    if key is None:
        for element in filterfalse(seen.__contains__, iterable):
            seen_add(element)
            yield element
    else:
        for element in iterable:
            k = key(element)
            if k not in seen:
                seen_add(k)
                yield element


# copied from more_itertools 8.8
def always_iterable(obj, base_type=(str, bytes)):
    """"""If *obj* is iterable, return an iterator over its items::

        >>> obj = (1, 2, 3)
        >>> list(always_iterable(obj))
        [1, 2, 3]

    If *obj* is not iterable, return a one-item iterable containing *obj*::

        >>> obj = 1
        >>> list(always_iterable(obj))
        [1]

    If *obj* is ``None``, return an empty iterable:

        >>> obj = None
        >>> list(always_iterable(None))
        []

    By default, binary and text strings are not considered iterable::

        >>> obj = 'foo'
        >>> list(always_iterable(obj))
        ['foo']

    If *base_type* is set, objects for which ``isinstance(obj, base_type)``
    returns ``True`` won't be considered iterable.

        >>> obj = {'a': 1}
        >>> list(always_iterable(obj))  # Iterate over the dict's keys
        ['a']
        >>> list(always_iterable(obj, base_type=dict))  # Treat dicts as a unit
        [{'a': 1}]

    Set *base_type* to ``None`` to avoid any special handling and treat objects
    Python considers iterable as iterable:

        >>> obj = 'foo'
        >>> list(always_iterable(obj, base_type=None))
        ['f', 'o', 'o']
    """"""
    if obj is None:
        return iter(())

    if (base_type is not None) and isinstance(obj, base_type):
        return iter((obj,))

    try:
        return iter(obj)
    except TypeError:
        return iter((obj,))"
493	jackson	1	"#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# ""License""); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
from __future__ import annotations

from unittest import mock

import pytest

from airflow.cli import cli_parser
from airflow.cli.commands import dag_processor_command
from airflow.configuration import conf
from tests.test_utils.config import conf_vars


class TestDagProcessorCommand:
    """"""
    Tests the CLI interface and that it correctly calls the DagProcessor
    """"""

    @classmethod
    def setup_class(cls):
        cls.parser = cli_parser.get_parser()

    @conf_vars(
        {
            (""scheduler"", ""standalone_dag_processor""): ""True"",
            (""core"", ""load_examples""): ""False"",
        }
    )
    @mock.patch(""airflow.cli.commands.dag_processor_command.DagProcessorJob"")
    @pytest.mark.skipif(
        conf.get_mandatory_value(""database"", ""sql_alchemy_conn"").lower().startswith(""sqlite""),
        reason=""Standalone Dag Processor doesn't support sqlite."",
    )
    def test_start_job(
        self,
        mock_dag_job,
    ):
        """"""Ensure that DagFileProcessorManager is started""""""
        with conf_vars({(""scheduler"", ""standalone_dag_processor""): ""True""}):
            args = self.parser.parse_args([""dag-processor""])
            dag_processor_command.dag_processor(args)
            mock_dag_job.return_value.run.assert_called()"
493	donghui	2	"#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# ""License""); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
from __future__ import annotations

from unittest import mock

import pytest

from airflow.cli import cli_parser
from airflow.cli.commands import dag_processor_command
from airflow.configuration import conf
from tests.test_utils.config import conf_vars


class TestDagProcessorCommand:
    """"""
    Tests the CLI interface and that it correctly calls the DagProcessor
    """"""

    @classmethod
    def setup_class(cls):
        cls.parser = cli_parser.get_parser()

    @conf_vars(
        {
            (""scheduler"", ""standalone_dag_processor""): ""True"",
            (""core"", ""load_examples""): ""False"",
        }
    )
    @mock.patch(""airflow.cli.commands.dag_processor_command.DagProcessorJob"")
    @pytest.mark.skipif(
        conf.get_mandatory_value(""database"", ""sql_alchemy_conn"").lower().startswith(""sqlite""),
        reason=""Standalone Dag Processor doesn't support sqlite."",
    )
    def test_start_job(
        self,
        mock_dag_job,
    ):
        """"""Ensure that DagFileProcessorManager is started""""""
        with conf_vars({(""scheduler"", ""standalone_dag_processor""): ""True""}):
            args = self.parser.parse_args([""dag-processor""])
            dag_processor_command.dag_processor(args)
            mock_dag_job.return_value.run.assert_called()"
431	jackson	3	"import os
import string
import urllib.parse
import urllib.request
from typing import Optional

from .compat import WINDOWS


def get_url_scheme(url: str) -> Optional[str]:
    if "":"" not in url:
        return None
    return url.split("":"", 1)[0].lower()


def path_to_url(path: str) -> str:
    """"""
    Convert a path to a file: URL.  The path will be made absolute and have
    quoted path parts.
    """"""
    path = os.path.normpath(os.path.abspath(path))
    url = urllib.parse.urljoin(""file:"", urllib.request.pathname2url(path))
    return url


def url_to_path(url: str) -> str:
    """"""
    Convert a file: URL to a path.
    """"""
    assert url.startswith(
        ""file:""
    ), f""You can only turn file: urls into filenames (not {url!r})""

    _, netloc, path, _, _ = urllib.parse.urlsplit(url)

    if not netloc or netloc == ""localhost"":
        # According to RFC 8089, same as empty authority.
        netloc = """"
    elif WINDOWS:
        # If we have a UNC path, prepend UNC share notation.
        netloc = ""\\\\"" + netloc
    else:
        raise ValueError(
            f""non-local file URIs are not supported on this platform: {url!r}""
        )

    path = urllib.request.url2pathname(netloc + path)

    # On Windows, urlsplit parses the path as something like ""/C:/Users/foo"".
    # This creates issues for path-related functions like io.open(), so we try
    # to detect and strip the leading slash.
    if (
        WINDOWS
        and not netloc  # Not UNC.
        and len(path) >= 3
        and path[0] == ""/""  # Leading slash to strip.
        and path[1] in string.ascii_letters  # Drive letter.
        and path[2:4] in ("":"", "":/"")  # Colon + end of string, or colon + absolute path.
    ):
        path = path[1:]

    return path"
431	donghui	3	"import os
import string
import urllib.parse
import urllib.request
from typing import Optional

from .compat import WINDOWS


def get_url_scheme(url: str) -> Optional[str]:
    if "":"" not in url:
        return None
    return url.split("":"", 1)[0].lower()


def path_to_url(path: str) -> str:
    """"""
    Convert a path to a file: URL.  The path will be made absolute and have
    quoted path parts.
    """"""
    path = os.path.normpath(os.path.abspath(path))
    url = urllib.parse.urljoin(""file:"", urllib.request.pathname2url(path))
    return url


def url_to_path(url: str) -> str:
    """"""
    Convert a file: URL to a path.
    """"""
    assert url.startswith(
        ""file:""
    ), f""You can only turn file: urls into filenames (not {url!r})""

    _, netloc, path, _, _ = urllib.parse.urlsplit(url)

    if not netloc or netloc == ""localhost"":
        # According to RFC 8089, same as empty authority.
        netloc = """"
    elif WINDOWS:
        # If we have a UNC path, prepend UNC share notation.
        netloc = ""\\\\"" + netloc
    else:
        raise ValueError(
            f""non-local file URIs are not supported on this platform: {url!r}""
        )

    path = urllib.request.url2pathname(netloc + path)

    # On Windows, urlsplit parses the path as something like ""/C:/Users/foo"".
    # This creates issues for path-related functions like io.open(), so we try
    # to detect and strip the leading slash.
    if (
        WINDOWS
        and not netloc  # Not UNC.
        and len(path) >= 3
        and path[0] == ""/""  # Leading slash to strip.
        and path[1] in string.ascii_letters  # Drive letter.
        and path[2:4] in ("":"", "":/"")  # Colon + end of string, or colon + absolute path.
    ):
        path = path[1:]

    return path"
460	jackson	1	"#!/usr/bin/env python
"""""" pygame.examples.setmodescale

On high resolution displays(4k, 1080p) and tiny graphics games (640x480)
show up very small so that they are unplayable. SCALED scales up the window
for you. The game thinks it's a 640x480 window, but really it can be bigger.
Mouse events are scaled for you, so your game doesn't need to do it.

Passing SCALED to pygame.display.set_mode means the resolution depends
on desktop size and the graphics are scaled.
""""""

import pygame as pg

pg.init()

RES = (160, 120)
FPS = 30
clock = pg.time.Clock()

print(""desktops"", pg.display.get_desktop_sizes())
screen = pg.display.set_mode(RES, pg.SCALED | pg.RESIZABLE)

# MAIN LOOP

done = False

i = 0
j = 0

r_name, r_flags = pg.display._get_renderer_info()
print(""renderer:"", r_name, ""flags:"", bin(r_flags))
for flag, name in [
    (1, ""software""),
    (2, ""accelerated""),
    (4, ""VSync""),
    (8, ""render to texture""),
]:
    if flag & r_flags:
        print(name)

while not done:
    for event in pg.event.get():
        if event.type == pg.KEYDOWN and event.key == pg.K_q:
            done = True
        if event.type == pg.QUIT:
            done = True
        if event.type == pg.KEYDOWN and event.key == pg.K_f:
            pg.display.toggle_fullscreen()
        if event.type == pg.VIDEORESIZE:
            pg.display._resize_event(event)

    i += 1
    i = i % screen.get_width()
    j += i % 2
    j = j % screen.get_height()

    screen.fill((255, 0, 255))
    pg.draw.circle(screen, (0, 0, 0), (100, 100), 20)
    pg.draw.circle(screen, (0, 0, 200), (0, 0), 10)
    pg.draw.circle(screen, (200, 0, 0), (160, 120), 30)
    pg.draw.line(screen, (250, 250, 0), (0, 120), (160, 0))
    pg.draw.circle(screen, (255, 255, 255), (i, j), 5)

    pg.display.flip()
    clock.tick(FPS)
pg.quit()"
460	donghui	1	"#!/usr/bin/env python
"""""" pygame.examples.setmodescale

On high resolution displays(4k, 1080p) and tiny graphics games (640x480)
show up very small so that they are unplayable. SCALED scales up the window
for you. The game thinks it's a 640x480 window, but really it can be bigger.
Mouse events are scaled for you, so your game doesn't need to do it.

Passing SCALED to pygame.display.set_mode means the resolution depends
on desktop size and the graphics are scaled.
""""""

import pygame as pg

pg.init()

RES = (160, 120)
FPS = 30
clock = pg.time.Clock()

print(""desktops"", pg.display.get_desktop_sizes())
screen = pg.display.set_mode(RES, pg.SCALED | pg.RESIZABLE)

# MAIN LOOP

done = False

i = 0
j = 0

r_name, r_flags = pg.display._get_renderer_info()
print(""renderer:"", r_name, ""flags:"", bin(r_flags))
for flag, name in [
    (1, ""software""),
    (2, ""accelerated""),
    (4, ""VSync""),
    (8, ""render to texture""),
]:
    if flag & r_flags:
        print(name)

while not done:
    for event in pg.event.get():
        if event.type == pg.KEYDOWN and event.key == pg.K_q:
            done = True
        if event.type == pg.QUIT:
            done = True
        if event.type == pg.KEYDOWN and event.key == pg.K_f:
            pg.display.toggle_fullscreen()
        if event.type == pg.VIDEORESIZE:
            pg.display._resize_event(event)

    i += 1
    i = i % screen.get_width()
    j += i % 2
    j = j % screen.get_height()

    screen.fill((255, 0, 255))
    pg.draw.circle(screen, (0, 0, 0), (100, 100), 20)
    pg.draw.circle(screen, (0, 0, 200), (0, 0), 10)
    pg.draw.circle(screen, (200, 0, 0), (160, 120), 30)
    pg.draw.line(screen, (250, 250, 0), (0, 120), (160, 0))
    pg.draw.circle(screen, (255, 255, 255), (i, j), 5)

    pg.display.flip()
    clock.tick(FPS)
pg.quit()"
412	jackson	2	"import numpy as np
import pytest

import pandas as pd
from pandas import (
    Index,
    MultiIndex,
)


# Note: identical the ""multi"" entry in the top-level ""index"" fixture
@pytest.fixture
def idx():
    # a MultiIndex used to test the general functionality of the
    # general functionality of this object
    major_axis = Index([""foo"", ""bar"", ""baz"", ""qux""])
    minor_axis = Index([""one"", ""two""])

    major_codes = np.array([0, 0, 1, 2, 3, 3])
    minor_codes = np.array([0, 1, 0, 1, 0, 1])
    index_names = [""first"", ""second""]
    mi = MultiIndex(
        levels=[major_axis, minor_axis],
        codes=[major_codes, minor_codes],
        names=index_names,
        verify_integrity=False,
    )
    return mi


@pytest.fixture
def idx_dup():
    # compare tests/indexes/multi/conftest.py
    major_axis = Index([""foo"", ""bar"", ""baz"", ""qux""])
    minor_axis = Index([""one"", ""two""])

    major_codes = np.array([0, 0, 1, 0, 1, 1])
    minor_codes = np.array([0, 1, 0, 1, 0, 1])
    index_names = [""first"", ""second""]
    mi = MultiIndex(
        levels=[major_axis, minor_axis],
        codes=[major_codes, minor_codes],
        names=index_names,
        verify_integrity=False,
    )
    return mi


@pytest.fixture
def index_names():
    # names that match those in the idx fixture for testing equality of
    # names assigned to the idx
    return [""first"", ""second""]


@pytest.fixture
def narrow_multi_index():
    """"""
    Return a MultiIndex that is narrower than the display (<80 characters).
    """"""
    n = 1000
    ci = pd.CategoricalIndex(list(""a"" * n) + ([""abc""] * n))
    dti = pd.date_range(""2000-01-01"", freq=""s"", periods=n * 2)
    return MultiIndex.from_arrays([ci, ci.codes + 9, dti], names=[""a"", ""b"", ""dti""])


@pytest.fixture
def wide_multi_index():
    """"""
    Return a MultiIndex that is wider than the display (>80 characters).
    """"""
    n = 1000
    ci = pd.CategoricalIndex(list(""a"" * n) + ([""abc""] * n))
    dti = pd.date_range(""2000-01-01"", freq=""s"", periods=n * 2)
    levels = [ci, ci.codes + 9, dti, dti, dti]
    names = [""a"", ""b"", ""dti_1"", ""dti_2"", ""dti_3""]
    return MultiIndex.from_arrays(levels, names=names)"
412	donghui	2	"import numpy as np
import pytest

import pandas as pd
from pandas import (
    Index,
    MultiIndex,
)


# Note: identical the ""multi"" entry in the top-level ""index"" fixture
@pytest.fixture
def idx():
    # a MultiIndex used to test the general functionality of the
    # general functionality of this object
    major_axis = Index([""foo"", ""bar"", ""baz"", ""qux""])
    minor_axis = Index([""one"", ""two""])

    major_codes = np.array([0, 0, 1, 2, 3, 3])
    minor_codes = np.array([0, 1, 0, 1, 0, 1])
    index_names = [""first"", ""second""]
    mi = MultiIndex(
        levels=[major_axis, minor_axis],
        codes=[major_codes, minor_codes],
        names=index_names,
        verify_integrity=False,
    )
    return mi


@pytest.fixture
def idx_dup():
    # compare tests/indexes/multi/conftest.py
    major_axis = Index([""foo"", ""bar"", ""baz"", ""qux""])
    minor_axis = Index([""one"", ""two""])

    major_codes = np.array([0, 0, 1, 0, 1, 1])
    minor_codes = np.array([0, 1, 0, 1, 0, 1])
    index_names = [""first"", ""second""]
    mi = MultiIndex(
        levels=[major_axis, minor_axis],
        codes=[major_codes, minor_codes],
        names=index_names,
        verify_integrity=False,
    )
    return mi


@pytest.fixture
def index_names():
    # names that match those in the idx fixture for testing equality of
    # names assigned to the idx
    return [""first"", ""second""]


@pytest.fixture
def narrow_multi_index():
    """"""
    Return a MultiIndex that is narrower than the display (<80 characters).
    """"""
    n = 1000
    ci = pd.CategoricalIndex(list(""a"" * n) + ([""abc""] * n))
    dti = pd.date_range(""2000-01-01"", freq=""s"", periods=n * 2)
    return MultiIndex.from_arrays([ci, ci.codes + 9, dti], names=[""a"", ""b"", ""dti""])


@pytest.fixture
def wide_multi_index():
    """"""
    Return a MultiIndex that is wider than the display (>80 characters).
    """"""
    n = 1000
    ci = pd.CategoricalIndex(list(""a"" * n) + ([""abc""] * n))
    dti = pd.date_range(""2000-01-01"", freq=""s"", periods=n * 2)
    levels = [ci, ci.codes + 9, dti, dti, dti]
    names = [""a"", ""b"", ""dti_1"", ""dti_2"", ""dti_3""]
    return MultiIndex.from_arrays(levels, names=names)"
443	jackson	0	"# This file is dual licensed under the terms of the Apache License, Version
# 2.0, and the BSD License. See the LICENSE file in the root of this repository
# for complete details.


import typing

from cryptography.hazmat.primitives.asymmetric import dh
from cryptography.hazmat.primitives.asymmetric.types import (
    PRIVATE_KEY_TYPES,
    PUBLIC_KEY_TYPES,
)


def load_pem_private_key(
    data: bytes,
    password: typing.Optional[bytes],
    backend: typing.Any = None,
    *,
    unsafe_skip_rsa_key_validation: bool = False,
) -> PRIVATE_KEY_TYPES:
    from cryptography.hazmat.backends.openssl.backend import backend as ossl

    return ossl.load_pem_private_key(
        data, password, unsafe_skip_rsa_key_validation
    )


def load_pem_public_key(
    data: bytes, backend: typing.Any = None
) -> PUBLIC_KEY_TYPES:
    from cryptography.hazmat.backends.openssl.backend import backend as ossl

    return ossl.load_pem_public_key(data)


def load_pem_parameters(
    data: bytes, backend: typing.Any = None
) -> ""dh.DHParameters"":
    from cryptography.hazmat.backends.openssl.backend import backend as ossl

    return ossl.load_pem_parameters(data)


def load_der_private_key(
    data: bytes,
    password: typing.Optional[bytes],
    backend: typing.Any = None,
    *,
    unsafe_skip_rsa_key_validation: bool = False,
) -> PRIVATE_KEY_TYPES:
    from cryptography.hazmat.backends.openssl.backend import backend as ossl

    return ossl.load_der_private_key(
        data, password, unsafe_skip_rsa_key_validation
    )


def load_der_public_key(
    data: bytes, backend: typing.Any = None
) -> PUBLIC_KEY_TYPES:
    from cryptography.hazmat.backends.openssl.backend import backend as ossl

    return ossl.load_der_public_key(data)


def load_der_parameters(
    data: bytes, backend: typing.Any = None
) -> ""dh.DHParameters"":
    from cryptography.hazmat.backends.openssl.backend import backend as ossl

    return ossl.load_der_parameters(data)"
443	donghui	0	"# This file is dual licensed under the terms of the Apache License, Version
# 2.0, and the BSD License. See the LICENSE file in the root of this repository
# for complete details.


import typing

from cryptography.hazmat.primitives.asymmetric import dh
from cryptography.hazmat.primitives.asymmetric.types import (
    PRIVATE_KEY_TYPES,
    PUBLIC_KEY_TYPES,
)


def load_pem_private_key(
    data: bytes,
    password: typing.Optional[bytes],
    backend: typing.Any = None,
    *,
    unsafe_skip_rsa_key_validation: bool = False,
) -> PRIVATE_KEY_TYPES:
    from cryptography.hazmat.backends.openssl.backend import backend as ossl

    return ossl.load_pem_private_key(
        data, password, unsafe_skip_rsa_key_validation
    )


def load_pem_public_key(
    data: bytes, backend: typing.Any = None
) -> PUBLIC_KEY_TYPES:
    from cryptography.hazmat.backends.openssl.backend import backend as ossl

    return ossl.load_pem_public_key(data)


def load_pem_parameters(
    data: bytes, backend: typing.Any = None
) -> ""dh.DHParameters"":
    from cryptography.hazmat.backends.openssl.backend import backend as ossl

    return ossl.load_pem_parameters(data)


def load_der_private_key(
    data: bytes,
    password: typing.Optional[bytes],
    backend: typing.Any = None,
    *,
    unsafe_skip_rsa_key_validation: bool = False,
) -> PRIVATE_KEY_TYPES:
    from cryptography.hazmat.backends.openssl.backend import backend as ossl

    return ossl.load_der_private_key(
        data, password, unsafe_skip_rsa_key_validation
    )


def load_der_public_key(
    data: bytes, backend: typing.Any = None
) -> PUBLIC_KEY_TYPES:
    from cryptography.hazmat.backends.openssl.backend import backend as ossl

    return ossl.load_der_public_key(data)


def load_der_parameters(
    data: bytes, backend: typing.Any = None
) -> ""dh.DHParameters"":
    from cryptography.hazmat.backends.openssl.backend import backend as ossl

    return ossl.load_der_parameters(data)"
484	jackson	1	"""""""
views.py        # Houses `SchemaView`, `APIView` subclass.

See schemas.__init__.py for package overview.
""""""
from rest_framework import exceptions, renderers
from rest_framework.response import Response
from rest_framework.schemas import coreapi
from rest_framework.settings import api_settings
from rest_framework.views import APIView


class SchemaView(APIView):
    _ignore_model_permissions = True
    schema = None  # exclude from schema
    renderer_classes = None
    schema_generator = None
    public = False

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        if self.renderer_classes is None:
            if coreapi.is_enabled():
                self.renderer_classes = [
                    renderers.CoreAPIOpenAPIRenderer,
                    renderers.CoreJSONRenderer,
                ]
            else:
                self.renderer_classes = [
                    renderers.OpenAPIRenderer,
                    renderers.JSONOpenAPIRenderer,
                ]
            if renderers.BrowsableAPIRenderer in api_settings.DEFAULT_RENDERER_CLASSES:
                self.renderer_classes += [renderers.BrowsableAPIRenderer]

    def get(self, request, *args, **kwargs):
        schema = self.schema_generator.get_schema(request, self.public)
        if schema is None:
            raise exceptions.PermissionDenied()
        return Response(schema)

    def handle_exception(self, exc):
        # Schema renderers do not render exceptions, so re-perform content
        # negotiation with default renderers.
        self.renderer_classes = api_settings.DEFAULT_RENDERER_CLASSES
        neg = self.perform_content_negotiation(self.request, force=True)
        self.request.accepted_renderer, self.request.accepted_media_type = neg
        return super().handle_exception(exc)"
484	donghui	1	"""""""
views.py        # Houses `SchemaView`, `APIView` subclass.

See schemas.__init__.py for package overview.
""""""
from rest_framework import exceptions, renderers
from rest_framework.response import Response
from rest_framework.schemas import coreapi
from rest_framework.settings import api_settings
from rest_framework.views import APIView


class SchemaView(APIView):
    _ignore_model_permissions = True
    schema = None  # exclude from schema
    renderer_classes = None
    schema_generator = None
    public = False

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        if self.renderer_classes is None:
            if coreapi.is_enabled():
                self.renderer_classes = [
                    renderers.CoreAPIOpenAPIRenderer,
                    renderers.CoreJSONRenderer,
                ]
            else:
                self.renderer_classes = [
                    renderers.OpenAPIRenderer,
                    renderers.JSONOpenAPIRenderer,
                ]
            if renderers.BrowsableAPIRenderer in api_settings.DEFAULT_RENDERER_CLASSES:
                self.renderer_classes += [renderers.BrowsableAPIRenderer]

    def get(self, request, *args, **kwargs):
        schema = self.schema_generator.get_schema(request, self.public)
        if schema is None:
            raise exceptions.PermissionDenied()
        return Response(schema)

    def handle_exception(self, exc):
        # Schema renderers do not render exceptions, so re-perform content
        # negotiation with default renderers.
        self.renderer_classes = api_settings.DEFAULT_RENDERER_CLASSES
        neg = self.perform_content_negotiation(self.request, force=True)
        self.request.accepted_renderer, self.request.accepted_media_type = neg
        return super().handle_exception(exc)"
477	jackson	3	"""""""
    pygments.styles.autumn
    ~~~~~~~~~~~~~~~~~~~~~~

    A colorful style, inspired by the terminal highlighting style.

    :copyright: Copyright 2006-2022 by the Pygments team, see AUTHORS.
    :license: BSD, see LICENSE for details.
""""""

from pygments.style import Style
from pygments.token import Keyword, Name, Comment, String, Error, \
     Number, Operator, Generic, Whitespace


class AutumnStyle(Style):
    """"""
    A colorful style, inspired by the terminal highlighting style.
    """"""

    styles = {
        Whitespace:                 '#bbbbbb',

        Comment:                    'italic #aaaaaa',
        Comment.Preproc:            'noitalic #4c8317',
        Comment.Special:            'italic #0000aa',

        Keyword:                    '#0000aa',
        Keyword.Type:               '#00aaaa',

        Operator.Word:              '#0000aa',

        Name.Builtin:               '#00aaaa',
        Name.Function:              '#00aa00',
        Name.Class:                 'underline #00aa00',
        Name.Namespace:             'underline #00aaaa',
        Name.Variable:              '#aa0000',
        Name.Constant:              '#aa0000',
        Name.Entity:                'bold #800',
        Name.Attribute:             '#1e90ff',
        Name.Tag:                   'bold #1e90ff',
        Name.Decorator:             '#888888',

        String:                     '#aa5500',
        String.Symbol:              '#0000aa',
        String.Regex:               '#009999',

        Number:                     '#009999',

        Generic.Heading:            'bold #000080',
        Generic.Subheading:         'bold #800080',
        Generic.Deleted:            '#aa0000',
        Generic.Inserted:           '#00aa00',
        Generic.Error:              '#aa0000',
        Generic.Emph:               'italic',
        Generic.Strong:             'bold',
        Generic.Prompt:             '#555555',
        Generic.Output:             '#888888',
        Generic.Traceback:          '#aa0000',

        Error:                      '#F00 bg:#FAA'
    }"
477	donghui	2	"""""""
    pygments.styles.autumn
    ~~~~~~~~~~~~~~~~~~~~~~

    A colorful style, inspired by the terminal highlighting style.

    :copyright: Copyright 2006-2022 by the Pygments team, see AUTHORS.
    :license: BSD, see LICENSE for details.
""""""

from pygments.style import Style
from pygments.token import Keyword, Name, Comment, String, Error, \
     Number, Operator, Generic, Whitespace


class AutumnStyle(Style):
    """"""
    A colorful style, inspired by the terminal highlighting style.
    """"""

    styles = {
        Whitespace:                 '#bbbbbb',

        Comment:                    'italic #aaaaaa',
        Comment.Preproc:            'noitalic #4c8317',
        Comment.Special:            'italic #0000aa',

        Keyword:                    '#0000aa',
        Keyword.Type:               '#00aaaa',

        Operator.Word:              '#0000aa',

        Name.Builtin:               '#00aaaa',
        Name.Function:              '#00aa00',
        Name.Class:                 'underline #00aa00',
        Name.Namespace:             'underline #00aaaa',
        Name.Variable:              '#aa0000',
        Name.Constant:              '#aa0000',
        Name.Entity:                'bold #800',
        Name.Attribute:             '#1e90ff',
        Name.Tag:                   'bold #1e90ff',
        Name.Decorator:             '#888888',

        String:                     '#aa5500',
        String.Symbol:              '#0000aa',
        String.Regex:               '#009999',

        Number:                     '#009999',

        Generic.Heading:            'bold #000080',
        Generic.Subheading:         'bold #800080',
        Generic.Deleted:            '#aa0000',
        Generic.Inserted:           '#00aa00',
        Generic.Error:              '#aa0000',
        Generic.Emph:               'italic',
        Generic.Strong:             'bold',
        Generic.Prompt:             '#555555',
        Generic.Output:             '#888888',
        Generic.Traceback:          '#aa0000',

        Error:                      '#F00 bg:#FAA'
    }"
426	jackson	2	"""""""
Aliases for functions which may be accelerated by Scipy.

Scipy_ can be built to use accelerated or otherwise improved libraries
for FFTs, linear algebra, and special functions. This module allows
developers to transparently support these accelerated functions when
scipy is available but still support users who have only installed
NumPy.

.. _Scipy : https://www.scipy.org

""""""
# This module should be used for functions both in numpy and scipy if
#  you want to use the numpy version if available but the scipy version
#  otherwise.
#  Usage  --- from numpy.dual import fft, inv

__all__ = ['fft', 'ifft', 'fftn', 'ifftn', 'fft2', 'ifft2',
           'norm', 'inv', 'svd', 'solve', 'det', 'eig', 'eigvals',
           'eigh', 'eigvalsh', 'lstsq', 'pinv', 'cholesky', 'i0']

import numpy.linalg as linpkg
import numpy.fft as fftpkg
from numpy.lib import i0
import sys


fft = fftpkg.fft
ifft = fftpkg.ifft
fftn = fftpkg.fftn
ifftn = fftpkg.ifftn
fft2 = fftpkg.fft2
ifft2 = fftpkg.ifft2

norm = linpkg.norm
inv = linpkg.inv
svd = linpkg.svd
solve = linpkg.solve
det = linpkg.det
eig = linpkg.eig
eigvals = linpkg.eigvals
eigh = linpkg.eigh
eigvalsh = linpkg.eigvalsh
lstsq = linpkg.lstsq
pinv = linpkg.pinv
cholesky = linpkg.cholesky

_restore_dict = {}

def register_func(name, func):
    if name not in __all__:
        raise ValueError(""{} not a dual function."".format(name))
    f = sys._getframe(0).f_globals
    _restore_dict[name] = f[name]
    f[name] = func

def restore_func(name):
    if name not in __all__:
        raise ValueError(""{} not a dual function."".format(name))
    try:
        val = _restore_dict[name]
    except KeyError:
        return
    else:
        sys._getframe(0).f_globals[name] = val

def restore_all():
    for name in _restore_dict.keys():
        restore_func(name)"
426	donghui	1	"""""""
Aliases for functions which may be accelerated by Scipy.

Scipy_ can be built to use accelerated or otherwise improved libraries
for FFTs, linear algebra, and special functions. This module allows
developers to transparently support these accelerated functions when
scipy is available but still support users who have only installed
NumPy.

.. _Scipy : https://www.scipy.org

""""""
# This module should be used for functions both in numpy and scipy if
#  you want to use the numpy version if available but the scipy version
#  otherwise.
#  Usage  --- from numpy.dual import fft, inv

__all__ = ['fft', 'ifft', 'fftn', 'ifftn', 'fft2', 'ifft2',
           'norm', 'inv', 'svd', 'solve', 'det', 'eig', 'eigvals',
           'eigh', 'eigvalsh', 'lstsq', 'pinv', 'cholesky', 'i0']

import numpy.linalg as linpkg
import numpy.fft as fftpkg
from numpy.lib import i0
import sys


fft = fftpkg.fft
ifft = fftpkg.ifft
fftn = fftpkg.fftn
ifftn = fftpkg.ifftn
fft2 = fftpkg.fft2
ifft2 = fftpkg.ifft2

norm = linpkg.norm
inv = linpkg.inv
svd = linpkg.svd
solve = linpkg.solve
det = linpkg.det
eig = linpkg.eig
eigvals = linpkg.eigvals
eigh = linpkg.eigh
eigvalsh = linpkg.eigvalsh
lstsq = linpkg.lstsq
pinv = linpkg.pinv
cholesky = linpkg.cholesky

_restore_dict = {}

def register_func(name, func):
    if name not in __all__:
        raise ValueError(""{} not a dual function."".format(name))
    f = sys._getframe(0).f_globals
    _restore_dict[name] = f[name]
    f[name] = func

def restore_func(name):
    if name not in __all__:
        raise ValueError(""{} not a dual function."".format(name))
    try:
        val = _restore_dict[name]
    except KeyError:
        return
    else:
        sys._getframe(0).f_globals[name] = val

def restore_all():
    for name in _restore_dict.keys():
        restore_func(name)"
368	jackson	0	"# mssql/__init__.py
# Copyright (C) 2005-2023 the SQLAlchemy authors and contributors
# <see AUTHORS file>
#
# This module is part of SQLAlchemy and is released under
# the MIT License: https://www.opensource.org/licenses/mit-license.php
# mypy: ignore-errors


from . import base  # noqa
from . import pymssql  # noqa
from . import pyodbc  # noqa
from .base import BIGINT
from .base import BINARY
from .base import BIT
from .base import CHAR
from .base import DATE
from .base import DATETIME
from .base import DATETIME2
from .base import DATETIMEOFFSET
from .base import DECIMAL
from .base import FLOAT
from .base import IMAGE
from .base import INTEGER
from .base import JSON
from .base import MONEY
from .base import NCHAR
from .base import NTEXT
from .base import NUMERIC
from .base import NVARCHAR
from .base import REAL
from .base import ROWVERSION
from .base import SMALLDATETIME
from .base import SMALLINT
from .base import SMALLMONEY
from .base import SQL_VARIANT
from .base import TEXT
from .base import TIME
from .base import TIMESTAMP
from .base import TINYINT
from .base import try_cast
from .base import UNIQUEIDENTIFIER
from .base import VARBINARY
from .base import VARCHAR
from .base import XML


base.dialect = dialect = pyodbc.dialect


__all__ = (
    ""JSON"",
    ""INTEGER"",
    ""BIGINT"",
    ""SMALLINT"",
    ""TINYINT"",
    ""VARCHAR"",
    ""NVARCHAR"",
    ""CHAR"",
    ""NCHAR"",
    ""TEXT"",
    ""NTEXT"",
    ""DECIMAL"",
    ""NUMERIC"",
    ""FLOAT"",
    ""DATETIME"",
    ""DATETIME2"",
    ""DATETIMEOFFSET"",
    ""DATE"",
    ""TIME"",
    ""SMALLDATETIME"",
    ""BINARY"",
    ""VARBINARY"",
    ""BIT"",
    ""REAL"",
    ""IMAGE"",
    ""TIMESTAMP"",
    ""ROWVERSION"",
    ""MONEY"",
    ""SMALLMONEY"",
    ""UNIQUEIDENTIFIER"",
    ""SQL_VARIANT"",
    ""XML"",
    ""dialect"",
    ""try_cast"",
)"
368	donghui	0	"# mssql/__init__.py
# Copyright (C) 2005-2023 the SQLAlchemy authors and contributors
# <see AUTHORS file>
#
# This module is part of SQLAlchemy and is released under
# the MIT License: https://www.opensource.org/licenses/mit-license.php
# mypy: ignore-errors


from . import base  # noqa
from . import pymssql  # noqa
from . import pyodbc  # noqa
from .base import BIGINT
from .base import BINARY
from .base import BIT
from .base import CHAR
from .base import DATE
from .base import DATETIME
from .base import DATETIME2
from .base import DATETIMEOFFSET
from .base import DECIMAL
from .base import FLOAT
from .base import IMAGE
from .base import INTEGER
from .base import JSON
from .base import MONEY
from .base import NCHAR
from .base import NTEXT
from .base import NUMERIC
from .base import NVARCHAR
from .base import REAL
from .base import ROWVERSION
from .base import SMALLDATETIME
from .base import SMALLINT
from .base import SMALLMONEY
from .base import SQL_VARIANT
from .base import TEXT
from .base import TIME
from .base import TIMESTAMP
from .base import TINYINT
from .base import try_cast
from .base import UNIQUEIDENTIFIER
from .base import VARBINARY
from .base import VARCHAR
from .base import XML


base.dialect = dialect = pyodbc.dialect


__all__ = (
    ""JSON"",
    ""INTEGER"",
    ""BIGINT"",
    ""SMALLINT"",
    ""TINYINT"",
    ""VARCHAR"",
    ""NVARCHAR"",
    ""CHAR"",
    ""NCHAR"",
    ""TEXT"",
    ""NTEXT"",
    ""DECIMAL"",
    ""NUMERIC"",
    ""FLOAT"",
    ""DATETIME"",
    ""DATETIME2"",
    ""DATETIMEOFFSET"",
    ""DATE"",
    ""TIME"",
    ""SMALLDATETIME"",
    ""BINARY"",
    ""VARBINARY"",
    ""BIT"",
    ""REAL"",
    ""IMAGE"",
    ""TIMESTAMP"",
    ""ROWVERSION"",
    ""MONEY"",
    ""SMALLMONEY"",
    ""UNIQUEIDENTIFIER"",
    ""SQL_VARIANT"",
    ""XML"",
    ""dialect"",
    ""try_cast"",
)"
339	jackson	0	"from fontTools.misc.textTools import safeEval
from . import DefaultTable
import struct


GASP_SYMMETRIC_GRIDFIT = 0x0004
GASP_SYMMETRIC_SMOOTHING = 0x0008
GASP_DOGRAY = 0x0002
GASP_GRIDFIT = 0x0001


class table__g_a_s_p(DefaultTable.DefaultTable):
    def decompile(self, data, ttFont):
        self.version, numRanges = struct.unpack("">HH"", data[:4])
        assert 0 <= self.version <= 1, ""unknown 'gasp' format: %s"" % self.version
        data = data[4:]
        self.gaspRange = {}
        for i in range(numRanges):
            rangeMaxPPEM, rangeGaspBehavior = struct.unpack("">HH"", data[:4])
            self.gaspRange[int(rangeMaxPPEM)] = int(rangeGaspBehavior)
            data = data[4:]
        assert not data, ""too much data""

    def compile(self, ttFont):
        version = 0  # ignore self.version
        numRanges = len(self.gaspRange)
        data = b""""
        items = sorted(self.gaspRange.items())
        for rangeMaxPPEM, rangeGaspBehavior in items:
            data = data + struct.pack("">HH"", rangeMaxPPEM, rangeGaspBehavior)
            if rangeGaspBehavior & ~(GASP_GRIDFIT | GASP_DOGRAY):
                version = 1
        data = struct.pack("">HH"", version, numRanges) + data
        return data

    def toXML(self, writer, ttFont):
        items = sorted(self.gaspRange.items())
        for rangeMaxPPEM, rangeGaspBehavior in items:
            writer.simpletag(
                ""gaspRange"",
                [
                    (""rangeMaxPPEM"", rangeMaxPPEM),
                    (""rangeGaspBehavior"", rangeGaspBehavior),
                ],
            )
            writer.newline()

    def fromXML(self, name, attrs, content, ttFont):
        if name != ""gaspRange"":
            return
        if not hasattr(self, ""gaspRange""):
            self.gaspRange = {}
        self.gaspRange[safeEval(attrs[""rangeMaxPPEM""])] = safeEval(
            attrs[""rangeGaspBehavior""]
        )"
339	donghui	0	"from fontTools.misc.textTools import safeEval
from . import DefaultTable
import struct


GASP_SYMMETRIC_GRIDFIT = 0x0004
GASP_SYMMETRIC_SMOOTHING = 0x0008
GASP_DOGRAY = 0x0002
GASP_GRIDFIT = 0x0001


class table__g_a_s_p(DefaultTable.DefaultTable):
    def decompile(self, data, ttFont):
        self.version, numRanges = struct.unpack("">HH"", data[:4])
        assert 0 <= self.version <= 1, ""unknown 'gasp' format: %s"" % self.version
        data = data[4:]
        self.gaspRange = {}
        for i in range(numRanges):
            rangeMaxPPEM, rangeGaspBehavior = struct.unpack("">HH"", data[:4])
            self.gaspRange[int(rangeMaxPPEM)] = int(rangeGaspBehavior)
            data = data[4:]
        assert not data, ""too much data""

    def compile(self, ttFont):
        version = 0  # ignore self.version
        numRanges = len(self.gaspRange)
        data = b""""
        items = sorted(self.gaspRange.items())
        for rangeMaxPPEM, rangeGaspBehavior in items:
            data = data + struct.pack("">HH"", rangeMaxPPEM, rangeGaspBehavior)
            if rangeGaspBehavior & ~(GASP_GRIDFIT | GASP_DOGRAY):
                version = 1
        data = struct.pack("">HH"", version, numRanges) + data
        return data

    def toXML(self, writer, ttFont):
        items = sorted(self.gaspRange.items())
        for rangeMaxPPEM, rangeGaspBehavior in items:
            writer.simpletag(
                ""gaspRange"",
                [
                    (""rangeMaxPPEM"", rangeMaxPPEM),
                    (""rangeGaspBehavior"", rangeGaspBehavior),
                ],
            )
            writer.newline()

    def fromXML(self, name, attrs, content, ttFont):
        if name != ""gaspRange"":
            return
        if not hasattr(self, ""gaspRange""):
            self.gaspRange = {}
        self.gaspRange[safeEval(attrs[""rangeMaxPPEM""])] = safeEval(
            attrs[""rangeGaspBehavior""]
        )"
279	jackson	4	"""""""Pillow (Fork of the Python Imaging Library)

Pillow is the friendly PIL fork by Alex Clark and Contributors.
    https://github.com/python-pillow/Pillow/

Pillow is forked from PIL 1.1.7.

PIL is the Python Imaging Library by Fredrik Lundh and Contributors.
Copyright (c) 1999 by Secret Labs AB.

Use PIL.__version__ for this Pillow version.

;-)
""""""

from . import _version

# VERSION was removed in Pillow 6.0.0.
# PILLOW_VERSION was removed in Pillow 9.0.0.
# Use __version__ instead.
__version__ = _version.__version__
del _version


_plugins = [
    ""BlpImagePlugin"",
    ""BmpImagePlugin"",
    ""BufrStubImagePlugin"",
    ""CurImagePlugin"",
    ""DcxImagePlugin"",
    ""DdsImagePlugin"",
    ""EpsImagePlugin"",
    ""FitsImagePlugin"",
    ""FitsStubImagePlugin"",
    ""FliImagePlugin"",
    ""FpxImagePlugin"",
    ""FtexImagePlugin"",
    ""GbrImagePlugin"",
    ""GifImagePlugin"",
    ""GribStubImagePlugin"",
    ""Hdf5StubImagePlugin"",
    ""IcnsImagePlugin"",
    ""IcoImagePlugin"",
    ""ImImagePlugin"",
    ""ImtImagePlugin"",
    ""IptcImagePlugin"",
    ""JpegImagePlugin"",
    ""Jpeg2KImagePlugin"",
    ""McIdasImagePlugin"",
    ""MicImagePlugin"",
    ""MpegImagePlugin"",
    ""MpoImagePlugin"",
    ""MspImagePlugin"",
    ""PalmImagePlugin"",
    ""PcdImagePlugin"",
    ""PcxImagePlugin"",
    ""PdfImagePlugin"",
    ""PixarImagePlugin"",
    ""PngImagePlugin"",
    ""PpmImagePlugin"",
    ""PsdImagePlugin"",
    ""SgiImagePlugin"",
    ""SpiderImagePlugin"",
    ""SunImagePlugin"",
    ""TgaImagePlugin"",
    ""TiffImagePlugin"",
    ""WebPImagePlugin"",
    ""WmfImagePlugin"",
    ""XbmImagePlugin"",
    ""XpmImagePlugin"",
    ""XVThumbImagePlugin"",
]


class UnidentifiedImageError(OSError):
    """"""
    Raised in :py:meth:`PIL.Image.open` if an image cannot be opened and identified.
    """"""

    pass"
279	donghui	2	"""""""Pillow (Fork of the Python Imaging Library)

Pillow is the friendly PIL fork by Alex Clark and Contributors.
    https://github.com/python-pillow/Pillow/

Pillow is forked from PIL 1.1.7.

PIL is the Python Imaging Library by Fredrik Lundh and Contributors.
Copyright (c) 1999 by Secret Labs AB.

Use PIL.__version__ for this Pillow version.

;-)
""""""

from . import _version

# VERSION was removed in Pillow 6.0.0.
# PILLOW_VERSION was removed in Pillow 9.0.0.
# Use __version__ instead.
__version__ = _version.__version__
del _version


_plugins = [
    ""BlpImagePlugin"",
    ""BmpImagePlugin"",
    ""BufrStubImagePlugin"",
    ""CurImagePlugin"",
    ""DcxImagePlugin"",
    ""DdsImagePlugin"",
    ""EpsImagePlugin"",
    ""FitsImagePlugin"",
    ""FitsStubImagePlugin"",
    ""FliImagePlugin"",
    ""FpxImagePlugin"",
    ""FtexImagePlugin"",
    ""GbrImagePlugin"",
    ""GifImagePlugin"",
    ""GribStubImagePlugin"",
    ""Hdf5StubImagePlugin"",
    ""IcnsImagePlugin"",
    ""IcoImagePlugin"",
    ""ImImagePlugin"",
    ""ImtImagePlugin"",
    ""IptcImagePlugin"",
    ""JpegImagePlugin"",
    ""Jpeg2KImagePlugin"",
    ""McIdasImagePlugin"",
    ""MicImagePlugin"",
    ""MpegImagePlugin"",
    ""MpoImagePlugin"",
    ""MspImagePlugin"",
    ""PalmImagePlugin"",
    ""PcdImagePlugin"",
    ""PcxImagePlugin"",
    ""PdfImagePlugin"",
    ""PixarImagePlugin"",
    ""PngImagePlugin"",
    ""PpmImagePlugin"",
    ""PsdImagePlugin"",
    ""SgiImagePlugin"",
    ""SpiderImagePlugin"",
    ""SunImagePlugin"",
    ""TgaImagePlugin"",
    ""TiffImagePlugin"",
    ""WebPImagePlugin"",
    ""WmfImagePlugin"",
    ""XbmImagePlugin"",
    ""XpmImagePlugin"",
    ""XVThumbImagePlugin"",
]


class UnidentifiedImageError(OSError):
    """"""
    Raised in :py:meth:`PIL.Image.open` if an image cannot be opened and identified.
    """"""

    pass"
269	jackson	3	"# -*- coding: utf-8 -*-
""""""Payload system for IPython.

Authors:

* Fernando Perez
* Brian Granger
""""""

#-----------------------------------------------------------------------------
#       Copyright (C) 2008-2011 The IPython Development Team
#
#  Distributed under the terms of the BSD License.  The full license is in
#  the file COPYING, distributed as part of this software.
#-----------------------------------------------------------------------------

#-----------------------------------------------------------------------------
# Imports
#-----------------------------------------------------------------------------

from traitlets.config.configurable import Configurable
from traitlets import List

#-----------------------------------------------------------------------------
# Main payload class
#-----------------------------------------------------------------------------

class PayloadManager(Configurable):

    _payload = List([])

    def write_payload(self, data, single=True):
        """"""Include or update the specified `data` payload in the PayloadManager.

        If a previous payload with the same source exists and `single` is True,
        it will be overwritten with the new one.
        """"""

        if not isinstance(data, dict):
            raise TypeError('Each payload write must be a dict, got: %r' % data)

        if single and 'source' in data:
            source = data['source']
            for i, pl in enumerate(self._payload):
                if 'source' in pl and pl['source'] == source:
                    self._payload[i] = data
                    return

        self._payload.append(data)

    def read_payload(self):
        return self._payload

    def clear_payload(self):
        self._payload = []"
269	donghui	1	"# -*- coding: utf-8 -*-
""""""Payload system for IPython.

Authors:

* Fernando Perez
* Brian Granger
""""""

#-----------------------------------------------------------------------------
#       Copyright (C) 2008-2011 The IPython Development Team
#
#  Distributed under the terms of the BSD License.  The full license is in
#  the file COPYING, distributed as part of this software.
#-----------------------------------------------------------------------------

#-----------------------------------------------------------------------------
# Imports
#-----------------------------------------------------------------------------

from traitlets.config.configurable import Configurable
from traitlets import List

#-----------------------------------------------------------------------------
# Main payload class
#-----------------------------------------------------------------------------

class PayloadManager(Configurable):

    _payload = List([])

    def write_payload(self, data, single=True):
        """"""Include or update the specified `data` payload in the PayloadManager.

        If a previous payload with the same source exists and `single` is True,
        it will be overwritten with the new one.
        """"""

        if not isinstance(data, dict):
            raise TypeError('Each payload write must be a dict, got: %r' % data)

        if single and 'source' in data:
            source = data['source']
            for i, pl in enumerate(self._payload):
                if 'source' in pl and pl['source'] == source:
                    self._payload[i] = data
                    return

        self._payload.append(data)

    def read_payload(self):
        return self._payload

    def clear_payload(self):
        self._payload = []"
329	jackson	1	"from splunk.persistconn.application import PersistentServerConnectionApplication
import json
import requests
import logging


class request(PersistentServerConnectionApplication):
    def __init__(self, command_line, command_arg, logger=None):
        super(PersistentServerConnectionApplication, self).__init__()
        self.logger = logger
        if self.logger == None:
            self.logger = logging.getLogger(f""splunk.appserver.badmsc"")

        PersistentServerConnectionApplication.__init__(self)

    def handle(self, in_string):
        args = json.loads(in_string)

        if args[""method""] != ""POST"":
            self.logger.info(f""Method {args['method']} not allowed"")
            return {
                ""payload"": ""Method Not Allowed"",
                ""status"": 405,
                ""headers"": {""Allow"": ""POST""},
            }

        try:
            options = json.loads(args[""payload""])
        except Exception as e:
            self.logger.info(f""Invalid payload. {e}"")
            return {""payload"": ""Invalid JSON payload"", ""status"": 400}

        self.logger.info(args[""payload""])

        # Handle local requests by adding FQDN and auth token
        if options[""url""].startswith(""/services""):
            options[""verify""] = False
            options[""url""] = f""{args['server']['rest_uri']}{options['url']}""
            options[""headers""][
                ""Authorization""
            ] = f""Splunk {args['session']['authtoken']}""
        elif not (
            options[""url""].startswith(""https://"")
            or options[""url""].startswith(""http://"")
        ):
            options[""url""] = f""https://{options['url']}""

        try:
            r = requests.request(**options)
            self.logger.info(f""{r.status_code} {r.text}"")
            return {""payload"": r.text, ""status"": r.status_code}
        except Exception as e:
            self.logger.info(f""Request failed. {e}"")
            return {""payload"": str(e), ""status"": 500}"
329	donghui	1	"from splunk.persistconn.application import PersistentServerConnectionApplication
import json
import requests
import logging


class request(PersistentServerConnectionApplication):
    def __init__(self, command_line, command_arg, logger=None):
        super(PersistentServerConnectionApplication, self).__init__()
        self.logger = logger
        if self.logger == None:
            self.logger = logging.getLogger(f""splunk.appserver.badmsc"")

        PersistentServerConnectionApplication.__init__(self)

    def handle(self, in_string):
        args = json.loads(in_string)

        if args[""method""] != ""POST"":
            self.logger.info(f""Method {args['method']} not allowed"")
            return {
                ""payload"": ""Method Not Allowed"",
                ""status"": 405,
                ""headers"": {""Allow"": ""POST""},
            }

        try:
            options = json.loads(args[""payload""])
        except Exception as e:
            self.logger.info(f""Invalid payload. {e}"")
            return {""payload"": ""Invalid JSON payload"", ""status"": 400}

        self.logger.info(args[""payload""])

        # Handle local requests by adding FQDN and auth token
        if options[""url""].startswith(""/services""):
            options[""verify""] = False
            options[""url""] = f""{args['server']['rest_uri']}{options['url']}""
            options[""headers""][
                ""Authorization""
            ] = f""Splunk {args['session']['authtoken']}""
        elif not (
            options[""url""].startswith(""https://"")
            or options[""url""].startswith(""http://"")
        ):
            options[""url""] = f""https://{options['url']}""

        try:
            r = requests.request(**options)
            self.logger.info(f""{r.status_code} {r.text}"")
            return {""payload"": r.text, ""status"": r.status_code}
        except Exception as e:
            self.logger.info(f""Request failed. {e}"")
            return {""payload"": str(e), ""status"": 500}"
378	jackson	0	"######################## BEGIN LICENSE BLOCK ########################
# The Original Code is mozilla.org code.
#
# The Initial Developer of the Original Code is
# Netscape Communications Corporation.
# Portions created by the Initial Developer are Copyright (C) 1998
# the Initial Developer. All Rights Reserved.
#
# Contributor(s):
#   Mark Pilgrim - port to Python
#
# This library is free software; you can redistribute it and/or
# modify it under the terms of the GNU Lesser General Public
# License as published by the Free Software Foundation; either
# version 2.1 of the License, or (at your option) any later version.
#
# This library is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public
# License along with this library; if not, write to the Free Software
# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA
# 02110-1301  USA
######################### END LICENSE BLOCK #########################

from .chardistribution import EUCTWDistributionAnalysis
from .codingstatemachine import CodingStateMachine
from .mbcharsetprober import MultiByteCharSetProber
from .mbcssm import EUCTW_SM_MODEL


class EUCTWProber(MultiByteCharSetProber):
    def __init__(self) -> None:
        super().__init__()
        self.coding_sm = CodingStateMachine(EUCTW_SM_MODEL)
        self.distribution_analyzer = EUCTWDistributionAnalysis()
        self.reset()

    @property
    def charset_name(self) -> str:
        return ""EUC-TW""

    @property
    def language(self) -> str:
        return ""Taiwan"""
378	donghui	0	"######################## BEGIN LICENSE BLOCK ########################
# The Original Code is mozilla.org code.
#
# The Initial Developer of the Original Code is
# Netscape Communications Corporation.
# Portions created by the Initial Developer are Copyright (C) 1998
# the Initial Developer. All Rights Reserved.
#
# Contributor(s):
#   Mark Pilgrim - port to Python
#
# This library is free software; you can redistribute it and/or
# modify it under the terms of the GNU Lesser General Public
# License as published by the Free Software Foundation; either
# version 2.1 of the License, or (at your option) any later version.
#
# This library is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public
# License along with this library; if not, write to the Free Software
# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA
# 02110-1301  USA
######################### END LICENSE BLOCK #########################

from .chardistribution import EUCTWDistributionAnalysis
from .codingstatemachine import CodingStateMachine
from .mbcharsetprober import MultiByteCharSetProber
from .mbcssm import EUCTW_SM_MODEL


class EUCTWProber(MultiByteCharSetProber):
    def __init__(self) -> None:
        super().__init__()
        self.coding_sm = CodingStateMachine(EUCTW_SM_MODEL)
        self.distribution_analyzer = EUCTWDistributionAnalysis()
        self.reset()

    @property
    def charset_name(self) -> str:
        return ""EUC-TW""

    @property
    def language(self) -> str:
        return ""Taiwan"""
436	jackson	1	"""""""xmlrpclib.Transport implementation
""""""

import logging
import urllib.parse
import xmlrpc.client
from typing import TYPE_CHECKING, Tuple

from pip._internal.exceptions import NetworkConnectionError
from pip._internal.network.session import PipSession
from pip._internal.network.utils import raise_for_status

if TYPE_CHECKING:
    from xmlrpc.client import _HostType, _Marshallable

logger = logging.getLogger(__name__)


class PipXmlrpcTransport(xmlrpc.client.Transport):
    """"""Provide a `xmlrpclib.Transport` implementation via a `PipSession`
    object.
    """"""

    def __init__(
        self, index_url: str, session: PipSession, use_datetime: bool = False
    ) -> None:
        super().__init__(use_datetime)
        index_parts = urllib.parse.urlparse(index_url)
        self._scheme = index_parts.scheme
        self._session = session

    def request(
        self,
        host: ""_HostType"",
        handler: str,
        request_body: bytes,
        verbose: bool = False,
    ) -> Tuple[""_Marshallable"", ...]:
        assert isinstance(host, str)
        parts = (self._scheme, host, handler, None, None, None)
        url = urllib.parse.urlunparse(parts)
        try:
            headers = {""Content-Type"": ""text/xml""}
            response = self._session.post(
                url,
                data=request_body,
                headers=headers,
                stream=True,
            )
            raise_for_status(response)
            self.verbose = verbose
            return self.parse_response(response.raw)
        except NetworkConnectionError as exc:
            assert exc.response
            logger.critical(
                ""HTTP error %s while getting %s"",
                exc.response.status_code,
                url,
            )
            raise"
436	donghui	1	"""""""xmlrpclib.Transport implementation
""""""

import logging
import urllib.parse
import xmlrpc.client
from typing import TYPE_CHECKING, Tuple

from pip._internal.exceptions import NetworkConnectionError
from pip._internal.network.session import PipSession
from pip._internal.network.utils import raise_for_status

if TYPE_CHECKING:
    from xmlrpc.client import _HostType, _Marshallable

logger = logging.getLogger(__name__)


class PipXmlrpcTransport(xmlrpc.client.Transport):
    """"""Provide a `xmlrpclib.Transport` implementation via a `PipSession`
    object.
    """"""

    def __init__(
        self, index_url: str, session: PipSession, use_datetime: bool = False
    ) -> None:
        super().__init__(use_datetime)
        index_parts = urllib.parse.urlparse(index_url)
        self._scheme = index_parts.scheme
        self._session = session

    def request(
        self,
        host: ""_HostType"",
        handler: str,
        request_body: bytes,
        verbose: bool = False,
    ) -> Tuple[""_Marshallable"", ...]:
        assert isinstance(host, str)
        parts = (self._scheme, host, handler, None, None, None)
        url = urllib.parse.urlunparse(parts)
        try:
            headers = {""Content-Type"": ""text/xml""}
            response = self._session.post(
                url,
                data=request_body,
                headers=headers,
                stream=True,
            )
            raise_for_status(response)
            self.verbose = verbose
            return self.parse_response(response.raw)
        except NetworkConnectionError as exc:
            assert exc.response
            logger.critical(
                ""HTTP error %s while getting %s"",
                exc.response.status_code,
                url,
            )
            raise"
467	jackson	2	"import json

from django import forms
from django.core.exceptions import ValidationError
from django.utils.translation import gettext_lazy as _

__all__ = [""HStoreField""]


class HStoreField(forms.CharField):
    """"""
    A field for HStore data which accepts dictionary JSON input.
    """"""

    widget = forms.Textarea
    default_error_messages = {
        ""invalid_json"": _(""Could not load JSON data.""),
        ""invalid_format"": _(""Input must be a JSON dictionary.""),
    }

    def prepare_value(self, value):
        if isinstance(value, dict):
            return json.dumps(value)
        return value

    def to_python(self, value):
        if not value:
            return {}
        if not isinstance(value, dict):
            try:
                value = json.loads(value)
            except json.JSONDecodeError:
                raise ValidationError(
                    self.error_messages[""invalid_json""],
                    code=""invalid_json"",
                )

        if not isinstance(value, dict):
            raise ValidationError(
                self.error_messages[""invalid_format""],
                code=""invalid_format"",
            )

        # Cast everything to strings for ease.
        for key, val in value.items():
            if val is not None:
                val = str(val)
            value[key] = val
        return value

    def has_changed(self, initial, data):
        """"""
        Return True if data differs from initial.
        """"""
        # For purposes of seeing whether something has changed, None is
        # the same as an empty dict, if the data or initial value we get
        # is None, replace it w/ {}.
        initial_value = self.to_python(initial)
        return super().has_changed(initial_value, data)"
467	donghui	2	"import json

from django import forms
from django.core.exceptions import ValidationError
from django.utils.translation import gettext_lazy as _

__all__ = [""HStoreField""]


class HStoreField(forms.CharField):
    """"""
    A field for HStore data which accepts dictionary JSON input.
    """"""

    widget = forms.Textarea
    default_error_messages = {
        ""invalid_json"": _(""Could not load JSON data.""),
        ""invalid_format"": _(""Input must be a JSON dictionary.""),
    }

    def prepare_value(self, value):
        if isinstance(value, dict):
            return json.dumps(value)
        return value

    def to_python(self, value):
        if not value:
            return {}
        if not isinstance(value, dict):
            try:
                value = json.loads(value)
            except json.JSONDecodeError:
                raise ValidationError(
                    self.error_messages[""invalid_json""],
                    code=""invalid_json"",
                )

        if not isinstance(value, dict):
            raise ValidationError(
                self.error_messages[""invalid_format""],
                code=""invalid_format"",
            )

        # Cast everything to strings for ease.
        for key, val in value.items():
            if val is not None:
                val = str(val)
            value[key] = val
        return value

    def has_changed(self, initial, data):
        """"""
        Return True if data differs from initial.
        """"""
        # For purposes of seeing whether something has changed, None is
        # the same as an empty dict, if the data or initial value we get
        # is None, replace it w/ {}.
        initial_value = self.to_python(initial)
        return super().has_changed(initial_value, data)"
494	jackson	2	"import _plotly_utils.basevalidators


class InsidetextfontValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(self, plotly_name=""insidetextfont"", parent_name=""funnel"", **kwargs):
        super(InsidetextfontValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Insidetextfont""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            color

            colorsrc
                Sets the source reference on Chart Studio Cloud
                for `color`.
            family
                HTML font family - the typeface that will be
                applied by the web browser. The web browser
                will only be able to apply a font if it is
                available on the system which it operates.
                Provide multiple font families, separated by
                commas, to indicate the preference in which to
                apply fonts if they aren't available on the
                system. The Chart Studio Cloud (at
                https://chart-studio.plotly.com or on-premise)
                generates images on a server, where only a
                select number of fonts are installed and
                supported. These include ""Arial"", ""Balto"",
                ""Courier New"", ""Droid Sans"",, ""Droid Serif"",
                ""Droid Sans Mono"", ""Gravitas One"", ""Old
                Standard TT"", ""Open Sans"", ""Overpass"", ""PT Sans
                Narrow"", ""Raleway"", ""Times New Roman"".
            familysrc
                Sets the source reference on Chart Studio Cloud
                for `family`.
            size

            sizesrc
                Sets the source reference on Chart Studio Cloud
                for `size`.
"""""",
            ),
            **kwargs,
        )"
494	donghui	1	"import _plotly_utils.basevalidators


class InsidetextfontValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(self, plotly_name=""insidetextfont"", parent_name=""funnel"", **kwargs):
        super(InsidetextfontValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Insidetextfont""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            color

            colorsrc
                Sets the source reference on Chart Studio Cloud
                for `color`.
            family
                HTML font family - the typeface that will be
                applied by the web browser. The web browser
                will only be able to apply a font if it is
                available on the system which it operates.
                Provide multiple font families, separated by
                commas, to indicate the preference in which to
                apply fonts if they aren't available on the
                system. The Chart Studio Cloud (at
                https://chart-studio.plotly.com or on-premise)
                generates images on a server, where only a
                select number of fonts are installed and
                supported. These include ""Arial"", ""Balto"",
                ""Courier New"", ""Droid Sans"",, ""Droid Serif"",
                ""Droid Sans Mono"", ""Gravitas One"", ""Old
                Standard TT"", ""Open Sans"", ""Overpass"", ""PT Sans
                Narrow"", ""Raleway"", ""Times New Roman"".
            familysrc
                Sets the source reference on Chart Studio Cloud
                for `family`.
            size

            sizesrc
                Sets the source reference on Chart Studio Cloud
                for `size`.
"""""",
            ),
            **kwargs,
        )"
453	jackson	0	"from node import HuffmanNode


class Huffman():

    def __init__(self, s:str) -> None:
        self.__weights = self.__get_weights(s)
        self.__buffer = [b' ' for _ in range(round(len(self.__weights)))]
        self.__tree = self.__build_huffman_tree(self.__weights)
        self.__code = self.__build_code(self.__tree)

    @property
    def weights(self):
        return self.__weights

    @property
    def tree(self):
        return self.__tree

    @property
    def code(self):
        return self.__code

    def __get_weights(self, s:str) -> dict:
        weights = dict()
        for i in s:
            weights[i] = weights.get(i, 0)+1
        return weights
    
    def __build_huffman_tree(self, weights:dict):
        nodes = [HuffmanNode(value, weight) for value,weight in weights.items()]
        while len(nodes) > 1:
            nodes.sort(key=lambda node:node.weight, reverse=True)
            c = HuffmanNode(value=None, weight=(nodes[-1].weight+nodes[-2].weight))
            c.left_node = nodes.pop()
            c.right_node = nodes.pop()
            nodes.append(c)
        return nodes[0]

    def __build_code(self, tree):
        
        def func(tree:HuffmanNode, length:int):
            nonlocal code, self
            node = tree
            if not node:
                return
            elif node.value:
                code[node.value] = b''.join( self.__buffer[:length] )
                return
            self.__buffer[length] = b'0'
            func(node.left_node, length+1)
            self.__buffer[length] = b'1'
            func(node.right_node, length+1)
        
        code = dict()
        func(tree, 0)
        return code


if __name__ == ""__main__"":
    s = ""aabbccdddeefgenajojfonadkjfwqnioaerweggrefdsfassasdfgr""

    huffman = Huffman(s)

    print(huffman.weights)
    print(huffman.code)"
453	donghui	0	"from node import HuffmanNode


class Huffman():

    def __init__(self, s:str) -> None:
        self.__weights = self.__get_weights(s)
        self.__buffer = [b' ' for _ in range(round(len(self.__weights)))]
        self.__tree = self.__build_huffman_tree(self.__weights)
        self.__code = self.__build_code(self.__tree)

    @property
    def weights(self):
        return self.__weights

    @property
    def tree(self):
        return self.__tree

    @property
    def code(self):
        return self.__code

    def __get_weights(self, s:str) -> dict:
        weights = dict()
        for i in s:
            weights[i] = weights.get(i, 0)+1
        return weights
    
    def __build_huffman_tree(self, weights:dict):
        nodes = [HuffmanNode(value, weight) for value,weight in weights.items()]
        while len(nodes) > 1:
            nodes.sort(key=lambda node:node.weight, reverse=True)
            c = HuffmanNode(value=None, weight=(nodes[-1].weight+nodes[-2].weight))
            c.left_node = nodes.pop()
            c.right_node = nodes.pop()
            nodes.append(c)
        return nodes[0]

    def __build_code(self, tree):
        
        def func(tree:HuffmanNode, length:int):
            nonlocal code, self
            node = tree
            if not node:
                return
            elif node.value:
                code[node.value] = b''.join( self.__buffer[:length] )
                return
            self.__buffer[length] = b'0'
            func(node.left_node, length+1)
            self.__buffer[length] = b'1'
            func(node.right_node, length+1)
        
        code = dict()
        func(tree, 0)
        return code


if __name__ == ""__main__"":
    s = ""aabbccdddeefgenajojfonadkjfwqnioaerweggrefdsfassasdfgr""

    huffman = Huffman(s)

    print(huffman.weights)
    print(huffman.code)"
513	jackson	1	"# coding: utf8
from __future__ import unicode_literals

from ...attrs import LIKE_NUM

_num_words = [
    ""אפס"",
    ""אחד"",
    ""אחת"",
    ""שתיים"",
    ""שתים"",
    ""שניים"",
    ""שנים"",
    ""שלוש"",
    ""שלושה"",
    ""ארבע"",
    ""ארבעה"",
    ""חמש"",
    ""חמישה"",
    ""שש"",
    ""שישה"",
    ""שבע"",
    ""שבעה"",
    ""שמונה"",
    ""תשע"",
    ""תשעה"",
    ""עשר"",
    ""עשרה"",
    ""אחד עשר"",
    ""אחת עשרה"",
    ""שנים עשר"",
    ""שתים עשרה"",
    ""שלושה עשר"",
    ""שלוש עשרה"",
    ""ארבעה עשר"",
    ""ארבע עשרה"",
    ""חמישה עשר"",
    ""חמש עשרה"",
    ""ששה עשר"",
    ""שש עשרה"",
    ""שבעה עשר"",
    ""שבע עשרה"",
    ""שמונה עשר"",
    ""שמונה עשרה"",
    ""תשעה עשר"",
    ""תשע עשרה"",
    ""עשרים"",
    ""שלושים"",
    ""ארבעים"",
    ""חמישים"",
    ""שישים"",
    ""שבעים"",
    ""שמונים"",
    ""תשעים"",
    ""מאה"",
    ""אלף"",
    ""מליון"",
    ""מליארד"",
    ""טריליון"",
]


_ordinal_words = [
    ""ראשון"",
    ""שני"",
    ""שלישי"",
    ""רביעי"",
    ""חמישי"",
    ""שישי"",
    ""שביעי"",
    ""שמיני"",
    ""תשיעי"",
    ""עשירי"",
]

def like_num(text):
    if text.startswith((""+"", ""-"", ""±"", ""~"")):
        text = text[1:]
    text = text.replace("","", """").replace(""."", """")
    if text.isdigit():
        return True

    if text.count(""/"") == 1:
        num, denom = text.split(""/"")
        if num.isdigit() and denom.isdigit():
            return True
    
    if text in _num_words:
        return True

    # CHeck ordinal number
    if text in _ordinal_words:
        return True
    return False


LEX_ATTRS = {LIKE_NUM: like_num}"
513	donghui	1	"# coding: utf8
from __future__ import unicode_literals

from ...attrs import LIKE_NUM

_num_words = [
    ""אפס"",
    ""אחד"",
    ""אחת"",
    ""שתיים"",
    ""שתים"",
    ""שניים"",
    ""שנים"",
    ""שלוש"",
    ""שלושה"",
    ""ארבע"",
    ""ארבעה"",
    ""חמש"",
    ""חמישה"",
    ""שש"",
    ""שישה"",
    ""שבע"",
    ""שבעה"",
    ""שמונה"",
    ""תשע"",
    ""תשעה"",
    ""עשר"",
    ""עשרה"",
    ""אחד עשר"",
    ""אחת עשרה"",
    ""שנים עשר"",
    ""שתים עשרה"",
    ""שלושה עשר"",
    ""שלוש עשרה"",
    ""ארבעה עשר"",
    ""ארבע עשרה"",
    ""חמישה עשר"",
    ""חמש עשרה"",
    ""ששה עשר"",
    ""שש עשרה"",
    ""שבעה עשר"",
    ""שבע עשרה"",
    ""שמונה עשר"",
    ""שמונה עשרה"",
    ""תשעה עשר"",
    ""תשע עשרה"",
    ""עשרים"",
    ""שלושים"",
    ""ארבעים"",
    ""חמישים"",
    ""שישים"",
    ""שבעים"",
    ""שמונים"",
    ""תשעים"",
    ""מאה"",
    ""אלף"",
    ""מליון"",
    ""מליארד"",
    ""טריליון"",
]


_ordinal_words = [
    ""ראשון"",
    ""שני"",
    ""שלישי"",
    ""רביעי"",
    ""חמישי"",
    ""שישי"",
    ""שביעי"",
    ""שמיני"",
    ""תשיעי"",
    ""עשירי"",
]

def like_num(text):
    if text.startswith((""+"", ""-"", ""±"", ""~"")):
        text = text[1:]
    text = text.replace("","", """").replace(""."", """")
    if text.isdigit():
        return True

    if text.count(""/"") == 1:
        num, denom = text.split(""/"")
        if num.isdigit() and denom.isdigit():
            return True
    
    if text in _num_words:
        return True

    # CHeck ordinal number
    if text in _ordinal_words:
        return True
    return False


LEX_ATTRS = {LIKE_NUM: like_num}"
402	jackson	4	"# This file is distributed under the same license as the Django package.
#
# The *_FORMAT strings use the Django date format syntax,
# see https://docs.djangoproject.com/en/dev/ref/templates/builtins/#date
DATE_FORMAT = ""l, j F, Y""
TIME_FORMAT = ""h:i a""
DATETIME_FORMAT = ""j F, Y h:i a""
YEAR_MONTH_FORMAT = ""F, Y""
MONTH_DAY_FORMAT = ""j F""
SHORT_DATE_FORMAT = ""j.M.Y""
SHORT_DATETIME_FORMAT = ""j.M.Y H:i""
FIRST_DAY_OF_WEEK = 1  # (Monday)

# The *_INPUT_FORMATS strings use the Python strftime format syntax,
# see https://docs.python.org/library/datetime.html#strftime-strptime-behavior
# Kept ISO formats as they are in first position
DATE_INPUT_FORMATS = [
    ""%Y-%m-%d"",  # '2006-10-25'
    ""%m/%d/%Y"",  # '10/25/2006'
    ""%m/%d/%y"",  # '10/25/06'
    ""%d.%m.%Y"",  # '25.10.2006'
    ""%d.%m.%y"",  # '25.10.06'
    # ""%d %b %Y"",  # '25 Oct 2006'
    # ""%d %b, %Y"",  # '25 Oct, 2006'
    # ""%d %b. %Y"",  # '25 Oct. 2006'
    # ""%d %B %Y"",  # '25 October 2006'
    # ""%d %B, %Y"",  # '25 October, 2006'
]
DATETIME_INPUT_FORMATS = [
    ""%Y-%m-%d %H:%M:%S"",  # '2006-10-25 14:30:59'
    ""%Y-%m-%d %H:%M:%S.%f"",  # '2006-10-25 14:30:59.000200'
    ""%Y-%m-%d %H:%M"",  # '2006-10-25 14:30'
    ""%d.%m.%Y %H:%M:%S"",  # '25.10.2006 14:30:59'
    ""%d.%m.%Y %H:%M:%S.%f"",  # '25.10.2006 14:30:59.000200'
    ""%d.%m.%Y %H:%M"",  # '25.10.2006 14:30'
    ""%d.%m.%y %H:%M:%S"",  # '25.10.06 14:30:59'
    ""%d.%m.%y %H:%M:%S.%f"",  # '25.10.06 14:30:59.000200'
    ""%d.%m.%y %H:%M"",  # '25.10.06 14:30'
    ""%m/%d/%Y %H:%M:%S"",  # '10/25/2006 14:30:59'
    ""%m/%d/%Y %H:%M:%S.%f"",  # '10/25/2006 14:30:59.000200'
    ""%m/%d/%Y %H:%M"",  # '10/25/2006 14:30'
    ""%m/%d/%y %H:%M:%S"",  # '10/25/06 14:30:59'
    ""%m/%d/%y %H:%M:%S.%f"",  # '10/25/06 14:30:59.000200'
    ""%m/%d/%y %H:%M"",  # '10/25/06 14:30'
]
DECIMAL_SEPARATOR = "".""
THOUSAND_SEPARATOR = "" ""
NUMBER_GROUPING = 3"
402	donghui	1	"# This file is distributed under the same license as the Django package.
#
# The *_FORMAT strings use the Django date format syntax,
# see https://docs.djangoproject.com/en/dev/ref/templates/builtins/#date
DATE_FORMAT = ""l, j F, Y""
TIME_FORMAT = ""h:i a""
DATETIME_FORMAT = ""j F, Y h:i a""
YEAR_MONTH_FORMAT = ""F, Y""
MONTH_DAY_FORMAT = ""j F""
SHORT_DATE_FORMAT = ""j.M.Y""
SHORT_DATETIME_FORMAT = ""j.M.Y H:i""
FIRST_DAY_OF_WEEK = 1  # (Monday)

# The *_INPUT_FORMATS strings use the Python strftime format syntax,
# see https://docs.python.org/library/datetime.html#strftime-strptime-behavior
# Kept ISO formats as they are in first position
DATE_INPUT_FORMATS = [
    ""%Y-%m-%d"",  # '2006-10-25'
    ""%m/%d/%Y"",  # '10/25/2006'
    ""%m/%d/%y"",  # '10/25/06'
    ""%d.%m.%Y"",  # '25.10.2006'
    ""%d.%m.%y"",  # '25.10.06'
    # ""%d %b %Y"",  # '25 Oct 2006'
    # ""%d %b, %Y"",  # '25 Oct, 2006'
    # ""%d %b. %Y"",  # '25 Oct. 2006'
    # ""%d %B %Y"",  # '25 October 2006'
    # ""%d %B, %Y"",  # '25 October, 2006'
]
DATETIME_INPUT_FORMATS = [
    ""%Y-%m-%d %H:%M:%S"",  # '2006-10-25 14:30:59'
    ""%Y-%m-%d %H:%M:%S.%f"",  # '2006-10-25 14:30:59.000200'
    ""%Y-%m-%d %H:%M"",  # '2006-10-25 14:30'
    ""%d.%m.%Y %H:%M:%S"",  # '25.10.2006 14:30:59'
    ""%d.%m.%Y %H:%M:%S.%f"",  # '25.10.2006 14:30:59.000200'
    ""%d.%m.%Y %H:%M"",  # '25.10.2006 14:30'
    ""%d.%m.%y %H:%M:%S"",  # '25.10.06 14:30:59'
    ""%d.%m.%y %H:%M:%S.%f"",  # '25.10.06 14:30:59.000200'
    ""%d.%m.%y %H:%M"",  # '25.10.06 14:30'
    ""%m/%d/%Y %H:%M:%S"",  # '10/25/2006 14:30:59'
    ""%m/%d/%Y %H:%M:%S.%f"",  # '10/25/2006 14:30:59.000200'
    ""%m/%d/%Y %H:%M"",  # '10/25/2006 14:30'
    ""%m/%d/%y %H:%M:%S"",  # '10/25/06 14:30:59'
    ""%m/%d/%y %H:%M:%S.%f"",  # '10/25/06 14:30:59.000200'
    ""%m/%d/%y %H:%M"",  # '10/25/06 14:30'
]
DECIMAL_SEPARATOR = "".""
THOUSAND_SEPARATOR = "" ""
NUMBER_GROUPING = 3"
480	jackson	1	"""""""
Customized Mixin2to3 support:

 - adds support for converting doctests


This module raises an ImportError on Python 2.
""""""

from distutils.util import Mixin2to3 as _Mixin2to3
from distutils import log
from lib2to3.refactor import RefactoringTool, get_fixers_from_package

import setuptools


class DistutilsRefactoringTool(RefactoringTool):
    def log_error(self, msg, *args, **kw):
        log.error(msg, *args)

    def log_message(self, msg, *args):
        log.info(msg, *args)

    def log_debug(self, msg, *args):
        log.debug(msg, *args)


class Mixin2to3(_Mixin2to3):
    def run_2to3(self, files, doctests=False):
        # See of the distribution option has been set, otherwise check the
        # setuptools default.
        if self.distribution.use_2to3 is not True:
            return
        if not files:
            return
        log.info(""Fixing "" + "" "".join(files))
        self.__build_fixer_names()
        self.__exclude_fixers()
        if doctests:
            if setuptools.run_2to3_on_doctests:
                r = DistutilsRefactoringTool(self.fixer_names)
                r.refactor(files, write=True, doctests_only=True)
        else:
            _Mixin2to3.run_2to3(self, files)

    def __build_fixer_names(self):
        if self.fixer_names:
            return
        self.fixer_names = []
        for p in setuptools.lib2to3_fixer_packages:
            self.fixer_names.extend(get_fixers_from_package(p))
        if self.distribution.use_2to3_fixers is not None:
            for p in self.distribution.use_2to3_fixers:
                self.fixer_names.extend(get_fixers_from_package(p))

    def __exclude_fixers(self):
        excluded_fixers = getattr(self, 'exclude_fixers', [])
        if self.distribution.use_2to3_exclude_fixers is not None:
            excluded_fixers.extend(self.distribution.use_2to3_exclude_fixers)
        for fixer_name in excluded_fixers:
            if fixer_name in self.fixer_names:
                self.fixer_names.remove(fixer_name)"
480	donghui	1	"""""""
Customized Mixin2to3 support:

 - adds support for converting doctests


This module raises an ImportError on Python 2.
""""""

from distutils.util import Mixin2to3 as _Mixin2to3
from distutils import log
from lib2to3.refactor import RefactoringTool, get_fixers_from_package

import setuptools


class DistutilsRefactoringTool(RefactoringTool):
    def log_error(self, msg, *args, **kw):
        log.error(msg, *args)

    def log_message(self, msg, *args):
        log.info(msg, *args)

    def log_debug(self, msg, *args):
        log.debug(msg, *args)


class Mixin2to3(_Mixin2to3):
    def run_2to3(self, files, doctests=False):
        # See of the distribution option has been set, otherwise check the
        # setuptools default.
        if self.distribution.use_2to3 is not True:
            return
        if not files:
            return
        log.info(""Fixing "" + "" "".join(files))
        self.__build_fixer_names()
        self.__exclude_fixers()
        if doctests:
            if setuptools.run_2to3_on_doctests:
                r = DistutilsRefactoringTool(self.fixer_names)
                r.refactor(files, write=True, doctests_only=True)
        else:
            _Mixin2to3.run_2to3(self, files)

    def __build_fixer_names(self):
        if self.fixer_names:
            return
        self.fixer_names = []
        for p in setuptools.lib2to3_fixer_packages:
            self.fixer_names.extend(get_fixers_from_package(p))
        if self.distribution.use_2to3_fixers is not None:
            for p in self.distribution.use_2to3_fixers:
                self.fixer_names.extend(get_fixers_from_package(p))

    def __exclude_fixers(self):
        excluded_fixers = getattr(self, 'exclude_fixers', [])
        if self.distribution.use_2to3_exclude_fixers is not None:
            excluded_fixers.extend(self.distribution.use_2to3_exclude_fixers)
        for fixer_name in excluded_fixers:
            if fixer_name in self.fixer_names:
                self.fixer_names.remove(fixer_name)"
422	jackson	3	"import importlib.metadata
from typing import Any, Optional, Protocol, cast


class BadMetadata(ValueError):
    def __init__(self, dist: importlib.metadata.Distribution, *, reason: str) -> None:
        self.dist = dist
        self.reason = reason

    def __str__(self) -> str:
        return f""Bad metadata in {self.dist} ({self.reason})""


class BasePath(Protocol):
    """"""A protocol that various path objects conform.

    This exists because importlib.metadata uses both ``pathlib.Path`` and
    ``zipfile.Path``, and we need a common base for type hints (Union does not
    work well since ``zipfile.Path`` is too new for our linter setup).

    This does not mean to be exhaustive, but only contains things that present
    in both classes *that we need*.
    """"""

    @property
    def name(self) -> str:
        raise NotImplementedError()

    @property
    def parent(self) -> ""BasePath"":
        raise NotImplementedError()


def get_info_location(d: importlib.metadata.Distribution) -> Optional[BasePath]:
    """"""Find the path to the distribution's metadata directory.

    HACK: This relies on importlib.metadata's private ``_path`` attribute. Not
    all distributions exist on disk, so importlib.metadata is correct to not
    expose the attribute as public. But pip's code base is old and not as clean,
    so we do this to avoid having to rewrite too many things. Hopefully we can
    eliminate this some day.
    """"""
    return getattr(d, ""_path"", None)


def get_dist_name(dist: importlib.metadata.Distribution) -> str:
    """"""Get the distribution's project name.

    The ``name`` attribute is only available in Python 3.10 or later. We are
    targeting exactly that, but Mypy does not know this.
    """"""
    name = cast(Any, dist).name
    if not isinstance(name, str):
        raise BadMetadata(dist, reason=""invalid metadata entry 'name'"")
    return name"
422	donghui	3	"import importlib.metadata
from typing import Any, Optional, Protocol, cast


class BadMetadata(ValueError):
    def __init__(self, dist: importlib.metadata.Distribution, *, reason: str) -> None:
        self.dist = dist
        self.reason = reason

    def __str__(self) -> str:
        return f""Bad metadata in {self.dist} ({self.reason})""


class BasePath(Protocol):
    """"""A protocol that various path objects conform.

    This exists because importlib.metadata uses both ``pathlib.Path`` and
    ``zipfile.Path``, and we need a common base for type hints (Union does not
    work well since ``zipfile.Path`` is too new for our linter setup).

    This does not mean to be exhaustive, but only contains things that present
    in both classes *that we need*.
    """"""

    @property
    def name(self) -> str:
        raise NotImplementedError()

    @property
    def parent(self) -> ""BasePath"":
        raise NotImplementedError()


def get_info_location(d: importlib.metadata.Distribution) -> Optional[BasePath]:
    """"""Find the path to the distribution's metadata directory.

    HACK: This relies on importlib.metadata's private ``_path`` attribute. Not
    all distributions exist on disk, so importlib.metadata is correct to not
    expose the attribute as public. But pip's code base is old and not as clean,
    so we do this to avoid having to rewrite too many things. Hopefully we can
    eliminate this some day.
    """"""
    return getattr(d, ""_path"", None)


def get_dist_name(dist: importlib.metadata.Distribution) -> str:
    """"""Get the distribution's project name.

    The ``name`` attribute is only available in Python 3.10 or later. We are
    targeting exactly that, but Mypy does not know this.
    """"""
    name = cast(Any, dist).name
    if not isinstance(name, str):
        raise BadMetadata(dist, reason=""invalid metadata entry 'name'"")
    return name"
447	jackson	2	"import _plotly_utils.basevalidators


class LineValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(self, plotly_name=""line"", parent_name=""scatterpolar"", **kwargs):
        super(LineValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Line""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            backoff
                Sets the line back off from the end point of
                the nth line segment (in px). This option is
                useful e.g. to avoid overlap with arrowhead
                markers. With ""auto"" the lines would trim
                before markers if `marker.angleref` is set to
                ""previous"".
            backoffsrc
                Sets the source reference on Chart Studio Cloud
                for `backoff`.
            color
                Sets the line color.
            dash
                Sets the dash style of lines. Set to a dash
                type string (""solid"", ""dot"", ""dash"",
                ""longdash"", ""dashdot"", or ""longdashdot"") or a
                dash length list in px (eg ""5px,10px,2px,2px"").
            shape
                Determines the line shape. With ""spline"" the
                lines are drawn using spline interpolation. The
                other available values correspond to step-wise
                line shapes.
            smoothing
                Has an effect only if `shape` is set to
                ""spline"" Sets the amount of smoothing. 0
                corresponds to no smoothing (equivalent to a
                ""linear"" shape).
            width
                Sets the line width (in px).
"""""",
            ),
            **kwargs,
        )"
447	donghui	1	"import _plotly_utils.basevalidators


class LineValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(self, plotly_name=""line"", parent_name=""scatterpolar"", **kwargs):
        super(LineValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Line""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            backoff
                Sets the line back off from the end point of
                the nth line segment (in px). This option is
                useful e.g. to avoid overlap with arrowhead
                markers. With ""auto"" the lines would trim
                before markers if `marker.angleref` is set to
                ""previous"".
            backoffsrc
                Sets the source reference on Chart Studio Cloud
                for `backoff`.
            color
                Sets the line color.
            dash
                Sets the dash style of lines. Set to a dash
                type string (""solid"", ""dot"", ""dash"",
                ""longdash"", ""dashdot"", or ""longdashdot"") or a
                dash length list in px (eg ""5px,10px,2px,2px"").
            shape
                Determines the line shape. With ""spline"" the
                lines are drawn using spline interpolation. The
                other available values correspond to step-wise
                line shapes.
            smoothing
                Has an effect only if `shape` is set to
                ""spline"" Sets the amount of smoothing. 0
                corresponds to no smoothing (equivalent to a
                ""linear"" shape).
            width
                Sets the line width (in px).
"""""",
            ),
            **kwargs,
        )"
416	jackson	0	"import pytest

from pandas import (
    DatetimeIndex,
    date_range,
)
import pandas._testing as tm


def astype_non_nano(dti_nano, unit):
    # TODO(2.0): remove once DTI/DTA.astype supports non-nano
    if unit == ""ns"":
        return dti_nano

    dta_nano = dti_nano._data
    arr_nano = dta_nano._ndarray

    arr = arr_nano.astype(f""M8[{unit}]"")
    if dti_nano.tz is None:
        dtype = arr.dtype
    else:
        dtype = type(dti_nano.dtype)(tz=dti_nano.tz, unit=unit)
    dta = type(dta_nano)._simple_new(arr, dtype=dtype)
    dti = DatetimeIndex(dta, name=dti_nano.name)
    assert dti.dtype == dtype
    return dti


@pytest.mark.filterwarnings(""ignore::DeprecationWarning"")
@pytest.mark.parametrize(""tz"", [None, ""Asia/Shanghai"", ""Europe/Berlin""])
@pytest.mark.parametrize(""name"", [None, ""my_dti""])
@pytest.mark.parametrize(""unit"", [""ns"", ""us"", ""ms"", ""s""])
def test_dti_snap(name, tz, unit):
    dti = DatetimeIndex(
        [
            ""1/1/2002"",
            ""1/2/2002"",
            ""1/3/2002"",
            ""1/4/2002"",
            ""1/5/2002"",
            ""1/6/2002"",
            ""1/7/2002"",
        ],
        name=name,
        tz=tz,
        freq=""D"",
    )
    dti = astype_non_nano(dti, unit)

    result = dti.snap(freq=""W-MON"")
    expected = date_range(""12/31/2001"", ""1/7/2002"", name=name, tz=tz, freq=""w-mon"")
    expected = expected.repeat([3, 4])
    expected = astype_non_nano(expected, unit)
    tm.assert_index_equal(result, expected)
    assert result.tz == expected.tz
    assert result.freq is None
    assert expected.freq is None

    result = dti.snap(freq=""B"")

    expected = date_range(""1/1/2002"", ""1/7/2002"", name=name, tz=tz, freq=""b"")
    expected = expected.repeat([1, 1, 1, 2, 2])
    expected = astype_non_nano(expected, unit)
    tm.assert_index_equal(result, expected)
    assert result.tz == expected.tz
    assert result.freq is None
    assert expected.freq is None"
416	donghui	0	"import pytest

from pandas import (
    DatetimeIndex,
    date_range,
)
import pandas._testing as tm


def astype_non_nano(dti_nano, unit):
    # TODO(2.0): remove once DTI/DTA.astype supports non-nano
    if unit == ""ns"":
        return dti_nano

    dta_nano = dti_nano._data
    arr_nano = dta_nano._ndarray

    arr = arr_nano.astype(f""M8[{unit}]"")
    if dti_nano.tz is None:
        dtype = arr.dtype
    else:
        dtype = type(dti_nano.dtype)(tz=dti_nano.tz, unit=unit)
    dta = type(dta_nano)._simple_new(arr, dtype=dtype)
    dti = DatetimeIndex(dta, name=dti_nano.name)
    assert dti.dtype == dtype
    return dti


@pytest.mark.filterwarnings(""ignore::DeprecationWarning"")
@pytest.mark.parametrize(""tz"", [None, ""Asia/Shanghai"", ""Europe/Berlin""])
@pytest.mark.parametrize(""name"", [None, ""my_dti""])
@pytest.mark.parametrize(""unit"", [""ns"", ""us"", ""ms"", ""s""])
def test_dti_snap(name, tz, unit):
    dti = DatetimeIndex(
        [
            ""1/1/2002"",
            ""1/2/2002"",
            ""1/3/2002"",
            ""1/4/2002"",
            ""1/5/2002"",
            ""1/6/2002"",
            ""1/7/2002"",
        ],
        name=name,
        tz=tz,
        freq=""D"",
    )
    dti = astype_non_nano(dti, unit)

    result = dti.snap(freq=""W-MON"")
    expected = date_range(""12/31/2001"", ""1/7/2002"", name=name, tz=tz, freq=""w-mon"")
    expected = expected.repeat([3, 4])
    expected = astype_non_nano(expected, unit)
    tm.assert_index_equal(result, expected)
    assert result.tz == expected.tz
    assert result.freq is None
    assert expected.freq is None

    result = dti.snap(freq=""B"")

    expected = date_range(""1/1/2002"", ""1/7/2002"", name=name, tz=tz, freq=""b"")
    expected = expected.repeat([1, 1, 1, 2, 2])
    expected = astype_non_nano(expected, unit)
    tm.assert_index_equal(result, expected)
    assert result.tz == expected.tz
    assert result.freq is None
    assert expected.freq is None"
358	jackson	1	"# Copyright 2017-present Adtran, Inc.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from voltha.adapters.adtran_onu.pon_port import PonPort
from mock import MagicMock
import pytest

## Test class PonPort init settings  ###############
def test_PonPort_inits():
    handler = MagicMock()
    handler.device_id = 100
    portnum = 1
    testponport = PonPort(handler, portnum)

    assert testponport._enabled is False
    assert testponport._valid is True
    assert testponport._handler is handler
    assert testponport._deferred is None
    assert testponport._port is None
    assert testponport._port_number == 1
    assert testponport._entity_id is None
    assert testponport._next_entity_id == PonPort.MIN_GEM_ENTITY_ID




## Test PonPort staticmethod #########
def test_create():
    handler = MagicMock()
    handler.device_id = 200
    port_no = 2
    testcreate = PonPort.create(handler, port_no)

    assert isinstance(testcreate, PonPort)
    assert testcreate._handler is handler
    assert testcreate._port_number is port_no





## Test PonPort @property #########
def test_PonPort_properties():
    handler = MagicMock()
    handler.device_id = 300
    port_no = 3
    testprop1 = PonPort(handler, port_no)

    assert testprop1.enabled is False
    assert testprop1.port_number == 3
    assert testprop1.entity_id is None
    assert testprop1.next_gem_entity_id == PonPort.MIN_GEM_ENTITY_ID
    assert testprop1.tconts == {}
    assert testprop1.gem_ports == {}

"
358	donghui	1	"# Copyright 2017-present Adtran, Inc.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from voltha.adapters.adtran_onu.pon_port import PonPort
from mock import MagicMock
import pytest

## Test class PonPort init settings  ###############
def test_PonPort_inits():
    handler = MagicMock()
    handler.device_id = 100
    portnum = 1
    testponport = PonPort(handler, portnum)

    assert testponport._enabled is False
    assert testponport._valid is True
    assert testponport._handler is handler
    assert testponport._deferred is None
    assert testponport._port is None
    assert testponport._port_number == 1
    assert testponport._entity_id is None
    assert testponport._next_entity_id == PonPort.MIN_GEM_ENTITY_ID




## Test PonPort staticmethod #########
def test_create():
    handler = MagicMock()
    handler.device_id = 200
    port_no = 2
    testcreate = PonPort.create(handler, port_no)

    assert isinstance(testcreate, PonPort)
    assert testcreate._handler is handler
    assert testcreate._port_number is port_no





## Test PonPort @property #########
def test_PonPort_properties():
    handler = MagicMock()
    handler.device_id = 300
    port_no = 3
    testprop1 = PonPort(handler, port_no)

    assert testprop1.enabled is False
    assert testprop1.port_number == 3
    assert testprop1.entity_id is None
    assert testprop1.next_gem_entity_id == PonPort.MIN_GEM_ENTITY_ID
    assert testprop1.tconts == {}
    assert testprop1.gem_ports == {}

"
309	jackson	3	"from datetime import datetime
from .virtualtimescheduler import VirtualTimeScheduler


class HistoricalScheduler(VirtualTimeScheduler):
    """"""Provides a virtual time scheduler that uses datetime for absolute time
    and timedelta for relative time.""""""

    def __init__(self, initial_clock=None, comparer=None):
        """"""Creates a new historical scheduler with the specified initial clock
        value.

        Keyword arguments:
        initial_clock -- {Number} Initial value for the clock.
        comparer -- {Function} Comparer to determine causality of events based
            on absolute time.""""""

        def compare_datetimes(a, b):
            return (a > b) - (a < b)

        clock = initial_clock or datetime.fromtimestamp(0)
        comparer = comparer or compare_datetimes

        super(HistoricalScheduler, self).__init__(clock)

    @property
    def now(self):
        """"""Represents a notion of time for this scheduler. Tasks being scheduled
        on a scheduler will adhere to the time denoted by this property.""""""

        return self.clock

    @staticmethod
    def add(absolute, relative):
        """"""Adds a relative time value to an absolute time value.

        Keyword arguments:
        absolute -- {datetime} Absolute virtual time value.
        relative -- {timedelta} Relative virtual time value to add.

        Returns resulting absolute virtual time sum value.""""""

        return absolute + relative

    def to_datetime_offset(self, absolute):
        """"""Converts the absolute time value to a datetime value.""""""

        # datetime -> datetime
        return absolute

    def to_relative(self, timespan):
        """"""Converts the timespan value to a relative virtual time value.

        Keyword arguments:
        timespan -- {timedelta} Time_span value to convert.

        Returns corresponding relative virtual time value.""""""

        # timedelta -> timedelta
        return timespan"
309	donghui	3	"from datetime import datetime
from .virtualtimescheduler import VirtualTimeScheduler


class HistoricalScheduler(VirtualTimeScheduler):
    """"""Provides a virtual time scheduler that uses datetime for absolute time
    and timedelta for relative time.""""""

    def __init__(self, initial_clock=None, comparer=None):
        """"""Creates a new historical scheduler with the specified initial clock
        value.

        Keyword arguments:
        initial_clock -- {Number} Initial value for the clock.
        comparer -- {Function} Comparer to determine causality of events based
            on absolute time.""""""

        def compare_datetimes(a, b):
            return (a > b) - (a < b)

        clock = initial_clock or datetime.fromtimestamp(0)
        comparer = comparer or compare_datetimes

        super(HistoricalScheduler, self).__init__(clock)

    @property
    def now(self):
        """"""Represents a notion of time for this scheduler. Tasks being scheduled
        on a scheduler will adhere to the time denoted by this property.""""""

        return self.clock

    @staticmethod
    def add(absolute, relative):
        """"""Adds a relative time value to an absolute time value.

        Keyword arguments:
        absolute -- {datetime} Absolute virtual time value.
        relative -- {timedelta} Relative virtual time value to add.

        Returns resulting absolute virtual time sum value.""""""

        return absolute + relative

    def to_datetime_offset(self, absolute):
        """"""Converts the absolute time value to a datetime value.""""""

        # datetime -> datetime
        return absolute

    def to_relative(self, timespan):
        """"""Converts the timespan value to a relative virtual time value.

        Keyword arguments:
        timespan -- {timedelta} Time_span value to convert.

        Returns corresponding relative virtual time value.""""""

        # timedelta -> timedelta
        return timespan"
319	jackson	1	"from django.contrib.sites.models import Site
from django.db import models
from django.urls import NoReverseMatch, get_script_prefix, reverse
from django.utils.encoding import iri_to_uri
from django.utils.translation import gettext_lazy as _


class FlatPage(models.Model):
    url = models.CharField(_(""URL""), max_length=100, db_index=True)
    title = models.CharField(_(""title""), max_length=200)
    content = models.TextField(_(""content""), blank=True)
    enable_comments = models.BooleanField(_(""enable comments""), default=False)
    template_name = models.CharField(
        _(""template name""),
        max_length=70,
        blank=True,
        help_text=_(
            ""Example: “flatpages/contact_page.html”. If this isn’t provided, ""
            ""the system will use “flatpages/default.html”.""
        ),
    )
    registration_required = models.BooleanField(
        _(""registration required""),
        help_text=_(
            ""If this is checked, only logged-in users will be able to view the page.""
        ),
        default=False,
    )
    sites = models.ManyToManyField(Site, verbose_name=_(""sites""))

    class Meta:
        db_table = ""django_flatpage""
        verbose_name = _(""flat page"")
        verbose_name_plural = _(""flat pages"")
        ordering = [""url""]

    def __str__(self):
        return ""%s -- %s"" % (self.url, self.title)

    def get_absolute_url(self):
        from .views import flatpage

        for url in (self.url.lstrip(""/""), self.url):
            try:
                return reverse(flatpage, kwargs={""url"": url})
            except NoReverseMatch:
                pass
        # Handle script prefix manually because we bypass reverse()
        return iri_to_uri(get_script_prefix().rstrip(""/"") + self.url)"
319	donghui	1	"from django.contrib.sites.models import Site
from django.db import models
from django.urls import NoReverseMatch, get_script_prefix, reverse
from django.utils.encoding import iri_to_uri
from django.utils.translation import gettext_lazy as _


class FlatPage(models.Model):
    url = models.CharField(_(""URL""), max_length=100, db_index=True)
    title = models.CharField(_(""title""), max_length=200)
    content = models.TextField(_(""content""), blank=True)
    enable_comments = models.BooleanField(_(""enable comments""), default=False)
    template_name = models.CharField(
        _(""template name""),
        max_length=70,
        blank=True,
        help_text=_(
            ""Example: “flatpages/contact_page.html”. If this isn’t provided, ""
            ""the system will use “flatpages/default.html”.""
        ),
    )
    registration_required = models.BooleanField(
        _(""registration required""),
        help_text=_(
            ""If this is checked, only logged-in users will be able to view the page.""
        ),
        default=False,
    )
    sites = models.ManyToManyField(Site, verbose_name=_(""sites""))

    class Meta:
        db_table = ""django_flatpage""
        verbose_name = _(""flat page"")
        verbose_name_plural = _(""flat pages"")
        ordering = [""url""]

    def __str__(self):
        return ""%s -- %s"" % (self.url, self.title)

    def get_absolute_url(self):
        from .views import flatpage

        for url in (self.url.lstrip(""/""), self.url):
            try:
                return reverse(flatpage, kwargs={""url"": url})
            except NoReverseMatch:
                pass
        # Handle script prefix manually because we bypass reverse()
        return iri_to_uri(get_script_prefix().rstrip(""/"") + self.url)"
259	jackson	4	"""""""tst_tc1357_uxusylnz_68580 URL Configuration

The `urlpatterns` list routes URLs to views. For more information please see:
    https://docs.djangoproject.com/en/2.2/topics/http/urls/
Examples:
Function views
    1. Add an import:  from my_app import views
    2. Add a URL to urlpatterns:  path('', views.home, name='home')
Class-based views
    1. Add an import:  from other_app.views import Home
    2. Add a URL to urlpatterns:  path('', Home.as_view(), name='home')
Including another URLconf
    1. Import the include() function: from django.urls import include, path
    2. Add a URL to urlpatterns:  path('blog/', include('blog.urls'))
""""""

from django.contrib import admin
from django.urls import path, include, re_path
from django.views.generic.base import TemplateView
from allauth.account.views import confirm_email
from rest_framework import permissions
from drf_spectacular.views import SpectacularJSONAPIView, SpectacularSwaggerView

urlpatterns = [
    
    path(""accounts/"", include(""allauth.urls"")),
    path(""modules/"", include(""modules.urls"")),
    path(""api/v1/"", include(""home.api.v1.urls"")),
    path(""admin/"", admin.site.urls),
    path(""users/"", include(""users.urls"", namespace=""users"")),
    path(""rest-auth/"", include(""rest_auth.urls"")),
    # Override email confirm to use allauth's HTML view instead of rest_auth's API view
    path(""rest-auth/registration/account-confirm-email/<str:key>/"", confirm_email),
    path(""rest-auth/registration/"", include(""rest_auth.registration.urls"")),
]

admin.site.site_header = ""TST-TC1357-uxusylnzzz""
admin.site.site_title = ""TST-TC1357-uxusylnzzz Admin Portal""
admin.site.index_title = ""TST-TC1357-uxusylnzzz Admin""

# swagger
urlpatterns += [
    path(""api-docs/schema/"", SpectacularJSONAPIView.as_view(), name=""schema""),
    path(""api-docs/"", SpectacularSwaggerView.as_view(url_name='schema'), name=""api_docs"")
]


urlpatterns += [re_path(r"".*"",TemplateView.as_view(template_name='index.html'))]"
259	donghui	3	"""""""tst_tc1357_uxusylnz_68580 URL Configuration

The `urlpatterns` list routes URLs to views. For more information please see:
    https://docs.djangoproject.com/en/2.2/topics/http/urls/
Examples:
Function views
    1. Add an import:  from my_app import views
    2. Add a URL to urlpatterns:  path('', views.home, name='home')
Class-based views
    1. Add an import:  from other_app.views import Home
    2. Add a URL to urlpatterns:  path('', Home.as_view(), name='home')
Including another URLconf
    1. Import the include() function: from django.urls import include, path
    2. Add a URL to urlpatterns:  path('blog/', include('blog.urls'))
""""""

from django.contrib import admin
from django.urls import path, include, re_path
from django.views.generic.base import TemplateView
from allauth.account.views import confirm_email
from rest_framework import permissions
from drf_spectacular.views import SpectacularJSONAPIView, SpectacularSwaggerView

urlpatterns = [
    
    path(""accounts/"", include(""allauth.urls"")),
    path(""modules/"", include(""modules.urls"")),
    path(""api/v1/"", include(""home.api.v1.urls"")),
    path(""admin/"", admin.site.urls),
    path(""users/"", include(""users.urls"", namespace=""users"")),
    path(""rest-auth/"", include(""rest_auth.urls"")),
    # Override email confirm to use allauth's HTML view instead of rest_auth's API view
    path(""rest-auth/registration/account-confirm-email/<str:key>/"", confirm_email),
    path(""rest-auth/registration/"", include(""rest_auth.registration.urls"")),
]

admin.site.site_header = ""TST-TC1357-uxusylnzzz""
admin.site.site_title = ""TST-TC1357-uxusylnzzz Admin Portal""
admin.site.index_title = ""TST-TC1357-uxusylnzzz Admin""

# swagger
urlpatterns += [
    path(""api-docs/schema/"", SpectacularJSONAPIView.as_view(), name=""schema""),
    path(""api-docs/"", SpectacularSwaggerView.as_view(url_name='schema'), name=""api_docs"")
]


urlpatterns += [re_path(r"".*"",TemplateView.as_view(template_name='index.html'))]"
348	jackson	3	"""""""
https://adventofcode.com/2016/day/15
""""""
from utils import extract_ints, read_data

USE_TEST_DATA = False
SPLIT_BY_LINE = True
data = read_data(USE_TEST_DATA, SPLIT_BY_LINE)


def parse_data(data_in):
    """""" Read the input data to retrieve the disc positions """"""
    num_positions = []
    starting_pos = []

    for line in data_in:
        ints = extract_ints(line)
        num_positions.append(ints[1])
        starting_pos.append(ints[3])

    return num_positions, starting_pos


def is_at_zero(num_positions, starting_pos, disc_index, time):
    """""" Is the specified disc at the zero position at the given time? """"""
    pos = (starting_pos[disc_index] + time) % num_positions[disc_index]
    return pos == 0


def find_time(num_positions, starting_pos):
    """"""
    Find the time at which to release a capsule so that it passes through all
    discs successfully
    """"""

    # What's the first time that we can release the disc where it reaches the
    # first disc as it's at position 0?
    candidate_time = num_positions[0] - starting_pos[0] - 1

    while True:
        # Are all discs at position 0 when the capsule reaches them?
        collision = False
        for disc_index in range(len(num_positions)):
            time_capsule_reaches_disc = candidate_time + disc_index + 1
            if not is_at_zero(num_positions, starting_pos, disc_index, time_capsule_reaches_disc):
                collision = True
                break

        # There was no collision with any disc! candidate_time is the correct answer!
        if not collision:
            return candidate_time

        # There was a collision so candidate_time isn't a valid result.
        # Increment it to the next time that disc 1 (index 0) is at the zero position.
        candidate_time += num_positions[0]


positions, starting = parse_data(data)

# Part 1
# At what time can we release the capsule to pass through all of the discs?
print(find_time(positions, starting))

# Part 2
# What if we add another disc at the bottom?
positions.append(11)
starting.append(0)
print(find_time(positions, starting))"
348	donghui	3	"""""""
https://adventofcode.com/2016/day/15
""""""
from utils import extract_ints, read_data

USE_TEST_DATA = False
SPLIT_BY_LINE = True
data = read_data(USE_TEST_DATA, SPLIT_BY_LINE)


def parse_data(data_in):
    """""" Read the input data to retrieve the disc positions """"""
    num_positions = []
    starting_pos = []

    for line in data_in:
        ints = extract_ints(line)
        num_positions.append(ints[1])
        starting_pos.append(ints[3])

    return num_positions, starting_pos


def is_at_zero(num_positions, starting_pos, disc_index, time):
    """""" Is the specified disc at the zero position at the given time? """"""
    pos = (starting_pos[disc_index] + time) % num_positions[disc_index]
    return pos == 0


def find_time(num_positions, starting_pos):
    """"""
    Find the time at which to release a capsule so that it passes through all
    discs successfully
    """"""

    # What's the first time that we can release the disc where it reaches the
    # first disc as it's at position 0?
    candidate_time = num_positions[0] - starting_pos[0] - 1

    while True:
        # Are all discs at position 0 when the capsule reaches them?
        collision = False
        for disc_index in range(len(num_positions)):
            time_capsule_reaches_disc = candidate_time + disc_index + 1
            if not is_at_zero(num_positions, starting_pos, disc_index, time_capsule_reaches_disc):
                collision = True
                break

        # There was no collision with any disc! candidate_time is the correct answer!
        if not collision:
            return candidate_time

        # There was a collision so candidate_time isn't a valid result.
        # Increment it to the next time that disc 1 (index 0) is at the zero position.
        candidate_time += num_positions[0]


positions, starting = parse_data(data)

# Part 1
# At what time can we release the capsule to pass through all of the discs?
print(find_time(positions, starting))

# Part 2
# What if we add another disc at the bottom?
positions.append(11)
starting.append(0)
print(find_time(positions, starting))"
406	jackson	4	"""""""
This module deals with interpreting the parse tree as Python
would have done, in the compiler.

For now this only covers parse tree to value conversion of
compile-time values.
""""""

from __future__ import absolute_import

from .Nodes import *
from .ExprNodes import *
from .Errors import CompileError


class EmptyScope(object):
    def lookup(self, name):
        return None

empty_scope = EmptyScope()

def interpret_compiletime_options(optlist, optdict, type_env=None, type_args=()):
    """"""
    Tries to interpret a list of compile time option nodes.
    The result will be a tuple (optlist, optdict) but where
    all expression nodes have been interpreted. The result is
    in the form of tuples (value, pos).

    optlist is a list of nodes, while optdict is a DictNode (the
    result optdict is a dict)

    If type_env is set, all type nodes will be analysed and the resulting
    type set. Otherwise only interpretateable ExprNodes
    are allowed, other nodes raises errors.

    A CompileError will be raised if there are problems.
    """"""

    def interpret(node, ix):
        if ix in type_args:
            if type_env:
                type = node.analyse_as_type(type_env)
                if not type:
                    raise CompileError(node.pos, ""Invalid type."")
                return (type, node.pos)
            else:
                raise CompileError(node.pos, ""Type not allowed here."")
        else:
            if (sys.version_info[0] >=3 and
                isinstance(node, StringNode) and
                node.unicode_value is not None):
                return (node.unicode_value, node.pos)
            return (node.compile_time_value(empty_scope), node.pos)

    if optlist:
        optlist = [interpret(x, ix) for ix, x in enumerate(optlist)]
    if optdict:
        assert isinstance(optdict, DictNode)
        new_optdict = {}
        for item in optdict.key_value_pairs:
            new_key, dummy = interpret(item.key, None)
            new_optdict[new_key] = interpret(item.value, item.key.value)
        optdict = new_optdict
    return (optlist, new_optdict)"
406	donghui	2	"""""""
This module deals with interpreting the parse tree as Python
would have done, in the compiler.

For now this only covers parse tree to value conversion of
compile-time values.
""""""

from __future__ import absolute_import

from .Nodes import *
from .ExprNodes import *
from .Errors import CompileError


class EmptyScope(object):
    def lookup(self, name):
        return None

empty_scope = EmptyScope()

def interpret_compiletime_options(optlist, optdict, type_env=None, type_args=()):
    """"""
    Tries to interpret a list of compile time option nodes.
    The result will be a tuple (optlist, optdict) but where
    all expression nodes have been interpreted. The result is
    in the form of tuples (value, pos).

    optlist is a list of nodes, while optdict is a DictNode (the
    result optdict is a dict)

    If type_env is set, all type nodes will be analysed and the resulting
    type set. Otherwise only interpretateable ExprNodes
    are allowed, other nodes raises errors.

    A CompileError will be raised if there are problems.
    """"""

    def interpret(node, ix):
        if ix in type_args:
            if type_env:
                type = node.analyse_as_type(type_env)
                if not type:
                    raise CompileError(node.pos, ""Invalid type."")
                return (type, node.pos)
            else:
                raise CompileError(node.pos, ""Type not allowed here."")
        else:
            if (sys.version_info[0] >=3 and
                isinstance(node, StringNode) and
                node.unicode_value is not None):
                return (node.unicode_value, node.pos)
            return (node.compile_time_value(empty_scope), node.pos)

    if optlist:
        optlist = [interpret(x, ix) for ix, x in enumerate(optlist)]
    if optdict:
        assert isinstance(optdict, DictNode)
        new_optdict = {}
        for item in optdict.key_value_pairs:
            new_key, dummy = interpret(item.key, None)
            new_optdict[new_key] = interpret(item.value, item.key.value)
        optdict = new_optdict
    return (optlist, new_optdict)"
457	jackson	4	"""""""
String utility functions.
""""""

from typing import Any, Optional, Union


def safe_repr(obj: Any, clip: Optional[int] = None) -> str:
    """"""
    Convert object to string representation, yielding the same result a `repr`
    but catches all exceptions and returns 'N/A' instead of raising the
    exception. Strings may be truncated by providing `clip`.

    >>> safe_repr(42)
    '42'
    >>> safe_repr('Clipped text', clip=8)
    'Clip..xt'
    >>> safe_repr([1,2,3,4], clip=8)
    '[1,2..4]'
    """"""
    try:
        s = repr(obj)
        if not clip or len(s) <= clip:
            return s
        else:
            return s[:clip - 4] + '..' + s[-2:]
    except:
        return 'N/A'


def trunc(obj: str, max: int, left: bool = False) -> str:
    """"""
    Convert `obj` to string, eliminate newlines and truncate the string to
    `max` characters. If there are more characters in the string add ``...`` to
    the string. With `left=True`, the string can be truncated at the beginning.

    @note: Does not catch exceptions when converting `obj` to string with
        `str`.

    >>> trunc('This is a long text.', 8)
    This ...
    >>> trunc('This is a long text.', 8, left=True)
    ...text.
    """"""
    s = str(obj)
    s = s.replace('\n', '|')
    if len(s) > max:
        if left:
            return '...' + s[len(s) - max + 3:]
        else:
            return s[:(max - 3)] + '...'
    else:
        return s


def pp(i: Union[int, float], base: int = 1024) -> str:
    """"""
    Pretty-print the integer `i` as a human-readable size representation.
    """"""
    degree = 0
    pattern = ""%4d     %s""
    while i > base:
        pattern = ""%7.2f %s""
        i = i / float(base)
        degree += 1
    scales = ['B', 'KB', 'MB', 'GB', 'TB', 'EB']
    return pattern % (i, scales[degree])


def pp_timestamp(t: Optional[float]) -> str:
    """"""
    Get a friendly timestamp represented as a string.
    """"""
    if t is None:
        return ''
    h, m, s = int(t / 3600), int(t / 60 % 60), t % 60
    return ""%02d:%02d:%05.2f"" % (h, m, s)"
457	donghui	4	"""""""
String utility functions.
""""""

from typing import Any, Optional, Union


def safe_repr(obj: Any, clip: Optional[int] = None) -> str:
    """"""
    Convert object to string representation, yielding the same result a `repr`
    but catches all exceptions and returns 'N/A' instead of raising the
    exception. Strings may be truncated by providing `clip`.

    >>> safe_repr(42)
    '42'
    >>> safe_repr('Clipped text', clip=8)
    'Clip..xt'
    >>> safe_repr([1,2,3,4], clip=8)
    '[1,2..4]'
    """"""
    try:
        s = repr(obj)
        if not clip or len(s) <= clip:
            return s
        else:
            return s[:clip - 4] + '..' + s[-2:]
    except:
        return 'N/A'


def trunc(obj: str, max: int, left: bool = False) -> str:
    """"""
    Convert `obj` to string, eliminate newlines and truncate the string to
    `max` characters. If there are more characters in the string add ``...`` to
    the string. With `left=True`, the string can be truncated at the beginning.

    @note: Does not catch exceptions when converting `obj` to string with
        `str`.

    >>> trunc('This is a long text.', 8)
    This ...
    >>> trunc('This is a long text.', 8, left=True)
    ...text.
    """"""
    s = str(obj)
    s = s.replace('\n', '|')
    if len(s) > max:
        if left:
            return '...' + s[len(s) - max + 3:]
        else:
            return s[:(max - 3)] + '...'
    else:
        return s


def pp(i: Union[int, float], base: int = 1024) -> str:
    """"""
    Pretty-print the integer `i` as a human-readable size representation.
    """"""
    degree = 0
    pattern = ""%4d     %s""
    while i > base:
        pattern = ""%7.2f %s""
        i = i / float(base)
        degree += 1
    scales = ['B', 'KB', 'MB', 'GB', 'TB', 'EB']
    return pattern % (i, scales[degree])


def pp_timestamp(t: Optional[float]) -> str:
    """"""
    Get a friendly timestamp represented as a string.
    """"""
    if t is None:
        return ''
    h, m, s = int(t / 3600), int(t / 60 % 60), t % 60
    return ""%02d:%02d:%05.2f"" % (h, m, s)"
517	jackson	4	"""""""
Kakao OAuth2 backend, docs at:
    https://python-social-auth.readthedocs.io/en/latest/backends/kakao.html
""""""
from .oauth import BaseOAuth2


class KakaoOAuth2(BaseOAuth2):
    """"""Kakao OAuth authentication backend""""""
    name = 'kakao'
    AUTHORIZATION_URL = 'https://kauth.kakao.com/oauth/authorize'
    ACCESS_TOKEN_URL = 'https://kauth.kakao.com/oauth/token'
    ACCESS_TOKEN_METHOD = 'POST'
    REDIRECT_STATE = False
    EXTRA_DATA = [
        ('properties', 'properties'),
    ]

    def get_user_id(self, details, response):
        return response['id']

    def get_user_details(self, response):
        """"""Return user details from Kakao account""""""

        kakao_account = response.get('kakao_account', '')
        kaccount_email = kakao_account.get('email', '')
        properties = response.get('properties', '')
        nickname = properties.get('nickname') if properties else ''
        return {
            'username': nickname,
            'email': kaccount_email,
            'fullname': nickname,
            'first_name': nickname[1:] if nickname else '',
            'last_name': nickname[0] if nickname else '',
        }

    def user_data(self, access_token, *args, **kwargs):
        """"""Loads user data from service""""""
        return self.get_json(
            'https://kapi.kakao.com/v2/user/me',
            headers={
                'Authorization': f'Bearer {access_token}',
                'Content_Type': 'application/x-www-form-urlencoded;charset=utf-8',
            },
            params={'access_token': access_token}
        )

    def auth_complete_params(self, state=None):
        client_id, client_secret = self.get_key_and_secret()
        return {
            'grant_type': 'authorization_code',
            'code': self.data.get('code', ''),
            'client_id': client_id,
            'client_secret': client_secret,
        }"
517	donghui	4	"""""""
Kakao OAuth2 backend, docs at:
    https://python-social-auth.readthedocs.io/en/latest/backends/kakao.html
""""""
from .oauth import BaseOAuth2


class KakaoOAuth2(BaseOAuth2):
    """"""Kakao OAuth authentication backend""""""
    name = 'kakao'
    AUTHORIZATION_URL = 'https://kauth.kakao.com/oauth/authorize'
    ACCESS_TOKEN_URL = 'https://kauth.kakao.com/oauth/token'
    ACCESS_TOKEN_METHOD = 'POST'
    REDIRECT_STATE = False
    EXTRA_DATA = [
        ('properties', 'properties'),
    ]

    def get_user_id(self, details, response):
        return response['id']

    def get_user_details(self, response):
        """"""Return user details from Kakao account""""""

        kakao_account = response.get('kakao_account', '')
        kaccount_email = kakao_account.get('email', '')
        properties = response.get('properties', '')
        nickname = properties.get('nickname') if properties else ''
        return {
            'username': nickname,
            'email': kaccount_email,
            'fullname': nickname,
            'first_name': nickname[1:] if nickname else '',
            'last_name': nickname[0] if nickname else '',
        }

    def user_data(self, access_token, *args, **kwargs):
        """"""Loads user data from service""""""
        return self.get_json(
            'https://kapi.kakao.com/v2/user/me',
            headers={
                'Authorization': f'Bearer {access_token}',
                'Content_Type': 'application/x-www-form-urlencoded;charset=utf-8',
            },
            params={'access_token': access_token}
        )

    def auth_complete_params(self, state=None):
        client_id, client_secret = self.get_key_and_secret()
        return {
            'grant_type': 'authorization_code',
            'code': self.data.get('code', ''),
            'client_id': client_id,
            'client_secret': client_secret,
        }"
463	jackson	3	"from contextlib import contextmanager
import os
import tempfile

import pytest

from pandas.io.pytables import HDFStore

tables = pytest.importorskip(""tables"")
# set these parameters so we don't have file sharing
tables.parameters.MAX_NUMEXPR_THREADS = 1
tables.parameters.MAX_BLOSC_THREADS = 1
tables.parameters.MAX_THREADS = 1


def safe_remove(path):
    if path is not None:
        try:
            os.remove(path)  # noqa: PDF008
        except OSError:
            pass


def safe_close(store):
    try:
        if store is not None:
            store.close()
    except OSError:
        pass


def create_tempfile(path):
    """"""create an unopened named temporary file""""""
    return os.path.join(tempfile.gettempdir(), path)


# contextmanager to ensure the file cleanup
@contextmanager
def ensure_clean_store(path, mode=""a"", complevel=None, complib=None, fletcher32=False):

    try:

        # put in the temporary path if we don't have one already
        if not len(os.path.dirname(path)):
            path = create_tempfile(path)

        store = HDFStore(
            path, mode=mode, complevel=complevel, complib=complib, fletcher32=False
        )
        yield store
    finally:
        safe_close(store)
        if mode == ""w"" or mode == ""a"":
            safe_remove(path)


@contextmanager
def ensure_clean_path(path):
    """"""
    return essentially a named temporary file that is not opened
    and deleted on exiting; if path is a list, then create and
    return list of filenames
    """"""
    try:
        if isinstance(path, list):
            filenames = [create_tempfile(p) for p in path]
            yield filenames
        else:
            filenames = [create_tempfile(path)]
            yield filenames[0]
    finally:
        for f in filenames:
            safe_remove(f)


def _maybe_remove(store, key):
    """"""
    For tests using tables, try removing the table to be sure there is
    no content from previous tests using the same table name.
    """"""
    try:
        store.remove(key)
    except (ValueError, KeyError):
        pass"
463	donghui	2	"from contextlib import contextmanager
import os
import tempfile

import pytest

from pandas.io.pytables import HDFStore

tables = pytest.importorskip(""tables"")
# set these parameters so we don't have file sharing
tables.parameters.MAX_NUMEXPR_THREADS = 1
tables.parameters.MAX_BLOSC_THREADS = 1
tables.parameters.MAX_THREADS = 1


def safe_remove(path):
    if path is not None:
        try:
            os.remove(path)  # noqa: PDF008
        except OSError:
            pass


def safe_close(store):
    try:
        if store is not None:
            store.close()
    except OSError:
        pass


def create_tempfile(path):
    """"""create an unopened named temporary file""""""
    return os.path.join(tempfile.gettempdir(), path)


# contextmanager to ensure the file cleanup
@contextmanager
def ensure_clean_store(path, mode=""a"", complevel=None, complib=None, fletcher32=False):

    try:

        # put in the temporary path if we don't have one already
        if not len(os.path.dirname(path)):
            path = create_tempfile(path)

        store = HDFStore(
            path, mode=mode, complevel=complevel, complib=complib, fletcher32=False
        )
        yield store
    finally:
        safe_close(store)
        if mode == ""w"" or mode == ""a"":
            safe_remove(path)


@contextmanager
def ensure_clean_path(path):
    """"""
    return essentially a named temporary file that is not opened
    and deleted on exiting; if path is a list, then create and
    return list of filenames
    """"""
    try:
        if isinstance(path, list):
            filenames = [create_tempfile(p) for p in path]
            yield filenames
        else:
            filenames = [create_tempfile(path)]
            yield filenames[0]
    finally:
        for f in filenames:
            safe_remove(f)


def _maybe_remove(store, key):
    """"""
    For tests using tables, try removing the table to be sure there is
    no content from previous tests using the same table name.
    """"""
    try:
        store.remove(key)
    except (ValueError, KeyError):
        pass"
432	jackson	0	"from __future__ import absolute_import

from django import VERSION as django_version
from django import forms
from django.conf import settings
from django.utils.encoding import force_text
from django.utils.safestring import mark_safe
from django.utils.html import format_html

from .utils import get_icon_choices

CHOICES = get_icon_choices()

class IconWidget(forms.Select):

    def __init__(self, attrs=None):
        super(IconWidget, self).__init__(attrs, choices=CHOICES)

    if django_version >= (1, 11):
        def create_option(self, name, value, label, selected, index, subindex=None, attrs=None):
            option = super(IconWidget, self).create_option(name, value, label, selected, index, subindex=subindex, attrs=attrs)
            option[""attrs""][""data-icon""] = value
            return option
    else:
        def render_option(self, selected_choices, option_value, option_label):
            if option_value is None:
                option_value = ''
            option_value = force_text(option_value)
            if option_value in selected_choices:
                selected_html = mark_safe(' selected=""selected""')
                if not self.allow_multiple_selected:
                    # Only allow for a single selection.
                    selected_choices.remove(option_value)
            else:
                selected_html = ''
            return format_html('<option data-icon=""{0}"" value=""{0}""{1}>{2}</option>',
                option_value,
                selected_html,
                force_text(option_label),
            )

    class Media:

        js = (
            'fontawesome/js/django_fontawesome.js',
            'fontawesome/select2/select2.min.js'
        )

        css = {
            'all': (
                getattr(settings, 'FONTAWESOME_CSS_URL', 'fontawesome/css/font-awesome.min.css'),
                'fontawesome/select2/select2.css',
                'fontawesome/select2/select2-bootstrap.css'
            )
        }"
432	donghui	0	"from __future__ import absolute_import

from django import VERSION as django_version
from django import forms
from django.conf import settings
from django.utils.encoding import force_text
from django.utils.safestring import mark_safe
from django.utils.html import format_html

from .utils import get_icon_choices

CHOICES = get_icon_choices()

class IconWidget(forms.Select):

    def __init__(self, attrs=None):
        super(IconWidget, self).__init__(attrs, choices=CHOICES)

    if django_version >= (1, 11):
        def create_option(self, name, value, label, selected, index, subindex=None, attrs=None):
            option = super(IconWidget, self).create_option(name, value, label, selected, index, subindex=subindex, attrs=attrs)
            option[""attrs""][""data-icon""] = value
            return option
    else:
        def render_option(self, selected_choices, option_value, option_label):
            if option_value is None:
                option_value = ''
            option_value = force_text(option_value)
            if option_value in selected_choices:
                selected_html = mark_safe(' selected=""selected""')
                if not self.allow_multiple_selected:
                    # Only allow for a single selection.
                    selected_choices.remove(option_value)
            else:
                selected_html = ''
            return format_html('<option data-icon=""{0}"" value=""{0}""{1}>{2}</option>',
                option_value,
                selected_html,
                force_text(option_label),
            )

    class Media:

        js = (
            'fontawesome/js/django_fontawesome.js',
            'fontawesome/select2/select2.min.js'
        )

        css = {
            'all': (
                getattr(settings, 'FONTAWESOME_CSS_URL', 'fontawesome/css/font-awesome.min.css'),
                'fontawesome/select2/select2.css',
                'fontawesome/select2/select2-bootstrap.css'
            )
        }"
490	jackson	2	"import _plotly_utils.basevalidators


class FontValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(
        self, plotly_name=""font"", parent_name=""sankey.node.hoverlabel"", **kwargs
    ):
        super(FontValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Font""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            color

            colorsrc
                Sets the source reference on Chart Studio Cloud
                for `color`.
            family
                HTML font family - the typeface that will be
                applied by the web browser. The web browser
                will only be able to apply a font if it is
                available on the system which it operates.
                Provide multiple font families, separated by
                commas, to indicate the preference in which to
                apply fonts if they aren't available on the
                system. The Chart Studio Cloud (at
                https://chart-studio.plotly.com or on-premise)
                generates images on a server, where only a
                select number of fonts are installed and
                supported. These include ""Arial"", ""Balto"",
                ""Courier New"", ""Droid Sans"",, ""Droid Serif"",
                ""Droid Sans Mono"", ""Gravitas One"", ""Old
                Standard TT"", ""Open Sans"", ""Overpass"", ""PT Sans
                Narrow"", ""Raleway"", ""Times New Roman"".
            familysrc
                Sets the source reference on Chart Studio Cloud
                for `family`.
            size

            sizesrc
                Sets the source reference on Chart Studio Cloud
                for `size`.
"""""",
            ),
            **kwargs,
        )"
490	donghui	1	"import _plotly_utils.basevalidators


class FontValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(
        self, plotly_name=""font"", parent_name=""sankey.node.hoverlabel"", **kwargs
    ):
        super(FontValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Font""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            color

            colorsrc
                Sets the source reference on Chart Studio Cloud
                for `color`.
            family
                HTML font family - the typeface that will be
                applied by the web browser. The web browser
                will only be able to apply a font if it is
                available on the system which it operates.
                Provide multiple font families, separated by
                commas, to indicate the preference in which to
                apply fonts if they aren't available on the
                system. The Chart Studio Cloud (at
                https://chart-studio.plotly.com or on-premise)
                generates images on a server, where only a
                select number of fonts are installed and
                supported. These include ""Arial"", ""Balto"",
                ""Courier New"", ""Droid Sans"",, ""Droid Serif"",
                ""Droid Sans Mono"", ""Gravitas One"", ""Old
                Standard TT"", ""Open Sans"", ""Overpass"", ""PT Sans
                Narrow"", ""Raleway"", ""Times New Roman"".
            familysrc
                Sets the source reference on Chart Studio Cloud
                for `family`.
            size

            sizesrc
                Sets the source reference on Chart Studio Cloud
                for `size`.
"""""",
            ),
            **kwargs,
        )"
481	jackson	3	"""""""
This module includes some utility functions for inspecting the layout
of a GDAL data source -- the functionality is analogous to the output
produced by the `ogrinfo` utility.
""""""

from django.contrib.gis.gdal import DataSource
from django.contrib.gis.gdal.geometries import GEO_CLASSES


def ogrinfo(data_source, num_features=10):
    """"""
    Walk the available layers in the supplied `data_source`, displaying
    the fields for the first `num_features` features.
    """"""

    # Checking the parameters.
    if isinstance(data_source, str):
        data_source = DataSource(data_source)
    elif isinstance(data_source, DataSource):
        pass
    else:
        raise Exception(
            ""Data source parameter must be a string or a DataSource object.""
        )

    for i, layer in enumerate(data_source):
        print(""data source : %s"" % data_source.name)
        print(""==== layer %s"" % i)
        print(""  shape type: %s"" % GEO_CLASSES[layer.geom_type.num].__name__)
        print(""  # features: %s"" % len(layer))
        print(""         srs: %s"" % layer.srs)
        extent_tup = layer.extent.tuple
        print(""      extent: %s - %s"" % (extent_tup[0:2], extent_tup[2:4]))
        print(""Displaying the first %s features ===="" % num_features)

        width = max(*map(len, layer.fields))
        fmt = "" %%%ss: %%s"" % width
        for j, feature in enumerate(layer[:num_features]):
            print(""=== Feature %s"" % j)
            for fld_name in layer.fields:
                type_name = feature[fld_name].type_name
                output = fmt % (fld_name, type_name)
                val = feature.get(fld_name)
                if val:
                    if isinstance(val, str):
                        val_fmt = ' (""%s"")'
                    else:
                        val_fmt = "" (%s)""
                    output += val_fmt % val
                else:
                    output += "" (None)""
                print(output)"
481	donghui	2	"""""""
This module includes some utility functions for inspecting the layout
of a GDAL data source -- the functionality is analogous to the output
produced by the `ogrinfo` utility.
""""""

from django.contrib.gis.gdal import DataSource
from django.contrib.gis.gdal.geometries import GEO_CLASSES


def ogrinfo(data_source, num_features=10):
    """"""
    Walk the available layers in the supplied `data_source`, displaying
    the fields for the first `num_features` features.
    """"""

    # Checking the parameters.
    if isinstance(data_source, str):
        data_source = DataSource(data_source)
    elif isinstance(data_source, DataSource):
        pass
    else:
        raise Exception(
            ""Data source parameter must be a string or a DataSource object.""
        )

    for i, layer in enumerate(data_source):
        print(""data source : %s"" % data_source.name)
        print(""==== layer %s"" % i)
        print(""  shape type: %s"" % GEO_CLASSES[layer.geom_type.num].__name__)
        print(""  # features: %s"" % len(layer))
        print(""         srs: %s"" % layer.srs)
        extent_tup = layer.extent.tuple
        print(""      extent: %s - %s"" % (extent_tup[0:2], extent_tup[2:4]))
        print(""Displaying the first %s features ===="" % num_features)

        width = max(*map(len, layer.fields))
        fmt = "" %%%ss: %%s"" % width
        for j, feature in enumerate(layer[:num_features]):
            print(""=== Feature %s"" % j)
            for fld_name in layer.fields:
                type_name = feature[fld_name].type_name
                output = fmt % (fld_name, type_name)
                val = feature.get(fld_name)
                if val:
                    if isinstance(val, str):
                        val_fmt = ' (""%s"")'
                    else:
                        val_fmt = "" (%s)""
                    output += val_fmt % val
                else:
                    output += "" (None)""
                print(output)"
423	jackson	0	"import os
import asyncio
import pytest

import txaio

# because py.test tries to collect it as a test-case
from unittest.mock import Mock

from autobahn.asyncio.websocket import WebSocketServerFactory


async def echo_async(what, when):
    await asyncio.sleep(when)
    return what


@pytest.mark.skipif(not os.environ.get('USE_ASYNCIO', False), reason='test runs on asyncio only')
@pytest.mark.asyncio
async def test_echo_async():
    assert 'Hello!' == await echo_async('Hello!', 0)


# @pytest.mark.asyncio(forbid_global_loop=True)
@pytest.mark.skipif(not os.environ.get('USE_ASYNCIO', False), reason='test runs on asyncio only')
def test_websocket_custom_loop(event_loop):
    factory = WebSocketServerFactory(loop=event_loop)
    server = factory()
    transport = Mock()
    server.connection_made(transport)


@pytest.mark.skipif(not os.environ.get('USE_ASYNCIO', False), reason='test runs on asyncio only')
@pytest.mark.asyncio
async def test_async_on_connect_server(event_loop):

    num = 42
    done = txaio.create_future()
    values = []

    async def foo(x):
        await asyncio.sleep(1)
        return x * x

    async def on_connect(req):
        v = await foo(num)
        values.append(v)
        txaio.resolve(done, req)

    factory = WebSocketServerFactory()
    server = factory()
    server.onConnect = on_connect
    transport = Mock()

    server.connection_made(transport)
    server.data = b'\r\n'.join([
        b'GET /ws HTTP/1.1',
        b'Host: www.example.com',
        b'Sec-WebSocket-Version: 13',
        b'Origin: http://www.example.com.malicious.com',
        b'Sec-WebSocket-Extensions: permessage-deflate',
        b'Sec-WebSocket-Key: tXAxWFUqnhi86Ajj7dRY5g==',
        b'Connection: keep-alive, Upgrade',
        b'Upgrade: websocket',
        b'\r\n',  # last string doesn't get a \r\n from join()
    ])
    server.processHandshake()
    await done

    assert len(values) == 1
    assert values[0] == num * num"
423	donghui	1	"import os
import asyncio
import pytest

import txaio

# because py.test tries to collect it as a test-case
from unittest.mock import Mock

from autobahn.asyncio.websocket import WebSocketServerFactory


async def echo_async(what, when):
    await asyncio.sleep(when)
    return what


@pytest.mark.skipif(not os.environ.get('USE_ASYNCIO', False), reason='test runs on asyncio only')
@pytest.mark.asyncio
async def test_echo_async():
    assert 'Hello!' == await echo_async('Hello!', 0)


# @pytest.mark.asyncio(forbid_global_loop=True)
@pytest.mark.skipif(not os.environ.get('USE_ASYNCIO', False), reason='test runs on asyncio only')
def test_websocket_custom_loop(event_loop):
    factory = WebSocketServerFactory(loop=event_loop)
    server = factory()
    transport = Mock()
    server.connection_made(transport)


@pytest.mark.skipif(not os.environ.get('USE_ASYNCIO', False), reason='test runs on asyncio only')
@pytest.mark.asyncio
async def test_async_on_connect_server(event_loop):

    num = 42
    done = txaio.create_future()
    values = []

    async def foo(x):
        await asyncio.sleep(1)
        return x * x

    async def on_connect(req):
        v = await foo(num)
        values.append(v)
        txaio.resolve(done, req)

    factory = WebSocketServerFactory()
    server = factory()
    server.onConnect = on_connect
    transport = Mock()

    server.connection_made(transport)
    server.data = b'\r\n'.join([
        b'GET /ws HTTP/1.1',
        b'Host: www.example.com',
        b'Sec-WebSocket-Version: 13',
        b'Origin: http://www.example.com.malicious.com',
        b'Sec-WebSocket-Extensions: permessage-deflate',
        b'Sec-WebSocket-Key: tXAxWFUqnhi86Ajj7dRY5g==',
        b'Connection: keep-alive, Upgrade',
        b'Upgrade: websocket',
        b'\r\n',  # last string doesn't get a \r\n from join()
    ])
    server.processHandshake()
    await done

    assert len(values) == 1
    assert values[0] == num * num"
472	jackson	2	"""""""
 The GeometryColumns and SpatialRefSys models for the SpatiaLite backend.
""""""
from django.contrib.gis.db.backends.base.models import SpatialRefSysMixin
from django.db import models


class SpatialiteGeometryColumns(models.Model):
    """"""
    The 'geometry_columns' table from SpatiaLite.
    """"""

    f_table_name = models.CharField(max_length=256)
    f_geometry_column = models.CharField(max_length=256)
    coord_dimension = models.IntegerField()
    srid = models.IntegerField(primary_key=True)
    spatial_index_enabled = models.IntegerField()
    type = models.IntegerField(db_column=""geometry_type"")

    class Meta:
        app_label = ""gis""
        db_table = ""geometry_columns""
        managed = False

    def __str__(self):
        return ""%s.%s - %dD %s field (SRID: %d)"" % (
            self.f_table_name,
            self.f_geometry_column,
            self.coord_dimension,
            self.type,
            self.srid,
        )

    @classmethod
    def table_name_col(cls):
        """"""
        Return the name of the metadata column used to store the feature table
        name.
        """"""
        return ""f_table_name""

    @classmethod
    def geom_col_name(cls):
        """"""
        Return the name of the metadata column used to store the feature
        geometry column.
        """"""
        return ""f_geometry_column""


class SpatialiteSpatialRefSys(models.Model, SpatialRefSysMixin):
    """"""
    The 'spatial_ref_sys' table from SpatiaLite.
    """"""

    srid = models.IntegerField(primary_key=True)
    auth_name = models.CharField(max_length=256)
    auth_srid = models.IntegerField()
    ref_sys_name = models.CharField(max_length=256)
    proj4text = models.CharField(max_length=2048)
    srtext = models.CharField(max_length=2048)

    class Meta:
        app_label = ""gis""
        db_table = ""spatial_ref_sys""
        managed = False

    @property
    def wkt(self):
        return self.srtext"
472	donghui	2	"""""""
 The GeometryColumns and SpatialRefSys models for the SpatiaLite backend.
""""""
from django.contrib.gis.db.backends.base.models import SpatialRefSysMixin
from django.db import models


class SpatialiteGeometryColumns(models.Model):
    """"""
    The 'geometry_columns' table from SpatiaLite.
    """"""

    f_table_name = models.CharField(max_length=256)
    f_geometry_column = models.CharField(max_length=256)
    coord_dimension = models.IntegerField()
    srid = models.IntegerField(primary_key=True)
    spatial_index_enabled = models.IntegerField()
    type = models.IntegerField(db_column=""geometry_type"")

    class Meta:
        app_label = ""gis""
        db_table = ""geometry_columns""
        managed = False

    def __str__(self):
        return ""%s.%s - %dD %s field (SRID: %d)"" % (
            self.f_table_name,
            self.f_geometry_column,
            self.coord_dimension,
            self.type,
            self.srid,
        )

    @classmethod
    def table_name_col(cls):
        """"""
        Return the name of the metadata column used to store the feature table
        name.
        """"""
        return ""f_table_name""

    @classmethod
    def geom_col_name(cls):
        """"""
        Return the name of the metadata column used to store the feature
        geometry column.
        """"""
        return ""f_geometry_column""


class SpatialiteSpatialRefSys(models.Model, SpatialRefSysMixin):
    """"""
    The 'spatial_ref_sys' table from SpatiaLite.
    """"""

    srid = models.IntegerField(primary_key=True)
    auth_name = models.CharField(max_length=256)
    auth_srid = models.IntegerField()
    ref_sys_name = models.CharField(max_length=256)
    proj4text = models.CharField(max_length=2048)
    srtext = models.CharField(max_length=2048)

    class Meta:
        app_label = ""gis""
        db_table = ""spatial_ref_sys""
        managed = False

    @property
    def wkt(self):
        return self.srtext"
506	jackson	3	"""""""
    pygments.styles.trac
    ~~~~~~~~~~~~~~~~~~~~

    Port of the default trac highlighter design.

    :copyright: Copyright 2006-2022 by the Pygments team, see AUTHORS.
    :license: BSD, see LICENSE for details.
""""""

from pygments.style import Style
from pygments.token import Keyword, Name, Comment, String, Error, \
     Number, Operator, Generic, Whitespace


class TracStyle(Style):
    """"""
    Port of the default trac highlighter design.
    """"""

    styles = {
        Whitespace:             '#bbbbbb',
        Comment:                'italic #999988',
        Comment.Preproc:        'bold noitalic #999999',
        Comment.Special:        'bold #999999',

        Operator:               'bold',

        String:                 '#bb8844',
        String.Regex:           '#808000',

        Number:                 '#009999',

        Keyword:                'bold',
        Keyword.Type:           '#445588',

        Name.Builtin:           '#999999',
        Name.Function:          'bold #990000',
        Name.Class:             'bold #445588',
        Name.Exception:         'bold #990000',
        Name.Namespace:         '#555555',
        Name.Variable:          '#008080',
        Name.Constant:          '#008080',
        Name.Tag:               '#000080',
        Name.Attribute:         '#008080',
        Name.Entity:            '#800080',

        Generic.Heading:        '#999999',
        Generic.Subheading:     '#aaaaaa',
        Generic.Deleted:        'bg:#ffdddd #000000',
        Generic.Inserted:       'bg:#ddffdd #000000',
        Generic.Error:          '#aa0000',
        Generic.Emph:           'italic',
        Generic.Strong:         'bold',
        Generic.Prompt:         '#555555',
        Generic.Output:         '#888888',
        Generic.Traceback:      '#aa0000',

        Error:                  'bg:#e3d2d2 #a61717'
    }"
506	donghui	1	"""""""
    pygments.styles.trac
    ~~~~~~~~~~~~~~~~~~~~

    Port of the default trac highlighter design.

    :copyright: Copyright 2006-2022 by the Pygments team, see AUTHORS.
    :license: BSD, see LICENSE for details.
""""""

from pygments.style import Style
from pygments.token import Keyword, Name, Comment, String, Error, \
     Number, Operator, Generic, Whitespace


class TracStyle(Style):
    """"""
    Port of the default trac highlighter design.
    """"""

    styles = {
        Whitespace:             '#bbbbbb',
        Comment:                'italic #999988',
        Comment.Preproc:        'bold noitalic #999999',
        Comment.Special:        'bold #999999',

        Operator:               'bold',

        String:                 '#bb8844',
        String.Regex:           '#808000',

        Number:                 '#009999',

        Keyword:                'bold',
        Keyword.Type:           '#445588',

        Name.Builtin:           '#999999',
        Name.Function:          'bold #990000',
        Name.Class:             'bold #445588',
        Name.Exception:         'bold #990000',
        Name.Namespace:         '#555555',
        Name.Variable:          '#008080',
        Name.Constant:          '#008080',
        Name.Tag:               '#000080',
        Name.Attribute:         '#008080',
        Name.Entity:            '#800080',

        Generic.Heading:        '#999999',
        Generic.Subheading:     '#aaaaaa',
        Generic.Deleted:        'bg:#ffdddd #000000',
        Generic.Inserted:       'bg:#ddffdd #000000',
        Generic.Error:          '#aa0000',
        Generic.Emph:           'italic',
        Generic.Strong:         'bold',
        Generic.Prompt:         '#555555',
        Generic.Output:         '#888888',
        Generic.Traceback:      '#aa0000',

        Error:                  'bg:#e3d2d2 #a61717'
    }"
446	jackson	1	"#!/usr/bin/env python3

import re

from homeassistant.components.binary_sensor import BinarySensorDeviceClass
from homeassistant.components.button import ButtonDeviceClass
from homeassistant.components.cover import CoverDeviceClass
from homeassistant.components.number import NumberDeviceClass
from homeassistant.components.sensor import SensorDeviceClass
from homeassistant.components.switch import SwitchDeviceClass

BLOCKLIST = (
    # requires special support on HA side
    ""enum"",
)

DOMAINS = {
    ""binary_sensor"": BinarySensorDeviceClass,
    ""button"": ButtonDeviceClass,
    ""cover"": CoverDeviceClass,
    ""number"": NumberDeviceClass,
    ""sensor"": SensorDeviceClass,
    ""switch"": SwitchDeviceClass,
}


def sub(path, pattern, repl):
    with open(path, ""r"") as handle:
        content = handle.read()
    content = re.sub(pattern, repl, content, flags=re.MULTILINE, count=1)
    with open(path, ""w"") as handle:
        handle.write(content)


def main():
    classes = {""EMPTY"": """"}
    allowed = {}

    for domain, enum in DOMAINS.items():
        available = {
            cls.value.upper(): cls.value for cls in enum if cls.value not in BLOCKLIST
        }

        classes.update(available)
        allowed[domain] = list(available.keys()) + [""EMPTY""]

    # replace constant defines in const.py
    out = """"
    for cls in sorted(classes):
        out += f'DEVICE_CLASS_{cls.upper()} = ""{classes[cls]}""\n'
    sub(""esphome/const.py"", '(DEVICE_CLASS_\w+ = ""\w*""\r?\n)+', out)

    for domain in sorted(allowed):
        # replace imports
        out = """"
        for item in sorted(allowed[domain]):
            out += f""    DEVICE_CLASS_{item.upper()},\n""

        sub(
            f""esphome/components/{domain}/__init__.py"",
            ""(    DEVICE_CLASS_\w+,\r?\n)+"",
            out,
        )


if __name__ == ""__main__"":
    main()"
446	donghui	0	"#!/usr/bin/env python3

import re

from homeassistant.components.binary_sensor import BinarySensorDeviceClass
from homeassistant.components.button import ButtonDeviceClass
from homeassistant.components.cover import CoverDeviceClass
from homeassistant.components.number import NumberDeviceClass
from homeassistant.components.sensor import SensorDeviceClass
from homeassistant.components.switch import SwitchDeviceClass

BLOCKLIST = (
    # requires special support on HA side
    ""enum"",
)

DOMAINS = {
    ""binary_sensor"": BinarySensorDeviceClass,
    ""button"": ButtonDeviceClass,
    ""cover"": CoverDeviceClass,
    ""number"": NumberDeviceClass,
    ""sensor"": SensorDeviceClass,
    ""switch"": SwitchDeviceClass,
}


def sub(path, pattern, repl):
    with open(path, ""r"") as handle:
        content = handle.read()
    content = re.sub(pattern, repl, content, flags=re.MULTILINE, count=1)
    with open(path, ""w"") as handle:
        handle.write(content)


def main():
    classes = {""EMPTY"": """"}
    allowed = {}

    for domain, enum in DOMAINS.items():
        available = {
            cls.value.upper(): cls.value for cls in enum if cls.value not in BLOCKLIST
        }

        classes.update(available)
        allowed[domain] = list(available.keys()) + [""EMPTY""]

    # replace constant defines in const.py
    out = """"
    for cls in sorted(classes):
        out += f'DEVICE_CLASS_{cls.upper()} = ""{classes[cls]}""\n'
    sub(""esphome/const.py"", '(DEVICE_CLASS_\w+ = ""\w*""\r?\n)+', out)

    for domain in sorted(allowed):
        # replace imports
        out = """"
        for item in sorted(allowed[domain]):
            out += f""    DEVICE_CLASS_{item.upper()},\n""

        sub(
            f""esphome/components/{domain}/__init__.py"",
            ""(    DEVICE_CLASS_\w+,\r?\n)+"",
            out,
        )


if __name__ == ""__main__"":
    main()"
417	jackson	0	"import pytest
from spacy.lang.en import English
from spacy.training import Example
from thinc.api import Config

default_tok2vec_config = """"""
[model]
@architectures = ""spacy-legacy.HashEmbedCNN.v1""
pretrained_vectors = null
width = 96
depth = 4
embed_size = 2000
window_size = 1
maxout_pieces = 3
subword_features = true
""""""
DEFAULT_TOK2VEC_MODEL = Config().from_str(default_tok2vec_config)[""model""]

TRAIN_DATA = [
    (
        ""They trade mortgage-backed securities."",
        {
            ""heads"": [1, 1, 4, 4, 5, 1, 1],
            ""deps"": [""nsubj"", ""ROOT"", ""compound"", ""punct"", ""nmod"", ""dobj"", ""punct""],
        },
    ),
    (
        ""I like London and Berlin."",
        {
            ""heads"": [1, 1, 1, 2, 2, 1],
            ""deps"": [""nsubj"", ""ROOT"", ""dobj"", ""cc"", ""conj"", ""punct""],
        },
    ),
]


@pytest.mark.parametrize(
    ""parser_config"",
    [
        {
            ""@architectures"": ""spacy-legacy.TransitionBasedParser.v1"",
            ""state_type"": ""parser"",
            ""extra_state_tokens"": False,
            ""hidden_width"": 66,
            ""maxout_pieces"": 2,
            ""use_upper"": True,
            ""tok2vec"": DEFAULT_TOK2VEC_MODEL,
        }
    ],
)
def test_parser(parser_config):
    pipe_config = {""model"": parser_config}
    nlp = English()
    parser = nlp.add_pipe(""parser"", config=pipe_config)
    train_examples = []
    for text, annotations in TRAIN_DATA:
        train_examples.append(Example.from_dict(nlp.make_doc(text), annotations))
        for dep in annotations.get(""deps"", []):
            if dep is not None:
                parser.add_label(dep)
    optimizer = nlp.initialize(get_examples=lambda: train_examples)
    for i in range(150):
        losses = {}
        nlp.update(train_examples, sgd=optimizer, losses=losses)
    assert losses[""parser""] < 0.0001"
417	donghui	0	"import pytest
from spacy.lang.en import English
from spacy.training import Example
from thinc.api import Config

default_tok2vec_config = """"""
[model]
@architectures = ""spacy-legacy.HashEmbedCNN.v1""
pretrained_vectors = null
width = 96
depth = 4
embed_size = 2000
window_size = 1
maxout_pieces = 3
subword_features = true
""""""
DEFAULT_TOK2VEC_MODEL = Config().from_str(default_tok2vec_config)[""model""]

TRAIN_DATA = [
    (
        ""They trade mortgage-backed securities."",
        {
            ""heads"": [1, 1, 4, 4, 5, 1, 1],
            ""deps"": [""nsubj"", ""ROOT"", ""compound"", ""punct"", ""nmod"", ""dobj"", ""punct""],
        },
    ),
    (
        ""I like London and Berlin."",
        {
            ""heads"": [1, 1, 1, 2, 2, 1],
            ""deps"": [""nsubj"", ""ROOT"", ""dobj"", ""cc"", ""conj"", ""punct""],
        },
    ),
]


@pytest.mark.parametrize(
    ""parser_config"",
    [
        {
            ""@architectures"": ""spacy-legacy.TransitionBasedParser.v1"",
            ""state_type"": ""parser"",
            ""extra_state_tokens"": False,
            ""hidden_width"": 66,
            ""maxout_pieces"": 2,
            ""use_upper"": True,
            ""tok2vec"": DEFAULT_TOK2VEC_MODEL,
        }
    ],
)
def test_parser(parser_config):
    pipe_config = {""model"": parser_config}
    nlp = English()
    parser = nlp.add_pipe(""parser"", config=pipe_config)
    train_examples = []
    for text, annotations in TRAIN_DATA:
        train_examples.append(Example.from_dict(nlp.make_doc(text), annotations))
        for dep in annotations.get(""deps"", []):
            if dep is not None:
                parser.add_label(dep)
    optimizer = nlp.initialize(get_examples=lambda: train_examples)
    for i in range(150):
        losses = {}
        nlp.update(train_examples, sgd=optimizer, losses=losses)
    assert losses[""parser""] < 0.0001"
359	jackson	1	"######################## BEGIN LICENSE BLOCK ########################
# The Original Code is mozilla.org code.
#
# The Initial Developer of the Original Code is
# Netscape Communications Corporation.
# Portions created by the Initial Developer are Copyright (C) 1998
# the Initial Developer. All Rights Reserved.
#
# Contributor(s):
#   Mark Pilgrim - port to Python
#
# This library is free software; you can redistribute it and/or
# modify it under the terms of the GNU Lesser General Public
# License as published by the Free Software Foundation; either
# version 2.1 of the License, or (at your option) any later version.
#
# This library is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public
# License along with this library; if not, write to the Free Software
# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA
# 02110-1301  USA
######################### END LICENSE BLOCK #########################

from .chardistribution import EUCKRDistributionAnalysis
from .codingstatemachine import CodingStateMachine
from .mbcharsetprober import MultiByteCharSetProber
from .mbcssm import CP949_SM_MODEL


class CP949Prober(MultiByteCharSetProber):
    def __init__(self):
        super(CP949Prober, self).__init__()
        self.coding_sm = CodingStateMachine(CP949_SM_MODEL)
        # NOTE: CP949 is a superset of EUC-KR, so the distribution should be
        #       not different.
        self.distribution_analyzer = EUCKRDistributionAnalysis()
        self.reset()

    @property
    def charset_name(self):
        return ""CP949""

    @property
    def language(self):
        return ""Korean"""
359	donghui	1	"######################## BEGIN LICENSE BLOCK ########################
# The Original Code is mozilla.org code.
#
# The Initial Developer of the Original Code is
# Netscape Communications Corporation.
# Portions created by the Initial Developer are Copyright (C) 1998
# the Initial Developer. All Rights Reserved.
#
# Contributor(s):
#   Mark Pilgrim - port to Python
#
# This library is free software; you can redistribute it and/or
# modify it under the terms of the GNU Lesser General Public
# License as published by the Free Software Foundation; either
# version 2.1 of the License, or (at your option) any later version.
#
# This library is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public
# License along with this library; if not, write to the Free Software
# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA
# 02110-1301  USA
######################### END LICENSE BLOCK #########################

from .chardistribution import EUCKRDistributionAnalysis
from .codingstatemachine import CodingStateMachine
from .mbcharsetprober import MultiByteCharSetProber
from .mbcssm import CP949_SM_MODEL


class CP949Prober(MultiByteCharSetProber):
    def __init__(self):
        super(CP949Prober, self).__init__()
        self.coding_sm = CodingStateMachine(CP949_SM_MODEL)
        # NOTE: CP949 is a superset of EUC-KR, so the distribution should be
        #       not different.
        self.distribution_analyzer = EUCKRDistributionAnalysis()
        self.reset()

    @property
    def charset_name(self):
        return ""CP949""

    @property
    def language(self):
        return ""Korean"""
308	jackson	2	"""""""
    pygments.lexers.jmespath
    ~~~~~~~~~~~~~~~~~~~~~~~~

    Lexers for the JMESPath language

    :copyright: Copyright 2006-2022 by the Pygments team, see AUTHORS.
    :license: BSD, see LICENSE for details.
""""""

from pygments.lexer import RegexLexer, bygroups, include
from pygments.token import String, Punctuation, Whitespace, Name, Operator, \
    Number, Literal, Keyword

__all__ = ['JMESPathLexer']


class JMESPathLexer(RegexLexer):
    """"""
    For JMESPath queries.
    """"""
    name = 'JMESPath'
    url = 'https://jmespath.org'
    filenames = ['*.jp']
    aliases = ['jmespath', 'jp']

    tokens = {
        'string': [
            (r""'(\\(.|\n)|[^'\\])*'"", String),
        ],
        'punctuation': [
            (r'(\[\?|[\.\*\[\],:\(\)\{\}\|])', Punctuation),
        ],
        'ws': [
            (r"" |\t|\n|\r"", Whitespace)
        ],
        ""dq-identifier"": [
            (r'[^\\""]+', Name.Variable),
            (r'\\""', Name.Variable),
            (r'.', Punctuation, '#pop'),
        ],
        'identifier': [
            (r'(&)?("")', bygroups(Name.Variable, Punctuation), 'dq-identifier'),
            (r'("")?(&?[A-Za-z][A-Za-z0-9_-]*)("")?', bygroups(Punctuation, Name.Variable, Punctuation)),
        ],
        'root': [
            include('ws'),
            include('string'),
            (r'(==|!=|<=|>=|<|>|&&|\|\||!)', Operator),
            include('punctuation'),
            (r'@', Name.Variable.Global),
            (r'(&?[A-Za-z][A-Za-z0-9_]*)(\()', bygroups(Name.Function, Punctuation)),
            (r'(&)(\()', bygroups(Name.Variable, Punctuation)),
            include('identifier'),
            (r'-?\d+', Number),
            (r'`', Literal, 'literal'),
        ],
        'literal': [
            include('ws'),
            include('string'),
            include('punctuation'),
            (r'(false|true|null)\b', Keyword.Constant),
            include('identifier'),
            (r'-?\d+\.?\d*([eE][-+]\d+)?', Number),
            (r'\\`', Literal),
            (r'`', Literal, '#pop'),
        ]
    }"
308	donghui	1	"""""""
    pygments.lexers.jmespath
    ~~~~~~~~~~~~~~~~~~~~~~~~

    Lexers for the JMESPath language

    :copyright: Copyright 2006-2022 by the Pygments team, see AUTHORS.
    :license: BSD, see LICENSE for details.
""""""

from pygments.lexer import RegexLexer, bygroups, include
from pygments.token import String, Punctuation, Whitespace, Name, Operator, \
    Number, Literal, Keyword

__all__ = ['JMESPathLexer']


class JMESPathLexer(RegexLexer):
    """"""
    For JMESPath queries.
    """"""
    name = 'JMESPath'
    url = 'https://jmespath.org'
    filenames = ['*.jp']
    aliases = ['jmespath', 'jp']

    tokens = {
        'string': [
            (r""'(\\(.|\n)|[^'\\])*'"", String),
        ],
        'punctuation': [
            (r'(\[\?|[\.\*\[\],:\(\)\{\}\|])', Punctuation),
        ],
        'ws': [
            (r"" |\t|\n|\r"", Whitespace)
        ],
        ""dq-identifier"": [
            (r'[^\\""]+', Name.Variable),
            (r'\\""', Name.Variable),
            (r'.', Punctuation, '#pop'),
        ],
        'identifier': [
            (r'(&)?("")', bygroups(Name.Variable, Punctuation), 'dq-identifier'),
            (r'("")?(&?[A-Za-z][A-Za-z0-9_-]*)("")?', bygroups(Punctuation, Name.Variable, Punctuation)),
        ],
        'root': [
            include('ws'),
            include('string'),
            (r'(==|!=|<=|>=|<|>|&&|\|\||!)', Operator),
            include('punctuation'),
            (r'@', Name.Variable.Global),
            (r'(&?[A-Za-z][A-Za-z0-9_]*)(\()', bygroups(Name.Function, Punctuation)),
            (r'(&)(\()', bygroups(Name.Variable, Punctuation)),
            include('identifier'),
            (r'-?\d+', Number),
            (r'`', Literal, 'literal'),
        ],
        'literal': [
            include('ws'),
            include('string'),
            include('punctuation'),
            (r'(false|true|null)\b', Keyword.Constant),
            include('identifier'),
            (r'-?\d+\.?\d*([eE][-+]\d+)?', Number),
            (r'\\`', Literal),
            (r'`', Literal, '#pop'),
        ]
    }"
318	jackson	3	"""""""
    pygments.lexers.graphviz
    ~~~~~~~~~~~~~~~~~~~~~~~~

    Lexer for the DOT language (graphviz).

    :copyright: Copyright 2006-2022 by the Pygments team, see AUTHORS.
    :license: BSD, see LICENSE for details.
""""""

from pygments.lexer import RegexLexer, bygroups
from pygments.token import Comment, Keyword, Operator, Name, String, Number, \
    Punctuation, Whitespace


__all__ = ['GraphvizLexer']


class GraphvizLexer(RegexLexer):
    """"""
    For graphviz DOT graph description language.

    .. versionadded:: 2.8
    """"""
    name = 'Graphviz'
    url = 'https://www.graphviz.org/doc/info/lang.html'
    aliases = ['graphviz', 'dot']
    filenames = ['*.gv', '*.dot']
    mimetypes = ['text/x-graphviz', 'text/vnd.graphviz']
    tokens = {
        'root': [
            (r'\s+', Whitespace),
            (r'(#|//).*?$', Comment.Single),
            (r'/(\\\n)?[*](.|\n)*?[*](\\\n)?/', Comment.Multiline),
            (r'(?i)(node|edge|graph|digraph|subgraph|strict)\b', Keyword),
            (r'--|->', Operator),
            (r'[{}[\]:;,]', Punctuation),
            (r'(\b\D\w*)(\s*)(=)(\s*)',
                bygroups(Name.Attribute, Whitespace, Punctuation, Whitespace),
                'attr_id'),
            (r'\b(n|ne|e|se|s|sw|w|nw|c|_)\b', Name.Builtin),
            (r'\b\D\w*', Name.Tag),  # node
            (r'[-]?((\.[0-9]+)|([0-9]+(\.[0-9]*)?))', Number),
            (r'""(\\""|[^""])*?""', Name.Tag),  # quoted node
            (r'<', Punctuation, 'xml'),
        ],
        'attr_id': [
            (r'\b\D\w*', String, '#pop'),
            (r'[-]?((\.[0-9]+)|([0-9]+(\.[0-9]*)?))', Number, '#pop'),
            (r'""(\\""|[^""])*?""', String.Double, '#pop'),
            (r'<', Punctuation, ('#pop', 'xml')),
        ],
        'xml': [
            (r'<', Punctuation, '#push'),
            (r'>', Punctuation, '#pop'),
            (r'\s+', Whitespace),
            (r'[^<>\s]', Name.Tag),
        ]
    }"
318	donghui	1	"""""""
    pygments.lexers.graphviz
    ~~~~~~~~~~~~~~~~~~~~~~~~

    Lexer for the DOT language (graphviz).

    :copyright: Copyright 2006-2022 by the Pygments team, see AUTHORS.
    :license: BSD, see LICENSE for details.
""""""

from pygments.lexer import RegexLexer, bygroups
from pygments.token import Comment, Keyword, Operator, Name, String, Number, \
    Punctuation, Whitespace


__all__ = ['GraphvizLexer']


class GraphvizLexer(RegexLexer):
    """"""
    For graphviz DOT graph description language.

    .. versionadded:: 2.8
    """"""
    name = 'Graphviz'
    url = 'https://www.graphviz.org/doc/info/lang.html'
    aliases = ['graphviz', 'dot']
    filenames = ['*.gv', '*.dot']
    mimetypes = ['text/x-graphviz', 'text/vnd.graphviz']
    tokens = {
        'root': [
            (r'\s+', Whitespace),
            (r'(#|//).*?$', Comment.Single),
            (r'/(\\\n)?[*](.|\n)*?[*](\\\n)?/', Comment.Multiline),
            (r'(?i)(node|edge|graph|digraph|subgraph|strict)\b', Keyword),
            (r'--|->', Operator),
            (r'[{}[\]:;,]', Punctuation),
            (r'(\b\D\w*)(\s*)(=)(\s*)',
                bygroups(Name.Attribute, Whitespace, Punctuation, Whitespace),
                'attr_id'),
            (r'\b(n|ne|e|se|s|sw|w|nw|c|_)\b', Name.Builtin),
            (r'\b\D\w*', Name.Tag),  # node
            (r'[-]?((\.[0-9]+)|([0-9]+(\.[0-9]*)?))', Number),
            (r'""(\\""|[^""])*?""', Name.Tag),  # quoted node
            (r'<', Punctuation, 'xml'),
        ],
        'attr_id': [
            (r'\b\D\w*', String, '#pop'),
            (r'[-]?((\.[0-9]+)|([0-9]+(\.[0-9]*)?))', Number, '#pop'),
            (r'""(\\""|[^""])*?""', String.Double, '#pop'),
            (r'<', Punctuation, ('#pop', 'xml')),
        ],
        'xml': [
            (r'<', Punctuation, '#push'),
            (r'>', Punctuation, '#pop'),
            (r'\s+', Whitespace),
            (r'[^<>\s]', Name.Tag),
        ]
    }"
258	jackson	2	"import re
import textwrap
import email.message

from ._text import FoldedCase


class Message(email.message.Message):
    multiple_use_keys = set(
        map(
            FoldedCase,
            [
                'Classifier',
                'Obsoletes-Dist',
                'Platform',
                'Project-URL',
                'Provides-Dist',
                'Provides-Extra',
                'Requires-Dist',
                'Requires-External',
                'Supported-Platform',
                'Dynamic',
            ],
        )
    )
    """"""
    Keys that may be indicated multiple times per PEP 566.
    """"""

    def __new__(cls, orig: email.message.Message):
        res = super().__new__(cls)
        vars(res).update(vars(orig))
        return res

    def __init__(self, *args, **kwargs):
        self._headers = self._repair_headers()

    # suppress spurious error from mypy
    def __iter__(self):
        return super().__iter__()

    def _repair_headers(self):
        def redent(value):
            ""Correct for RFC822 indentation""
            if not value or '\n' not in value:
                return value
            return textwrap.dedent(' ' * 8 + value)

        headers = [(key, redent(value)) for key, value in vars(self)['_headers']]
        if self._payload:
            headers.append(('Description', self.get_payload()))
        return headers

    @property
    def json(self):
        """"""
        Convert PackageMetadata to a JSON-compatible format
        per PEP 0566.
        """"""

        def transform(key):
            value = self.get_all(key) if key in self.multiple_use_keys else self[key]
            if key == 'Keywords':
                value = re.split(r'\s+', value)
            tk = key.lower().replace('-', '_')
            return tk, value

        return dict(map(transform, map(FoldedCase, self)))"
258	donghui	2	"import re
import textwrap
import email.message

from ._text import FoldedCase


class Message(email.message.Message):
    multiple_use_keys = set(
        map(
            FoldedCase,
            [
                'Classifier',
                'Obsoletes-Dist',
                'Platform',
                'Project-URL',
                'Provides-Dist',
                'Provides-Extra',
                'Requires-Dist',
                'Requires-External',
                'Supported-Platform',
                'Dynamic',
            ],
        )
    )
    """"""
    Keys that may be indicated multiple times per PEP 566.
    """"""

    def __new__(cls, orig: email.message.Message):
        res = super().__new__(cls)
        vars(res).update(vars(orig))
        return res

    def __init__(self, *args, **kwargs):
        self._headers = self._repair_headers()

    # suppress spurious error from mypy
    def __iter__(self):
        return super().__iter__()

    def _repair_headers(self):
        def redent(value):
            ""Correct for RFC822 indentation""
            if not value or '\n' not in value:
                return value
            return textwrap.dedent(' ' * 8 + value)

        headers = [(key, redent(value)) for key, value in vars(self)['_headers']]
        if self._payload:
            headers.append(('Description', self.get_payload()))
        return headers

    @property
    def json(self):
        """"""
        Convert PackageMetadata to a JSON-compatible format
        per PEP 0566.
        """"""

        def transform(key):
            value = self.get_all(key) if key in self.multiple_use_keys else self[key]
            if key == 'Keywords':
                value = re.split(r'\s+', value)
            tk = key.lower().replace('-', '_')
            return tk, value

        return dict(map(transform, map(FoldedCase, self)))"
349	jackson	3	"import typing as t
from threading import local

if t.TYPE_CHECKING:
    import typing_extensions as te
    from .core import Context

_local = local()


@t.overload
def get_current_context(silent: ""te.Literal[False]"" = False) -> ""Context"":
    ...


@t.overload
def get_current_context(silent: bool = ...) -> t.Optional[""Context""]:
    ...


def get_current_context(silent: bool = False) -> t.Optional[""Context""]:
    """"""Returns the current click context.  This can be used as a way to
    access the current context object from anywhere.  This is a more implicit
    alternative to the :func:`pass_context` decorator.  This function is
    primarily useful for helpers such as :func:`echo` which might be
    interested in changing its behavior based on the current context.

    To push the current context, :meth:`Context.scope` can be used.

    .. versionadded:: 5.0

    :param silent: if set to `True` the return value is `None` if no context
                   is available.  The default behavior is to raise a
                   :exc:`RuntimeError`.
    """"""
    try:
        return t.cast(""Context"", _local.stack[-1])
    except (AttributeError, IndexError) as e:
        if not silent:
            raise RuntimeError(""There is no active click context."") from e

    return None


def push_context(ctx: ""Context"") -> None:
    """"""Pushes a new context to the current stack.""""""
    _local.__dict__.setdefault(""stack"", []).append(ctx)


def pop_context() -> None:
    """"""Removes the top level from the stack.""""""
    _local.stack.pop()


def resolve_color_default(color: t.Optional[bool] = None) -> t.Optional[bool]:
    """"""Internal helper to get the default value of the color flag.  If a
    value is passed it's returned unchanged, otherwise it's looked up from
    the current context.
    """"""
    if color is not None:
        return color

    ctx = get_current_context(silent=True)

    if ctx is not None:
        return ctx.color

    return None"
349	donghui	4	"import typing as t
from threading import local

if t.TYPE_CHECKING:
    import typing_extensions as te
    from .core import Context

_local = local()


@t.overload
def get_current_context(silent: ""te.Literal[False]"" = False) -> ""Context"":
    ...


@t.overload
def get_current_context(silent: bool = ...) -> t.Optional[""Context""]:
    ...


def get_current_context(silent: bool = False) -> t.Optional[""Context""]:
    """"""Returns the current click context.  This can be used as a way to
    access the current context object from anywhere.  This is a more implicit
    alternative to the :func:`pass_context` decorator.  This function is
    primarily useful for helpers such as :func:`echo` which might be
    interested in changing its behavior based on the current context.

    To push the current context, :meth:`Context.scope` can be used.

    .. versionadded:: 5.0

    :param silent: if set to `True` the return value is `None` if no context
                   is available.  The default behavior is to raise a
                   :exc:`RuntimeError`.
    """"""
    try:
        return t.cast(""Context"", _local.stack[-1])
    except (AttributeError, IndexError) as e:
        if not silent:
            raise RuntimeError(""There is no active click context."") from e

    return None


def push_context(ctx: ""Context"") -> None:
    """"""Pushes a new context to the current stack.""""""
    _local.__dict__.setdefault(""stack"", []).append(ctx)


def pop_context() -> None:
    """"""Removes the top level from the stack.""""""
    _local.stack.pop()


def resolve_color_default(color: t.Optional[bool] = None) -> t.Optional[bool]:
    """"""Internal helper to get the default value of the color flag.  If a
    value is passed it's returned unchanged, otherwise it's looked up from
    the current context.
    """"""
    if color is not None:
        return color

    ctx = get_current_context(silent=True)

    if ctx is not None:
        return ctx.color

    return None"
456	jackson	0	"import json

from .oauth import OAuth2Test


class GiteaOAuth2Test(OAuth2Test):
    backend_path = 'social_core.backends.gitea.GiteaOAuth2'
    user_data_url = 'https://gitea.com/api/v1/user'
    expected_username = 'foobar'
    access_token_body = json.dumps({
        'access_token': 'foobar',
        'token_type': 'bearer',
        'expires_in': 7200,
        'refresh_token': 'barfoo'
    })
    user_data_body = json.dumps({
        'id': 123456,
        'login': 'foobar',
        'full_name': 'Foo Bar',
        'email': 'foobar@example.com',
        'avatar_url': 'https://gitea.com/user/avatar/foobar/-1',
        'language': 'en-US',
        'is_admin': False,
        'last_login': '2016-12-28T12:26:19+01:00',
        'created': '2016-12-28T12:26:19+01:00',
        'restricted': False,
        'username': 'foobar'
    })

    def test_login(self):
        self.do_login()

    def test_partial_pipeline(self):
        self.do_partial_pipeline()


class GiteaCustomDomainOAuth2Test(OAuth2Test):
    backend_path = 'social_core.backends.gitea.GiteaOAuth2'
    user_data_url = 'https://example.com/api/v1/user'
    expected_username = 'foobar'
    access_token_body = json.dumps({
        'access_token': 'foobar',
        'token_type': 'bearer',
        'expires_in': 7200,
        'refresh_token': 'barfoo'
    })
    user_data_body = json.dumps({
        'id': 123456,
        'login': 'foobar',
        'full_name': 'Foo Bar',
        'email': 'foobar@example.com',
        'avatar_url': 'https://example.com/user/avatar/foobar/-1',
        'language': 'en-US',
        'is_admin': False,
        'last_login': '2016-12-28T12:26:19+01:00',
        'created': '2016-12-28T12:26:19+01:00',
        'restricted': False,
        'username': 'foobar'
    })

    def test_login(self):
        self.strategy.set_settings({
            'SOCIAL_AUTH_GITEA_API_URL': 'https://example.com'
        })
        self.do_login()

    def test_partial_pipeline(self):
        self.strategy.set_settings({
            'SOCIAL_AUTH_GITEA_API_URL': 'https://example.com'
        })
        self.do_partial_pipeline()"
456	donghui	0	"import json

from .oauth import OAuth2Test


class GiteaOAuth2Test(OAuth2Test):
    backend_path = 'social_core.backends.gitea.GiteaOAuth2'
    user_data_url = 'https://gitea.com/api/v1/user'
    expected_username = 'foobar'
    access_token_body = json.dumps({
        'access_token': 'foobar',
        'token_type': 'bearer',
        'expires_in': 7200,
        'refresh_token': 'barfoo'
    })
    user_data_body = json.dumps({
        'id': 123456,
        'login': 'foobar',
        'full_name': 'Foo Bar',
        'email': 'foobar@example.com',
        'avatar_url': 'https://gitea.com/user/avatar/foobar/-1',
        'language': 'en-US',
        'is_admin': False,
        'last_login': '2016-12-28T12:26:19+01:00',
        'created': '2016-12-28T12:26:19+01:00',
        'restricted': False,
        'username': 'foobar'
    })

    def test_login(self):
        self.do_login()

    def test_partial_pipeline(self):
        self.do_partial_pipeline()


class GiteaCustomDomainOAuth2Test(OAuth2Test):
    backend_path = 'social_core.backends.gitea.GiteaOAuth2'
    user_data_url = 'https://example.com/api/v1/user'
    expected_username = 'foobar'
    access_token_body = json.dumps({
        'access_token': 'foobar',
        'token_type': 'bearer',
        'expires_in': 7200,
        'refresh_token': 'barfoo'
    })
    user_data_body = json.dumps({
        'id': 123456,
        'login': 'foobar',
        'full_name': 'Foo Bar',
        'email': 'foobar@example.com',
        'avatar_url': 'https://example.com/user/avatar/foobar/-1',
        'language': 'en-US',
        'is_admin': False,
        'last_login': '2016-12-28T12:26:19+01:00',
        'created': '2016-12-28T12:26:19+01:00',
        'restricted': False,
        'username': 'foobar'
    })

    def test_login(self):
        self.strategy.set_settings({
            'SOCIAL_AUTH_GITEA_API_URL': 'https://example.com'
        })
        self.do_login()

    def test_partial_pipeline(self):
        self.strategy.set_settings({
            'SOCIAL_AUTH_GITEA_API_URL': 'https://example.com'
        })
        self.do_partial_pipeline()"
462	jackson	2	"""""""
Dummy database backend for Django.

Django uses this if the database ENGINE setting is empty (None or empty string).

Each of these API functions, except connection.close(), raise
ImproperlyConfigured.
""""""

from django.core.exceptions import ImproperlyConfigured
from django.db.backends.base.base import BaseDatabaseWrapper
from django.db.backends.base.client import BaseDatabaseClient
from django.db.backends.base.creation import BaseDatabaseCreation
from django.db.backends.base.introspection import BaseDatabaseIntrospection
from django.db.backends.base.operations import BaseDatabaseOperations
from django.db.backends.dummy.features import DummyDatabaseFeatures


def complain(*args, **kwargs):
    raise ImproperlyConfigured(
        ""settings.DATABASES is improperly configured. ""
        ""Please supply the ENGINE value. Check ""
        ""settings documentation for more details.""
    )


def ignore(*args, **kwargs):
    pass


class DatabaseOperations(BaseDatabaseOperations):
    quote_name = complain


class DatabaseClient(BaseDatabaseClient):
    runshell = complain


class DatabaseCreation(BaseDatabaseCreation):
    create_test_db = ignore
    destroy_test_db = ignore


class DatabaseIntrospection(BaseDatabaseIntrospection):
    get_table_list = complain
    get_table_description = complain
    get_relations = complain
    get_indexes = complain


class DatabaseWrapper(BaseDatabaseWrapper):
    operators = {}
    # Override the base class implementations with null
    # implementations. Anything that tries to actually
    # do something raises complain; anything that tries
    # to rollback or undo something raises ignore.
    _cursor = complain
    ensure_connection = complain
    _commit = complain
    _rollback = ignore
    _close = ignore
    _savepoint = ignore
    _savepoint_commit = complain
    _savepoint_rollback = ignore
    _set_autocommit = complain
    # Classes instantiated in __init__().
    client_class = DatabaseClient
    creation_class = DatabaseCreation
    features_class = DummyDatabaseFeatures
    introspection_class = DatabaseIntrospection
    ops_class = DatabaseOperations

    def is_usable(self):
        return True"
462	donghui	2	"""""""
Dummy database backend for Django.

Django uses this if the database ENGINE setting is empty (None or empty string).

Each of these API functions, except connection.close(), raise
ImproperlyConfigured.
""""""

from django.core.exceptions import ImproperlyConfigured
from django.db.backends.base.base import BaseDatabaseWrapper
from django.db.backends.base.client import BaseDatabaseClient
from django.db.backends.base.creation import BaseDatabaseCreation
from django.db.backends.base.introspection import BaseDatabaseIntrospection
from django.db.backends.base.operations import BaseDatabaseOperations
from django.db.backends.dummy.features import DummyDatabaseFeatures


def complain(*args, **kwargs):
    raise ImproperlyConfigured(
        ""settings.DATABASES is improperly configured. ""
        ""Please supply the ENGINE value. Check ""
        ""settings documentation for more details.""
    )


def ignore(*args, **kwargs):
    pass


class DatabaseOperations(BaseDatabaseOperations):
    quote_name = complain


class DatabaseClient(BaseDatabaseClient):
    runshell = complain


class DatabaseCreation(BaseDatabaseCreation):
    create_test_db = ignore
    destroy_test_db = ignore


class DatabaseIntrospection(BaseDatabaseIntrospection):
    get_table_list = complain
    get_table_description = complain
    get_relations = complain
    get_indexes = complain


class DatabaseWrapper(BaseDatabaseWrapper):
    operators = {}
    # Override the base class implementations with null
    # implementations. Anything that tries to actually
    # do something raises complain; anything that tries
    # to rollback or undo something raises ignore.
    _cursor = complain
    ensure_connection = complain
    _commit = complain
    _rollback = ignore
    _close = ignore
    _savepoint = ignore
    _savepoint_commit = complain
    _savepoint_rollback = ignore
    _set_autocommit = complain
    # Classes instantiated in __init__().
    client_class = DatabaseClient
    creation_class = DatabaseCreation
    features_class = DummyDatabaseFeatures
    introspection_class = DatabaseIntrospection
    ops_class = DatabaseOperations

    def is_usable(self):
        return True"
433	jackson	2	"# Copyright 2016 Julien Danjou
# Copyright 2016 Joshua Harlow
# Copyright 2013-2014 Ray Holder
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import sys
import typing


# sys.maxsize:
# An integer giving the maximum value a variable of type Py_ssize_t can take.
MAX_WAIT = sys.maxsize / 2


def find_ordinal(pos_num: int) -> str:
    # See: https://en.wikipedia.org/wiki/English_numerals#Ordinal_numbers
    if pos_num == 0:
        return ""th""
    elif pos_num == 1:
        return ""st""
    elif pos_num == 2:
        return ""nd""
    elif pos_num == 3:
        return ""rd""
    elif 4 <= pos_num <= 20:
        return ""th""
    else:
        return find_ordinal(pos_num % 10)


def to_ordinal(pos_num: int) -> str:
    return f""{pos_num}{find_ordinal(pos_num)}""


def get_callback_name(cb: typing.Callable[..., typing.Any]) -> str:
    """"""Get a callback fully-qualified name.

    If no name can be produced ``repr(cb)`` is called and returned.
    """"""
    segments = []
    try:
        segments.append(cb.__qualname__)
    except AttributeError:
        try:
            segments.append(cb.__name__)
        except AttributeError:
            pass
    if not segments:
        return repr(cb)
    else:
        try:
            # When running under sphinx it appears this can be none?
            if cb.__module__:
                segments.insert(0, cb.__module__)
        except AttributeError:
            pass
        return ""."".join(segments)"
433	donghui	2	"# Copyright 2016 Julien Danjou
# Copyright 2016 Joshua Harlow
# Copyright 2013-2014 Ray Holder
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import sys
import typing


# sys.maxsize:
# An integer giving the maximum value a variable of type Py_ssize_t can take.
MAX_WAIT = sys.maxsize / 2


def find_ordinal(pos_num: int) -> str:
    # See: https://en.wikipedia.org/wiki/English_numerals#Ordinal_numbers
    if pos_num == 0:
        return ""th""
    elif pos_num == 1:
        return ""st""
    elif pos_num == 2:
        return ""nd""
    elif pos_num == 3:
        return ""rd""
    elif 4 <= pos_num <= 20:
        return ""th""
    else:
        return find_ordinal(pos_num % 10)


def to_ordinal(pos_num: int) -> str:
    return f""{pos_num}{find_ordinal(pos_num)}""


def get_callback_name(cb: typing.Callable[..., typing.Any]) -> str:
    """"""Get a callback fully-qualified name.

    If no name can be produced ``repr(cb)`` is called and returned.
    """"""
    segments = []
    try:
        segments.append(cb.__qualname__)
    except AttributeError:
        try:
            segments.append(cb.__name__)
        except AttributeError:
            pass
    if not segments:
        return repr(cb)
    else:
        try:
            # When running under sphinx it appears this can be none?
            if cb.__module__:
                segments.insert(0, cb.__module__)
        except AttributeError:
            pass
        return ""."".join(segments)"
491	jackson	3	"# Copyright 2020 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.


# [START automl_video_classification_list_datasets_beta]
# [START automl_video_object_tracking_list_datasets_beta]
from google.cloud import automl_v1beta1 as automl


def list_datasets(project_id=""YOUR_PROJECT_ID""):
    """"""List datasets.""""""
    client = automl.AutoMlClient()
    # A resource that represents Google Cloud Platform location.
    project_location = f""projects/{project_id}/locations/us-central1""

    # List all the datasets available in the region.
    request = automl.ListDatasetsRequest(parent=project_location, filter="""")
    response = client.list_datasets(request=request)

    print(""List of datasets:"")
    for dataset in response:
        print(""Dataset name: {}"".format(dataset.name))
        print(""Dataset id: {}"".format(dataset.name.split(""/"")[-1]))
        print(""Dataset display name: {}"".format(dataset.display_name))
        print(""Dataset create time: {}"".format(dataset.create_time))
        # [END automl_video_object_tracking_list_datasets_beta]

        print(
            ""Video classification dataset metadata: {}"".format(
                dataset.video_classification_dataset_metadata
            )
        )
        # [END automl_video_classification_list_datasets_beta]

        # [START automl_video_object_tracking_list_datasets_beta]
        print(
            ""Video object tracking dataset metadata: {}"".format(
                dataset.video_object_tracking_dataset_metadata
            )
        )
        # [END automl_video_object_tracking_list_datasets_beta]"
491	donghui	2	"# Copyright 2020 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.


# [START automl_video_classification_list_datasets_beta]
# [START automl_video_object_tracking_list_datasets_beta]
from google.cloud import automl_v1beta1 as automl


def list_datasets(project_id=""YOUR_PROJECT_ID""):
    """"""List datasets.""""""
    client = automl.AutoMlClient()
    # A resource that represents Google Cloud Platform location.
    project_location = f""projects/{project_id}/locations/us-central1""

    # List all the datasets available in the region.
    request = automl.ListDatasetsRequest(parent=project_location, filter="""")
    response = client.list_datasets(request=request)

    print(""List of datasets:"")
    for dataset in response:
        print(""Dataset name: {}"".format(dataset.name))
        print(""Dataset id: {}"".format(dataset.name.split(""/"")[-1]))
        print(""Dataset display name: {}"".format(dataset.display_name))
        print(""Dataset create time: {}"".format(dataset.create_time))
        # [END automl_video_object_tracking_list_datasets_beta]

        print(
            ""Video classification dataset metadata: {}"".format(
                dataset.video_classification_dataset_metadata
            )
        )
        # [END automl_video_classification_list_datasets_beta]

        # [START automl_video_object_tracking_list_datasets_beta]
        print(
            ""Video object tracking dataset metadata: {}"".format(
                dataset.video_object_tracking_dataset_metadata
            )
        )
        # [END automl_video_object_tracking_list_datasets_beta]"
413	jackson	2	"# -*- coding: utf-8 -*-
""""""
set_fake_passwords.py

    Reset all user passwords to a common value. Useful for testing in a
    development environment. As such, this command is only available when
    setting.DEBUG is True.

""""""
from typing import List

from django.conf import settings
from django.contrib.auth import get_user_model
from django.core.management.base import BaseCommand, CommandError

from django_extensions.management.utils import signalcommand

DEFAULT_FAKE_PASSWORD = 'password'


class Command(BaseCommand):
    help = 'DEBUG only: sets all user passwords to a common value (""%s"" by default)' % (DEFAULT_FAKE_PASSWORD, )
    requires_system_checks: List[str] = []

    def add_arguments(self, parser):
        super().add_arguments(parser)
        parser.add_argument(
            '--prompt', dest='prompt_passwd', default=False,
            action='store_true',
            help='Prompts for the new password to apply to all users'
        )
        parser.add_argument(
            '--password', dest='default_passwd', default=DEFAULT_FAKE_PASSWORD,
            help='Use this as default password.'
        )

    @signalcommand
    def handle(self, *args, **options):
        if not settings.DEBUG:
            raise CommandError('Only available in debug mode')

        if options['prompt_passwd']:
            from getpass import getpass
            passwd = getpass('Password: ')
            if not passwd:
                raise CommandError('You must enter a valid password')
        else:
            passwd = options['default_passwd']

        User = get_user_model()
        user = User()
        user.set_password(passwd)
        count = User.objects.all().update(password=user.password)

        print('Reset %d passwords' % count)"
413	donghui	1	"# -*- coding: utf-8 -*-
""""""
set_fake_passwords.py

    Reset all user passwords to a common value. Useful for testing in a
    development environment. As such, this command is only available when
    setting.DEBUG is True.

""""""
from typing import List

from django.conf import settings
from django.contrib.auth import get_user_model
from django.core.management.base import BaseCommand, CommandError

from django_extensions.management.utils import signalcommand

DEFAULT_FAKE_PASSWORD = 'password'


class Command(BaseCommand):
    help = 'DEBUG only: sets all user passwords to a common value (""%s"" by default)' % (DEFAULT_FAKE_PASSWORD, )
    requires_system_checks: List[str] = []

    def add_arguments(self, parser):
        super().add_arguments(parser)
        parser.add_argument(
            '--prompt', dest='prompt_passwd', default=False,
            action='store_true',
            help='Prompts for the new password to apply to all users'
        )
        parser.add_argument(
            '--password', dest='default_passwd', default=DEFAULT_FAKE_PASSWORD,
            help='Use this as default password.'
        )

    @signalcommand
    def handle(self, *args, **options):
        if not settings.DEBUG:
            raise CommandError('Only available in debug mode')

        if options['prompt_passwd']:
            from getpass import getpass
            passwd = getpass('Password: ')
            if not passwd:
                raise CommandError('You must enter a valid password')
        else:
            passwd = options['default_passwd']

        User = get_user_model()
        user = User()
        user.set_password(passwd)
        count = User.objects.all().update(password=user.password)

        print('Reset %d passwords' % count)"
442	jackson	0	"import pytest

from pandas.util._decorators import deprecate_kwarg

import pandas._testing as tm


@deprecate_kwarg(""old"", ""new"")
def _f1(new=False):
    return new


_f2_mappings = {""yes"": True, ""no"": False}


@deprecate_kwarg(""old"", ""new"", _f2_mappings)
def _f2(new=False):
    return new


def _f3_mapping(x):
    return x + 1


@deprecate_kwarg(""old"", ""new"", _f3_mapping)
def _f3(new=0):
    return new


@pytest.mark.parametrize(""key,klass"", [(""old"", FutureWarning), (""new"", None)])
def test_deprecate_kwarg(key, klass):
    x = 78

    with tm.assert_produces_warning(klass):
        assert _f1(**{key: x}) == x


@pytest.mark.parametrize(""key"", list(_f2_mappings.keys()))
def test_dict_deprecate_kwarg(key):
    with tm.assert_produces_warning(FutureWarning):
        assert _f2(old=key) == _f2_mappings[key]


@pytest.mark.parametrize(""key"", [""bogus"", 12345, -1.23])
def test_missing_deprecate_kwarg(key):
    with tm.assert_produces_warning(FutureWarning):
        assert _f2(old=key) == key


@pytest.mark.parametrize(""x"", [1, -1.4, 0])
def test_callable_deprecate_kwarg(x):
    with tm.assert_produces_warning(FutureWarning):
        assert _f3(old=x) == _f3_mapping(x)


def test_callable_deprecate_kwarg_fail():
    msg = ""((can only|cannot) concatenate)|(must be str)|(Can't convert)""

    with pytest.raises(TypeError, match=msg):
        _f3(old=""hello"")


def test_bad_deprecate_kwarg():
    msg = ""mapping from old to new argument values must be dict or callable!""

    with pytest.raises(TypeError, match=msg):

        @deprecate_kwarg(""old"", ""new"", 0)
        def f4(new=None):
            return new


@deprecate_kwarg(""old"", None)
def _f4(old=True, unchanged=True):
    return old, unchanged


@pytest.mark.parametrize(""key"", [""old"", ""unchanged""])
def test_deprecate_keyword(key):
    x = 9

    if key == ""old"":
        klass = FutureWarning
        expected = (x, True)
    else:
        klass = None
        expected = (True, x)

    with tm.assert_produces_warning(klass):
        assert _f4(**{key: x}) == expected"
442	donghui	0	"import pytest

from pandas.util._decorators import deprecate_kwarg

import pandas._testing as tm


@deprecate_kwarg(""old"", ""new"")
def _f1(new=False):
    return new


_f2_mappings = {""yes"": True, ""no"": False}


@deprecate_kwarg(""old"", ""new"", _f2_mappings)
def _f2(new=False):
    return new


def _f3_mapping(x):
    return x + 1


@deprecate_kwarg(""old"", ""new"", _f3_mapping)
def _f3(new=0):
    return new


@pytest.mark.parametrize(""key,klass"", [(""old"", FutureWarning), (""new"", None)])
def test_deprecate_kwarg(key, klass):
    x = 78

    with tm.assert_produces_warning(klass):
        assert _f1(**{key: x}) == x


@pytest.mark.parametrize(""key"", list(_f2_mappings.keys()))
def test_dict_deprecate_kwarg(key):
    with tm.assert_produces_warning(FutureWarning):
        assert _f2(old=key) == _f2_mappings[key]


@pytest.mark.parametrize(""key"", [""bogus"", 12345, -1.23])
def test_missing_deprecate_kwarg(key):
    with tm.assert_produces_warning(FutureWarning):
        assert _f2(old=key) == key


@pytest.mark.parametrize(""x"", [1, -1.4, 0])
def test_callable_deprecate_kwarg(x):
    with tm.assert_produces_warning(FutureWarning):
        assert _f3(old=x) == _f3_mapping(x)


def test_callable_deprecate_kwarg_fail():
    msg = ""((can only|cannot) concatenate)|(must be str)|(Can't convert)""

    with pytest.raises(TypeError, match=msg):
        _f3(old=""hello"")


def test_bad_deprecate_kwarg():
    msg = ""mapping from old to new argument values must be dict or callable!""

    with pytest.raises(TypeError, match=msg):

        @deprecate_kwarg(""old"", ""new"", 0)
        def f4(new=None):
            return new


@deprecate_kwarg(""old"", None)
def _f4(old=True, unchanged=True):
    return old, unchanged


@pytest.mark.parametrize(""key"", [""old"", ""unchanged""])
def test_deprecate_keyword(key):
    x = 9

    if key == ""old"":
        klass = FutureWarning
        expected = (x, True)
    else:
        klass = None
        expected = (True, x)

    with tm.assert_produces_warning(klass):
        assert _f4(**{key: x}) == expected"
485	jackson	4	"""""""
NGP VAN's `ActionID` Provider

http://developers.ngpvan.com/action-id
""""""
from openid.extensions import ax

from .open_id import OpenIdAuth


class ActionIDOpenID(OpenIdAuth):
    """"""
    NGP VAN's ActionID OpenID 1.1 authentication backend
    """"""
    name = 'actionid-openid'
    URL = 'https://accounts.ngpvan.com/Home/Xrds'
    USERNAME_KEY = 'email'

    def get_ax_attributes(self):
        """"""
        Return the AX attributes that ActionID responds with, as well as the
        user data result that it must map to.
        """"""
        return [
            ('http://openid.net/schema/contact/internet/email', 'email'),
            ('http://openid.net/schema/contact/phone/business', 'phone'),
            ('http://openid.net/schema/namePerson/first', 'first_name'),
            ('http://openid.net/schema/namePerson/last', 'last_name'),
            ('http://openid.net/schema/namePerson', 'fullname'),
        ]

    def setup_request(self, params=None):
        """"""
        Setup the OpenID request

        Because ActionID does not advertise the availiability of AX attributes
        nor use standard attribute aliases, we need to setup the attributes
        manually instead of rely on the parent OpenIdAuth.setup_request()
        """"""
        request = self.openid_request(params)

        fetch_request = ax.FetchRequest()
        fetch_request.add(ax.AttrInfo(
            'http://openid.net/schema/contact/internet/email',
            alias='ngpvanemail',
            required=True
        ))

        fetch_request.add(ax.AttrInfo(
            'http://openid.net/schema/contact/phone/business',
            alias='ngpvanphone',
            required=False
        ))
        fetch_request.add(ax.AttrInfo(
            'http://openid.net/schema/namePerson/first',
            alias='ngpvanfirstname',
            required=False
        ))
        fetch_request.add(ax.AttrInfo(
            'http://openid.net/schema/namePerson/last',
            alias='ngpvanlastname',
            required=False
        ))
        request.addExtension(fetch_request)

        return request"
485	donghui	3	"""""""
NGP VAN's `ActionID` Provider

http://developers.ngpvan.com/action-id
""""""
from openid.extensions import ax

from .open_id import OpenIdAuth


class ActionIDOpenID(OpenIdAuth):
    """"""
    NGP VAN's ActionID OpenID 1.1 authentication backend
    """"""
    name = 'actionid-openid'
    URL = 'https://accounts.ngpvan.com/Home/Xrds'
    USERNAME_KEY = 'email'

    def get_ax_attributes(self):
        """"""
        Return the AX attributes that ActionID responds with, as well as the
        user data result that it must map to.
        """"""
        return [
            ('http://openid.net/schema/contact/internet/email', 'email'),
            ('http://openid.net/schema/contact/phone/business', 'phone'),
            ('http://openid.net/schema/namePerson/first', 'first_name'),
            ('http://openid.net/schema/namePerson/last', 'last_name'),
            ('http://openid.net/schema/namePerson', 'fullname'),
        ]

    def setup_request(self, params=None):
        """"""
        Setup the OpenID request

        Because ActionID does not advertise the availiability of AX attributes
        nor use standard attribute aliases, we need to setup the attributes
        manually instead of rely on the parent OpenIdAuth.setup_request()
        """"""
        request = self.openid_request(params)

        fetch_request = ax.FetchRequest()
        fetch_request.add(ax.AttrInfo(
            'http://openid.net/schema/contact/internet/email',
            alias='ngpvanemail',
            required=True
        ))

        fetch_request.add(ax.AttrInfo(
            'http://openid.net/schema/contact/phone/business',
            alias='ngpvanphone',
            required=False
        ))
        fetch_request.add(ax.AttrInfo(
            'http://openid.net/schema/namePerson/first',
            alias='ngpvanfirstname',
            required=False
        ))
        fetch_request.add(ax.AttrInfo(
            'http://openid.net/schema/namePerson/last',
            alias='ngpvanlastname',
            required=False
        ))
        request.addExtension(fetch_request)

        return request"
476	jackson	1	"import pytest

from pandas.util._validators import validate_args

_fname = ""func""


def test_bad_min_fname_arg_count():
    msg = ""'max_fname_arg_count' must be non-negative""

    with pytest.raises(ValueError, match=msg):
        validate_args(_fname, (None,), -1, ""foo"")


def test_bad_arg_length_max_value_single():
    args = (None, None)
    compat_args = (""foo"",)

    min_fname_arg_count = 0
    max_length = len(compat_args) + min_fname_arg_count
    actual_length = len(args) + min_fname_arg_count
    msg = (
        rf""{_fname}\(\) takes at most {max_length} ""
        rf""argument \({actual_length} given\)""
    )

    with pytest.raises(TypeError, match=msg):
        validate_args(_fname, args, min_fname_arg_count, compat_args)


def test_bad_arg_length_max_value_multiple():
    args = (None, None)
    compat_args = {""foo"": None}

    min_fname_arg_count = 2
    max_length = len(compat_args) + min_fname_arg_count
    actual_length = len(args) + min_fname_arg_count
    msg = (
        rf""{_fname}\(\) takes at most {max_length} ""
        rf""arguments \({actual_length} given\)""
    )

    with pytest.raises(TypeError, match=msg):
        validate_args(_fname, args, min_fname_arg_count, compat_args)


@pytest.mark.parametrize(""i"", range(1, 3))
def test_not_all_defaults(i):
    bad_arg = ""foo""
    msg = (
        f""the '{bad_arg}' parameter is not supported ""
        rf""in the pandas implementation of {_fname}\(\)""
    )

    compat_args = {""foo"": 2, ""bar"": -1, ""baz"": 3}
    arg_vals = (1, -1, 3)

    with pytest.raises(ValueError, match=msg):
        validate_args(_fname, arg_vals[:i], 2, compat_args)


def test_validation():
    # No exceptions should be raised.
    validate_args(_fname, (None,), 2, {""out"": None})

    compat_args = {""axis"": 1, ""out"": None}
    validate_args(_fname, (1, None), 2, compat_args)"
476	donghui	1	"import pytest

from pandas.util._validators import validate_args

_fname = ""func""


def test_bad_min_fname_arg_count():
    msg = ""'max_fname_arg_count' must be non-negative""

    with pytest.raises(ValueError, match=msg):
        validate_args(_fname, (None,), -1, ""foo"")


def test_bad_arg_length_max_value_single():
    args = (None, None)
    compat_args = (""foo"",)

    min_fname_arg_count = 0
    max_length = len(compat_args) + min_fname_arg_count
    actual_length = len(args) + min_fname_arg_count
    msg = (
        rf""{_fname}\(\) takes at most {max_length} ""
        rf""argument \({actual_length} given\)""
    )

    with pytest.raises(TypeError, match=msg):
        validate_args(_fname, args, min_fname_arg_count, compat_args)


def test_bad_arg_length_max_value_multiple():
    args = (None, None)
    compat_args = {""foo"": None}

    min_fname_arg_count = 2
    max_length = len(compat_args) + min_fname_arg_count
    actual_length = len(args) + min_fname_arg_count
    msg = (
        rf""{_fname}\(\) takes at most {max_length} ""
        rf""arguments \({actual_length} given\)""
    )

    with pytest.raises(TypeError, match=msg):
        validate_args(_fname, args, min_fname_arg_count, compat_args)


@pytest.mark.parametrize(""i"", range(1, 3))
def test_not_all_defaults(i):
    bad_arg = ""foo""
    msg = (
        f""the '{bad_arg}' parameter is not supported ""
        rf""in the pandas implementation of {_fname}\(\)""
    )

    compat_args = {""foo"": 2, ""bar"": -1, ""baz"": 3}
    arg_vals = (1, -1, 3)

    with pytest.raises(ValueError, match=msg):
        validate_args(_fname, arg_vals[:i], 2, compat_args)


def test_validation():
    # No exceptions should be raised.
    validate_args(_fname, (None,), 2, {""out"": None})

    compat_args = {""axis"": 1, ""out"": None}
    validate_args(_fname, (1, None), 2, compat_args)"
427	jackson	3	"""""""
 The GeometryColumns and SpatialRefSys models for the PostGIS backend.
""""""
from django.contrib.gis.db.backends.base.models import SpatialRefSysMixin
from django.db import models


class PostGISGeometryColumns(models.Model):
    """"""
    The 'geometry_columns' view from PostGIS. See the PostGIS
    documentation at Ch. 4.3.2.
    """"""

    f_table_catalog = models.CharField(max_length=256)
    f_table_schema = models.CharField(max_length=256)
    f_table_name = models.CharField(max_length=256)
    f_geometry_column = models.CharField(max_length=256)
    coord_dimension = models.IntegerField()
    srid = models.IntegerField(primary_key=True)
    type = models.CharField(max_length=30)

    class Meta:
        app_label = ""gis""
        db_table = ""geometry_columns""
        managed = False

    def __str__(self):
        return ""%s.%s - %dD %s field (SRID: %d)"" % (
            self.f_table_name,
            self.f_geometry_column,
            self.coord_dimension,
            self.type,
            self.srid,
        )

    @classmethod
    def table_name_col(cls):
        """"""
        Return the name of the metadata column used to store the feature table
        name.
        """"""
        return ""f_table_name""

    @classmethod
    def geom_col_name(cls):
        """"""
        Return the name of the metadata column used to store the feature
        geometry column.
        """"""
        return ""f_geometry_column""


class PostGISSpatialRefSys(models.Model, SpatialRefSysMixin):
    """"""
    The 'spatial_ref_sys' table from PostGIS. See the PostGIS
    documentation at Ch. 4.2.1.
    """"""

    srid = models.IntegerField(primary_key=True)
    auth_name = models.CharField(max_length=256)
    auth_srid = models.IntegerField()
    srtext = models.CharField(max_length=2048)
    proj4text = models.CharField(max_length=2048)

    class Meta:
        app_label = ""gis""
        db_table = ""spatial_ref_sys""
        managed = False

    @property
    def wkt(self):
        return self.srtext"
427	donghui	2	"""""""
 The GeometryColumns and SpatialRefSys models for the PostGIS backend.
""""""
from django.contrib.gis.db.backends.base.models import SpatialRefSysMixin
from django.db import models


class PostGISGeometryColumns(models.Model):
    """"""
    The 'geometry_columns' view from PostGIS. See the PostGIS
    documentation at Ch. 4.3.2.
    """"""

    f_table_catalog = models.CharField(max_length=256)
    f_table_schema = models.CharField(max_length=256)
    f_table_name = models.CharField(max_length=256)
    f_geometry_column = models.CharField(max_length=256)
    coord_dimension = models.IntegerField()
    srid = models.IntegerField(primary_key=True)
    type = models.CharField(max_length=30)

    class Meta:
        app_label = ""gis""
        db_table = ""geometry_columns""
        managed = False

    def __str__(self):
        return ""%s.%s - %dD %s field (SRID: %d)"" % (
            self.f_table_name,
            self.f_geometry_column,
            self.coord_dimension,
            self.type,
            self.srid,
        )

    @classmethod
    def table_name_col(cls):
        """"""
        Return the name of the metadata column used to store the feature table
        name.
        """"""
        return ""f_table_name""

    @classmethod
    def geom_col_name(cls):
        """"""
        Return the name of the metadata column used to store the feature
        geometry column.
        """"""
        return ""f_geometry_column""


class PostGISSpatialRefSys(models.Model, SpatialRefSysMixin):
    """"""
    The 'spatial_ref_sys' table from PostGIS. See the PostGIS
    documentation at Ch. 4.2.1.
    """"""

    srid = models.IntegerField(primary_key=True)
    auth_name = models.CharField(max_length=256)
    auth_srid = models.IntegerField()
    srtext = models.CharField(max_length=2048)
    proj4text = models.CharField(max_length=2048)

    class Meta:
        app_label = ""gis""
        db_table = ""spatial_ref_sys""
        managed = False

    @property
    def wkt(self):
        return self.srtext"
369	jackson	3	"""""""
    pygments.filter
    ~~~~~~~~~~~~~~~

    Module that implements the default filter.

    :copyright: Copyright 2006-2022 by the Pygments team, see AUTHORS.
    :license: BSD, see LICENSE for details.
""""""


def apply_filters(stream, filters, lexer=None):
    """"""
    Use this method to apply an iterable of filters to
    a stream. If lexer is given it's forwarded to the
    filter, otherwise the filter receives `None`.
    """"""
    def _apply(filter_, stream):
        yield from filter_.filter(lexer, stream)
    for filter_ in filters:
        stream = _apply(filter_, stream)
    return stream


def simplefilter(f):
    """"""
    Decorator that converts a function into a filter::

        @simplefilter
        def lowercase(self, lexer, stream, options):
            for ttype, value in stream:
                yield ttype, value.lower()
    """"""
    return type(f.__name__, (FunctionFilter,), {
        '__module__': getattr(f, '__module__'),
        '__doc__': f.__doc__,
        'function': f,
    })


class Filter:
    """"""
    Default filter. Subclass this class or use the `simplefilter`
    decorator to create own filters.
    """"""

    def __init__(self, **options):
        self.options = options

    def filter(self, lexer, stream):
        raise NotImplementedError()


class FunctionFilter(Filter):
    """"""
    Abstract class used by `simplefilter` to create simple
    function filters on the fly. The `simplefilter` decorator
    automatically creates subclasses of this class for
    functions passed to it.
    """"""
    function = None

    def __init__(self, **options):
        if not hasattr(self, 'function'):
            raise TypeError('%r used without bound function' %
                            self.__class__.__name__)
        Filter.__init__(self, **options)

    def filter(self, lexer, stream):
        # pylint: disable=not-callable
        yield from self.function(lexer, stream, self.options)"
369	donghui	2	"""""""
    pygments.filter
    ~~~~~~~~~~~~~~~

    Module that implements the default filter.

    :copyright: Copyright 2006-2022 by the Pygments team, see AUTHORS.
    :license: BSD, see LICENSE for details.
""""""


def apply_filters(stream, filters, lexer=None):
    """"""
    Use this method to apply an iterable of filters to
    a stream. If lexer is given it's forwarded to the
    filter, otherwise the filter receives `None`.
    """"""
    def _apply(filter_, stream):
        yield from filter_.filter(lexer, stream)
    for filter_ in filters:
        stream = _apply(filter_, stream)
    return stream


def simplefilter(f):
    """"""
    Decorator that converts a function into a filter::

        @simplefilter
        def lowercase(self, lexer, stream, options):
            for ttype, value in stream:
                yield ttype, value.lower()
    """"""
    return type(f.__name__, (FunctionFilter,), {
        '__module__': getattr(f, '__module__'),
        '__doc__': f.__doc__,
        'function': f,
    })


class Filter:
    """"""
    Default filter. Subclass this class or use the `simplefilter`
    decorator to create own filters.
    """"""

    def __init__(self, **options):
        self.options = options

    def filter(self, lexer, stream):
        raise NotImplementedError()


class FunctionFilter(Filter):
    """"""
    Abstract class used by `simplefilter` to create simple
    function filters on the fly. The `simplefilter` decorator
    automatically creates subclasses of this class for
    functions passed to it.
    """"""
    function = None

    def __init__(self, **options):
        if not hasattr(self, 'function'):
            raise TypeError('%r used without bound function' %
                            self.__class__.__name__)
        Filter.__init__(self, **options)

    def filter(self, lexer, stream):
        # pylint: disable=not-callable
        yield from self.function(lexer, stream, self.options)"
338	jackson	2	""""""" Test functions for linalg module using the matrix class.""""""
import numpy as np

from numpy.linalg.tests.test_linalg import (
    LinalgCase, apply_tag, TestQR as _TestQR, LinalgTestCase,
    _TestNorm2D, _TestNormDoubleBase, _TestNormSingleBase, _TestNormInt64Base,
    SolveCases, InvCases, EigvalsCases, EigCases, SVDCases, CondCases,
    PinvCases, DetCases, LstsqCases)


CASES = []

# square test cases
CASES += apply_tag('square', [
    LinalgCase(""0x0_matrix"",
               np.empty((0, 0), dtype=np.double).view(np.matrix),
               np.empty((0, 1), dtype=np.double).view(np.matrix),
               tags={'size-0'}),
    LinalgCase(""matrix_b_only"",
               np.array([[1., 2.], [3., 4.]]),
               np.matrix([2., 1.]).T),
    LinalgCase(""matrix_a_and_b"",
               np.matrix([[1., 2.], [3., 4.]]),
               np.matrix([2., 1.]).T),
])

# hermitian test-cases
CASES += apply_tag('hermitian', [
    LinalgCase(""hmatrix_a_and_b"",
               np.matrix([[1., 2.], [2., 1.]]),
               None),
])
# No need to make generalized or strided cases for matrices.


class MatrixTestCase(LinalgTestCase):
    TEST_CASES = CASES


class TestSolveMatrix(SolveCases, MatrixTestCase):
    pass


class TestInvMatrix(InvCases, MatrixTestCase):
    pass


class TestEigvalsMatrix(EigvalsCases, MatrixTestCase):
    pass


class TestEigMatrix(EigCases, MatrixTestCase):
    pass


class TestSVDMatrix(SVDCases, MatrixTestCase):
    pass


class TestCondMatrix(CondCases, MatrixTestCase):
    pass


class TestPinvMatrix(PinvCases, MatrixTestCase):
    pass


class TestDetMatrix(DetCases, MatrixTestCase):
    pass


class TestLstsqMatrix(LstsqCases, MatrixTestCase):
    pass


class _TestNorm2DMatrix(_TestNorm2D):
    array = np.matrix


class TestNormDoubleMatrix(_TestNorm2DMatrix, _TestNormDoubleBase):
    pass


class TestNormSingleMatrix(_TestNorm2DMatrix, _TestNormSingleBase):
    pass


class TestNormInt64Matrix(_TestNorm2DMatrix, _TestNormInt64Base):
    pass


class TestQRMatrix(_TestQR):
    array = np.matrix"
338	donghui	1	""""""" Test functions for linalg module using the matrix class.""""""
import numpy as np

from numpy.linalg.tests.test_linalg import (
    LinalgCase, apply_tag, TestQR as _TestQR, LinalgTestCase,
    _TestNorm2D, _TestNormDoubleBase, _TestNormSingleBase, _TestNormInt64Base,
    SolveCases, InvCases, EigvalsCases, EigCases, SVDCases, CondCases,
    PinvCases, DetCases, LstsqCases)


CASES = []

# square test cases
CASES += apply_tag('square', [
    LinalgCase(""0x0_matrix"",
               np.empty((0, 0), dtype=np.double).view(np.matrix),
               np.empty((0, 1), dtype=np.double).view(np.matrix),
               tags={'size-0'}),
    LinalgCase(""matrix_b_only"",
               np.array([[1., 2.], [3., 4.]]),
               np.matrix([2., 1.]).T),
    LinalgCase(""matrix_a_and_b"",
               np.matrix([[1., 2.], [3., 4.]]),
               np.matrix([2., 1.]).T),
])

# hermitian test-cases
CASES += apply_tag('hermitian', [
    LinalgCase(""hmatrix_a_and_b"",
               np.matrix([[1., 2.], [2., 1.]]),
               None),
])
# No need to make generalized or strided cases for matrices.


class MatrixTestCase(LinalgTestCase):
    TEST_CASES = CASES


class TestSolveMatrix(SolveCases, MatrixTestCase):
    pass


class TestInvMatrix(InvCases, MatrixTestCase):
    pass


class TestEigvalsMatrix(EigvalsCases, MatrixTestCase):
    pass


class TestEigMatrix(EigCases, MatrixTestCase):
    pass


class TestSVDMatrix(SVDCases, MatrixTestCase):
    pass


class TestCondMatrix(CondCases, MatrixTestCase):
    pass


class TestPinvMatrix(PinvCases, MatrixTestCase):
    pass


class TestDetMatrix(DetCases, MatrixTestCase):
    pass


class TestLstsqMatrix(LstsqCases, MatrixTestCase):
    pass


class _TestNorm2DMatrix(_TestNorm2D):
    array = np.matrix


class TestNormDoubleMatrix(_TestNorm2DMatrix, _TestNormDoubleBase):
    pass


class TestNormSingleMatrix(_TestNorm2DMatrix, _TestNormSingleBase):
    pass


class TestNormInt64Matrix(_TestNorm2DMatrix, _TestNormInt64Base):
    pass


class TestQRMatrix(_TestQR):
    array = np.matrix"
278	jackson	0	"# Copyright (c) 2020, Oracle and/or its affiliates.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License, version 2.0, as
# published by the Free Software Foundation.
#
# This program is also distributed with certain software (including
# but not limited to OpenSSL) that is licensed under separate terms,
# as designated in a particular file or component or in included license
# documentation.  The authors of MySQL hereby grant you an
# additional permission to link the program and your derivative works
# with the separately licensed software that they have included with
# MySQL.
#
# Without limiting anything contained in the foregoing, this file,
# which is part of MySQL Connector/Python, is also subject to the
# Universal FOSS Exception, version 1.0, a copy of which can be found at
# http://oss.oracle.com/licenses/universal-foss-exception.
#
# This program is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
# See the GNU General Public License, version 2.0, for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software Foundation, Inc.,
# 51 Franklin St, Fifth Floor, Boston, MA 02110-1301  USA

from django.db.backends.mysql.features import DatabaseFeatures as MySQLDatabaseFeatures
from django.utils.functional import cached_property


class DatabaseFeatures(MySQLDatabaseFeatures):
    empty_fetchmany_value = []

    @cached_property
    def can_introspect_check_constraints(self):
        return self.connection.mysql_version >= (8, 0, 16)

    @cached_property
    def supports_microsecond_precision(self):
        if self.connection.mysql_version >= (5, 6, 3):
            return True
        return False"
278	donghui	0	"# Copyright (c) 2020, Oracle and/or its affiliates.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License, version 2.0, as
# published by the Free Software Foundation.
#
# This program is also distributed with certain software (including
# but not limited to OpenSSL) that is licensed under separate terms,
# as designated in a particular file or component or in included license
# documentation.  The authors of MySQL hereby grant you an
# additional permission to link the program and your derivative works
# with the separately licensed software that they have included with
# MySQL.
#
# Without limiting anything contained in the foregoing, this file,
# which is part of MySQL Connector/Python, is also subject to the
# Universal FOSS Exception, version 1.0, a copy of which can be found at
# http://oss.oracle.com/licenses/universal-foss-exception.
#
# This program is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
# See the GNU General Public License, version 2.0, for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software Foundation, Inc.,
# 51 Franklin St, Fifth Floor, Boston, MA 02110-1301  USA

from django.db.backends.mysql.features import DatabaseFeatures as MySQLDatabaseFeatures
from django.utils.functional import cached_property


class DatabaseFeatures(MySQLDatabaseFeatures):
    empty_fetchmany_value = []

    @cached_property
    def can_introspect_check_constraints(self):
        return self.connection.mysql_version >= (8, 0, 16)

    @cached_property
    def supports_microsecond_precision(self):
        if self.connection.mysql_version >= (5, 6, 3):
            return True
        return False"
268	jackson	1	"from distutils.util import convert_path
from distutils import log
from distutils.errors import DistutilsOptionError
import os
import shutil

from setuptools.extern import six

from setuptools import Command


class rotate(Command):
    """"""Delete older distributions""""""

    description = ""delete older distributions, keeping N newest files""
    user_options = [
        ('match=', 'm', ""patterns to match (required)""),
        ('dist-dir=', 'd', ""directory where the distributions are""),
        ('keep=', 'k', ""number of matching distributions to keep""),
    ]

    boolean_options = []

    def initialize_options(self):
        self.match = None
        self.dist_dir = None
        self.keep = None

    def finalize_options(self):
        if self.match is None:
            raise DistutilsOptionError(
                ""Must specify one or more (comma-separated) match patterns ""
                ""(e.g. '.zip' or '.egg')""
            )
        if self.keep is None:
            raise DistutilsOptionError(""Must specify number of files to keep"")
        try:
            self.keep = int(self.keep)
        except ValueError:
            raise DistutilsOptionError(""--keep must be an integer"")
        if isinstance(self.match, six.string_types):
            self.match = [
                convert_path(p.strip()) for p in self.match.split(',')
            ]
        self.set_undefined_options('bdist', ('dist_dir', 'dist_dir'))

    def run(self):
        self.run_command(""egg_info"")
        from glob import glob

        for pattern in self.match:
            pattern = self.distribution.get_name() + '*' + pattern
            files = glob(os.path.join(self.dist_dir, pattern))
            files = [(os.path.getmtime(f), f) for f in files]
            files.sort()
            files.reverse()

            log.info(""%d file(s) matching %s"", len(files), pattern)
            files = files[self.keep:]
            for (t, f) in files:
                log.info(""Deleting %s"", f)
                if not self.dry_run:
                    if os.path.isdir(f):
                        shutil.rmtree(f)
                    else:
                        os.unlink(f)"
268	donghui	1	"from distutils.util import convert_path
from distutils import log
from distutils.errors import DistutilsOptionError
import os
import shutil

from setuptools.extern import six

from setuptools import Command


class rotate(Command):
    """"""Delete older distributions""""""

    description = ""delete older distributions, keeping N newest files""
    user_options = [
        ('match=', 'm', ""patterns to match (required)""),
        ('dist-dir=', 'd', ""directory where the distributions are""),
        ('keep=', 'k', ""number of matching distributions to keep""),
    ]

    boolean_options = []

    def initialize_options(self):
        self.match = None
        self.dist_dir = None
        self.keep = None

    def finalize_options(self):
        if self.match is None:
            raise DistutilsOptionError(
                ""Must specify one or more (comma-separated) match patterns ""
                ""(e.g. '.zip' or '.egg')""
            )
        if self.keep is None:
            raise DistutilsOptionError(""Must specify number of files to keep"")
        try:
            self.keep = int(self.keep)
        except ValueError:
            raise DistutilsOptionError(""--keep must be an integer"")
        if isinstance(self.match, six.string_types):
            self.match = [
                convert_path(p.strip()) for p in self.match.split(',')
            ]
        self.set_undefined_options('bdist', ('dist_dir', 'dist_dir'))

    def run(self):
        self.run_command(""egg_info"")
        from glob import glob

        for pattern in self.match:
            pattern = self.distribution.get_name() + '*' + pattern
            files = glob(os.path.join(self.dist_dir, pattern))
            files = [(os.path.getmtime(f), f) for f in files]
            files.sort()
            files.reverse()

            log.info(""%d file(s) matching %s"", len(files), pattern)
            files = files[self.keep:]
            for (t, f) in files:
                log.info(""Deleting %s"", f)
                if not self.dry_run:
                    if os.path.isdir(f):
                        shutil.rmtree(f)
                    else:
                        os.unlink(f)"
328	jackson	0	"from esphome.components import fan
import esphome.config_validation as cv
import esphome.codegen as cg
from esphome.const import CONF_OUTPUT_ID, CONF_SPEED_COUNT, CONF_SWITCH_DATAPOINT
from .. import tuya_ns, CONF_TUYA_ID, Tuya

DEPENDENCIES = [""tuya""]

CONF_SPEED_DATAPOINT = ""speed_datapoint""
CONF_OSCILLATION_DATAPOINT = ""oscillation_datapoint""
CONF_DIRECTION_DATAPOINT = ""direction_datapoint""

TuyaFan = tuya_ns.class_(""TuyaFan"", cg.Component, fan.Fan)

CONFIG_SCHEMA = cv.All(
    fan.FAN_SCHEMA.extend(
        {
            cv.GenerateID(CONF_OUTPUT_ID): cv.declare_id(TuyaFan),
            cv.GenerateID(CONF_TUYA_ID): cv.use_id(Tuya),
            cv.Optional(CONF_OSCILLATION_DATAPOINT): cv.uint8_t,
            cv.Optional(CONF_SPEED_DATAPOINT): cv.uint8_t,
            cv.Optional(CONF_SWITCH_DATAPOINT): cv.uint8_t,
            cv.Optional(CONF_DIRECTION_DATAPOINT): cv.uint8_t,
            cv.Optional(CONF_SPEED_COUNT, default=3): cv.int_range(min=1, max=256),
        }
    ).extend(cv.COMPONENT_SCHEMA),
    cv.has_at_least_one_key(CONF_SPEED_DATAPOINT, CONF_SWITCH_DATAPOINT),
)


async def to_code(config):
    parent = await cg.get_variable(config[CONF_TUYA_ID])

    var = cg.new_Pvariable(config[CONF_OUTPUT_ID], parent, config[CONF_SPEED_COUNT])
    await cg.register_component(var, config)
    await fan.register_fan(var, config)

    if CONF_SPEED_DATAPOINT in config:
        cg.add(var.set_speed_id(config[CONF_SPEED_DATAPOINT]))
    if CONF_SWITCH_DATAPOINT in config:
        cg.add(var.set_switch_id(config[CONF_SWITCH_DATAPOINT]))
    if CONF_OSCILLATION_DATAPOINT in config:
        cg.add(var.set_oscillation_id(config[CONF_OSCILLATION_DATAPOINT]))
    if CONF_DIRECTION_DATAPOINT in config:
        cg.add(var.set_direction_id(config[CONF_DIRECTION_DATAPOINT]))"
328	donghui	0	"from esphome.components import fan
import esphome.config_validation as cv
import esphome.codegen as cg
from esphome.const import CONF_OUTPUT_ID, CONF_SPEED_COUNT, CONF_SWITCH_DATAPOINT
from .. import tuya_ns, CONF_TUYA_ID, Tuya

DEPENDENCIES = [""tuya""]

CONF_SPEED_DATAPOINT = ""speed_datapoint""
CONF_OSCILLATION_DATAPOINT = ""oscillation_datapoint""
CONF_DIRECTION_DATAPOINT = ""direction_datapoint""

TuyaFan = tuya_ns.class_(""TuyaFan"", cg.Component, fan.Fan)

CONFIG_SCHEMA = cv.All(
    fan.FAN_SCHEMA.extend(
        {
            cv.GenerateID(CONF_OUTPUT_ID): cv.declare_id(TuyaFan),
            cv.GenerateID(CONF_TUYA_ID): cv.use_id(Tuya),
            cv.Optional(CONF_OSCILLATION_DATAPOINT): cv.uint8_t,
            cv.Optional(CONF_SPEED_DATAPOINT): cv.uint8_t,
            cv.Optional(CONF_SWITCH_DATAPOINT): cv.uint8_t,
            cv.Optional(CONF_DIRECTION_DATAPOINT): cv.uint8_t,
            cv.Optional(CONF_SPEED_COUNT, default=3): cv.int_range(min=1, max=256),
        }
    ).extend(cv.COMPONENT_SCHEMA),
    cv.has_at_least_one_key(CONF_SPEED_DATAPOINT, CONF_SWITCH_DATAPOINT),
)


async def to_code(config):
    parent = await cg.get_variable(config[CONF_TUYA_ID])

    var = cg.new_Pvariable(config[CONF_OUTPUT_ID], parent, config[CONF_SPEED_COUNT])
    await cg.register_component(var, config)
    await fan.register_fan(var, config)

    if CONF_SPEED_DATAPOINT in config:
        cg.add(var.set_speed_id(config[CONF_SPEED_DATAPOINT]))
    if CONF_SWITCH_DATAPOINT in config:
        cg.add(var.set_switch_id(config[CONF_SWITCH_DATAPOINT]))
    if CONF_OSCILLATION_DATAPOINT in config:
        cg.add(var.set_oscillation_id(config[CONF_OSCILLATION_DATAPOINT]))
    if CONF_DIRECTION_DATAPOINT in config:
        cg.add(var.set_direction_id(config[CONF_DIRECTION_DATAPOINT]))"
379	jackson	0	"from typing import Optional

from fastapi import Depends, Request
from fastapi_users import BaseUserManager, FastAPIUsers, IntegerIDMixin
from fastapi_users.authentication import AuthenticationBackend, BearerTransport, CookieTransport, JWTStrategy
from fastapi_users.db import SQLAlchemyUserDatabase

import crud
from app import models
from app.deps import get_db
from .db import User, get_user_db
from .secrets import secrets

class UserManager(IntegerIDMixin, BaseUserManager[User, int]):
    reset_password_token_secret = secrets['SECRET_KEY']
    verification_token_secret = secrets['SECRET_KEY']

    async def on_after_register(self, user: User, request: Optional[Request] = None):
        print(f""User {user.id} has registered."")

    async def on_after_forgot_password(
        self, user: User, token: str, request: Optional[Request] = None
    ):
        print(f""User {user.id} has forgot their password. Reset token: {token}"")

    async def on_after_request_verify(
        self, user: User, token: str, request: Optional[Request] = None
    ):
        print(f""Verification requested for user {user.id}. Verification token: {token}"")


async def get_user_manager(user_db: SQLAlchemyUserDatabase = Depends(get_user_db)):
    yield UserManager(user_db)


bearer_transport = BearerTransport(tokenUrl=""auth/jwt/login"")


def get_jwt_strategy() -> JWTStrategy:
    return JWTStrategy(secret=secrets['SECRET_KEY'], lifetime_seconds=3600)


jwt_auth_backend = AuthenticationBackend(
    name=""jwt"",
    transport=bearer_transport,
    get_strategy=get_jwt_strategy,
)

cookie_transport = CookieTransport(cookie_max_age=3600)

cookie_auth_backend = AuthenticationBackend(
    name=""cookie"",
    transport=cookie_transport,
    get_strategy=get_jwt_strategy,
)

fastapi_users = FastAPIUsers[User, int](get_user_manager, [jwt_auth_backend, cookie_auth_backend])

current_active_user = fastapi_users.current_user(active=True)

async def get_current_profile(user: User = Depends(current_active_user), db = Depends(get_db)):
    profile = await crud.read(_id=user.id, db=db, model=models.Profile)
    return profile"
379	donghui	0	"from typing import Optional

from fastapi import Depends, Request
from fastapi_users import BaseUserManager, FastAPIUsers, IntegerIDMixin
from fastapi_users.authentication import AuthenticationBackend, BearerTransport, CookieTransport, JWTStrategy
from fastapi_users.db import SQLAlchemyUserDatabase

import crud
from app import models
from app.deps import get_db
from .db import User, get_user_db
from .secrets import secrets

class UserManager(IntegerIDMixin, BaseUserManager[User, int]):
    reset_password_token_secret = secrets['SECRET_KEY']
    verification_token_secret = secrets['SECRET_KEY']

    async def on_after_register(self, user: User, request: Optional[Request] = None):
        print(f""User {user.id} has registered."")

    async def on_after_forgot_password(
        self, user: User, token: str, request: Optional[Request] = None
    ):
        print(f""User {user.id} has forgot their password. Reset token: {token}"")

    async def on_after_request_verify(
        self, user: User, token: str, request: Optional[Request] = None
    ):
        print(f""Verification requested for user {user.id}. Verification token: {token}"")


async def get_user_manager(user_db: SQLAlchemyUserDatabase = Depends(get_user_db)):
    yield UserManager(user_db)


bearer_transport = BearerTransport(tokenUrl=""auth/jwt/login"")


def get_jwt_strategy() -> JWTStrategy:
    return JWTStrategy(secret=secrets['SECRET_KEY'], lifetime_seconds=3600)


jwt_auth_backend = AuthenticationBackend(
    name=""jwt"",
    transport=bearer_transport,
    get_strategy=get_jwt_strategy,
)

cookie_transport = CookieTransport(cookie_max_age=3600)

cookie_auth_backend = AuthenticationBackend(
    name=""cookie"",
    transport=cookie_transport,
    get_strategy=get_jwt_strategy,
)

fastapi_users = FastAPIUsers[User, int](get_user_manager, [jwt_auth_backend, cookie_auth_backend])

current_active_user = fastapi_users.current_user(active=True)

async def get_current_profile(user: User = Depends(current_active_user), db = Depends(get_db)):
    profile = await crud.read(_id=user.id, db=db, model=models.Profile)
    return profile"
437	jackson	3	"import sys
import platform


__all__ = ['install', 'NullFinder', 'Protocol']


try:
    from typing import Protocol
except ImportError:  # pragma: no cover
    # Python 3.7 compatibility
    from ..typing_extensions import Protocol  # type: ignore


def install(cls):
    """"""
    Class decorator for installation on sys.meta_path.

    Adds the backport DistributionFinder to sys.meta_path and
    attempts to disable the finder functionality of the stdlib
    DistributionFinder.
    """"""
    sys.meta_path.append(cls())
    disable_stdlib_finder()
    return cls


def disable_stdlib_finder():
    """"""
    Give the backport primacy for discovering path-based distributions
    by monkey-patching the stdlib O_O.

    See #91 for more background for rationale on this sketchy
    behavior.
    """"""

    def matches(finder):
        return getattr(
            finder, '__module__', None
        ) == '_frozen_importlib_external' and hasattr(finder, 'find_distributions')

    for finder in filter(matches, sys.meta_path):  # pragma: nocover
        del finder.find_distributions


class NullFinder:
    """"""
    A ""Finder"" (aka ""MetaClassFinder"") that never finds any modules,
    but may find distributions.
    """"""

    @staticmethod
    def find_spec(*args, **kwargs):
        return None

    # In Python 2, the import system requires finders
    # to have a find_module() method, but this usage
    # is deprecated in Python 3 in favor of find_spec().
    # For the purposes of this finder (i.e. being present
    # on sys.meta_path but having no other import
    # system functionality), the two methods are identical.
    find_module = find_spec


def pypy_partial(val):
    """"""
    Adjust for variable stacklevel on partial under PyPy.

    Workaround for #327.
    """"""
    is_pypy = platform.python_implementation() == 'PyPy'
    return val + is_pypy"
437	donghui	3	"import sys
import platform


__all__ = ['install', 'NullFinder', 'Protocol']


try:
    from typing import Protocol
except ImportError:  # pragma: no cover
    # Python 3.7 compatibility
    from ..typing_extensions import Protocol  # type: ignore


def install(cls):
    """"""
    Class decorator for installation on sys.meta_path.

    Adds the backport DistributionFinder to sys.meta_path and
    attempts to disable the finder functionality of the stdlib
    DistributionFinder.
    """"""
    sys.meta_path.append(cls())
    disable_stdlib_finder()
    return cls


def disable_stdlib_finder():
    """"""
    Give the backport primacy for discovering path-based distributions
    by monkey-patching the stdlib O_O.

    See #91 for more background for rationale on this sketchy
    behavior.
    """"""

    def matches(finder):
        return getattr(
            finder, '__module__', None
        ) == '_frozen_importlib_external' and hasattr(finder, 'find_distributions')

    for finder in filter(matches, sys.meta_path):  # pragma: nocover
        del finder.find_distributions


class NullFinder:
    """"""
    A ""Finder"" (aka ""MetaClassFinder"") that never finds any modules,
    but may find distributions.
    """"""

    @staticmethod
    def find_spec(*args, **kwargs):
        return None

    # In Python 2, the import system requires finders
    # to have a find_module() method, but this usage
    # is deprecated in Python 3 in favor of find_spec().
    # For the purposes of this finder (i.e. being present
    # on sys.meta_path but having no other import
    # system functionality), the two methods are identical.
    find_module = find_spec


def pypy_partial(val):
    """"""
    Adjust for variable stacklevel on partial under PyPy.

    Workaround for #327.
    """"""
    is_pypy = platform.python_implementation() == 'PyPy'
    return val + is_pypy"
466	jackson	1	"import argparse
import unittest
from typing import Any, Dict, Sequence

import torch
from fairseq.models import transformer

from tests.test_roberta import FakeTask


def mk_sample(tok: Sequence[int] = None, batch_size: int = 2) -> Dict[str, Any]:
    if not tok:
        tok = [10, 11, 12, 13, 14, 15, 2]

    batch = torch.stack([torch.tensor(tok, dtype=torch.long)] * batch_size)
    sample = {
        ""net_input"": {
            ""src_tokens"": batch,
            ""prev_output_tokens"": batch,
            ""src_lengths"": torch.tensor(
                [len(tok)] * batch_size, dtype=torch.long, device=batch.device
            ),
        },
        ""target"": batch[:, 1:],
    }
    return sample


def mk_transformer(**extra_args: Any):
    overrides = {
        # Use characteristics dimensions
        ""encoder_embed_dim"": 12,
        ""encoder_ffn_embed_dim"": 14,
        ""decoder_embed_dim"": 12,
        ""decoder_ffn_embed_dim"": 14,
        # Disable dropout so we have comparable tests.
        ""dropout"": 0,
        ""attention_dropout"": 0,
        ""activation_dropout"": 0,
        ""encoder_layerdrop"": 0,
    }
    overrides.update(extra_args)
    # Overrides the defaults from the parser
    args = argparse.Namespace(**overrides)
    transformer.tiny_architecture(args)

    torch.manual_seed(0)
    task = FakeTask(args)
    return transformer.TransformerModel.build_model(args, task)


class TransformerTestCase(unittest.TestCase):
    def test_forward_backward(self):
        model = mk_transformer(encoder_embed_dim=12, decoder_embed_dim=12)
        sample = mk_sample()
        o, _ = model.forward(**sample[""net_input""])
        loss = o.sum()
        loss.backward()

    def test_different_encoder_decoder_embed_dim(self):
        model = mk_transformer(encoder_embed_dim=12, decoder_embed_dim=16)
        sample = mk_sample()
        o, _ = model.forward(**sample[""net_input""])
        loss = o.sum()
        loss.backward()"
466	donghui	1	"import argparse
import unittest
from typing import Any, Dict, Sequence

import torch
from fairseq.models import transformer

from tests.test_roberta import FakeTask


def mk_sample(tok: Sequence[int] = None, batch_size: int = 2) -> Dict[str, Any]:
    if not tok:
        tok = [10, 11, 12, 13, 14, 15, 2]

    batch = torch.stack([torch.tensor(tok, dtype=torch.long)] * batch_size)
    sample = {
        ""net_input"": {
            ""src_tokens"": batch,
            ""prev_output_tokens"": batch,
            ""src_lengths"": torch.tensor(
                [len(tok)] * batch_size, dtype=torch.long, device=batch.device
            ),
        },
        ""target"": batch[:, 1:],
    }
    return sample


def mk_transformer(**extra_args: Any):
    overrides = {
        # Use characteristics dimensions
        ""encoder_embed_dim"": 12,
        ""encoder_ffn_embed_dim"": 14,
        ""decoder_embed_dim"": 12,
        ""decoder_ffn_embed_dim"": 14,
        # Disable dropout so we have comparable tests.
        ""dropout"": 0,
        ""attention_dropout"": 0,
        ""activation_dropout"": 0,
        ""encoder_layerdrop"": 0,
    }
    overrides.update(extra_args)
    # Overrides the defaults from the parser
    args = argparse.Namespace(**overrides)
    transformer.tiny_architecture(args)

    torch.manual_seed(0)
    task = FakeTask(args)
    return transformer.TransformerModel.build_model(args, task)


class TransformerTestCase(unittest.TestCase):
    def test_forward_backward(self):
        model = mk_transformer(encoder_embed_dim=12, decoder_embed_dim=12)
        sample = mk_sample()
        o, _ = model.forward(**sample[""net_input""])
        loss = o.sum()
        loss.backward()

    def test_different_encoder_decoder_embed_dim(self):
        model = mk_transformer(encoder_embed_dim=12, decoder_embed_dim=16)
        sample = mk_sample()
        o, _ = model.forward(**sample[""net_input""])
        loss = o.sum()
        loss.backward()"
452	jackson	3	"import os
import string
import urllib.parse
import urllib.request
from typing import Optional

from .compat import WINDOWS


def get_url_scheme(url):
    # type: (str) -> Optional[str]
    if "":"" not in url:
        return None
    return url.split("":"", 1)[0].lower()


def path_to_url(path):
    # type: (str) -> str
    """"""
    Convert a path to a file: URL.  The path will be made absolute and have
    quoted path parts.
    """"""
    path = os.path.normpath(os.path.abspath(path))
    url = urllib.parse.urljoin(""file:"", urllib.request.pathname2url(path))
    return url


def url_to_path(url):
    # type: (str) -> str
    """"""
    Convert a file: URL to a path.
    """"""
    assert url.startswith(
        ""file:""
    ), f""You can only turn file: urls into filenames (not {url!r})""

    _, netloc, path, _, _ = urllib.parse.urlsplit(url)

    if not netloc or netloc == ""localhost"":
        # According to RFC 8089, same as empty authority.
        netloc = """"
    elif WINDOWS:
        # If we have a UNC path, prepend UNC share notation.
        netloc = ""\\\\"" + netloc
    else:
        raise ValueError(
            f""non-local file URIs are not supported on this platform: {url!r}""
        )

    path = urllib.request.url2pathname(netloc + path)

    # On Windows, urlsplit parses the path as something like ""/C:/Users/foo"".
    # This creates issues for path-related functions like io.open(), so we try
    # to detect and strip the leading slash.
    if (
        WINDOWS
        and not netloc  # Not UNC.
        and len(path) >= 3
        and path[0] == ""/""  # Leading slash to strip.
        and path[1] in string.ascii_letters  # Drive letter.
        and path[2:4] in ("":"", "":/"")  # Colon + end of string, or colon + absolute path.
    ):
        path = path[1:]

    return path"
452	donghui	3	"import os
import string
import urllib.parse
import urllib.request
from typing import Optional

from .compat import WINDOWS


def get_url_scheme(url):
    # type: (str) -> Optional[str]
    if "":"" not in url:
        return None
    return url.split("":"", 1)[0].lower()


def path_to_url(path):
    # type: (str) -> str
    """"""
    Convert a path to a file: URL.  The path will be made absolute and have
    quoted path parts.
    """"""
    path = os.path.normpath(os.path.abspath(path))
    url = urllib.parse.urljoin(""file:"", urllib.request.pathname2url(path))
    return url


def url_to_path(url):
    # type: (str) -> str
    """"""
    Convert a file: URL to a path.
    """"""
    assert url.startswith(
        ""file:""
    ), f""You can only turn file: urls into filenames (not {url!r})""

    _, netloc, path, _, _ = urllib.parse.urlsplit(url)

    if not netloc or netloc == ""localhost"":
        # According to RFC 8089, same as empty authority.
        netloc = """"
    elif WINDOWS:
        # If we have a UNC path, prepend UNC share notation.
        netloc = ""\\\\"" + netloc
    else:
        raise ValueError(
            f""non-local file URIs are not supported on this platform: {url!r}""
        )

    path = urllib.request.url2pathname(netloc + path)

    # On Windows, urlsplit parses the path as something like ""/C:/Users/foo"".
    # This creates issues for path-related functions like io.open(), so we try
    # to detect and strip the leading slash.
    if (
        WINDOWS
        and not netloc  # Not UNC.
        and len(path) >= 3
        and path[0] == ""/""  # Leading slash to strip.
        and path[1] in string.ascii_letters  # Drive letter.
        and path[2:4] in ("":"", "":/"")  # Colon + end of string, or colon + absolute path.
    ):
        path = path[1:]

    return path"
512	jackson	0	"# -*- coding: utf-8 -*-

# Copyright 2010 Dirk Holtwick, holtwick.it
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import six
import logging


from xhtml2pdf.util import pisaTempFile, getFile, PyPDF2


log = logging.getLogger(""xhtml2pdf"")


class pisaPDF:
    def __init__(self, capacity=-1):
        self.capacity = capacity
        self.files = []

    def addFromURI(self, url, basepath=None):
        obj = getFile(url, basepath)
        if obj and (not obj.notFound()):
            self.files.append(obj.getFile())

    addFromFileName = addFromURI

    def addFromFile(self, f):
        if hasattr(f, ""read""):
            self.files.append(f)
        else:
            self.addFromURI(f)

    def addFromString(self, data):
        self.files.append(pisaTempFile(data, capacity=self.capacity))

    def addDocument(self, doc):
        if hasattr(doc.dest, ""read""):
            self.files.append(doc.dest)

    def join(self, file=None):
        output = PyPDF2.PdfFileWriter()
        for pdffile in self.files:
            input = PyPDF2.PdfFileReader(pdffile)
            for pageNumber in six.moves.range(input.getNumPages()):
                output.addPage(input.getPage(pageNumber))

        if file is not None:
            output.write(file)
            return file
        out = pisaTempFile(capacity=self.capacity)
        output.write(out)
        return out.getvalue()

    getvalue = join
    __str__ = join"
512	donghui	0	"# -*- coding: utf-8 -*-

# Copyright 2010 Dirk Holtwick, holtwick.it
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import six
import logging


from xhtml2pdf.util import pisaTempFile, getFile, PyPDF2


log = logging.getLogger(""xhtml2pdf"")


class pisaPDF:
    def __init__(self, capacity=-1):
        self.capacity = capacity
        self.files = []

    def addFromURI(self, url, basepath=None):
        obj = getFile(url, basepath)
        if obj and (not obj.notFound()):
            self.files.append(obj.getFile())

    addFromFileName = addFromURI

    def addFromFile(self, f):
        if hasattr(f, ""read""):
            self.files.append(f)
        else:
            self.addFromURI(f)

    def addFromString(self, data):
        self.files.append(pisaTempFile(data, capacity=self.capacity))

    def addDocument(self, doc):
        if hasattr(doc.dest, ""read""):
            self.files.append(doc.dest)

    def join(self, file=None):
        output = PyPDF2.PdfFileWriter()
        for pdffile in self.files:
            input = PyPDF2.PdfFileReader(pdffile)
            for pageNumber in six.moves.range(input.getNumPages()):
                output.addPage(input.getPage(pageNumber))

        if file is not None:
            output.write(file)
            return file
        out = pisaTempFile(capacity=self.capacity)
        output.write(out)
        return out.getvalue()

    getvalue = join
    __str__ = join"
403	jackson	2	"import _plotly_utils.basevalidators


class HoverlabelValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(self, plotly_name=""hoverlabel"", parent_name=""box"", **kwargs):
        super(HoverlabelValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Hoverlabel""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            align
                Sets the horizontal alignment of the text
                content within hover label box. Has an effect
                only if the hover label text spans more two or
                more lines
            alignsrc
                Sets the source reference on Chart Studio Cloud
                for `align`.
            bgcolor
                Sets the background color of the hover labels
                for this trace
            bgcolorsrc
                Sets the source reference on Chart Studio Cloud
                for `bgcolor`.
            bordercolor
                Sets the border color of the hover labels for
                this trace.
            bordercolorsrc
                Sets the source reference on Chart Studio Cloud
                for `bordercolor`.
            font
                Sets the font used in hover labels.
            namelength
                Sets the default length (in number of
                characters) of the trace name in the hover
                labels for all traces. -1 shows the whole name
                regardless of length. 0-3 shows the first 0-3
                characters, and an integer >3 will show the
                whole name if it is less than that many
                characters, but if it is longer, will truncate
                to `namelength - 3` characters and add an
                ellipsis.
            namelengthsrc
                Sets the source reference on Chart Studio Cloud
                for `namelength`.
"""""",
            ),
            **kwargs,
        )"
403	donghui	1	"import _plotly_utils.basevalidators


class HoverlabelValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(self, plotly_name=""hoverlabel"", parent_name=""box"", **kwargs):
        super(HoverlabelValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Hoverlabel""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            align
                Sets the horizontal alignment of the text
                content within hover label box. Has an effect
                only if the hover label text spans more two or
                more lines
            alignsrc
                Sets the source reference on Chart Studio Cloud
                for `align`.
            bgcolor
                Sets the background color of the hover labels
                for this trace
            bgcolorsrc
                Sets the source reference on Chart Studio Cloud
                for `bgcolor`.
            bordercolor
                Sets the border color of the hover labels for
                this trace.
            bordercolorsrc
                Sets the source reference on Chart Studio Cloud
                for `bordercolor`.
            font
                Sets the font used in hover labels.
            namelength
                Sets the default length (in number of
                characters) of the trace name in the hover
                labels for all traces. -1 shows the whole name
                regardless of length. 0-3 shows the first 0-3
                characters, and an integer >3 will show the
                whole name if it is less than that many
                characters, but if it is longer, will truncate
                to `namelength - 3` characters and add an
                ellipsis.
            namelengthsrc
                Sets the source reference on Chart Studio Cloud
                for `namelength`.
"""""",
            ),
            **kwargs,
        )"
280	jackson	3	"from django.urls import get_script_prefix, resolve


def get_breadcrumbs(url, request=None):
    """"""
    Given a url returns a list of breadcrumbs, which are each a
    tuple of (name, url).
    """"""
    from rest_framework.reverse import preserve_builtin_query_params
    from rest_framework.views import APIView

    def breadcrumbs_recursive(url, breadcrumbs_list, prefix, seen):
        """"""
        Add tuples of (name, url) to the breadcrumbs list,
        progressively chomping off parts of the url.
        """"""
        try:
            (view, unused_args, unused_kwargs) = resolve(url)
        except Exception:
            pass
        else:
            # Check if this is a REST framework view,
            # and if so add it to the breadcrumbs
            cls = getattr(view, ""cls"", None)
            initkwargs = getattr(view, ""initkwargs"", {})
            if cls is not None and issubclass(cls, APIView):
                # Don't list the same view twice in a row.
                # Probably an optional trailing slash.
                if not seen or seen[-1] != view:
                    c = cls(**initkwargs)
                    name = c.get_view_name()
                    insert_url = preserve_builtin_query_params(prefix + url, request)
                    breadcrumbs_list.insert(0, (name, insert_url))
                    seen.append(view)

        if url == """":
            # All done
            return breadcrumbs_list

        elif url.endswith(""/""):
            # Drop trailing slash off the end and continue to try to
            # resolve more breadcrumbs
            url = url.rstrip(""/"")
            return breadcrumbs_recursive(url, breadcrumbs_list, prefix, seen)

        # Drop trailing non-slash off the end and continue to try to
        # resolve more breadcrumbs
        url = url[: url.rfind(""/"") + 1]
        return breadcrumbs_recursive(url, breadcrumbs_list, prefix, seen)

    prefix = get_script_prefix().rstrip(""/"")
    url = url[len(prefix) :]
    return breadcrumbs_recursive(url, [], prefix, [])"
280	donghui	3	"from django.urls import get_script_prefix, resolve


def get_breadcrumbs(url, request=None):
    """"""
    Given a url returns a list of breadcrumbs, which are each a
    tuple of (name, url).
    """"""
    from rest_framework.reverse import preserve_builtin_query_params
    from rest_framework.views import APIView

    def breadcrumbs_recursive(url, breadcrumbs_list, prefix, seen):
        """"""
        Add tuples of (name, url) to the breadcrumbs list,
        progressively chomping off parts of the url.
        """"""
        try:
            (view, unused_args, unused_kwargs) = resolve(url)
        except Exception:
            pass
        else:
            # Check if this is a REST framework view,
            # and if so add it to the breadcrumbs
            cls = getattr(view, ""cls"", None)
            initkwargs = getattr(view, ""initkwargs"", {})
            if cls is not None and issubclass(cls, APIView):
                # Don't list the same view twice in a row.
                # Probably an optional trailing slash.
                if not seen or seen[-1] != view:
                    c = cls(**initkwargs)
                    name = c.get_view_name()
                    insert_url = preserve_builtin_query_params(prefix + url, request)
                    breadcrumbs_list.insert(0, (name, insert_url))
                    seen.append(view)

        if url == """":
            # All done
            return breadcrumbs_list

        elif url.endswith(""/""):
            # Drop trailing slash off the end and continue to try to
            # resolve more breadcrumbs
            url = url.rstrip(""/"")
            return breadcrumbs_recursive(url, breadcrumbs_list, prefix, seen)

        # Drop trailing non-slash off the end and continue to try to
        # resolve more breadcrumbs
        url = url[: url.rfind(""/"") + 1]
        return breadcrumbs_recursive(url, breadcrumbs_list, prefix, seen)

    prefix = get_script_prefix().rstrip(""/"")
    url = url[len(prefix) :]
    return breadcrumbs_recursive(url, [], prefix, [])"
391	jackson	1	"#
# The Python Imaging Library.
# $Id$
#
# XV Thumbnail file handler by Charles E. ""Gene"" Cash
# (gcash@magicnet.net)
#
# see xvcolor.c and xvbrowse.c in the sources to John Bradley's XV,
# available from ftp://ftp.cis.upenn.edu/pub/xv/
#
# history:
# 98-08-15 cec  created (b/w only)
# 98-12-09 cec  added color palette
# 98-12-28 fl   added to PIL (with only a few very minor modifications)
#
# To do:
# FIXME: make save work (this requires quantization support)
#

from . import Image, ImageFile, ImagePalette
from ._binary import o8

_MAGIC = b""P7 332""

# standard color palette for thumbnails (RGB332)
PALETTE = b""""
for r in range(8):
    for g in range(8):
        for b in range(4):
            PALETTE = PALETTE + (
                o8((r * 255) // 7) + o8((g * 255) // 7) + o8((b * 255) // 3)
            )


def _accept(prefix):
    return prefix[:6] == _MAGIC


##
# Image plugin for XV thumbnail images.


class XVThumbImageFile(ImageFile.ImageFile):

    format = ""XVThumb""
    format_description = ""XV thumbnail image""

    def _open(self):

        # check magic
        if not _accept(self.fp.read(6)):
            msg = ""not an XV thumbnail file""
            raise SyntaxError(msg)

        # Skip to beginning of next line
        self.fp.readline()

        # skip info comments
        while True:
            s = self.fp.readline()
            if not s:
                msg = ""Unexpected EOF reading XV thumbnail file""
                raise SyntaxError(msg)
            if s[0] != 35:  # ie. when not a comment: '#'
                break

        # parse header line (already read)
        s = s.strip().split()

        self.mode = ""P""
        self._size = int(s[0]), int(s[1])

        self.palette = ImagePalette.raw(""RGB"", PALETTE)

        self.tile = [(""raw"", (0, 0) + self.size, self.fp.tell(), (self.mode, 0, 1))]


# --------------------------------------------------------------------

Image.register_open(XVThumbImageFile.format, XVThumbImageFile, _accept)"
391	donghui	1	"#
# The Python Imaging Library.
# $Id$
#
# XV Thumbnail file handler by Charles E. ""Gene"" Cash
# (gcash@magicnet.net)
#
# see xvcolor.c and xvbrowse.c in the sources to John Bradley's XV,
# available from ftp://ftp.cis.upenn.edu/pub/xv/
#
# history:
# 98-08-15 cec  created (b/w only)
# 98-12-09 cec  added color palette
# 98-12-28 fl   added to PIL (with only a few very minor modifications)
#
# To do:
# FIXME: make save work (this requires quantization support)
#

from . import Image, ImageFile, ImagePalette
from ._binary import o8

_MAGIC = b""P7 332""

# standard color palette for thumbnails (RGB332)
PALETTE = b""""
for r in range(8):
    for g in range(8):
        for b in range(4):
            PALETTE = PALETTE + (
                o8((r * 255) // 7) + o8((g * 255) // 7) + o8((b * 255) // 3)
            )


def _accept(prefix):
    return prefix[:6] == _MAGIC


##
# Image plugin for XV thumbnail images.


class XVThumbImageFile(ImageFile.ImageFile):

    format = ""XVThumb""
    format_description = ""XV thumbnail image""

    def _open(self):

        # check magic
        if not _accept(self.fp.read(6)):
            msg = ""not an XV thumbnail file""
            raise SyntaxError(msg)

        # Skip to beginning of next line
        self.fp.readline()

        # skip info comments
        while True:
            s = self.fp.readline()
            if not s:
                msg = ""Unexpected EOF reading XV thumbnail file""
                raise SyntaxError(msg)
            if s[0] != 35:  # ie. when not a comment: '#'
                break

        # parse header line (already read)
        s = s.strip().split()

        self.mode = ""P""
        self._size = int(s[0]), int(s[1])

        self.palette = ImagePalette.raw(""RGB"", PALETTE)

        self.tile = [(""raw"", (0, 0) + self.size, self.fp.tell(), (self.mode, 0, 1))]


# --------------------------------------------------------------------

Image.register_open(XVThumbImageFile.format, XVThumbImageFile, _accept)"
362	jackson	3	"import pytest
import pytest_asyncio

from rtsu_students_bot.rtsu import RTSUApi

pytest_plugins = ('pytest_asyncio',)

TEST_DATA = {
    ""login"": ""your login"",
    ""password"": ""your pass"",
}


@pytest_asyncio.fixture()
async def rtsu_client():
    """"""
    Initializes client
    :return: Prepared `RTSUApi` client
    """"""

    async with RTSUApi() as api:
        yield api


@pytest.mark.asyncio
async def test_rtsu_login(rtsu_client: RTSUApi):
    """"""
    Tests rtsu login
    :param rtsu_client: A RTSU API client
    :return:
    """"""

    resp = await rtsu_client.auth(TEST_DATA.get(""login""), TEST_DATA.get(""password""))

    assert resp.token is not None


@pytest.mark.asyncio
async def test_rtsu_profile_fetching(rtsu_client: RTSUApi):
    """"""
    Tests rtsu profile fetching
    :param rtsu_client:
    :return:
    """"""

    await rtsu_client.auth(TEST_DATA.get(""login""), TEST_DATA.get(""password""))

    profile = await rtsu_client.get_profile()

    assert profile is not None
    assert profile.full_name is not None


@pytest.mark.asyncio
async def test_rtsu_academic_years_fetching(rtsu_client: RTSUApi):
    """"""
    Tests rtsu academic years fetching
    :param rtsu_client:
    :return:
    """"""

    await rtsu_client.auth(TEST_DATA.get(""login""), TEST_DATA.get(""password""))

    years = await rtsu_client.get_academic_years()

    assert type(years) == list
    assert len(years) > 0


@pytest.mark.asyncio
async def test_rtsu_academic_year_subjects_fetching(rtsu_client: RTSUApi):
    """"""
    Tests rtsu academic year fetching
    :param rtsu_client:
    :return:
    """"""

    await rtsu_client.auth(TEST_DATA.get(""login""), TEST_DATA.get(""password""))

    ac_years = await rtsu_client.get_academic_years()
    year = ac_years[0].id
    years = await rtsu_client.get_academic_year_subjects(year)

    assert type(years) == list
    assert len(years) > 0"
362	donghui	2	"import pytest
import pytest_asyncio

from rtsu_students_bot.rtsu import RTSUApi

pytest_plugins = ('pytest_asyncio',)

TEST_DATA = {
    ""login"": ""your login"",
    ""password"": ""your pass"",
}


@pytest_asyncio.fixture()
async def rtsu_client():
    """"""
    Initializes client
    :return: Prepared `RTSUApi` client
    """"""

    async with RTSUApi() as api:
        yield api


@pytest.mark.asyncio
async def test_rtsu_login(rtsu_client: RTSUApi):
    """"""
    Tests rtsu login
    :param rtsu_client: A RTSU API client
    :return:
    """"""

    resp = await rtsu_client.auth(TEST_DATA.get(""login""), TEST_DATA.get(""password""))

    assert resp.token is not None


@pytest.mark.asyncio
async def test_rtsu_profile_fetching(rtsu_client: RTSUApi):
    """"""
    Tests rtsu profile fetching
    :param rtsu_client:
    :return:
    """"""

    await rtsu_client.auth(TEST_DATA.get(""login""), TEST_DATA.get(""password""))

    profile = await rtsu_client.get_profile()

    assert profile is not None
    assert profile.full_name is not None


@pytest.mark.asyncio
async def test_rtsu_academic_years_fetching(rtsu_client: RTSUApi):
    """"""
    Tests rtsu academic years fetching
    :param rtsu_client:
    :return:
    """"""

    await rtsu_client.auth(TEST_DATA.get(""login""), TEST_DATA.get(""password""))

    years = await rtsu_client.get_academic_years()

    assert type(years) == list
    assert len(years) > 0


@pytest.mark.asyncio
async def test_rtsu_academic_year_subjects_fetching(rtsu_client: RTSUApi):
    """"""
    Tests rtsu academic year fetching
    :param rtsu_client:
    :return:
    """"""

    await rtsu_client.auth(TEST_DATA.get(""login""), TEST_DATA.get(""password""))

    ac_years = await rtsu_client.get_academic_years()
    year = ac_years[0].id
    years = await rtsu_client.get_academic_year_subjects(year)

    assert type(years) == list
    assert len(years) > 0"
333	jackson	0	"# This file is dual licensed under the terms of the Apache License, Version
# 2.0, and the BSD License. See the LICENSE file in the root of this repository
# for complete details.


import typing

from cryptography import utils
from cryptography.exceptions import (
    AlreadyFinalized,
    InvalidKey,
    UnsupportedAlgorithm,
    _Reasons,
)
from cryptography.hazmat.primitives import constant_time, hashes
from cryptography.hazmat.primitives.kdf import KeyDerivationFunction


class PBKDF2HMAC(KeyDerivationFunction):
    def __init__(
        self,
        algorithm: hashes.HashAlgorithm,
        length: int,
        salt: bytes,
        iterations: int,
        backend: typing.Any = None,
    ):
        from cryptography.hazmat.backends.openssl.backend import (
            backend as ossl,
        )

        if not ossl.pbkdf2_hmac_supported(algorithm):
            raise UnsupportedAlgorithm(
                ""{} is not supported for PBKDF2 by this backend."".format(
                    algorithm.name
                ),
                _Reasons.UNSUPPORTED_HASH,
            )
        self._used = False
        self._algorithm = algorithm
        self._length = length
        utils._check_bytes(""salt"", salt)
        self._salt = salt
        self._iterations = iterations

    def derive(self, key_material: bytes) -> bytes:
        if self._used:
            raise AlreadyFinalized(""PBKDF2 instances can only be used once."")
        self._used = True

        utils._check_byteslike(""key_material"", key_material)
        from cryptography.hazmat.backends.openssl.backend import backend

        return backend.derive_pbkdf2_hmac(
            self._algorithm,
            self._length,
            self._salt,
            self._iterations,
            key_material,
        )

    def verify(self, key_material: bytes, expected_key: bytes) -> None:
        derived_key = self.derive(key_material)
        if not constant_time.bytes_eq(derived_key, expected_key):
            raise InvalidKey(""Keys do not match."")"
333	donghui	0	"# This file is dual licensed under the terms of the Apache License, Version
# 2.0, and the BSD License. See the LICENSE file in the root of this repository
# for complete details.


import typing

from cryptography import utils
from cryptography.exceptions import (
    AlreadyFinalized,
    InvalidKey,
    UnsupportedAlgorithm,
    _Reasons,
)
from cryptography.hazmat.primitives import constant_time, hashes
from cryptography.hazmat.primitives.kdf import KeyDerivationFunction


class PBKDF2HMAC(KeyDerivationFunction):
    def __init__(
        self,
        algorithm: hashes.HashAlgorithm,
        length: int,
        salt: bytes,
        iterations: int,
        backend: typing.Any = None,
    ):
        from cryptography.hazmat.backends.openssl.backend import (
            backend as ossl,
        )

        if not ossl.pbkdf2_hmac_supported(algorithm):
            raise UnsupportedAlgorithm(
                ""{} is not supported for PBKDF2 by this backend."".format(
                    algorithm.name
                ),
                _Reasons.UNSUPPORTED_HASH,
            )
        self._used = False
        self._algorithm = algorithm
        self._length = length
        utils._check_bytes(""salt"", salt)
        self._salt = salt
        self._iterations = iterations

    def derive(self, key_material: bytes) -> bytes:
        if self._used:
            raise AlreadyFinalized(""PBKDF2 instances can only be used once."")
        self._used = True

        utils._check_byteslike(""key_material"", key_material)
        from cryptography.hazmat.backends.openssl.backend import backend

        return backend.derive_pbkdf2_hmac(
            self._algorithm,
            self._length,
            self._salt,
            self._iterations,
            key_material,
        )

    def verify(self, key_material: bytes, expected_key: bytes) -> None:
        derived_key = self.derive(key_material)
        if not constant_time.bytes_eq(derived_key, expected_key):
            raise InvalidKey(""Keys do not match."")"
273	jackson	0	"# Copyright (C) 2017-2023 The Sipwise Team - http://sipwise.com
#
# This program is free software: you can redistribute it and/or modify it
# under the terms of the GNU General Public License as published by the Free
# Software Foundation, either version 3 of the License, or (at your option)
# any later version.
#
# This program is distributed in the hope that it will be useful, but WITHOUT
# ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
# FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
# more details.
#
# You should have received a copy of the GNU General Public License along
# with this program.  If not, see <http://www.gnu.org/licenses/>.
from django.contrib import admin
from import_export import resources
from import_export.admin import ExportActionModelAdmin
from import_export.admin import ImportExportModelAdmin

from . import models


class BuildReleaseResource(resources.ModelResource):
    class Meta:
        model = models.BuildRelease


@admin.register(models.BuildRelease)
class BuildReleaseAdmin(ImportExportModelAdmin, ExportActionModelAdmin):
    resource_class = BuildReleaseResource
    list_filter = (""release"",)
    readonly_fields = (
        ""projects"",
        ""triggered_projects"",
        ""built_projects"",
        ""failed_projects"",
        ""pool_size"",
        ""triggered_jobs"",
        ""build_deps"",
    )
    modify_readonly_fields = (
        ""uuid"",
        ""release"",
    ) + readonly_fields

    def get_readonly_fields(self, request, obj=None):
        if obj is None:
            return self.readonly_fields
        return self.modify_readonly_fields

    def save_model(self, request, obj, form, change):
        if change:
            super(BuildReleaseAdmin, self).save_model(
                request, obj, form, change
            )
        else:
            new_obj = models.BuildRelease.objects.create_build_release(
                uuid=obj.uuid, release=obj.release
            )
            obj.pk = new_obj.pk"
273	donghui	0	"# Copyright (C) 2017-2023 The Sipwise Team - http://sipwise.com
#
# This program is free software: you can redistribute it and/or modify it
# under the terms of the GNU General Public License as published by the Free
# Software Foundation, either version 3 of the License, or (at your option)
# any later version.
#
# This program is distributed in the hope that it will be useful, but WITHOUT
# ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
# FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
# more details.
#
# You should have received a copy of the GNU General Public License along
# with this program.  If not, see <http://www.gnu.org/licenses/>.
from django.contrib import admin
from import_export import resources
from import_export.admin import ExportActionModelAdmin
from import_export.admin import ImportExportModelAdmin

from . import models


class BuildReleaseResource(resources.ModelResource):
    class Meta:
        model = models.BuildRelease


@admin.register(models.BuildRelease)
class BuildReleaseAdmin(ImportExportModelAdmin, ExportActionModelAdmin):
    resource_class = BuildReleaseResource
    list_filter = (""release"",)
    readonly_fields = (
        ""projects"",
        ""triggered_projects"",
        ""built_projects"",
        ""failed_projects"",
        ""pool_size"",
        ""triggered_jobs"",
        ""build_deps"",
    )
    modify_readonly_fields = (
        ""uuid"",
        ""release"",
    ) + readonly_fields

    def get_readonly_fields(self, request, obj=None):
        if obj is None:
            return self.readonly_fields
        return self.modify_readonly_fields

    def save_model(self, request, obj, form, change):
        if change:
            super(BuildReleaseAdmin, self).save_model(
                request, obj, form, change
            )
        else:
            new_obj = models.BuildRelease.objects.create_build_release(
                uuid=obj.uuid, release=obj.release
            )
            obj.pk = new_obj.pk"
307	jackson	0	"CONSOLE_HTML_FORMAT = """"""\
<!DOCTYPE html>
<head>
<meta charset=""UTF-8"">
<style>
{stylesheet}
body {{
    color: {foreground};
    background-color: {background};
}}
</style>
</head>
<html>
<body>
    <pre style=""font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace""><code>{code}</code></pre>
</body>
</html>
""""""

CONSOLE_SVG_FORMAT = """"""\
<svg class=""rich-terminal"" viewBox=""0 0 {width} {height}"" xmlns=""http://www.w3.org/2000/svg"">
    <!-- Generated with Rich https://www.textualize.io -->
    <style>

    @font-face {{
        font-family: ""Fira Code"";
        src: local(""FiraCode-Regular""),
                url(""https://cdnjs.cloudflare.com/ajax/libs/firacode/6.2.0/woff2/FiraCode-Regular.woff2"") format(""woff2""),
                url(""https://cdnjs.cloudflare.com/ajax/libs/firacode/6.2.0/woff/FiraCode-Regular.woff"") format(""woff"");
        font-style: normal;
        font-weight: 400;
    }}
    @font-face {{
        font-family: ""Fira Code"";
        src: local(""FiraCode-Bold""),
                url(""https://cdnjs.cloudflare.com/ajax/libs/firacode/6.2.0/woff2/FiraCode-Bold.woff2"") format(""woff2""),
                url(""https://cdnjs.cloudflare.com/ajax/libs/firacode/6.2.0/woff/FiraCode-Bold.woff"") format(""woff"");
        font-style: bold;
        font-weight: 700;
    }}

    .{unique_id}-matrix {{
        font-family: Fira Code, monospace;
        font-size: {char_height}px;
        line-height: {line_height}px;
        font-variant-east-asian: full-width;
    }}

    .{unique_id}-title {{
        font-size: 18px;
        font-weight: bold;
        font-family: arial;
    }}

    {styles}
    </style>

    <defs>
    <clipPath id=""{unique_id}-clip-terminal"">
      <rect x=""0"" y=""0"" width=""{terminal_width}"" height=""{terminal_height}"" />
    </clipPath>
    {lines}
    </defs>

    {chrome}
    <g transform=""translate({terminal_x}, {terminal_y})"" clip-path=""url(#{unique_id}-clip-terminal)"">
    {backgrounds}
    <g class=""{unique_id}-matrix"">
    {matrix}
    </g>
    </g>
</svg>
""""""

_SVG_FONT_FAMILY = ""Rich Fira Code""
_SVG_CLASSES_PREFIX = ""rich-svg"""
307	donghui	0	"CONSOLE_HTML_FORMAT = """"""\
<!DOCTYPE html>
<head>
<meta charset=""UTF-8"">
<style>
{stylesheet}
body {{
    color: {foreground};
    background-color: {background};
}}
</style>
</head>
<html>
<body>
    <pre style=""font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace""><code>{code}</code></pre>
</body>
</html>
""""""

CONSOLE_SVG_FORMAT = """"""\
<svg class=""rich-terminal"" viewBox=""0 0 {width} {height}"" xmlns=""http://www.w3.org/2000/svg"">
    <!-- Generated with Rich https://www.textualize.io -->
    <style>

    @font-face {{
        font-family: ""Fira Code"";
        src: local(""FiraCode-Regular""),
                url(""https://cdnjs.cloudflare.com/ajax/libs/firacode/6.2.0/woff2/FiraCode-Regular.woff2"") format(""woff2""),
                url(""https://cdnjs.cloudflare.com/ajax/libs/firacode/6.2.0/woff/FiraCode-Regular.woff"") format(""woff"");
        font-style: normal;
        font-weight: 400;
    }}
    @font-face {{
        font-family: ""Fira Code"";
        src: local(""FiraCode-Bold""),
                url(""https://cdnjs.cloudflare.com/ajax/libs/firacode/6.2.0/woff2/FiraCode-Bold.woff2"") format(""woff2""),
                url(""https://cdnjs.cloudflare.com/ajax/libs/firacode/6.2.0/woff/FiraCode-Bold.woff"") format(""woff"");
        font-style: bold;
        font-weight: 700;
    }}

    .{unique_id}-matrix {{
        font-family: Fira Code, monospace;
        font-size: {char_height}px;
        line-height: {line_height}px;
        font-variant-east-asian: full-width;
    }}

    .{unique_id}-title {{
        font-size: 18px;
        font-weight: bold;
        font-family: arial;
    }}

    {styles}
    </style>

    <defs>
    <clipPath id=""{unique_id}-clip-terminal"">
      <rect x=""0"" y=""0"" width=""{terminal_width}"" height=""{terminal_height}"" />
    </clipPath>
    {lines}
    </defs>

    {chrome}
    <g transform=""translate({terminal_x}, {terminal_y})"" clip-path=""url(#{unique_id}-clip-terminal)"">
    {backgrounds}
    <g class=""{unique_id}-matrix"">
    {matrix}
    </g>
    </g>
</svg>
""""""

_SVG_FONT_FAMILY = ""Rich Fira Code""
_SVG_CLASSES_PREFIX = ""rich-svg"""
356	jackson	1	"#
# This file is part of pyasn1-modules software.
#
# Created by Russ Housley.
#
# Copyright (c) 2019, Vigil Security, LLC
# License: http://snmplabs.com/pyasn1/license.html
#
# RSAES-OAEP Key Transport Algorithm in CMS
#
# Notice that all of the things needed in RFC 3560 are also defined
# in RFC 4055.  So, they are all pulled from the RFC 4055 module into
# this one so that people looking a RFC 3560 can easily find them.
#
# ASN.1 source from:
# https://www.rfc-editor.org/rfc/rfc3560.txt
#

from pyasn1_modules import rfc4055

id_sha1 = rfc4055.id_sha1

id_sha256 = rfc4055.id_sha256

id_sha384 = rfc4055.id_sha384

id_sha512 = rfc4055.id_sha512

id_mgf1 = rfc4055.id_mgf1

rsaEncryption = rfc4055.rsaEncryption

id_RSAES_OAEP = rfc4055.id_RSAES_OAEP

id_pSpecified = rfc4055.id_pSpecified

sha1Identifier = rfc4055.sha1Identifier

sha256Identifier = rfc4055.sha256Identifier

sha384Identifier = rfc4055.sha384Identifier

sha512Identifier = rfc4055.sha512Identifier

mgf1SHA1Identifier = rfc4055.mgf1SHA1Identifier

mgf1SHA256Identifier = rfc4055.mgf1SHA256Identifier

mgf1SHA384Identifier = rfc4055.mgf1SHA384Identifier

mgf1SHA512Identifier = rfc4055.mgf1SHA512Identifier

pSpecifiedEmptyIdentifier = rfc4055.pSpecifiedEmptyIdentifier


class RSAES_OAEP_params(rfc4055.RSAES_OAEP_params):
    pass


rSAES_OAEP_Default_Params = RSAES_OAEP_params()

rSAES_OAEP_Default_Identifier = rfc4055.rSAES_OAEP_Default_Identifier

rSAES_OAEP_SHA256_Params = rfc4055.rSAES_OAEP_SHA256_Params

rSAES_OAEP_SHA256_Identifier = rfc4055.rSAES_OAEP_SHA256_Identifier

rSAES_OAEP_SHA384_Params = rfc4055.rSAES_OAEP_SHA384_Params

rSAES_OAEP_SHA384_Identifier = rfc4055.rSAES_OAEP_SHA384_Identifier

rSAES_OAEP_SHA512_Params = rfc4055.rSAES_OAEP_SHA512_Params

rSAES_OAEP_SHA512_Identifier = rfc4055.rSAES_OAEP_SHA512_Identifier"
356	donghui	0	"#
# This file is part of pyasn1-modules software.
#
# Created by Russ Housley.
#
# Copyright (c) 2019, Vigil Security, LLC
# License: http://snmplabs.com/pyasn1/license.html
#
# RSAES-OAEP Key Transport Algorithm in CMS
#
# Notice that all of the things needed in RFC 3560 are also defined
# in RFC 4055.  So, they are all pulled from the RFC 4055 module into
# this one so that people looking a RFC 3560 can easily find them.
#
# ASN.1 source from:
# https://www.rfc-editor.org/rfc/rfc3560.txt
#

from pyasn1_modules import rfc4055

id_sha1 = rfc4055.id_sha1

id_sha256 = rfc4055.id_sha256

id_sha384 = rfc4055.id_sha384

id_sha512 = rfc4055.id_sha512

id_mgf1 = rfc4055.id_mgf1

rsaEncryption = rfc4055.rsaEncryption

id_RSAES_OAEP = rfc4055.id_RSAES_OAEP

id_pSpecified = rfc4055.id_pSpecified

sha1Identifier = rfc4055.sha1Identifier

sha256Identifier = rfc4055.sha256Identifier

sha384Identifier = rfc4055.sha384Identifier

sha512Identifier = rfc4055.sha512Identifier

mgf1SHA1Identifier = rfc4055.mgf1SHA1Identifier

mgf1SHA256Identifier = rfc4055.mgf1SHA256Identifier

mgf1SHA384Identifier = rfc4055.mgf1SHA384Identifier

mgf1SHA512Identifier = rfc4055.mgf1SHA512Identifier

pSpecifiedEmptyIdentifier = rfc4055.pSpecifiedEmptyIdentifier


class RSAES_OAEP_params(rfc4055.RSAES_OAEP_params):
    pass


rSAES_OAEP_Default_Params = RSAES_OAEP_params()

rSAES_OAEP_Default_Identifier = rfc4055.rSAES_OAEP_Default_Identifier

rSAES_OAEP_SHA256_Params = rfc4055.rSAES_OAEP_SHA256_Params

rSAES_OAEP_SHA256_Identifier = rfc4055.rSAES_OAEP_SHA256_Identifier

rSAES_OAEP_SHA384_Params = rfc4055.rSAES_OAEP_SHA384_Params

rSAES_OAEP_SHA384_Identifier = rfc4055.rSAES_OAEP_SHA384_Identifier

rSAES_OAEP_SHA512_Params = rfc4055.rSAES_OAEP_SHA512_Params

rSAES_OAEP_SHA512_Identifier = rfc4055.rSAES_OAEP_SHA512_Identifier"
418	jackson	4	"#
# Copyright 2011 Facebook
#
# Licensed under the Apache License, Version 2.0 (the ""License""); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

""""""Implementation of platform-specific functionality.

For each function or class described in `tornado.platform.interface`,
the appropriate platform-specific implementation exists in this module.
Most code that needs access to this functionality should do e.g.::

    from tornado.platform.auto import set_close_exec
""""""

from __future__ import absolute_import, division, print_function

import os

if 'APPENGINE_RUNTIME' in os.environ:
    from tornado.platform.common import Waker

    def set_close_exec(fd):
        pass
elif os.name == 'nt':
    from tornado.platform.common import Waker
    from tornado.platform.windows import set_close_exec
else:
    from tornado.platform.posix import set_close_exec, Waker

try:
    # monotime monkey-patches the time module to have a monotonic function
    # in versions of python before 3.3.
    import monotime
    # Silence pyflakes warning about this unused import
    monotime
except ImportError:
    pass
try:
    # monotonic can provide a monotonic function in versions of python before
    # 3.3, too.
    from monotonic import monotonic as monotonic_time
except ImportError:
    try:
        from time import monotonic as monotonic_time
    except ImportError:
        monotonic_time = None

__all__ = ['Waker', 'set_close_exec', 'monotonic_time']"
418	donghui	2	"#
# Copyright 2011 Facebook
#
# Licensed under the Apache License, Version 2.0 (the ""License""); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

""""""Implementation of platform-specific functionality.

For each function or class described in `tornado.platform.interface`,
the appropriate platform-specific implementation exists in this module.
Most code that needs access to this functionality should do e.g.::

    from tornado.platform.auto import set_close_exec
""""""

from __future__ import absolute_import, division, print_function

import os

if 'APPENGINE_RUNTIME' in os.environ:
    from tornado.platform.common import Waker

    def set_close_exec(fd):
        pass
elif os.name == 'nt':
    from tornado.platform.common import Waker
    from tornado.platform.windows import set_close_exec
else:
    from tornado.platform.posix import set_close_exec, Waker

try:
    # monotime monkey-patches the time module to have a monotonic function
    # in versions of python before 3.3.
    import monotime
    # Silence pyflakes warning about this unused import
    monotime
except ImportError:
    pass
try:
    # monotonic can provide a monotonic function in versions of python before
    # 3.3, too.
    from monotonic import monotonic as monotonic_time
except ImportError:
    try:
        from time import monotonic as monotonic_time
    except ImportError:
        monotonic_time = None

__all__ = ['Waker', 'set_close_exec', 'monotonic_time']"
509	jackson	1	"#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# ""License""); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
""""""Example DAG demonstrating the usage of the BranchPythonOperator.""""""
from __future__ import annotations

import random

import pendulum

from airflow import DAG
from airflow.operators.empty import EmptyOperator
from airflow.operators.python import BranchPythonOperator
from airflow.utils.edgemodifier import Label
from airflow.utils.trigger_rule import TriggerRule

with DAG(
    dag_id=""example_branch_operator"",
    start_date=pendulum.datetime(2021, 1, 1, tz=""UTC""),
    catchup=False,
    schedule=""@daily"",
    tags=[""example"", ""example2""],
) as dag:
    run_this_first = EmptyOperator(
        task_id=""run_this_first"",
    )

    options = [""branch_a"", ""branch_b"", ""branch_c"", ""branch_d""]

    branching = BranchPythonOperator(
        task_id=""branching"",
        python_callable=lambda: random.choice(options),
    )
    run_this_first >> branching

    join = EmptyOperator(
        task_id=""join"",
        trigger_rule=TriggerRule.NONE_FAILED_MIN_ONE_SUCCESS,
    )

    for option in options:
        t = EmptyOperator(
            task_id=option,
        )

        empty_follow = EmptyOperator(
            task_id=""follow_"" + option,
        )

        # Label is optional here, but it can help identify more complex branches
        branching >> Label(option) >> t >> empty_follow >> join"
509	donghui	1	"#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# ""License""); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
""""""Example DAG demonstrating the usage of the BranchPythonOperator.""""""
from __future__ import annotations

import random

import pendulum

from airflow import DAG
from airflow.operators.empty import EmptyOperator
from airflow.operators.python import BranchPythonOperator
from airflow.utils.edgemodifier import Label
from airflow.utils.trigger_rule import TriggerRule

with DAG(
    dag_id=""example_branch_operator"",
    start_date=pendulum.datetime(2021, 1, 1, tz=""UTC""),
    catchup=False,
    schedule=""@daily"",
    tags=[""example"", ""example2""],
) as dag:
    run_this_first = EmptyOperator(
        task_id=""run_this_first"",
    )

    options = [""branch_a"", ""branch_b"", ""branch_c"", ""branch_d""]

    branching = BranchPythonOperator(
        task_id=""branching"",
        python_callable=lambda: random.choice(options),
    )
    run_this_first >> branching

    join = EmptyOperator(
        task_id=""join"",
        trigger_rule=TriggerRule.NONE_FAILED_MIN_ONE_SUCCESS,
    )

    for option in options:
        t = EmptyOperator(
            task_id=option,
        )

        empty_follow = EmptyOperator(
            task_id=""follow_"" + option,
        )

        # Label is optional here, but it can help identify more complex branches
        branching >> Label(option) >> t >> empty_follow >> join"
449	jackson	3	"""""""distutils.command.install_scripts

Implements the Distutils 'install_scripts' command, for installing
Python scripts.""""""

# contributed by Bastian Kleineidam

import os
from distutils.core import Command
from distutils import log
from stat import ST_MODE


class install_scripts(Command):
    description = ""install scripts (Python or otherwise)""

    user_options = [
        (""install-dir="", ""d"", ""directory to install scripts to""),
        (""build-dir="", ""b"", ""build directory (where to install from)""),
        (""force"", ""f"", ""force installation (overwrite existing files)""),
        (""skip-build"", None, ""skip the build steps""),
    ]

    boolean_options = [""force"", ""skip-build""]

    def initialize_options(self):
        self.install_dir = None
        self.force = 0
        self.build_dir = None
        self.skip_build = None

    def finalize_options(self):
        self.set_undefined_options(""build"", (""build_scripts"", ""build_dir""))
        self.set_undefined_options(
            ""install"",
            (""install_scripts"", ""install_dir""),
            (""force"", ""force""),
            (""skip_build"", ""skip_build""),
        )

    def run(self):
        if not self.skip_build:
            self.run_command(""build_scripts"")
        self.outfiles = self.copy_tree(self.build_dir, self.install_dir)
        if os.name == ""posix"":
            # Set the executable bits (owner, group, and world) on
            # all the scripts we just installed.
            for file in self.get_outputs():
                if self.dry_run:
                    log.info(""changing mode of %s"", file)
                else:
                    mode = ((os.stat(file)[ST_MODE]) | 0o555) & 0o7777
                    log.info(""changing mode of %s to %o"", file, mode)
                    os.chmod(file, mode)

    def get_inputs(self):
        return self.distribution.scripts or []

    def get_outputs(self):
        return self.outfiles or []"
449	donghui	1	"""""""distutils.command.install_scripts

Implements the Distutils 'install_scripts' command, for installing
Python scripts.""""""

# contributed by Bastian Kleineidam

import os
from distutils.core import Command
from distutils import log
from stat import ST_MODE


class install_scripts(Command):
    description = ""install scripts (Python or otherwise)""

    user_options = [
        (""install-dir="", ""d"", ""directory to install scripts to""),
        (""build-dir="", ""b"", ""build directory (where to install from)""),
        (""force"", ""f"", ""force installation (overwrite existing files)""),
        (""skip-build"", None, ""skip the build steps""),
    ]

    boolean_options = [""force"", ""skip-build""]

    def initialize_options(self):
        self.install_dir = None
        self.force = 0
        self.build_dir = None
        self.skip_build = None

    def finalize_options(self):
        self.set_undefined_options(""build"", (""build_scripts"", ""build_dir""))
        self.set_undefined_options(
            ""install"",
            (""install_scripts"", ""install_dir""),
            (""force"", ""force""),
            (""skip_build"", ""skip_build""),
        )

    def run(self):
        if not self.skip_build:
            self.run_command(""build_scripts"")
        self.outfiles = self.copy_tree(self.build_dir, self.install_dir)
        if os.name == ""posix"":
            # Set the executable bits (owner, group, and world) on
            # all the scripts we just installed.
            for file in self.get_outputs():
                if self.dry_run:
                    log.info(""changing mode of %s"", file)
                else:
                    mode = ((os.stat(file)[ST_MODE]) | 0o555) & 0o7777
                    log.info(""changing mode of %s to %o"", file, mode)
                    os.chmod(file, mode)

    def get_inputs(self):
        return self.distribution.scripts or []

    def get_outputs(self):
        return self.outfiles or []"
459	jackson	1	"# This source code is licensed under the MIT license found in the
# LICENSE file in the root directory of this source tree.

import os
import shutil
import sys
import tempfile
import unittest
from typing import Optional
from unittest.mock import MagicMock


class TestFileIO(unittest.TestCase):

    _tmpdir: Optional[str] = None
    _tmpfile: Optional[str] = None
    _tmpfile_contents = ""Hello, World""

    @classmethod
    def setUpClass(cls) -> None:
        cls._tmpdir = tempfile.mkdtemp()
        with open(os.path.join(cls._tmpdir, ""test.txt""), ""w"") as f:
            cls._tmpfile = f.name
            f.write(cls._tmpfile_contents)
            f.flush()

    @classmethod
    def tearDownClass(cls) -> None:
        # Cleanup temp working dir.
        if cls._tmpdir is not None:
            shutil.rmtree(cls._tmpdir)  # type: ignore

    def test_file_io(self):
        from fairseq.file_io import PathManager

        with PathManager.open(os.path.join(self._tmpdir, ""test.txt""), ""r"") as f:
            s = f.read()
        self.assertEqual(s, self._tmpfile_contents)

    def test_file_io_oss(self):
        # Mock iopath to simulate oss environment.
        sys.modules[""iopath""] = MagicMock()
        from fairseq.file_io import PathManager

        with PathManager.open(os.path.join(self._tmpdir, ""test.txt""), ""r"") as f:
            s = f.read()
        self.assertEqual(s, self._tmpfile_contents)

    def test_file_io_async(self):
        # ioPath `PathManager` is initialized after the first `opena` call.
        try:
            from fairseq.file_io import IOPathManager, PathManager
            _asyncfile = os.path.join(self._tmpdir, ""async.txt"")
            f = PathManager.opena(_asyncfile, ""wb"")
            f.close()

        finally:
            self.assertTrue(PathManager.async_close())"
459	donghui	1	"# This source code is licensed under the MIT license found in the
# LICENSE file in the root directory of this source tree.

import os
import shutil
import sys
import tempfile
import unittest
from typing import Optional
from unittest.mock import MagicMock


class TestFileIO(unittest.TestCase):

    _tmpdir: Optional[str] = None
    _tmpfile: Optional[str] = None
    _tmpfile_contents = ""Hello, World""

    @classmethod
    def setUpClass(cls) -> None:
        cls._tmpdir = tempfile.mkdtemp()
        with open(os.path.join(cls._tmpdir, ""test.txt""), ""w"") as f:
            cls._tmpfile = f.name
            f.write(cls._tmpfile_contents)
            f.flush()

    @classmethod
    def tearDownClass(cls) -> None:
        # Cleanup temp working dir.
        if cls._tmpdir is not None:
            shutil.rmtree(cls._tmpdir)  # type: ignore

    def test_file_io(self):
        from fairseq.file_io import PathManager

        with PathManager.open(os.path.join(self._tmpdir, ""test.txt""), ""r"") as f:
            s = f.read()
        self.assertEqual(s, self._tmpfile_contents)

    def test_file_io_oss(self):
        # Mock iopath to simulate oss environment.
        sys.modules[""iopath""] = MagicMock()
        from fairseq.file_io import PathManager

        with PathManager.open(os.path.join(self._tmpdir, ""test.txt""), ""r"") as f:
            s = f.read()
        self.assertEqual(s, self._tmpfile_contents)

    def test_file_io_async(self):
        # ioPath `PathManager` is initialized after the first `opena` call.
        try:
            from fairseq.file_io import IOPathManager, PathManager
            _asyncfile = os.path.join(self._tmpdir, ""async.txt"")
            f = PathManager.opena(_asyncfile, ""wb"")
            f.close()

        finally:
            self.assertTrue(PathManager.async_close())"
408	jackson	0	"# Copyright Jonathan Hartley 2013. BSD 3-Clause license, see LICENSE file.
import atexit
import contextlib
import sys

from .ansitowin32 import AnsiToWin32


orig_stdout = None
orig_stderr = None

wrapped_stdout = None
wrapped_stderr = None

atexit_done = False


def reset_all():
    if AnsiToWin32 is not None:  # Issue #74: objects might become None at exit
        AnsiToWin32(orig_stdout).reset_all()


def init(autoreset=False, convert=None, strip=None, wrap=True):
    if not wrap and any([autoreset, convert, strip]):
        raise ValueError(""wrap=False conflicts with any other arg=True"")

    global wrapped_stdout, wrapped_stderr
    global orig_stdout, orig_stderr

    orig_stdout = sys.stdout
    orig_stderr = sys.stderr

    if sys.stdout is None:
        wrapped_stdout = None
    else:
        sys.stdout = wrapped_stdout = wrap_stream(
            orig_stdout, convert, strip, autoreset, wrap
        )
    if sys.stderr is None:
        wrapped_stderr = None
    else:
        sys.stderr = wrapped_stderr = wrap_stream(
            orig_stderr, convert, strip, autoreset, wrap
        )

    global atexit_done
    if not atexit_done:
        atexit.register(reset_all)
        atexit_done = True


def deinit():
    if orig_stdout is not None:
        sys.stdout = orig_stdout
    if orig_stderr is not None:
        sys.stderr = orig_stderr


@contextlib.contextmanager
def colorama_text(*args, **kwargs):
    init(*args, **kwargs)
    try:
        yield
    finally:
        deinit()


def reinit():
    if wrapped_stdout is not None:
        sys.stdout = wrapped_stdout
    if wrapped_stderr is not None:
        sys.stderr = wrapped_stderr


def wrap_stream(stream, convert, strip, autoreset, wrap):
    if wrap:
        wrapper = AnsiToWin32(stream, convert=convert, strip=strip, autoreset=autoreset)
        if wrapper.should_wrap():
            stream = wrapper.stream
    return stream"
408	donghui	1	"# Copyright Jonathan Hartley 2013. BSD 3-Clause license, see LICENSE file.
import atexit
import contextlib
import sys

from .ansitowin32 import AnsiToWin32


orig_stdout = None
orig_stderr = None

wrapped_stdout = None
wrapped_stderr = None

atexit_done = False


def reset_all():
    if AnsiToWin32 is not None:  # Issue #74: objects might become None at exit
        AnsiToWin32(orig_stdout).reset_all()


def init(autoreset=False, convert=None, strip=None, wrap=True):
    if not wrap and any([autoreset, convert, strip]):
        raise ValueError(""wrap=False conflicts with any other arg=True"")

    global wrapped_stdout, wrapped_stderr
    global orig_stdout, orig_stderr

    orig_stdout = sys.stdout
    orig_stderr = sys.stderr

    if sys.stdout is None:
        wrapped_stdout = None
    else:
        sys.stdout = wrapped_stdout = wrap_stream(
            orig_stdout, convert, strip, autoreset, wrap
        )
    if sys.stderr is None:
        wrapped_stderr = None
    else:
        sys.stderr = wrapped_stderr = wrap_stream(
            orig_stderr, convert, strip, autoreset, wrap
        )

    global atexit_done
    if not atexit_done:
        atexit.register(reset_all)
        atexit_done = True


def deinit():
    if orig_stdout is not None:
        sys.stdout = orig_stdout
    if orig_stderr is not None:
        sys.stderr = orig_stderr


@contextlib.contextmanager
def colorama_text(*args, **kwargs):
    init(*args, **kwargs)
    try:
        yield
    finally:
        deinit()


def reinit():
    if wrapped_stdout is not None:
        sys.stdout = wrapped_stdout
    if wrapped_stderr is not None:
        sys.stderr = wrapped_stderr


def wrap_stream(stream, convert, strip, autoreset, wrap):
    if wrap:
        wrapper = AnsiToWin32(stream, convert=convert, strip=strip, autoreset=autoreset)
        if wrapper.should_wrap():
            stream = wrapper.stream
    return stream"
346	jackson	1	"from textwrap import dedent

from flaky import flaky

from .test_embed_kernel import setup_kernel

TIMEOUT = 15


@flaky(max_runs=3)
def test_ipython_start_kernel_userns():
    cmd = dedent(
        """"""
        from ipykernel.kernelapp import launch_new_instance
        ns = {""tre"": 123}
        launch_new_instance(user_ns=ns)
        """"""
    )

    with setup_kernel(cmd) as client:
        client.inspect(""tre"")
        msg = client.get_shell_msg(timeout=TIMEOUT)
        content = msg[""content""]
        assert content[""found""]
        text = content[""data""][""text/plain""]
        assert ""123"" in text

        # user_module should be an instance of DummyMod
        client.execute(""usermod = get_ipython().user_module"")
        msg = client.get_shell_msg(timeout=TIMEOUT)
        content = msg[""content""]
        assert content[""status""] == ""ok""
        client.inspect(""usermod"")
        msg = client.get_shell_msg(timeout=TIMEOUT)
        content = msg[""content""]
        assert content[""found""]
        text = content[""data""][""text/plain""]
        assert ""DummyMod"" in text


@flaky(max_runs=3)
def test_ipython_start_kernel_no_userns():
    # Issue #4188 - user_ns should be passed to shell as None, not {}
    cmd = dedent(
        """"""
        from ipykernel.kernelapp import launch_new_instance
        launch_new_instance()
        """"""
    )

    with setup_kernel(cmd) as client:
        # user_module should not be an instance of DummyMod
        client.execute(""usermod = get_ipython().user_module"")
        msg = client.get_shell_msg(timeout=TIMEOUT)
        content = msg[""content""]
        assert content[""status""] == ""ok""
        client.inspect(""usermod"")
        msg = client.get_shell_msg(timeout=TIMEOUT)
        content = msg[""content""]
        assert content[""found""]
        text = content[""data""][""text/plain""]
        assert ""DummyMod"" not in text"
346	donghui	2	"from textwrap import dedent

from flaky import flaky

from .test_embed_kernel import setup_kernel

TIMEOUT = 15


@flaky(max_runs=3)
def test_ipython_start_kernel_userns():
    cmd = dedent(
        """"""
        from ipykernel.kernelapp import launch_new_instance
        ns = {""tre"": 123}
        launch_new_instance(user_ns=ns)
        """"""
    )

    with setup_kernel(cmd) as client:
        client.inspect(""tre"")
        msg = client.get_shell_msg(timeout=TIMEOUT)
        content = msg[""content""]
        assert content[""found""]
        text = content[""data""][""text/plain""]
        assert ""123"" in text

        # user_module should be an instance of DummyMod
        client.execute(""usermod = get_ipython().user_module"")
        msg = client.get_shell_msg(timeout=TIMEOUT)
        content = msg[""content""]
        assert content[""status""] == ""ok""
        client.inspect(""usermod"")
        msg = client.get_shell_msg(timeout=TIMEOUT)
        content = msg[""content""]
        assert content[""found""]
        text = content[""data""][""text/plain""]
        assert ""DummyMod"" in text


@flaky(max_runs=3)
def test_ipython_start_kernel_no_userns():
    # Issue #4188 - user_ns should be passed to shell as None, not {}
    cmd = dedent(
        """"""
        from ipykernel.kernelapp import launch_new_instance
        launch_new_instance()
        """"""
    )

    with setup_kernel(cmd) as client:
        # user_module should not be an instance of DummyMod
        client.execute(""usermod = get_ipython().user_module"")
        msg = client.get_shell_msg(timeout=TIMEOUT)
        content = msg[""content""]
        assert content[""status""] == ""ok""
        client.inspect(""usermod"")
        msg = client.get_shell_msg(timeout=TIMEOUT)
        content = msg[""content""]
        assert content[""found""]
        text = content[""data""][""text/plain""]
        assert ""DummyMod"" not in text"
317	jackson	4	"# This file is distributed under the same license as the Django package.
#
# The *_FORMAT strings use the Django date format syntax,
# see https://docs.djangoproject.com/en/dev/ref/templates/builtins/#date
DATE_FORMAT = r'j\-\a \d\e F Y'         # '26-a de julio 1887'
TIME_FORMAT = 'H:i'                     # '18:59'
DATETIME_FORMAT = r'j\-\a \d\e F Y\, \j\e H:i'  # '26-a de julio 1887, je 18:59'
YEAR_MONTH_FORMAT = r'F \d\e Y'         # 'julio de 1887'
MONTH_DAY_FORMAT = r'j\-\a \d\e F'      # '26-a de julio'
SHORT_DATE_FORMAT = 'Y-m-d'             # '1887-07-26'
SHORT_DATETIME_FORMAT = 'Y-m-d H:i'     # '1887-07-26 18:59'
FIRST_DAY_OF_WEEK = 1  # Monday (lundo)

# The *_INPUT_FORMATS strings use the Python strftime format syntax,
# see https://docs.python.org/library/datetime.html#strftime-strptime-behavior
DATE_INPUT_FORMATS = [
    '%Y-%m-%d',                         # '1887-07-26'
    '%y-%m-%d',                         # '87-07-26'
    '%Y %m %d',                         # '1887 07 26'
    '%Y.%m.%d',                         # '1887.07.26'
    '%d-a de %b %Y',                    # '26-a de jul 1887'
    '%d %b %Y',                         # '26 jul 1887'
    '%d-a de %B %Y',                    # '26-a de julio 1887'
    '%d %B %Y',                         # '26 julio 1887'
    '%d %m %Y',                         # '26 07 1887'
    '%d/%m/%Y',                         # '26/07/1887'
]
TIME_INPUT_FORMATS = [
    '%H:%M:%S',                         # '18:59:00'
    '%H:%M',                            # '18:59'
]
DATETIME_INPUT_FORMATS = [
    '%Y-%m-%d %H:%M:%S',                # '1887-07-26 18:59:00'
    '%Y-%m-%d %H:%M',                   # '1887-07-26 18:59'

    '%Y.%m.%d %H:%M:%S',                # '1887.07.26 18:59:00'
    '%Y.%m.%d %H:%M',                   # '1887.07.26 18:59'

    '%d/%m/%Y %H:%M:%S',                # '26/07/1887 18:59:00'
    '%d/%m/%Y %H:%M',                   # '26/07/1887 18:59'

    '%y-%m-%d %H:%M:%S',                # '87-07-26 18:59:00'
    '%y-%m-%d %H:%M',                   # '87-07-26 18:59'
]
DECIMAL_SEPARATOR = ','
THOUSAND_SEPARATOR = '\xa0'  # non-breaking space
NUMBER_GROUPING = 3"
317	donghui	1	"# This file is distributed under the same license as the Django package.
#
# The *_FORMAT strings use the Django date format syntax,
# see https://docs.djangoproject.com/en/dev/ref/templates/builtins/#date
DATE_FORMAT = r'j\-\a \d\e F Y'         # '26-a de julio 1887'
TIME_FORMAT = 'H:i'                     # '18:59'
DATETIME_FORMAT = r'j\-\a \d\e F Y\, \j\e H:i'  # '26-a de julio 1887, je 18:59'
YEAR_MONTH_FORMAT = r'F \d\e Y'         # 'julio de 1887'
MONTH_DAY_FORMAT = r'j\-\a \d\e F'      # '26-a de julio'
SHORT_DATE_FORMAT = 'Y-m-d'             # '1887-07-26'
SHORT_DATETIME_FORMAT = 'Y-m-d H:i'     # '1887-07-26 18:59'
FIRST_DAY_OF_WEEK = 1  # Monday (lundo)

# The *_INPUT_FORMATS strings use the Python strftime format syntax,
# see https://docs.python.org/library/datetime.html#strftime-strptime-behavior
DATE_INPUT_FORMATS = [
    '%Y-%m-%d',                         # '1887-07-26'
    '%y-%m-%d',                         # '87-07-26'
    '%Y %m %d',                         # '1887 07 26'
    '%Y.%m.%d',                         # '1887.07.26'
    '%d-a de %b %Y',                    # '26-a de jul 1887'
    '%d %b %Y',                         # '26 jul 1887'
    '%d-a de %B %Y',                    # '26-a de julio 1887'
    '%d %B %Y',                         # '26 julio 1887'
    '%d %m %Y',                         # '26 07 1887'
    '%d/%m/%Y',                         # '26/07/1887'
]
TIME_INPUT_FORMATS = [
    '%H:%M:%S',                         # '18:59:00'
    '%H:%M',                            # '18:59'
]
DATETIME_INPUT_FORMATS = [
    '%Y-%m-%d %H:%M:%S',                # '1887-07-26 18:59:00'
    '%Y-%m-%d %H:%M',                   # '1887-07-26 18:59'

    '%Y.%m.%d %H:%M:%S',                # '1887.07.26 18:59:00'
    '%Y.%m.%d %H:%M',                   # '1887.07.26 18:59'

    '%d/%m/%Y %H:%M:%S',                # '26/07/1887 18:59:00'
    '%d/%m/%Y %H:%M',                   # '26/07/1887 18:59'

    '%y-%m-%d %H:%M:%S',                # '87-07-26 18:59:00'
    '%y-%m-%d %H:%M',                   # '87-07-26 18:59'
]
DECIMAL_SEPARATOR = ','
THOUSAND_SEPARATOR = '\xa0'  # non-breaking space
NUMBER_GROUPING = 3"
257	jackson	0	"import json
import openpyxl
from configparser import ConfigParser
from core.infrastructure.constants.data import PROJECT_PATH, CONFIG_PATH


def read_config(key: str, value: str) -> str:
    config = ConfigParser()
    config.read(CONFIG_PATH)
    return config.get(key, value)


def read_json(path: str) -> dict:
    with open(path, 'r', encoding='utf-8') as json_file:
        file = json.load(json_file)
        return file


def write_json(path: str, key: str, value: str) -> None:
    data = read_json(path)
    data[key] = value
    with open(path, 'w', encoding='utf-8') as json_file:
        json.dump(data, json_file)


def read_excel(sheet_name: str, value: str) -> dict[str]:
    path = fr""{PROJECT_PATH}\{read_config('path', 'page_base')}""
    workbook = openpyxl.load_workbook(path)
    sheet = workbook[sheet_name]
    cache = {}
    for row in sheet.iter_rows(min_row=2, values_only=True):
        result = {
            'name': row[0],
            'locator': row[1],
            'type': row[2],
            'image': row[3]
        }
        cache[result['name']] = result
    try:
        match cache[value]['name']:
            case _:
                return {
                    'name': cache[value]['name'],
                    'locator': cache[value]['locator'],
                    'type': cache[value]['type'],
                    'image': cache[value]['image']
                }
    except ValueError:
        raise Exception('no such type')


def get_name(*args: str) -> str:
    return read_excel(*args)['name']


def get_locator(*args: str) -> str:
    return read_excel(*args)['locator']


def get_type(*args: str) -> str:
    return read_excel(*args)['type']


def get_image(*args: str) -> str:
    return read_excel(*args)['image']"
257	donghui	0	"import json
import openpyxl
from configparser import ConfigParser
from core.infrastructure.constants.data import PROJECT_PATH, CONFIG_PATH


def read_config(key: str, value: str) -> str:
    config = ConfigParser()
    config.read(CONFIG_PATH)
    return config.get(key, value)


def read_json(path: str) -> dict:
    with open(path, 'r', encoding='utf-8') as json_file:
        file = json.load(json_file)
        return file


def write_json(path: str, key: str, value: str) -> None:
    data = read_json(path)
    data[key] = value
    with open(path, 'w', encoding='utf-8') as json_file:
        json.dump(data, json_file)


def read_excel(sheet_name: str, value: str) -> dict[str]:
    path = fr""{PROJECT_PATH}\{read_config('path', 'page_base')}""
    workbook = openpyxl.load_workbook(path)
    sheet = workbook[sheet_name]
    cache = {}
    for row in sheet.iter_rows(min_row=2, values_only=True):
        result = {
            'name': row[0],
            'locator': row[1],
            'type': row[2],
            'image': row[3]
        }
        cache[result['name']] = result
    try:
        match cache[value]['name']:
            case _:
                return {
                    'name': cache[value]['name'],
                    'locator': cache[value]['locator'],
                    'type': cache[value]['type'],
                    'image': cache[value]['image']
                }
    except ValueError:
        raise Exception('no such type')


def get_name(*args: str) -> str:
    return read_excel(*args)['name']


def get_locator(*args: str) -> str:
    return read_excel(*args)['locator']


def get_type(*args: str) -> str:
    return read_excel(*args)['type']


def get_image(*args: str) -> str:
    return read_excel(*args)['image']"
263	jackson	0	"from graphql.language.location import SourceLocation
from graphql.validation.rules import LoneAnonymousOperation

from .utils import expect_fails_rule, expect_passes_rule


def anon_not_alone(line, column):
    return {
        ""message"": LoneAnonymousOperation.anonymous_operation_not_alone_message(),
        ""locations"": [SourceLocation(line, column)],
    }


def test_no_operations():
    expect_passes_rule(
        LoneAnonymousOperation,
        """"""
      fragment fragA on Type {
        field
      }
    """""",
    )


def test_one_anon_operation():
    expect_passes_rule(
        LoneAnonymousOperation,
        """"""
      {
        field
      }
    """""",
    )


def test_multiple_named_operation():
    expect_passes_rule(
        LoneAnonymousOperation,
        """"""
      query Foo {
        field
      }

      query Bar {
        field
      }
    """""",
    )


def test_anon_operation_with_fragment():
    expect_passes_rule(
        LoneAnonymousOperation,
        """"""
      {
        ...Foo
      }
      fragment Foo on Type {
        field
      }
    """""",
    )


def test_multiple_anon_operations():
    expect_fails_rule(
        LoneAnonymousOperation,
        """"""
      {
        fieldA
      }
      {
        fieldB
      }
    """""",
        [anon_not_alone(2, 7), anon_not_alone(5, 7)],
    )


def test_anon_operation_with_a_mutation():
    expect_fails_rule(
        LoneAnonymousOperation,
        """"""
      {
        fieldA
      }
      mutation Foo {
        fieldB
      }
    """""",
        [anon_not_alone(2, 7)],
    )


def test_anon_operation_with_a_subscription():
    expect_fails_rule(
        LoneAnonymousOperation,
        """"""
      {
        fieldA
      }
      subscription Foo {
        fieldB
      }
    """""",
        [anon_not_alone(2, 7)],
    )"
263	donghui	0	"from graphql.language.location import SourceLocation
from graphql.validation.rules import LoneAnonymousOperation

from .utils import expect_fails_rule, expect_passes_rule


def anon_not_alone(line, column):
    return {
        ""message"": LoneAnonymousOperation.anonymous_operation_not_alone_message(),
        ""locations"": [SourceLocation(line, column)],
    }


def test_no_operations():
    expect_passes_rule(
        LoneAnonymousOperation,
        """"""
      fragment fragA on Type {
        field
      }
    """""",
    )


def test_one_anon_operation():
    expect_passes_rule(
        LoneAnonymousOperation,
        """"""
      {
        field
      }
    """""",
    )


def test_multiple_named_operation():
    expect_passes_rule(
        LoneAnonymousOperation,
        """"""
      query Foo {
        field
      }

      query Bar {
        field
      }
    """""",
    )


def test_anon_operation_with_fragment():
    expect_passes_rule(
        LoneAnonymousOperation,
        """"""
      {
        ...Foo
      }
      fragment Foo on Type {
        field
      }
    """""",
    )


def test_multiple_anon_operations():
    expect_fails_rule(
        LoneAnonymousOperation,
        """"""
      {
        fieldA
      }
      {
        fieldB
      }
    """""",
        [anon_not_alone(2, 7), anon_not_alone(5, 7)],
    )


def test_anon_operation_with_a_mutation():
    expect_fails_rule(
        LoneAnonymousOperation,
        """"""
      {
        fieldA
      }
      mutation Foo {
        fieldB
      }
    """""",
        [anon_not_alone(2, 7)],
    )


def test_anon_operation_with_a_subscription():
    expect_fails_rule(
        LoneAnonymousOperation,
        """"""
      {
        fieldA
      }
      subscription Foo {
        fieldB
      }
    """""",
        [anon_not_alone(2, 7)],
    )"
323	jackson	4	"# -*- coding: utf-8 -*-
""""""
Charset-Normalizer
~~~~~~~~~~~~~~
The Real First Universal Charset Detector.
A library that helps you read text from an unknown charset encoding.
Motivated by chardet, This package is trying to resolve the issue by taking a new approach.
All IANA character set names for which the Python core library provides codecs are supported.

Basic usage:
   >>> from charset_normalizer import from_bytes
   >>> results = from_bytes('Bсеки човек има право на образование. Oбразованието!'.encode('utf_8'))
   >>> best_guess = results.best()
   >>> str(best_guess)
   'Bсеки човек има право на образование. Oбразованието!'

Others methods and usages are available - see the full documentation
at <https://github.com/Ousret/charset_normalizer>.
:copyright: (c) 2021 by Ahmed TAHRI
:license: MIT, see LICENSE for more details.
""""""
import logging

from .api import from_bytes, from_fp, from_path, normalize
from .legacy import (
    CharsetDetector,
    CharsetDoctor,
    CharsetNormalizerMatch,
    CharsetNormalizerMatches,
    detect,
)
from .models import CharsetMatch, CharsetMatches
from .utils import set_logging_handler
from .version import VERSION, __version__

__all__ = (
    ""from_fp"",
    ""from_path"",
    ""from_bytes"",
    ""normalize"",
    ""detect"",
    ""CharsetMatch"",
    ""CharsetMatches"",
    ""CharsetNormalizerMatch"",
    ""CharsetNormalizerMatches"",
    ""CharsetDetector"",
    ""CharsetDoctor"",
    ""__version__"",
    ""VERSION"",
    ""set_logging_handler"",
)

# Attach a NullHandler to the top level logger by default
# https://docs.python.org/3.3/howto/logging.html#configuring-logging-for-a-library

logging.getLogger(""charset_normalizer"").addHandler(logging.NullHandler())"
323	donghui	3	"# -*- coding: utf-8 -*-
""""""
Charset-Normalizer
~~~~~~~~~~~~~~
The Real First Universal Charset Detector.
A library that helps you read text from an unknown charset encoding.
Motivated by chardet, This package is trying to resolve the issue by taking a new approach.
All IANA character set names for which the Python core library provides codecs are supported.

Basic usage:
   >>> from charset_normalizer import from_bytes
   >>> results = from_bytes('Bсеки човек има право на образование. Oбразованието!'.encode('utf_8'))
   >>> best_guess = results.best()
   >>> str(best_guess)
   'Bсеки човек има право на образование. Oбразованието!'

Others methods and usages are available - see the full documentation
at <https://github.com/Ousret/charset_normalizer>.
:copyright: (c) 2021 by Ahmed TAHRI
:license: MIT, see LICENSE for more details.
""""""
import logging

from .api import from_bytes, from_fp, from_path, normalize
from .legacy import (
    CharsetDetector,
    CharsetDoctor,
    CharsetNormalizerMatch,
    CharsetNormalizerMatches,
    detect,
)
from .models import CharsetMatch, CharsetMatches
from .utils import set_logging_handler
from .version import VERSION, __version__

__all__ = (
    ""from_fp"",
    ""from_path"",
    ""from_bytes"",
    ""normalize"",
    ""detect"",
    ""CharsetMatch"",
    ""CharsetMatches"",
    ""CharsetNormalizerMatch"",
    ""CharsetNormalizerMatches"",
    ""CharsetDetector"",
    ""CharsetDoctor"",
    ""__version__"",
    ""VERSION"",
    ""set_logging_handler"",
)

# Attach a NullHandler to the top level logger by default
# https://docs.python.org/3.3/howto/logging.html#configuring-logging-for-a-library

logging.getLogger(""charset_normalizer"").addHandler(logging.NullHandler())"
372	jackson	4	"""""""uestc URL Configuration

The `urlpatterns` list routes URLs to views. For more information please see:
    https://docs.djangoproject.com/en/1.9/topics/http/urls/
Examples:
Function views
    1. Add an import:  from my_app import views
    2. Add a URL to urlpatterns:  url(r'^$', views.home, name='home')
Class-based views
    1. Add an import:  from other_app.views import Home
    2. Add a URL to urlpatterns:  url(r'^$', Home.as_view(), name='home')
Including another URLconf
    1. Import the include() function: from django.conf.urls import url, include
    2. Add a URL to urlpatterns:  url(r'^blog/', include('blog.urls'))
""""""
from django.conf.urls import url
from django.contrib import admin
from subject import views

urlpatterns = [
    url(r'^admin/$', views.admin_login, name='admin_login'),
    url(r'^$', views.login, name='login'),
    url(r'^login/$', views.user_login, name='user_login'),
    url(r'^index/$', views.index, name='index'),
    url(r'^admin/index/$', views.admin_index, name='admin_index'),
    url(r'^course/$', views.get_course, name='get_course'),
    url(r'^log/$', views.get_log, name='get_log'),
    url(r'^choose/$', views.get_already_choose, name='get_already_choose'),
    url(r'^logout/$', views.logout, name='logout'),
    url(r'^select/$', views.select_course, name='select_course'),
    url(r'^cancel/$', views.cancel_course, name='cancel_course'),
    url(r'^admin/get/course/$', views.list_course, name='list_course'),
    url(r'^admin/get/student/$', views.list_student, name='list_student'),
    url(r'^admin/get/teacher/$', views.list_teacher, name='list_teacher'),
    url(r'^admin/delete/course/$', views.delete_course, name='delete_course'),
    url(r'^admin/add/course/$', views.add_course, name='add_course'),
    url(r'^admin/add/student/$', views.add_student, name='add_student'),
    url(r'^admin/resetPassword/$', views.reset_passwd, name='reset_passwd'),
    url(r'^admin/delete/teacher/$', views.delete_teacher, name='delete_teacher'),
    url(r'^admin/add/teacher/$', views.add_teacher, name='add_teacher'),
    url(r'^search/$', views.search, name='search'),
    url(r'^password/$', views.change_passwd, name='change_passwd'),
]"
372	donghui	4	"""""""uestc URL Configuration

The `urlpatterns` list routes URLs to views. For more information please see:
    https://docs.djangoproject.com/en/1.9/topics/http/urls/
Examples:
Function views
    1. Add an import:  from my_app import views
    2. Add a URL to urlpatterns:  url(r'^$', views.home, name='home')
Class-based views
    1. Add an import:  from other_app.views import Home
    2. Add a URL to urlpatterns:  url(r'^$', Home.as_view(), name='home')
Including another URLconf
    1. Import the include() function: from django.conf.urls import url, include
    2. Add a URL to urlpatterns:  url(r'^blog/', include('blog.urls'))
""""""
from django.conf.urls import url
from django.contrib import admin
from subject import views

urlpatterns = [
    url(r'^admin/$', views.admin_login, name='admin_login'),
    url(r'^$', views.login, name='login'),
    url(r'^login/$', views.user_login, name='user_login'),
    url(r'^index/$', views.index, name='index'),
    url(r'^admin/index/$', views.admin_index, name='admin_index'),
    url(r'^course/$', views.get_course, name='get_course'),
    url(r'^log/$', views.get_log, name='get_log'),
    url(r'^choose/$', views.get_already_choose, name='get_already_choose'),
    url(r'^logout/$', views.logout, name='logout'),
    url(r'^select/$', views.select_course, name='select_course'),
    url(r'^cancel/$', views.cancel_course, name='cancel_course'),
    url(r'^admin/get/course/$', views.list_course, name='list_course'),
    url(r'^admin/get/student/$', views.list_student, name='list_student'),
    url(r'^admin/get/teacher/$', views.list_teacher, name='list_teacher'),
    url(r'^admin/delete/course/$', views.delete_course, name='delete_course'),
    url(r'^admin/add/course/$', views.add_course, name='add_course'),
    url(r'^admin/add/student/$', views.add_student, name='add_student'),
    url(r'^admin/resetPassword/$', views.reset_passwd, name='reset_passwd'),
    url(r'^admin/delete/teacher/$', views.delete_teacher, name='delete_teacher'),
    url(r'^admin/add/teacher/$', views.add_teacher, name='add_teacher'),
    url(r'^search/$', views.search, name='search'),
    url(r'^password/$', views.change_passwd, name='change_passwd'),
]"
381	jackson	3	"""""""
 The GeometryColumns and SpatialRefSys models for the Oracle spatial
 backend.

 It should be noted that Oracle Spatial does not have database tables
 named according to the OGC standard, so the closest analogs are used.
 For example, the `USER_SDO_GEOM_METADATA` is used for the GeometryColumns
 model and the `SDO_COORD_REF_SYS` is used for the SpatialRefSys model.
""""""
from django.contrib.gis.db import models
from django.contrib.gis.db.backends.base.models import SpatialRefSysMixin


class OracleGeometryColumns(models.Model):
    ""Maps to the Oracle USER_SDO_GEOM_METADATA table.""
    table_name = models.CharField(max_length=32)
    column_name = models.CharField(max_length=1024)
    srid = models.IntegerField(primary_key=True)
    # TODO: Add support for `diminfo` column (type MDSYS.SDO_DIM_ARRAY).

    class Meta:
        app_label = ""gis""
        db_table = ""USER_SDO_GEOM_METADATA""
        managed = False

    def __str__(self):
        return ""%s - %s (SRID: %s)"" % (self.table_name, self.column_name, self.srid)

    @classmethod
    def table_name_col(cls):
        """"""
        Return the name of the metadata column used to store the feature table
        name.
        """"""
        return ""table_name""

    @classmethod
    def geom_col_name(cls):
        """"""
        Return the name of the metadata column used to store the feature
        geometry column.
        """"""
        return ""column_name""


class OracleSpatialRefSys(models.Model, SpatialRefSysMixin):
    ""Maps to the Oracle MDSYS.CS_SRS table.""
    cs_name = models.CharField(max_length=68)
    srid = models.IntegerField(primary_key=True)
    auth_srid = models.IntegerField()
    auth_name = models.CharField(max_length=256)
    wktext = models.CharField(max_length=2046)
    # Optional geometry representing the bounds of this coordinate
    # system.  By default, all are NULL in the table.
    cs_bounds = models.PolygonField(null=True)

    class Meta:
        app_label = ""gis""
        db_table = ""CS_SRS""
        managed = False

    @property
    def wkt(self):
        return self.wktext"
381	donghui	2	"""""""
 The GeometryColumns and SpatialRefSys models for the Oracle spatial
 backend.

 It should be noted that Oracle Spatial does not have database tables
 named according to the OGC standard, so the closest analogs are used.
 For example, the `USER_SDO_GEOM_METADATA` is used for the GeometryColumns
 model and the `SDO_COORD_REF_SYS` is used for the SpatialRefSys model.
""""""
from django.contrib.gis.db import models
from django.contrib.gis.db.backends.base.models import SpatialRefSysMixin


class OracleGeometryColumns(models.Model):
    ""Maps to the Oracle USER_SDO_GEOM_METADATA table.""
    table_name = models.CharField(max_length=32)
    column_name = models.CharField(max_length=1024)
    srid = models.IntegerField(primary_key=True)
    # TODO: Add support for `diminfo` column (type MDSYS.SDO_DIM_ARRAY).

    class Meta:
        app_label = ""gis""
        db_table = ""USER_SDO_GEOM_METADATA""
        managed = False

    def __str__(self):
        return ""%s - %s (SRID: %s)"" % (self.table_name, self.column_name, self.srid)

    @classmethod
    def table_name_col(cls):
        """"""
        Return the name of the metadata column used to store the feature table
        name.
        """"""
        return ""table_name""

    @classmethod
    def geom_col_name(cls):
        """"""
        Return the name of the metadata column used to store the feature
        geometry column.
        """"""
        return ""column_name""


class OracleSpatialRefSys(models.Model, SpatialRefSysMixin):
    ""Maps to the Oracle MDSYS.CS_SRS table.""
    cs_name = models.CharField(max_length=68)
    srid = models.IntegerField(primary_key=True)
    auth_srid = models.IntegerField()
    auth_name = models.CharField(max_length=256)
    wktext = models.CharField(max_length=2046)
    # Optional geometry representing the bounds of this coordinate
    # system.  By default, all are NULL in the table.
    cs_bounds = models.PolygonField(null=True)

    class Meta:
        app_label = ""gis""
        db_table = ""CS_SRS""
        managed = False

    @property
    def wkt(self):
        return self.wktext"
290	jackson	0	"# Copyright (c) 2020, Oracle and/or its affiliates.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License, version 2.0, as
# published by the Free Software Foundation.
#
# This program is also distributed with certain software (including
# but not limited to OpenSSL) that is licensed under separate terms,
# as designated in a particular file or component or in included license
# documentation.  The authors of MySQL hereby grant you an
# additional permission to link the program and your derivative works
# with the separately licensed software that they have included with
# MySQL.
#
# Without limiting anything contained in the foregoing, this file,
# which is part of MySQL Connector/Python, is also subject to the
# Universal FOSS Exception, version 1.0, a copy of which can be found at
# http://oss.oracle.com/licenses/universal-foss-exception.
#
# This program is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
# See the GNU General Public License, version 2.0, for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software Foundation, Inc.,
# 51 Franklin St, Fifth Floor, Boston, MA 02110-1301  USA

from django.db.backends.mysql.schema import DatabaseSchemaEditor as MySQLDatabaseSchemaEditor


class DatabaseSchemaEditor(MySQLDatabaseSchemaEditor):

    def quote_value(self, value):
        self.connection.ensure_connection()
        if isinstance(value, str):
            value = value.replace('%', '%%')
        quoted = self.connection.connection.converter.escape(value)
        if isinstance(value, str) and isinstance(quoted, bytes):
            quoted = quoted.decode()
        return quoted"
290	donghui	0	"# Copyright (c) 2020, Oracle and/or its affiliates.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License, version 2.0, as
# published by the Free Software Foundation.
#
# This program is also distributed with certain software (including
# but not limited to OpenSSL) that is licensed under separate terms,
# as designated in a particular file or component or in included license
# documentation.  The authors of MySQL hereby grant you an
# additional permission to link the program and your derivative works
# with the separately licensed software that they have included with
# MySQL.
#
# Without limiting anything contained in the foregoing, this file,
# which is part of MySQL Connector/Python, is also subject to the
# Universal FOSS Exception, version 1.0, a copy of which can be found at
# http://oss.oracle.com/licenses/universal-foss-exception.
#
# This program is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
# See the GNU General Public License, version 2.0, for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software Foundation, Inc.,
# 51 Franklin St, Fifth Floor, Boston, MA 02110-1301  USA

from django.db.backends.mysql.schema import DatabaseSchemaEditor as MySQLDatabaseSchemaEditor


class DatabaseSchemaEditor(MySQLDatabaseSchemaEditor):

    def quote_value(self, value):
        self.connection.ensure_connection()
        if isinstance(value, str):
            value = value.replace('%', '%%')
        quoted = self.connection.connection.converter.escape(value)
        if isinstance(value, str) and isinstance(quoted, bytes):
            quoted = quoted.decode()
        return quoted"
352	jackson	0	"from time import time

from bot import DOWNLOAD_DIR, LOGGER
from bot.helper.ext_utils.bot_utils import get_readable_file_size, MirrorStatus, EngineStatus, get_readable_time
from bot.helper.ext_utils.fs_utils import get_path_size

class ZipStatus:
    def __init__(self, name, size, gid, listener):
        self.__name = name
        self.__size = size
        self.__gid = gid
        self.__listener = listener
        self.__uid = listener.uid
        self.__start_time = time()
        self.message = listener.message

    def gid(self):
        return self.__gid

    def speed_raw(self):
        return self.processed_bytes() / (time() - self.__start_time)

    def progress_raw(self):
        try:
            return self.processed_bytes() / self.__size * 100
        except:
            return 0

    def progress(self):
        return f'{round(self.progress_raw(), 2)}%'

    def speed(self):
        return f'{get_readable_file_size(self.speed_raw())}/s'

    def name(self):
        return self.__name

    def size_raw(self):
        return self.__size

    def size(self):
        return get_readable_file_size(self.__size)

    def eta(self):
        try:
            seconds = (self.size_raw() - self.processed_bytes()) / self.speed_raw()
            return f'{get_readable_time(seconds)}'
        except:
            return '-'

    def status(self):
        return MirrorStatus.STATUS_ARCHIVING

    def processed_bytes(self):
        if self.__listener.newDir:
            return get_path_size(f""{DOWNLOAD_DIR}{self.__uid}10000"")
        else:
            return get_path_size(f""{DOWNLOAD_DIR}{self.__uid}"") - self.__size

    def download(self):
        return self

    def cancel_download(self):
        LOGGER.info(f'Cancelling Archive: {self.__name}')
        if self.__listener.suproc is not None:
            self.__listener.suproc.kill()
        self.__listener.onUploadError('archiving stopped by user!')

    def eng(self):
        return EngineStatus.STATUS_ZIP"
352	donghui	0	"from time import time

from bot import DOWNLOAD_DIR, LOGGER
from bot.helper.ext_utils.bot_utils import get_readable_file_size, MirrorStatus, EngineStatus, get_readable_time
from bot.helper.ext_utils.fs_utils import get_path_size

class ZipStatus:
    def __init__(self, name, size, gid, listener):
        self.__name = name
        self.__size = size
        self.__gid = gid
        self.__listener = listener
        self.__uid = listener.uid
        self.__start_time = time()
        self.message = listener.message

    def gid(self):
        return self.__gid

    def speed_raw(self):
        return self.processed_bytes() / (time() - self.__start_time)

    def progress_raw(self):
        try:
            return self.processed_bytes() / self.__size * 100
        except:
            return 0

    def progress(self):
        return f'{round(self.progress_raw(), 2)}%'

    def speed(self):
        return f'{get_readable_file_size(self.speed_raw())}/s'

    def name(self):
        return self.__name

    def size_raw(self):
        return self.__size

    def size(self):
        return get_readable_file_size(self.__size)

    def eta(self):
        try:
            seconds = (self.size_raw() - self.processed_bytes()) / self.speed_raw()
            return f'{get_readable_time(seconds)}'
        except:
            return '-'

    def status(self):
        return MirrorStatus.STATUS_ARCHIVING

    def processed_bytes(self):
        if self.__listener.newDir:
            return get_path_size(f""{DOWNLOAD_DIR}{self.__uid}10000"")
        else:
            return get_path_size(f""{DOWNLOAD_DIR}{self.__uid}"") - self.__size

    def download(self):
        return self

    def cancel_download(self):
        LOGGER.info(f'Cancelling Archive: {self.__name}')
        if self.__listener.suproc is not None:
            self.__listener.suproc.kill()
        self.__listener.onUploadError('archiving stopped by user!')

    def eng(self):
        return EngineStatus.STATUS_ZIP"
303	jackson	4	"#  Copyright 2022 Google LLC
#
#  Licensed under the Apache License, Version 2.0 (the ""License"");
#  you may not use this file except in compliance with the License.
#  You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an ""AS IS"" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
#  limitations under the License.


# This is an ingredient file. It is not meant to be run directly. Check the samples/snippets
# folder for complete code samples that are ready to be used.
# Disabling flake8 for the ingredients file, as it would fail F821 - undefined name check.
# flake8: noqa
from google.cloud import compute_v1


# <INGREDIENT set_deprecation_status>
def set_deprecation_status(project_id: str, image_name: str, status: compute_v1.DeprecationStatus.State) -> None:
    """"""
    Modify the deprecation status of an image.

    Note: Image objects by default don't have the `deprecated` attribute at all unless it's set.

    Args:
        project_id: project ID or project number of the Cloud project that hosts the image.
        image_name: name of the image you want to modify
        status: the status you want to set for the image. Available values are available in
            `compute_v1.DeprecationStatus.State` enum. Learn more about image deprecation statuses:
            https://cloud.google.com/compute/docs/images/create-delete-deprecate-private-images#deprecation-states
    """"""
    image_client = compute_v1.ImagesClient()
    deprecation_status = compute_v1.DeprecationStatus()
    deprecation_status.state = status.name
    operation = image_client.deprecate(project=project_id, image=image_name,
                                       deprecation_status_resource=deprecation_status)

    wait_for_extended_operation(operation, ""changing deprecation state of an image"")
# </INGREDIENT>"
303	donghui	4	"#  Copyright 2022 Google LLC
#
#  Licensed under the Apache License, Version 2.0 (the ""License"");
#  you may not use this file except in compliance with the License.
#  You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an ""AS IS"" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
#  limitations under the License.


# This is an ingredient file. It is not meant to be run directly. Check the samples/snippets
# folder for complete code samples that are ready to be used.
# Disabling flake8 for the ingredients file, as it would fail F821 - undefined name check.
# flake8: noqa
from google.cloud import compute_v1


# <INGREDIENT set_deprecation_status>
def set_deprecation_status(project_id: str, image_name: str, status: compute_v1.DeprecationStatus.State) -> None:
    """"""
    Modify the deprecation status of an image.

    Note: Image objects by default don't have the `deprecated` attribute at all unless it's set.

    Args:
        project_id: project ID or project number of the Cloud project that hosts the image.
        image_name: name of the image you want to modify
        status: the status you want to set for the image. Available values are available in
            `compute_v1.DeprecationStatus.State` enum. Learn more about image deprecation statuses:
            https://cloud.google.com/compute/docs/images/create-delete-deprecate-private-images#deprecation-states
    """"""
    image_client = compute_v1.ImagesClient()
    deprecation_status = compute_v1.DeprecationStatus()
    deprecation_status.state = status.name
    operation = image_client.deprecate(project=project_id, image=image_name,
                                       deprecation_status_resource=deprecation_status)

    wait_for_extended_operation(operation, ""changing deprecation state of an image"")
# </INGREDIENT>"
395	jackson	0	"# -*- coding: utf-8 -*-
# Copyright (C) 2006-2007 Søren Roug, European Environment Agency
#
# This library is free software; you can redistribute it and/or
# modify it under the terms of the GNU Lesser General Public
# License as published by the Free Software Foundation; either
# version 2.1 of the License, or (at your option) any later version.
#
# This library is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public
# License along with this library; if not, write to the Free Software
# Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301  USA
#
# Contributor(s):
#

from odf.namespaces import ANIMNS
from odf.element import Element


# Autogenerated
def Animate(**args):
    return Element(qname = (ANIMNS,'animate'), **args)

def Animatecolor(**args):
    return Element(qname = (ANIMNS,'animateColor'), **args)

def Animatemotion(**args):
    return Element(qname = (ANIMNS,'animateMotion'), **args)

def Animatetransform(**args):
    return Element(qname = (ANIMNS,'animateTransform'), **args)

def Audio(**args):
    return Element(qname = (ANIMNS,'audio'), **args)

def Command(**args):
    return Element(qname = (ANIMNS,'command'), **args)

def Iterate(**args):
    return Element(qname = (ANIMNS,'iterate'), **args)

def Par(**args):
    return Element(qname = (ANIMNS,'par'), **args)

def Param(**args):
    return Element(qname = (ANIMNS,'param'), **args)

def Seq(**args):
    return Element(qname = (ANIMNS,'seq'), **args)

def Set(**args):
    return Element(qname = (ANIMNS,'set'), **args)

def Transitionfilter(**args):
    return Element(qname = (ANIMNS,'transitionFilter'), **args)
"
395	donghui	0	"# -*- coding: utf-8 -*-
# Copyright (C) 2006-2007 Søren Roug, European Environment Agency
#
# This library is free software; you can redistribute it and/or
# modify it under the terms of the GNU Lesser General Public
# License as published by the Free Software Foundation; either
# version 2.1 of the License, or (at your option) any later version.
#
# This library is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public
# License along with this library; if not, write to the Free Software
# Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301  USA
#
# Contributor(s):
#

from odf.namespaces import ANIMNS
from odf.element import Element


# Autogenerated
def Animate(**args):
    return Element(qname = (ANIMNS,'animate'), **args)

def Animatecolor(**args):
    return Element(qname = (ANIMNS,'animateColor'), **args)

def Animatemotion(**args):
    return Element(qname = (ANIMNS,'animateMotion'), **args)

def Animatetransform(**args):
    return Element(qname = (ANIMNS,'animateTransform'), **args)

def Audio(**args):
    return Element(qname = (ANIMNS,'audio'), **args)

def Command(**args):
    return Element(qname = (ANIMNS,'command'), **args)

def Iterate(**args):
    return Element(qname = (ANIMNS,'iterate'), **args)

def Par(**args):
    return Element(qname = (ANIMNS,'par'), **args)

def Param(**args):
    return Element(qname = (ANIMNS,'param'), **args)

def Seq(**args):
    return Element(qname = (ANIMNS,'seq'), **args)

def Set(**args):
    return Element(qname = (ANIMNS,'set'), **args)

def Transitionfilter(**args):
    return Element(qname = (ANIMNS,'transitionFilter'), **args)
"
284	jackson	3	"""""""
 The GeometryColumns and SpatialRefSys models for the PostGIS backend.
""""""
from django.contrib.gis.db.backends.base.models import SpatialRefSysMixin
from django.db import models


class PostGISGeometryColumns(models.Model):
    """"""
    The 'geometry_columns' view from PostGIS. See the PostGIS
    documentation at Ch. 4.3.2.
    """"""
    f_table_catalog = models.CharField(max_length=256)
    f_table_schema = models.CharField(max_length=256)
    f_table_name = models.CharField(max_length=256)
    f_geometry_column = models.CharField(max_length=256)
    coord_dimension = models.IntegerField()
    srid = models.IntegerField(primary_key=True)
    type = models.CharField(max_length=30)

    class Meta:
        app_label = 'gis'
        db_table = 'geometry_columns'
        managed = False

    def __str__(self):
        return '%s.%s - %dD %s field (SRID: %d)' % (
            self.f_table_name,
            self.f_geometry_column,
            self.coord_dimension,
            self.type,
            self.srid,
        )

    @classmethod
    def table_name_col(cls):
        """"""
        Return the name of the metadata column used to store the feature table
        name.
        """"""
        return 'f_table_name'

    @classmethod
    def geom_col_name(cls):
        """"""
        Return the name of the metadata column used to store the feature
        geometry column.
        """"""
        return 'f_geometry_column'


class PostGISSpatialRefSys(models.Model, SpatialRefSysMixin):
    """"""
    The 'spatial_ref_sys' table from PostGIS. See the PostGIS
    documentation at Ch. 4.2.1.
    """"""
    srid = models.IntegerField(primary_key=True)
    auth_name = models.CharField(max_length=256)
    auth_srid = models.IntegerField()
    srtext = models.CharField(max_length=2048)
    proj4text = models.CharField(max_length=2048)

    class Meta:
        app_label = 'gis'
        db_table = 'spatial_ref_sys'
        managed = False

    @property
    def wkt(self):
        return self.srtext"
284	donghui	2	"""""""
 The GeometryColumns and SpatialRefSys models for the PostGIS backend.
""""""
from django.contrib.gis.db.backends.base.models import SpatialRefSysMixin
from django.db import models


class PostGISGeometryColumns(models.Model):
    """"""
    The 'geometry_columns' view from PostGIS. See the PostGIS
    documentation at Ch. 4.3.2.
    """"""
    f_table_catalog = models.CharField(max_length=256)
    f_table_schema = models.CharField(max_length=256)
    f_table_name = models.CharField(max_length=256)
    f_geometry_column = models.CharField(max_length=256)
    coord_dimension = models.IntegerField()
    srid = models.IntegerField(primary_key=True)
    type = models.CharField(max_length=30)

    class Meta:
        app_label = 'gis'
        db_table = 'geometry_columns'
        managed = False

    def __str__(self):
        return '%s.%s - %dD %s field (SRID: %d)' % (
            self.f_table_name,
            self.f_geometry_column,
            self.coord_dimension,
            self.type,
            self.srid,
        )

    @classmethod
    def table_name_col(cls):
        """"""
        Return the name of the metadata column used to store the feature table
        name.
        """"""
        return 'f_table_name'

    @classmethod
    def geom_col_name(cls):
        """"""
        Return the name of the metadata column used to store the feature
        geometry column.
        """"""
        return 'f_geometry_column'


class PostGISSpatialRefSys(models.Model, SpatialRefSysMixin):
    """"""
    The 'spatial_ref_sys' table from PostGIS. See the PostGIS
    documentation at Ch. 4.2.1.
    """"""
    srid = models.IntegerField(primary_key=True)
    auth_name = models.CharField(max_length=256)
    auth_srid = models.IntegerField()
    srtext = models.CharField(max_length=2048)
    proj4text = models.CharField(max_length=2048)

    class Meta:
        app_label = 'gis'
        db_table = 'spatial_ref_sys'
        managed = False

    @property
    def wkt(self):
        return self.srtext"
337	jackson	0	"######################## BEGIN LICENSE BLOCK ########################
# The Original Code is Mozilla Universal charset detector code.
#
# The Initial Developer of the Original Code is
# Netscape Communications Corporation.
# Portions created by the Initial Developer are Copyright (C) 2001
# the Initial Developer. All Rights Reserved.
#
# Contributor(s):
#   Mark Pilgrim - port to Python
#   Shy Shalom - original C code
#   Proofpoint, Inc.
#
# This library is free software; you can redistribute it and/or
# modify it under the terms of the GNU Lesser General Public
# License as published by the Free Software Foundation; either
# version 2.1 of the License, or (at your option) any later version.
#
# This library is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public
# License along with this library; if not, write to the Free Software
# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA
# 02110-1301  USA
######################### END LICENSE BLOCK #########################

from .charsetgroupprober import CharSetGroupProber
from .utf8prober import UTF8Prober
from .sjisprober import SJISProber
from .eucjpprober import EUCJPProber
from .gb2312prober import GB2312Prober
from .euckrprober import EUCKRProber
from .cp949prober import CP949Prober
from .big5prober import Big5Prober
from .euctwprober import EUCTWProber


class MBCSGroupProber(CharSetGroupProber):
    def __init__(self, lang_filter=None):
        super(MBCSGroupProber, self).__init__(lang_filter=lang_filter)
        self.probers = [
            UTF8Prober(),
            SJISProber(),
            EUCJPProber(),
            GB2312Prober(),
            EUCKRProber(),
            CP949Prober(),
            Big5Prober(),
            EUCTWProber()
        ]
        self.reset()"
337	donghui	0	"######################## BEGIN LICENSE BLOCK ########################
# The Original Code is Mozilla Universal charset detector code.
#
# The Initial Developer of the Original Code is
# Netscape Communications Corporation.
# Portions created by the Initial Developer are Copyright (C) 2001
# the Initial Developer. All Rights Reserved.
#
# Contributor(s):
#   Mark Pilgrim - port to Python
#   Shy Shalom - original C code
#   Proofpoint, Inc.
#
# This library is free software; you can redistribute it and/or
# modify it under the terms of the GNU Lesser General Public
# License as published by the Free Software Foundation; either
# version 2.1 of the License, or (at your option) any later version.
#
# This library is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public
# License along with this library; if not, write to the Free Software
# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA
# 02110-1301  USA
######################### END LICENSE BLOCK #########################

from .charsetgroupprober import CharSetGroupProber
from .utf8prober import UTF8Prober
from .sjisprober import SJISProber
from .eucjpprober import EUCJPProber
from .gb2312prober import GB2312Prober
from .euckrprober import EUCKRProber
from .cp949prober import CP949Prober
from .big5prober import Big5Prober
from .euctwprober import EUCTWProber


class MBCSGroupProber(CharSetGroupProber):
    def __init__(self, lang_filter=None):
        super(MBCSGroupProber, self).__init__(lang_filter=lang_filter)
        self.probers = [
            UTF8Prober(),
            SJISProber(),
            EUCJPProber(),
            GB2312Prober(),
            EUCKRProber(),
            CP949Prober(),
            Big5Prober(),
            EUCTWProber()
        ]
        self.reset()"
277	jackson	4	"from __future__ import annotations

import numpy as np

from pandas._typing import NumpyIndexT

from pandas.core.dtypes.common import is_list_like


def cartesian_product(X) -> list[np.ndarray]:
    """"""
    Numpy version of itertools.product.
    Sometimes faster (for large inputs)...

    Parameters
    ----------
    X : list-like of list-likes

    Returns
    -------
    product : list of ndarrays

    Examples
    --------
    >>> cartesian_product([list('ABC'), [1, 2]])
    [array(['A', 'A', 'B', 'B', 'C', 'C'], dtype='<U1'), array([1, 2, 1, 2, 1, 2])]

    See Also
    --------
    itertools.product : Cartesian product of input iterables.  Equivalent to
        nested for-loops.
    """"""
    msg = ""Input must be a list-like of list-likes""
    if not is_list_like(X):
        raise TypeError(msg)
    for x in X:
        if not is_list_like(x):
            raise TypeError(msg)

    if len(X) == 0:
        return []

    lenX = np.fromiter((len(x) for x in X), dtype=np.intp)
    cumprodX = np.cumproduct(lenX)

    if np.any(cumprodX < 0):
        raise ValueError(""Product space too large to allocate arrays!"")

    a = np.roll(cumprodX, 1)
    a[0] = 1

    if cumprodX[-1] != 0:
        b = cumprodX[-1] / cumprodX
    else:
        # if any factor is empty, the cartesian product is empty
        b = np.zeros_like(cumprodX)

    # error: Argument of type ""int_"" cannot be assigned to parameter ""num"" of
    # type ""int"" in function ""tile_compat""
    return [
        tile_compat(
            np.repeat(x, b[i]),
            np.product(a[i]),  # pyright: ignore[reportGeneralTypeIssues]
        )
        for i, x in enumerate(X)
    ]


def tile_compat(arr: NumpyIndexT, num: int) -> NumpyIndexT:
    """"""
    Index compat for np.tile.

    Notes
    -----
    Does not support multi-dimensional `num`.
    """"""
    if isinstance(arr, np.ndarray):
        return np.tile(arr, num)

    # Otherwise we have an Index
    taker = np.tile(np.arange(len(arr)), num)
    return arr.take(taker)"
277	donghui	4	"from __future__ import annotations

import numpy as np

from pandas._typing import NumpyIndexT

from pandas.core.dtypes.common import is_list_like


def cartesian_product(X) -> list[np.ndarray]:
    """"""
    Numpy version of itertools.product.
    Sometimes faster (for large inputs)...

    Parameters
    ----------
    X : list-like of list-likes

    Returns
    -------
    product : list of ndarrays

    Examples
    --------
    >>> cartesian_product([list('ABC'), [1, 2]])
    [array(['A', 'A', 'B', 'B', 'C', 'C'], dtype='<U1'), array([1, 2, 1, 2, 1, 2])]

    See Also
    --------
    itertools.product : Cartesian product of input iterables.  Equivalent to
        nested for-loops.
    """"""
    msg = ""Input must be a list-like of list-likes""
    if not is_list_like(X):
        raise TypeError(msg)
    for x in X:
        if not is_list_like(x):
            raise TypeError(msg)

    if len(X) == 0:
        return []

    lenX = np.fromiter((len(x) for x in X), dtype=np.intp)
    cumprodX = np.cumproduct(lenX)

    if np.any(cumprodX < 0):
        raise ValueError(""Product space too large to allocate arrays!"")

    a = np.roll(cumprodX, 1)
    a[0] = 1

    if cumprodX[-1] != 0:
        b = cumprodX[-1] / cumprodX
    else:
        # if any factor is empty, the cartesian product is empty
        b = np.zeros_like(cumprodX)

    # error: Argument of type ""int_"" cannot be assigned to parameter ""num"" of
    # type ""int"" in function ""tile_compat""
    return [
        tile_compat(
            np.repeat(x, b[i]),
            np.product(a[i]),  # pyright: ignore[reportGeneralTypeIssues]
        )
        for i, x in enumerate(X)
    ]


def tile_compat(arr: NumpyIndexT, num: int) -> NumpyIndexT:
    """"""
    Index compat for np.tile.

    Notes
    -----
    Does not support multi-dimensional `num`.
    """"""
    if isinstance(arr, np.ndarray):
        return np.tile(arr, num)

    # Otherwise we have an Index
    taker = np.tile(np.arange(len(arr)), num)
    return arr.take(taker)"
366	jackson	3	"from six import string_types

from .base import GraphQLDocument

# Necessary for static type checking
if False:  # flake8: noqa
    from ..type.schema import GraphQLSchema
    from typing import Any, Optional, Dict, Callable, Union


class GraphQLCompiledDocument(GraphQLDocument):
    @classmethod
    def from_code(
        cls,
        schema,  # type: GraphQLSchema
        code,  # type: Union[str, Any]
        uptodate=None,  # type: Optional[bool]
        extra_namespace=None,  # type: Optional[Dict[str, Any]]
    ):
        # type: (...) -> GraphQLCompiledDocument
        """"""Creates a GraphQLDocument object from compiled code and the globals.  This
        is used by the loaders and schema to create a document object.
        """"""
        if isinstance(code, string_types):
            filename = ""<document>""
            code = compile(code, filename, ""exec"")
        namespace = {""__file__"": code.co_filename}
        exec(code, namespace)
        if extra_namespace:
            namespace.update(extra_namespace)
        rv = cls._from_namespace(schema, namespace)
        # rv._uptodate = uptodate
        return rv

    @classmethod
    def from_module_dict(cls, schema, module_dict):
        # type: (GraphQLSchema, Dict[str, Any]) -> GraphQLCompiledDocument
        """"""Creates a template object from a module.  This is used by the
        module loader to create a document object.
        """"""
        return cls._from_namespace(schema, module_dict)

    @classmethod
    def _from_namespace(cls, schema, namespace):
        # type: (GraphQLSchema, Dict[str, Any]) -> GraphQLCompiledDocument
        document_string = namespace.get(""document_string"", """")  # type: str
        document_ast = namespace.get(""document_ast"")  # type: ignore
        execute = namespace[""execute""]  # type: Callable

        namespace[""schema""] = schema
        return cls(
            schema=schema,
            document_string=document_string,
            document_ast=document_ast,  # type: ignore
            execute=execute,
        )"
366	donghui	3	"from six import string_types

from .base import GraphQLDocument

# Necessary for static type checking
if False:  # flake8: noqa
    from ..type.schema import GraphQLSchema
    from typing import Any, Optional, Dict, Callable, Union


class GraphQLCompiledDocument(GraphQLDocument):
    @classmethod
    def from_code(
        cls,
        schema,  # type: GraphQLSchema
        code,  # type: Union[str, Any]
        uptodate=None,  # type: Optional[bool]
        extra_namespace=None,  # type: Optional[Dict[str, Any]]
    ):
        # type: (...) -> GraphQLCompiledDocument
        """"""Creates a GraphQLDocument object from compiled code and the globals.  This
        is used by the loaders and schema to create a document object.
        """"""
        if isinstance(code, string_types):
            filename = ""<document>""
            code = compile(code, filename, ""exec"")
        namespace = {""__file__"": code.co_filename}
        exec(code, namespace)
        if extra_namespace:
            namespace.update(extra_namespace)
        rv = cls._from_namespace(schema, namespace)
        # rv._uptodate = uptodate
        return rv

    @classmethod
    def from_module_dict(cls, schema, module_dict):
        # type: (GraphQLSchema, Dict[str, Any]) -> GraphQLCompiledDocument
        """"""Creates a template object from a module.  This is used by the
        module loader to create a document object.
        """"""
        return cls._from_namespace(schema, module_dict)

    @classmethod
    def _from_namespace(cls, schema, namespace):
        # type: (GraphQLSchema, Dict[str, Any]) -> GraphQLCompiledDocument
        document_string = namespace.get(""document_string"", """")  # type: str
        document_ast = namespace.get(""document_ast"")  # type: ignore
        execute = namespace[""execute""]  # type: Callable

        namespace[""schema""] = schema
        return cls(
            schema=schema,
            document_string=document_string,
            document_ast=document_ast,  # type: ignore
            execute=execute,
        )"
428	jackson	1	"# Copyright 2017-present Open Networking Foundation
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
import structlog
from enum import Enum
from google.protobuf.json_format import MessageToDict
from google.protobuf.message import Message
from simplejson import dumps

from common.event_bus import EventBusClient
from voltha.core.config.config_proxy import CallbackType
from voltha.protos import third_party
from voltha.protos.events_pb2 import ConfigEvent, ConfigEventType

IGNORED_CALLBACKS = [CallbackType.PRE_ADD, CallbackType.GET,
                     CallbackType.POST_LISTCHANGE, CallbackType.PRE_REMOVE,
                     CallbackType.PRE_UPDATE]

log = structlog.get_logger()

class ConfigEventBus(object):

    __slots__ = (
        '_event_bus_client',  # The event bus client used to publish events.
        '_topic'  # the topic to publish to
    )

    def __init__(self):
        self._event_bus_client = EventBusClient()
        self._topic = 'model-change-events'

    def advertise(self, type, data, hash=None):
        if type in IGNORED_CALLBACKS:
            log.info('Ignoring event {} with data {}'.format(type, data))
            return

        if type is CallbackType.POST_ADD:
            kind = ConfigEventType.add
        elif type is CallbackType.POST_REMOVE:
            kind = ConfigEventType.remove
        else:
            kind = ConfigEventType.update

        if isinstance(data, Message):
            msg = dumps(MessageToDict(data, True, True))
        else:
            msg = data

        event = ConfigEvent(
            type=kind,
            hash=hash,
            data=msg
        )

        self._event_bus_client.publish(self._topic, event)
"
428	donghui	1	"# Copyright 2017-present Open Networking Foundation
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
import structlog
from enum import Enum
from google.protobuf.json_format import MessageToDict
from google.protobuf.message import Message
from simplejson import dumps

from common.event_bus import EventBusClient
from voltha.core.config.config_proxy import CallbackType
from voltha.protos import third_party
from voltha.protos.events_pb2 import ConfigEvent, ConfigEventType

IGNORED_CALLBACKS = [CallbackType.PRE_ADD, CallbackType.GET,
                     CallbackType.POST_LISTCHANGE, CallbackType.PRE_REMOVE,
                     CallbackType.PRE_UPDATE]

log = structlog.get_logger()

class ConfigEventBus(object):

    __slots__ = (
        '_event_bus_client',  # The event bus client used to publish events.
        '_topic'  # the topic to publish to
    )

    def __init__(self):
        self._event_bus_client = EventBusClient()
        self._topic = 'model-change-events'

    def advertise(self, type, data, hash=None):
        if type in IGNORED_CALLBACKS:
            log.info('Ignoring event {} with data {}'.format(type, data))
            return

        if type is CallbackType.POST_ADD:
            kind = ConfigEventType.add
        elif type is CallbackType.POST_REMOVE:
            kind = ConfigEventType.remove
        else:
            kind = ConfigEventType.update

        if isinstance(data, Message):
            msg = dumps(MessageToDict(data, True, True))
        else:
            msg = data

        event = ConfigEvent(
            type=kind,
            hash=hash,
            data=msg
        )

        self._event_bus_client.publish(self._topic, event)
"
479	jackson	3	"# A demo for the IDsObjectPicker interface.
import win32clipboard
import pythoncom
from win32com.adsi import adsi
from win32com.adsi.adsicon import *

cf_objectpicker = win32clipboard.RegisterClipboardFormat(CFSTR_DSOP_DS_SELECTION_LIST)


def main():
    hwnd = 0

    # Create an instance of the object picker.
    picker = pythoncom.CoCreateInstance(
        adsi.CLSID_DsObjectPicker,
        None,
        pythoncom.CLSCTX_INPROC_SERVER,
        adsi.IID_IDsObjectPicker,
    )

    # Create our scope init info.
    siis = adsi.DSOP_SCOPE_INIT_INFOs(1)
    sii = siis[0]

    # Combine multiple scope types in a single array entry.

    sii.type = (
        DSOP_SCOPE_TYPE_UPLEVEL_JOINED_DOMAIN | DSOP_SCOPE_TYPE_DOWNLEVEL_JOINED_DOMAIN
    )

    # Set uplevel and downlevel filters to include only computer objects.
    # Uplevel filters apply to both mixed and native modes.
    # Notice that the uplevel and downlevel flags are different.

    sii.filterFlags.uplevel.bothModes = DSOP_FILTER_COMPUTERS
    sii.filterFlags.downlevel = DSOP_DOWNLEVEL_FILTER_COMPUTERS

    # Initialize the interface.
    picker.Initialize(
        None,  # Target is the local computer.
        siis,  # scope infos
        DSOP_FLAG_MULTISELECT,  # options
        (""objectGUID"", ""displayName""),
    )  # attributes to fetch

    do = picker.InvokeDialog(hwnd)
    # Extract the data from the IDataObject.
    format_etc = (
        cf_objectpicker,
        None,
        pythoncom.DVASPECT_CONTENT,
        -1,
        pythoncom.TYMED_HGLOBAL,
    )
    medium = do.GetData(format_etc)
    data = adsi.StringAsDS_SELECTION_LIST(medium.data)
    for item in data:
        name, klass, adspath, upn, attrs, flags = item
        print(""Item"", name)
        print("" Class:"", klass)
        print("" AdsPath:"", adspath)
        print("" UPN:"", upn)
        print("" Attrs:"", attrs)
        print("" Flags:"", flags)


if __name__ == ""__main__"":
    main()"
479	donghui	2	"# A demo for the IDsObjectPicker interface.
import win32clipboard
import pythoncom
from win32com.adsi import adsi
from win32com.adsi.adsicon import *

cf_objectpicker = win32clipboard.RegisterClipboardFormat(CFSTR_DSOP_DS_SELECTION_LIST)


def main():
    hwnd = 0

    # Create an instance of the object picker.
    picker = pythoncom.CoCreateInstance(
        adsi.CLSID_DsObjectPicker,
        None,
        pythoncom.CLSCTX_INPROC_SERVER,
        adsi.IID_IDsObjectPicker,
    )

    # Create our scope init info.
    siis = adsi.DSOP_SCOPE_INIT_INFOs(1)
    sii = siis[0]

    # Combine multiple scope types in a single array entry.

    sii.type = (
        DSOP_SCOPE_TYPE_UPLEVEL_JOINED_DOMAIN | DSOP_SCOPE_TYPE_DOWNLEVEL_JOINED_DOMAIN
    )

    # Set uplevel and downlevel filters to include only computer objects.
    # Uplevel filters apply to both mixed and native modes.
    # Notice that the uplevel and downlevel flags are different.

    sii.filterFlags.uplevel.bothModes = DSOP_FILTER_COMPUTERS
    sii.filterFlags.downlevel = DSOP_DOWNLEVEL_FILTER_COMPUTERS

    # Initialize the interface.
    picker.Initialize(
        None,  # Target is the local computer.
        siis,  # scope infos
        DSOP_FLAG_MULTISELECT,  # options
        (""objectGUID"", ""displayName""),
    )  # attributes to fetch

    do = picker.InvokeDialog(hwnd)
    # Extract the data from the IDataObject.
    format_etc = (
        cf_objectpicker,
        None,
        pythoncom.DVASPECT_CONTENT,
        -1,
        pythoncom.TYMED_HGLOBAL,
    )
    medium = do.GetData(format_etc)
    data = adsi.StringAsDS_SELECTION_LIST(medium.data)
    for item in data:
        name, klass, adspath, upn, attrs, flags = item
        print(""Item"", name)
        print("" Class:"", klass)
        print("" AdsPath:"", adspath)
        print("" UPN:"", upn)
        print("" Attrs:"", attrs)
        print("" Flags:"", flags)


if __name__ == ""__main__"":
    main()"
469	jackson	2	"import _plotly_utils.basevalidators


class HoverlabelValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(
        self, plotly_name=""hoverlabel"", parent_name=""choroplethmapbox"", **kwargs
    ):
        super(HoverlabelValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Hoverlabel""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            align
                Sets the horizontal alignment of the text
                content within hover label box. Has an effect
                only if the hover label text spans more two or
                more lines
            alignsrc
                Sets the source reference on Chart Studio Cloud
                for `align`.
            bgcolor
                Sets the background color of the hover labels
                for this trace
            bgcolorsrc
                Sets the source reference on Chart Studio Cloud
                for `bgcolor`.
            bordercolor
                Sets the border color of the hover labels for
                this trace.
            bordercolorsrc
                Sets the source reference on Chart Studio Cloud
                for `bordercolor`.
            font
                Sets the font used in hover labels.
            namelength
                Sets the default length (in number of
                characters) of the trace name in the hover
                labels for all traces. -1 shows the whole name
                regardless of length. 0-3 shows the first 0-3
                characters, and an integer >3 will show the
                whole name if it is less than that many
                characters, but if it is longer, will truncate
                to `namelength - 3` characters and add an
                ellipsis.
            namelengthsrc
                Sets the source reference on Chart Studio Cloud
                for `namelength`.
"""""",
            ),
            **kwargs,
        )"
469	donghui	1	"import _plotly_utils.basevalidators


class HoverlabelValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(
        self, plotly_name=""hoverlabel"", parent_name=""choroplethmapbox"", **kwargs
    ):
        super(HoverlabelValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Hoverlabel""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            align
                Sets the horizontal alignment of the text
                content within hover label box. Has an effect
                only if the hover label text spans more two or
                more lines
            alignsrc
                Sets the source reference on Chart Studio Cloud
                for `align`.
            bgcolor
                Sets the background color of the hover labels
                for this trace
            bgcolorsrc
                Sets the source reference on Chart Studio Cloud
                for `bgcolor`.
            bordercolor
                Sets the border color of the hover labels for
                this trace.
            bordercolorsrc
                Sets the source reference on Chart Studio Cloud
                for `bordercolor`.
            font
                Sets the font used in hover labels.
            namelength
                Sets the default length (in number of
                characters) of the trace name in the hover
                labels for all traces. -1 shows the whole name
                regardless of length. 0-3 shows the first 0-3
                characters, and an integer >3 will show the
                whole name if it is less than that many
                characters, but if it is longer, will truncate
                to `namelength - 3` characters and add an
                ellipsis.
            namelengthsrc
                Sets the source reference on Chart Studio Cloud
                for `namelength`.
"""""",
            ),
            **kwargs,
        )"
438	jackson	0	"import time

from jet_bridge_base.settings import set_settings


class Configuration(object):

    def __init__(self):
        self.init_time = time.time()

    def get_type(self):
        pass

    def get_version(self):
        pass

    def get_model_description(self, db_table):
        pass

    def get_hidden_model_description(self):
        return []

    def get_settings(self):
        pass

    def on_model_pre_create(self, model, pk):
        pass

    def on_model_post_create(self, model, instance):
        pass

    def on_model_pre_update(self, model, instance):
        pass

    def on_model_post_update(self, model, instance):
        pass

    def on_model_pre_delete(self, model, instance):
        pass

    def on_model_post_delete(self, model, instance):
        pass

    def media_get_available_name(self, path):
        pass

    def media_exists(self, path):
        pass

    def media_listdir(self, path):
        pass

    def media_get_modified_time(self, path):
        pass

    def media_open(self, path, mode='rb'):
        pass

    def media_save(self, path, content):
        pass

    def media_delete(self, path):
        pass

    def media_url(self, path, request):
        pass

    def session_set(self, request, name, value, secure=True):
        pass

    def session_get(self, request, name, default=None, decode=True, secure=True):
        pass

    def session_clear(self, request, name):
        pass

    def clean_sso_application_name(self, name):
        return name.lower().replace('-', '')

    def clean_sso_applications(self, applications):
        return dict(map(lambda x: (self.clean_sso_application_name(x[0]), x[1]), applications.items()))


configuration = Configuration()


def set_configuration(new_configuration):
    global configuration
    configuration = new_configuration
    set_settings(configuration.get_settings())"
438	donghui	0	"import time

from jet_bridge_base.settings import set_settings


class Configuration(object):

    def __init__(self):
        self.init_time = time.time()

    def get_type(self):
        pass

    def get_version(self):
        pass

    def get_model_description(self, db_table):
        pass

    def get_hidden_model_description(self):
        return []

    def get_settings(self):
        pass

    def on_model_pre_create(self, model, pk):
        pass

    def on_model_post_create(self, model, instance):
        pass

    def on_model_pre_update(self, model, instance):
        pass

    def on_model_post_update(self, model, instance):
        pass

    def on_model_pre_delete(self, model, instance):
        pass

    def on_model_post_delete(self, model, instance):
        pass

    def media_get_available_name(self, path):
        pass

    def media_exists(self, path):
        pass

    def media_listdir(self, path):
        pass

    def media_get_modified_time(self, path):
        pass

    def media_open(self, path, mode='rb'):
        pass

    def media_save(self, path, content):
        pass

    def media_delete(self, path):
        pass

    def media_url(self, path, request):
        pass

    def session_set(self, request, name, value, secure=True):
        pass

    def session_get(self, request, name, default=None, decode=True, secure=True):
        pass

    def session_clear(self, request, name):
        pass

    def clean_sso_application_name(self, name):
        return name.lower().replace('-', '')

    def clean_sso_applications(self, applications):
        return dict(map(lambda x: (self.clean_sso_application_name(x[0]), x[1]), applications.items()))


configuration = Configuration()


def set_configuration(new_configuration):
    global configuration
    configuration = new_configuration
    set_settings(configuration.get_settings())"
376	jackson	2	"import _plotly_utils.basevalidators


class FontValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(
        self, plotly_name=""font"", parent_name=""scattersmith.hoverlabel"", **kwargs
    ):
        super(FontValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Font""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            color

            colorsrc
                Sets the source reference on Chart Studio Cloud
                for `color`.
            family
                HTML font family - the typeface that will be
                applied by the web browser. The web browser
                will only be able to apply a font if it is
                available on the system which it operates.
                Provide multiple font families, separated by
                commas, to indicate the preference in which to
                apply fonts if they aren't available on the
                system. The Chart Studio Cloud (at
                https://chart-studio.plotly.com or on-premise)
                generates images on a server, where only a
                select number of fonts are installed and
                supported. These include ""Arial"", ""Balto"",
                ""Courier New"", ""Droid Sans"",, ""Droid Serif"",
                ""Droid Sans Mono"", ""Gravitas One"", ""Old
                Standard TT"", ""Open Sans"", ""Overpass"", ""PT Sans
                Narrow"", ""Raleway"", ""Times New Roman"".
            familysrc
                Sets the source reference on Chart Studio Cloud
                for `family`.
            size

            sizesrc
                Sets the source reference on Chart Studio Cloud
                for `size`.
"""""",
            ),
            **kwargs,
        )"
376	donghui	1	"import _plotly_utils.basevalidators


class FontValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(
        self, plotly_name=""font"", parent_name=""scattersmith.hoverlabel"", **kwargs
    ):
        super(FontValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Font""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            color

            colorsrc
                Sets the source reference on Chart Studio Cloud
                for `color`.
            family
                HTML font family - the typeface that will be
                applied by the web browser. The web browser
                will only be able to apply a font if it is
                available on the system which it operates.
                Provide multiple font families, separated by
                commas, to indicate the preference in which to
                apply fonts if they aren't available on the
                system. The Chart Studio Cloud (at
                https://chart-studio.plotly.com or on-premise)
                generates images on a server, where only a
                select number of fonts are installed and
                supported. These include ""Arial"", ""Balto"",
                ""Courier New"", ""Droid Sans"",, ""Droid Serif"",
                ""Droid Sans Mono"", ""Gravitas One"", ""Old
                Standard TT"", ""Open Sans"", ""Overpass"", ""PT Sans
                Narrow"", ""Raleway"", ""Times New Roman"".
            familysrc
                Sets the source reference on Chart Studio Cloud
                for `family`.
            size

            sizesrc
                Sets the source reference on Chart Studio Cloud
                for `size`.
"""""",
            ),
            **kwargs,
        )"
267	jackson	2	"""""""Basic implementation to support SOAP-Attachments

See https://www.w3.org/TR/SOAP-attachments

""""""

import base64

from cached_property import cached_property
from requests.structures import CaseInsensitiveDict


class MessagePack:
    def __init__(self, parts):
        self._parts = parts

    def __repr__(self):
        return ""<MessagePack(attachments=[%s])>"" % (
            "", "".join(repr(a) for a in self.attachments)
        )

    @property
    def root(self):
        return self._root

    def _set_root(self, root):
        self._root = root

    @cached_property
    def attachments(self):
        """"""Return a list of attachments.

        :rtype: list of Attachment

        """"""
        return [Attachment(part) for part in self._parts]

    def get_by_content_id(self, content_id):
        """"""get_by_content_id

        :param content_id: The content-id to return
        :type content_id: str
        :rtype: Attachment

        """"""
        for attachment in self.attachments:
            if attachment.content_id == content_id:
                return attachment


class Attachment:
    def __init__(self, part):
        encoding = part.encoding or ""utf-8""
        self.headers = CaseInsensitiveDict(
            {k.decode(encoding): v.decode(encoding) for k, v in part.headers.items()}
        )
        self.content_type = self.headers.get(""Content-Type"", None)
        self.content_id = self.headers.get(""Content-ID"", None)
        self.content_location = self.headers.get(""Content-Location"", None)
        self._part = part

    def __repr__(self):
        return ""<Attachment(%r, %r)>"" % (self.content_id, self.content_type)

    @cached_property
    def content(self):
        """"""Return the content of the attachment

        :rtype: bytes or str

        """"""
        encoding = self.headers.get(""Content-Transfer-Encoding"", None)
        content = self._part.content

        if encoding == ""base64"":
            return base64.b64decode(content)
        elif encoding == ""binary"":
            return content.strip(b""\r\n"")
        else:
            return content"
267	donghui	2	"""""""Basic implementation to support SOAP-Attachments

See https://www.w3.org/TR/SOAP-attachments

""""""

import base64

from cached_property import cached_property
from requests.structures import CaseInsensitiveDict


class MessagePack:
    def __init__(self, parts):
        self._parts = parts

    def __repr__(self):
        return ""<MessagePack(attachments=[%s])>"" % (
            "", "".join(repr(a) for a in self.attachments)
        )

    @property
    def root(self):
        return self._root

    def _set_root(self, root):
        self._root = root

    @cached_property
    def attachments(self):
        """"""Return a list of attachments.

        :rtype: list of Attachment

        """"""
        return [Attachment(part) for part in self._parts]

    def get_by_content_id(self, content_id):
        """"""get_by_content_id

        :param content_id: The content-id to return
        :type content_id: str
        :rtype: Attachment

        """"""
        for attachment in self.attachments:
            if attachment.content_id == content_id:
                return attachment


class Attachment:
    def __init__(self, part):
        encoding = part.encoding or ""utf-8""
        self.headers = CaseInsensitiveDict(
            {k.decode(encoding): v.decode(encoding) for k, v in part.headers.items()}
        )
        self.content_type = self.headers.get(""Content-Type"", None)
        self.content_id = self.headers.get(""Content-ID"", None)
        self.content_location = self.headers.get(""Content-Location"", None)
        self._part = part

    def __repr__(self):
        return ""<Attachment(%r, %r)>"" % (self.content_id, self.content_type)

    @cached_property
    def content(self):
        """"""Return the content of the attachment

        :rtype: bytes or str

        """"""
        encoding = self.headers.get(""Content-Transfer-Encoding"", None)
        content = self._part.content

        if encoding == ""base64"":
            return base64.b64decode(content)
        elif encoding == ""binary"":
            return content.strip(b""\r\n"")
        else:
            return content"
327	jackson	1	"######################## BEGIN LICENSE BLOCK ########################
# The Original Code is mozilla.org code.
#
# The Initial Developer of the Original Code is
# Netscape Communications Corporation.
# Portions created by the Initial Developer are Copyright (C) 1998
# the Initial Developer. All Rights Reserved.
#
# Contributor(s):
#   Mark Pilgrim - port to Python
#
# This library is free software; you can redistribute it and/or
# modify it under the terms of the GNU Lesser General Public
# License as published by the Free Software Foundation; either
# version 2.1 of the License, or (at your option) any later version.
#
# This library is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public
# License along with this library; if not, write to the Free Software
# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA
# 02110-1301  USA
######################### END LICENSE BLOCK #########################

from .chardistribution import EUCKRDistributionAnalysis
from .codingstatemachine import CodingStateMachine
from .mbcharsetprober import MultiByteCharSetProber
from .mbcssm import CP949_SM_MODEL


class CP949Prober(MultiByteCharSetProber):
    def __init__(self) -> None:
        super().__init__()
        self.coding_sm = CodingStateMachine(CP949_SM_MODEL)
        # NOTE: CP949 is a superset of EUC-KR, so the distribution should be
        #       not different.
        self.distribution_analyzer = EUCKRDistributionAnalysis()
        self.reset()

    @property
    def charset_name(self) -> str:
        return ""CP949""

    @property
    def language(self) -> str:
        return ""Korean"""
327	donghui	1	"######################## BEGIN LICENSE BLOCK ########################
# The Original Code is mozilla.org code.
#
# The Initial Developer of the Original Code is
# Netscape Communications Corporation.
# Portions created by the Initial Developer are Copyright (C) 1998
# the Initial Developer. All Rights Reserved.
#
# Contributor(s):
#   Mark Pilgrim - port to Python
#
# This library is free software; you can redistribute it and/or
# modify it under the terms of the GNU Lesser General Public
# License as published by the Free Software Foundation; either
# version 2.1 of the License, or (at your option) any later version.
#
# This library is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public
# License along with this library; if not, write to the Free Software
# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA
# 02110-1301  USA
######################### END LICENSE BLOCK #########################

from .chardistribution import EUCKRDistributionAnalysis
from .codingstatemachine import CodingStateMachine
from .mbcharsetprober import MultiByteCharSetProber
from .mbcssm import CP949_SM_MODEL


class CP949Prober(MultiByteCharSetProber):
    def __init__(self) -> None:
        super().__init__()
        self.coding_sm = CodingStateMachine(CP949_SM_MODEL)
        # NOTE: CP949 is a superset of EUC-KR, so the distribution should be
        #       not different.
        self.distribution_analyzer = EUCKRDistributionAnalysis()
        self.reset()

    @property
    def charset_name(self) -> str:
        return ""CP949""

    @property
    def language(self) -> str:
        return ""Korean"""
294	jackson	2	"# Copyright 2021 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import os

from google.cloud import workflows_v1beta

import main

PROJECT = os.environ[""GOOGLE_CLOUD_PROJECT""]
LOCATION = ""us-central1""
WORKFLOW_ID = ""myFirstWorkflow""


def test_workflow_execution():
    assert PROJECT != """"

    if not workflow_exists():
        workflow_file = open(""myFirstWorkflow.workflows.yaml"", ""r"").read()

        workflows_client = workflows_v1beta.WorkflowsClient()
        workflows_client.create_workflow(request={
            # Manually construct the location
            # https://github.com/googleapis/python-workflows/issues/21
            ""parent"": f'projects/{PROJECT}/locations/{LOCATION}',
            ""workflow_id"": WORKFLOW_ID,
            ""workflow"": {
                ""name"": WORKFLOW_ID,
                ""source_contents"": workflow_file
            }
        })

    result = main.execute_workflow(PROJECT)
    assert len(result) > 0


def workflow_exists():
    """"""Returns True if the workflow exists in this project
    """"""
    try:
        workflows_client = workflows_v1beta.WorkflowsClient()
        workflow_name = workflows_client.workflow_path(PROJECT, LOCATION, WORKFLOW_ID)
        workflows_client.get_workflow(request={""name"": workflow_name})
        return True
    except Exception as e:
        print(f""Workflow doesn't exist: {e}"")
        return False"
294	donghui	1	"# Copyright 2021 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import os

from google.cloud import workflows_v1beta

import main

PROJECT = os.environ[""GOOGLE_CLOUD_PROJECT""]
LOCATION = ""us-central1""
WORKFLOW_ID = ""myFirstWorkflow""


def test_workflow_execution():
    assert PROJECT != """"

    if not workflow_exists():
        workflow_file = open(""myFirstWorkflow.workflows.yaml"", ""r"").read()

        workflows_client = workflows_v1beta.WorkflowsClient()
        workflows_client.create_workflow(request={
            # Manually construct the location
            # https://github.com/googleapis/python-workflows/issues/21
            ""parent"": f'projects/{PROJECT}/locations/{LOCATION}',
            ""workflow_id"": WORKFLOW_ID,
            ""workflow"": {
                ""name"": WORKFLOW_ID,
                ""source_contents"": workflow_file
            }
        })

    result = main.execute_workflow(PROJECT)
    assert len(result) > 0


def workflow_exists():
    """"""Returns True if the workflow exists in this project
    """"""
    try:
        workflows_client = workflows_v1beta.WorkflowsClient()
        workflow_name = workflows_client.workflow_path(PROJECT, LOCATION, WORKFLOW_ID)
        workflows_client.get_workflow(request={""name"": workflow_name})
        return True
    except Exception as e:
        print(f""Workflow doesn't exist: {e}"")
        return False"
385	jackson	2	"import _plotly_utils.basevalidators


class FontValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(
        self, plotly_name=""font"", parent_name=""histogram2dcontour.hoverlabel"", **kwargs
    ):
        super(FontValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Font""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            color

            colorsrc
                Sets the source reference on Chart Studio Cloud
                for `color`.
            family
                HTML font family - the typeface that will be
                applied by the web browser. The web browser
                will only be able to apply a font if it is
                available on the system which it operates.
                Provide multiple font families, separated by
                commas, to indicate the preference in which to
                apply fonts if they aren't available on the
                system. The Chart Studio Cloud (at
                https://chart-studio.plotly.com or on-premise)
                generates images on a server, where only a
                select number of fonts are installed and
                supported. These include ""Arial"", ""Balto"",
                ""Courier New"", ""Droid Sans"",, ""Droid Serif"",
                ""Droid Sans Mono"", ""Gravitas One"", ""Old
                Standard TT"", ""Open Sans"", ""Overpass"", ""PT Sans
                Narrow"", ""Raleway"", ""Times New Roman"".
            familysrc
                Sets the source reference on Chart Studio Cloud
                for `family`.
            size

            sizesrc
                Sets the source reference on Chart Studio Cloud
                for `size`.
"""""",
            ),
            **kwargs,
        )"
385	donghui	1	"import _plotly_utils.basevalidators


class FontValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(
        self, plotly_name=""font"", parent_name=""histogram2dcontour.hoverlabel"", **kwargs
    ):
        super(FontValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Font""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            color

            colorsrc
                Sets the source reference on Chart Studio Cloud
                for `color`.
            family
                HTML font family - the typeface that will be
                applied by the web browser. The web browser
                will only be able to apply a font if it is
                available on the system which it operates.
                Provide multiple font families, separated by
                commas, to indicate the preference in which to
                apply fonts if they aren't available on the
                system. The Chart Studio Cloud (at
                https://chart-studio.plotly.com or on-premise)
                generates images on a server, where only a
                select number of fonts are installed and
                supported. These include ""Arial"", ""Balto"",
                ""Courier New"", ""Droid Sans"",, ""Droid Serif"",
                ""Droid Sans Mono"", ""Gravitas One"", ""Old
                Standard TT"", ""Open Sans"", ""Overpass"", ""PT Sans
                Narrow"", ""Raleway"", ""Times New Roman"".
            familysrc
                Sets the source reference on Chart Studio Cloud
                for `family`.
            size

            sizesrc
                Sets the source reference on Chart Studio Cloud
                for `size`.
"""""",
            ),
            **kwargs,
        )"
313	jackson	2	"import _plotly_utils.basevalidators


class FontValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(
        self, plotly_name=""font"", parent_name=""scatterpolar.hoverlabel"", **kwargs
    ):
        super(FontValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Font""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            color

            colorsrc
                Sets the source reference on Chart Studio Cloud
                for `color`.
            family
                HTML font family - the typeface that will be
                applied by the web browser. The web browser
                will only be able to apply a font if it is
                available on the system which it operates.
                Provide multiple font families, separated by
                commas, to indicate the preference in which to
                apply fonts if they aren't available on the
                system. The Chart Studio Cloud (at
                https://chart-studio.plotly.com or on-premise)
                generates images on a server, where only a
                select number of fonts are installed and
                supported. These include ""Arial"", ""Balto"",
                ""Courier New"", ""Droid Sans"",, ""Droid Serif"",
                ""Droid Sans Mono"", ""Gravitas One"", ""Old
                Standard TT"", ""Open Sans"", ""Overpass"", ""PT Sans
                Narrow"", ""Raleway"", ""Times New Roman"".
            familysrc
                Sets the source reference on Chart Studio Cloud
                for `family`.
            size

            sizesrc
                Sets the source reference on Chart Studio Cloud
                for `size`.
"""""",
            ),
            **kwargs,
        )"
313	donghui	1	"import _plotly_utils.basevalidators


class FontValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(
        self, plotly_name=""font"", parent_name=""scatterpolar.hoverlabel"", **kwargs
    ):
        super(FontValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Font""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            color

            colorsrc
                Sets the source reference on Chart Studio Cloud
                for `color`.
            family
                HTML font family - the typeface that will be
                applied by the web browser. The web browser
                will only be able to apply a font if it is
                available on the system which it operates.
                Provide multiple font families, separated by
                commas, to indicate the preference in which to
                apply fonts if they aren't available on the
                system. The Chart Studio Cloud (at
                https://chart-studio.plotly.com or on-premise)
                generates images on a server, where only a
                select number of fonts are installed and
                supported. These include ""Arial"", ""Balto"",
                ""Courier New"", ""Droid Sans"",, ""Droid Serif"",
                ""Droid Sans Mono"", ""Gravitas One"", ""Old
                Standard TT"", ""Open Sans"", ""Overpass"", ""PT Sans
                Narrow"", ""Raleway"", ""Times New Roman"".
            familysrc
                Sets the source reference on Chart Studio Cloud
                for `family`.
            size

            sizesrc
                Sets the source reference on Chart Studio Cloud
                for `size`.
"""""",
            ),
            **kwargs,
        )"
342	jackson	0	"from playwright.sync_api import sync_playwright
import pandas as pd
import numpy as np

def pagina_min_fazenda(cidade, estado, pagina):
    pagina.goto(""https://www.airbnb.com.br/"")


    #FAZENDO A PESQUISA 
    pagina.locator('''xpath = /html/body/div[5]/div/div/div[1]/div/div[2]/div/div/div/div/div/div[2]/div/div/div/
    header/div/div[2]/div[1]/div/button[1]''').click()
    pagina.locator('''xpath = /html/body/div[5]/div/div/div[1]/div/div[2]/div/div/div/div/div/div[2]/div/div/div/
    header/div/div[2]/div[2]/div/div/div/form/div[2]/div/div[1]/div/label/div/input''').fill(cidade + ', ' + estado)
    if cidade in pagina.locator('''/html/body/div[5]/div/div/div[1]/div/div[2]/div/div/div/div/div/div[2]/div/div/div/header/div/div[2]/div[2]/div/div/div/form/div[2]/div/div[1]/div/div[2]/div''').inner_text():
        pagina.locator('''xpath = /html/body/div[5]/div/div/div[1]/div/div[2]/div/div/div/div/div/div[2]/div/div/div/header/div/div[2]/div[2]/div/div/div/form/div[2]/div/div[1]/div/div[2]/div''').locator('nth = 0').click()
        pagina.locator('''xpath = /html/body/div[5]/div/div/div[1]/div/div[2]/div/div/div/div/div/div[2]/div/div/div/header/div/div[2]/div[2]/div/div/div/form/div[2]/div/div[3]/div[2]/div/div/section/div/div/div/div/div[1]/div/div[1]/div/button[2]''').click()
        pagina.locator('''xpath = /html/body/div[5]/div/div/div[1]/div/div[2]/div/div/div/div/div/div[2]/div/div/div/header/div/div[2]/div[2]/div/div/div/form/div[2]/div/div[3]/div[2]/div/div/section/div/div/div/div/div[2]/div[2]/div/div[1]/div[2]/div[2]/label''').click()
        pagina.locator('''xpath = /html/body/div[5]/div/div/div[1]/div/div[2]/div/div/div/div/div/div[2]/div/div/div/header/div/div[2]/div[2]/div/div/div/form/div[2]/div/div[5]/div[1]/div[2]/button/div/div[1]/svg''').click()


    return 
with sync_playwright() as p:
    
    navegador = p.chromium.launch(headless = False)
    pagina = navegador.new_page(viewport = {'width': 1200, 'height': 800})"
342	donghui	0	"from playwright.sync_api import sync_playwright
import pandas as pd
import numpy as np

def pagina_min_fazenda(cidade, estado, pagina):
    pagina.goto(""https://www.airbnb.com.br/"")


    #FAZENDO A PESQUISA 
    pagina.locator('''xpath = /html/body/div[5]/div/div/div[1]/div/div[2]/div/div/div/div/div/div[2]/div/div/div/
    header/div/div[2]/div[1]/div/button[1]''').click()
    pagina.locator('''xpath = /html/body/div[5]/div/div/div[1]/div/div[2]/div/div/div/div/div/div[2]/div/div/div/
    header/div/div[2]/div[2]/div/div/div/form/div[2]/div/div[1]/div/label/div/input''').fill(cidade + ', ' + estado)
    if cidade in pagina.locator('''/html/body/div[5]/div/div/div[1]/div/div[2]/div/div/div/div/div/div[2]/div/div/div/header/div/div[2]/div[2]/div/div/div/form/div[2]/div/div[1]/div/div[2]/div''').inner_text():
        pagina.locator('''xpath = /html/body/div[5]/div/div/div[1]/div/div[2]/div/div/div/div/div/div[2]/div/div/div/header/div/div[2]/div[2]/div/div/div/form/div[2]/div/div[1]/div/div[2]/div''').locator('nth = 0').click()
        pagina.locator('''xpath = /html/body/div[5]/div/div/div[1]/div/div[2]/div/div/div/div/div/div[2]/div/div/div/header/div/div[2]/div[2]/div/div/div/form/div[2]/div/div[3]/div[2]/div/div/section/div/div/div/div/div[1]/div/div[1]/div/button[2]''').click()
        pagina.locator('''xpath = /html/body/div[5]/div/div/div[1]/div/div[2]/div/div/div/div/div/div[2]/div/div/div/header/div/div[2]/div[2]/div/div/div/form/div[2]/div/div[3]/div[2]/div/div/section/div/div/div/div/div[2]/div[2]/div/div[1]/div[2]/div[2]/label''').click()
        pagina.locator('''xpath = /html/body/div[5]/div/div/div[1]/div/div[2]/div/div/div/div/div/div[2]/div/div/div/header/div/div[2]/div[2]/div/div/div/form/div[2]/div/div[5]/div[1]/div[2]/button/div/div[1]/svg''').click()


    return 
with sync_playwright() as p:
    
    navegador = p.chromium.launch(headless = False)
    pagina = navegador.new_page(viewport = {'width': 1200, 'height': 800})"
353	jackson	0	"import json
import os
import time
import random
from linkedin_api import Linkedin
from dotenv import load_dotenv
import sys

class Scrape():
    def __init__(self, api):
        self.api = api
        
    def read_json(self, filename='jobs.json'):
        with open(filename, 'r') as file:
            return json.load(file)

    def write_json(self, newData):
        with open('jobs.json', 'r+') as file:
            data = self.read_json()
            data['job-list'].append(newData)
            json.dump(data, file)

    def searchJobs(self, apiChosen, numberOfSearches, keywordChosen, offsetNumber):
        jobs = apiChosen.search_jobs(keywordChosen, remote = 1, limit = \
                            numberOfSearches, offset = offsetNumber)
        for job in jobs:
            title = job['title']
            jobID = job['dashEntityUrn'].split(':')[-1] 
            location = job['formattedLocation']
            #jobDetails = api.get_job(jobID)
            jobLink = f'https://www.linkedin.com/jobs/view/{jobID}/'
            job = {
                ""Job title"":title,
                ""Job link"":jobLink,
                ""Location"":location
            }
            print(f""{title} : {jobID} : {location}"")
            self.write_json(job)
        

    def findSWEJobs(self, apiChosen):
        listOfJobs = [""Software Developer"",""Software Engineer"", ""Software Intern"",""SDET"",""Developer Intern"",""Software co-op"",""Junior Developer""] 
        for i in range(11,101):
            
            for element in listOfJobs:
                self.searchJobs(apiChosen, 1, element, i)
                time.sleep(1*random.randint(1,5)+2)
    
if(__name__ == ""__main__""):
    load_dotenv()
    Password = os.getenv('PASSWORD')
    Email = os.getenv('EMAIL')
    api = Linkedin(Email, Password)
    data = {""job-list"": [{}]}
    with open('jobs.json', 'w') as file:
        json.dump(data, file)

    scraper = Scrape(api)
    scraper.findSWEJobs(api)
    
    "
353	donghui	0	"import json
import os
import time
import random
from linkedin_api import Linkedin
from dotenv import load_dotenv
import sys

class Scrape():
    def __init__(self, api):
        self.api = api
        
    def read_json(self, filename='jobs.json'):
        with open(filename, 'r') as file:
            return json.load(file)

    def write_json(self, newData):
        with open('jobs.json', 'r+') as file:
            data = self.read_json()
            data['job-list'].append(newData)
            json.dump(data, file)

    def searchJobs(self, apiChosen, numberOfSearches, keywordChosen, offsetNumber):
        jobs = apiChosen.search_jobs(keywordChosen, remote = 1, limit = \
                            numberOfSearches, offset = offsetNumber)
        for job in jobs:
            title = job['title']
            jobID = job['dashEntityUrn'].split(':')[-1] 
            location = job['formattedLocation']
            #jobDetails = api.get_job(jobID)
            jobLink = f'https://www.linkedin.com/jobs/view/{jobID}/'
            job = {
                ""Job title"":title,
                ""Job link"":jobLink,
                ""Location"":location
            }
            print(f""{title} : {jobID} : {location}"")
            self.write_json(job)
        

    def findSWEJobs(self, apiChosen):
        listOfJobs = [""Software Developer"",""Software Engineer"", ""Software Intern"",""SDET"",""Developer Intern"",""Software co-op"",""Junior Developer""] 
        for i in range(11,101):
            
            for element in listOfJobs:
                self.searchJobs(apiChosen, 1, element, i)
                time.sleep(1*random.randint(1,5)+2)
    
if(__name__ == ""__main__""):
    load_dotenv()
    Password = os.getenv('PASSWORD')
    Email = os.getenv('EMAIL')
    api = Linkedin(Email, Password)
    data = {""job-list"": [{}]}
    with open('jobs.json', 'w') as file:
        json.dump(data, file)

    scraper = Scrape(api)
    scraper.findSWEJobs(api)
    
    "
302	jackson	3	"from rx.core import Observable
from rx.internal import extensionmethod
import math


def determine_median(sorted_list):
    if len(sorted_list) == 0:
        raise Exception(""The input sequence was empty"")

    if len(sorted_list) % 2 == 1:
        return sorted_list[int((len(sorted_list) + 1) / 2) - 1]
    else:
        median_1 = sorted_list[int((len(sorted_list) + 1) / 2) - 1]
        median_2 = sorted_list[int((len(sorted_list) + 1) / 2)]
        return float(median_1 + median_2) / 2.0


@extensionmethod(Observable)
def median(self):
    """"""
    Calculates the statistical median on numerical emissions. The sequence must be finite.
    """"""
    return self.to_sorted_list().map(lambda l: determine_median(l))


@extensionmethod(Observable)
def mode(self):
    """"""
    Returns the most frequently emitted value (or ""values"" if they have the same number of occurrences).
    The sequence must be finite.
    """"""
    return self.group_by(lambda v: v) \
        .flat_map(lambda grp: grp.count().map(lambda ct: (grp.key, ct))) \
        .to_sorted_list(lambda t: t[1], reverse=True) \
        .flat_map(lambda l: Observable.from_(l).take_while(lambda t: t[1] == l[0][1])) \
        .map(lambda t: t[0])


@extensionmethod(Observable)
def variance(self):
    """"""
    Returns the statistical variance of the numerical emissions.
    The sequence must be finite.
    """"""
    squared_values = self.to_list() \
        .flat_map(lambda l: Observable.from_(l).average().flat_map(lambda avg: Observable.from_(l).map(lambda i: i - avg))) \
        .map(lambda i: i * i) \
        .publish() \
        .auto_connect(2)

    return Observable.zip(squared_values.sum(), squared_values.count(), lambda sum, ct: sum / (ct - 1))


@extensionmethod(Observable)
def standard_deviation(self):
    """"""
    Returns the standard deviation of the numerical emissions:
    The sequence must be finite.
    """"""
    return self.variance().map(lambda i: math.sqrt(i))

"
302	donghui	2	"from rx.core import Observable
from rx.internal import extensionmethod
import math


def determine_median(sorted_list):
    if len(sorted_list) == 0:
        raise Exception(""The input sequence was empty"")

    if len(sorted_list) % 2 == 1:
        return sorted_list[int((len(sorted_list) + 1) / 2) - 1]
    else:
        median_1 = sorted_list[int((len(sorted_list) + 1) / 2) - 1]
        median_2 = sorted_list[int((len(sorted_list) + 1) / 2)]
        return float(median_1 + median_2) / 2.0


@extensionmethod(Observable)
def median(self):
    """"""
    Calculates the statistical median on numerical emissions. The sequence must be finite.
    """"""
    return self.to_sorted_list().map(lambda l: determine_median(l))


@extensionmethod(Observable)
def mode(self):
    """"""
    Returns the most frequently emitted value (or ""values"" if they have the same number of occurrences).
    The sequence must be finite.
    """"""
    return self.group_by(lambda v: v) \
        .flat_map(lambda grp: grp.count().map(lambda ct: (grp.key, ct))) \
        .to_sorted_list(lambda t: t[1], reverse=True) \
        .flat_map(lambda l: Observable.from_(l).take_while(lambda t: t[1] == l[0][1])) \
        .map(lambda t: t[0])


@extensionmethod(Observable)
def variance(self):
    """"""
    Returns the statistical variance of the numerical emissions.
    The sequence must be finite.
    """"""
    squared_values = self.to_list() \
        .flat_map(lambda l: Observable.from_(l).average().flat_map(lambda avg: Observable.from_(l).map(lambda i: i - avg))) \
        .map(lambda i: i * i) \
        .publish() \
        .auto_connect(2)

    return Observable.zip(squared_values.sum(), squared_values.count(), lambda sum, ct: sum / (ct - 1))


@extensionmethod(Observable)
def standard_deviation(self):
    """"""
    Returns the standard deviation of the numerical emissions:
    The sequence must be finite.
    """"""
    return self.variance().map(lambda i: math.sqrt(i))

"
394	jackson	2	"import sys
from dataclasses import dataclass


@dataclass
class WindowsConsoleFeatures:
    """"""Windows features available.""""""

    vt: bool = False
    """"""The console supports VT codes.""""""
    truecolor: bool = False
    """"""The console supports truecolor.""""""


try:
    import ctypes
    from ctypes import LibraryLoader

    if sys.platform == ""win32"":
        windll = LibraryLoader(ctypes.WinDLL)
    else:
        windll = None
        raise ImportError(""Not windows"")

    from pip._vendor.rich._win32_console import (
        ENABLE_VIRTUAL_TERMINAL_PROCESSING,
        GetConsoleMode,
        GetStdHandle,
        LegacyWindowsError,
    )

except (AttributeError, ImportError, ValueError):

    # Fallback if we can't load the Windows DLL
    def get_windows_console_features() -> WindowsConsoleFeatures:
        features = WindowsConsoleFeatures()
        return features

else:

    def get_windows_console_features() -> WindowsConsoleFeatures:
        """"""Get windows console features.

        Returns:
            WindowsConsoleFeatures: An instance of WindowsConsoleFeatures.
        """"""
        handle = GetStdHandle()
        try:
            console_mode = GetConsoleMode(handle)
            success = True
        except LegacyWindowsError:
            console_mode = 0
            success = False
        vt = bool(success and console_mode & ENABLE_VIRTUAL_TERMINAL_PROCESSING)
        truecolor = False
        if vt:
            win_version = sys.getwindowsversion()
            truecolor = win_version.major > 10 or (
                win_version.major == 10 and win_version.build >= 15063
            )
        features = WindowsConsoleFeatures(vt=vt, truecolor=truecolor)
        return features


if __name__ == ""__main__"":
    import platform

    features = get_windows_console_features()
    from pip._vendor.rich import print

    print(f'platform=""{platform.system()}""')
    print(repr(features))"
394	donghui	2	"import sys
from dataclasses import dataclass


@dataclass
class WindowsConsoleFeatures:
    """"""Windows features available.""""""

    vt: bool = False
    """"""The console supports VT codes.""""""
    truecolor: bool = False
    """"""The console supports truecolor.""""""


try:
    import ctypes
    from ctypes import LibraryLoader

    if sys.platform == ""win32"":
        windll = LibraryLoader(ctypes.WinDLL)
    else:
        windll = None
        raise ImportError(""Not windows"")

    from pip._vendor.rich._win32_console import (
        ENABLE_VIRTUAL_TERMINAL_PROCESSING,
        GetConsoleMode,
        GetStdHandle,
        LegacyWindowsError,
    )

except (AttributeError, ImportError, ValueError):

    # Fallback if we can't load the Windows DLL
    def get_windows_console_features() -> WindowsConsoleFeatures:
        features = WindowsConsoleFeatures()
        return features

else:

    def get_windows_console_features() -> WindowsConsoleFeatures:
        """"""Get windows console features.

        Returns:
            WindowsConsoleFeatures: An instance of WindowsConsoleFeatures.
        """"""
        handle = GetStdHandle()
        try:
            console_mode = GetConsoleMode(handle)
            success = True
        except LegacyWindowsError:
            console_mode = 0
            success = False
        vt = bool(success and console_mode & ENABLE_VIRTUAL_TERMINAL_PROCESSING)
        truecolor = False
        if vt:
            win_version = sys.getwindowsversion()
            truecolor = win_version.major > 10 or (
                win_version.major == 10 and win_version.build >= 15063
            )
        features = WindowsConsoleFeatures(vt=vt, truecolor=truecolor)
        return features


if __name__ == ""__main__"":
    import platform

    features = get_windows_console_features()
    from pip._vendor.rich import print

    print(f'platform=""{platform.system()}""')
    print(repr(features))"
285	jackson	1	"import sys


def patch_sys_module():

    def patched_exc_info(fun):

        def pydev_debugger_exc_info():
            type, value, traceback = fun()
            if type == ImportError:
                # we should not show frame added by plugin_import call
                if traceback and hasattr(traceback, ""tb_next""):
                    return type, value, traceback.tb_next
            return type, value, traceback

        return pydev_debugger_exc_info

    system_exc_info = sys.exc_info
    sys.exc_info = patched_exc_info(system_exc_info)
    if not hasattr(sys, ""system_exc_info""):
        sys.system_exc_info = system_exc_info


def patched_reload(orig_reload):

    def pydev_debugger_reload(module):
        orig_reload(module)
        if module.__name__ == ""sys"":
            # if sys module was reloaded we should patch it again
            patch_sys_module()

    return pydev_debugger_reload


def patch_reload():
    import builtins  # Py3

    if hasattr(builtins, ""reload""):
        sys.builtin_orig_reload = builtins.reload
        builtins.reload = patched_reload(sys.builtin_orig_reload)  # @UndefinedVariable
        try:
            import imp
            sys.imp_orig_reload = imp.reload
            imp.reload = patched_reload(sys.imp_orig_reload)  # @UndefinedVariable
        except:
            pass
    else:
        try:
            import importlib
            sys.importlib_orig_reload = importlib.reload  # @UndefinedVariable
            importlib.reload = patched_reload(sys.importlib_orig_reload)  # @UndefinedVariable
        except:
            pass

    del builtins


def cancel_patches_in_sys_module():
    sys.exc_info = sys.system_exc_info  # @UndefinedVariable
    import builtins  # Py3

    if hasattr(sys, ""builtin_orig_reload""):
        builtins.reload = sys.builtin_orig_reload

    if hasattr(sys, ""imp_orig_reload""):
        import imp
        imp.reload = sys.imp_orig_reload

    if hasattr(sys, ""importlib_orig_reload""):
        import importlib
        importlib.reload = sys.importlib_orig_reload

    del builtins"
285	donghui	1	"import sys


def patch_sys_module():

    def patched_exc_info(fun):

        def pydev_debugger_exc_info():
            type, value, traceback = fun()
            if type == ImportError:
                # we should not show frame added by plugin_import call
                if traceback and hasattr(traceback, ""tb_next""):
                    return type, value, traceback.tb_next
            return type, value, traceback

        return pydev_debugger_exc_info

    system_exc_info = sys.exc_info
    sys.exc_info = patched_exc_info(system_exc_info)
    if not hasattr(sys, ""system_exc_info""):
        sys.system_exc_info = system_exc_info


def patched_reload(orig_reload):

    def pydev_debugger_reload(module):
        orig_reload(module)
        if module.__name__ == ""sys"":
            # if sys module was reloaded we should patch it again
            patch_sys_module()

    return pydev_debugger_reload


def patch_reload():
    import builtins  # Py3

    if hasattr(builtins, ""reload""):
        sys.builtin_orig_reload = builtins.reload
        builtins.reload = patched_reload(sys.builtin_orig_reload)  # @UndefinedVariable
        try:
            import imp
            sys.imp_orig_reload = imp.reload
            imp.reload = patched_reload(sys.imp_orig_reload)  # @UndefinedVariable
        except:
            pass
    else:
        try:
            import importlib
            sys.importlib_orig_reload = importlib.reload  # @UndefinedVariable
            importlib.reload = patched_reload(sys.importlib_orig_reload)  # @UndefinedVariable
        except:
            pass

    del builtins


def cancel_patches_in_sys_module():
    sys.exc_info = sys.system_exc_info  # @UndefinedVariable
    import builtins  # Py3

    if hasattr(sys, ""builtin_orig_reload""):
        builtins.reload = sys.builtin_orig_reload

    if hasattr(sys, ""imp_orig_reload""):
        import imp
        imp.reload = sys.imp_orig_reload

    if hasattr(sys, ""importlib_orig_reload""):
        import importlib
        importlib.reload = sys.importlib_orig_reload

    del builtins"
336	jackson	2	"from fontTools.pens.basePen import BasePen
from reportlab.graphics.shapes import Path


__all__ = [""ReportLabPen""]


class ReportLabPen(BasePen):

    """"""A pen for drawing onto a ``reportlab.graphics.shapes.Path`` object.""""""

    def __init__(self, glyphSet, path=None):
        BasePen.__init__(self, glyphSet)
        if path is None:
            path = Path()
        self.path = path

    def _moveTo(self, p):
        (x, y) = p
        self.path.moveTo(x, y)

    def _lineTo(self, p):
        (x, y) = p
        self.path.lineTo(x, y)

    def _curveToOne(self, p1, p2, p3):
        (x1, y1) = p1
        (x2, y2) = p2
        (x3, y3) = p3
        self.path.curveTo(x1, y1, x2, y2, x3, y3)

    def _closePath(self):
        self.path.closePath()


if __name__ == ""__main__"":
    import sys

    if len(sys.argv) < 3:
        print(
            ""Usage: reportLabPen.py <OTF/TTF font> <glyphname> [<image file to create>]""
        )
        print(
            ""  If no image file name is created, by default <glyphname>.png is created.""
        )
        print(""  example: reportLabPen.py Arial.TTF R test.png"")
        print(
            ""  (The file format will be PNG, regardless of the image file name supplied)""
        )
        sys.exit(0)

    from fontTools.ttLib import TTFont
    from reportlab.lib import colors

    path = sys.argv[1]
    glyphName = sys.argv[2]
    if len(sys.argv) > 3:
        imageFile = sys.argv[3]
    else:
        imageFile = ""%s.png"" % glyphName

    font = TTFont(path)  # it would work just as well with fontTools.t1Lib.T1Font
    gs = font.getGlyphSet()
    pen = ReportLabPen(gs, Path(fillColor=colors.red, strokeWidth=5))
    g = gs[glyphName]
    g.draw(pen)

    w, h = g.width, 1000
    from reportlab.graphics import renderPM
    from reportlab.graphics.shapes import Group, Drawing, scale

    # Everything is wrapped in a group to allow transformations.
    g = Group(pen.path)
    g.translate(0, 200)
    g.scale(0.3, 0.3)

    d = Drawing(w, h)
    d.add(g)

    renderPM.drawToFile(d, imageFile, fmt=""PNG"")"
336	donghui	2	"from fontTools.pens.basePen import BasePen
from reportlab.graphics.shapes import Path


__all__ = [""ReportLabPen""]


class ReportLabPen(BasePen):

    """"""A pen for drawing onto a ``reportlab.graphics.shapes.Path`` object.""""""

    def __init__(self, glyphSet, path=None):
        BasePen.__init__(self, glyphSet)
        if path is None:
            path = Path()
        self.path = path

    def _moveTo(self, p):
        (x, y) = p
        self.path.moveTo(x, y)

    def _lineTo(self, p):
        (x, y) = p
        self.path.lineTo(x, y)

    def _curveToOne(self, p1, p2, p3):
        (x1, y1) = p1
        (x2, y2) = p2
        (x3, y3) = p3
        self.path.curveTo(x1, y1, x2, y2, x3, y3)

    def _closePath(self):
        self.path.closePath()


if __name__ == ""__main__"":
    import sys

    if len(sys.argv) < 3:
        print(
            ""Usage: reportLabPen.py <OTF/TTF font> <glyphname> [<image file to create>]""
        )
        print(
            ""  If no image file name is created, by default <glyphname>.png is created.""
        )
        print(""  example: reportLabPen.py Arial.TTF R test.png"")
        print(
            ""  (The file format will be PNG, regardless of the image file name supplied)""
        )
        sys.exit(0)

    from fontTools.ttLib import TTFont
    from reportlab.lib import colors

    path = sys.argv[1]
    glyphName = sys.argv[2]
    if len(sys.argv) > 3:
        imageFile = sys.argv[3]
    else:
        imageFile = ""%s.png"" % glyphName

    font = TTFont(path)  # it would work just as well with fontTools.t1Lib.T1Font
    gs = font.getGlyphSet()
    pen = ReportLabPen(gs, Path(fillColor=colors.red, strokeWidth=5))
    g = gs[glyphName]
    g.draw(pen)

    w, h = g.width, 1000
    from reportlab.graphics import renderPM
    from reportlab.graphics.shapes import Group, Drawing, scale

    # Everything is wrapped in a group to allow transformations.
    g = Group(pen.path)
    g.translate(0, 200)
    g.scale(0.3, 0.3)

    d = Drawing(w, h)
    d.add(g)

    renderPM.drawToFile(d, imageFile, fmt=""PNG"")"
276	jackson	4	"# Kills a process by process name
#
# Uses the Performance Data Helper to locate the PID, then kills it.
# Will only kill the process if there is only one process of that name
# (eg, attempting to kill ""Python.exe"" will only work if there is only
# one Python.exe running.  (Note that the current process does not
# count - ie, if Python.exe is hosting this script, you can still kill
# another Python.exe (as long as there is only one other Python.exe)

# Really just a demo for the win32pdh(util) module, which allows you
# to get all sorts of information about a running process and many
# other aspects of your system.

import win32api, win32pdhutil, win32con, sys


def killProcName(procname):
    # Change suggested by Dan Knierim, who found that this performed a
    # ""refresh"", allowing us to kill processes created since this was run
    # for the first time.
    try:
        win32pdhutil.GetPerformanceAttributes(""Process"", ""ID Process"", procname)
    except:
        pass

    pids = win32pdhutil.FindPerformanceAttributesByName(procname)

    # If _my_ pid in there, remove it!
    try:
        pids.remove(win32api.GetCurrentProcessId())
    except ValueError:
        pass

    if len(pids) == 0:
        result = ""Can't find %s"" % procname
    elif len(pids) > 1:
        result = ""Found too many %s's - pids=`%s`"" % (procname, pids)
    else:
        handle = win32api.OpenProcess(win32con.PROCESS_TERMINATE, 0, pids[0])
        win32api.TerminateProcess(handle, 0)
        win32api.CloseHandle(handle)
        result = """"

    return result


if __name__ == ""__main__"":
    if len(sys.argv) > 1:
        for procname in sys.argv[1:]:
            result = killProcName(procname)
            if result:
                print(result)
                print(""Dumping all processes..."")
                win32pdhutil.ShowAllProcesses()
            else:
                print(""Killed %s"" % procname)
    else:
        print(""Usage: killProcName.py procname ..."")"
276	donghui	4	"# Kills a process by process name
#
# Uses the Performance Data Helper to locate the PID, then kills it.
# Will only kill the process if there is only one process of that name
# (eg, attempting to kill ""Python.exe"" will only work if there is only
# one Python.exe running.  (Note that the current process does not
# count - ie, if Python.exe is hosting this script, you can still kill
# another Python.exe (as long as there is only one other Python.exe)

# Really just a demo for the win32pdh(util) module, which allows you
# to get all sorts of information about a running process and many
# other aspects of your system.

import win32api, win32pdhutil, win32con, sys


def killProcName(procname):
    # Change suggested by Dan Knierim, who found that this performed a
    # ""refresh"", allowing us to kill processes created since this was run
    # for the first time.
    try:
        win32pdhutil.GetPerformanceAttributes(""Process"", ""ID Process"", procname)
    except:
        pass

    pids = win32pdhutil.FindPerformanceAttributesByName(procname)

    # If _my_ pid in there, remove it!
    try:
        pids.remove(win32api.GetCurrentProcessId())
    except ValueError:
        pass

    if len(pids) == 0:
        result = ""Can't find %s"" % procname
    elif len(pids) > 1:
        result = ""Found too many %s's - pids=`%s`"" % (procname, pids)
    else:
        handle = win32api.OpenProcess(win32con.PROCESS_TERMINATE, 0, pids[0])
        win32api.TerminateProcess(handle, 0)
        win32api.CloseHandle(handle)
        result = """"

    return result


if __name__ == ""__main__"":
    if len(sys.argv) > 1:
        for procname in sys.argv[1:]:
            result = killProcName(procname)
            if result:
                print(result)
                print(""Dumping all processes..."")
                win32pdhutil.ShowAllProcesses()
            else:
                print(""Killed %s"" % procname)
    else:
        print(""Usage: killProcName.py procname ..."")"
429	jackson	1	"from django.apps import apps
from django.conf import settings
from django.contrib.redirects.models import Redirect
from django.contrib.sites.shortcuts import get_current_site
from django.core.exceptions import ImproperlyConfigured
from django.http import HttpResponseGone, HttpResponsePermanentRedirect
from django.utils.deprecation import MiddlewareMixin


class RedirectFallbackMiddleware(MiddlewareMixin):
    # Defined as class-level attributes to be subclassing-friendly.
    response_gone_class = HttpResponseGone
    response_redirect_class = HttpResponsePermanentRedirect

    def __init__(self, get_response):
        if not apps.is_installed(""django.contrib.sites""):
            raise ImproperlyConfigured(
                ""You cannot use RedirectFallbackMiddleware when ""
                ""django.contrib.sites is not installed.""
            )
        super().__init__(get_response)

    def process_response(self, request, response):
        # No need to check for a redirect for non-404 responses.
        if response.status_code != 404:
            return response

        full_path = request.get_full_path()
        current_site = get_current_site(request)

        r = None
        try:
            r = Redirect.objects.get(site=current_site, old_path=full_path)
        except Redirect.DoesNotExist:
            pass
        if r is None and settings.APPEND_SLASH and not request.path.endswith(""/""):
            try:
                r = Redirect.objects.get(
                    site=current_site,
                    old_path=request.get_full_path(force_append_slash=True),
                )
            except Redirect.DoesNotExist:
                pass
        if r is not None:
            if r.new_path == """":
                return self.response_gone_class()
            return self.response_redirect_class(r.new_path)

        # No redirect was found. Return the response.
        return response"
429	donghui	1	"from django.apps import apps
from django.conf import settings
from django.contrib.redirects.models import Redirect
from django.contrib.sites.shortcuts import get_current_site
from django.core.exceptions import ImproperlyConfigured
from django.http import HttpResponseGone, HttpResponsePermanentRedirect
from django.utils.deprecation import MiddlewareMixin


class RedirectFallbackMiddleware(MiddlewareMixin):
    # Defined as class-level attributes to be subclassing-friendly.
    response_gone_class = HttpResponseGone
    response_redirect_class = HttpResponsePermanentRedirect

    def __init__(self, get_response):
        if not apps.is_installed(""django.contrib.sites""):
            raise ImproperlyConfigured(
                ""You cannot use RedirectFallbackMiddleware when ""
                ""django.contrib.sites is not installed.""
            )
        super().__init__(get_response)

    def process_response(self, request, response):
        # No need to check for a redirect for non-404 responses.
        if response.status_code != 404:
            return response

        full_path = request.get_full_path()
        current_site = get_current_site(request)

        r = None
        try:
            r = Redirect.objects.get(site=current_site, old_path=full_path)
        except Redirect.DoesNotExist:
            pass
        if r is None and settings.APPEND_SLASH and not request.path.endswith(""/""):
            try:
                r = Redirect.objects.get(
                    site=current_site,
                    old_path=request.get_full_path(force_append_slash=True),
                )
            except Redirect.DoesNotExist:
                pass
        if r is not None:
            if r.new_path == """":
                return self.response_gone_class()
            return self.response_redirect_class(r.new_path)

        # No redirect was found. Return the response.
        return response"
478	jackson	2	"import numpy as np
import pytest

import pandas as pd
import pandas._testing as tm
from pandas.arrays import BooleanArray
from pandas.tests.arrays.masked_shared import ComparisonOps


@pytest.fixture
def data():
    """"""Fixture returning boolean array with valid and missing data""""""
    return pd.array(
        [True, False] * 4 + [np.nan] + [True, False] * 44 + [np.nan] + [True, False],
        dtype=""boolean"",
    )


@pytest.fixture
def dtype():
    """"""Fixture returning BooleanDtype""""""
    return pd.BooleanDtype()


class TestComparisonOps(ComparisonOps):
    def test_compare_scalar(self, data, comparison_op):
        self._compare_other(data, comparison_op, True)

    def test_compare_array(self, data, comparison_op):
        other = pd.array([True] * len(data), dtype=""boolean"")
        self._compare_other(data, comparison_op, other)
        other = np.array([True] * len(data))
        self._compare_other(data, comparison_op, other)
        other = pd.Series([True] * len(data))
        self._compare_other(data, comparison_op, other)

    @pytest.mark.parametrize(""other"", [True, False, pd.NA])
    def test_scalar(self, other, comparison_op, dtype):
        ComparisonOps.test_scalar(self, other, comparison_op, dtype)

    def test_array(self, comparison_op):
        op = comparison_op
        a = pd.array([True] * 3 + [False] * 3 + [None] * 3, dtype=""boolean"")
        b = pd.array([True, False, None] * 3, dtype=""boolean"")

        result = op(a, b)

        values = op(a._data, b._data)
        mask = a._mask | b._mask
        expected = BooleanArray(values, mask)
        tm.assert_extension_array_equal(result, expected)

        # ensure we haven't mutated anything inplace
        result[0] = None
        tm.assert_extension_array_equal(
            a, pd.array([True] * 3 + [False] * 3 + [None] * 3, dtype=""boolean"")
        )
        tm.assert_extension_array_equal(
            b, pd.array([True, False, None] * 3, dtype=""boolean"")
        )"
478	donghui	2	"import numpy as np
import pytest

import pandas as pd
import pandas._testing as tm
from pandas.arrays import BooleanArray
from pandas.tests.arrays.masked_shared import ComparisonOps


@pytest.fixture
def data():
    """"""Fixture returning boolean array with valid and missing data""""""
    return pd.array(
        [True, False] * 4 + [np.nan] + [True, False] * 44 + [np.nan] + [True, False],
        dtype=""boolean"",
    )


@pytest.fixture
def dtype():
    """"""Fixture returning BooleanDtype""""""
    return pd.BooleanDtype()


class TestComparisonOps(ComparisonOps):
    def test_compare_scalar(self, data, comparison_op):
        self._compare_other(data, comparison_op, True)

    def test_compare_array(self, data, comparison_op):
        other = pd.array([True] * len(data), dtype=""boolean"")
        self._compare_other(data, comparison_op, other)
        other = np.array([True] * len(data))
        self._compare_other(data, comparison_op, other)
        other = pd.Series([True] * len(data))
        self._compare_other(data, comparison_op, other)

    @pytest.mark.parametrize(""other"", [True, False, pd.NA])
    def test_scalar(self, other, comparison_op, dtype):
        ComparisonOps.test_scalar(self, other, comparison_op, dtype)

    def test_array(self, comparison_op):
        op = comparison_op
        a = pd.array([True] * 3 + [False] * 3 + [None] * 3, dtype=""boolean"")
        b = pd.array([True, False, None] * 3, dtype=""boolean"")

        result = op(a, b)

        values = op(a._data, b._data)
        mask = a._mask | b._mask
        expected = BooleanArray(values, mask)
        tm.assert_extension_array_equal(result, expected)

        # ensure we haven't mutated anything inplace
        result[0] = None
        tm.assert_extension_array_equal(
            a, pd.array([True] * 3 + [False] * 3 + [None] * 3, dtype=""boolean"")
        )
        tm.assert_extension_array_equal(
            b, pd.array([True, False, None] * 3, dtype=""boolean"")
        )"
468	jackson	2	"import _plotly_utils.basevalidators


class HoverlabelValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(self, plotly_name=""hoverlabel"", parent_name=""scatter3d"", **kwargs):
        super(HoverlabelValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Hoverlabel""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            align
                Sets the horizontal alignment of the text
                content within hover label box. Has an effect
                only if the hover label text spans more two or
                more lines
            alignsrc
                Sets the source reference on Chart Studio Cloud
                for `align`.
            bgcolor
                Sets the background color of the hover labels
                for this trace
            bgcolorsrc
                Sets the source reference on Chart Studio Cloud
                for `bgcolor`.
            bordercolor
                Sets the border color of the hover labels for
                this trace.
            bordercolorsrc
                Sets the source reference on Chart Studio Cloud
                for `bordercolor`.
            font
                Sets the font used in hover labels.
            namelength
                Sets the default length (in number of
                characters) of the trace name in the hover
                labels for all traces. -1 shows the whole name
                regardless of length. 0-3 shows the first 0-3
                characters, and an integer >3 will show the
                whole name if it is less than that many
                characters, but if it is longer, will truncate
                to `namelength - 3` characters and add an
                ellipsis.
            namelengthsrc
                Sets the source reference on Chart Studio Cloud
                for `namelength`.
"""""",
            ),
            **kwargs,
        )"
468	donghui	1	"import _plotly_utils.basevalidators


class HoverlabelValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(self, plotly_name=""hoverlabel"", parent_name=""scatter3d"", **kwargs):
        super(HoverlabelValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Hoverlabel""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            align
                Sets the horizontal alignment of the text
                content within hover label box. Has an effect
                only if the hover label text spans more two or
                more lines
            alignsrc
                Sets the source reference on Chart Studio Cloud
                for `align`.
            bgcolor
                Sets the background color of the hover labels
                for this trace
            bgcolorsrc
                Sets the source reference on Chart Studio Cloud
                for `bgcolor`.
            bordercolor
                Sets the border color of the hover labels for
                this trace.
            bordercolorsrc
                Sets the source reference on Chart Studio Cloud
                for `bordercolor`.
            font
                Sets the font used in hover labels.
            namelength
                Sets the default length (in number of
                characters) of the trace name in the hover
                labels for all traces. -1 shows the whole name
                regardless of length. 0-3 shows the first 0-3
                characters, and an integer >3 will show the
                whole name if it is less than that many
                characters, but if it is longer, will truncate
                to `namelength - 3` characters and add an
                ellipsis.
            namelengthsrc
                Sets the source reference on Chart Studio Cloud
                for `namelength`.
"""""",
            ),
            **kwargs,
        )"
439	jackson	3	"from __future__ import unicode_literals

# For backwards-compatibility. keep this file.
# (Many people are going to have key bindings that rely on this file.)
from .app import *

__all__ = [
    # Old names.
    'HasArg',
    'HasCompletions',
    'HasFocus',
    'HasSelection',
    'HasValidationError',
    'IsDone',
    'IsReadOnly',
    'IsMultiline',
    'RendererHeightIsKnown',
    'InEditingMode',
    'InPasteMode',

    'ViMode',
    'ViNavigationMode',
    'ViInsertMode',
    'ViInsertMultipleMode',
    'ViReplaceMode',
    'ViSelectionMode',
    'ViWaitingForTextObjectMode',
    'ViDigraphMode',

    'EmacsMode',
    'EmacsInsertMode',
    'EmacsSelectionMode',

    'IsSearching',
    'HasSearch',
    'ControlIsSearchable',
]

# Keep the original classnames for backwards compatibility.
HasValidationError = lambda: has_validation_error
HasArg = lambda: has_arg
IsDone = lambda: is_done
RendererHeightIsKnown = lambda: renderer_height_is_known
ViNavigationMode = lambda: vi_navigation_mode
InPasteMode = lambda: in_paste_mode
EmacsMode = lambda: emacs_mode
EmacsInsertMode = lambda: emacs_insert_mode
ViMode = lambda: vi_mode
IsSearching = lambda: is_searching
HasSearch = lambda: is_searching
ControlIsSearchable = lambda: control_is_searchable
EmacsSelectionMode = lambda: emacs_selection_mode
ViDigraphMode = lambda: vi_digraph_mode
ViWaitingForTextObjectMode = lambda: vi_waiting_for_text_object_mode
ViSelectionMode = lambda: vi_selection_mode
ViReplaceMode = lambda: vi_replace_mode
ViInsertMultipleMode = lambda: vi_insert_multiple_mode
ViInsertMode = lambda: vi_insert_mode
HasSelection = lambda: has_selection
HasCompletions = lambda: has_completions
IsReadOnly = lambda: is_read_only
IsMultiline = lambda: is_multiline

HasFocus = has_focus  # No lambda here! (Has_focus is callable that returns a callable.)
InEditingMode = in_editing_mode"
439	donghui	1	"from __future__ import unicode_literals

# For backwards-compatibility. keep this file.
# (Many people are going to have key bindings that rely on this file.)
from .app import *

__all__ = [
    # Old names.
    'HasArg',
    'HasCompletions',
    'HasFocus',
    'HasSelection',
    'HasValidationError',
    'IsDone',
    'IsReadOnly',
    'IsMultiline',
    'RendererHeightIsKnown',
    'InEditingMode',
    'InPasteMode',

    'ViMode',
    'ViNavigationMode',
    'ViInsertMode',
    'ViInsertMultipleMode',
    'ViReplaceMode',
    'ViSelectionMode',
    'ViWaitingForTextObjectMode',
    'ViDigraphMode',

    'EmacsMode',
    'EmacsInsertMode',
    'EmacsSelectionMode',

    'IsSearching',
    'HasSearch',
    'ControlIsSearchable',
]

# Keep the original classnames for backwards compatibility.
HasValidationError = lambda: has_validation_error
HasArg = lambda: has_arg
IsDone = lambda: is_done
RendererHeightIsKnown = lambda: renderer_height_is_known
ViNavigationMode = lambda: vi_navigation_mode
InPasteMode = lambda: in_paste_mode
EmacsMode = lambda: emacs_mode
EmacsInsertMode = lambda: emacs_insert_mode
ViMode = lambda: vi_mode
IsSearching = lambda: is_searching
HasSearch = lambda: is_searching
ControlIsSearchable = lambda: control_is_searchable
EmacsSelectionMode = lambda: emacs_selection_mode
ViDigraphMode = lambda: vi_digraph_mode
ViWaitingForTextObjectMode = lambda: vi_waiting_for_text_object_mode
ViSelectionMode = lambda: vi_selection_mode
ViReplaceMode = lambda: vi_replace_mode
ViInsertMultipleMode = lambda: vi_insert_multiple_mode
ViInsertMode = lambda: vi_insert_mode
HasSelection = lambda: has_selection
HasCompletions = lambda: has_completions
IsReadOnly = lambda: is_read_only
IsMultiline = lambda: is_multiline

HasFocus = has_focus  # No lambda here! (Has_focus is callable that returns a callable.)
InEditingMode = in_editing_mode"
377	jackson	1	"######################## BEGIN LICENSE BLOCK ########################
# The Original Code is mozilla.org code.
#
# The Initial Developer of the Original Code is
# Netscape Communications Corporation.
# Portions created by the Initial Developer are Copyright (C) 1998
# the Initial Developer. All Rights Reserved.
#
# Contributor(s):
#   Mark Pilgrim - port to Python
#
# This library is free software; you can redistribute it and/or
# modify it under the terms of the GNU Lesser General Public
# License as published by the Free Software Foundation; either
# version 2.1 of the License, or (at your option) any later version.
#
# This library is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public
# License along with this library; if not, write to the Free Software
# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA
# 02110-1301  USA
######################### END LICENSE BLOCK #########################

from .chardistribution import EUCKRDistributionAnalysis
from .codingstatemachine import CodingStateMachine
from .mbcharsetprober import MultiByteCharSetProber
from .mbcssm import CP949_SM_MODEL


class CP949Prober(MultiByteCharSetProber):
    def __init__(self):
        super().__init__()
        self.coding_sm = CodingStateMachine(CP949_SM_MODEL)
        # NOTE: CP949 is a superset of EUC-KR, so the distribution should be
        #       not different.
        self.distribution_analyzer = EUCKRDistributionAnalysis()
        self.reset()

    @property
    def charset_name(self):
        return ""CP949""

    @property
    def language(self):
        return ""Korean"""
377	donghui	1	"######################## BEGIN LICENSE BLOCK ########################
# The Original Code is mozilla.org code.
#
# The Initial Developer of the Original Code is
# Netscape Communications Corporation.
# Portions created by the Initial Developer are Copyright (C) 1998
# the Initial Developer. All Rights Reserved.
#
# Contributor(s):
#   Mark Pilgrim - port to Python
#
# This library is free software; you can redistribute it and/or
# modify it under the terms of the GNU Lesser General Public
# License as published by the Free Software Foundation; either
# version 2.1 of the License, or (at your option) any later version.
#
# This library is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public
# License along with this library; if not, write to the Free Software
# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA
# 02110-1301  USA
######################### END LICENSE BLOCK #########################

from .chardistribution import EUCKRDistributionAnalysis
from .codingstatemachine import CodingStateMachine
from .mbcharsetprober import MultiByteCharSetProber
from .mbcssm import CP949_SM_MODEL


class CP949Prober(MultiByteCharSetProber):
    def __init__(self):
        super().__init__()
        self.coding_sm = CodingStateMachine(CP949_SM_MODEL)
        # NOTE: CP949 is a superset of EUC-KR, so the distribution should be
        #       not different.
        self.distribution_analyzer = EUCKRDistributionAnalysis()
        self.reset()

    @property
    def charset_name(self):
        return ""CP949""

    @property
    def language(self):
        return ""Korean"""
266	jackson	1	"# Copyright (c) Meta Platforms, Inc. and affiliates.
#
# This source code is licensed under the MIT license found in the
# LICENSE file in the root directory of this source tree.

from viktor._vendor.libcst._parser.grammar import _should_include
from viktor._vendor.libcst._parser.parso.utils import PythonVersionInfo
from viktor._vendor.libcst.testing.utils import data_provider, UnitTest


class VersionCompareTest(UnitTest):
    @data_provider(
        (
            # Simple equality
            (""==3.6"", PythonVersionInfo(3, 6), True),
            (""!=3.6"", PythonVersionInfo(3, 6), False),
            # Equal or GT/LT
            ("">=3.6"", PythonVersionInfo(3, 5), False),
            ("">=3.6"", PythonVersionInfo(3, 6), True),
            ("">=3.6"", PythonVersionInfo(3, 7), True),
            (""<=3.6"", PythonVersionInfo(3, 5), True),
            (""<=3.6"", PythonVersionInfo(3, 6), True),
            (""<=3.6"", PythonVersionInfo(3, 7), False),
            # GT/LT
            ("">3.6"", PythonVersionInfo(3, 5), False),
            ("">3.6"", PythonVersionInfo(3, 6), False),
            ("">3.6"", PythonVersionInfo(3, 7), True),
            (""<3.6"", PythonVersionInfo(3, 5), True),
            (""<3.6"", PythonVersionInfo(3, 6), False),
            (""<3.6"", PythonVersionInfo(3, 7), False),
            # Multiple checks
            ("">3.6,<3.8"", PythonVersionInfo(3, 6), False),
            ("">3.6,<3.8"", PythonVersionInfo(3, 7), True),
            ("">3.6,<3.8"", PythonVersionInfo(3, 8), False),
        )
    )
    def test_tokenize(
        self,
        requested_version: str,
        actual_version: PythonVersionInfo,
        expected_result: bool,
    ) -> None:
        self.assertEqual(
            _should_include(requested_version, actual_version), expected_result
        )"
266	donghui	1	"# Copyright (c) Meta Platforms, Inc. and affiliates.
#
# This source code is licensed under the MIT license found in the
# LICENSE file in the root directory of this source tree.

from viktor._vendor.libcst._parser.grammar import _should_include
from viktor._vendor.libcst._parser.parso.utils import PythonVersionInfo
from viktor._vendor.libcst.testing.utils import data_provider, UnitTest


class VersionCompareTest(UnitTest):
    @data_provider(
        (
            # Simple equality
            (""==3.6"", PythonVersionInfo(3, 6), True),
            (""!=3.6"", PythonVersionInfo(3, 6), False),
            # Equal or GT/LT
            ("">=3.6"", PythonVersionInfo(3, 5), False),
            ("">=3.6"", PythonVersionInfo(3, 6), True),
            ("">=3.6"", PythonVersionInfo(3, 7), True),
            (""<=3.6"", PythonVersionInfo(3, 5), True),
            (""<=3.6"", PythonVersionInfo(3, 6), True),
            (""<=3.6"", PythonVersionInfo(3, 7), False),
            # GT/LT
            ("">3.6"", PythonVersionInfo(3, 5), False),
            ("">3.6"", PythonVersionInfo(3, 6), False),
            ("">3.6"", PythonVersionInfo(3, 7), True),
            (""<3.6"", PythonVersionInfo(3, 5), True),
            (""<3.6"", PythonVersionInfo(3, 6), False),
            (""<3.6"", PythonVersionInfo(3, 7), False),
            # Multiple checks
            ("">3.6,<3.8"", PythonVersionInfo(3, 6), False),
            ("">3.6,<3.8"", PythonVersionInfo(3, 7), True),
            ("">3.6,<3.8"", PythonVersionInfo(3, 8), False),
        )
    )
    def test_tokenize(
        self,
        requested_version: str,
        actual_version: PythonVersionInfo,
        expected_result: bool,
    ) -> None:
        self.assertEqual(
            _should_include(requested_version, actual_version), expected_result
        )"
326	jackson	2	"""""""
    pygments.lexers.pointless
    ~~~~~~~~~~~~~~~~~~~~~~~~~

    Lexers for Pointless.

    :copyright: Copyright 2006-2022 by the Pygments team, see AUTHORS.
    :license: BSD, see LICENSE for details.
""""""

from pygments.lexer import RegexLexer, words
from pygments.token import Comment, Error, Keyword, Name, Number, Operator, \
    Punctuation, String, Text

__all__ = ['PointlessLexer']


class PointlessLexer(RegexLexer):
    """"""
    For Pointless source code.

    .. versionadded:: 2.7
    """"""

    name = 'Pointless'
    url = 'https://ptls.dev'
    aliases = ['pointless']
    filenames = ['*.ptls']

    ops = words([
        ""+"", ""-"", ""*"", ""/"", ""**"", ""%"", ""+="", ""-="", ""*="",
        ""/="", ""**="", ""%="", ""|>"", ""="", ""=="", ""!="", ""<"", "">"",
        ""<="", "">="", ""=>"", ""$"", ""++"",
    ])

    keywords = words([
        ""if"", ""then"", ""else"", ""where"", ""with"", ""cond"",
        ""case"", ""and"", ""or"", ""not"", ""in"", ""as"", ""for"",
        ""requires"", ""throw"", ""try"", ""catch"", ""when"",
        ""yield"", ""upval"",
    ], suffix=r'\b')

    tokens = {
        'root': [
            (r'[ \n\r]+', Text),
            (r'--.*$', Comment.Single),
            (r'""""""', String, 'multiString'),
            (r'""', String, 'string'),
            (r'[\[\](){}:;,.]', Punctuation),
            (ops, Operator),
            (keywords, Keyword),
            (r'\d+|\d*\.\d+', Number),
            (r'(true|false)\b', Name.Builtin),
            (r'[A-Z][a-zA-Z0-9]*\b', String.Symbol),
            (r'output\b', Name.Variable.Magic),
            (r'(export|import)\b', Keyword.Namespace),
            (r'[a-z][a-zA-Z0-9]*\b', Name.Variable)
        ],
        'multiString': [
            (r'\\.', String.Escape),
            (r'""""""', String, '#pop'),
            (r'""', String),
            (r'[^\\""]+', String),
        ],
        'string': [
            (r'\\.', String.Escape),
            (r'""', String, '#pop'),
            (r'\n', Error),
            (r'[^\\""]+', String),
        ],
    }"
326	donghui	1	"""""""
    pygments.lexers.pointless
    ~~~~~~~~~~~~~~~~~~~~~~~~~

    Lexers for Pointless.

    :copyright: Copyright 2006-2022 by the Pygments team, see AUTHORS.
    :license: BSD, see LICENSE for details.
""""""

from pygments.lexer import RegexLexer, words
from pygments.token import Comment, Error, Keyword, Name, Number, Operator, \
    Punctuation, String, Text

__all__ = ['PointlessLexer']


class PointlessLexer(RegexLexer):
    """"""
    For Pointless source code.

    .. versionadded:: 2.7
    """"""

    name = 'Pointless'
    url = 'https://ptls.dev'
    aliases = ['pointless']
    filenames = ['*.ptls']

    ops = words([
        ""+"", ""-"", ""*"", ""/"", ""**"", ""%"", ""+="", ""-="", ""*="",
        ""/="", ""**="", ""%="", ""|>"", ""="", ""=="", ""!="", ""<"", "">"",
        ""<="", "">="", ""=>"", ""$"", ""++"",
    ])

    keywords = words([
        ""if"", ""then"", ""else"", ""where"", ""with"", ""cond"",
        ""case"", ""and"", ""or"", ""not"", ""in"", ""as"", ""for"",
        ""requires"", ""throw"", ""try"", ""catch"", ""when"",
        ""yield"", ""upval"",
    ], suffix=r'\b')

    tokens = {
        'root': [
            (r'[ \n\r]+', Text),
            (r'--.*$', Comment.Single),
            (r'""""""', String, 'multiString'),
            (r'""', String, 'string'),
            (r'[\[\](){}:;,.]', Punctuation),
            (ops, Operator),
            (keywords, Keyword),
            (r'\d+|\d*\.\d+', Number),
            (r'(true|false)\b', Name.Builtin),
            (r'[A-Z][a-zA-Z0-9]*\b', String.Symbol),
            (r'output\b', Name.Variable.Magic),
            (r'(export|import)\b', Keyword.Namespace),
            (r'[a-z][a-zA-Z0-9]*\b', Name.Variable)
        ],
        'multiString': [
            (r'\\.', String.Escape),
            (r'""""""', String, '#pop'),
            (r'""', String),
            (r'[^\\""]+', String),
        ],
        'string': [
            (r'\\.', String.Escape),
            (r'""', String, '#pop'),
            (r'\n', Error),
            (r'[^\\""]+', String),
        ],
    }"
295	jackson	3	"from typing import Any, Dict, Optional, Union
from warnings import warn

from .api import from_bytes
from .constant import CHARDET_CORRESPONDENCE


def detect(
    byte_str: bytes, should_rename_legacy: bool = False, **kwargs: Any
) -> Dict[str, Optional[Union[str, float]]]:
    """"""
    chardet legacy method
    Detect the encoding of the given byte string. It should be mostly backward-compatible.
    Encoding name will match Chardet own writing whenever possible. (Not on encoding name unsupported by it)
    This function is deprecated and should be used to migrate your project easily, consult the documentation for
    further information. Not planned for removal.

    :param byte_str:     The byte sequence to examine.
    :param should_rename_legacy:  Should we rename legacy encodings
                                  to their more modern equivalents?
    """"""
    if len(kwargs):
        warn(
            f""charset-normalizer disregard arguments '{','.join(list(kwargs.keys()))}' in legacy function detect()""
        )

    if not isinstance(byte_str, (bytearray, bytes)):
        raise TypeError(  # pragma: nocover
            ""Expected object of type bytes or bytearray, got: ""
            ""{0}"".format(type(byte_str))
        )

    if isinstance(byte_str, bytearray):
        byte_str = bytes(byte_str)

    r = from_bytes(byte_str).best()

    encoding = r.encoding if r is not None else None
    language = r.language if r is not None and r.language != ""Unknown"" else """"
    confidence = 1.0 - r.chaos if r is not None else None

    # Note: CharsetNormalizer does not return 'UTF-8-SIG' as the sig get stripped in the detection/normalization process
    # but chardet does return 'utf-8-sig' and it is a valid codec name.
    if r is not None and encoding == ""utf_8"" and r.bom:
        encoding += ""_sig""

    if should_rename_legacy is False and encoding in CHARDET_CORRESPONDENCE:
        encoding = CHARDET_CORRESPONDENCE[encoding]

    return {
        ""encoding"": encoding,
        ""language"": language,
        ""confidence"": confidence,
    }"
295	donghui	3	"from typing import Any, Dict, Optional, Union
from warnings import warn

from .api import from_bytes
from .constant import CHARDET_CORRESPONDENCE


def detect(
    byte_str: bytes, should_rename_legacy: bool = False, **kwargs: Any
) -> Dict[str, Optional[Union[str, float]]]:
    """"""
    chardet legacy method
    Detect the encoding of the given byte string. It should be mostly backward-compatible.
    Encoding name will match Chardet own writing whenever possible. (Not on encoding name unsupported by it)
    This function is deprecated and should be used to migrate your project easily, consult the documentation for
    further information. Not planned for removal.

    :param byte_str:     The byte sequence to examine.
    :param should_rename_legacy:  Should we rename legacy encodings
                                  to their more modern equivalents?
    """"""
    if len(kwargs):
        warn(
            f""charset-normalizer disregard arguments '{','.join(list(kwargs.keys()))}' in legacy function detect()""
        )

    if not isinstance(byte_str, (bytearray, bytes)):
        raise TypeError(  # pragma: nocover
            ""Expected object of type bytes or bytearray, got: ""
            ""{0}"".format(type(byte_str))
        )

    if isinstance(byte_str, bytearray):
        byte_str = bytes(byte_str)

    r = from_bytes(byte_str).best()

    encoding = r.encoding if r is not None else None
    language = r.language if r is not None and r.language != ""Unknown"" else """"
    confidence = 1.0 - r.chaos if r is not None else None

    # Note: CharsetNormalizer does not return 'UTF-8-SIG' as the sig get stripped in the detection/normalization process
    # but chardet does return 'utf-8-sig' and it is a valid codec name.
    if r is not None and encoding == ""utf_8"" and r.bom:
        encoding += ""_sig""

    if should_rename_legacy is False and encoding in CHARDET_CORRESPONDENCE:
        encoding = CHARDET_CORRESPONDENCE[encoding]

    return {
        ""encoding"": encoding,
        ""language"": language,
        ""confidence"": confidence,
    }"
384	jackson	0	"import esphome.codegen as cg
import esphome.config_validation as cv
from esphome import pins
from esphome.components import display
from esphome.const import (
    CONF_BRIGHTNESS,
    CONF_EXTERNAL_VCC,
    CONF_LAMBDA,
    CONF_MODEL,
    CONF_RESET_PIN,
)

CODEOWNERS = [""@kbx81""]

ssd1325_base_ns = cg.esphome_ns.namespace(""ssd1325_base"")
SSD1325 = ssd1325_base_ns.class_(""SSD1325"", cg.PollingComponent, display.DisplayBuffer)
SSD1325Model = ssd1325_base_ns.enum(""SSD1325Model"")

MODELS = {
    ""SSD1325_128X32"": SSD1325Model.SSD1325_MODEL_128_32,
    ""SSD1325_128X64"": SSD1325Model.SSD1325_MODEL_128_64,
    ""SSD1325_96X16"": SSD1325Model.SSD1325_MODEL_96_16,
    ""SSD1325_64X48"": SSD1325Model.SSD1325_MODEL_64_48,
    ""SSD1327_128X128"": SSD1325Model.SSD1327_MODEL_128_128,
}

SSD1325_MODEL = cv.enum(MODELS, upper=True, space=""_"")

SSD1325_SCHEMA = display.FULL_DISPLAY_SCHEMA.extend(
    {
        cv.Required(CONF_MODEL): SSD1325_MODEL,
        cv.Optional(CONF_RESET_PIN): pins.gpio_output_pin_schema,
        cv.Optional(CONF_BRIGHTNESS, default=1.0): cv.percentage,
        cv.Optional(CONF_EXTERNAL_VCC): cv.boolean,
    }
).extend(cv.polling_component_schema(""1s""))


async def setup_ssd1325(var, config):
    await cg.register_component(var, config)
    await display.register_display(var, config)

    cg.add(var.set_model(config[CONF_MODEL]))
    if CONF_RESET_PIN in config:
        reset = await cg.gpio_pin_expression(config[CONF_RESET_PIN])
        cg.add(var.set_reset_pin(reset))
    if CONF_BRIGHTNESS in config:
        cg.add(var.init_brightness(config[CONF_BRIGHTNESS]))
    if CONF_EXTERNAL_VCC in config:
        cg.add(var.set_external_vcc(config[CONF_EXTERNAL_VCC]))
    if CONF_LAMBDA in config:
        lambda_ = await cg.process_lambda(
            config[CONF_LAMBDA], [(display.DisplayBufferRef, ""it"")], return_type=cg.void
        )
        cg.add(var.set_writer(lambda_))"
384	donghui	0	"import esphome.codegen as cg
import esphome.config_validation as cv
from esphome import pins
from esphome.components import display
from esphome.const import (
    CONF_BRIGHTNESS,
    CONF_EXTERNAL_VCC,
    CONF_LAMBDA,
    CONF_MODEL,
    CONF_RESET_PIN,
)

CODEOWNERS = [""@kbx81""]

ssd1325_base_ns = cg.esphome_ns.namespace(""ssd1325_base"")
SSD1325 = ssd1325_base_ns.class_(""SSD1325"", cg.PollingComponent, display.DisplayBuffer)
SSD1325Model = ssd1325_base_ns.enum(""SSD1325Model"")

MODELS = {
    ""SSD1325_128X32"": SSD1325Model.SSD1325_MODEL_128_32,
    ""SSD1325_128X64"": SSD1325Model.SSD1325_MODEL_128_64,
    ""SSD1325_96X16"": SSD1325Model.SSD1325_MODEL_96_16,
    ""SSD1325_64X48"": SSD1325Model.SSD1325_MODEL_64_48,
    ""SSD1327_128X128"": SSD1325Model.SSD1327_MODEL_128_128,
}

SSD1325_MODEL = cv.enum(MODELS, upper=True, space=""_"")

SSD1325_SCHEMA = display.FULL_DISPLAY_SCHEMA.extend(
    {
        cv.Required(CONF_MODEL): SSD1325_MODEL,
        cv.Optional(CONF_RESET_PIN): pins.gpio_output_pin_schema,
        cv.Optional(CONF_BRIGHTNESS, default=1.0): cv.percentage,
        cv.Optional(CONF_EXTERNAL_VCC): cv.boolean,
    }
).extend(cv.polling_component_schema(""1s""))


async def setup_ssd1325(var, config):
    await cg.register_component(var, config)
    await display.register_display(var, config)

    cg.add(var.set_model(config[CONF_MODEL]))
    if CONF_RESET_PIN in config:
        reset = await cg.gpio_pin_expression(config[CONF_RESET_PIN])
        cg.add(var.set_reset_pin(reset))
    if CONF_BRIGHTNESS in config:
        cg.add(var.init_brightness(config[CONF_BRIGHTNESS]))
    if CONF_EXTERNAL_VCC in config:
        cg.add(var.set_external_vcc(config[CONF_EXTERNAL_VCC]))
    if CONF_LAMBDA in config:
        lambda_ = await cg.process_lambda(
            config[CONF_LAMBDA], [(display.DisplayBufferRef, ""it"")], return_type=cg.void
        )
        cg.add(var.set_writer(lambda_))"
312	jackson	4	"#!../env.py
#
# SPDX-License-Identifier: BSD-3-Clause
# Copyright 2020, Intel Corporation

import testframework as t
from testframework import granularity as g
import futils
import os


# All test cases in pmem2_persist_valgrind use Valgrind, which is not available
# on Windows systems.
@t.windows_exclude
@t.require_valgrind_enabled('pmemcheck')
# XXX In the match file, there are two possible numbers of errors. It varies
# from compiler to compiler. There should be only one number when pmemcheck
# will be fixed. Please also remove the below requirement after pmemcheck fix.
# https://github.com/pmem/valgrind/pull/76
@g.require_granularity(g.CL_OR_LESS)
class PMEM2_PERSIST(t.Test):
    test_type = t.Medium
    available_granularity = None

    def run(self, ctx):
        filepath = ctx.create_holey_file(2 * t.MiB, 'testfile')
        ctx.exec('pmem2_persist_valgrind', self.test_case, filepath)


class TEST0(PMEM2_PERSIST):
    """"""persist continuous data in a range of pmem""""""
    test_case = ""test_persist_continuous_range""


class TEST1(PMEM2_PERSIST):
    """"""persist discontinuous data in a range of pmem""""""
    test_case = ""test_persist_discontinuous_range""


class TEST2(PMEM2_PERSIST):
    """"""persist part of discontinuous data in a range of pmem""""""
    test_case = ""test_persist_discontinuous_range_partially""

    def run(self, ctx):
        filepath = ctx.create_holey_file(16 * t.KiB, 'testfile')
        ctx.exec('pmem2_persist_valgrind', self.test_case, filepath)
        pmemecheck_log = os.path.join(
            os.getcwd(), 'pmem2_persist_valgrind', 'pmemcheck2.log')
        futils.tail(pmemecheck_log, 2)


class TEST3(PMEM2_PERSIST):
    """"""persist data in a range of the memory mapped by mmap()""""""
    test_case = ""test_persist_nonpmem_data"""
312	donghui	3	"#!../env.py
#
# SPDX-License-Identifier: BSD-3-Clause
# Copyright 2020, Intel Corporation

import testframework as t
from testframework import granularity as g
import futils
import os


# All test cases in pmem2_persist_valgrind use Valgrind, which is not available
# on Windows systems.
@t.windows_exclude
@t.require_valgrind_enabled('pmemcheck')
# XXX In the match file, there are two possible numbers of errors. It varies
# from compiler to compiler. There should be only one number when pmemcheck
# will be fixed. Please also remove the below requirement after pmemcheck fix.
# https://github.com/pmem/valgrind/pull/76
@g.require_granularity(g.CL_OR_LESS)
class PMEM2_PERSIST(t.Test):
    test_type = t.Medium
    available_granularity = None

    def run(self, ctx):
        filepath = ctx.create_holey_file(2 * t.MiB, 'testfile')
        ctx.exec('pmem2_persist_valgrind', self.test_case, filepath)


class TEST0(PMEM2_PERSIST):
    """"""persist continuous data in a range of pmem""""""
    test_case = ""test_persist_continuous_range""


class TEST1(PMEM2_PERSIST):
    """"""persist discontinuous data in a range of pmem""""""
    test_case = ""test_persist_discontinuous_range""


class TEST2(PMEM2_PERSIST):
    """"""persist part of discontinuous data in a range of pmem""""""
    test_case = ""test_persist_discontinuous_range_partially""

    def run(self, ctx):
        filepath = ctx.create_holey_file(16 * t.KiB, 'testfile')
        ctx.exec('pmem2_persist_valgrind', self.test_case, filepath)
        pmemecheck_log = os.path.join(
            os.getcwd(), 'pmem2_persist_valgrind', 'pmemcheck2.log')
        futils.tail(pmemecheck_log, 2)


class TEST3(PMEM2_PERSIST):
    """"""persist data in a range of the memory mapped by mmap()""""""
    test_case = ""test_persist_nonpmem_data"""
252	jackson	0	"import datetime, json, os, sys
from lib.crawling import Crawling
from lib.database import DB
from lib.telegram import TeleGram
from lib.make_data import Make_Data
from lib.settings import logger, make_folder, args_check, json_check, now, photo_path

def run(hscode_dict, crawling, db, tele, make):
    menus = ''
    menus += '#수출입데이터 ' + now.strftime('#%Y년%m월%d일') + '\n'

    for title in hscode_dict:
        tag = '#수출입데이터' + ' ' + '#' + title + ' ' + now.strftime('#%Y년%m월%d일')    
        photo_name = photo_path + str(hscode_dict[title]) + '_' + str(now.year) + str(now.month) + str(now.day) + '.png'
        crawling_dict = crawling.get_search(hscode_dict[title])
        df, template, month = make.data_remodel(crawling_dict, hscode_dict[title], tag, db)
        make.make_photo(df, title, hscode_dict[title], photo_name, month)
        tele.send_photo(photo_name, template)
        db.insert_update_db(title, hscode_dict[title], crawling_dict)
        menus += '#' + title + '\n'
    tele.send_message(menus)

def main():
    crawling = Crawling()
    db = DB()
    tele = TeleGram()
    make = Make_Data()

    for json_file in json_list:
        with open(json_path + json_file, 'r', encoding='utf-8') as f:
            hscode_dict = json.load(f)
        run(hscode_dict, crawling, db, tele, make)
        f.close()    

    db.cs.close()
    crawling.driver.close()

if __name__ == '__main__':
    args = sys.argv
    json_path = args_check(args)
    json_list = json_check(json_path)

    logger(f""Main started at {now}"")
    make_folder()
    main()
    et = datetime.datetime.now()
    logger(f""Main finished at {et}"")
    logger(f""Main time for task: {et-now}"")
    os.system(""sudo rm -rf {photo_path}*.png"".format(photo_path=photo_path))"
252	donghui	0	"import datetime, json, os, sys
from lib.crawling import Crawling
from lib.database import DB
from lib.telegram import TeleGram
from lib.make_data import Make_Data
from lib.settings import logger, make_folder, args_check, json_check, now, photo_path

def run(hscode_dict, crawling, db, tele, make):
    menus = ''
    menus += '#수출입데이터 ' + now.strftime('#%Y년%m월%d일') + '\n'

    for title in hscode_dict:
        tag = '#수출입데이터' + ' ' + '#' + title + ' ' + now.strftime('#%Y년%m월%d일')    
        photo_name = photo_path + str(hscode_dict[title]) + '_' + str(now.year) + str(now.month) + str(now.day) + '.png'
        crawling_dict = crawling.get_search(hscode_dict[title])
        df, template, month = make.data_remodel(crawling_dict, hscode_dict[title], tag, db)
        make.make_photo(df, title, hscode_dict[title], photo_name, month)
        tele.send_photo(photo_name, template)
        db.insert_update_db(title, hscode_dict[title], crawling_dict)
        menus += '#' + title + '\n'
    tele.send_message(menus)

def main():
    crawling = Crawling()
    db = DB()
    tele = TeleGram()
    make = Make_Data()

    for json_file in json_list:
        with open(json_path + json_file, 'r', encoding='utf-8') as f:
            hscode_dict = json.load(f)
        run(hscode_dict, crawling, db, tele, make)
        f.close()    

    db.cs.close()
    crawling.driver.close()

if __name__ == '__main__':
    args = sys.argv
    json_path = args_check(args)
    json_list = json_check(json_path)

    logger(f""Main started at {now}"")
    make_folder()
    main()
    et = datetime.datetime.now()
    logger(f""Main finished at {et}"")
    logger(f""Main time for task: {et-now}"")
    os.system(""sudo rm -rf {photo_path}*.png"".format(photo_path=photo_path))"
343	jackson	4	"""""""
PostGIS to GDAL conversion constant definitions
""""""
# Lookup to convert pixel type values from GDAL to PostGIS
GDAL_TO_POSTGIS = [None, 4, 6, 5, 8, 7, 10, 11, None, None, None, None]

# Lookup to convert pixel type values from PostGIS to GDAL
POSTGIS_TO_GDAL = [1, 1, 1, 3, 1, 3, 2, 5, 4, None, 6, 7, None, None]

# Struct pack structure for raster header, the raster header has the
# following structure:
#
# Endianness, PostGIS raster version, number of bands, scale, origin,
# skew, srid, width, and height.
#
# Scale, origin, and skew have x and y values. PostGIS currently uses
# a fixed endianness (1) and there is only one version (0).
POSTGIS_HEADER_STRUCTURE = 'B H H d d d d d d i H H'

# Lookup values to convert GDAL pixel types to struct characters. This is
# used to pack and unpack the pixel values of PostGIS raster bands.
GDAL_TO_STRUCT = [
    None, 'B', 'H', 'h', 'L', 'l', 'f', 'd',
    None, None, None, None,
]

# Size of the packed value in bytes for different numerical types.
# This is needed to cut chunks of band data out of PostGIS raster strings
# when decomposing them into GDALRasters.
# See https://docs.python.org/library/struct.html#format-characters
STRUCT_SIZE = {
    'b': 1,  # Signed char
    'B': 1,  # Unsigned char
    '?': 1,  # _Bool
    'h': 2,  # Short
    'H': 2,  # Unsigned short
    'i': 4,  # Integer
    'I': 4,  # Unsigned Integer
    'l': 4,  # Long
    'L': 4,  # Unsigned Long
    'f': 4,  # Float
    'd': 8,  # Double
}

# Pixel type specifies type of pixel values in a band. Storage flag specifies
# whether the band data is stored as part of the datum or is to be found on the
# server's filesystem. There are currently 11 supported pixel value types, so 4
# bits are enough to account for all. Reserve the upper 4 bits for generic
# flags.
# See https://trac.osgeo.org/postgis/wiki/WKTRaster/RFC/RFC1_V0SerialFormat#Pixeltypeandstorageflag
BANDTYPE_PIXTYPE_MASK = 0x0F
BANDTYPE_FLAG_HASNODATA = 1 << 6"
343	donghui	2	"""""""
PostGIS to GDAL conversion constant definitions
""""""
# Lookup to convert pixel type values from GDAL to PostGIS
GDAL_TO_POSTGIS = [None, 4, 6, 5, 8, 7, 10, 11, None, None, None, None]

# Lookup to convert pixel type values from PostGIS to GDAL
POSTGIS_TO_GDAL = [1, 1, 1, 3, 1, 3, 2, 5, 4, None, 6, 7, None, None]

# Struct pack structure for raster header, the raster header has the
# following structure:
#
# Endianness, PostGIS raster version, number of bands, scale, origin,
# skew, srid, width, and height.
#
# Scale, origin, and skew have x and y values. PostGIS currently uses
# a fixed endianness (1) and there is only one version (0).
POSTGIS_HEADER_STRUCTURE = 'B H H d d d d d d i H H'

# Lookup values to convert GDAL pixel types to struct characters. This is
# used to pack and unpack the pixel values of PostGIS raster bands.
GDAL_TO_STRUCT = [
    None, 'B', 'H', 'h', 'L', 'l', 'f', 'd',
    None, None, None, None,
]

# Size of the packed value in bytes for different numerical types.
# This is needed to cut chunks of band data out of PostGIS raster strings
# when decomposing them into GDALRasters.
# See https://docs.python.org/library/struct.html#format-characters
STRUCT_SIZE = {
    'b': 1,  # Signed char
    'B': 1,  # Unsigned char
    '?': 1,  # _Bool
    'h': 2,  # Short
    'H': 2,  # Unsigned short
    'i': 4,  # Integer
    'I': 4,  # Unsigned Integer
    'l': 4,  # Long
    'L': 4,  # Unsigned Long
    'f': 4,  # Float
    'd': 8,  # Double
}

# Pixel type specifies type of pixel values in a band. Storage flag specifies
# whether the band data is stored as part of the datum or is to be found on the
# server's filesystem. There are currently 11 supported pixel value types, so 4
# bits are enough to account for all. Reserve the upper 4 bits for generic
# flags.
# See https://trac.osgeo.org/postgis/wiki/WKTRaster/RFC/RFC1_V0SerialFormat#Pixeltypeandstorageflag
BANDTYPE_PIXTYPE_MASK = 0x0F
BANDTYPE_FLAG_HASNODATA = 1 << 6"
390	jackson	0	"######################## BEGIN LICENSE BLOCK ########################
# The Original Code is mozilla.org code.
#
# The Initial Developer of the Original Code is
# Netscape Communications Corporation.
# Portions created by the Initial Developer are Copyright (C) 1998
# the Initial Developer. All Rights Reserved.
#
# Contributor(s):
#   Mark Pilgrim - port to Python
#
# This library is free software; you can redistribute it and/or
# modify it under the terms of the GNU Lesser General Public
# License as published by the Free Software Foundation; either
# version 2.1 of the License, or (at your option) any later version.
#
# This library is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public
# License along with this library; if not, write to the Free Software
# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA
# 02110-1301  USA
######################### END LICENSE BLOCK #########################

from .chardistribution import EUCKRDistributionAnalysis
from .codingstatemachine import CodingStateMachine
from .mbcharsetprober import MultiByteCharSetProber
from .mbcssm import EUCKR_SM_MODEL


class EUCKRProber(MultiByteCharSetProber):
    def __init__(self) -> None:
        super().__init__()
        self.coding_sm = CodingStateMachine(EUCKR_SM_MODEL)
        self.distribution_analyzer = EUCKRDistributionAnalysis()
        self.reset()

    @property
    def charset_name(self) -> str:
        return ""EUC-KR""

    @property
    def language(self) -> str:
        return ""Korean"""
390	donghui	0	"######################## BEGIN LICENSE BLOCK ########################
# The Original Code is mozilla.org code.
#
# The Initial Developer of the Original Code is
# Netscape Communications Corporation.
# Portions created by the Initial Developer are Copyright (C) 1998
# the Initial Developer. All Rights Reserved.
#
# Contributor(s):
#   Mark Pilgrim - port to Python
#
# This library is free software; you can redistribute it and/or
# modify it under the terms of the GNU Lesser General Public
# License as published by the Free Software Foundation; either
# version 2.1 of the License, or (at your option) any later version.
#
# This library is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public
# License along with this library; if not, write to the Free Software
# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA
# 02110-1301  USA
######################### END LICENSE BLOCK #########################

from .chardistribution import EUCKRDistributionAnalysis
from .codingstatemachine import CodingStateMachine
from .mbcharsetprober import MultiByteCharSetProber
from .mbcssm import EUCKR_SM_MODEL


class EUCKRProber(MultiByteCharSetProber):
    def __init__(self) -> None:
        super().__init__()
        self.coding_sm = CodingStateMachine(EUCKR_SM_MODEL)
        self.distribution_analyzer = EUCKRDistributionAnalysis()
        self.reset()

    @property
    def charset_name(self) -> str:
        return ""EUC-KR""

    @property
    def language(self) -> str:
        return ""Korean"""
363	jackson	0	"import esphome.codegen as cg
import esphome.config_validation as cv
from esphome.components import switch
from esphome.const import ICON_POWER
from .. import CONF_PIPSOLAR_ID, PIPSOLAR_COMPONENT_SCHEMA, pipsolar_ns

DEPENDENCIES = [""uart""]

CONF_OUTPUT_SOURCE_PRIORITY_UTILITY = ""output_source_priority_utility""
CONF_OUTPUT_SOURCE_PRIORITY_SOLAR = ""output_source_priority_solar""
CONF_OUTPUT_SOURCE_PRIORITY_BATTERY = ""output_source_priority_battery""
CONF_INPUT_VOLTAGE_RANGE = ""input_voltage_range""
CONF_PV_OK_CONDITION_FOR_PARALLEL = ""pv_ok_condition_for_parallel""
CONF_PV_POWER_BALANCE = ""pv_power_balance""

TYPES = {
    CONF_OUTPUT_SOURCE_PRIORITY_UTILITY: (""POP00"", None),
    CONF_OUTPUT_SOURCE_PRIORITY_SOLAR: (""POP01"", None),
    CONF_OUTPUT_SOURCE_PRIORITY_BATTERY: (""POP02"", None),
    CONF_INPUT_VOLTAGE_RANGE: (""PGR01"", ""PGR00""),
    CONF_PV_OK_CONDITION_FOR_PARALLEL: (""PPVOKC1"", ""PPVOKC0""),
    CONF_PV_POWER_BALANCE: (""PSPB1"", ""PSPB0""),
}

PipsolarSwitch = pipsolar_ns.class_(""PipsolarSwitch"", switch.Switch, cg.Component)

PIPSWITCH_SCHEMA = switch.switch_schema(
    PipsolarSwitch, icon=ICON_POWER, block_inverted=True
).extend(cv.COMPONENT_SCHEMA)

CONFIG_SCHEMA = PIPSOLAR_COMPONENT_SCHEMA.extend(
    {cv.Optional(type): PIPSWITCH_SCHEMA for type in TYPES}
)


async def to_code(config):
    paren = await cg.get_variable(config[CONF_PIPSOLAR_ID])

    for type, (on, off) in TYPES.items():
        if type in config:
            conf = config[type]
            var = await switch.new_switch(conf)
            await cg.register_component(var, conf)
            cg.add(getattr(paren, f""set_{type}_switch"")(var))
            cg.add(var.set_parent(paren))
            cg.add(var.set_on_command(on))
            if off is not None:
                cg.add(var.set_off_command(off))"
363	donghui	0	"import esphome.codegen as cg
import esphome.config_validation as cv
from esphome.components import switch
from esphome.const import ICON_POWER
from .. import CONF_PIPSOLAR_ID, PIPSOLAR_COMPONENT_SCHEMA, pipsolar_ns

DEPENDENCIES = [""uart""]

CONF_OUTPUT_SOURCE_PRIORITY_UTILITY = ""output_source_priority_utility""
CONF_OUTPUT_SOURCE_PRIORITY_SOLAR = ""output_source_priority_solar""
CONF_OUTPUT_SOURCE_PRIORITY_BATTERY = ""output_source_priority_battery""
CONF_INPUT_VOLTAGE_RANGE = ""input_voltage_range""
CONF_PV_OK_CONDITION_FOR_PARALLEL = ""pv_ok_condition_for_parallel""
CONF_PV_POWER_BALANCE = ""pv_power_balance""

TYPES = {
    CONF_OUTPUT_SOURCE_PRIORITY_UTILITY: (""POP00"", None),
    CONF_OUTPUT_SOURCE_PRIORITY_SOLAR: (""POP01"", None),
    CONF_OUTPUT_SOURCE_PRIORITY_BATTERY: (""POP02"", None),
    CONF_INPUT_VOLTAGE_RANGE: (""PGR01"", ""PGR00""),
    CONF_PV_OK_CONDITION_FOR_PARALLEL: (""PPVOKC1"", ""PPVOKC0""),
    CONF_PV_POWER_BALANCE: (""PSPB1"", ""PSPB0""),
}

PipsolarSwitch = pipsolar_ns.class_(""PipsolarSwitch"", switch.Switch, cg.Component)

PIPSWITCH_SCHEMA = switch.switch_schema(
    PipsolarSwitch, icon=ICON_POWER, block_inverted=True
).extend(cv.COMPONENT_SCHEMA)

CONFIG_SCHEMA = PIPSOLAR_COMPONENT_SCHEMA.extend(
    {cv.Optional(type): PIPSWITCH_SCHEMA for type in TYPES}
)


async def to_code(config):
    paren = await cg.get_variable(config[CONF_PIPSOLAR_ID])

    for type, (on, off) in TYPES.items():
        if type in config:
            conf = config[type]
            var = await switch.new_switch(conf)
            await cg.register_component(var, conf)
            cg.add(getattr(paren, f""set_{type}_switch"")(var))
            cg.add(var.set_parent(paren))
            cg.add(var.set_on_command(on))
            if off is not None:
                cg.add(var.set_off_command(off))"
332	jackson	4	"from __future__ import annotations

import numpy as np

from pandas._typing import NumpyIndexT

from pandas.core.dtypes.common import is_list_like


def cartesian_product(X) -> list[np.ndarray]:
    """"""
    Numpy version of itertools.product.
    Sometimes faster (for large inputs)...

    Parameters
    ----------
    X : list-like of list-likes

    Returns
    -------
    product : list of ndarrays

    Examples
    --------
    >>> cartesian_product([list('ABC'), [1, 2]])
    [array(['A', 'A', 'B', 'B', 'C', 'C'], dtype='<U1'), array([1, 2, 1, 2, 1, 2])]

    See Also
    --------
    itertools.product : Cartesian product of input iterables.  Equivalent to
        nested for-loops.
    """"""
    msg = ""Input must be a list-like of list-likes""
    if not is_list_like(X):
        raise TypeError(msg)
    for x in X:
        if not is_list_like(x):
            raise TypeError(msg)

    if len(X) == 0:
        return []

    lenX = np.fromiter((len(x) for x in X), dtype=np.intp)
    cumprodX = np.cumproduct(lenX)

    if np.any(cumprodX < 0):
        raise ValueError(""Product space too large to allocate arrays!"")

    a = np.roll(cumprodX, 1)
    a[0] = 1

    if cumprodX[-1] != 0:
        b = cumprodX[-1] / cumprodX
    else:
        # if any factor is empty, the cartesian product is empty
        b = np.zeros_like(cumprodX)

    # error: Argument of type ""int_"" cannot be assigned to parameter ""num"" of
    # type ""int"" in function ""tile_compat""
    return [
        tile_compat(
            np.repeat(x, b[i]),
            np.product(a[i]),  # pyright: ignore[reportGeneralTypeIssues]
        )
        for i, x in enumerate(X)
    ]


def tile_compat(arr: NumpyIndexT, num: int) -> NumpyIndexT:
    """"""
    Index compat for np.tile.

    Notes
    -----
    Does not support multi-dimensional `num`.
    """"""
    if isinstance(arr, np.ndarray):
        return np.tile(arr, num)

    # Otherwise we have an Index
    taker = np.tile(np.arange(len(arr)), num)
    return arr.take(taker)"
332	donghui	4	"from __future__ import annotations

import numpy as np

from pandas._typing import NumpyIndexT

from pandas.core.dtypes.common import is_list_like


def cartesian_product(X) -> list[np.ndarray]:
    """"""
    Numpy version of itertools.product.
    Sometimes faster (for large inputs)...

    Parameters
    ----------
    X : list-like of list-likes

    Returns
    -------
    product : list of ndarrays

    Examples
    --------
    >>> cartesian_product([list('ABC'), [1, 2]])
    [array(['A', 'A', 'B', 'B', 'C', 'C'], dtype='<U1'), array([1, 2, 1, 2, 1, 2])]

    See Also
    --------
    itertools.product : Cartesian product of input iterables.  Equivalent to
        nested for-loops.
    """"""
    msg = ""Input must be a list-like of list-likes""
    if not is_list_like(X):
        raise TypeError(msg)
    for x in X:
        if not is_list_like(x):
            raise TypeError(msg)

    if len(X) == 0:
        return []

    lenX = np.fromiter((len(x) for x in X), dtype=np.intp)
    cumprodX = np.cumproduct(lenX)

    if np.any(cumprodX < 0):
        raise ValueError(""Product space too large to allocate arrays!"")

    a = np.roll(cumprodX, 1)
    a[0] = 1

    if cumprodX[-1] != 0:
        b = cumprodX[-1] / cumprodX
    else:
        # if any factor is empty, the cartesian product is empty
        b = np.zeros_like(cumprodX)

    # error: Argument of type ""int_"" cannot be assigned to parameter ""num"" of
    # type ""int"" in function ""tile_compat""
    return [
        tile_compat(
            np.repeat(x, b[i]),
            np.product(a[i]),  # pyright: ignore[reportGeneralTypeIssues]
        )
        for i, x in enumerate(X)
    ]


def tile_compat(arr: NumpyIndexT, num: int) -> NumpyIndexT:
    """"""
    Index compat for np.tile.

    Notes
    -----
    Does not support multi-dimensional `num`.
    """"""
    if isinstance(arr, np.ndarray):
        return np.tile(arr, num)

    # Otherwise we have an Index
    taker = np.tile(np.arange(len(arr)), num)
    return arr.take(taker)"
272	jackson	3	"#!/usr/bin/env python
#
# Copyright 2020 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
""""""Demo for receiving notifications.""""""


def receive_notifications(project_id, subscription_name):
    # [START securitycenter_receive_notifications]
    # Requires https://cloud.google.com/pubsub/docs/quickstart-client-libraries#pubsub-client-libraries-python
    import concurrent

    from google.cloud import pubsub_v1
    from google.cloud.securitycenter_v1 import NotificationMessage

    # TODO: project_id = ""your-project-id""
    # TODO: subscription_name = ""your-subscription-name""

    def callback(message):

        # Print the data received for debugging purpose if needed
        print(f""Received message: {message.data}"")

        notification_msg = NotificationMessage.from_json(message.data)

        print(
            ""Notification config name: {}"".format(
                notification_msg.notification_config_name
            )
        )
        print(""Finding: {}"".format(notification_msg.finding))

        # Ack the message to prevent it from being pulled again
        message.ack()

    subscriber = pubsub_v1.SubscriberClient()
    subscription_path = subscriber.subscription_path(project_id, subscription_name)

    streaming_pull_future = subscriber.subscribe(subscription_path, callback=callback)

    print(""Listening for messages on {}...\n"".format(subscription_path))
    try:
        streaming_pull_future.result(timeout=1)  # Block for 1 second
    except concurrent.futures.TimeoutError:
        streaming_pull_future.cancel()
    # [END securitycenter_receive_notifications]
    return True"
272	donghui	1	"#!/usr/bin/env python
#
# Copyright 2020 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
""""""Demo for receiving notifications.""""""


def receive_notifications(project_id, subscription_name):
    # [START securitycenter_receive_notifications]
    # Requires https://cloud.google.com/pubsub/docs/quickstart-client-libraries#pubsub-client-libraries-python
    import concurrent

    from google.cloud import pubsub_v1
    from google.cloud.securitycenter_v1 import NotificationMessage

    # TODO: project_id = ""your-project-id""
    # TODO: subscription_name = ""your-subscription-name""

    def callback(message):

        # Print the data received for debugging purpose if needed
        print(f""Received message: {message.data}"")

        notification_msg = NotificationMessage.from_json(message.data)

        print(
            ""Notification config name: {}"".format(
                notification_msg.notification_config_name
            )
        )
        print(""Finding: {}"".format(notification_msg.finding))

        # Ack the message to prevent it from being pulled again
        message.ack()

    subscriber = pubsub_v1.SubscriberClient()
    subscription_path = subscriber.subscription_path(project_id, subscription_name)

    streaming_pull_future = subscriber.subscribe(subscription_path, callback=callback)

    print(""Listening for messages on {}...\n"".format(subscription_path))
    try:
        streaming_pull_future.result(timeout=1)  # Block for 1 second
    except concurrent.futures.TimeoutError:
        streaming_pull_future.cancel()
    # [END securitycenter_receive_notifications]
    return True"
306	jackson	3	"from . import engines
from .exceptions import TemplateDoesNotExist


def get_template(template_name, using=None):
    """"""
    Load and return a template for the given name.

    Raise TemplateDoesNotExist if no such template exists.
    """"""
    chain = []
    engines = _engine_list(using)
    for engine in engines:
        try:
            return engine.get_template(template_name)
        except TemplateDoesNotExist as e:
            chain.append(e)

    raise TemplateDoesNotExist(template_name, chain=chain)


def select_template(template_name_list, using=None):
    """"""
    Load and return a template for one of the given names.

    Try names in order and return the first template found.

    Raise TemplateDoesNotExist if no such template exists.
    """"""
    if isinstance(template_name_list, str):
        raise TypeError(
            ""select_template() takes an iterable of template names but got a ""
            ""string: %r. Use get_template() if you want to load a single ""
            ""template by name."" % template_name_list
        )

    chain = []
    engines = _engine_list(using)
    for template_name in template_name_list:
        for engine in engines:
            try:
                return engine.get_template(template_name)
            except TemplateDoesNotExist as e:
                chain.append(e)

    if template_name_list:
        raise TemplateDoesNotExist("", "".join(template_name_list), chain=chain)
    else:
        raise TemplateDoesNotExist(""No template names provided"")


def render_to_string(template_name, context=None, request=None, using=None):
    """"""
    Load a template and render it with a context. Return a string.

    template_name may be a string or a list of strings.
    """"""
    if isinstance(template_name, (list, tuple)):
        template = select_template(template_name, using=using)
    else:
        template = get_template(template_name, using=using)
    return template.render(context, request)


def _engine_list(using=None):
    return engines.all() if using is None else [engines[using]]"
306	donghui	2	"from . import engines
from .exceptions import TemplateDoesNotExist


def get_template(template_name, using=None):
    """"""
    Load and return a template for the given name.

    Raise TemplateDoesNotExist if no such template exists.
    """"""
    chain = []
    engines = _engine_list(using)
    for engine in engines:
        try:
            return engine.get_template(template_name)
        except TemplateDoesNotExist as e:
            chain.append(e)

    raise TemplateDoesNotExist(template_name, chain=chain)


def select_template(template_name_list, using=None):
    """"""
    Load and return a template for one of the given names.

    Try names in order and return the first template found.

    Raise TemplateDoesNotExist if no such template exists.
    """"""
    if isinstance(template_name_list, str):
        raise TypeError(
            ""select_template() takes an iterable of template names but got a ""
            ""string: %r. Use get_template() if you want to load a single ""
            ""template by name."" % template_name_list
        )

    chain = []
    engines = _engine_list(using)
    for template_name in template_name_list:
        for engine in engines:
            try:
                return engine.get_template(template_name)
            except TemplateDoesNotExist as e:
                chain.append(e)

    if template_name_list:
        raise TemplateDoesNotExist("", "".join(template_name_list), chain=chain)
    else:
        raise TemplateDoesNotExist(""No template names provided"")


def render_to_string(template_name, context=None, request=None, using=None):
    """"""
    Load a template and render it with a context. Return a string.

    template_name may be a string or a list of strings.
    """"""
    if isinstance(template_name, (list, tuple)):
        template = select_template(template_name, using=using)
    else:
        template = get_template(template_name, using=using)
    return template.render(context, request)


def _engine_list(using=None):
    return engines.all() if using is None else [engines[using]]"
357	jackson	0	"from typing import List
from collections import deque

class Solution:
    def maxAreaofIsland(self, grid: List[List[int]]) -> int:
        row = len(grid)
        col = len(grid[0])
        biggest_island = 0
        visited = [[False for i in range(col)] for j in range(row)]
        for i in range(row):
            for j in range(col):
                if grid[i][j] == 1 and visited[i][j] is False:
                    island_area = self.visitIsland(grid, visited, i,j)
                    biggest_island = max(island_area,biggest_island)
        return biggest_island

    def visitIsland(self, grid: List[List[int]], visited: List[List[int]], i: int, j: int) -> int:
        neighbours = deque([(i,j)])
        area = 0
        while neighbours:
            row,col = neighbours.popleft()
            if row < 0 or row >= len(grid) or col < 0 or col >= len(grid[0]):
                continue
            if visited[row][col] is False and grid[row][col] == 1:
                visited[row][col] = True
                area += 1
                neighbours.extend([(row + 1,col)])
                neighbours.extend([(row - 1, col)])
                neighbours.extend([(row, col + 1)])
                neighbours.extend([(row, col - 1)])
        return area

if __name__ == '__main__':
    solution = Solution()
    case1 = [[0,0,0,0,0,0,0,0]]
    assert solution.maxAreaofIsland(case1) == 0

    case2 = [[0,0,1,0,0,0,0,1,0,0,0,0,0],[0,0,0,0,0,0,0,1,1,1,0,0,0],[0,1,1,0,1,0,0,0,0,0,0,0,0],[0,1,0,0,1,1,0,0,1,0,1,0,0],
             [0,1,0,0,1,1,0,0,1,1,1,0,0],[0,0,0,0,0,0,0,0,0,0,1,0,0],[0,0,0,0,0,0,0,1,1,1,0,0,0],[0,0,0,0,0,0,0,1,1,0,0,0,0]]
    assert solution.maxAreaofIsland(case2) == 6

    case3 = [[1, 1, 1, 0, 0], [0, 1, 0, 0, 1], [0, 0, 1, 1, 0], [0, 1, 1, 0, 0], [0, 0, 1, 0, 0]]

    assert solution.maxAreaofIsland(case3) == 5"
357	donghui	0	"from typing import List
from collections import deque

class Solution:
    def maxAreaofIsland(self, grid: List[List[int]]) -> int:
        row = len(grid)
        col = len(grid[0])
        biggest_island = 0
        visited = [[False for i in range(col)] for j in range(row)]
        for i in range(row):
            for j in range(col):
                if grid[i][j] == 1 and visited[i][j] is False:
                    island_area = self.visitIsland(grid, visited, i,j)
                    biggest_island = max(island_area,biggest_island)
        return biggest_island

    def visitIsland(self, grid: List[List[int]], visited: List[List[int]], i: int, j: int) -> int:
        neighbours = deque([(i,j)])
        area = 0
        while neighbours:
            row,col = neighbours.popleft()
            if row < 0 or row >= len(grid) or col < 0 or col >= len(grid[0]):
                continue
            if visited[row][col] is False and grid[row][col] == 1:
                visited[row][col] = True
                area += 1
                neighbours.extend([(row + 1,col)])
                neighbours.extend([(row - 1, col)])
                neighbours.extend([(row, col + 1)])
                neighbours.extend([(row, col - 1)])
        return area

if __name__ == '__main__':
    solution = Solution()
    case1 = [[0,0,0,0,0,0,0,0]]
    assert solution.maxAreaofIsland(case1) == 0

    case2 = [[0,0,1,0,0,0,0,1,0,0,0,0,0],[0,0,0,0,0,0,0,1,1,1,0,0,0],[0,1,1,0,1,0,0,0,0,0,0,0,0],[0,1,0,0,1,1,0,0,1,0,1,0,0],
             [0,1,0,0,1,1,0,0,1,1,1,0,0],[0,0,0,0,0,0,0,0,0,0,1,0,0],[0,0,0,0,0,0,0,1,1,1,0,0,0],[0,0,0,0,0,0,0,1,1,0,0,0,0]]
    assert solution.maxAreaofIsland(case2) == 6

    case3 = [[1, 1, 1, 0, 0], [0, 1, 0, 0, 1], [0, 0, 1, 1, 0], [0, 1, 1, 0, 0], [0, 0, 1, 0, 0]]

    assert solution.maxAreaofIsland(case3) == 5"
419	jackson	2	"import _plotly_utils.basevalidators


class InsidetextfontValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(self, plotly_name=""insidetextfont"", parent_name=""pie"", **kwargs):
        super(InsidetextfontValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Insidetextfont""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            color

            colorsrc
                Sets the source reference on Chart Studio Cloud
                for `color`.
            family
                HTML font family - the typeface that will be
                applied by the web browser. The web browser
                will only be able to apply a font if it is
                available on the system which it operates.
                Provide multiple font families, separated by
                commas, to indicate the preference in which to
                apply fonts if they aren't available on the
                system. The Chart Studio Cloud (at
                https://chart-studio.plotly.com or on-premise)
                generates images on a server, where only a
                select number of fonts are installed and
                supported. These include ""Arial"", ""Balto"",
                ""Courier New"", ""Droid Sans"",, ""Droid Serif"",
                ""Droid Sans Mono"", ""Gravitas One"", ""Old
                Standard TT"", ""Open Sans"", ""Overpass"", ""PT Sans
                Narrow"", ""Raleway"", ""Times New Roman"".
            familysrc
                Sets the source reference on Chart Studio Cloud
                for `family`.
            size

            sizesrc
                Sets the source reference on Chart Studio Cloud
                for `size`.
"""""",
            ),
            **kwargs,
        )"
419	donghui	1	"import _plotly_utils.basevalidators


class InsidetextfontValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(self, plotly_name=""insidetextfont"", parent_name=""pie"", **kwargs):
        super(InsidetextfontValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Insidetextfont""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            color

            colorsrc
                Sets the source reference on Chart Studio Cloud
                for `color`.
            family
                HTML font family - the typeface that will be
                applied by the web browser. The web browser
                will only be able to apply a font if it is
                available on the system which it operates.
                Provide multiple font families, separated by
                commas, to indicate the preference in which to
                apply fonts if they aren't available on the
                system. The Chart Studio Cloud (at
                https://chart-studio.plotly.com or on-premise)
                generates images on a server, where only a
                select number of fonts are installed and
                supported. These include ""Arial"", ""Balto"",
                ""Courier New"", ""Droid Sans"",, ""Droid Serif"",
                ""Droid Sans Mono"", ""Gravitas One"", ""Old
                Standard TT"", ""Open Sans"", ""Overpass"", ""PT Sans
                Narrow"", ""Raleway"", ""Times New Roman"".
            familysrc
                Sets the source reference on Chart Studio Cloud
                for `family`.
            size

            sizesrc
                Sets the source reference on Chart Studio Cloud
                for `size`.
"""""",
            ),
            **kwargs,
        )"
508	jackson	1	"#
# Copyright 2018 the original author or authors.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
from google.protobuf.json_format import MessageToDict
from google.protobuf.message import Message
from simplejson import dumps
from common.event_bus import EventBusClient
from voltha.protos.omci_mib_db_pb2 import OpenOmciEvent
from voltha.protos.omci_alarm_db_pb2 import AlarmOpenOmciEvent
from common.utils.json_format import MessageToDict


class OpenOmciEventBus(object):
    """""" Event bus for publishing OpenOMCI related events. """"""
    __slots__ = (
        '_event_bus_client',  # The event bus client used to publish events.
        '_topic'              # the topic to publish to
    )

    def __init__(self):
        self._event_bus_client = EventBusClient()
        self._topic = 'openomci-events'

    def message_to_dict(m):
        return MessageToDict(m, True, True, False)

    def advertise(self, event_type, data):
        if isinstance(data, Message):
            msg = dumps(MessageToDict(data, True, True))
        elif isinstance(data, dict):
            msg = dumps(data)
        else:
            msg = str(data)

        event_func = AlarmOpenOmciEvent if 'AlarmSynchronizer' in msg \
                                  else OpenOmciEvent
        event = event_func(
                type=event_type,
                data=msg
        )

        self._event_bus_client.publish(self._topic, event)"
508	donghui	1	"#
# Copyright 2018 the original author or authors.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
from google.protobuf.json_format import MessageToDict
from google.protobuf.message import Message
from simplejson import dumps
from common.event_bus import EventBusClient
from voltha.protos.omci_mib_db_pb2 import OpenOmciEvent
from voltha.protos.omci_alarm_db_pb2 import AlarmOpenOmciEvent
from common.utils.json_format import MessageToDict


class OpenOmciEventBus(object):
    """""" Event bus for publishing OpenOMCI related events. """"""
    __slots__ = (
        '_event_bus_client',  # The event bus client used to publish events.
        '_topic'              # the topic to publish to
    )

    def __init__(self):
        self._event_bus_client = EventBusClient()
        self._topic = 'openomci-events'

    def message_to_dict(m):
        return MessageToDict(m, True, True, False)

    def advertise(self, event_type, data):
        if isinstance(data, Message):
            msg = dumps(MessageToDict(data, True, True))
        elif isinstance(data, dict):
            msg = dumps(data)
        else:
            msg = str(data)

        event_func = AlarmOpenOmciEvent if 'AlarmSynchronizer' in msg \
                                  else OpenOmciEvent
        event = event_func(
                type=event_type,
                data=msg
        )

        self._event_bus_client.publish(self._topic, event)"
448	jackson	1	"# -*- coding: utf-8 -
#
# This file is part of gunicorn released under the MIT license.
# See the NOTICE for more information.

import os

from gunicorn.errors import ConfigError
from gunicorn.app.base import Application
from gunicorn import util


class WSGIApplication(Application):
    def init(self, parser, opts, args):
        self.app_uri = None

        if opts.paste:
            from .pasterapp import has_logging_config

            config_uri = os.path.abspath(opts.paste)
            config_file = config_uri.split('#')[0]

            if not os.path.exists(config_file):
                raise ConfigError(""%r not found"" % config_file)

            self.cfg.set(""default_proc_name"", config_file)
            self.app_uri = config_uri

            if has_logging_config(config_file):
                self.cfg.set(""logconfig"", config_file)

            return

        if len(args) > 0:
            self.cfg.set(""default_proc_name"", args[0])
            self.app_uri = args[0]

    def load_config(self):
        super().load_config()

        if self.app_uri is None:
            if self.cfg.wsgi_app is not None:
                self.app_uri = self.cfg.wsgi_app
            else:
                raise ConfigError(""No application module specified."")

    def load_wsgiapp(self):
        return util.import_app(self.app_uri)

    def load_pasteapp(self):
        from .pasterapp import get_wsgi_app
        return get_wsgi_app(self.app_uri, defaults=self.cfg.paste_global_conf)

    def load(self):
        if self.cfg.paste is not None:
            return self.load_pasteapp()
        else:
            return self.load_wsgiapp()


def run():
    """"""\
    The ``gunicorn`` command line runner for launching Gunicorn with
    generic WSGI applications.
    """"""
    from gunicorn.app.wsgiapp import WSGIApplication
    WSGIApplication(""%(prog)s [OPTIONS] [APP_MODULE]"").run()


if __name__ == '__main__':
    run()"
448	donghui	1	"# -*- coding: utf-8 -
#
# This file is part of gunicorn released under the MIT license.
# See the NOTICE for more information.

import os

from gunicorn.errors import ConfigError
from gunicorn.app.base import Application
from gunicorn import util


class WSGIApplication(Application):
    def init(self, parser, opts, args):
        self.app_uri = None

        if opts.paste:
            from .pasterapp import has_logging_config

            config_uri = os.path.abspath(opts.paste)
            config_file = config_uri.split('#')[0]

            if not os.path.exists(config_file):
                raise ConfigError(""%r not found"" % config_file)

            self.cfg.set(""default_proc_name"", config_file)
            self.app_uri = config_uri

            if has_logging_config(config_file):
                self.cfg.set(""logconfig"", config_file)

            return

        if len(args) > 0:
            self.cfg.set(""default_proc_name"", args[0])
            self.app_uri = args[0]

    def load_config(self):
        super().load_config()

        if self.app_uri is None:
            if self.cfg.wsgi_app is not None:
                self.app_uri = self.cfg.wsgi_app
            else:
                raise ConfigError(""No application module specified."")

    def load_wsgiapp(self):
        return util.import_app(self.app_uri)

    def load_pasteapp(self):
        from .pasterapp import get_wsgi_app
        return get_wsgi_app(self.app_uri, defaults=self.cfg.paste_global_conf)

    def load(self):
        if self.cfg.paste is not None:
            return self.load_pasteapp()
        else:
            return self.load_wsgiapp()


def run():
    """"""\
    The ``gunicorn`` command line runner for launching Gunicorn with
    generic WSGI applications.
    """"""
    from gunicorn.app.wsgiapp import WSGIApplication
    WSGIApplication(""%(prog)s [OPTIONS] [APP_MODULE]"").run()


if __name__ == '__main__':
    run()"
458	jackson	1	"# This file is part of Scapy
# See http://www.secdev.org/projects/scapy for more information
# Copyright (C) Philippe Biondi <phil@secdev.org>
# This program is published under a GPLv2 license

""""""
External link to programs
""""""

import os
import subprocess
from scapy.error import log_loading

# Notice: this file must not be called before main.py, if started
# in interactive mode, because it needs to be called after the
# logger has been setup, to be able to print the warning messages

# MATPLOTLIB

try:
    from matplotlib import get_backend as matplotlib_get_backend
    from matplotlib import pyplot as plt
    from matplotlib.lines import Line2D
    MATPLOTLIB = 1
    if ""inline"" in matplotlib_get_backend():
        MATPLOTLIB_INLINED = 1
    else:
        MATPLOTLIB_INLINED = 0
    MATPLOTLIB_DEFAULT_PLOT_KARGS = {""marker"": ""+""}
# RuntimeError to catch gtk ""Cannot open display"" error
except (ImportError, RuntimeError):
    plt = None
    Line2D = None
    MATPLOTLIB = 0
    MATPLOTLIB_INLINED = 0
    MATPLOTLIB_DEFAULT_PLOT_KARGS = dict()
    log_loading.info(""Can't import matplotlib. Won't be able to plot."")

# PYX


def _test_pyx():
    # type: () -> bool
    """"""Returns if PyX is correctly installed or not""""""
    try:
        with open(os.devnull, 'wb') as devnull:
            r = subprocess.check_call([""pdflatex"", ""--version""],
                                      stdout=devnull, stderr=subprocess.STDOUT)
    except (subprocess.CalledProcessError, OSError):
        return False
    else:
        return r == 0


try:
    import pyx  # noqa: F401
    if _test_pyx():
        PYX = 1
    else:
        log_loading.info(""PyX dependencies are not installed ! Please install TexLive or MikTeX."")  # noqa: E501
        PYX = 0
except ImportError:
    log_loading.info(""Can't import PyX. Won't be able to use psdump() or pdfdump()."")  # noqa: E501
    PYX = 0"
458	donghui	2	"# This file is part of Scapy
# See http://www.secdev.org/projects/scapy for more information
# Copyright (C) Philippe Biondi <phil@secdev.org>
# This program is published under a GPLv2 license

""""""
External link to programs
""""""

import os
import subprocess
from scapy.error import log_loading

# Notice: this file must not be called before main.py, if started
# in interactive mode, because it needs to be called after the
# logger has been setup, to be able to print the warning messages

# MATPLOTLIB

try:
    from matplotlib import get_backend as matplotlib_get_backend
    from matplotlib import pyplot as plt
    from matplotlib.lines import Line2D
    MATPLOTLIB = 1
    if ""inline"" in matplotlib_get_backend():
        MATPLOTLIB_INLINED = 1
    else:
        MATPLOTLIB_INLINED = 0
    MATPLOTLIB_DEFAULT_PLOT_KARGS = {""marker"": ""+""}
# RuntimeError to catch gtk ""Cannot open display"" error
except (ImportError, RuntimeError):
    plt = None
    Line2D = None
    MATPLOTLIB = 0
    MATPLOTLIB_INLINED = 0
    MATPLOTLIB_DEFAULT_PLOT_KARGS = dict()
    log_loading.info(""Can't import matplotlib. Won't be able to plot."")

# PYX


def _test_pyx():
    # type: () -> bool
    """"""Returns if PyX is correctly installed or not""""""
    try:
        with open(os.devnull, 'wb') as devnull:
            r = subprocess.check_call([""pdflatex"", ""--version""],
                                      stdout=devnull, stderr=subprocess.STDOUT)
    except (subprocess.CalledProcessError, OSError):
        return False
    else:
        return r == 0


try:
    import pyx  # noqa: F401
    if _test_pyx():
        PYX = 1
    else:
        log_loading.info(""PyX dependencies are not installed ! Please install TexLive or MikTeX."")  # noqa: E501
        PYX = 0
except ImportError:
    log_loading.info(""Can't import PyX. Won't be able to use psdump() or pdfdump()."")  # noqa: E501
    PYX = 0"
409	jackson	2	"import os
import json

import torch
from PIL import Image
from torchvision import transforms
import matplotlib.pyplot as plt

from vit_model import vit_base_patch16_224_in21k as create_model


def main():
    device = torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu"")

    data_transform = transforms.Compose(
        [transforms.Resize(256),
         transforms.CenterCrop(224),
         transforms.ToTensor(),
         transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])

    # load image
    img_path = ""../tulip.jpg""
    assert os.path.exists(img_path), ""file: '{}' dose not exist."".format(img_path)
    img = Image.open(img_path)
    plt.imshow(img)
    # [N, C, H, W]
    img = data_transform(img)
    # expand batch dimension
    img = torch.unsqueeze(img, dim=0)

    # read class_indict
    json_path = './class_indices.json'
    assert os.path.exists(json_path), ""file: '{}' dose not exist."".format(json_path)

    with open(json_path, ""r"") as f:
        class_indict = json.load(f)

    # create model
    model = create_model(num_classes=5, has_logits=False).to(device)
    # load model weights
    model_weight_path = ""./weights/model-9.pth""
    model.load_state_dict(torch.load(model_weight_path, map_location=device))
    model.eval()
    with torch.no_grad():
        # predict class
        output = torch.squeeze(model(img.to(device))).cpu()
        predict = torch.softmax(output, dim=0)
        predict_cla = torch.argmax(predict).numpy()

    print_res = ""class: {}   prob: {:.3}"".format(class_indict[str(predict_cla)],
                                                 predict[predict_cla].numpy())
    plt.title(print_res)
    for i in range(len(predict)):
        print(""class: {:10}   prob: {:.3}"".format(class_indict[str(i)],
                                                  predict[i].numpy()))
    plt.show()


if __name__ == '__main__':
    main()"
409	donghui	2	"import os
import json

import torch
from PIL import Image
from torchvision import transforms
import matplotlib.pyplot as plt

from vit_model import vit_base_patch16_224_in21k as create_model


def main():
    device = torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu"")

    data_transform = transforms.Compose(
        [transforms.Resize(256),
         transforms.CenterCrop(224),
         transforms.ToTensor(),
         transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])

    # load image
    img_path = ""../tulip.jpg""
    assert os.path.exists(img_path), ""file: '{}' dose not exist."".format(img_path)
    img = Image.open(img_path)
    plt.imshow(img)
    # [N, C, H, W]
    img = data_transform(img)
    # expand batch dimension
    img = torch.unsqueeze(img, dim=0)

    # read class_indict
    json_path = './class_indices.json'
    assert os.path.exists(json_path), ""file: '{}' dose not exist."".format(json_path)

    with open(json_path, ""r"") as f:
        class_indict = json.load(f)

    # create model
    model = create_model(num_classes=5, has_logits=False).to(device)
    # load model weights
    model_weight_path = ""./weights/model-9.pth""
    model.load_state_dict(torch.load(model_weight_path, map_location=device))
    model.eval()
    with torch.no_grad():
        # predict class
        output = torch.squeeze(model(img.to(device))).cpu()
        predict = torch.softmax(output, dim=0)
        predict_cla = torch.argmax(predict).numpy()

    print_res = ""class: {}   prob: {:.3}"".format(class_indict[str(predict_cla)],
                                                 predict[predict_cla].numpy())
    plt.title(print_res)
    for i in range(len(predict)):
        print(""class: {:10}   prob: {:.3}"".format(class_indict[str(i)],
                                                  predict[i].numpy()))
    plt.show()


if __name__ == '__main__':
    main()"
347	jackson	0	"from functools import partial

import pytest

from ..argument import Argument, to_arguments
from ..field import Field
from ..inputfield import InputField
from ..scalars import String
from ..structures import NonNull


def test_argument():
    arg = Argument(String, default_value=""a"", description=""desc"", name=""b"")
    assert arg.type == String
    assert arg.default_value == ""a""
    assert arg.description == ""desc""
    assert arg.name == ""b""


def test_argument_comparasion():
    arg1 = Argument(String, name=""Hey"", description=""Desc"", default_value=""default"")
    arg2 = Argument(String, name=""Hey"", description=""Desc"", default_value=""default"")

    assert arg1 == arg2
    assert arg1 != String()


def test_argument_required():
    arg = Argument(String, required=True)
    assert arg.type == NonNull(String)


def test_to_arguments():
    args = {""arg_string"": Argument(String), ""unmounted_arg"": String(required=True)}

    my_args = to_arguments(args)
    assert my_args == {
        ""arg_string"": Argument(String),
        ""unmounted_arg"": Argument(String, required=True),
    }


def test_to_arguments_raises_if_field():
    args = {""arg_string"": Field(String)}

    with pytest.raises(ValueError) as exc_info:
        to_arguments(args)

    assert str(exc_info.value) == (
        ""Expected arg_string to be Argument, but received Field. Try using ""
        ""Argument(String).""
    )


def test_to_arguments_raises_if_inputfield():
    args = {""arg_string"": InputField(String)}

    with pytest.raises(ValueError) as exc_info:
        to_arguments(args)

    assert str(exc_info.value) == (
        ""Expected arg_string to be Argument, but received InputField. Try ""
        ""using Argument(String).""
    )


def test_argument_with_lazy_type():
    MyType = object()
    arg = Argument(lambda: MyType)
    assert arg.type == MyType


def test_argument_with_lazy_partial_type():
    MyType = object()
    arg = Argument(partial(lambda: MyType))
    assert arg.type == MyType"
347	donghui	0	"from functools import partial

import pytest

from ..argument import Argument, to_arguments
from ..field import Field
from ..inputfield import InputField
from ..scalars import String
from ..structures import NonNull


def test_argument():
    arg = Argument(String, default_value=""a"", description=""desc"", name=""b"")
    assert arg.type == String
    assert arg.default_value == ""a""
    assert arg.description == ""desc""
    assert arg.name == ""b""


def test_argument_comparasion():
    arg1 = Argument(String, name=""Hey"", description=""Desc"", default_value=""default"")
    arg2 = Argument(String, name=""Hey"", description=""Desc"", default_value=""default"")

    assert arg1 == arg2
    assert arg1 != String()


def test_argument_required():
    arg = Argument(String, required=True)
    assert arg.type == NonNull(String)


def test_to_arguments():
    args = {""arg_string"": Argument(String), ""unmounted_arg"": String(required=True)}

    my_args = to_arguments(args)
    assert my_args == {
        ""arg_string"": Argument(String),
        ""unmounted_arg"": Argument(String, required=True),
    }


def test_to_arguments_raises_if_field():
    args = {""arg_string"": Field(String)}

    with pytest.raises(ValueError) as exc_info:
        to_arguments(args)

    assert str(exc_info.value) == (
        ""Expected arg_string to be Argument, but received Field. Try using ""
        ""Argument(String).""
    )


def test_to_arguments_raises_if_inputfield():
    args = {""arg_string"": InputField(String)}

    with pytest.raises(ValueError) as exc_info:
        to_arguments(args)

    assert str(exc_info.value) == (
        ""Expected arg_string to be Argument, but received InputField. Try ""
        ""using Argument(String).""
    )


def test_argument_with_lazy_type():
    MyType = object()
    arg = Argument(lambda: MyType)
    assert arg.type == MyType


def test_argument_with_lazy_partial_type():
    MyType = object()
    arg = Argument(partial(lambda: MyType))
    assert arg.type == MyType"
316	jackson	3	"import numpy as np
import numba

# - plotStratigraphy takes 1) XorY_StratiOverTime (time and either x or y dimensions): strati__elevation selected for only the basin area and either averaged or selected for one across (y)/down(x) basin distance 
#     2) XorY_GrainSizeOverTime (time and either x or y dimensions) the grain size or erosion rate or other desired variable that will be used to fill the stratigraphy. This also needs to be selected or averaged for one x/y distance. 
# - stratigraphy as it is written assumes that channels are draining either in the x or y direction (mountain along one axis) and stratigraphy is generated along one axis.     
# - plotStratigraphy averages the nearby nodes (grain size or erosion rate or other desired value passed) to fill a given cell of stratigraphy.
# -plotStraigraphy2 does not average the nearest nodes and takes the first closest value to fill the stratigraphy. 

@numba.njit
def plotStratigraphy(XorY_StratiOverTime,XorY_GrainSizeOverTime):
    i=0
    j=0
    C=np.zeros(((XorY_StratiOverTime.shape[0]),(XorY_StratiOverTime.shape[1])))
    for i in range(0,(XorY_StratiOverTime.shape[1])):
        for j in range(0,(XorY_StratiOverTime.shape[0])):
            tryff=np.array([XorY_GrainSizeOverTime[j,i],XorY_GrainSizeOverTime[j,i+1],XorY_GrainSizeOverTime[j+1,i],XorY_GrainSizeOverTime[j+1,i+1]])
            C[j,i]=np.nanmean(tryff)
    return C

@numba.njit
def plotStratigraphy2(XorY_StratiOverTime,XorY_GrainSizeOverTime):
    i=0
    j=0
    C=np.zeros(((XorY_StratiOverTime.shape[0]),(XorY_StratiOverTime.shape[1])))
    for i in range(0,(XorY_StratiOverTime.shape[1])):
        for j in range(0,(XorY_StratiOverTime.shape[0])):
            #tryff=np.array([XorY_GrainSizeOverTime[j,i],XorY_GrainSizeOverTime[j,i+1],XorY_GrainSizeOverTime[j+1,i],XorY_GrainSizeOverTime[j+1,i+1]])
            C[j,i]=(XorY_GrainSizeOverTime[j,i])
    return C"
316	donghui	1	"import numpy as np
import numba

# - plotStratigraphy takes 1) XorY_StratiOverTime (time and either x or y dimensions): strati__elevation selected for only the basin area and either averaged or selected for one across (y)/down(x) basin distance 
#     2) XorY_GrainSizeOverTime (time and either x or y dimensions) the grain size or erosion rate or other desired variable that will be used to fill the stratigraphy. This also needs to be selected or averaged for one x/y distance. 
# - stratigraphy as it is written assumes that channels are draining either in the x or y direction (mountain along one axis) and stratigraphy is generated along one axis.     
# - plotStratigraphy averages the nearby nodes (grain size or erosion rate or other desired value passed) to fill a given cell of stratigraphy.
# -plotStraigraphy2 does not average the nearest nodes and takes the first closest value to fill the stratigraphy. 

@numba.njit
def plotStratigraphy(XorY_StratiOverTime,XorY_GrainSizeOverTime):
    i=0
    j=0
    C=np.zeros(((XorY_StratiOverTime.shape[0]),(XorY_StratiOverTime.shape[1])))
    for i in range(0,(XorY_StratiOverTime.shape[1])):
        for j in range(0,(XorY_StratiOverTime.shape[0])):
            tryff=np.array([XorY_GrainSizeOverTime[j,i],XorY_GrainSizeOverTime[j,i+1],XorY_GrainSizeOverTime[j+1,i],XorY_GrainSizeOverTime[j+1,i+1]])
            C[j,i]=np.nanmean(tryff)
    return C

@numba.njit
def plotStratigraphy2(XorY_StratiOverTime,XorY_GrainSizeOverTime):
    i=0
    j=0
    C=np.zeros(((XorY_StratiOverTime.shape[0]),(XorY_StratiOverTime.shape[1])))
    for i in range(0,(XorY_StratiOverTime.shape[1])):
        for j in range(0,(XorY_StratiOverTime.shape[0])):
            #tryff=np.array([XorY_GrainSizeOverTime[j,i],XorY_GrainSizeOverTime[j,i+1],XorY_GrainSizeOverTime[j+1,i],XorY_GrainSizeOverTime[j+1,i+1]])
            C[j,i]=(XorY_GrainSizeOverTime[j,i])
    return C"
256	jackson	1	"import win32api, win32security
import win32con, ntsecuritycon, winnt
import os

temp_dir = win32api.GetTempPath()
fname = win32api.GetTempFileName(temp_dir, ""rsk"")[0]
print(fname)
## file can't exist
os.remove(fname)

## enable backup and restore privs
required_privs = (
    (
        win32security.LookupPrivilegeValue("""", ntsecuritycon.SE_BACKUP_NAME),
        win32con.SE_PRIVILEGE_ENABLED,
    ),
    (
        win32security.LookupPrivilegeValue("""", ntsecuritycon.SE_RESTORE_NAME),
        win32con.SE_PRIVILEGE_ENABLED,
    ),
)
ph = win32api.GetCurrentProcess()
th = win32security.OpenProcessToken(
    ph, win32con.TOKEN_READ | win32con.TOKEN_ADJUST_PRIVILEGES
)
adjusted_privs = win32security.AdjustTokenPrivileges(th, 0, required_privs)

try:
    sa = win32security.SECURITY_ATTRIBUTES()
    my_sid = win32security.GetTokenInformation(th, ntsecuritycon.TokenUser)[0]
    sa.SECURITY_DESCRIPTOR.SetSecurityDescriptorOwner(my_sid, 0)

    k, disp = win32api.RegCreateKeyEx(
        win32con.HKEY_CURRENT_USER,
        ""Python test key"",
        SecurityAttributes=sa,
        samDesired=win32con.KEY_ALL_ACCESS,
        Class=""some class"",
        Options=0,
    )
    win32api.RegSetValue(k, None, win32con.REG_SZ, ""Default value for python test key"")

    subk, disp = win32api.RegCreateKeyEx(
        k,
        ""python test subkey"",
        SecurityAttributes=sa,
        samDesired=win32con.KEY_ALL_ACCESS,
        Class=""some other class"",
        Options=0,
    )
    win32api.RegSetValue(subk, None, win32con.REG_SZ, ""Default value for subkey"")

    win32api.RegSaveKeyEx(
        k, fname, Flags=winnt.REG_STANDARD_FORMAT, SecurityAttributes=sa
    )

    restored_key, disp = win32api.RegCreateKeyEx(
        win32con.HKEY_CURRENT_USER,
        ""Python test key(restored)"",
        SecurityAttributes=sa,
        samDesired=win32con.KEY_ALL_ACCESS,
        Class=""restored class"",
        Options=0,
    )
    win32api.RegRestoreKey(restored_key, fname)
finally:
    win32security.AdjustTokenPrivileges(th, 0, adjusted_privs)"
256	donghui	1	"import win32api, win32security
import win32con, ntsecuritycon, winnt
import os

temp_dir = win32api.GetTempPath()
fname = win32api.GetTempFileName(temp_dir, ""rsk"")[0]
print(fname)
## file can't exist
os.remove(fname)

## enable backup and restore privs
required_privs = (
    (
        win32security.LookupPrivilegeValue("""", ntsecuritycon.SE_BACKUP_NAME),
        win32con.SE_PRIVILEGE_ENABLED,
    ),
    (
        win32security.LookupPrivilegeValue("""", ntsecuritycon.SE_RESTORE_NAME),
        win32con.SE_PRIVILEGE_ENABLED,
    ),
)
ph = win32api.GetCurrentProcess()
th = win32security.OpenProcessToken(
    ph, win32con.TOKEN_READ | win32con.TOKEN_ADJUST_PRIVILEGES
)
adjusted_privs = win32security.AdjustTokenPrivileges(th, 0, required_privs)

try:
    sa = win32security.SECURITY_ATTRIBUTES()
    my_sid = win32security.GetTokenInformation(th, ntsecuritycon.TokenUser)[0]
    sa.SECURITY_DESCRIPTOR.SetSecurityDescriptorOwner(my_sid, 0)

    k, disp = win32api.RegCreateKeyEx(
        win32con.HKEY_CURRENT_USER,
        ""Python test key"",
        SecurityAttributes=sa,
        samDesired=win32con.KEY_ALL_ACCESS,
        Class=""some class"",
        Options=0,
    )
    win32api.RegSetValue(k, None, win32con.REG_SZ, ""Default value for python test key"")

    subk, disp = win32api.RegCreateKeyEx(
        k,
        ""python test subkey"",
        SecurityAttributes=sa,
        samDesired=win32con.KEY_ALL_ACCESS,
        Class=""some other class"",
        Options=0,
    )
    win32api.RegSetValue(subk, None, win32con.REG_SZ, ""Default value for subkey"")

    win32api.RegSaveKeyEx(
        k, fname, Flags=winnt.REG_STANDARD_FORMAT, SecurityAttributes=sa
    )

    restored_key, disp = win32api.RegCreateKeyEx(
        win32con.HKEY_CURRENT_USER,
        ""Python test key(restored)"",
        SecurityAttributes=sa,
        samDesired=win32con.KEY_ALL_ACCESS,
        Class=""restored class"",
        Options=0,
    )
    win32api.RegRestoreKey(restored_key, fname)
finally:
    win32security.AdjustTokenPrivileges(th, 0, adjusted_privs)"
262	jackson	1	"import hashlib
import hmac
import re
import time
from binascii import a2b_hex


AUTH_TOKEN_NAME = ""__cld_token__""
AUTH_TOKEN_SEPARATOR = ""~""
AUTH_TOKEN_UNSAFE_RE = r'([ ""#%&\'\/:;<=>?@\[\\\]^`{\|}~]+)'


def generate(url=None, acl=None, start_time=None, duration=None,
             expiration=None, ip=None, key=None, token_name=AUTH_TOKEN_NAME):

    if expiration is None:
        if duration is not None:
            start = start_time if start_time is not None else int(time.time())
            expiration = start + duration
        else:
            raise Exception(""Must provide either expiration or duration"")

    if url is None and acl is None:
        raise Exception(""Must provide either acl or url"")

    token_parts = []
    if ip is not None:
        token_parts.append(""ip="" + ip)
    if start_time is not None:
        token_parts.append(""st=%d"" % start_time)
    token_parts.append(""exp=%d"" % expiration)
    if acl is not None:
        acl_list = acl if type(acl) is list else [acl]
        acl_list = [_escape_to_lower(a) for a in acl_list] 
        token_parts.append(""acl=%s"" % ""!"".join(acl_list))
    to_sign = list(token_parts)
    if url is not None and acl is None:
        to_sign.append(""url=%s"" % _escape_to_lower(url))
    auth = _digest(AUTH_TOKEN_SEPARATOR.join(to_sign), key)
    token_parts.append(""hmac=%s"" % auth)
    return ""%(token_name)s=%(token)s"" % {""token_name"": token_name, ""token"": AUTH_TOKEN_SEPARATOR.join(token_parts)}


def _digest(message, key):
    bin_key = a2b_hex(key)
    return hmac.new(bin_key, message.encode('utf-8'), hashlib.sha256).hexdigest()


def _escape_to_lower(url):
    # There is a circular import issue in this file, need to resolve it in the next major release
    from cloudinary.utils import smart_escape
    escaped_url = smart_escape(url, unsafe=AUTH_TOKEN_UNSAFE_RE)
    escaped_url = re.sub(r""%[0-9A-F]{2}"", lambda x: x.group(0).lower(), escaped_url)
    return escaped_url"
262	donghui	1	"import hashlib
import hmac
import re
import time
from binascii import a2b_hex


AUTH_TOKEN_NAME = ""__cld_token__""
AUTH_TOKEN_SEPARATOR = ""~""
AUTH_TOKEN_UNSAFE_RE = r'([ ""#%&\'\/:;<=>?@\[\\\]^`{\|}~]+)'


def generate(url=None, acl=None, start_time=None, duration=None,
             expiration=None, ip=None, key=None, token_name=AUTH_TOKEN_NAME):

    if expiration is None:
        if duration is not None:
            start = start_time if start_time is not None else int(time.time())
            expiration = start + duration
        else:
            raise Exception(""Must provide either expiration or duration"")

    if url is None and acl is None:
        raise Exception(""Must provide either acl or url"")

    token_parts = []
    if ip is not None:
        token_parts.append(""ip="" + ip)
    if start_time is not None:
        token_parts.append(""st=%d"" % start_time)
    token_parts.append(""exp=%d"" % expiration)
    if acl is not None:
        acl_list = acl if type(acl) is list else [acl]
        acl_list = [_escape_to_lower(a) for a in acl_list] 
        token_parts.append(""acl=%s"" % ""!"".join(acl_list))
    to_sign = list(token_parts)
    if url is not None and acl is None:
        to_sign.append(""url=%s"" % _escape_to_lower(url))
    auth = _digest(AUTH_TOKEN_SEPARATOR.join(to_sign), key)
    token_parts.append(""hmac=%s"" % auth)
    return ""%(token_name)s=%(token)s"" % {""token_name"": token_name, ""token"": AUTH_TOKEN_SEPARATOR.join(token_parts)}


def _digest(message, key):
    bin_key = a2b_hex(key)
    return hmac.new(bin_key, message.encode('utf-8'), hashlib.sha256).hexdigest()


def _escape_to_lower(url):
    # There is a circular import issue in this file, need to resolve it in the next major release
    from cloudinary.utils import smart_escape
    escaped_url = smart_escape(url, unsafe=AUTH_TOKEN_UNSAFE_RE)
    escaped_url = re.sub(r""%[0-9A-F]{2}"", lambda x: x.group(0).lower(), escaped_url)
    return escaped_url"
322	jackson	4	"""""""
Mozilla Persona authentication backend, docs at:
    https://python-social-auth.readthedocs.io/en/latest/backends/persona.html
""""""
from ..exceptions import AuthFailed, AuthMissingParameter
from ..utils import handle_http_errors
from .base import BaseAuth


class PersonaAuth(BaseAuth):
    """"""BrowserID authentication backend""""""
    name = 'persona'

    def get_user_id(self, details, response):
        """"""Use BrowserID email as ID""""""
        return details['email']

    def get_user_details(self, response):
        """"""Return user details, BrowserID only provides Email.""""""
        # {'status': 'okay',
        #  'audience': 'localhost:8000',
        #  'expires': 1328983575529,
        #  'email': 'name@server.com',
        #  'issuer': 'browserid.org'}
        email = response['email']
        return {'username': email.split('@', 1)[0],
                'email': email,
                'fullname': '',
                'first_name': '',
                'last_name': ''}

    def extra_data(self, user, uid, response, details=None, *args, **kwargs):
        """"""Return users extra data""""""
        return {'audience': response['audience'],
                'issuer': response['issuer']}

    @handle_http_errors
    def auth_complete(self, *args, **kwargs):
        """"""Completes login process, must return user instance""""""
        if 'assertion' not in self.data:
            raise AuthMissingParameter(self, 'assertion')

        response = self.get_json('https://browserid.org/verify', data={
            'assertion': self.data['assertion'],
            'audience': self.strategy.request_host()
        }, method='POST')
        if response.get('status') == 'failure':
            raise AuthFailed(self)
        kwargs.update({'response': response, 'backend': self})
        return self.strategy.authenticate(*args, **kwargs)"
322	donghui	2	"""""""
Mozilla Persona authentication backend, docs at:
    https://python-social-auth.readthedocs.io/en/latest/backends/persona.html
""""""
from ..exceptions import AuthFailed, AuthMissingParameter
from ..utils import handle_http_errors
from .base import BaseAuth


class PersonaAuth(BaseAuth):
    """"""BrowserID authentication backend""""""
    name = 'persona'

    def get_user_id(self, details, response):
        """"""Use BrowserID email as ID""""""
        return details['email']

    def get_user_details(self, response):
        """"""Return user details, BrowserID only provides Email.""""""
        # {'status': 'okay',
        #  'audience': 'localhost:8000',
        #  'expires': 1328983575529,
        #  'email': 'name@server.com',
        #  'issuer': 'browserid.org'}
        email = response['email']
        return {'username': email.split('@', 1)[0],
                'email': email,
                'fullname': '',
                'first_name': '',
                'last_name': ''}

    def extra_data(self, user, uid, response, details=None, *args, **kwargs):
        """"""Return users extra data""""""
        return {'audience': response['audience'],
                'issuer': response['issuer']}

    @handle_http_errors
    def auth_complete(self, *args, **kwargs):
        """"""Completes login process, must return user instance""""""
        if 'assertion' not in self.data:
            raise AuthMissingParameter(self, 'assertion')

        response = self.get_json('https://browserid.org/verify', data={
            'assertion': self.data['assertion'],
            'audience': self.strategy.request_host()
        }, method='POST')
        if response.get('status') == 'failure':
            raise AuthFailed(self)
        kwargs.update({'response': response, 'backend': self})
        return self.strategy.authenticate(*args, **kwargs)"
380	jackson	0	"######################## BEGIN LICENSE BLOCK ########################
# The Original Code is Mozilla Universal charset detector code.
#
# The Initial Developer of the Original Code is
# Netscape Communications Corporation.
# Portions created by the Initial Developer are Copyright (C) 2001
# the Initial Developer. All Rights Reserved.
#
# Contributor(s):
#   Mark Pilgrim - port to Python
#   Shy Shalom - original C code
#   Proofpoint, Inc.
#
# This library is free software; you can redistribute it and/or
# modify it under the terms of the GNU Lesser General Public
# License as published by the Free Software Foundation; either
# version 2.1 of the License, or (at your option) any later version.
#
# This library is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public
# License along with this library; if not, write to the Free Software
# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA
# 02110-1301  USA
######################### END LICENSE BLOCK #########################

from .big5prober import Big5Prober
from .charsetgroupprober import CharSetGroupProber
from .cp949prober import CP949Prober
from .enums import LanguageFilter
from .eucjpprober import EUCJPProber
from .euckrprober import EUCKRProber
from .euctwprober import EUCTWProber
from .gb2312prober import GB2312Prober
from .johabprober import JOHABProber
from .sjisprober import SJISProber
from .utf8prober import UTF8Prober


class MBCSGroupProber(CharSetGroupProber):
    def __init__(self, lang_filter: LanguageFilter = LanguageFilter.NONE) -> None:
        super().__init__(lang_filter=lang_filter)
        self.probers = [
            UTF8Prober(),
            SJISProber(),
            EUCJPProber(),
            GB2312Prober(),
            EUCKRProber(),
            CP949Prober(),
            Big5Prober(),
            EUCTWProber(),
            JOHABProber(),
        ]
        self.reset()"
380	donghui	0	"######################## BEGIN LICENSE BLOCK ########################
# The Original Code is Mozilla Universal charset detector code.
#
# The Initial Developer of the Original Code is
# Netscape Communications Corporation.
# Portions created by the Initial Developer are Copyright (C) 2001
# the Initial Developer. All Rights Reserved.
#
# Contributor(s):
#   Mark Pilgrim - port to Python
#   Shy Shalom - original C code
#   Proofpoint, Inc.
#
# This library is free software; you can redistribute it and/or
# modify it under the terms of the GNU Lesser General Public
# License as published by the Free Software Foundation; either
# version 2.1 of the License, or (at your option) any later version.
#
# This library is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public
# License along with this library; if not, write to the Free Software
# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA
# 02110-1301  USA
######################### END LICENSE BLOCK #########################

from .big5prober import Big5Prober
from .charsetgroupprober import CharSetGroupProber
from .cp949prober import CP949Prober
from .enums import LanguageFilter
from .eucjpprober import EUCJPProber
from .euckrprober import EUCKRProber
from .euctwprober import EUCTWProber
from .gb2312prober import GB2312Prober
from .johabprober import JOHABProber
from .sjisprober import SJISProber
from .utf8prober import UTF8Prober


class MBCSGroupProber(CharSetGroupProber):
    def __init__(self, lang_filter: LanguageFilter = LanguageFilter.NONE) -> None:
        super().__init__(lang_filter=lang_filter)
        self.probers = [
            UTF8Prober(),
            SJISProber(),
            EUCJPProber(),
            GB2312Prober(),
            EUCKRProber(),
            CP949Prober(),
            Big5Prober(),
            EUCTWProber(),
            JOHABProber(),
        ]
        self.reset()"
291	jackson	4	"from django import template
from django.contrib.admin.models import LogEntry

register = template.Library()


class AdminLogNode(template.Node):
    def __init__(self, limit, varname, user):
        self.limit, self.varname, self.user = limit, varname, user

    def __repr__(self):
        return ""<GetAdminLog Node>""

    def render(self, context):
        if self.user is None:
            entries = LogEntry.objects.all()
        else:
            user_id = self.user
            if not user_id.isdigit():
                user_id = context[self.user].pk
            entries = LogEntry.objects.filter(user__pk=user_id)
        context[self.varname] = entries.select_related('content_type', 'user')[:int(self.limit)]
        return ''


@register.tag
def get_admin_log(parser, token):
    """"""
    Populate a template variable with the admin log for the given criteria.

    Usage::

        {% get_admin_log [limit] as [varname] for_user [context_var_containing_user_obj] %}

    Examples::

        {% get_admin_log 10 as admin_log for_user 23 %}
        {% get_admin_log 10 as admin_log for_user user %}
        {% get_admin_log 10 as admin_log %}

    Note that ``context_var_containing_user_obj`` can be a hard-coded integer
    (user ID) or the name of a template context variable containing the user
    object whose ID you want.
    """"""
    tokens = token.contents.split()
    if len(tokens) < 4:
        raise template.TemplateSyntaxError(
            ""'get_admin_log' statements require two arguments"")
    if not tokens[1].isdigit():
        raise template.TemplateSyntaxError(
            ""First argument to 'get_admin_log' must be an integer"")
    if tokens[2] != 'as':
        raise template.TemplateSyntaxError(
            ""Second argument to 'get_admin_log' must be 'as'"")
    if len(tokens) > 4:
        if tokens[4] != 'for_user':
            raise template.TemplateSyntaxError(
                ""Fourth argument to 'get_admin_log' must be 'for_user'"")
    return AdminLogNode(limit=tokens[1], varname=tokens[3], user=(tokens[5] if len(tokens) > 5 else None))"
291	donghui	4	"from django import template
from django.contrib.admin.models import LogEntry

register = template.Library()


class AdminLogNode(template.Node):
    def __init__(self, limit, varname, user):
        self.limit, self.varname, self.user = limit, varname, user

    def __repr__(self):
        return ""<GetAdminLog Node>""

    def render(self, context):
        if self.user is None:
            entries = LogEntry.objects.all()
        else:
            user_id = self.user
            if not user_id.isdigit():
                user_id = context[self.user].pk
            entries = LogEntry.objects.filter(user__pk=user_id)
        context[self.varname] = entries.select_related('content_type', 'user')[:int(self.limit)]
        return ''


@register.tag
def get_admin_log(parser, token):
    """"""
    Populate a template variable with the admin log for the given criteria.

    Usage::

        {% get_admin_log [limit] as [varname] for_user [context_var_containing_user_obj] %}

    Examples::

        {% get_admin_log 10 as admin_log for_user 23 %}
        {% get_admin_log 10 as admin_log for_user user %}
        {% get_admin_log 10 as admin_log %}

    Note that ``context_var_containing_user_obj`` can be a hard-coded integer
    (user ID) or the name of a template context variable containing the user
    object whose ID you want.
    """"""
    tokens = token.contents.split()
    if len(tokens) < 4:
        raise template.TemplateSyntaxError(
            ""'get_admin_log' statements require two arguments"")
    if not tokens[1].isdigit():
        raise template.TemplateSyntaxError(
            ""First argument to 'get_admin_log' must be an integer"")
    if tokens[2] != 'as':
        raise template.TemplateSyntaxError(
            ""Second argument to 'get_admin_log' must be 'as'"")
    if len(tokens) > 4:
        if tokens[4] != 'for_user':
            raise template.TemplateSyntaxError(
                ""Fourth argument to 'get_admin_log' must be 'for_user'"")
    return AdminLogNode(limit=tokens[1], varname=tokens[3], user=(tokens[5] if len(tokens) > 5 else None))"
