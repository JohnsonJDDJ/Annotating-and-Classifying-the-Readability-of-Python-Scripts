301	jackson	4	# This file is distributed under the same license as the Django package.<<NEWL>>#<<NEWL>># The *_FORMAT strings use the Django date format syntax,<<NEWL>># see https://docs.djangoproject.com/en/dev/ref/templates/builtins/#date<<NEWL>>DATE_FORMAT = r'Y. \g\a\d\a j. F'<<NEWL>>TIME_FORMAT = 'H:i'<<NEWL>>DATETIME_FORMAT = r'Y. \g\a\d\a j. F, H:i'<<NEWL>>YEAR_MONTH_FORMAT = r'Y. \g. F'<<NEWL>>MONTH_DAY_FORMAT = 'j. F'<<NEWL>>SHORT_DATE_FORMAT = r'j.m.Y'<<NEWL>>SHORT_DATETIME_FORMAT = 'j.m.Y H:i'<<NEWL>>FIRST_DAY_OF_WEEK = 1  # Monday<<NEWL>><<NEWL>># The *_INPUT_FORMATS strings use the Python strftime format syntax,<<NEWL>># see https://docs.python.org/library/datetime.html#strftime-strptime-behavior<<NEWL>># Kept ISO formats as they are in first position<<NEWL>>DATE_INPUT_FORMATS = [<<NEWL>>    '%Y-%m-%d', '%d.%m.%Y', '%d.%m.%y',  # '2006-10-25', '25.10.2006', '25.10.06'<<NEWL>>]<<NEWL>>TIME_INPUT_FORMATS = [<<NEWL>>    '%H:%M:%S',     # '14:30:59'<<NEWL>>    '%H:%M:%S.%f',  # '14:30:59.000200'<<NEWL>>    '%H:%M',        # '14:30'<<NEWL>>    '%H.%M.%S',     # '14.30.59'<<NEWL>>    '%H.%M.%S.%f',  # '14.30.59.000200'<<NEWL>>    '%H.%M',        # '14.30'<<NEWL>>]<<NEWL>>DATETIME_INPUT_FORMATS = [<<NEWL>>    '%Y-%m-%d %H:%M:%S',     # '2006-10-25 14:30:59'<<NEWL>>    '%Y-%m-%d %H:%M:%S.%f',  # '2006-10-25 14:30:59.000200'<<NEWL>>    '%Y-%m-%d %H:%M',        # '2006-10-25 14:30'<<NEWL>>    '%d.%m.%Y %H:%M:%S',     # '25.10.2006 14:30:59'<<NEWL>>    '%d.%m.%Y %H:%M:%S.%f',  # '25.10.2006 14:30:59.000200'<<NEWL>>    '%d.%m.%Y %H:%M',        # '25.10.2006 14:30'<<NEWL>>    '%d.%m.%y %H:%M:%S',     # '25.10.06 14:30:59'<<NEWL>>    '%d.%m.%y %H:%M:%S.%f',  # '25.10.06 14:30:59.000200'<<NEWL>>    '%d.%m.%y %H:%M',        # '25.10.06 14:30'<<NEWL>>    '%d.%m.%y %H.%M.%S',     # '25.10.06 14.30.59'<<NEWL>>    '%d.%m.%y %H.%M.%S.%f',  # '25.10.06 14.30.59.000200'<<NEWL>>    '%d.%m.%y %H.%M',        # '25.10.06 14.30'<<NEWL>>]<<NEWL>>DECIMAL_SEPARATOR = ','<<NEWL>>THOUSAND_SEPARATOR = ' '  # Non-breaking space<<NEWL>>NUMBER_GROUPING = 3
301	donghui	1	# This file is distributed under the same license as the Django package.<<NEWL>>#<<NEWL>># The *_FORMAT strings use the Django date format syntax,<<NEWL>># see https://docs.djangoproject.com/en/dev/ref/templates/builtins/#date<<NEWL>>DATE_FORMAT = r'Y. \g\a\d\a j. F'<<NEWL>>TIME_FORMAT = 'H:i'<<NEWL>>DATETIME_FORMAT = r'Y. \g\a\d\a j. F, H:i'<<NEWL>>YEAR_MONTH_FORMAT = r'Y. \g. F'<<NEWL>>MONTH_DAY_FORMAT = 'j. F'<<NEWL>>SHORT_DATE_FORMAT = r'j.m.Y'<<NEWL>>SHORT_DATETIME_FORMAT = 'j.m.Y H:i'<<NEWL>>FIRST_DAY_OF_WEEK = 1  # Monday<<NEWL>><<NEWL>># The *_INPUT_FORMATS strings use the Python strftime format syntax,<<NEWL>># see https://docs.python.org/library/datetime.html#strftime-strptime-behavior<<NEWL>># Kept ISO formats as they are in first position<<NEWL>>DATE_INPUT_FORMATS = [<<NEWL>>    '%Y-%m-%d', '%d.%m.%Y', '%d.%m.%y',  # '2006-10-25', '25.10.2006', '25.10.06'<<NEWL>>]<<NEWL>>TIME_INPUT_FORMATS = [<<NEWL>>    '%H:%M:%S',     # '14:30:59'<<NEWL>>    '%H:%M:%S.%f',  # '14:30:59.000200'<<NEWL>>    '%H:%M',        # '14:30'<<NEWL>>    '%H.%M.%S',     # '14.30.59'<<NEWL>>    '%H.%M.%S.%f',  # '14.30.59.000200'<<NEWL>>    '%H.%M',        # '14.30'<<NEWL>>]<<NEWL>>DATETIME_INPUT_FORMATS = [<<NEWL>>    '%Y-%m-%d %H:%M:%S',     # '2006-10-25 14:30:59'<<NEWL>>    '%Y-%m-%d %H:%M:%S.%f',  # '2006-10-25 14:30:59.000200'<<NEWL>>    '%Y-%m-%d %H:%M',        # '2006-10-25 14:30'<<NEWL>>    '%d.%m.%Y %H:%M:%S',     # '25.10.2006 14:30:59'<<NEWL>>    '%d.%m.%Y %H:%M:%S.%f',  # '25.10.2006 14:30:59.000200'<<NEWL>>    '%d.%m.%Y %H:%M',        # '25.10.2006 14:30'<<NEWL>>    '%d.%m.%y %H:%M:%S',     # '25.10.06 14:30:59'<<NEWL>>    '%d.%m.%y %H:%M:%S.%f',  # '25.10.06 14:30:59.000200'<<NEWL>>    '%d.%m.%y %H:%M',        # '25.10.06 14:30'<<NEWL>>    '%d.%m.%y %H.%M.%S',     # '25.10.06 14.30.59'<<NEWL>>    '%d.%m.%y %H.%M.%S.%f',  # '25.10.06 14.30.59.000200'<<NEWL>>    '%d.%m.%y %H.%M',        # '25.10.06 14.30'<<NEWL>>]<<NEWL>>DECIMAL_SEPARATOR = ','<<NEWL>>THOUSAND_SEPARATOR = ' '  # Non-breaking space<<NEWL>>NUMBER_GROUPING = 3
350	jackson	1	import os<<NEWL>>import signal<<NEWL>>import subprocess<<NEWL>><<NEWL>>from django.db.backends.base.client import BaseDatabaseClient<<NEWL>><<NEWL>><<NEWL>>class DatabaseClient(BaseDatabaseClient):<<NEWL>>    executable_name = 'psql'<<NEWL>><<NEWL>>    @classmethod<<NEWL>>    def runshell_db(cls, conn_params, parameters):<<NEWL>>        args = [cls.executable_name]<<NEWL>><<NEWL>>        host = conn_params.get('host', '')<<NEWL>>        port = conn_params.get('port', '')<<NEWL>>        dbname = conn_params.get('database', '')<<NEWL>>        user = conn_params.get('user', '')<<NEWL>>        passwd = conn_params.get('password', '')<<NEWL>>        sslmode = conn_params.get('sslmode', '')<<NEWL>>        sslrootcert = conn_params.get('sslrootcert', '')<<NEWL>>        sslcert = conn_params.get('sslcert', '')<<NEWL>>        sslkey = conn_params.get('sslkey', '')<<NEWL>><<NEWL>>        if user:<<NEWL>>            args += ['-U', user]<<NEWL>>        if host:<<NEWL>>            args += ['-h', host]<<NEWL>>        if port:<<NEWL>>            args += ['-p', str(port)]<<NEWL>>        args += [dbname]<<NEWL>>        args.extend(parameters)<<NEWL>><<NEWL>>        sigint_handler = signal.getsignal(signal.SIGINT)<<NEWL>>        subprocess_env = os.environ.copy()<<NEWL>>        if passwd:<<NEWL>>            subprocess_env['PGPASSWORD'] = str(passwd)<<NEWL>>        if sslmode:<<NEWL>>            subprocess_env['PGSSLMODE'] = str(sslmode)<<NEWL>>        if sslrootcert:<<NEWL>>            subprocess_env['PGSSLROOTCERT'] = str(sslrootcert)<<NEWL>>        if sslcert:<<NEWL>>            subprocess_env['PGSSLCERT'] = str(sslcert)<<NEWL>>        if sslkey:<<NEWL>>            subprocess_env['PGSSLKEY'] = str(sslkey)<<NEWL>>        try:<<NEWL>>            # Allow SIGINT to pass to psql to abort queries.<<NEWL>>            signal.signal(signal.SIGINT, signal.SIG_IGN)<<NEWL>>            subprocess.run(args, check=True, env=subprocess_env)<<NEWL>>        finally:<<NEWL>>            # Restore the original SIGINT handler.<<NEWL>>            signal.signal(signal.SIGINT, sigint_handler)<<NEWL>><<NEWL>>    def runshell(self, parameters):<<NEWL>>        self.runshell_db(self.connection.get_connection_params(), parameters)
350	donghui	1	import os<<NEWL>>import signal<<NEWL>>import subprocess<<NEWL>><<NEWL>>from django.db.backends.base.client import BaseDatabaseClient<<NEWL>><<NEWL>><<NEWL>>class DatabaseClient(BaseDatabaseClient):<<NEWL>>    executable_name = 'psql'<<NEWL>><<NEWL>>    @classmethod<<NEWL>>    def runshell_db(cls, conn_params, parameters):<<NEWL>>        args = [cls.executable_name]<<NEWL>><<NEWL>>        host = conn_params.get('host', '')<<NEWL>>        port = conn_params.get('port', '')<<NEWL>>        dbname = conn_params.get('database', '')<<NEWL>>        user = conn_params.get('user', '')<<NEWL>>        passwd = conn_params.get('password', '')<<NEWL>>        sslmode = conn_params.get('sslmode', '')<<NEWL>>        sslrootcert = conn_params.get('sslrootcert', '')<<NEWL>>        sslcert = conn_params.get('sslcert', '')<<NEWL>>        sslkey = conn_params.get('sslkey', '')<<NEWL>><<NEWL>>        if user:<<NEWL>>            args += ['-U', user]<<NEWL>>        if host:<<NEWL>>            args += ['-h', host]<<NEWL>>        if port:<<NEWL>>            args += ['-p', str(port)]<<NEWL>>        args += [dbname]<<NEWL>>        args.extend(parameters)<<NEWL>><<NEWL>>        sigint_handler = signal.getsignal(signal.SIGINT)<<NEWL>>        subprocess_env = os.environ.copy()<<NEWL>>        if passwd:<<NEWL>>            subprocess_env['PGPASSWORD'] = str(passwd)<<NEWL>>        if sslmode:<<NEWL>>            subprocess_env['PGSSLMODE'] = str(sslmode)<<NEWL>>        if sslrootcert:<<NEWL>>            subprocess_env['PGSSLROOTCERT'] = str(sslrootcert)<<NEWL>>        if sslcert:<<NEWL>>            subprocess_env['PGSSLCERT'] = str(sslcert)<<NEWL>>        if sslkey:<<NEWL>>            subprocess_env['PGSSLKEY'] = str(sslkey)<<NEWL>>        try:<<NEWL>>            # Allow SIGINT to pass to psql to abort queries.<<NEWL>>            signal.signal(signal.SIGINT, signal.SIG_IGN)<<NEWL>>            subprocess.run(args, check=True, env=subprocess_env)<<NEWL>>        finally:<<NEWL>>            # Restore the original SIGINT handler.<<NEWL>>            signal.signal(signal.SIGINT, sigint_handler)<<NEWL>><<NEWL>>    def runshell(self, parameters):<<NEWL>>        self.runshell_db(self.connection.get_connection_params(), parameters)
364	jackson	1	import numpy as np<<NEWL>>import numba as nb<<NEWL>><<NEWL>>from numpy.random import PCG64<<NEWL>>from timeit import timeit<<NEWL>><<NEWL>>bit_gen = PCG64()<<NEWL>>next_d = bit_gen.cffi.next_double<<NEWL>>state_addr = bit_gen.cffi.state_address<<NEWL>><<NEWL>>def normals(n, state):<<NEWL>>    out = np.empty(n)<<NEWL>>    for i in range((n + 1) // 2):<<NEWL>>        x1 = 2.0 * next_d(state) - 1.0<<NEWL>>        x2 = 2.0 * next_d(state) - 1.0<<NEWL>>        r2 = x1 * x1 + x2 * x2<<NEWL>>        while r2 >= 1.0 or r2 == 0.0:<<NEWL>>            x1 = 2.0 * next_d(state) - 1.0<<NEWL>>            x2 = 2.0 * next_d(state) - 1.0<<NEWL>>            r2 = x1 * x1 + x2 * x2<<NEWL>>        f = np.sqrt(-2.0 * np.log(r2) / r2)<<NEWL>>        out[2 * i] = f * x1<<NEWL>>        if 2 * i + 1 < n:<<NEWL>>            out[2 * i + 1] = f * x2<<NEWL>>    return out<<NEWL>><<NEWL>># Compile using Numba<<NEWL>>normalsj = nb.jit(normals, nopython=True)<<NEWL>># Must use state address not state with numba<<NEWL>>n = 10000<<NEWL>><<NEWL>>def numbacall():<<NEWL>>    return normalsj(n, state_addr)<<NEWL>><<NEWL>>rg = np.random.Generator(PCG64())<<NEWL>><<NEWL>>def numpycall():<<NEWL>>    return rg.normal(size=n)<<NEWL>><<NEWL>># Check that the functions work<<NEWL>>r1 = numbacall()<<NEWL>>r2 = numpycall()<<NEWL>>assert r1.shape == (n,)<<NEWL>>assert r1.shape == r2.shape<<NEWL>><<NEWL>>t1 = timeit(numbacall, number=1000)<<NEWL>>print(f'{t1:.2f} secs for {n} PCG64 (Numba/PCG64) gaussian randoms')<<NEWL>>t2 = timeit(numpycall, number=1000)<<NEWL>>print(f'{t2:.2f} secs for {n} PCG64 (NumPy/PCG64) gaussian randoms')<<NEWL>><<NEWL>># example 2<<NEWL>><<NEWL>>next_u32 = bit_gen.ctypes.next_uint32<<NEWL>>ctypes_state = bit_gen.ctypes.state<<NEWL>><<NEWL>>@nb.jit(nopython=True)<<NEWL>>def bounded_uint(lb, ub, state):<<NEWL>>    mask = delta = ub - lb<<NEWL>>    mask |= mask >> 1<<NEWL>>    mask |= mask >> 2<<NEWL>>    mask |= mask >> 4<<NEWL>>    mask |= mask >> 8<<NEWL>>    mask |= mask >> 16<<NEWL>><<NEWL>>    val = next_u32(state) & mask<<NEWL>>    while val > delta:<<NEWL>>        val = next_u32(state) & mask<<NEWL>><<NEWL>>    return lb + val<<NEWL>><<NEWL>><<NEWL>>print(bounded_uint(323, 2394691, ctypes_state.value))<<NEWL>><<NEWL>><<NEWL>>@nb.jit(nopython=True)<<NEWL>>def bounded_uints(lb, ub, n, state):<<NEWL>>    out = np.empty(n, dtype=np.uint32)<<NEWL>>    for i in range(n):<<NEWL>>        out[i] = bounded_uint(lb, ub, state)<<NEWL>><<NEWL>><<NEWL>>bounded_uints(323, 2394691, 10000000, ctypes_state.value)<<NEWL>><<NEWL>>
364	donghui	1	import numpy as np<<NEWL>>import numba as nb<<NEWL>><<NEWL>>from numpy.random import PCG64<<NEWL>>from timeit import timeit<<NEWL>><<NEWL>>bit_gen = PCG64()<<NEWL>>next_d = bit_gen.cffi.next_double<<NEWL>>state_addr = bit_gen.cffi.state_address<<NEWL>><<NEWL>>def normals(n, state):<<NEWL>>    out = np.empty(n)<<NEWL>>    for i in range((n + 1) // 2):<<NEWL>>        x1 = 2.0 * next_d(state) - 1.0<<NEWL>>        x2 = 2.0 * next_d(state) - 1.0<<NEWL>>        r2 = x1 * x1 + x2 * x2<<NEWL>>        while r2 >= 1.0 or r2 == 0.0:<<NEWL>>            x1 = 2.0 * next_d(state) - 1.0<<NEWL>>            x2 = 2.0 * next_d(state) - 1.0<<NEWL>>            r2 = x1 * x1 + x2 * x2<<NEWL>>        f = np.sqrt(-2.0 * np.log(r2) / r2)<<NEWL>>        out[2 * i] = f * x1<<NEWL>>        if 2 * i + 1 < n:<<NEWL>>            out[2 * i + 1] = f * x2<<NEWL>>    return out<<NEWL>><<NEWL>># Compile using Numba<<NEWL>>normalsj = nb.jit(normals, nopython=True)<<NEWL>># Must use state address not state with numba<<NEWL>>n = 10000<<NEWL>><<NEWL>>def numbacall():<<NEWL>>    return normalsj(n, state_addr)<<NEWL>><<NEWL>>rg = np.random.Generator(PCG64())<<NEWL>><<NEWL>>def numpycall():<<NEWL>>    return rg.normal(size=n)<<NEWL>><<NEWL>># Check that the functions work<<NEWL>>r1 = numbacall()<<NEWL>>r2 = numpycall()<<NEWL>>assert r1.shape == (n,)<<NEWL>>assert r1.shape == r2.shape<<NEWL>><<NEWL>>t1 = timeit(numbacall, number=1000)<<NEWL>>print(f'{t1:.2f} secs for {n} PCG64 (Numba/PCG64) gaussian randoms')<<NEWL>>t2 = timeit(numpycall, number=1000)<<NEWL>>print(f'{t2:.2f} secs for {n} PCG64 (NumPy/PCG64) gaussian randoms')<<NEWL>><<NEWL>># example 2<<NEWL>><<NEWL>>next_u32 = bit_gen.ctypes.next_uint32<<NEWL>>ctypes_state = bit_gen.ctypes.state<<NEWL>><<NEWL>>@nb.jit(nopython=True)<<NEWL>>def bounded_uint(lb, ub, state):<<NEWL>>    mask = delta = ub - lb<<NEWL>>    mask |= mask >> 1<<NEWL>>    mask |= mask >> 2<<NEWL>>    mask |= mask >> 4<<NEWL>>    mask |= mask >> 8<<NEWL>>    mask |= mask >> 16<<NEWL>><<NEWL>>    val = next_u32(state) & mask<<NEWL>>    while val > delta:<<NEWL>>        val = next_u32(state) & mask<<NEWL>><<NEWL>>    return lb + val<<NEWL>><<NEWL>><<NEWL>>print(bounded_uint(323, 2394691, ctypes_state.value))<<NEWL>><<NEWL>><<NEWL>>@nb.jit(nopython=True)<<NEWL>>def bounded_uints(lb, ub, n, state):<<NEWL>>    out = np.empty(n, dtype=np.uint32)<<NEWL>>    for i in range(n):<<NEWL>>        out[i] = bounded_uint(lb, ub, state)<<NEWL>><<NEWL>><<NEWL>>bounded_uints(323, 2394691, 10000000, ctypes_state.value)<<NEWL>><<NEWL>>
335	jackson	3	"import functools<<NEWL>>import operator<<NEWL>>import itertools<<NEWL>><<NEWL>>from .extern.jaraco.text import yield_lines<<NEWL>>from .extern.jaraco.functools import pass_none<<NEWL>>from ._importlib import metadata<<NEWL>>from ._itertools import ensure_unique<<NEWL>>from .extern.more_itertools import consume<<NEWL>><<NEWL>><<NEWL>>def ensure_valid(ep):<<NEWL>>    """"""<<NEWL>>    Exercise one of the dynamic properties to trigger<<NEWL>>    the pattern match.<<NEWL>>    """"""<<NEWL>>    ep.extras<<NEWL>><<NEWL>><<NEWL>>def load_group(value, group):<<NEWL>>    """"""<<NEWL>>    Given a value of an entry point or series of entry points,<<NEWL>>    return each as an EntryPoint.<<NEWL>>    """"""<<NEWL>>    # normalize to a single sequence of lines<<NEWL>>    lines = yield_lines(value)<<NEWL>>    text = f'[{group}]\n' + '\n'.join(lines)<<NEWL>>    return metadata.EntryPoints._from_text(text)<<NEWL>><<NEWL>><<NEWL>>def by_group_and_name(ep):<<NEWL>>    return ep.group, ep.name<<NEWL>><<NEWL>><<NEWL>>def validate(eps: metadata.EntryPoints):<<NEWL>>    """"""<<NEWL>>    Ensure entry points are unique by group and name and validate each.<<NEWL>>    """"""<<NEWL>>    consume(map(ensure_valid, ensure_unique(eps, key=by_group_and_name)))<<NEWL>>    return eps<<NEWL>><<NEWL>><<NEWL>>@functools.singledispatch<<NEWL>>def load(eps):<<NEWL>>    """"""<<NEWL>>    Given a Distribution.entry_points, produce EntryPoints.<<NEWL>>    """"""<<NEWL>>    groups = itertools.chain.from_iterable(<<NEWL>>        load_group(value, group)<<NEWL>>        for group, value in eps.items())<<NEWL>>    return validate(metadata.EntryPoints(groups))<<NEWL>><<NEWL>><<NEWL>>@load.register(str)<<NEWL>>def _(eps):<<NEWL>>    r""""""<<NEWL>>    >>> ep, = load('[console_scripts]\nfoo=bar')<<NEWL>>    >>> ep.group<<NEWL>>    'console_scripts'<<NEWL>>    >>> ep.name<<NEWL>>    'foo'<<NEWL>>    >>> ep.value<<NEWL>>    'bar'<<NEWL>>    """"""<<NEWL>>    return validate(metadata.EntryPoints(metadata.EntryPoints._from_text(eps)))<<NEWL>><<NEWL>><<NEWL>>load.register(type(None), lambda x: x)<<NEWL>><<NEWL>><<NEWL>>@pass_none<<NEWL>>def render(eps: metadata.EntryPoints):<<NEWL>>    by_group = operator.attrgetter('group')<<NEWL>>    groups = itertools.groupby(sorted(eps, key=by_group), by_group)<<NEWL>><<NEWL>>    return '\n'.join(<<NEWL>>        f'[{group}]\n{render_items(items)}\n'<<NEWL>>        for group, items in groups<<NEWL>>    )<<NEWL>><<NEWL>><<NEWL>>def render_items(eps):<<NEWL>>    return '\n'.join(<<NEWL>>        f'{ep.name} = {ep.value}'<<NEWL>>        for ep in sorted(eps)<<NEWL>>    )"
335	donghui	3	"import functools<<NEWL>>import operator<<NEWL>>import itertools<<NEWL>><<NEWL>>from .extern.jaraco.text import yield_lines<<NEWL>>from .extern.jaraco.functools import pass_none<<NEWL>>from ._importlib import metadata<<NEWL>>from ._itertools import ensure_unique<<NEWL>>from .extern.more_itertools import consume<<NEWL>><<NEWL>><<NEWL>>def ensure_valid(ep):<<NEWL>>    """"""<<NEWL>>    Exercise one of the dynamic properties to trigger<<NEWL>>    the pattern match.<<NEWL>>    """"""<<NEWL>>    ep.extras<<NEWL>><<NEWL>><<NEWL>>def load_group(value, group):<<NEWL>>    """"""<<NEWL>>    Given a value of an entry point or series of entry points,<<NEWL>>    return each as an EntryPoint.<<NEWL>>    """"""<<NEWL>>    # normalize to a single sequence of lines<<NEWL>>    lines = yield_lines(value)<<NEWL>>    text = f'[{group}]\n' + '\n'.join(lines)<<NEWL>>    return metadata.EntryPoints._from_text(text)<<NEWL>><<NEWL>><<NEWL>>def by_group_and_name(ep):<<NEWL>>    return ep.group, ep.name<<NEWL>><<NEWL>><<NEWL>>def validate(eps: metadata.EntryPoints):<<NEWL>>    """"""<<NEWL>>    Ensure entry points are unique by group and name and validate each.<<NEWL>>    """"""<<NEWL>>    consume(map(ensure_valid, ensure_unique(eps, key=by_group_and_name)))<<NEWL>>    return eps<<NEWL>><<NEWL>><<NEWL>>@functools.singledispatch<<NEWL>>def load(eps):<<NEWL>>    """"""<<NEWL>>    Given a Distribution.entry_points, produce EntryPoints.<<NEWL>>    """"""<<NEWL>>    groups = itertools.chain.from_iterable(<<NEWL>>        load_group(value, group)<<NEWL>>        for group, value in eps.items())<<NEWL>>    return validate(metadata.EntryPoints(groups))<<NEWL>><<NEWL>><<NEWL>>@load.register(str)<<NEWL>>def _(eps):<<NEWL>>    r""""""<<NEWL>>    >>> ep, = load('[console_scripts]\nfoo=bar')<<NEWL>>    >>> ep.group<<NEWL>>    'console_scripts'<<NEWL>>    >>> ep.name<<NEWL>>    'foo'<<NEWL>>    >>> ep.value<<NEWL>>    'bar'<<NEWL>>    """"""<<NEWL>>    return validate(metadata.EntryPoints(metadata.EntryPoints._from_text(eps)))<<NEWL>><<NEWL>><<NEWL>>load.register(type(None), lambda x: x)<<NEWL>><<NEWL>><<NEWL>>@pass_none<<NEWL>>def render(eps: metadata.EntryPoints):<<NEWL>>    by_group = operator.attrgetter('group')<<NEWL>>    groups = itertools.groupby(sorted(eps, key=by_group), by_group)<<NEWL>><<NEWL>>    return '\n'.join(<<NEWL>>        f'[{group}]\n{render_items(items)}\n'<<NEWL>>        for group, items in groups<<NEWL>>    )<<NEWL>><<NEWL>><<NEWL>>def render_items(eps):<<NEWL>>    return '\n'.join(<<NEWL>>        f'{ep.name} = {ep.value}'<<NEWL>>        for ep in sorted(eps)<<NEWL>>    )"
275	jackson	1	"import pytest<<NEWL>>from traitlets import HasTraits, TraitError<<NEWL>>from traitlets.utils.importstring import import_item<<NEWL>><<NEWL>>from notebook.traittypes import (<<NEWL>>    InstanceFromClasses,<<NEWL>>    TypeFromClasses<<NEWL>>)<<NEWL>>from notebook.services.contents.largefilemanager import LargeFileManager<<NEWL>><<NEWL>><<NEWL>>class DummyClass:<<NEWL>>    """"""Dummy class for testing Instance""""""<<NEWL>><<NEWL>><<NEWL>>class DummyInt(int):<<NEWL>>    """"""Dummy class for testing types.""""""<<NEWL>><<NEWL>><<NEWL>>class Thing(HasTraits):<<NEWL>><<NEWL>>    a = InstanceFromClasses(<<NEWL>>        default_value=2,<<NEWL>>        klasses=[<<NEWL>>            int,<<NEWL>>            str,<<NEWL>>            DummyClass,<<NEWL>>        ]<<NEWL>>    )<<NEWL>><<NEWL>>    b = TypeFromClasses(<<NEWL>>        default_value=None,<<NEWL>>        allow_none=True,<<NEWL>>        klasses=[<<NEWL>>            DummyClass,<<NEWL>>            int,<<NEWL>>            'notebook.services.contents.manager.ContentsManager'<<NEWL>>        ]<<NEWL>>    )<<NEWL>><<NEWL>><<NEWL>>class TestInstanceFromClasses:<<NEWL>><<NEWL>>    @pytest.mark.parametrize(<<NEWL>>        'value',<<NEWL>>        [1, 'test', DummyClass()]<<NEWL>>    )<<NEWL>>    def test_good_values(self, value):<<NEWL>>        thing = Thing(a=value)<<NEWL>>        assert thing.a == value<<NEWL>><<NEWL>>    @pytest.mark.parametrize(<<NEWL>>        'value',<<NEWL>>        [2.4, object()]<<NEWL>>    )<<NEWL>>    def test_bad_values(self, value):<<NEWL>>        with pytest.raises(TraitError) as e:<<NEWL>>            thing = Thing(a=value)<<NEWL>><<NEWL>><<NEWL>>class TestTypeFromClasses:<<NEWL>><<NEWL>>    @pytest.mark.parametrize(<<NEWL>>        'value',<<NEWL>>        [DummyClass, DummyInt, LargeFileManager,<<NEWL>>            'notebook.services.contents.manager.ContentsManager']<<NEWL>>    )<<NEWL>>    def test_good_values(self, value):<<NEWL>>        thing = Thing(b=value)<<NEWL>>        if isinstance(value, str):<<NEWL>>            value = import_item(value)<<NEWL>>        assert thing.b == value<<NEWL>><<NEWL>>    @pytest.mark.parametrize(<<NEWL>>        'value',<<NEWL>>        [float, object]<<NEWL>>    )<<NEWL>>    def test_bad_values(self, value):<<NEWL>>        with pytest.raises(TraitError) as e:<<NEWL>>            thing = Thing(b=value)"
275	donghui	1	"import pytest<<NEWL>>from traitlets import HasTraits, TraitError<<NEWL>>from traitlets.utils.importstring import import_item<<NEWL>><<NEWL>>from notebook.traittypes import (<<NEWL>>    InstanceFromClasses,<<NEWL>>    TypeFromClasses<<NEWL>>)<<NEWL>>from notebook.services.contents.largefilemanager import LargeFileManager<<NEWL>><<NEWL>><<NEWL>>class DummyClass:<<NEWL>>    """"""Dummy class for testing Instance""""""<<NEWL>><<NEWL>><<NEWL>>class DummyInt(int):<<NEWL>>    """"""Dummy class for testing types.""""""<<NEWL>><<NEWL>><<NEWL>>class Thing(HasTraits):<<NEWL>><<NEWL>>    a = InstanceFromClasses(<<NEWL>>        default_value=2,<<NEWL>>        klasses=[<<NEWL>>            int,<<NEWL>>            str,<<NEWL>>            DummyClass,<<NEWL>>        ]<<NEWL>>    )<<NEWL>><<NEWL>>    b = TypeFromClasses(<<NEWL>>        default_value=None,<<NEWL>>        allow_none=True,<<NEWL>>        klasses=[<<NEWL>>            DummyClass,<<NEWL>>            int,<<NEWL>>            'notebook.services.contents.manager.ContentsManager'<<NEWL>>        ]<<NEWL>>    )<<NEWL>><<NEWL>><<NEWL>>class TestInstanceFromClasses:<<NEWL>><<NEWL>>    @pytest.mark.parametrize(<<NEWL>>        'value',<<NEWL>>        [1, 'test', DummyClass()]<<NEWL>>    )<<NEWL>>    def test_good_values(self, value):<<NEWL>>        thing = Thing(a=value)<<NEWL>>        assert thing.a == value<<NEWL>><<NEWL>>    @pytest.mark.parametrize(<<NEWL>>        'value',<<NEWL>>        [2.4, object()]<<NEWL>>    )<<NEWL>>    def test_bad_values(self, value):<<NEWL>>        with pytest.raises(TraitError) as e:<<NEWL>>            thing = Thing(a=value)<<NEWL>><<NEWL>><<NEWL>>class TestTypeFromClasses:<<NEWL>><<NEWL>>    @pytest.mark.parametrize(<<NEWL>>        'value',<<NEWL>>        [DummyClass, DummyInt, LargeFileManager,<<NEWL>>            'notebook.services.contents.manager.ContentsManager']<<NEWL>>    )<<NEWL>>    def test_good_values(self, value):<<NEWL>>        thing = Thing(b=value)<<NEWL>>        if isinstance(value, str):<<NEWL>>            value = import_item(value)<<NEWL>>        assert thing.b == value<<NEWL>><<NEWL>>    @pytest.mark.parametrize(<<NEWL>>        'value',<<NEWL>>        [float, object]<<NEWL>>    )<<NEWL>>    def test_bad_values(self, value):<<NEWL>>        with pytest.raises(TraitError) as e:<<NEWL>>            thing = Thing(b=value)"
286	jackson	4	"""""""<<NEWL>>Behavioral based tests for offsets and date_range.<<NEWL>><<NEWL>>This file is adapted from https://github.com/pandas-dev/pandas/pull/18761 -<<NEWL>>which was more ambitious but less idiomatic in its use of Hypothesis.<<NEWL>><<NEWL>>You may wish to consult the previous version for inspiration on further<<NEWL>>tests, or when trying to pin down the bugs exposed by the tests below.<<NEWL>>""""""<<NEWL>>from hypothesis import (<<NEWL>>    assume,<<NEWL>>    given,<<NEWL>>)<<NEWL>>import pytest<<NEWL>>import pytz<<NEWL>><<NEWL>>import pandas as pd<<NEWL>>from pandas._testing._hypothesis import (<<NEWL>>    DATETIME_JAN_1_1900_OPTIONAL_TZ,<<NEWL>>    YQM_OFFSET,<<NEWL>>)<<NEWL>><<NEWL>># ----------------------------------------------------------------<<NEWL>># Offset-specific behaviour tests<<NEWL>><<NEWL>><<NEWL>>@pytest.mark.arm_slow<<NEWL>>@given(DATETIME_JAN_1_1900_OPTIONAL_TZ, YQM_OFFSET)<<NEWL>>def test_on_offset_implementations(dt, offset):<<NEWL>>    assume(not offset.normalize)<<NEWL>>    # check that the class-specific implementations of is_on_offset match<<NEWL>>    # the general case definition:<<NEWL>>    #   (dt + offset) - offset == dt<<NEWL>>    try:<<NEWL>>        compare = (dt + offset) - offset<<NEWL>>    except (pytz.NonExistentTimeError, pytz.AmbiguousTimeError):<<NEWL>>        # When dt + offset does not exist or is DST-ambiguous, assume(False) to<<NEWL>>        # indicate to hypothesis that this is not a valid test case<<NEWL>>        # DST-ambiguous example (GH41906):<<NEWL>>        # dt = datetime.datetime(1900, 1, 1, tzinfo=pytz.timezone('Africa/Kinshasa'))<<NEWL>>        # offset = MonthBegin(66)<<NEWL>>        assume(False)<<NEWL>><<NEWL>>    assert offset.is_on_offset(dt) == (compare == dt)<<NEWL>><<NEWL>><<NEWL>>@given(YQM_OFFSET)<<NEWL>>def test_shift_across_dst(offset):<<NEWL>>    # GH#18319 check that 1) timezone is correctly normalized and<<NEWL>>    # 2) that hour is not incorrectly changed by this normalization<<NEWL>>    assume(not offset.normalize)<<NEWL>><<NEWL>>    # Note that dti includes a transition across DST boundary<<NEWL>>    dti = pd.date_range(<<NEWL>>        start=""2017-10-30 12:00:00"", end=""2017-11-06"", freq=""D"", tz=""US/Eastern""<<NEWL>>    )<<NEWL>>    assert (dti.hour == 12).all()  # we haven't screwed up yet<<NEWL>><<NEWL>>    res = dti + offset<<NEWL>>    assert (res.hour == 12).all()"
286	donghui	4	"""""""<<NEWL>>Behavioral based tests for offsets and date_range.<<NEWL>><<NEWL>>This file is adapted from https://github.com/pandas-dev/pandas/pull/18761 -<<NEWL>>which was more ambitious but less idiomatic in its use of Hypothesis.<<NEWL>><<NEWL>>You may wish to consult the previous version for inspiration on further<<NEWL>>tests, or when trying to pin down the bugs exposed by the tests below.<<NEWL>>""""""<<NEWL>>from hypothesis import (<<NEWL>>    assume,<<NEWL>>    given,<<NEWL>>)<<NEWL>>import pytest<<NEWL>>import pytz<<NEWL>><<NEWL>>import pandas as pd<<NEWL>>from pandas._testing._hypothesis import (<<NEWL>>    DATETIME_JAN_1_1900_OPTIONAL_TZ,<<NEWL>>    YQM_OFFSET,<<NEWL>>)<<NEWL>><<NEWL>># ----------------------------------------------------------------<<NEWL>># Offset-specific behaviour tests<<NEWL>><<NEWL>><<NEWL>>@pytest.mark.arm_slow<<NEWL>>@given(DATETIME_JAN_1_1900_OPTIONAL_TZ, YQM_OFFSET)<<NEWL>>def test_on_offset_implementations(dt, offset):<<NEWL>>    assume(not offset.normalize)<<NEWL>>    # check that the class-specific implementations of is_on_offset match<<NEWL>>    # the general case definition:<<NEWL>>    #   (dt + offset) - offset == dt<<NEWL>>    try:<<NEWL>>        compare = (dt + offset) - offset<<NEWL>>    except (pytz.NonExistentTimeError, pytz.AmbiguousTimeError):<<NEWL>>        # When dt + offset does not exist or is DST-ambiguous, assume(False) to<<NEWL>>        # indicate to hypothesis that this is not a valid test case<<NEWL>>        # DST-ambiguous example (GH41906):<<NEWL>>        # dt = datetime.datetime(1900, 1, 1, tzinfo=pytz.timezone('Africa/Kinshasa'))<<NEWL>>        # offset = MonthBegin(66)<<NEWL>>        assume(False)<<NEWL>><<NEWL>>    assert offset.is_on_offset(dt) == (compare == dt)<<NEWL>><<NEWL>><<NEWL>>@given(YQM_OFFSET)<<NEWL>>def test_shift_across_dst(offset):<<NEWL>>    # GH#18319 check that 1) timezone is correctly normalized and<<NEWL>>    # 2) that hour is not incorrectly changed by this normalization<<NEWL>>    assume(not offset.normalize)<<NEWL>><<NEWL>>    # Note that dti includes a transition across DST boundary<<NEWL>>    dti = pd.date_range(<<NEWL>>        start=""2017-10-30 12:00:00"", end=""2017-11-06"", freq=""D"", tz=""US/Eastern""<<NEWL>>    )<<NEWL>>    assert (dti.hour == 12).all()  # we haven't screwed up yet<<NEWL>><<NEWL>>    res = dti + offset<<NEWL>>    assert (res.hour == 12).all()"
397	jackson	2	"import tempfile, os<<NEWL>>from pathlib import Path<<NEWL>><<NEWL>>from traitlets.config.loader import Config<<NEWL>><<NEWL>><<NEWL>>def setup_module():<<NEWL>>    ip.magic('load_ext storemagic')<<NEWL>><<NEWL>>def test_store_restore():<<NEWL>>    assert 'bar' not in ip.user_ns, ""Error: some other test leaked `bar` in user_ns""<<NEWL>>    assert 'foo' not in ip.user_ns, ""Error: some other test leaked `foo` in user_ns""<<NEWL>>    assert 'foobar' not in ip.user_ns, ""Error: some other test leaked `foobar` in user_ns""<<NEWL>>    assert 'foobaz' not in ip.user_ns, ""Error: some other test leaked `foobaz` in user_ns""<<NEWL>>    ip.user_ns['foo'] = 78<<NEWL>>    ip.magic('alias bar echo ""hello""')<<NEWL>>    ip.user_ns['foobar'] = 79<<NEWL>>    ip.user_ns['foobaz'] = '80'<<NEWL>>    tmpd = tempfile.mkdtemp()<<NEWL>>    ip.magic('cd ' + tmpd)<<NEWL>>    ip.magic('store foo')<<NEWL>>    ip.magic('store bar')<<NEWL>>    ip.magic('store foobar foobaz')<<NEWL>><<NEWL>>    # Check storing<<NEWL>>    assert ip.db[""autorestore/foo""] == 78<<NEWL>>    assert ""bar"" in ip.db[""stored_aliases""]<<NEWL>>    assert ip.db[""autorestore/foobar""] == 79<<NEWL>>    assert ip.db[""autorestore/foobaz""] == ""80""<<NEWL>><<NEWL>>    # Remove those items<<NEWL>>    ip.user_ns.pop('foo', None)<<NEWL>>    ip.user_ns.pop('foobar', None)<<NEWL>>    ip.user_ns.pop('foobaz', None)<<NEWL>>    ip.alias_manager.undefine_alias('bar')<<NEWL>>    ip.magic('cd -')<<NEWL>>    ip.user_ns['_dh'][:] = []<<NEWL>><<NEWL>>    # Check restoring<<NEWL>>    ip.magic(""store -r foo bar foobar foobaz"")<<NEWL>>    assert ip.user_ns[""foo""] == 78<<NEWL>>    assert ip.alias_manager.is_alias(""bar"")<<NEWL>>    assert ip.user_ns[""foobar""] == 79<<NEWL>>    assert ip.user_ns[""foobaz""] == ""80""<<NEWL>><<NEWL>>    ip.magic(""store -r"")  # restores _dh too<<NEWL>>    assert any(Path(tmpd).samefile(p) for p in ip.user_ns[""_dh""])<<NEWL>><<NEWL>>    os.rmdir(tmpd)<<NEWL>><<NEWL>>def test_autorestore():<<NEWL>>    ip.user_ns['foo'] = 95<<NEWL>>    ip.magic('store foo')<<NEWL>>    del ip.user_ns['foo']<<NEWL>>    c = Config()<<NEWL>>    c.StoreMagics.autorestore = False<<NEWL>>    orig_config = ip.config<<NEWL>>    try:<<NEWL>>        ip.config = c<<NEWL>>        ip.extension_manager.reload_extension(""storemagic"")<<NEWL>>        assert ""foo"" not in ip.user_ns<<NEWL>>        c.StoreMagics.autorestore = True<<NEWL>>        ip.extension_manager.reload_extension(""storemagic"")<<NEWL>>        assert ip.user_ns[""foo""] == 95<<NEWL>>    finally:<<NEWL>>        ip.config = orig_config"
397	donghui	0	"import tempfile, os<<NEWL>>from pathlib import Path<<NEWL>><<NEWL>>from traitlets.config.loader import Config<<NEWL>><<NEWL>><<NEWL>>def setup_module():<<NEWL>>    ip.magic('load_ext storemagic')<<NEWL>><<NEWL>>def test_store_restore():<<NEWL>>    assert 'bar' not in ip.user_ns, ""Error: some other test leaked `bar` in user_ns""<<NEWL>>    assert 'foo' not in ip.user_ns, ""Error: some other test leaked `foo` in user_ns""<<NEWL>>    assert 'foobar' not in ip.user_ns, ""Error: some other test leaked `foobar` in user_ns""<<NEWL>>    assert 'foobaz' not in ip.user_ns, ""Error: some other test leaked `foobaz` in user_ns""<<NEWL>>    ip.user_ns['foo'] = 78<<NEWL>>    ip.magic('alias bar echo ""hello""')<<NEWL>>    ip.user_ns['foobar'] = 79<<NEWL>>    ip.user_ns['foobaz'] = '80'<<NEWL>>    tmpd = tempfile.mkdtemp()<<NEWL>>    ip.magic('cd ' + tmpd)<<NEWL>>    ip.magic('store foo')<<NEWL>>    ip.magic('store bar')<<NEWL>>    ip.magic('store foobar foobaz')<<NEWL>><<NEWL>>    # Check storing<<NEWL>>    assert ip.db[""autorestore/foo""] == 78<<NEWL>>    assert ""bar"" in ip.db[""stored_aliases""]<<NEWL>>    assert ip.db[""autorestore/foobar""] == 79<<NEWL>>    assert ip.db[""autorestore/foobaz""] == ""80""<<NEWL>><<NEWL>>    # Remove those items<<NEWL>>    ip.user_ns.pop('foo', None)<<NEWL>>    ip.user_ns.pop('foobar', None)<<NEWL>>    ip.user_ns.pop('foobaz', None)<<NEWL>>    ip.alias_manager.undefine_alias('bar')<<NEWL>>    ip.magic('cd -')<<NEWL>>    ip.user_ns['_dh'][:] = []<<NEWL>><<NEWL>>    # Check restoring<<NEWL>>    ip.magic(""store -r foo bar foobar foobaz"")<<NEWL>>    assert ip.user_ns[""foo""] == 78<<NEWL>>    assert ip.alias_manager.is_alias(""bar"")<<NEWL>>    assert ip.user_ns[""foobar""] == 79<<NEWL>>    assert ip.user_ns[""foobaz""] == ""80""<<NEWL>><<NEWL>>    ip.magic(""store -r"")  # restores _dh too<<NEWL>>    assert any(Path(tmpd).samefile(p) for p in ip.user_ns[""_dh""])<<NEWL>><<NEWL>>    os.rmdir(tmpd)<<NEWL>><<NEWL>>def test_autorestore():<<NEWL>>    ip.user_ns['foo'] = 95<<NEWL>>    ip.magic('store foo')<<NEWL>>    del ip.user_ns['foo']<<NEWL>>    c = Config()<<NEWL>>    c.StoreMagics.autorestore = False<<NEWL>>    orig_config = ip.config<<NEWL>>    try:<<NEWL>>        ip.config = c<<NEWL>>        ip.extension_manager.reload_extension(""storemagic"")<<NEWL>>        assert ""foo"" not in ip.user_ns<<NEWL>>        c.StoreMagics.autorestore = True<<NEWL>>        ip.extension_manager.reload_extension(""storemagic"")<<NEWL>>        assert ip.user_ns[""foo""] == 95<<NEWL>>    finally:<<NEWL>>        ip.config = orig_config"
488	jackson	0	"# coding: utf8<<NEWL>>from __future__ import unicode_literals<<NEWL>><<NEWL>>from ...attrs import LIKE_NUM<<NEWL>><<NEWL>><<NEWL>>_num_words = [<<NEWL>>    ""zero"",<<NEWL>>    ""um"",<<NEWL>>    ""dois"",<<NEWL>>    ""três"",<<NEWL>>    ""tres"",<<NEWL>>    ""quatro"",<<NEWL>>    ""cinco"",<<NEWL>>    ""seis"",<<NEWL>>    ""sete"",<<NEWL>>    ""oito"",<<NEWL>>    ""nove"",<<NEWL>>    ""dez"",<<NEWL>>    ""onze"",<<NEWL>>    ""doze"",<<NEWL>>    ""dúzia"",<<NEWL>>    ""dúzias"",<<NEWL>>    ""duzia"",<<NEWL>>    ""duzias"",<<NEWL>>    ""treze"",<<NEWL>>    ""catorze"",<<NEWL>>    ""quinze"",<<NEWL>>    ""dezasseis"",<<NEWL>>    ""dezassete"",<<NEWL>>    ""dezoito"",<<NEWL>>    ""dezanove"",<<NEWL>>    ""vinte"",<<NEWL>>    ""trinta"",<<NEWL>>    ""quarenta"",<<NEWL>>    ""cinquenta"",<<NEWL>>    ""sessenta"",<<NEWL>>    ""setenta"",<<NEWL>>    ""oitenta"",<<NEWL>>    ""noventa"",<<NEWL>>    ""cem"",<<NEWL>>    ""cento"",<<NEWL>>    ""duzentos"",<<NEWL>>    ""trezentos"",<<NEWL>>    ""quatrocentos"",<<NEWL>>    ""quinhentos"",<<NEWL>>    ""seicentos"",<<NEWL>>    ""setecentos"",<<NEWL>>    ""oitocentos"",<<NEWL>>    ""novecentos"",<<NEWL>>    ""mil"",<<NEWL>>    ""milhão"",<<NEWL>>    ""milhao"",<<NEWL>>    ""milhões"",<<NEWL>>    ""milhoes"",<<NEWL>>    ""bilhão"",<<NEWL>>    ""bilhao"",<<NEWL>>    ""bilhões"",<<NEWL>>    ""bilhoes"",<<NEWL>>    ""trilhão"",<<NEWL>>    ""trilhao"",<<NEWL>>    ""trilhões"",<<NEWL>>    ""trilhoes"",<<NEWL>>    ""quadrilhão"",<<NEWL>>    ""quadrilhao"",<<NEWL>>    ""quadrilhões"",<<NEWL>>    ""quadrilhoes"",<<NEWL>>]<<NEWL>><<NEWL>><<NEWL>>_ordinal_words = [<<NEWL>>    ""primeiro"",<<NEWL>>    ""segundo"",<<NEWL>>    ""terceiro"",<<NEWL>>    ""quarto"",<<NEWL>>    ""quinto"",<<NEWL>>    ""sexto"",<<NEWL>>    ""sétimo"",<<NEWL>>    ""oitavo"",<<NEWL>>    ""nono"",<<NEWL>>    ""décimo"",<<NEWL>>    ""vigésimo"",<<NEWL>>    ""trigésimo"",<<NEWL>>    ""quadragésimo"",<<NEWL>>    ""quinquagésimo"",<<NEWL>>    ""sexagésimo"",<<NEWL>>    ""septuagésimo"",<<NEWL>>    ""octogésimo"",<<NEWL>>    ""nonagésimo"",<<NEWL>>    ""centésimo"",<<NEWL>>    ""ducentésimo"",<<NEWL>>    ""trecentésimo"",<<NEWL>>    ""quadringentésimo"",<<NEWL>>    ""quingentésimo"",<<NEWL>>    ""sexcentésimo"",<<NEWL>>    ""septingentésimo"",<<NEWL>>    ""octingentésimo"",<<NEWL>>    ""nongentésimo"",<<NEWL>>    ""milésimo"",<<NEWL>>    ""milionésimo"",<<NEWL>>    ""bilionésimo"",<<NEWL>>]<<NEWL>><<NEWL>><<NEWL>>def like_num(text):<<NEWL>>    if text.startswith((""+"", ""-"", ""±"", ""~"")):<<NEWL>>        text = text[1:]<<NEWL>>    text = text.replace("","", """").replace(""."", """").replace(""º"", """").replace(""ª"", """")<<NEWL>>    if text.isdigit():<<NEWL>>        return True<<NEWL>>    if text.count(""/"") == 1:<<NEWL>>        num, denom = text.split(""/"")<<NEWL>>        if num.isdigit() and denom.isdigit():<<NEWL>>            return True<<NEWL>>    if text.lower() in _num_words:<<NEWL>>        return True<<NEWL>>    if text.lower() in _ordinal_words:<<NEWL>>        return True<<NEWL>>    return False<<NEWL>><<NEWL>><<NEWL>>LEX_ATTRS = {LIKE_NUM: like_num}"
488	donghui	0	"# coding: utf8<<NEWL>>from __future__ import unicode_literals<<NEWL>><<NEWL>>from ...attrs import LIKE_NUM<<NEWL>><<NEWL>><<NEWL>>_num_words = [<<NEWL>>    ""zero"",<<NEWL>>    ""um"",<<NEWL>>    ""dois"",<<NEWL>>    ""três"",<<NEWL>>    ""tres"",<<NEWL>>    ""quatro"",<<NEWL>>    ""cinco"",<<NEWL>>    ""seis"",<<NEWL>>    ""sete"",<<NEWL>>    ""oito"",<<NEWL>>    ""nove"",<<NEWL>>    ""dez"",<<NEWL>>    ""onze"",<<NEWL>>    ""doze"",<<NEWL>>    ""dúzia"",<<NEWL>>    ""dúzias"",<<NEWL>>    ""duzia"",<<NEWL>>    ""duzias"",<<NEWL>>    ""treze"",<<NEWL>>    ""catorze"",<<NEWL>>    ""quinze"",<<NEWL>>    ""dezasseis"",<<NEWL>>    ""dezassete"",<<NEWL>>    ""dezoito"",<<NEWL>>    ""dezanove"",<<NEWL>>    ""vinte"",<<NEWL>>    ""trinta"",<<NEWL>>    ""quarenta"",<<NEWL>>    ""cinquenta"",<<NEWL>>    ""sessenta"",<<NEWL>>    ""setenta"",<<NEWL>>    ""oitenta"",<<NEWL>>    ""noventa"",<<NEWL>>    ""cem"",<<NEWL>>    ""cento"",<<NEWL>>    ""duzentos"",<<NEWL>>    ""trezentos"",<<NEWL>>    ""quatrocentos"",<<NEWL>>    ""quinhentos"",<<NEWL>>    ""seicentos"",<<NEWL>>    ""setecentos"",<<NEWL>>    ""oitocentos"",<<NEWL>>    ""novecentos"",<<NEWL>>    ""mil"",<<NEWL>>    ""milhão"",<<NEWL>>    ""milhao"",<<NEWL>>    ""milhões"",<<NEWL>>    ""milhoes"",<<NEWL>>    ""bilhão"",<<NEWL>>    ""bilhao"",<<NEWL>>    ""bilhões"",<<NEWL>>    ""bilhoes"",<<NEWL>>    ""trilhão"",<<NEWL>>    ""trilhao"",<<NEWL>>    ""trilhões"",<<NEWL>>    ""trilhoes"",<<NEWL>>    ""quadrilhão"",<<NEWL>>    ""quadrilhao"",<<NEWL>>    ""quadrilhões"",<<NEWL>>    ""quadrilhoes"",<<NEWL>>]<<NEWL>><<NEWL>><<NEWL>>_ordinal_words = [<<NEWL>>    ""primeiro"",<<NEWL>>    ""segundo"",<<NEWL>>    ""terceiro"",<<NEWL>>    ""quarto"",<<NEWL>>    ""quinto"",<<NEWL>>    ""sexto"",<<NEWL>>    ""sétimo"",<<NEWL>>    ""oitavo"",<<NEWL>>    ""nono"",<<NEWL>>    ""décimo"",<<NEWL>>    ""vigésimo"",<<NEWL>>    ""trigésimo"",<<NEWL>>    ""quadragésimo"",<<NEWL>>    ""quinquagésimo"",<<NEWL>>    ""sexagésimo"",<<NEWL>>    ""septuagésimo"",<<NEWL>>    ""octogésimo"",<<NEWL>>    ""nonagésimo"",<<NEWL>>    ""centésimo"",<<NEWL>>    ""ducentésimo"",<<NEWL>>    ""trecentésimo"",<<NEWL>>    ""quadringentésimo"",<<NEWL>>    ""quingentésimo"",<<NEWL>>    ""sexcentésimo"",<<NEWL>>    ""septingentésimo"",<<NEWL>>    ""octingentésimo"",<<NEWL>>    ""nongentésimo"",<<NEWL>>    ""milésimo"",<<NEWL>>    ""milionésimo"",<<NEWL>>    ""bilionésimo"",<<NEWL>>]<<NEWL>><<NEWL>><<NEWL>>def like_num(text):<<NEWL>>    if text.startswith((""+"", ""-"", ""±"", ""~"")):<<NEWL>>        text = text[1:]<<NEWL>>    text = text.replace("","", """").replace(""."", """").replace(""º"", """").replace(""ª"", """")<<NEWL>>    if text.isdigit():<<NEWL>>        return True<<NEWL>>    if text.count(""/"") == 1:<<NEWL>>        num, denom = text.split(""/"")<<NEWL>>        if num.isdigit() and denom.isdigit():<<NEWL>>            return True<<NEWL>>    if text.lower() in _num_words:<<NEWL>>        return True<<NEWL>>    if text.lower() in _ordinal_words:<<NEWL>>        return True<<NEWL>>    return False<<NEWL>><<NEWL>><<NEWL>>LEX_ATTRS = {LIKE_NUM: like_num}"
498	jackson	2	"from django.conf import settings<<NEWL>>from django.core import checks<<NEWL>>from django.core.exceptions import FieldDoesNotExist<<NEWL>>from django.db import models<<NEWL>><<NEWL>><<NEWL>>class CurrentSiteManager(models.Manager):<<NEWL>>    ""Use this to limit objects to those associated with the current site.""<<NEWL>><<NEWL>>    use_in_migrations = True<<NEWL>><<NEWL>>    def __init__(self, field_name=None):<<NEWL>>        super().__init__()<<NEWL>>        self.__field_name = field_name<<NEWL>><<NEWL>>    def check(self, **kwargs):<<NEWL>>        errors = super().check(**kwargs)<<NEWL>>        errors.extend(self._check_field_name())<<NEWL>>        return errors<<NEWL>><<NEWL>>    def _check_field_name(self):<<NEWL>>        field_name = self._get_field_name()<<NEWL>>        try:<<NEWL>>            field = self.model._meta.get_field(field_name)<<NEWL>>        except FieldDoesNotExist:<<NEWL>>            return [<<NEWL>>                checks.Error(<<NEWL>>                    ""CurrentSiteManager could not find a field named '%s'."" % field_name,<<NEWL>>                    obj=self,<<NEWL>>                    id='sites.E001',<<NEWL>>                )<<NEWL>>            ]<<NEWL>><<NEWL>>        if not field.many_to_many and not isinstance(field, (models.ForeignKey)):<<NEWL>>            return [<<NEWL>>                checks.Error(<<NEWL>>                    ""CurrentSiteManager cannot use '%s.%s' as it is not a foreign key or a many-to-many field."" % (<<NEWL>>                        self.model._meta.object_name, field_name<<NEWL>>                    ),<<NEWL>>                    obj=self,<<NEWL>>                    id='sites.E002',<<NEWL>>                )<<NEWL>>            ]<<NEWL>><<NEWL>>        return []<<NEWL>><<NEWL>>    def _get_field_name(self):<<NEWL>>        """""" Return self.__field_name or 'site' or 'sites'. """"""<<NEWL>><<NEWL>>        if not self.__field_name:<<NEWL>>            try:<<NEWL>>                self.model._meta.get_field('site')<<NEWL>>            except FieldDoesNotExist:<<NEWL>>                self.__field_name = 'sites'<<NEWL>>            else:<<NEWL>>                self.__field_name = 'site'<<NEWL>>        return self.__field_name<<NEWL>><<NEWL>>    def get_queryset(self):<<NEWL>>        return super().get_queryset().filter(**{self._get_field_name() + '__id': settings.SITE_ID})"
498	donghui	2	"from django.conf import settings<<NEWL>>from django.core import checks<<NEWL>>from django.core.exceptions import FieldDoesNotExist<<NEWL>>from django.db import models<<NEWL>><<NEWL>><<NEWL>>class CurrentSiteManager(models.Manager):<<NEWL>>    ""Use this to limit objects to those associated with the current site.""<<NEWL>><<NEWL>>    use_in_migrations = True<<NEWL>><<NEWL>>    def __init__(self, field_name=None):<<NEWL>>        super().__init__()<<NEWL>>        self.__field_name = field_name<<NEWL>><<NEWL>>    def check(self, **kwargs):<<NEWL>>        errors = super().check(**kwargs)<<NEWL>>        errors.extend(self._check_field_name())<<NEWL>>        return errors<<NEWL>><<NEWL>>    def _check_field_name(self):<<NEWL>>        field_name = self._get_field_name()<<NEWL>>        try:<<NEWL>>            field = self.model._meta.get_field(field_name)<<NEWL>>        except FieldDoesNotExist:<<NEWL>>            return [<<NEWL>>                checks.Error(<<NEWL>>                    ""CurrentSiteManager could not find a field named '%s'."" % field_name,<<NEWL>>                    obj=self,<<NEWL>>                    id='sites.E001',<<NEWL>>                )<<NEWL>>            ]<<NEWL>><<NEWL>>        if not field.many_to_many and not isinstance(field, (models.ForeignKey)):<<NEWL>>            return [<<NEWL>>                checks.Error(<<NEWL>>                    ""CurrentSiteManager cannot use '%s.%s' as it is not a foreign key or a many-to-many field."" % (<<NEWL>>                        self.model._meta.object_name, field_name<<NEWL>>                    ),<<NEWL>>                    obj=self,<<NEWL>>                    id='sites.E002',<<NEWL>>                )<<NEWL>>            ]<<NEWL>><<NEWL>>        return []<<NEWL>><<NEWL>>    def _get_field_name(self):<<NEWL>>        """""" Return self.__field_name or 'site' or 'sites'. """"""<<NEWL>><<NEWL>>        if not self.__field_name:<<NEWL>>            try:<<NEWL>>                self.model._meta.get_field('site')<<NEWL>>            except FieldDoesNotExist:<<NEWL>>                self.__field_name = 'sites'<<NEWL>>            else:<<NEWL>>                self.__field_name = 'site'<<NEWL>>        return self.__field_name<<NEWL>><<NEWL>>    def get_queryset(self):<<NEWL>>        return super().get_queryset().filter(**{self._get_field_name() + '__id': settings.SITE_ID})"
387	jackson	2	"# Copyright 2022 Google LLC<<NEWL>>#<<NEWL>># Licensed under the Apache License, Version 2.0 (the ""License"");<<NEWL>># you may not use this file except in compliance with the License.<<NEWL>># You may obtain a copy of the License at<<NEWL>>#<<NEWL>>#     http://www.apache.org/licenses/LICENSE-2.0<<NEWL>>#<<NEWL>># Unless required by applicable law or agreed to in writing, software<<NEWL>># distributed under the License is distributed on an ""AS IS"" BASIS,<<NEWL>># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.<<NEWL>># See the License for the specific language governing permissions and<<NEWL>># limitations under the License.<<NEWL>><<NEWL>><<NEWL>># [START speech_quickstart_v2]<<NEWL>>import io<<NEWL>><<NEWL>>from google.cloud.speech_v2 import SpeechClient<<NEWL>>from google.cloud.speech_v2.types import cloud_speech<<NEWL>><<NEWL>><<NEWL>>def quickstart_v2(project_id, recognizer_id, audio_file):<<NEWL>>    # Instantiates a client<<NEWL>>    client = SpeechClient()<<NEWL>><<NEWL>>    request = cloud_speech.CreateRecognizerRequest(<<NEWL>>        parent=f""projects/{project_id}/locations/global"",<<NEWL>>        recognizer_id=recognizer_id,<<NEWL>>        recognizer=cloud_speech.Recognizer(<<NEWL>>            language_codes=[""en-US""], model=""latest_long""<<NEWL>>        ),<<NEWL>>    )<<NEWL>><<NEWL>>    # Creates a Recognizer<<NEWL>>    operation = client.create_recognizer(request=request)<<NEWL>>    recognizer = operation.result()<<NEWL>><<NEWL>>    # Reads a file as bytes<<NEWL>>    with io.open(audio_file, ""rb"") as f:<<NEWL>>        content = f.read()<<NEWL>><<NEWL>>    config = cloud_speech.RecognitionConfig(auto_decoding_config={})<<NEWL>><<NEWL>>    request = cloud_speech.RecognizeRequest(<<NEWL>>        recognizer=recognizer.name, config=config, content=content<<NEWL>>    )<<NEWL>><<NEWL>>    # Transcribes the audio into text<<NEWL>>    response = client.recognize(request=request)<<NEWL>><<NEWL>>    for result in response.results:<<NEWL>>        print(""Transcript: {}"".format(result.alternatives[0].transcript))<<NEWL>><<NEWL>>    return response<<NEWL>><<NEWL>><<NEWL>># [END speech_quickstart_v2]<<NEWL>><<NEWL>><<NEWL>>if __name__ == ""__main__"":<<NEWL>>    quickstart_v2()"
387	donghui	2	"# Copyright 2022 Google LLC<<NEWL>>#<<NEWL>># Licensed under the Apache License, Version 2.0 (the ""License"");<<NEWL>># you may not use this file except in compliance with the License.<<NEWL>># You may obtain a copy of the License at<<NEWL>>#<<NEWL>>#     http://www.apache.org/licenses/LICENSE-2.0<<NEWL>>#<<NEWL>># Unless required by applicable law or agreed to in writing, software<<NEWL>># distributed under the License is distributed on an ""AS IS"" BASIS,<<NEWL>># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.<<NEWL>># See the License for the specific language governing permissions and<<NEWL>># limitations under the License.<<NEWL>><<NEWL>><<NEWL>># [START speech_quickstart_v2]<<NEWL>>import io<<NEWL>><<NEWL>>from google.cloud.speech_v2 import SpeechClient<<NEWL>>from google.cloud.speech_v2.types import cloud_speech<<NEWL>><<NEWL>><<NEWL>>def quickstart_v2(project_id, recognizer_id, audio_file):<<NEWL>>    # Instantiates a client<<NEWL>>    client = SpeechClient()<<NEWL>><<NEWL>>    request = cloud_speech.CreateRecognizerRequest(<<NEWL>>        parent=f""projects/{project_id}/locations/global"",<<NEWL>>        recognizer_id=recognizer_id,<<NEWL>>        recognizer=cloud_speech.Recognizer(<<NEWL>>            language_codes=[""en-US""], model=""latest_long""<<NEWL>>        ),<<NEWL>>    )<<NEWL>><<NEWL>>    # Creates a Recognizer<<NEWL>>    operation = client.create_recognizer(request=request)<<NEWL>>    recognizer = operation.result()<<NEWL>><<NEWL>>    # Reads a file as bytes<<NEWL>>    with io.open(audio_file, ""rb"") as f:<<NEWL>>        content = f.read()<<NEWL>><<NEWL>>    config = cloud_speech.RecognitionConfig(auto_decoding_config={})<<NEWL>><<NEWL>>    request = cloud_speech.RecognizeRequest(<<NEWL>>        recognizer=recognizer.name, config=config, content=content<<NEWL>>    )<<NEWL>><<NEWL>>    # Transcribes the audio into text<<NEWL>>    response = client.recognize(request=request)<<NEWL>><<NEWL>>    for result in response.results:<<NEWL>>        print(""Transcript: {}"".format(result.alternatives[0].transcript))<<NEWL>><<NEWL>>    return response<<NEWL>><<NEWL>><<NEWL>># [END speech_quickstart_v2]<<NEWL>><<NEWL>><<NEWL>>if __name__ == ""__main__"":<<NEWL>>    quickstart_v2()"
296	jackson	2	"import _plotly_utils.basevalidators<<NEWL>><<NEWL>><<NEWL>>class HoverlabelValidator(_plotly_utils.basevalidators.CompoundValidator):<<NEWL>>    def __init__(<<NEWL>>        self, plotly_name=""hoverlabel"", parent_name=""scatterternary"", **kwargs<<NEWL>>    ):<<NEWL>>        super(HoverlabelValidator, self).__init__(<<NEWL>>            plotly_name=plotly_name,<<NEWL>>            parent_name=parent_name,<<NEWL>>            data_class_str=kwargs.pop(""data_class_str"", ""Hoverlabel""),<<NEWL>>            data_docs=kwargs.pop(<<NEWL>>                ""data_docs"",<<NEWL>>                """"""<<NEWL>>            align<<NEWL>>                Sets the horizontal alignment of the text<<NEWL>>                content within hover label box. Has an effect<<NEWL>>                only if the hover label text spans more two or<<NEWL>>                more lines<<NEWL>>            alignsrc<<NEWL>>                Sets the source reference on Chart Studio Cloud<<NEWL>>                for `align`.<<NEWL>>            bgcolor<<NEWL>>                Sets the background color of the hover labels<<NEWL>>                for this trace<<NEWL>>            bgcolorsrc<<NEWL>>                Sets the source reference on Chart Studio Cloud<<NEWL>>                for `bgcolor`.<<NEWL>>            bordercolor<<NEWL>>                Sets the border color of the hover labels for<<NEWL>>                this trace.<<NEWL>>            bordercolorsrc<<NEWL>>                Sets the source reference on Chart Studio Cloud<<NEWL>>                for `bordercolor`.<<NEWL>>            font<<NEWL>>                Sets the font used in hover labels.<<NEWL>>            namelength<<NEWL>>                Sets the default length (in number of<<NEWL>>                characters) of the trace name in the hover<<NEWL>>                labels for all traces. -1 shows the whole name<<NEWL>>                regardless of length. 0-3 shows the first 0-3<<NEWL>>                characters, and an integer >3 will show the<<NEWL>>                whole name if it is less than that many<<NEWL>>                characters, but if it is longer, will truncate<<NEWL>>                to `namelength - 3` characters and add an<<NEWL>>                ellipsis.<<NEWL>>            namelengthsrc<<NEWL>>                Sets the source reference on Chart Studio Cloud<<NEWL>>                for `namelength`.<<NEWL>>"""""",<<NEWL>>            ),<<NEWL>>            **kwargs,<<NEWL>>        )"
296	donghui	1	"import _plotly_utils.basevalidators<<NEWL>><<NEWL>><<NEWL>>class HoverlabelValidator(_plotly_utils.basevalidators.CompoundValidator):<<NEWL>>    def __init__(<<NEWL>>        self, plotly_name=""hoverlabel"", parent_name=""scatterternary"", **kwargs<<NEWL>>    ):<<NEWL>>        super(HoverlabelValidator, self).__init__(<<NEWL>>            plotly_name=plotly_name,<<NEWL>>            parent_name=parent_name,<<NEWL>>            data_class_str=kwargs.pop(""data_class_str"", ""Hoverlabel""),<<NEWL>>            data_docs=kwargs.pop(<<NEWL>>                ""data_docs"",<<NEWL>>                """"""<<NEWL>>            align<<NEWL>>                Sets the horizontal alignment of the text<<NEWL>>                content within hover label box. Has an effect<<NEWL>>                only if the hover label text spans more two or<<NEWL>>                more lines<<NEWL>>            alignsrc<<NEWL>>                Sets the source reference on Chart Studio Cloud<<NEWL>>                for `align`.<<NEWL>>            bgcolor<<NEWL>>                Sets the background color of the hover labels<<NEWL>>                for this trace<<NEWL>>            bgcolorsrc<<NEWL>>                Sets the source reference on Chart Studio Cloud<<NEWL>>                for `bgcolor`.<<NEWL>>            bordercolor<<NEWL>>                Sets the border color of the hover labels for<<NEWL>>                this trace.<<NEWL>>            bordercolorsrc<<NEWL>>                Sets the source reference on Chart Studio Cloud<<NEWL>>                for `bordercolor`.<<NEWL>>            font<<NEWL>>                Sets the font used in hover labels.<<NEWL>>            namelength<<NEWL>>                Sets the default length (in number of<<NEWL>>                characters) of the trace name in the hover<<NEWL>>                labels for all traces. -1 shows the whole name<<NEWL>>                regardless of length. 0-3 shows the first 0-3<<NEWL>>                characters, and an integer >3 will show the<<NEWL>>                whole name if it is less than that many<<NEWL>>                characters, but if it is longer, will truncate<<NEWL>>                to `namelength - 3` characters and add an<<NEWL>>                ellipsis.<<NEWL>>            namelengthsrc<<NEWL>>                Sets the source reference on Chart Studio Cloud<<NEWL>>                for `namelength`.<<NEWL>>"""""",<<NEWL>>            ),<<NEWL>>            **kwargs,<<NEWL>>        )"
265	jackson	0	"# Copyright 2019 Google, LLC.<<NEWL>>#<<NEWL>># Licensed under the Apache License, Version 2.0 (the ""License"");<<NEWL>># you may not use this file except in compliance with the License.<<NEWL>># You may obtain a copy of the License at<<NEWL>>#<<NEWL>>#    http://www.apache.org/licenses/LICENSE-2.0<<NEWL>>#<<NEWL>># Unless required by applicable law or agreed to in writing, software<<NEWL>># distributed under the License is distributed on an ""AS IS"" BASIS,<<NEWL>># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.<<NEWL>># See the License for the specific language governing permissions and<<NEWL>># limitations under the License.<<NEWL>><<NEWL>># NOTE:<<NEWL>># These tests are unit tests that mock Pub/Sub.<<NEWL>>import base64<<NEWL>>import json<<NEWL>>import uuid<<NEWL>><<NEWL>>import mock<<NEWL>>import pytest<<NEWL>><<NEWL>>import main<<NEWL>><<NEWL>><<NEWL>>@pytest.fixture<<NEWL>>def client():<<NEWL>>    main.app.testing = True<<NEWL>>    return main.app.test_client()<<NEWL>><<NEWL>><<NEWL>>def test_empty_payload(client):<<NEWL>>    r = client.post(""/"", json="""")<<NEWL>>    assert r.status_code == 400<<NEWL>><<NEWL>><<NEWL>>def test_invalid_payload(client):<<NEWL>>    r = client.post(""/"", json={""nomessage"": ""invalid""})<<NEWL>>    assert r.status_code == 400<<NEWL>><<NEWL>><<NEWL>>def test_invalid_mimetype(client):<<NEWL>>    r = client.post(""/"", json=""{ message: true }"")<<NEWL>>    assert r.status_code == 400<<NEWL>><<NEWL>><<NEWL>>@mock.patch(""image.blur_offensive_images"", mock.MagicMock(return_value=204))<<NEWL>>def test_minimally_valid_message(client):<<NEWL>>    data_json = json.dumps({""name"": True, ""bucket"": True})<<NEWL>>    data = base64.b64encode(data_json.encode()).decode()<<NEWL>><<NEWL>>    r = client.post(""/"", json={""message"": {""data"": data}})<<NEWL>>    assert r.status_code == 204<<NEWL>><<NEWL>><<NEWL>>def test_call_to_blur_image(client, capsys):<<NEWL>>    filename = str(uuid.uuid4())<<NEWL>>    blur_bucket = ""blurred-bucket-"" + str(uuid.uuid4())<<NEWL>><<NEWL>>    data_json = json.dumps({""name"": filename, ""bucket"": blur_bucket})<<NEWL>>    data = base64.b64encode(data_json.encode()).decode()<<NEWL>><<NEWL>>    r = client.post(""/"", json={""message"": {""data"": data}})<<NEWL>>    assert r.status_code == 204<<NEWL>><<NEWL>>    out, _ = capsys.readouterr()<<NEWL>>    assert f""The image {filename} was detected as OK"" in out"
265	donghui	0	"# Copyright 2019 Google, LLC.<<NEWL>>#<<NEWL>># Licensed under the Apache License, Version 2.0 (the ""License"");<<NEWL>># you may not use this file except in compliance with the License.<<NEWL>># You may obtain a copy of the License at<<NEWL>>#<<NEWL>>#    http://www.apache.org/licenses/LICENSE-2.0<<NEWL>>#<<NEWL>># Unless required by applicable law or agreed to in writing, software<<NEWL>># distributed under the License is distributed on an ""AS IS"" BASIS,<<NEWL>># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.<<NEWL>># See the License for the specific language governing permissions and<<NEWL>># limitations under the License.<<NEWL>><<NEWL>># NOTE:<<NEWL>># These tests are unit tests that mock Pub/Sub.<<NEWL>>import base64<<NEWL>>import json<<NEWL>>import uuid<<NEWL>><<NEWL>>import mock<<NEWL>>import pytest<<NEWL>><<NEWL>>import main<<NEWL>><<NEWL>><<NEWL>>@pytest.fixture<<NEWL>>def client():<<NEWL>>    main.app.testing = True<<NEWL>>    return main.app.test_client()<<NEWL>><<NEWL>><<NEWL>>def test_empty_payload(client):<<NEWL>>    r = client.post(""/"", json="""")<<NEWL>>    assert r.status_code == 400<<NEWL>><<NEWL>><<NEWL>>def test_invalid_payload(client):<<NEWL>>    r = client.post(""/"", json={""nomessage"": ""invalid""})<<NEWL>>    assert r.status_code == 400<<NEWL>><<NEWL>><<NEWL>>def test_invalid_mimetype(client):<<NEWL>>    r = client.post(""/"", json=""{ message: true }"")<<NEWL>>    assert r.status_code == 400<<NEWL>><<NEWL>><<NEWL>>@mock.patch(""image.blur_offensive_images"", mock.MagicMock(return_value=204))<<NEWL>>def test_minimally_valid_message(client):<<NEWL>>    data_json = json.dumps({""name"": True, ""bucket"": True})<<NEWL>>    data = base64.b64encode(data_json.encode()).decode()<<NEWL>><<NEWL>>    r = client.post(""/"", json={""message"": {""data"": data}})<<NEWL>>    assert r.status_code == 204<<NEWL>><<NEWL>><<NEWL>>def test_call_to_blur_image(client, capsys):<<NEWL>>    filename = str(uuid.uuid4())<<NEWL>>    blur_bucket = ""blurred-bucket-"" + str(uuid.uuid4())<<NEWL>><<NEWL>>    data_json = json.dumps({""name"": filename, ""bucket"": blur_bucket})<<NEWL>>    data = base64.b64encode(data_json.encode()).decode()<<NEWL>><<NEWL>>    r = client.post(""/"", json={""message"": {""data"": data}})<<NEWL>>    assert r.status_code == 204<<NEWL>><<NEWL>>    out, _ = capsys.readouterr()<<NEWL>>    assert f""The image {filename} was detected as OK"" in out"
325	jackson	1	"import pytest<<NEWL>><<NEWL>>import pandas as pd<<NEWL>>import pandas._testing as tm<<NEWL>><<NEWL>><<NEWL>>class TestDatetimeIndexFillNA:<<NEWL>>    @pytest.mark.parametrize(""tz"", [""US/Eastern"", ""Asia/Tokyo""])<<NEWL>>    def test_fillna_datetime64(self, tz):<<NEWL>>        # GH 11343<<NEWL>>        idx = pd.DatetimeIndex([""2011-01-01 09:00"", pd.NaT, ""2011-01-01 11:00""])<<NEWL>><<NEWL>>        exp = pd.DatetimeIndex(<<NEWL>>            [""2011-01-01 09:00"", ""2011-01-01 10:00"", ""2011-01-01 11:00""]<<NEWL>>        )<<NEWL>>        tm.assert_index_equal(idx.fillna(pd.Timestamp(""2011-01-01 10:00"")), exp)<<NEWL>><<NEWL>>        # tz mismatch<<NEWL>>        exp = pd.Index(<<NEWL>>            [<<NEWL>>                pd.Timestamp(""2011-01-01 09:00""),<<NEWL>>                pd.Timestamp(""2011-01-01 10:00"", tz=tz),<<NEWL>>                pd.Timestamp(""2011-01-01 11:00""),<<NEWL>>            ],<<NEWL>>            dtype=object,<<NEWL>>        )<<NEWL>>        tm.assert_index_equal(idx.fillna(pd.Timestamp(""2011-01-01 10:00"", tz=tz)), exp)<<NEWL>><<NEWL>>        # object<<NEWL>>        exp = pd.Index(<<NEWL>>            [pd.Timestamp(""2011-01-01 09:00""), ""x"", pd.Timestamp(""2011-01-01 11:00"")],<<NEWL>>            dtype=object,<<NEWL>>        )<<NEWL>>        tm.assert_index_equal(idx.fillna(""x""), exp)<<NEWL>><<NEWL>>        idx = pd.DatetimeIndex([""2011-01-01 09:00"", pd.NaT, ""2011-01-01 11:00""], tz=tz)<<NEWL>><<NEWL>>        exp = pd.DatetimeIndex(<<NEWL>>            [""2011-01-01 09:00"", ""2011-01-01 10:00"", ""2011-01-01 11:00""], tz=tz<<NEWL>>        )<<NEWL>>        tm.assert_index_equal(idx.fillna(pd.Timestamp(""2011-01-01 10:00"", tz=tz)), exp)<<NEWL>><<NEWL>>        exp = pd.Index(<<NEWL>>            [<<NEWL>>                pd.Timestamp(""2011-01-01 09:00"", tz=tz),<<NEWL>>                pd.Timestamp(""2011-01-01 10:00""),<<NEWL>>                pd.Timestamp(""2011-01-01 11:00"", tz=tz),<<NEWL>>            ],<<NEWL>>            dtype=object,<<NEWL>>        )<<NEWL>>        tm.assert_index_equal(idx.fillna(pd.Timestamp(""2011-01-01 10:00"")), exp)<<NEWL>><<NEWL>>        # object<<NEWL>>        exp = pd.Index(<<NEWL>>            [<<NEWL>>                pd.Timestamp(""2011-01-01 09:00"", tz=tz),<<NEWL>>                ""x"",<<NEWL>>                pd.Timestamp(""2011-01-01 11:00"", tz=tz),<<NEWL>>            ],<<NEWL>>            dtype=object,<<NEWL>>        )<<NEWL>>        tm.assert_index_equal(idx.fillna(""x""), exp)"
325	donghui	1	"import pytest<<NEWL>><<NEWL>>import pandas as pd<<NEWL>>import pandas._testing as tm<<NEWL>><<NEWL>><<NEWL>>class TestDatetimeIndexFillNA:<<NEWL>>    @pytest.mark.parametrize(""tz"", [""US/Eastern"", ""Asia/Tokyo""])<<NEWL>>    def test_fillna_datetime64(self, tz):<<NEWL>>        # GH 11343<<NEWL>>        idx = pd.DatetimeIndex([""2011-01-01 09:00"", pd.NaT, ""2011-01-01 11:00""])<<NEWL>><<NEWL>>        exp = pd.DatetimeIndex(<<NEWL>>            [""2011-01-01 09:00"", ""2011-01-01 10:00"", ""2011-01-01 11:00""]<<NEWL>>        )<<NEWL>>        tm.assert_index_equal(idx.fillna(pd.Timestamp(""2011-01-01 10:00"")), exp)<<NEWL>><<NEWL>>        # tz mismatch<<NEWL>>        exp = pd.Index(<<NEWL>>            [<<NEWL>>                pd.Timestamp(""2011-01-01 09:00""),<<NEWL>>                pd.Timestamp(""2011-01-01 10:00"", tz=tz),<<NEWL>>                pd.Timestamp(""2011-01-01 11:00""),<<NEWL>>            ],<<NEWL>>            dtype=object,<<NEWL>>        )<<NEWL>>        tm.assert_index_equal(idx.fillna(pd.Timestamp(""2011-01-01 10:00"", tz=tz)), exp)<<NEWL>><<NEWL>>        # object<<NEWL>>        exp = pd.Index(<<NEWL>>            [pd.Timestamp(""2011-01-01 09:00""), ""x"", pd.Timestamp(""2011-01-01 11:00"")],<<NEWL>>            dtype=object,<<NEWL>>        )<<NEWL>>        tm.assert_index_equal(idx.fillna(""x""), exp)<<NEWL>><<NEWL>>        idx = pd.DatetimeIndex([""2011-01-01 09:00"", pd.NaT, ""2011-01-01 11:00""], tz=tz)<<NEWL>><<NEWL>>        exp = pd.DatetimeIndex(<<NEWL>>            [""2011-01-01 09:00"", ""2011-01-01 10:00"", ""2011-01-01 11:00""], tz=tz<<NEWL>>        )<<NEWL>>        tm.assert_index_equal(idx.fillna(pd.Timestamp(""2011-01-01 10:00"", tz=tz)), exp)<<NEWL>><<NEWL>>        exp = pd.Index(<<NEWL>>            [<<NEWL>>                pd.Timestamp(""2011-01-01 09:00"", tz=tz),<<NEWL>>                pd.Timestamp(""2011-01-01 10:00""),<<NEWL>>                pd.Timestamp(""2011-01-01 11:00"", tz=tz),<<NEWL>>            ],<<NEWL>>            dtype=object,<<NEWL>>        )<<NEWL>>        tm.assert_index_equal(idx.fillna(pd.Timestamp(""2011-01-01 10:00"")), exp)<<NEWL>><<NEWL>>        # object<<NEWL>>        exp = pd.Index(<<NEWL>>            [<<NEWL>>                pd.Timestamp(""2011-01-01 09:00"", tz=tz),<<NEWL>>                ""x"",<<NEWL>>                pd.Timestamp(""2011-01-01 11:00"", tz=tz),<<NEWL>>            ],<<NEWL>>            dtype=object,<<NEWL>>        )<<NEWL>>        tm.assert_index_equal(idx.fillna(""x""), exp)"
374	jackson	3	"# A version of the ActiveScripting engine that enables rexec support<<NEWL>># This version supports hosting by IE - however, due to Python's<<NEWL>># rexec module being neither completely trusted nor private, it is<<NEWL>># *not* enabled by default.<<NEWL>># As of Python 2.2, rexec is simply not available - thus, if you use this,<<NEWL>># a HTML page can do almost *anything* at all on your machine.<<NEWL>><<NEWL>># You almost certainly do NOT want to use thus!<<NEWL>><<NEWL>>import pythoncom<<NEWL>>from win32com.axscript import axscript<<NEWL>>import winerror<<NEWL>>from . import pyscript<<NEWL>><<NEWL>>INTERFACE_USES_DISPEX = 0x00000004  # Object knows to use IDispatchEx<<NEWL>>INTERFACE_USES_SECURITY_MANAGER = (<<NEWL>>    0x00000008  # Object knows to use IInternetHostSecurityManager<<NEWL>>)<<NEWL>><<NEWL>><<NEWL>>class PyScriptRExec(pyscript.PyScript):<<NEWL>>    # Setup the auto-registration stuff...<<NEWL>>    _reg_verprogid_ = ""Python.AXScript-rexec.2""<<NEWL>>    _reg_progid_ = ""Python""  # Same ProgID as the standard engine.<<NEWL>>    # <<TAB>>_reg_policy_spec_ = default<<NEWL>>    _reg_catids_ = [axscript.CATID_ActiveScript, axscript.CATID_ActiveScriptParse]<<NEWL>>    _reg_desc_ = ""Python ActiveX Scripting Engine (with rexec support)""<<NEWL>>    _reg_clsid_ = ""{69c2454b-efa2-455b-988c-c3651c4a2f69}""<<NEWL>>    _reg_class_spec_ = ""win32com.axscript.client.pyscript_rexec.PyScriptRExec""<<NEWL>>    _reg_remove_keys_ = [("".pys"",), (""pysFile"",)]<<NEWL>>    _reg_threading_ = ""Apartment""<<NEWL>><<NEWL>>    def _GetSupportedInterfaceSafetyOptions(self):<<NEWL>>        # print ""**** calling"", pyscript.PyScript._GetSupportedInterfaceSafetyOptions, ""**->"", pyscript.PyScript._GetSupportedInterfaceSafetyOptions(self)<<NEWL>>        return (<<NEWL>>            INTERFACE_USES_DISPEX<<NEWL>>            | INTERFACE_USES_SECURITY_MANAGER<<NEWL>>            | axscript.INTERFACESAFE_FOR_UNTRUSTED_DATA<<NEWL>>            | axscript.INTERFACESAFE_FOR_UNTRUSTED_CALLER<<NEWL>>        )<<NEWL>><<NEWL>><<NEWL>>if __name__ == ""__main__"":<<NEWL>>    print(""WARNING: By registering this engine, you are giving remote HTML code"")<<NEWL>>    print(""the ability to execute *any* code on your system."")<<NEWL>>    print()<<NEWL>>    print(""You almost certainly do NOT want to do this."")<<NEWL>>    print(""You have been warned, and are doing this at your own (significant) risk"")<<NEWL>>    pyscript.Register(PyScriptRExec)"
374	donghui	2	"# A version of the ActiveScripting engine that enables rexec support<<NEWL>># This version supports hosting by IE - however, due to Python's<<NEWL>># rexec module being neither completely trusted nor private, it is<<NEWL>># *not* enabled by default.<<NEWL>># As of Python 2.2, rexec is simply not available - thus, if you use this,<<NEWL>># a HTML page can do almost *anything* at all on your machine.<<NEWL>><<NEWL>># You almost certainly do NOT want to use thus!<<NEWL>><<NEWL>>import pythoncom<<NEWL>>from win32com.axscript import axscript<<NEWL>>import winerror<<NEWL>>from . import pyscript<<NEWL>><<NEWL>>INTERFACE_USES_DISPEX = 0x00000004  # Object knows to use IDispatchEx<<NEWL>>INTERFACE_USES_SECURITY_MANAGER = (<<NEWL>>    0x00000008  # Object knows to use IInternetHostSecurityManager<<NEWL>>)<<NEWL>><<NEWL>><<NEWL>>class PyScriptRExec(pyscript.PyScript):<<NEWL>>    # Setup the auto-registration stuff...<<NEWL>>    _reg_verprogid_ = ""Python.AXScript-rexec.2""<<NEWL>>    _reg_progid_ = ""Python""  # Same ProgID as the standard engine.<<NEWL>>    # <<TAB>>_reg_policy_spec_ = default<<NEWL>>    _reg_catids_ = [axscript.CATID_ActiveScript, axscript.CATID_ActiveScriptParse]<<NEWL>>    _reg_desc_ = ""Python ActiveX Scripting Engine (with rexec support)""<<NEWL>>    _reg_clsid_ = ""{69c2454b-efa2-455b-988c-c3651c4a2f69}""<<NEWL>>    _reg_class_spec_ = ""win32com.axscript.client.pyscript_rexec.PyScriptRExec""<<NEWL>>    _reg_remove_keys_ = [("".pys"",), (""pysFile"",)]<<NEWL>>    _reg_threading_ = ""Apartment""<<NEWL>><<NEWL>>    def _GetSupportedInterfaceSafetyOptions(self):<<NEWL>>        # print ""**** calling"", pyscript.PyScript._GetSupportedInterfaceSafetyOptions, ""**->"", pyscript.PyScript._GetSupportedInterfaceSafetyOptions(self)<<NEWL>>        return (<<NEWL>>            INTERFACE_USES_DISPEX<<NEWL>>            | INTERFACE_USES_SECURITY_MANAGER<<NEWL>>            | axscript.INTERFACESAFE_FOR_UNTRUSTED_DATA<<NEWL>>            | axscript.INTERFACESAFE_FOR_UNTRUSTED_CALLER<<NEWL>>        )<<NEWL>><<NEWL>><<NEWL>>if __name__ == ""__main__"":<<NEWL>>    print(""WARNING: By registering this engine, you are giving remote HTML code"")<<NEWL>>    print(""the ability to execute *any* code on your system."")<<NEWL>>    print()<<NEWL>>    print(""You almost certainly do NOT want to do this."")<<NEWL>>    print(""You have been warned, and are doing this at your own (significant) risk"")<<NEWL>>    pyscript.Register(PyScriptRExec)"
340	jackson	2	# Copyright 2020 Google LLC<<NEWL>>#<<NEWL>># Licensed under the Apache License, Version 2.0 (the 'License');<<NEWL>># you may not use this file except in compliance with the License.<<NEWL>># You may obtain a copy of the License at<<NEWL>>#<<NEWL>>#     http://www.apache.org/licenses/LICENSE-2.0<<NEWL>>#<<NEWL>># Unless required by applicable law or agreed to in writing, software<<NEWL>># distributed under the License is distributed on an 'AS IS' BASIS,<<NEWL>># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.<<NEWL>># See the License for the specific language governing permissions and<<NEWL>># limitations under the License.<<NEWL>><<NEWL>># [START functions_pubsub_integration_test]<<NEWL>>import base64<<NEWL>>import os<<NEWL>>import subprocess<<NEWL>>import uuid<<NEWL>><<NEWL>>import requests<<NEWL>>from requests.packages.urllib3.util.retry import Retry<<NEWL>><<NEWL>><<NEWL>>def test_print_name():<<NEWL>>    name = str(uuid.uuid4())<<NEWL>>    port = 8088  # Each running framework instance needs a unique port<<NEWL>><<NEWL>>    encoded_name = base64.b64encode(name.encode('utf-8')).decode('utf-8')<<NEWL>>    pubsub_message = {<<NEWL>>        'data': {'data': encoded_name}<<NEWL>>    }<<NEWL>><<NEWL>>    process = subprocess.Popen(<<NEWL>>      [<<NEWL>>        'functions-framework',<<NEWL>>        '--target', 'hello_pubsub',<<NEWL>>        '--signature-type', 'event',<<NEWL>>        '--port', str(port)<<NEWL>>      ],<<NEWL>>      cwd=os.path.dirname(__file__),<<NEWL>>      stdout=subprocess.PIPE<<NEWL>>    )<<NEWL>><<NEWL>>    # Send HTTP request simulating Pub/Sub message<<NEWL>>    # (GCF translates Pub/Sub messages to HTTP requests internally)<<NEWL>>    url = f'http://localhost:{port}/'<<NEWL>><<NEWL>>    retry_policy = Retry(total=6, backoff_factor=1)<<NEWL>>    retry_adapter = requests.adapters.HTTPAdapter(<<NEWL>>      max_retries=retry_policy)<<NEWL>><<NEWL>>    session = requests.Session()<<NEWL>>    session.mount(url, retry_adapter)<<NEWL>><<NEWL>>    response = session.post(url, json=pubsub_message)<<NEWL>><<NEWL>>    assert response.status_code == 200<<NEWL>><<NEWL>>    # Stop the functions framework process<<NEWL>>    process.kill()<<NEWL>>    process.wait()<<NEWL>>    out, err = process.communicate()<<NEWL>><<NEWL>>    print(out, err, response.content)<<NEWL>><<NEWL>>    assert f'Hello {name}!' in str(out)<<NEWL>># [END functions_pubsub_integration_test]
340	donghui	1	# Copyright 2020 Google LLC<<NEWL>>#<<NEWL>># Licensed under the Apache License, Version 2.0 (the 'License');<<NEWL>># you may not use this file except in compliance with the License.<<NEWL>># You may obtain a copy of the License at<<NEWL>>#<<NEWL>>#     http://www.apache.org/licenses/LICENSE-2.0<<NEWL>>#<<NEWL>># Unless required by applicable law or agreed to in writing, software<<NEWL>># distributed under the License is distributed on an 'AS IS' BASIS,<<NEWL>># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.<<NEWL>># See the License for the specific language governing permissions and<<NEWL>># limitations under the License.<<NEWL>><<NEWL>># [START functions_pubsub_integration_test]<<NEWL>>import base64<<NEWL>>import os<<NEWL>>import subprocess<<NEWL>>import uuid<<NEWL>><<NEWL>>import requests<<NEWL>>from requests.packages.urllib3.util.retry import Retry<<NEWL>><<NEWL>><<NEWL>>def test_print_name():<<NEWL>>    name = str(uuid.uuid4())<<NEWL>>    port = 8088  # Each running framework instance needs a unique port<<NEWL>><<NEWL>>    encoded_name = base64.b64encode(name.encode('utf-8')).decode('utf-8')<<NEWL>>    pubsub_message = {<<NEWL>>        'data': {'data': encoded_name}<<NEWL>>    }<<NEWL>><<NEWL>>    process = subprocess.Popen(<<NEWL>>      [<<NEWL>>        'functions-framework',<<NEWL>>        '--target', 'hello_pubsub',<<NEWL>>        '--signature-type', 'event',<<NEWL>>        '--port', str(port)<<NEWL>>      ],<<NEWL>>      cwd=os.path.dirname(__file__),<<NEWL>>      stdout=subprocess.PIPE<<NEWL>>    )<<NEWL>><<NEWL>>    # Send HTTP request simulating Pub/Sub message<<NEWL>>    # (GCF translates Pub/Sub messages to HTTP requests internally)<<NEWL>>    url = f'http://localhost:{port}/'<<NEWL>><<NEWL>>    retry_policy = Retry(total=6, backoff_factor=1)<<NEWL>>    retry_adapter = requests.adapters.HTTPAdapter(<<NEWL>>      max_retries=retry_policy)<<NEWL>><<NEWL>>    session = requests.Session()<<NEWL>>    session.mount(url, retry_adapter)<<NEWL>><<NEWL>>    response = session.post(url, json=pubsub_message)<<NEWL>><<NEWL>>    assert response.status_code == 200<<NEWL>><<NEWL>>    # Stop the functions framework process<<NEWL>>    process.kill()<<NEWL>>    process.wait()<<NEWL>>    out, err = process.communicate()<<NEWL>><<NEWL>>    print(out, err, response.content)<<NEWL>><<NEWL>>    assert f'Hello {name}!' in str(out)<<NEWL>># [END functions_pubsub_integration_test]
311	jackson	0	"#!/usr/bin/env python<<NEWL>>#<<NEWL>># Copyright 2017 the original author or authors.<<NEWL>>#<<NEWL>># Licensed under the Apache License, Version 2.0 (the ""License"");<<NEWL>># you may not use this file except in compliance with the License.<<NEWL>># You may obtain a copy of the License at<<NEWL>>#<<NEWL>>#      http://www.apache.org/licenses/LICENSE-2.0<<NEWL>>#<<NEWL>># Unless required by applicable law or agreed to in writing, software<<NEWL>># distributed under the License is distributed on an ""AS IS"" BASIS,<<NEWL>># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.<<NEWL>># See the License for the specific language governing permissions and<<NEWL>># limitations under the License.<<NEWL>>#<<NEWL>><<NEWL>>from lxml import etree<<NEWL>>import structlog<<NEWL>>from netconf.nc_rpc.rpc import Rpc<<NEWL>>import netconf.nc_common.error as ncerror<<NEWL>><<NEWL>>log = structlog.get_logger()<<NEWL>><<NEWL>><<NEWL>>class CloseSession(Rpc):<<NEWL>>    def __init__(self, request, request_xml, grpc_client, session,<<NEWL>>                 capabilities):<<NEWL>>        super(CloseSession, self).__init__(request, request_xml, grpc_client,<<NEWL>>                                           session, capabilities)<<NEWL>>        self._validate_parameters()<<NEWL>><<NEWL>>    def execute(self):<<NEWL>>        log.info('close-session-request', session=self.session.session_id)<<NEWL>>        if self.rpc_response.is_error:<<NEWL>>            return self.rpc_response<<NEWL>><<NEWL>>        self.rpc_response.node = etree.Element(""ok"")<<NEWL>><<NEWL>>        # Set the close session flag<<NEWL>>        self.rpc_response.close_session = True<<NEWL>>        return self.rpc_response<<NEWL>><<NEWL>>    def _validate_parameters(self):<<NEWL>><<NEWL>>        if self.request:<<NEWL>>            try:<<NEWL>>                if self.request['command'] != 'close-session':<<NEWL>>                    self.rpc_response.is_error = True<<NEWL>>                    self.rpc_response.node = ncerror.BadMsg(self.request_xml)<<NEWL>>                    return<<NEWL>><<NEWL>>            except Exception as e:<<NEWL>>                self.rpc_response.is_error = True<<NEWL>>                self.rpc_response.node = ncerror.ServerException(<<NEWL>>                    self.request_xml)<<NEWL>>                return"
311	donghui	0	"#!/usr/bin/env python<<NEWL>>#<<NEWL>># Copyright 2017 the original author or authors.<<NEWL>>#<<NEWL>># Licensed under the Apache License, Version 2.0 (the ""License"");<<NEWL>># you may not use this file except in compliance with the License.<<NEWL>># You may obtain a copy of the License at<<NEWL>>#<<NEWL>>#      http://www.apache.org/licenses/LICENSE-2.0<<NEWL>>#<<NEWL>># Unless required by applicable law or agreed to in writing, software<<NEWL>># distributed under the License is distributed on an ""AS IS"" BASIS,<<NEWL>># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.<<NEWL>># See the License for the specific language governing permissions and<<NEWL>># limitations under the License.<<NEWL>>#<<NEWL>><<NEWL>>from lxml import etree<<NEWL>>import structlog<<NEWL>>from netconf.nc_rpc.rpc import Rpc<<NEWL>>import netconf.nc_common.error as ncerror<<NEWL>><<NEWL>>log = structlog.get_logger()<<NEWL>><<NEWL>><<NEWL>>class CloseSession(Rpc):<<NEWL>>    def __init__(self, request, request_xml, grpc_client, session,<<NEWL>>                 capabilities):<<NEWL>>        super(CloseSession, self).__init__(request, request_xml, grpc_client,<<NEWL>>                                           session, capabilities)<<NEWL>>        self._validate_parameters()<<NEWL>><<NEWL>>    def execute(self):<<NEWL>>        log.info('close-session-request', session=self.session.session_id)<<NEWL>>        if self.rpc_response.is_error:<<NEWL>>            return self.rpc_response<<NEWL>><<NEWL>>        self.rpc_response.node = etree.Element(""ok"")<<NEWL>><<NEWL>>        # Set the close session flag<<NEWL>>        self.rpc_response.close_session = True<<NEWL>>        return self.rpc_response<<NEWL>><<NEWL>>    def _validate_parameters(self):<<NEWL>><<NEWL>>        if self.request:<<NEWL>>            try:<<NEWL>>                if self.request['command'] != 'close-session':<<NEWL>>                    self.rpc_response.is_error = True<<NEWL>>                    self.rpc_response.node = ncerror.BadMsg(self.request_xml)<<NEWL>>                    return<<NEWL>><<NEWL>>            except Exception as e:<<NEWL>>                self.rpc_response.is_error = True<<NEWL>>                self.rpc_response.node = ncerror.ServerException(<<NEWL>>                    self.request_xml)<<NEWL>>                return"
251	jackson	4	"""""""<<NEWL>>Provide urlresolver functions that return fully qualified URLs or view names<<NEWL>>""""""<<NEWL>>from django.urls import NoReverseMatch<<NEWL>>from django.urls import reverse as django_reverse<<NEWL>>from django.utils.functional import lazy<<NEWL>><<NEWL>>from rest_framework.settings import api_settings<<NEWL>>from rest_framework.utils.urls import replace_query_param<<NEWL>><<NEWL>><<NEWL>>def preserve_builtin_query_params(url, request=None):<<NEWL>>    """"""<<NEWL>>    Given an incoming request, and an outgoing URL representation,<<NEWL>>    append the value of any built-in query parameters.<<NEWL>>    """"""<<NEWL>>    if request is None:<<NEWL>>        return url<<NEWL>><<NEWL>>    overrides = [<<NEWL>>        api_settings.URL_FORMAT_OVERRIDE,<<NEWL>>    ]<<NEWL>><<NEWL>>    for param in overrides:<<NEWL>>        if param and (param in request.GET):<<NEWL>>            value = request.GET[param]<<NEWL>>            url = replace_query_param(url, param, value)<<NEWL>><<NEWL>>    return url<<NEWL>><<NEWL>><<NEWL>>def reverse(viewname, args=None, kwargs=None, request=None, format=None, **extra):<<NEWL>>    """"""<<NEWL>>    If versioning is being used then we pass any `reverse` calls through<<NEWL>>    to the versioning scheme instance, so that the resulting URL<<NEWL>>    can be modified if needed.<<NEWL>>    """"""<<NEWL>>    scheme = getattr(request, 'versioning_scheme', None)<<NEWL>>    if scheme is not None:<<NEWL>>        try:<<NEWL>>            url = scheme.reverse(viewname, args, kwargs, request, format, **extra)<<NEWL>>        except NoReverseMatch:<<NEWL>>            # In case the versioning scheme reversal fails, fallback to the<<NEWL>>            # default implementation<<NEWL>>            url = _reverse(viewname, args, kwargs, request, format, **extra)<<NEWL>>    else:<<NEWL>>        url = _reverse(viewname, args, kwargs, request, format, **extra)<<NEWL>><<NEWL>>    return preserve_builtin_query_params(url, request)<<NEWL>><<NEWL>><<NEWL>>def _reverse(viewname, args=None, kwargs=None, request=None, format=None, **extra):<<NEWL>>    """"""<<NEWL>>    Same as `django.urls.reverse`, but optionally takes a request<<NEWL>>    and returns a fully qualified URL, using the request to get the base URL.<<NEWL>>    """"""<<NEWL>>    if format is not None:<<NEWL>>        kwargs = kwargs or {}<<NEWL>>        kwargs['format'] = format<<NEWL>>    url = django_reverse(viewname, args=args, kwargs=kwargs, **extra)<<NEWL>>    if request:<<NEWL>>        return request.build_absolute_uri(url)<<NEWL>>    return url<<NEWL>><<NEWL>><<NEWL>>reverse_lazy = lazy(reverse, str)"
251	donghui	2	"""""""<<NEWL>>Provide urlresolver functions that return fully qualified URLs or view names<<NEWL>>""""""<<NEWL>>from django.urls import NoReverseMatch<<NEWL>>from django.urls import reverse as django_reverse<<NEWL>>from django.utils.functional import lazy<<NEWL>><<NEWL>>from rest_framework.settings import api_settings<<NEWL>>from rest_framework.utils.urls import replace_query_param<<NEWL>><<NEWL>><<NEWL>>def preserve_builtin_query_params(url, request=None):<<NEWL>>    """"""<<NEWL>>    Given an incoming request, and an outgoing URL representation,<<NEWL>>    append the value of any built-in query parameters.<<NEWL>>    """"""<<NEWL>>    if request is None:<<NEWL>>        return url<<NEWL>><<NEWL>>    overrides = [<<NEWL>>        api_settings.URL_FORMAT_OVERRIDE,<<NEWL>>    ]<<NEWL>><<NEWL>>    for param in overrides:<<NEWL>>        if param and (param in request.GET):<<NEWL>>            value = request.GET[param]<<NEWL>>            url = replace_query_param(url, param, value)<<NEWL>><<NEWL>>    return url<<NEWL>><<NEWL>><<NEWL>>def reverse(viewname, args=None, kwargs=None, request=None, format=None, **extra):<<NEWL>>    """"""<<NEWL>>    If versioning is being used then we pass any `reverse` calls through<<NEWL>>    to the versioning scheme instance, so that the resulting URL<<NEWL>>    can be modified if needed.<<NEWL>>    """"""<<NEWL>>    scheme = getattr(request, 'versioning_scheme', None)<<NEWL>>    if scheme is not None:<<NEWL>>        try:<<NEWL>>            url = scheme.reverse(viewname, args, kwargs, request, format, **extra)<<NEWL>>        except NoReverseMatch:<<NEWL>>            # In case the versioning scheme reversal fails, fallback to the<<NEWL>>            # default implementation<<NEWL>>            url = _reverse(viewname, args, kwargs, request, format, **extra)<<NEWL>>    else:<<NEWL>>        url = _reverse(viewname, args, kwargs, request, format, **extra)<<NEWL>><<NEWL>>    return preserve_builtin_query_params(url, request)<<NEWL>><<NEWL>><<NEWL>>def _reverse(viewname, args=None, kwargs=None, request=None, format=None, **extra):<<NEWL>>    """"""<<NEWL>>    Same as `django.urls.reverse`, but optionally takes a request<<NEWL>>    and returns a fully qualified URL, using the request to get the base URL.<<NEWL>>    """"""<<NEWL>>    if format is not None:<<NEWL>>        kwargs = kwargs or {}<<NEWL>>        kwargs['format'] = format<<NEWL>>    url = django_reverse(viewname, args=args, kwargs=kwargs, **extra)<<NEWL>>    if request:<<NEWL>>        return request.build_absolute_uri(url)<<NEWL>>    return url<<NEWL>><<NEWL>><<NEWL>>reverse_lazy = lazy(reverse, str)"
331	jackson	3	"""""""<<NEWL>>    pygments.styles.native<<NEWL>>    ~~~~~~~~~~~~~~~~~~~~~~<<NEWL>><<NEWL>>    pygments version of my ""native"" vim theme.<<NEWL>><<NEWL>>    :copyright: Copyright 2006-2022 by the Pygments team, see AUTHORS.<<NEWL>>    :license: BSD, see LICENSE for details.<<NEWL>>""""""<<NEWL>><<NEWL>>from pygments.style import Style<<NEWL>>from pygments.token import Keyword, Name, Comment, String, Error, \<<NEWL>>     Number, Operator, Generic, Token, Whitespace<<NEWL>><<NEWL>><<NEWL>>class NativeStyle(Style):<<NEWL>>    """"""<<NEWL>>    Pygments version of the ""native"" vim theme.<<NEWL>>    """"""<<NEWL>><<NEWL>>    background_color = '#202020'<<NEWL>>    highlight_color = '#404040'<<NEWL>>    line_number_color = '#aaaaaa'<<NEWL>><<NEWL>>    styles = {<<NEWL>>        Token:              '#d0d0d0',<<NEWL>>        Whitespace:         '#666666',<<NEWL>><<NEWL>>        Comment:            'italic #ababab',<<NEWL>>        Comment.Preproc:    'noitalic bold #cd2828',<<NEWL>>        Comment.Special:    'noitalic bold #e50808 bg:#520000',<<NEWL>><<NEWL>>        Keyword:            'bold #6ebf26',<<NEWL>>        Keyword.Pseudo:     'nobold',<<NEWL>>        Operator.Word:      'bold #6ebf26',<<NEWL>><<NEWL>>        String:             '#ed9d13',<<NEWL>>        String.Other:       '#ffa500',<<NEWL>><<NEWL>>        Number:             '#51b2fd',<<NEWL>><<NEWL>>        Name.Builtin:       '#2fbccd',<<NEWL>>        Name.Variable:      '#40ffff',<<NEWL>>        Name.Constant:      '#40ffff',<<NEWL>>        Name.Class:         'underline #71adff',<<NEWL>>        Name.Function:      '#71adff',<<NEWL>>        Name.Namespace:     'underline #71adff',<<NEWL>>        Name.Exception:     '#bbbbbb',<<NEWL>>        Name.Tag:           'bold #6ebf26',<<NEWL>>        Name.Attribute:     '#bbbbbb',<<NEWL>>        Name.Decorator:     '#ffa500',<<NEWL>><<NEWL>>        Generic.Heading:    'bold #ffffff',<<NEWL>>        Generic.Subheading: 'underline #ffffff',<<NEWL>>        Generic.Deleted:    '#d22323',<<NEWL>>        Generic.Inserted:   '#589819',<<NEWL>>        Generic.Error:      '#d22323',<<NEWL>>        Generic.Emph:       'italic',<<NEWL>>        Generic.Strong:     'bold',<<NEWL>>        Generic.Prompt:     '#aaaaaa',<<NEWL>>        Generic.Output:     '#cccccc',<<NEWL>>        Generic.Traceback:  '#d22323',<<NEWL>><<NEWL>>        Error:              'bg:#e3d2d2 #a61717'<<NEWL>>    }"
331	donghui	1	"""""""<<NEWL>>    pygments.styles.native<<NEWL>>    ~~~~~~~~~~~~~~~~~~~~~~<<NEWL>><<NEWL>>    pygments version of my ""native"" vim theme.<<NEWL>><<NEWL>>    :copyright: Copyright 2006-2022 by the Pygments team, see AUTHORS.<<NEWL>>    :license: BSD, see LICENSE for details.<<NEWL>>""""""<<NEWL>><<NEWL>>from pygments.style import Style<<NEWL>>from pygments.token import Keyword, Name, Comment, String, Error, \<<NEWL>>     Number, Operator, Generic, Token, Whitespace<<NEWL>><<NEWL>><<NEWL>>class NativeStyle(Style):<<NEWL>>    """"""<<NEWL>>    Pygments version of the ""native"" vim theme.<<NEWL>>    """"""<<NEWL>><<NEWL>>    background_color = '#202020'<<NEWL>>    highlight_color = '#404040'<<NEWL>>    line_number_color = '#aaaaaa'<<NEWL>><<NEWL>>    styles = {<<NEWL>>        Token:              '#d0d0d0',<<NEWL>>        Whitespace:         '#666666',<<NEWL>><<NEWL>>        Comment:            'italic #ababab',<<NEWL>>        Comment.Preproc:    'noitalic bold #cd2828',<<NEWL>>        Comment.Special:    'noitalic bold #e50808 bg:#520000',<<NEWL>><<NEWL>>        Keyword:            'bold #6ebf26',<<NEWL>>        Keyword.Pseudo:     'nobold',<<NEWL>>        Operator.Word:      'bold #6ebf26',<<NEWL>><<NEWL>>        String:             '#ed9d13',<<NEWL>>        String.Other:       '#ffa500',<<NEWL>><<NEWL>>        Number:             '#51b2fd',<<NEWL>><<NEWL>>        Name.Builtin:       '#2fbccd',<<NEWL>>        Name.Variable:      '#40ffff',<<NEWL>>        Name.Constant:      '#40ffff',<<NEWL>>        Name.Class:         'underline #71adff',<<NEWL>>        Name.Function:      '#71adff',<<NEWL>>        Name.Namespace:     'underline #71adff',<<NEWL>>        Name.Exception:     '#bbbbbb',<<NEWL>>        Name.Tag:           'bold #6ebf26',<<NEWL>>        Name.Attribute:     '#bbbbbb',<<NEWL>>        Name.Decorator:     '#ffa500',<<NEWL>><<NEWL>>        Generic.Heading:    'bold #ffffff',<<NEWL>>        Generic.Subheading: 'underline #ffffff',<<NEWL>>        Generic.Deleted:    '#d22323',<<NEWL>>        Generic.Inserted:   '#589819',<<NEWL>>        Generic.Error:      '#d22323',<<NEWL>>        Generic.Emph:       'italic',<<NEWL>>        Generic.Strong:     'bold',<<NEWL>>        Generic.Prompt:     '#aaaaaa',<<NEWL>>        Generic.Output:     '#cccccc',<<NEWL>>        Generic.Traceback:  '#d22323',<<NEWL>><<NEWL>>        Error:              'bg:#e3d2d2 #a61717'<<NEWL>>    }"
271	jackson	2	"import _plotly_utils.basevalidators<<NEWL>><<NEWL>><<NEWL>>class InsidetextfontValidator(_plotly_utils.basevalidators.CompoundValidator):<<NEWL>>    def __init__(<<NEWL>>        self, plotly_name=""insidetextfont"", parent_name=""funnelarea"", **kwargs<<NEWL>>    ):<<NEWL>>        super(InsidetextfontValidator, self).__init__(<<NEWL>>            plotly_name=plotly_name,<<NEWL>>            parent_name=parent_name,<<NEWL>>            data_class_str=kwargs.pop(""data_class_str"", ""Insidetextfont""),<<NEWL>>            data_docs=kwargs.pop(<<NEWL>>                ""data_docs"",<<NEWL>>                """"""<<NEWL>>            color<<NEWL>><<NEWL>>            colorsrc<<NEWL>>                Sets the source reference on Chart Studio Cloud<<NEWL>>                for `color`.<<NEWL>>            family<<NEWL>>                HTML font family - the typeface that will be<<NEWL>>                applied by the web browser. The web browser<<NEWL>>                will only be able to apply a font if it is<<NEWL>>                available on the system which it operates.<<NEWL>>                Provide multiple font families, separated by<<NEWL>>                commas, to indicate the preference in which to<<NEWL>>                apply fonts if they aren't available on the<<NEWL>>                system. The Chart Studio Cloud (at<<NEWL>>                https://chart-studio.plotly.com or on-premise)<<NEWL>>                generates images on a server, where only a<<NEWL>>                select number of fonts are installed and<<NEWL>>                supported. These include ""Arial"", ""Balto"",<<NEWL>>                ""Courier New"", ""Droid Sans"",, ""Droid Serif"",<<NEWL>>                ""Droid Sans Mono"", ""Gravitas One"", ""Old<<NEWL>>                Standard TT"", ""Open Sans"", ""Overpass"", ""PT Sans<<NEWL>>                Narrow"", ""Raleway"", ""Times New Roman"".<<NEWL>>            familysrc<<NEWL>>                Sets the source reference on Chart Studio Cloud<<NEWL>>                for `family`.<<NEWL>>            size<<NEWL>><<NEWL>>            sizesrc<<NEWL>>                Sets the source reference on Chart Studio Cloud<<NEWL>>                for `size`.<<NEWL>>"""""",<<NEWL>>            ),<<NEWL>>            **kwargs,<<NEWL>>        )"
271	donghui	1	"import _plotly_utils.basevalidators<<NEWL>><<NEWL>><<NEWL>>class InsidetextfontValidator(_plotly_utils.basevalidators.CompoundValidator):<<NEWL>>    def __init__(<<NEWL>>        self, plotly_name=""insidetextfont"", parent_name=""funnelarea"", **kwargs<<NEWL>>    ):<<NEWL>>        super(InsidetextfontValidator, self).__init__(<<NEWL>>            plotly_name=plotly_name,<<NEWL>>            parent_name=parent_name,<<NEWL>>            data_class_str=kwargs.pop(""data_class_str"", ""Insidetextfont""),<<NEWL>>            data_docs=kwargs.pop(<<NEWL>>                ""data_docs"",<<NEWL>>                """"""<<NEWL>>            color<<NEWL>><<NEWL>>            colorsrc<<NEWL>>                Sets the source reference on Chart Studio Cloud<<NEWL>>                for `color`.<<NEWL>>            family<<NEWL>>                HTML font family - the typeface that will be<<NEWL>>                applied by the web browser. The web browser<<NEWL>>                will only be able to apply a font if it is<<NEWL>>                available on the system which it operates.<<NEWL>>                Provide multiple font families, separated by<<NEWL>>                commas, to indicate the preference in which to<<NEWL>>                apply fonts if they aren't available on the<<NEWL>>                system. The Chart Studio Cloud (at<<NEWL>>                https://chart-studio.plotly.com or on-premise)<<NEWL>>                generates images on a server, where only a<<NEWL>>                select number of fonts are installed and<<NEWL>>                supported. These include ""Arial"", ""Balto"",<<NEWL>>                ""Courier New"", ""Droid Sans"",, ""Droid Serif"",<<NEWL>>                ""Droid Sans Mono"", ""Gravitas One"", ""Old<<NEWL>>                Standard TT"", ""Open Sans"", ""Overpass"", ""PT Sans<<NEWL>>                Narrow"", ""Raleway"", ""Times New Roman"".<<NEWL>>            familysrc<<NEWL>>                Sets the source reference on Chart Studio Cloud<<NEWL>>                for `family`.<<NEWL>>            size<<NEWL>><<NEWL>>            sizesrc<<NEWL>>                Sets the source reference on Chart Studio Cloud<<NEWL>>                for `size`.<<NEWL>>"""""",<<NEWL>>            ),<<NEWL>>            **kwargs,<<NEWL>>        )"
360	jackson	2	"""""""Manager to read and modify frontend config data in JSON files.<<NEWL>>""""""<<NEWL>># Copyright (c) Jupyter Development Team.<<NEWL>># Distributed under the terms of the Modified BSD License.<<NEWL>><<NEWL>>import os.path<<NEWL>><<NEWL>>from notebook.config_manager import BaseJSONConfigManager, recursive_update<<NEWL>>from jupyter_core.paths import jupyter_config_dir, jupyter_config_path<<NEWL>>from traitlets import Unicode, Instance, List, observe, default<<NEWL>>from traitlets.config import LoggingConfigurable<<NEWL>><<NEWL>><<NEWL>>class ConfigManager(LoggingConfigurable):<<NEWL>>    """"""Config Manager used for storing notebook frontend config""""""<<NEWL>><<NEWL>>    # Public API<<NEWL>><<NEWL>>    def get(self, section_name):<<NEWL>>        """"""Get the config from all config sections.""""""<<NEWL>>        config = {}<<NEWL>>        # step through back to front, to ensure front of the list is top priority<<NEWL>>        for p in self.read_config_path[::-1]:<<NEWL>>            cm = BaseJSONConfigManager(config_dir=p)<<NEWL>>            recursive_update(config, cm.get(section_name))<<NEWL>>        return config<<NEWL>><<NEWL>>    def set(self, section_name, data):<<NEWL>>        """"""Set the config only to the user's config.""""""<<NEWL>>        return self.write_config_manager.set(section_name, data)<<NEWL>><<NEWL>>    def update(self, section_name, new_data):<<NEWL>>        """"""Update the config only to the user's config.""""""<<NEWL>>        return self.write_config_manager.update(section_name, new_data)<<NEWL>><<NEWL>>    # Private API<<NEWL>><<NEWL>>    read_config_path = List(Unicode())<<NEWL>><<NEWL>>    @default('read_config_path')<<NEWL>>    def _default_read_config_path(self):<<NEWL>>        return [os.path.join(p, 'nbconfig') for p in jupyter_config_path()]<<NEWL>><<NEWL>>    write_config_dir = Unicode()<<NEWL>><<NEWL>>    @default('write_config_dir')<<NEWL>>    def _default_write_config_dir(self):<<NEWL>>        return os.path.join(jupyter_config_dir(), 'nbconfig')<<NEWL>><<NEWL>>    write_config_manager = Instance(BaseJSONConfigManager)<<NEWL>><<NEWL>>    @default('write_config_manager')<<NEWL>>    def _default_write_config_manager(self):<<NEWL>>        return BaseJSONConfigManager(config_dir=self.write_config_dir)<<NEWL>><<NEWL>>    @observe('write_config_dir')<<NEWL>>    def _update_write_config_dir(self, change):<<NEWL>>        self.write_config_manager = BaseJSONConfigManager(config_dir=self.write_config_dir)"
360	donghui	2	"""""""Manager to read and modify frontend config data in JSON files.<<NEWL>>""""""<<NEWL>># Copyright (c) Jupyter Development Team.<<NEWL>># Distributed under the terms of the Modified BSD License.<<NEWL>><<NEWL>>import os.path<<NEWL>><<NEWL>>from notebook.config_manager import BaseJSONConfigManager, recursive_update<<NEWL>>from jupyter_core.paths import jupyter_config_dir, jupyter_config_path<<NEWL>>from traitlets import Unicode, Instance, List, observe, default<<NEWL>>from traitlets.config import LoggingConfigurable<<NEWL>><<NEWL>><<NEWL>>class ConfigManager(LoggingConfigurable):<<NEWL>>    """"""Config Manager used for storing notebook frontend config""""""<<NEWL>><<NEWL>>    # Public API<<NEWL>><<NEWL>>    def get(self, section_name):<<NEWL>>        """"""Get the config from all config sections.""""""<<NEWL>>        config = {}<<NEWL>>        # step through back to front, to ensure front of the list is top priority<<NEWL>>        for p in self.read_config_path[::-1]:<<NEWL>>            cm = BaseJSONConfigManager(config_dir=p)<<NEWL>>            recursive_update(config, cm.get(section_name))<<NEWL>>        return config<<NEWL>><<NEWL>>    def set(self, section_name, data):<<NEWL>>        """"""Set the config only to the user's config.""""""<<NEWL>>        return self.write_config_manager.set(section_name, data)<<NEWL>><<NEWL>>    def update(self, section_name, new_data):<<NEWL>>        """"""Update the config only to the user's config.""""""<<NEWL>>        return self.write_config_manager.update(section_name, new_data)<<NEWL>><<NEWL>>    # Private API<<NEWL>><<NEWL>>    read_config_path = List(Unicode())<<NEWL>><<NEWL>>    @default('read_config_path')<<NEWL>>    def _default_read_config_path(self):<<NEWL>>        return [os.path.join(p, 'nbconfig') for p in jupyter_config_path()]<<NEWL>><<NEWL>>    write_config_dir = Unicode()<<NEWL>><<NEWL>>    @default('write_config_dir')<<NEWL>>    def _default_write_config_dir(self):<<NEWL>>        return os.path.join(jupyter_config_dir(), 'nbconfig')<<NEWL>><<NEWL>>    write_config_manager = Instance(BaseJSONConfigManager)<<NEWL>><<NEWL>>    @default('write_config_manager')<<NEWL>>    def _default_write_config_manager(self):<<NEWL>>        return BaseJSONConfigManager(config_dir=self.write_config_dir)<<NEWL>><<NEWL>>    @observe('write_config_dir')<<NEWL>>    def _update_write_config_dir(self, change):<<NEWL>>        self.write_config_manager = BaseJSONConfigManager(config_dir=self.write_config_dir)"
393	jackson	3	"# Copyright 2017-present Open Networking Foundation<<NEWL>>#<<NEWL>># Licensed under the Apache License, Version 2.0 (the ""License"");<<NEWL>># you may not use this file except in compliance with the License.<<NEWL>># You may obtain a copy of the License at<<NEWL>>#<<NEWL>># http://www.apache.org/licenses/LICENSE-2.0<<NEWL>>#<<NEWL>># Unless required by applicable law or agreed to in writing, software<<NEWL>># distributed under the License is distributed on an ""AS IS"" BASIS,<<NEWL>># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.<<NEWL>># See the License for the specific language governing permissions and<<NEWL>># limitations under the License.<<NEWL>># Copyright (c) 2008 The Board of Trustees of The Leland Stanford Junior University<<NEWL>># Copyright (c) 2011, 2012 Open Networking Foundation<<NEWL>># Copyright (c) 2012, 2013 Big Switch Networks, Inc.<<NEWL>># See the file LICENSE.pyloxi which should have been included in the source distribution<<NEWL>># Automatically generated by LOXI from template toplevel_init.py<<NEWL>># Do not modify<<NEWL>><<NEWL>>version_names = {<<NEWL>>    1: ""1.0"",<<NEWL>>    2: ""1.1"",<<NEWL>>    3: ""1.2"",<<NEWL>>    4: ""1.3"",<<NEWL>>    5: ""1.4"",<<NEWL>>}<<NEWL>><<NEWL>>def protocol(ver):<<NEWL>>    """"""<<NEWL>>    Import and return the protocol module for the given wire version.<<NEWL>>    """"""<<NEWL>>    if ver == 1:<<NEWL>>        import of10<<NEWL>>        return of10<<NEWL>><<NEWL>>    if ver == 2:<<NEWL>>        import of11<<NEWL>>        return of11<<NEWL>><<NEWL>>    if ver == 3:<<NEWL>>        import of12<<NEWL>>        return of12<<NEWL>><<NEWL>>    if ver == 4:<<NEWL>>        import of13<<NEWL>>        return of13<<NEWL>><<NEWL>>    if ver == 5:<<NEWL>>        import of14<<NEWL>>        return of14<<NEWL>><<NEWL>>    raise ValueError<<NEWL>><<NEWL>>class ProtocolError(Exception):<<NEWL>>    """"""<<NEWL>>    Raised when failing to deserialize an invalid OpenFlow message.<<NEWL>>    """"""<<NEWL>>    pass<<NEWL>><<NEWL>>class Unimplemented(Exception):<<NEWL>>    """"""<<NEWL>>    Raised when an OpenFlow feature is not yet implemented in PyLoxi.<<NEWL>>    """"""<<NEWL>>    pass<<NEWL>><<NEWL>>def unimplemented(msg):<<NEWL>>    raise Unimplemented(msg)<<NEWL>><<NEWL>>class OFObject(object):<<NEWL>>    """"""<<NEWL>>    Superclass of all OpenFlow classes<<NEWL>>    """"""<<NEWL>>    def __init__(self, *args):<<NEWL>>        raise NotImplementedError(""cannot instantiate abstract class"")<<NEWL>><<NEWL>>    def __ne__(self, other):<<NEWL>>        return not self.__eq__(other)<<NEWL>><<NEWL>>    def show(self):<<NEWL>>        import loxi.pp<<NEWL>>        return loxi.pp.pp(self)"
393	donghui	2	"# Copyright 2017-present Open Networking Foundation<<NEWL>>#<<NEWL>># Licensed under the Apache License, Version 2.0 (the ""License"");<<NEWL>># you may not use this file except in compliance with the License.<<NEWL>># You may obtain a copy of the License at<<NEWL>>#<<NEWL>># http://www.apache.org/licenses/LICENSE-2.0<<NEWL>>#<<NEWL>># Unless required by applicable law or agreed to in writing, software<<NEWL>># distributed under the License is distributed on an ""AS IS"" BASIS,<<NEWL>># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.<<NEWL>># See the License for the specific language governing permissions and<<NEWL>># limitations under the License.<<NEWL>># Copyright (c) 2008 The Board of Trustees of The Leland Stanford Junior University<<NEWL>># Copyright (c) 2011, 2012 Open Networking Foundation<<NEWL>># Copyright (c) 2012, 2013 Big Switch Networks, Inc.<<NEWL>># See the file LICENSE.pyloxi which should have been included in the source distribution<<NEWL>># Automatically generated by LOXI from template toplevel_init.py<<NEWL>># Do not modify<<NEWL>><<NEWL>>version_names = {<<NEWL>>    1: ""1.0"",<<NEWL>>    2: ""1.1"",<<NEWL>>    3: ""1.2"",<<NEWL>>    4: ""1.3"",<<NEWL>>    5: ""1.4"",<<NEWL>>}<<NEWL>><<NEWL>>def protocol(ver):<<NEWL>>    """"""<<NEWL>>    Import and return the protocol module for the given wire version.<<NEWL>>    """"""<<NEWL>>    if ver == 1:<<NEWL>>        import of10<<NEWL>>        return of10<<NEWL>><<NEWL>>    if ver == 2:<<NEWL>>        import of11<<NEWL>>        return of11<<NEWL>><<NEWL>>    if ver == 3:<<NEWL>>        import of12<<NEWL>>        return of12<<NEWL>><<NEWL>>    if ver == 4:<<NEWL>>        import of13<<NEWL>>        return of13<<NEWL>><<NEWL>>    if ver == 5:<<NEWL>>        import of14<<NEWL>>        return of14<<NEWL>><<NEWL>>    raise ValueError<<NEWL>><<NEWL>>class ProtocolError(Exception):<<NEWL>>    """"""<<NEWL>>    Raised when failing to deserialize an invalid OpenFlow message.<<NEWL>>    """"""<<NEWL>>    pass<<NEWL>><<NEWL>>class Unimplemented(Exception):<<NEWL>>    """"""<<NEWL>>    Raised when an OpenFlow feature is not yet implemented in PyLoxi.<<NEWL>>    """"""<<NEWL>>    pass<<NEWL>><<NEWL>>def unimplemented(msg):<<NEWL>>    raise Unimplemented(msg)<<NEWL>><<NEWL>>class OFObject(object):<<NEWL>>    """"""<<NEWL>>    Superclass of all OpenFlow classes<<NEWL>>    """"""<<NEWL>>    def __init__(self, *args):<<NEWL>>        raise NotImplementedError(""cannot instantiate abstract class"")<<NEWL>><<NEWL>>    def __ne__(self, other):<<NEWL>>        return not self.__eq__(other)<<NEWL>><<NEWL>>    def show(self):<<NEWL>>        import loxi.pp<<NEWL>>        return loxi.pp.pp(self)"
282	jackson	0	"#!/usr/bin/env python<<NEWL>><<NEWL>># Copyright 2020 Google LLC<<NEWL>>#<<NEWL>># Licensed under the Apache License, Version 2.0 (the ""License"");<<NEWL>># you may not use this file except in compliance with the License.<<NEWL>># You may obtain a copy of the License at<<NEWL>>#<<NEWL>>#     http://www.apache.org/licenses/LICENSE-2.0<<NEWL>>#<<NEWL>># Unless required by applicable law or agreed to in writing, software<<NEWL>># distributed under the License is distributed on an ""AS IS"" BASIS,<<NEWL>># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.<<NEWL>># See the License for the specific language governing permissions and<<NEWL>># limitations under the License.<<NEWL>><<NEWL>>import os<<NEWL>><<NEWL>>from google.cloud.automl_v1beta1 import Model<<NEWL>><<NEWL>>import pytest<<NEWL>><<NEWL>>import automl_tables_model<<NEWL>>import automl_tables_predict<<NEWL>>import model_test<<NEWL>><<NEWL>><<NEWL>>PROJECT = os.environ[""GOOGLE_CLOUD_PROJECT""]<<NEWL>>REGION = ""us-central1""<<NEWL>>STATIC_MODEL = model_test.STATIC_MODEL<<NEWL>>GCS_INPUT = ""gs://{}-automl-tables-test/bank-marketing.csv"".format(PROJECT)<<NEWL>>GCS_OUTPUT = ""gs://{}-automl-tables-test/TABLE_TEST_OUTPUT/"".format(PROJECT)<<NEWL>>BQ_INPUT = ""bq://{}.automl_test.bank_marketing"".format(PROJECT)<<NEWL>>BQ_OUTPUT = ""bq://{}"".format(PROJECT)<<NEWL>>PARAMS = {}<<NEWL>><<NEWL>><<NEWL>>@pytest.mark.slow<<NEWL>>def test_batch_predict(capsys):<<NEWL>>    ensure_model_online()<<NEWL>><<NEWL>>    automl_tables_predict.batch_predict(<<NEWL>>        PROJECT, REGION, STATIC_MODEL, GCS_INPUT, GCS_OUTPUT, PARAMS<<NEWL>>    )<<NEWL>>    out, _ = capsys.readouterr()<<NEWL>>    assert ""Batch prediction complete"" in out<<NEWL>><<NEWL>><<NEWL>>@pytest.mark.slow<<NEWL>>def test_batch_predict_bq(capsys):<<NEWL>>    ensure_model_online()<<NEWL>>    automl_tables_predict.batch_predict_bq(<<NEWL>>        PROJECT, REGION, STATIC_MODEL, BQ_INPUT, BQ_OUTPUT, PARAMS<<NEWL>>    )<<NEWL>>    out, _ = capsys.readouterr()<<NEWL>>    assert ""Batch prediction complete"" in out<<NEWL>><<NEWL>><<NEWL>>def ensure_model_online():<<NEWL>>    model = model_test.ensure_model_ready()<<NEWL>>    if model.deployment_state != Model.DeploymentState.DEPLOYED:<<NEWL>>        automl_tables_model.deploy_model(PROJECT, REGION, model.display_name)<<NEWL>><<NEWL>>    return automl_tables_model.get_model(PROJECT, REGION, model.display_name)"
282	donghui	0	"#!/usr/bin/env python<<NEWL>><<NEWL>># Copyright 2020 Google LLC<<NEWL>>#<<NEWL>># Licensed under the Apache License, Version 2.0 (the ""License"");<<NEWL>># you may not use this file except in compliance with the License.<<NEWL>># You may obtain a copy of the License at<<NEWL>>#<<NEWL>>#     http://www.apache.org/licenses/LICENSE-2.0<<NEWL>>#<<NEWL>># Unless required by applicable law or agreed to in writing, software<<NEWL>># distributed under the License is distributed on an ""AS IS"" BASIS,<<NEWL>># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.<<NEWL>># See the License for the specific language governing permissions and<<NEWL>># limitations under the License.<<NEWL>><<NEWL>>import os<<NEWL>><<NEWL>>from google.cloud.automl_v1beta1 import Model<<NEWL>><<NEWL>>import pytest<<NEWL>><<NEWL>>import automl_tables_model<<NEWL>>import automl_tables_predict<<NEWL>>import model_test<<NEWL>><<NEWL>><<NEWL>>PROJECT = os.environ[""GOOGLE_CLOUD_PROJECT""]<<NEWL>>REGION = ""us-central1""<<NEWL>>STATIC_MODEL = model_test.STATIC_MODEL<<NEWL>>GCS_INPUT = ""gs://{}-automl-tables-test/bank-marketing.csv"".format(PROJECT)<<NEWL>>GCS_OUTPUT = ""gs://{}-automl-tables-test/TABLE_TEST_OUTPUT/"".format(PROJECT)<<NEWL>>BQ_INPUT = ""bq://{}.automl_test.bank_marketing"".format(PROJECT)<<NEWL>>BQ_OUTPUT = ""bq://{}"".format(PROJECT)<<NEWL>>PARAMS = {}<<NEWL>><<NEWL>><<NEWL>>@pytest.mark.slow<<NEWL>>def test_batch_predict(capsys):<<NEWL>>    ensure_model_online()<<NEWL>><<NEWL>>    automl_tables_predict.batch_predict(<<NEWL>>        PROJECT, REGION, STATIC_MODEL, GCS_INPUT, GCS_OUTPUT, PARAMS<<NEWL>>    )<<NEWL>>    out, _ = capsys.readouterr()<<NEWL>>    assert ""Batch prediction complete"" in out<<NEWL>><<NEWL>><<NEWL>>@pytest.mark.slow<<NEWL>>def test_batch_predict_bq(capsys):<<NEWL>>    ensure_model_online()<<NEWL>>    automl_tables_predict.batch_predict_bq(<<NEWL>>        PROJECT, REGION, STATIC_MODEL, BQ_INPUT, BQ_OUTPUT, PARAMS<<NEWL>>    )<<NEWL>>    out, _ = capsys.readouterr()<<NEWL>>    assert ""Batch prediction complete"" in out<<NEWL>><<NEWL>><<NEWL>>def ensure_model_online():<<NEWL>>    model = model_test.ensure_model_ready()<<NEWL>>    if model.deployment_state != Model.DeploymentState.DEPLOYED:<<NEWL>>        automl_tables_model.deploy_model(PROJECT, REGION, model.display_name)<<NEWL>><<NEWL>>    return automl_tables_model.get_model(PROJECT, REGION, model.display_name)"
354	jackson	1	"import builtins<<NEWL>>import logging<<NEWL>>import signal<<NEWL>>import threading<<NEWL>>import traceback<<NEWL>>import warnings<<NEWL>><<NEWL>>import trio<<NEWL>><<NEWL>><<NEWL>>class TrioRunner:<<NEWL>>    def __init__(self):<<NEWL>>        self._cell_cancel_scope = None<<NEWL>>        self._trio_token = None<<NEWL>><<NEWL>>    def initialize(self, kernel, io_loop):<<NEWL>>        kernel.shell.set_trio_runner(self)<<NEWL>>        kernel.shell.run_line_magic(""autoawait"", ""trio"")<<NEWL>>        kernel.shell.magics_manager.magics[""line""][""autoawait""] = lambda _: warnings.warn(<<NEWL>>            ""Autoawait isn't allowed in Trio background loop mode.""<<NEWL>>        )<<NEWL>>        bg_thread = threading.Thread(target=io_loop.start, daemon=True, name=""TornadoBackground"")<<NEWL>>        bg_thread.start()<<NEWL>><<NEWL>>    def interrupt(self, signum, frame):<<NEWL>>        if self._cell_cancel_scope:<<NEWL>>            self._cell_cancel_scope.cancel()<<NEWL>>        else:<<NEWL>>            raise Exception(""Kernel interrupted but no cell is running"")<<NEWL>><<NEWL>>    def run(self):<<NEWL>>        old_sig = signal.signal(signal.SIGINT, self.interrupt)<<NEWL>><<NEWL>>        def log_nursery_exc(exc):<<NEWL>>            exc = ""\n"".join(traceback.format_exception(type(exc), exc, exc.__traceback__))<<NEWL>>            logging.error(""An exception occurred in a global nursery task.\n%s"", exc)<<NEWL>><<NEWL>>        async def trio_main():<<NEWL>>            self._trio_token = trio.lowlevel.current_trio_token()<<NEWL>>            async with trio.open_nursery() as nursery:<<NEWL>>                # TODO This hack prevents the nursery from cancelling all child<<NEWL>>                # tasks when an uncaught exception occurs, but it's ugly.<<NEWL>>                nursery._add_exc = log_nursery_exc<<NEWL>>                builtins.GLOBAL_NURSERY = nursery  # type:ignore[attr-defined]<<NEWL>>                await trio.sleep_forever()<<NEWL>><<NEWL>>        trio.run(trio_main)<<NEWL>>        signal.signal(signal.SIGINT, old_sig)<<NEWL>><<NEWL>>    def __call__(self, async_fn):<<NEWL>>        async def loc(coro):<<NEWL>>            self._cell_cancel_scope = trio.CancelScope()<<NEWL>>            with self._cell_cancel_scope:<<NEWL>>                return await coro<<NEWL>>            self._cell_cancel_scope = None<<NEWL>><<NEWL>>        return trio.from_thread.run(loc, async_fn, trio_token=self._trio_token)"
354	donghui	1	"import builtins<<NEWL>>import logging<<NEWL>>import signal<<NEWL>>import threading<<NEWL>>import traceback<<NEWL>>import warnings<<NEWL>><<NEWL>>import trio<<NEWL>><<NEWL>><<NEWL>>class TrioRunner:<<NEWL>>    def __init__(self):<<NEWL>>        self._cell_cancel_scope = None<<NEWL>>        self._trio_token = None<<NEWL>><<NEWL>>    def initialize(self, kernel, io_loop):<<NEWL>>        kernel.shell.set_trio_runner(self)<<NEWL>>        kernel.shell.run_line_magic(""autoawait"", ""trio"")<<NEWL>>        kernel.shell.magics_manager.magics[""line""][""autoawait""] = lambda _: warnings.warn(<<NEWL>>            ""Autoawait isn't allowed in Trio background loop mode.""<<NEWL>>        )<<NEWL>>        bg_thread = threading.Thread(target=io_loop.start, daemon=True, name=""TornadoBackground"")<<NEWL>>        bg_thread.start()<<NEWL>><<NEWL>>    def interrupt(self, signum, frame):<<NEWL>>        if self._cell_cancel_scope:<<NEWL>>            self._cell_cancel_scope.cancel()<<NEWL>>        else:<<NEWL>>            raise Exception(""Kernel interrupted but no cell is running"")<<NEWL>><<NEWL>>    def run(self):<<NEWL>>        old_sig = signal.signal(signal.SIGINT, self.interrupt)<<NEWL>><<NEWL>>        def log_nursery_exc(exc):<<NEWL>>            exc = ""\n"".join(traceback.format_exception(type(exc), exc, exc.__traceback__))<<NEWL>>            logging.error(""An exception occurred in a global nursery task.\n%s"", exc)<<NEWL>><<NEWL>>        async def trio_main():<<NEWL>>            self._trio_token = trio.lowlevel.current_trio_token()<<NEWL>>            async with trio.open_nursery() as nursery:<<NEWL>>                # TODO This hack prevents the nursery from cancelling all child<<NEWL>>                # tasks when an uncaught exception occurs, but it's ugly.<<NEWL>>                nursery._add_exc = log_nursery_exc<<NEWL>>                builtins.GLOBAL_NURSERY = nursery  # type:ignore[attr-defined]<<NEWL>>                await trio.sleep_forever()<<NEWL>><<NEWL>>        trio.run(trio_main)<<NEWL>>        signal.signal(signal.SIGINT, old_sig)<<NEWL>><<NEWL>>    def __call__(self, async_fn):<<NEWL>>        async def loc(coro):<<NEWL>>            self._cell_cancel_scope = trio.CancelScope()<<NEWL>>            with self._cell_cancel_scope:<<NEWL>>                return await coro<<NEWL>>            self._cell_cancel_scope = None<<NEWL>><<NEWL>>        return trio.from_thread.run(loc, async_fn, trio_token=self._trio_token)"
315	jackson	0	"# This file is dual licensed under the terms of the Apache License, Version<<NEWL>># 2.0, and the BSD License. See the LICENSE file in the root of this repository<<NEWL>># for complete details.<<NEWL>><<NEWL>>import typing<<NEWL>><<NEWL>>from cryptography import utils<<NEWL>>from cryptography.exceptions import (<<NEWL>>    AlreadyFinalized,<<NEWL>>    UnsupportedAlgorithm,<<NEWL>>    _Reasons,<<NEWL>>)<<NEWL>>from cryptography.hazmat.backends.openssl.poly1305 import _Poly1305Context<<NEWL>><<NEWL>><<NEWL>>class Poly1305:<<NEWL>>    _ctx: typing.Optional[_Poly1305Context]<<NEWL>><<NEWL>>    def __init__(self, key: bytes):<<NEWL>>        from cryptography.hazmat.backends.openssl.backend import backend<<NEWL>><<NEWL>>        if not backend.poly1305_supported():<<NEWL>>            raise UnsupportedAlgorithm(<<NEWL>>                ""poly1305 is not supported by this version of OpenSSL."",<<NEWL>>                _Reasons.UNSUPPORTED_MAC,<<NEWL>>            )<<NEWL>>        self._ctx = backend.create_poly1305_ctx(key)<<NEWL>><<NEWL>>    def update(self, data: bytes) -> None:<<NEWL>>        if self._ctx is None:<<NEWL>>            raise AlreadyFinalized(""Context was already finalized."")<<NEWL>>        utils._check_byteslike(""data"", data)<<NEWL>>        self._ctx.update(data)<<NEWL>><<NEWL>>    def finalize(self) -> bytes:<<NEWL>>        if self._ctx is None:<<NEWL>>            raise AlreadyFinalized(""Context was already finalized."")<<NEWL>>        mac = self._ctx.finalize()<<NEWL>>        self._ctx = None<<NEWL>>        return mac<<NEWL>><<NEWL>>    def verify(self, tag: bytes) -> None:<<NEWL>>        utils._check_bytes(""tag"", tag)<<NEWL>>        if self._ctx is None:<<NEWL>>            raise AlreadyFinalized(""Context was already finalized."")<<NEWL>><<NEWL>>        ctx, self._ctx = self._ctx, None<<NEWL>>        ctx.verify(tag)<<NEWL>><<NEWL>>    @classmethod<<NEWL>>    def generate_tag(cls, key: bytes, data: bytes) -> bytes:<<NEWL>>        p = Poly1305(key)<<NEWL>>        p.update(data)<<NEWL>>        return p.finalize()<<NEWL>><<NEWL>>    @classmethod<<NEWL>>    def verify_tag(cls, key: bytes, data: bytes, tag: bytes) -> None:<<NEWL>>        p = Poly1305(key)<<NEWL>>        p.update(data)<<NEWL>>        p.verify(tag)"
315	donghui	0	"# This file is dual licensed under the terms of the Apache License, Version<<NEWL>># 2.0, and the BSD License. See the LICENSE file in the root of this repository<<NEWL>># for complete details.<<NEWL>><<NEWL>>import typing<<NEWL>><<NEWL>>from cryptography import utils<<NEWL>>from cryptography.exceptions import (<<NEWL>>    AlreadyFinalized,<<NEWL>>    UnsupportedAlgorithm,<<NEWL>>    _Reasons,<<NEWL>>)<<NEWL>>from cryptography.hazmat.backends.openssl.poly1305 import _Poly1305Context<<NEWL>><<NEWL>><<NEWL>>class Poly1305:<<NEWL>>    _ctx: typing.Optional[_Poly1305Context]<<NEWL>><<NEWL>>    def __init__(self, key: bytes):<<NEWL>>        from cryptography.hazmat.backends.openssl.backend import backend<<NEWL>><<NEWL>>        if not backend.poly1305_supported():<<NEWL>>            raise UnsupportedAlgorithm(<<NEWL>>                ""poly1305 is not supported by this version of OpenSSL."",<<NEWL>>                _Reasons.UNSUPPORTED_MAC,<<NEWL>>            )<<NEWL>>        self._ctx = backend.create_poly1305_ctx(key)<<NEWL>><<NEWL>>    def update(self, data: bytes) -> None:<<NEWL>>        if self._ctx is None:<<NEWL>>            raise AlreadyFinalized(""Context was already finalized."")<<NEWL>>        utils._check_byteslike(""data"", data)<<NEWL>>        self._ctx.update(data)<<NEWL>><<NEWL>>    def finalize(self) -> bytes:<<NEWL>>        if self._ctx is None:<<NEWL>>            raise AlreadyFinalized(""Context was already finalized."")<<NEWL>>        mac = self._ctx.finalize()<<NEWL>>        self._ctx = None<<NEWL>>        return mac<<NEWL>><<NEWL>>    def verify(self, tag: bytes) -> None:<<NEWL>>        utils._check_bytes(""tag"", tag)<<NEWL>>        if self._ctx is None:<<NEWL>>            raise AlreadyFinalized(""Context was already finalized."")<<NEWL>><<NEWL>>        ctx, self._ctx = self._ctx, None<<NEWL>>        ctx.verify(tag)<<NEWL>><<NEWL>>    @classmethod<<NEWL>>    def generate_tag(cls, key: bytes, data: bytes) -> bytes:<<NEWL>>        p = Poly1305(key)<<NEWL>>        p.update(data)<<NEWL>>        return p.finalize()<<NEWL>><<NEWL>>    @classmethod<<NEWL>>    def verify_tag(cls, key: bytes, data: bytes, tag: bytes) -> None:<<NEWL>>        p = Poly1305(key)<<NEWL>>        p.update(data)<<NEWL>>        p.verify(tag)"
255	jackson	2	"import _plotly_utils.basevalidators<<NEWL>><<NEWL>><<NEWL>>class HoverlabelValidator(_plotly_utils.basevalidators.CompoundValidator):<<NEWL>>    def __init__(self, plotly_name=""hoverlabel"", parent_name=""densitymapbox"", **kwargs):<<NEWL>>        super(HoverlabelValidator, self).__init__(<<NEWL>>            plotly_name=plotly_name,<<NEWL>>            parent_name=parent_name,<<NEWL>>            data_class_str=kwargs.pop(""data_class_str"", ""Hoverlabel""),<<NEWL>>            data_docs=kwargs.pop(<<NEWL>>                ""data_docs"",<<NEWL>>                """"""<<NEWL>>            align<<NEWL>>                Sets the horizontal alignment of the text<<NEWL>>                content within hover label box. Has an effect<<NEWL>>                only if the hover label text spans more two or<<NEWL>>                more lines<<NEWL>>            alignsrc<<NEWL>>                Sets the source reference on Chart Studio Cloud<<NEWL>>                for `align`.<<NEWL>>            bgcolor<<NEWL>>                Sets the background color of the hover labels<<NEWL>>                for this trace<<NEWL>>            bgcolorsrc<<NEWL>>                Sets the source reference on Chart Studio Cloud<<NEWL>>                for `bgcolor`.<<NEWL>>            bordercolor<<NEWL>>                Sets the border color of the hover labels for<<NEWL>>                this trace.<<NEWL>>            bordercolorsrc<<NEWL>>                Sets the source reference on Chart Studio Cloud<<NEWL>>                for `bordercolor`.<<NEWL>>            font<<NEWL>>                Sets the font used in hover labels.<<NEWL>>            namelength<<NEWL>>                Sets the default length (in number of<<NEWL>>                characters) of the trace name in the hover<<NEWL>>                labels for all traces. -1 shows the whole name<<NEWL>>                regardless of length. 0-3 shows the first 0-3<<NEWL>>                characters, and an integer >3 will show the<<NEWL>>                whole name if it is less than that many<<NEWL>>                characters, but if it is longer, will truncate<<NEWL>>                to `namelength - 3` characters and add an<<NEWL>>                ellipsis.<<NEWL>>            namelengthsrc<<NEWL>>                Sets the source reference on Chart Studio Cloud<<NEWL>>                for `namelength`.<<NEWL>>"""""",<<NEWL>>            ),<<NEWL>>            **kwargs,<<NEWL>>        )"
255	donghui	1	"import _plotly_utils.basevalidators<<NEWL>><<NEWL>><<NEWL>>class HoverlabelValidator(_plotly_utils.basevalidators.CompoundValidator):<<NEWL>>    def __init__(self, plotly_name=""hoverlabel"", parent_name=""densitymapbox"", **kwargs):<<NEWL>>        super(HoverlabelValidator, self).__init__(<<NEWL>>            plotly_name=plotly_name,<<NEWL>>            parent_name=parent_name,<<NEWL>>            data_class_str=kwargs.pop(""data_class_str"", ""Hoverlabel""),<<NEWL>>            data_docs=kwargs.pop(<<NEWL>>                ""data_docs"",<<NEWL>>                """"""<<NEWL>>            align<<NEWL>>                Sets the horizontal alignment of the text<<NEWL>>                content within hover label box. Has an effect<<NEWL>>                only if the hover label text spans more two or<<NEWL>>                more lines<<NEWL>>            alignsrc<<NEWL>>                Sets the source reference on Chart Studio Cloud<<NEWL>>                for `align`.<<NEWL>>            bgcolor<<NEWL>>                Sets the background color of the hover labels<<NEWL>>                for this trace<<NEWL>>            bgcolorsrc<<NEWL>>                Sets the source reference on Chart Studio Cloud<<NEWL>>                for `bgcolor`.<<NEWL>>            bordercolor<<NEWL>>                Sets the border color of the hover labels for<<NEWL>>                this trace.<<NEWL>>            bordercolorsrc<<NEWL>>                Sets the source reference on Chart Studio Cloud<<NEWL>>                for `bordercolor`.<<NEWL>>            font<<NEWL>>                Sets the font used in hover labels.<<NEWL>>            namelength<<NEWL>>                Sets the default length (in number of<<NEWL>>                characters) of the trace name in the hover<<NEWL>>                labels for all traces. -1 shows the whole name<<NEWL>>                regardless of length. 0-3 shows the first 0-3<<NEWL>>                characters, and an integer >3 will show the<<NEWL>>                whole name if it is less than that many<<NEWL>>                characters, but if it is longer, will truncate<<NEWL>>                to `namelength - 3` characters and add an<<NEWL>>                ellipsis.<<NEWL>>            namelengthsrc<<NEWL>>                Sets the source reference on Chart Studio Cloud<<NEWL>>                for `namelength`.<<NEWL>>"""""",<<NEWL>>            ),<<NEWL>>            **kwargs,<<NEWL>>        )"
344	jackson	1	"#<<NEWL>># The Python Imaging Library.<<NEWL>># $Id$<<NEWL>>#<<NEWL>># DCX file handling<<NEWL>>#<<NEWL>># DCX is a container file format defined by Intel, commonly used<<NEWL>># for fax applications.  Each DCX file consists of a directory<<NEWL>># (a list of file offsets) followed by a set of (usually 1-bit)<<NEWL>># PCX files.<<NEWL>>#<<NEWL>># History:<<NEWL>># 1995-09-09 fl   Created<<NEWL>># 1996-03-20 fl   Properly derived from PcxImageFile.<<NEWL>># 1998-07-15 fl   Renamed offset attribute to avoid name clash<<NEWL>># 2002-07-30 fl   Fixed file handling<<NEWL>>#<<NEWL>># Copyright (c) 1997-98 by Secret Labs AB.<<NEWL>># Copyright (c) 1995-96 by Fredrik Lundh.<<NEWL>>#<<NEWL>># See the README file for information on usage and redistribution.<<NEWL>>#<<NEWL>><<NEWL>>from . import Image<<NEWL>>from ._binary import i32le as i32<<NEWL>>from .PcxImagePlugin import PcxImageFile<<NEWL>><<NEWL>>MAGIC = 0x3ADE68B1  # QUIZ: what's this value, then?<<NEWL>><<NEWL>><<NEWL>>def _accept(prefix):<<NEWL>>    return len(prefix) >= 4 and i32(prefix) == MAGIC<<NEWL>><<NEWL>><<NEWL>>##<<NEWL>># Image plugin for the Intel DCX format.<<NEWL>><<NEWL>><<NEWL>>class DcxImageFile(PcxImageFile):<<NEWL>><<NEWL>>    format = ""DCX""<<NEWL>>    format_description = ""Intel DCX""<<NEWL>>    _close_exclusive_fp_after_loading = False<<NEWL>><<NEWL>>    def _open(self):<<NEWL>><<NEWL>>        # Header<<NEWL>>        s = self.fp.read(4)<<NEWL>>        if not _accept(s):<<NEWL>>            msg = ""not a DCX file""<<NEWL>>            raise SyntaxError(msg)<<NEWL>><<NEWL>>        # Component directory<<NEWL>>        self._offset = []<<NEWL>>        for i in range(1024):<<NEWL>>            offset = i32(self.fp.read(4))<<NEWL>>            if not offset:<<NEWL>>                break<<NEWL>>            self._offset.append(offset)<<NEWL>><<NEWL>>        self._fp = self.fp<<NEWL>>        self.frame = None<<NEWL>>        self.n_frames = len(self._offset)<<NEWL>>        self.is_animated = self.n_frames > 1<<NEWL>>        self.seek(0)<<NEWL>><<NEWL>>    def seek(self, frame):<<NEWL>>        if not self._seek_check(frame):<<NEWL>>            return<<NEWL>>        self.frame = frame<<NEWL>>        self.fp = self._fp<<NEWL>>        self.fp.seek(self._offset[frame])<<NEWL>>        PcxImageFile._open(self)<<NEWL>><<NEWL>>    def tell(self):<<NEWL>>        return self.frame<<NEWL>><<NEWL>><<NEWL>>Image.register_open(DcxImageFile.format, DcxImageFile, _accept)<<NEWL>><<NEWL>>Image.register_extension(DcxImageFile.format, "".dcx"")"
344	donghui	2	"#<<NEWL>># The Python Imaging Library.<<NEWL>># $Id$<<NEWL>>#<<NEWL>># DCX file handling<<NEWL>>#<<NEWL>># DCX is a container file format defined by Intel, commonly used<<NEWL>># for fax applications.  Each DCX file consists of a directory<<NEWL>># (a list of file offsets) followed by a set of (usually 1-bit)<<NEWL>># PCX files.<<NEWL>>#<<NEWL>># History:<<NEWL>># 1995-09-09 fl   Created<<NEWL>># 1996-03-20 fl   Properly derived from PcxImageFile.<<NEWL>># 1998-07-15 fl   Renamed offset attribute to avoid name clash<<NEWL>># 2002-07-30 fl   Fixed file handling<<NEWL>>#<<NEWL>># Copyright (c) 1997-98 by Secret Labs AB.<<NEWL>># Copyright (c) 1995-96 by Fredrik Lundh.<<NEWL>>#<<NEWL>># See the README file for information on usage and redistribution.<<NEWL>>#<<NEWL>><<NEWL>>from . import Image<<NEWL>>from ._binary import i32le as i32<<NEWL>>from .PcxImagePlugin import PcxImageFile<<NEWL>><<NEWL>>MAGIC = 0x3ADE68B1  # QUIZ: what's this value, then?<<NEWL>><<NEWL>><<NEWL>>def _accept(prefix):<<NEWL>>    return len(prefix) >= 4 and i32(prefix) == MAGIC<<NEWL>><<NEWL>><<NEWL>>##<<NEWL>># Image plugin for the Intel DCX format.<<NEWL>><<NEWL>><<NEWL>>class DcxImageFile(PcxImageFile):<<NEWL>><<NEWL>>    format = ""DCX""<<NEWL>>    format_description = ""Intel DCX""<<NEWL>>    _close_exclusive_fp_after_loading = False<<NEWL>><<NEWL>>    def _open(self):<<NEWL>><<NEWL>>        # Header<<NEWL>>        s = self.fp.read(4)<<NEWL>>        if not _accept(s):<<NEWL>>            msg = ""not a DCX file""<<NEWL>>            raise SyntaxError(msg)<<NEWL>><<NEWL>>        # Component directory<<NEWL>>        self._offset = []<<NEWL>>        for i in range(1024):<<NEWL>>            offset = i32(self.fp.read(4))<<NEWL>>            if not offset:<<NEWL>>                break<<NEWL>>            self._offset.append(offset)<<NEWL>><<NEWL>>        self._fp = self.fp<<NEWL>>        self.frame = None<<NEWL>>        self.n_frames = len(self._offset)<<NEWL>>        self.is_animated = self.n_frames > 1<<NEWL>>        self.seek(0)<<NEWL>><<NEWL>>    def seek(self, frame):<<NEWL>>        if not self._seek_check(frame):<<NEWL>>            return<<NEWL>>        self.frame = frame<<NEWL>>        self.fp = self._fp<<NEWL>>        self.fp.seek(self._offset[frame])<<NEWL>>        PcxImageFile._open(self)<<NEWL>><<NEWL>>    def tell(self):<<NEWL>>        return self.frame<<NEWL>><<NEWL>><<NEWL>>Image.register_open(DcxImageFile.format, DcxImageFile, _accept)<<NEWL>><<NEWL>>Image.register_extension(DcxImageFile.format, "".dcx"")"
292	jackson	0	"# This file is dual licensed under the terms of the Apache License, Version<<NEWL>># 2.0, and the BSD License. See the LICENSE file in the root of this repository<<NEWL>># for complete details.<<NEWL>><<NEWL>><<NEWL>>import typing<<NEWL>><<NEWL>>from cryptography import utils<<NEWL>>from cryptography.exceptions import AlreadyFinalized, InvalidKey<<NEWL>>from cryptography.hazmat.primitives import constant_time, hashes<<NEWL>>from cryptography.hazmat.primitives.kdf import KeyDerivationFunction<<NEWL>><<NEWL>><<NEWL>>def _int_to_u32be(n: int) -> bytes:<<NEWL>>    return n.to_bytes(length=4, byteorder=""big"")<<NEWL>><<NEWL>><<NEWL>>class X963KDF(KeyDerivationFunction):<<NEWL>>    def __init__(<<NEWL>>        self,<<NEWL>>        algorithm: hashes.HashAlgorithm,<<NEWL>>        length: int,<<NEWL>>        sharedinfo: typing.Optional[bytes],<<NEWL>>        backend: typing.Any = None,<<NEWL>>    ):<<NEWL>>        max_len = algorithm.digest_size * (2**32 - 1)<<NEWL>>        if length > max_len:<<NEWL>>            raise ValueError(<<NEWL>>                ""Cannot derive keys larger than {} bits."".format(max_len)<<NEWL>>            )<<NEWL>>        if sharedinfo is not None:<<NEWL>>            utils._check_bytes(""sharedinfo"", sharedinfo)<<NEWL>><<NEWL>>        self._algorithm = algorithm<<NEWL>>        self._length = length<<NEWL>>        self._sharedinfo = sharedinfo<<NEWL>>        self._used = False<<NEWL>><<NEWL>>    def derive(self, key_material: bytes) -> bytes:<<NEWL>>        if self._used:<<NEWL>>            raise AlreadyFinalized<<NEWL>>        self._used = True<<NEWL>>        utils._check_byteslike(""key_material"", key_material)<<NEWL>>        output = [b""""]<<NEWL>>        outlen = 0<<NEWL>>        counter = 1<<NEWL>><<NEWL>>        while self._length > outlen:<<NEWL>>            h = hashes.Hash(self._algorithm)<<NEWL>>            h.update(key_material)<<NEWL>>            h.update(_int_to_u32be(counter))<<NEWL>>            if self._sharedinfo is not None:<<NEWL>>                h.update(self._sharedinfo)<<NEWL>>            output.append(h.finalize())<<NEWL>>            outlen += len(output[-1])<<NEWL>>            counter += 1<<NEWL>><<NEWL>>        return b"""".join(output)[: self._length]<<NEWL>><<NEWL>>    def verify(self, key_material: bytes, expected_key: bytes) -> None:<<NEWL>>        if not constant_time.bytes_eq(self.derive(key_material), expected_key):<<NEWL>>            raise InvalidKey"
292	donghui	0	"# This file is dual licensed under the terms of the Apache License, Version<<NEWL>># 2.0, and the BSD License. See the LICENSE file in the root of this repository<<NEWL>># for complete details.<<NEWL>><<NEWL>><<NEWL>>import typing<<NEWL>><<NEWL>>from cryptography import utils<<NEWL>>from cryptography.exceptions import AlreadyFinalized, InvalidKey<<NEWL>>from cryptography.hazmat.primitives import constant_time, hashes<<NEWL>>from cryptography.hazmat.primitives.kdf import KeyDerivationFunction<<NEWL>><<NEWL>><<NEWL>>def _int_to_u32be(n: int) -> bytes:<<NEWL>>    return n.to_bytes(length=4, byteorder=""big"")<<NEWL>><<NEWL>><<NEWL>>class X963KDF(KeyDerivationFunction):<<NEWL>>    def __init__(<<NEWL>>        self,<<NEWL>>        algorithm: hashes.HashAlgorithm,<<NEWL>>        length: int,<<NEWL>>        sharedinfo: typing.Optional[bytes],<<NEWL>>        backend: typing.Any = None,<<NEWL>>    ):<<NEWL>>        max_len = algorithm.digest_size * (2**32 - 1)<<NEWL>>        if length > max_len:<<NEWL>>            raise ValueError(<<NEWL>>                ""Cannot derive keys larger than {} bits."".format(max_len)<<NEWL>>            )<<NEWL>>        if sharedinfo is not None:<<NEWL>>            utils._check_bytes(""sharedinfo"", sharedinfo)<<NEWL>><<NEWL>>        self._algorithm = algorithm<<NEWL>>        self._length = length<<NEWL>>        self._sharedinfo = sharedinfo<<NEWL>>        self._used = False<<NEWL>><<NEWL>>    def derive(self, key_material: bytes) -> bytes:<<NEWL>>        if self._used:<<NEWL>>            raise AlreadyFinalized<<NEWL>>        self._used = True<<NEWL>>        utils._check_byteslike(""key_material"", key_material)<<NEWL>>        output = [b""""]<<NEWL>>        outlen = 0<<NEWL>>        counter = 1<<NEWL>><<NEWL>>        while self._length > outlen:<<NEWL>>            h = hashes.Hash(self._algorithm)<<NEWL>>            h.update(key_material)<<NEWL>>            h.update(_int_to_u32be(counter))<<NEWL>>            if self._sharedinfo is not None:<<NEWL>>                h.update(self._sharedinfo)<<NEWL>>            output.append(h.finalize())<<NEWL>>            outlen += len(output[-1])<<NEWL>>            counter += 1<<NEWL>><<NEWL>>        return b"""".join(output)[: self._length]<<NEWL>><<NEWL>>    def verify(self, key_material: bytes, expected_key: bytes) -> None:<<NEWL>>        if not constant_time.bytes_eq(self.derive(key_material), expected_key):<<NEWL>>            raise InvalidKey"
383	jackson	1	"""""""distutils.command.install_scripts<<NEWL>><<NEWL>>Implements the Distutils 'install_scripts' command, for installing<<NEWL>>Python scripts.""""""<<NEWL>><<NEWL>># contributed by Bastian Kleineidam<<NEWL>><<NEWL>>import os<<NEWL>>from distutils.core import Command<<NEWL>>from distutils import log<<NEWL>>from stat import ST_MODE<<NEWL>><<NEWL>><<NEWL>>class install_scripts(Command):<<NEWL>><<NEWL>>    description = ""install scripts (Python or otherwise)""<<NEWL>><<NEWL>>    user_options = [<<NEWL>>        ('install-dir=', 'd', ""directory to install scripts to""),<<NEWL>>        ('build-dir=', 'b', ""build directory (where to install from)""),<<NEWL>>        ('force', 'f', ""force installation (overwrite existing files)""),<<NEWL>>        ('skip-build', None, ""skip the build steps""),<<NEWL>>    ]<<NEWL>><<NEWL>>    boolean_options = ['force', 'skip-build']<<NEWL>><<NEWL>>    def initialize_options(self):<<NEWL>>        self.install_dir = None<<NEWL>>        self.force = 0<<NEWL>>        self.build_dir = None<<NEWL>>        self.skip_build = None<<NEWL>><<NEWL>>    def finalize_options(self):<<NEWL>>        self.set_undefined_options('build', ('build_scripts', 'build_dir'))<<NEWL>>        self.set_undefined_options(<<NEWL>>            'install',<<NEWL>>            ('install_scripts', 'install_dir'),<<NEWL>>            ('force', 'force'),<<NEWL>>            ('skip_build', 'skip_build'),<<NEWL>>        )<<NEWL>><<NEWL>>    def run(self):<<NEWL>>        if not self.skip_build:<<NEWL>>            self.run_command('build_scripts')<<NEWL>>        self.outfiles = self.copy_tree(self.build_dir, self.install_dir)<<NEWL>>        if os.name == 'posix':<<NEWL>>            # Set the executable bits (owner, group, and world) on<<NEWL>>            # all the scripts we just installed.<<NEWL>>            for file in self.get_outputs():<<NEWL>>                if self.dry_run:<<NEWL>>                    log.info(""changing mode of %s"", file)<<NEWL>>                else:<<NEWL>>                    mode = ((os.stat(file)[ST_MODE]) | 0o555) & 0o7777<<NEWL>>                    log.info(""changing mode of %s to %o"", file, mode)<<NEWL>>                    os.chmod(file, mode)<<NEWL>><<NEWL>>    def get_inputs(self):<<NEWL>>        return self.distribution.scripts or []<<NEWL>><<NEWL>>    def get_outputs(self):<<NEWL>>        return self.outfiles or []"
383	donghui	1	"""""""distutils.command.install_scripts<<NEWL>><<NEWL>>Implements the Distutils 'install_scripts' command, for installing<<NEWL>>Python scripts.""""""<<NEWL>><<NEWL>># contributed by Bastian Kleineidam<<NEWL>><<NEWL>>import os<<NEWL>>from distutils.core import Command<<NEWL>>from distutils import log<<NEWL>>from stat import ST_MODE<<NEWL>><<NEWL>><<NEWL>>class install_scripts(Command):<<NEWL>><<NEWL>>    description = ""install scripts (Python or otherwise)""<<NEWL>><<NEWL>>    user_options = [<<NEWL>>        ('install-dir=', 'd', ""directory to install scripts to""),<<NEWL>>        ('build-dir=', 'b', ""build directory (where to install from)""),<<NEWL>>        ('force', 'f', ""force installation (overwrite existing files)""),<<NEWL>>        ('skip-build', None, ""skip the build steps""),<<NEWL>>    ]<<NEWL>><<NEWL>>    boolean_options = ['force', 'skip-build']<<NEWL>><<NEWL>>    def initialize_options(self):<<NEWL>>        self.install_dir = None<<NEWL>>        self.force = 0<<NEWL>>        self.build_dir = None<<NEWL>>        self.skip_build = None<<NEWL>><<NEWL>>    def finalize_options(self):<<NEWL>>        self.set_undefined_options('build', ('build_scripts', 'build_dir'))<<NEWL>>        self.set_undefined_options(<<NEWL>>            'install',<<NEWL>>            ('install_scripts', 'install_dir'),<<NEWL>>            ('force', 'force'),<<NEWL>>            ('skip_build', 'skip_build'),<<NEWL>>        )<<NEWL>><<NEWL>>    def run(self):<<NEWL>>        if not self.skip_build:<<NEWL>>            self.run_command('build_scripts')<<NEWL>>        self.outfiles = self.copy_tree(self.build_dir, self.install_dir)<<NEWL>>        if os.name == 'posix':<<NEWL>>            # Set the executable bits (owner, group, and world) on<<NEWL>>            # all the scripts we just installed.<<NEWL>>            for file in self.get_outputs():<<NEWL>>                if self.dry_run:<<NEWL>>                    log.info(""changing mode of %s"", file)<<NEWL>>                else:<<NEWL>>                    mode = ((os.stat(file)[ST_MODE]) | 0o555) & 0o7777<<NEWL>>                    log.info(""changing mode of %s to %o"", file, mode)<<NEWL>>                    os.chmod(file, mode)<<NEWL>><<NEWL>>    def get_inputs(self):<<NEWL>>        return self.distribution.scripts or []<<NEWL>><<NEWL>>    def get_outputs(self):<<NEWL>>        return self.outfiles or []"
370	jackson	2	"# Copyright 2022 Google LLC<<NEWL>>#<<NEWL>># Licensed under the Apache License, Version 2.0 (the ""License"");<<NEWL>># you may not use this file except in compliance with the License.<<NEWL>># You may obtain a copy of the License at<<NEWL>>#<<NEWL>>#     http://www.apache.org/licenses/LICENSE-2.0<<NEWL>>#<<NEWL>># Unless required by applicable law or agreed to in writing, software<<NEWL>># distributed under the License is distributed on an ""AS IS"" BASIS,<<NEWL>># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.<<NEWL>># See the License for the specific language governing permissions and<<NEWL>># limitations under the License.<<NEWL>><<NEWL>>import os<<NEWL>>import re<<NEWL>>import uuid<<NEWL>><<NEWL>>from google.cloud import iam_v2<<NEWL>>from google.cloud.iam_v2 import types<<NEWL>>import pytest<<NEWL>>from snippets.create_deny_policy import create_deny_policy<<NEWL>>from snippets.delete_deny_policy import delete_deny_policy<<NEWL>><<NEWL>>PROJECT_ID = os.environ[""IAM_PROJECT_ID""]<<NEWL>>GOOGLE_APPLICATION_CREDENTIALS = os.environ[""IAM_CREDENTIALS""]<<NEWL>><<NEWL>><<NEWL>>@pytest.fixture<<NEWL>>def deny_policy(capsys: ""pytest.CaptureFixture[str]"") -> None:<<NEWL>>    policy_id = f""test-deny-policy-{uuid.uuid4()}""<<NEWL>><<NEWL>>    # Delete any existing policies. Otherwise it might throw quota issue.<<NEWL>>    delete_existing_deny_policies(PROJECT_ID, ""test-deny-policy"")<<NEWL>><<NEWL>>    # Create the Deny policy.<<NEWL>>    create_deny_policy(PROJECT_ID, policy_id)<<NEWL>><<NEWL>>    yield policy_id<<NEWL>><<NEWL>>    # Delete the Deny policy and assert if deleted.<<NEWL>>    delete_deny_policy(PROJECT_ID, policy_id)<<NEWL>>    out, _ = capsys.readouterr()<<NEWL>>    assert re.search(f""Deleted the deny policy: {policy_id}"", out)<<NEWL>><<NEWL>><<NEWL>>def delete_existing_deny_policies(project_id: str, delete_name_prefix: str) -> None:<<NEWL>>    policies_client = iam_v2.PoliciesClient()<<NEWL>><<NEWL>>    attachment_point = f""cloudresourcemanager.googleapis.com%2Fprojects%2F{project_id}""<<NEWL>><<NEWL>>    request = types.ListPoliciesRequest()<<NEWL>>    request.parent = f""policies/{attachment_point}/denypolicies""<<NEWL>>    for policy in policies_client.list_policies(request=request):<<NEWL>>        if delete_name_prefix in policy.name:<<NEWL>>            delete_deny_policy(PROJECT_ID, str(policy.name).rsplit(""/"", 1)[-1])"
370	donghui	2	"# Copyright 2022 Google LLC<<NEWL>>#<<NEWL>># Licensed under the Apache License, Version 2.0 (the ""License"");<<NEWL>># you may not use this file except in compliance with the License.<<NEWL>># You may obtain a copy of the License at<<NEWL>>#<<NEWL>>#     http://www.apache.org/licenses/LICENSE-2.0<<NEWL>>#<<NEWL>># Unless required by applicable law or agreed to in writing, software<<NEWL>># distributed under the License is distributed on an ""AS IS"" BASIS,<<NEWL>># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.<<NEWL>># See the License for the specific language governing permissions and<<NEWL>># limitations under the License.<<NEWL>><<NEWL>>import os<<NEWL>>import re<<NEWL>>import uuid<<NEWL>><<NEWL>>from google.cloud import iam_v2<<NEWL>>from google.cloud.iam_v2 import types<<NEWL>>import pytest<<NEWL>>from snippets.create_deny_policy import create_deny_policy<<NEWL>>from snippets.delete_deny_policy import delete_deny_policy<<NEWL>><<NEWL>>PROJECT_ID = os.environ[""IAM_PROJECT_ID""]<<NEWL>>GOOGLE_APPLICATION_CREDENTIALS = os.environ[""IAM_CREDENTIALS""]<<NEWL>><<NEWL>><<NEWL>>@pytest.fixture<<NEWL>>def deny_policy(capsys: ""pytest.CaptureFixture[str]"") -> None:<<NEWL>>    policy_id = f""test-deny-policy-{uuid.uuid4()}""<<NEWL>><<NEWL>>    # Delete any existing policies. Otherwise it might throw quota issue.<<NEWL>>    delete_existing_deny_policies(PROJECT_ID, ""test-deny-policy"")<<NEWL>><<NEWL>>    # Create the Deny policy.<<NEWL>>    create_deny_policy(PROJECT_ID, policy_id)<<NEWL>><<NEWL>>    yield policy_id<<NEWL>><<NEWL>>    # Delete the Deny policy and assert if deleted.<<NEWL>>    delete_deny_policy(PROJECT_ID, policy_id)<<NEWL>>    out, _ = capsys.readouterr()<<NEWL>>    assert re.search(f""Deleted the deny policy: {policy_id}"", out)<<NEWL>><<NEWL>><<NEWL>>def delete_existing_deny_policies(project_id: str, delete_name_prefix: str) -> None:<<NEWL>>    policies_client = iam_v2.PoliciesClient()<<NEWL>><<NEWL>>    attachment_point = f""cloudresourcemanager.googleapis.com%2Fprojects%2F{project_id}""<<NEWL>><<NEWL>>    request = types.ListPoliciesRequest()<<NEWL>>    request.parent = f""policies/{attachment_point}/denypolicies""<<NEWL>>    for policy in policies_client.list_policies(request=request):<<NEWL>>        if delete_name_prefix in policy.name:<<NEWL>>            delete_deny_policy(PROJECT_ID, str(policy.name).rsplit(""/"", 1)[-1])"
261	jackson	0	"from django.contrib.auth.forms import UserCreationForm<<NEWL>>from django import forms<<NEWL>>from django.contrib.auth.models import User<<NEWL>>from .models import Post<<NEWL>><<NEWL>>class RegisterForm(UserCreationForm):<<NEWL>>    email = forms.EmailField(label = ""Email"")<<NEWL>>    firstname = forms.CharField(label = ""First name"")<<NEWL>>    lastname = forms.CharField(label = ""Last name"")<<NEWL>><<NEWL>>    class Meta:<<NEWL>>        model = User<<NEWL>>        fields = (""username"", ""firstname"", ""lastname"", ""email"", )<<NEWL>><<NEWL>>    def save(self, commit=True):<<NEWL>>        user = super(RegisterForm, self).save(commit=False)<<NEWL>>        firstname = self.cleaned_data[""firstname""]<<NEWL>>        lastname = self.cleaned_data[""lastname""]<<NEWL>>        user.first_name = firstname<<NEWL>>        user.last_name = lastname<<NEWL>>        user.email = self.cleaned_data[""email""]<<NEWL>>        if commit:<<NEWL>>            user.save()<<NEWL>>        return user<<NEWL>>    <<NEWL>>class PostForm(forms.ModelForm):<<NEWL>>    text = forms.CharField(max_length=1000, widget=forms.Textarea(attrs={'placeholder': 'What\'s on your mind?', 'onchange': 'character_count()', 'onkeypress': 'character_count()', 'onfocus': 'character_count()' ,'oninput': 'character_count()', 'onkeyup':'character_count()','onpaste':'character_count()'}))<<NEWL>>    images = forms.ImageField(required=False,widget=forms.ClearableFileInput(attrs={'multiple': True, 'onchange': 'previewImages(this)'}))<<NEWL>>    class Meta:<<NEWL>>        model = Post<<NEWL>>        fields = (""text"", )<<NEWL>><<NEWL>>class SearchForm(forms.Form):<<NEWL>>    search = forms.CharField(max_length=100, widget=forms.TextInput(attrs={'placeholder': 'Type something or someone to search for ...'}))<<NEWL>><<NEWL>>class UpdateProfileForm(forms.Form):<<NEWL>>    first_name = forms.CharField(max_length=100, required=False)<<NEWL>>    last_name = forms.CharField(max_length=100, required=False)<<NEWL>>    profile_image = forms.ImageField(required=False)<<NEWL>>    remove_profile_image = forms.BooleanField(required=False)<<NEWL>>    profile_cover_photo = forms.ImageField(required=False)<<NEWL>>    remove_cover_photo = forms.BooleanField(required=False)<<NEWL>>    profile_bio = forms.CharField(max_length=500, required=False, widget=forms.Textarea(attrs={'placeholder': 'Write something about yourself ...'}))"
261	donghui	0	"from django.contrib.auth.forms import UserCreationForm<<NEWL>>from django import forms<<NEWL>>from django.contrib.auth.models import User<<NEWL>>from .models import Post<<NEWL>><<NEWL>>class RegisterForm(UserCreationForm):<<NEWL>>    email = forms.EmailField(label = ""Email"")<<NEWL>>    firstname = forms.CharField(label = ""First name"")<<NEWL>>    lastname = forms.CharField(label = ""Last name"")<<NEWL>><<NEWL>>    class Meta:<<NEWL>>        model = User<<NEWL>>        fields = (""username"", ""firstname"", ""lastname"", ""email"", )<<NEWL>><<NEWL>>    def save(self, commit=True):<<NEWL>>        user = super(RegisterForm, self).save(commit=False)<<NEWL>>        firstname = self.cleaned_data[""firstname""]<<NEWL>>        lastname = self.cleaned_data[""lastname""]<<NEWL>>        user.first_name = firstname<<NEWL>>        user.last_name = lastname<<NEWL>>        user.email = self.cleaned_data[""email""]<<NEWL>>        if commit:<<NEWL>>            user.save()<<NEWL>>        return user<<NEWL>>    <<NEWL>>class PostForm(forms.ModelForm):<<NEWL>>    text = forms.CharField(max_length=1000, widget=forms.Textarea(attrs={'placeholder': 'What\'s on your mind?', 'onchange': 'character_count()', 'onkeypress': 'character_count()', 'onfocus': 'character_count()' ,'oninput': 'character_count()', 'onkeyup':'character_count()','onpaste':'character_count()'}))<<NEWL>>    images = forms.ImageField(required=False,widget=forms.ClearableFileInput(attrs={'multiple': True, 'onchange': 'previewImages(this)'}))<<NEWL>>    class Meta:<<NEWL>>        model = Post<<NEWL>>        fields = (""text"", )<<NEWL>><<NEWL>>class SearchForm(forms.Form):<<NEWL>>    search = forms.CharField(max_length=100, widget=forms.TextInput(attrs={'placeholder': 'Type something or someone to search for ...'}))<<NEWL>><<NEWL>>class UpdateProfileForm(forms.Form):<<NEWL>>    first_name = forms.CharField(max_length=100, required=False)<<NEWL>>    last_name = forms.CharField(max_length=100, required=False)<<NEWL>>    profile_image = forms.ImageField(required=False)<<NEWL>>    remove_profile_image = forms.BooleanField(required=False)<<NEWL>>    profile_cover_photo = forms.ImageField(required=False)<<NEWL>>    remove_cover_photo = forms.BooleanField(required=False)<<NEWL>>    profile_bio = forms.CharField(max_length=500, required=False, widget=forms.Textarea(attrs={'placeholder': 'Write something about yourself ...'}))"
321	jackson	3	"# Copyright (c) Facebook, Inc. and its affiliates.<<NEWL>>#<<NEWL>># This source code is licensed under the MIT license found in the<<NEWL>># LICENSE file in the root directory of this source tree.<<NEWL>><<NEWL>>from fairseq.optim import LegacyFairseqOptimizer, register_optimizer<<NEWL>><<NEWL>><<NEWL>>@register_optimizer(""lamb"")<<NEWL>>class FairseqLAMB(LegacyFairseqOptimizer):<<NEWL>>    """"""LAMB optimizer.""""""<<NEWL>><<NEWL>>    def __init__(self, args, params):<<NEWL>>        super().__init__(args)<<NEWL>>        try:<<NEWL>>            from apex.optimizers import FusedLAMB<<NEWL>><<NEWL>>            self._optimizer = FusedLAMB(params, **self.optimizer_config)<<NEWL>>        except ImportError:<<NEWL>>            raise ImportError(""Please install apex to use LAMB optimizer"")<<NEWL>><<NEWL>>    @staticmethod<<NEWL>>    def add_args(parser):<<NEWL>>        """"""Add optimizer-specific arguments to the parser.""""""<<NEWL>>        # fmt: off<<NEWL>>        parser.add_argument('--lamb-betas', default='(0.9, 0.999)', metavar='B',<<NEWL>>                            help='betas for LAMB optimizer')<<NEWL>>        parser.add_argument('--lamb-eps', type=float, default=1e-8, metavar='D',<<NEWL>>                            help='epsilon for LAMB optimizer')<<NEWL>>        parser.add_argument('--weight-decay', '--wd', default=0.0, type=float, metavar='WD',<<NEWL>>                            help='weight decay')<<NEWL>>        # fmt: on<<NEWL>><<NEWL>>    @property<<NEWL>>    def optimizer_config(self):<<NEWL>>        """"""<<NEWL>>        Return a kwarg dictionary that will be used to override optimizer<<NEWL>>        args stored in checkpoints. This allows us to load a checkpoint and<<NEWL>>        resume training using a different set of optimizer args, e.g., with a<<NEWL>>        different learning rate.<<NEWL>>        """"""<<NEWL>>        return {<<NEWL>>            ""lr"": self.args.lr[0],<<NEWL>>            ""betas"": eval(self.args.lamb_betas),<<NEWL>>            ""eps"": self.args.lamb_eps,<<NEWL>>            ""weight_decay"": self.args.weight_decay,<<NEWL>>        }<<NEWL>><<NEWL>>    @property<<NEWL>>    def supports_flat_params(self):<<NEWL>>        return False"
321	donghui	2	"# Copyright (c) Facebook, Inc. and its affiliates.<<NEWL>>#<<NEWL>># This source code is licensed under the MIT license found in the<<NEWL>># LICENSE file in the root directory of this source tree.<<NEWL>><<NEWL>>from fairseq.optim import LegacyFairseqOptimizer, register_optimizer<<NEWL>><<NEWL>><<NEWL>>@register_optimizer(""lamb"")<<NEWL>>class FairseqLAMB(LegacyFairseqOptimizer):<<NEWL>>    """"""LAMB optimizer.""""""<<NEWL>><<NEWL>>    def __init__(self, args, params):<<NEWL>>        super().__init__(args)<<NEWL>>        try:<<NEWL>>            from apex.optimizers import FusedLAMB<<NEWL>><<NEWL>>            self._optimizer = FusedLAMB(params, **self.optimizer_config)<<NEWL>>        except ImportError:<<NEWL>>            raise ImportError(""Please install apex to use LAMB optimizer"")<<NEWL>><<NEWL>>    @staticmethod<<NEWL>>    def add_args(parser):<<NEWL>>        """"""Add optimizer-specific arguments to the parser.""""""<<NEWL>>        # fmt: off<<NEWL>>        parser.add_argument('--lamb-betas', default='(0.9, 0.999)', metavar='B',<<NEWL>>                            help='betas for LAMB optimizer')<<NEWL>>        parser.add_argument('--lamb-eps', type=float, default=1e-8, metavar='D',<<NEWL>>                            help='epsilon for LAMB optimizer')<<NEWL>>        parser.add_argument('--weight-decay', '--wd', default=0.0, type=float, metavar='WD',<<NEWL>>                            help='weight decay')<<NEWL>>        # fmt: on<<NEWL>><<NEWL>>    @property<<NEWL>>    def optimizer_config(self):<<NEWL>>        """"""<<NEWL>>        Return a kwarg dictionary that will be used to override optimizer<<NEWL>>        args stored in checkpoints. This allows us to load a checkpoint and<<NEWL>>        resume training using a different set of optimizer args, e.g., with a<<NEWL>>        different learning rate.<<NEWL>>        """"""<<NEWL>>        return {<<NEWL>>            ""lr"": self.args.lr[0],<<NEWL>>            ""betas"": eval(self.args.lamb_betas),<<NEWL>>            ""eps"": self.args.lamb_eps,<<NEWL>>            ""weight_decay"": self.args.weight_decay,<<NEWL>>        }<<NEWL>><<NEWL>>    @property<<NEWL>>    def supports_flat_params(self):<<NEWL>>        return False"
330	jackson	1	"""""""Tests for the NumpyVersion class.<<NEWL>><<NEWL>>""""""<<NEWL>>from numpy.testing import assert_, assert_raises<<NEWL>>from numpy.lib import NumpyVersion<<NEWL>><<NEWL>><<NEWL>>def test_main_versions():<<NEWL>>    assert_(NumpyVersion('1.8.0') == '1.8.0')<<NEWL>>    for ver in ['1.9.0', '2.0.0', '1.8.1', '10.0.1']:<<NEWL>>        assert_(NumpyVersion('1.8.0') < ver)<<NEWL>><<NEWL>>    for ver in ['1.7.0', '1.7.1', '0.9.9']:<<NEWL>>        assert_(NumpyVersion('1.8.0') > ver)<<NEWL>><<NEWL>><<NEWL>>def test_version_1_point_10():<<NEWL>>    # regression test for gh-2998.<<NEWL>>    assert_(NumpyVersion('1.9.0') < '1.10.0')<<NEWL>>    assert_(NumpyVersion('1.11.0') < '1.11.1')<<NEWL>>    assert_(NumpyVersion('1.11.0') == '1.11.0')<<NEWL>>    assert_(NumpyVersion('1.99.11') < '1.99.12')<<NEWL>><<NEWL>><<NEWL>>def test_alpha_beta_rc():<<NEWL>>    assert_(NumpyVersion('1.8.0rc1') == '1.8.0rc1')<<NEWL>>    for ver in ['1.8.0', '1.8.0rc2']:<<NEWL>>        assert_(NumpyVersion('1.8.0rc1') < ver)<<NEWL>><<NEWL>>    for ver in ['1.8.0a2', '1.8.0b3', '1.7.2rc4']:<<NEWL>>        assert_(NumpyVersion('1.8.0rc1') > ver)<<NEWL>><<NEWL>>    assert_(NumpyVersion('1.8.0b1') > '1.8.0a2')<<NEWL>><<NEWL>><<NEWL>>def test_dev_version():<<NEWL>>    assert_(NumpyVersion('1.9.0.dev-Unknown') < '1.9.0')<<NEWL>>    for ver in ['1.9.0', '1.9.0a1', '1.9.0b2', '1.9.0b2.dev-ffffffff']:<<NEWL>>        assert_(NumpyVersion('1.9.0.dev-f16acvda') < ver)<<NEWL>><<NEWL>>    assert_(NumpyVersion('1.9.0.dev-f16acvda') == '1.9.0.dev-11111111')<<NEWL>><<NEWL>><<NEWL>>def test_dev_a_b_rc_mixed():<<NEWL>>    assert_(NumpyVersion('1.9.0a2.dev-f16acvda') == '1.9.0a2.dev-11111111')<<NEWL>>    assert_(NumpyVersion('1.9.0a2.dev-6acvda54') < '1.9.0a2')<<NEWL>><<NEWL>><<NEWL>>def test_dev0_version():<<NEWL>>    assert_(NumpyVersion('1.9.0.dev0+Unknown') < '1.9.0')<<NEWL>>    for ver in ['1.9.0', '1.9.0a1', '1.9.0b2', '1.9.0b2.dev0+ffffffff']:<<NEWL>>        assert_(NumpyVersion('1.9.0.dev0+f16acvda') < ver)<<NEWL>><<NEWL>>    assert_(NumpyVersion('1.9.0.dev0+f16acvda') == '1.9.0.dev0+11111111')<<NEWL>><<NEWL>><<NEWL>>def test_dev0_a_b_rc_mixed():<<NEWL>>    assert_(NumpyVersion('1.9.0a2.dev0+f16acvda') == '1.9.0a2.dev0+11111111')<<NEWL>>    assert_(NumpyVersion('1.9.0a2.dev0+6acvda54') < '1.9.0a2')<<NEWL>><<NEWL>><<NEWL>>def test_raises():<<NEWL>>    for ver in ['1.9', '1,9.0', '1.7.x']:<<NEWL>>        assert_raises(ValueError, NumpyVersion, ver)"
330	donghui	0	"""""""Tests for the NumpyVersion class.<<NEWL>><<NEWL>>""""""<<NEWL>>from numpy.testing import assert_, assert_raises<<NEWL>>from numpy.lib import NumpyVersion<<NEWL>><<NEWL>><<NEWL>>def test_main_versions():<<NEWL>>    assert_(NumpyVersion('1.8.0') == '1.8.0')<<NEWL>>    for ver in ['1.9.0', '2.0.0', '1.8.1', '10.0.1']:<<NEWL>>        assert_(NumpyVersion('1.8.0') < ver)<<NEWL>><<NEWL>>    for ver in ['1.7.0', '1.7.1', '0.9.9']:<<NEWL>>        assert_(NumpyVersion('1.8.0') > ver)<<NEWL>><<NEWL>><<NEWL>>def test_version_1_point_10():<<NEWL>>    # regression test for gh-2998.<<NEWL>>    assert_(NumpyVersion('1.9.0') < '1.10.0')<<NEWL>>    assert_(NumpyVersion('1.11.0') < '1.11.1')<<NEWL>>    assert_(NumpyVersion('1.11.0') == '1.11.0')<<NEWL>>    assert_(NumpyVersion('1.99.11') < '1.99.12')<<NEWL>><<NEWL>><<NEWL>>def test_alpha_beta_rc():<<NEWL>>    assert_(NumpyVersion('1.8.0rc1') == '1.8.0rc1')<<NEWL>>    for ver in ['1.8.0', '1.8.0rc2']:<<NEWL>>        assert_(NumpyVersion('1.8.0rc1') < ver)<<NEWL>><<NEWL>>    for ver in ['1.8.0a2', '1.8.0b3', '1.7.2rc4']:<<NEWL>>        assert_(NumpyVersion('1.8.0rc1') > ver)<<NEWL>><<NEWL>>    assert_(NumpyVersion('1.8.0b1') > '1.8.0a2')<<NEWL>><<NEWL>><<NEWL>>def test_dev_version():<<NEWL>>    assert_(NumpyVersion('1.9.0.dev-Unknown') < '1.9.0')<<NEWL>>    for ver in ['1.9.0', '1.9.0a1', '1.9.0b2', '1.9.0b2.dev-ffffffff']:<<NEWL>>        assert_(NumpyVersion('1.9.0.dev-f16acvda') < ver)<<NEWL>><<NEWL>>    assert_(NumpyVersion('1.9.0.dev-f16acvda') == '1.9.0.dev-11111111')<<NEWL>><<NEWL>><<NEWL>>def test_dev_a_b_rc_mixed():<<NEWL>>    assert_(NumpyVersion('1.9.0a2.dev-f16acvda') == '1.9.0a2.dev-11111111')<<NEWL>>    assert_(NumpyVersion('1.9.0a2.dev-6acvda54') < '1.9.0a2')<<NEWL>><<NEWL>><<NEWL>>def test_dev0_version():<<NEWL>>    assert_(NumpyVersion('1.9.0.dev0+Unknown') < '1.9.0')<<NEWL>>    for ver in ['1.9.0', '1.9.0a1', '1.9.0b2', '1.9.0b2.dev0+ffffffff']:<<NEWL>>        assert_(NumpyVersion('1.9.0.dev0+f16acvda') < ver)<<NEWL>><<NEWL>>    assert_(NumpyVersion('1.9.0.dev0+f16acvda') == '1.9.0.dev0+11111111')<<NEWL>><<NEWL>><<NEWL>>def test_dev0_a_b_rc_mixed():<<NEWL>>    assert_(NumpyVersion('1.9.0a2.dev0+f16acvda') == '1.9.0a2.dev0+11111111')<<NEWL>>    assert_(NumpyVersion('1.9.0a2.dev0+6acvda54') < '1.9.0a2')<<NEWL>><<NEWL>><<NEWL>>def test_raises():<<NEWL>>    for ver in ['1.9', '1,9.0', '1.7.x']:<<NEWL>>        assert_raises(ValueError, NumpyVersion, ver)"
270	jackson	2	"import _plotly_utils.basevalidators<<NEWL>><<NEWL>><<NEWL>>class FontValidator(_plotly_utils.basevalidators.CompoundValidator):<<NEWL>>    def __init__(<<NEWL>>        self, plotly_name=""font"", parent_name=""funnelarea.hoverlabel"", **kwargs<<NEWL>>    ):<<NEWL>>        super(FontValidator, self).__init__(<<NEWL>>            plotly_name=plotly_name,<<NEWL>>            parent_name=parent_name,<<NEWL>>            data_class_str=kwargs.pop(""data_class_str"", ""Font""),<<NEWL>>            data_docs=kwargs.pop(<<NEWL>>                ""data_docs"",<<NEWL>>                """"""<<NEWL>>            color<<NEWL>><<NEWL>>            colorsrc<<NEWL>>                Sets the source reference on Chart Studio Cloud<<NEWL>>                for `color`.<<NEWL>>            family<<NEWL>>                HTML font family - the typeface that will be<<NEWL>>                applied by the web browser. The web browser<<NEWL>>                will only be able to apply a font if it is<<NEWL>>                available on the system which it operates.<<NEWL>>                Provide multiple font families, separated by<<NEWL>>                commas, to indicate the preference in which to<<NEWL>>                apply fonts if they aren't available on the<<NEWL>>                system. The Chart Studio Cloud (at<<NEWL>>                https://chart-studio.plotly.com or on-premise)<<NEWL>>                generates images on a server, where only a<<NEWL>>                select number of fonts are installed and<<NEWL>>                supported. These include ""Arial"", ""Balto"",<<NEWL>>                ""Courier New"", ""Droid Sans"",, ""Droid Serif"",<<NEWL>>                ""Droid Sans Mono"", ""Gravitas One"", ""Old<<NEWL>>                Standard TT"", ""Open Sans"", ""Overpass"", ""PT Sans<<NEWL>>                Narrow"", ""Raleway"", ""Times New Roman"".<<NEWL>>            familysrc<<NEWL>>                Sets the source reference on Chart Studio Cloud<<NEWL>>                for `family`.<<NEWL>>            size<<NEWL>><<NEWL>>            sizesrc<<NEWL>>                Sets the source reference on Chart Studio Cloud<<NEWL>>                for `size`.<<NEWL>>"""""",<<NEWL>>            ),<<NEWL>>            **kwargs,<<NEWL>>        )"
270	donghui	1	"import _plotly_utils.basevalidators<<NEWL>><<NEWL>><<NEWL>>class FontValidator(_plotly_utils.basevalidators.CompoundValidator):<<NEWL>>    def __init__(<<NEWL>>        self, plotly_name=""font"", parent_name=""funnelarea.hoverlabel"", **kwargs<<NEWL>>    ):<<NEWL>>        super(FontValidator, self).__init__(<<NEWL>>            plotly_name=plotly_name,<<NEWL>>            parent_name=parent_name,<<NEWL>>            data_class_str=kwargs.pop(""data_class_str"", ""Font""),<<NEWL>>            data_docs=kwargs.pop(<<NEWL>>                ""data_docs"",<<NEWL>>                """"""<<NEWL>>            color<<NEWL>><<NEWL>>            colorsrc<<NEWL>>                Sets the source reference on Chart Studio Cloud<<NEWL>>                for `color`.<<NEWL>>            family<<NEWL>>                HTML font family - the typeface that will be<<NEWL>>                applied by the web browser. The web browser<<NEWL>>                will only be able to apply a font if it is<<NEWL>>                available on the system which it operates.<<NEWL>>                Provide multiple font families, separated by<<NEWL>>                commas, to indicate the preference in which to<<NEWL>>                apply fonts if they aren't available on the<<NEWL>>                system. The Chart Studio Cloud (at<<NEWL>>                https://chart-studio.plotly.com or on-premise)<<NEWL>>                generates images on a server, where only a<<NEWL>>                select number of fonts are installed and<<NEWL>>                supported. These include ""Arial"", ""Balto"",<<NEWL>>                ""Courier New"", ""Droid Sans"",, ""Droid Serif"",<<NEWL>>                ""Droid Sans Mono"", ""Gravitas One"", ""Old<<NEWL>>                Standard TT"", ""Open Sans"", ""Overpass"", ""PT Sans<<NEWL>>                Narrow"", ""Raleway"", ""Times New Roman"".<<NEWL>>            familysrc<<NEWL>>                Sets the source reference on Chart Studio Cloud<<NEWL>>                for `family`.<<NEWL>>            size<<NEWL>><<NEWL>>            sizesrc<<NEWL>>                Sets the source reference on Chart Studio Cloud<<NEWL>>                for `size`.<<NEWL>>"""""",<<NEWL>>            ),<<NEWL>>            **kwargs,<<NEWL>>        )"
361	jackson	3	"#!/usr/bin/env python<<NEWL>><<NEWL>># Copyright 2021 Google LLC<<NEWL>>#<<NEWL>># Licensed under the Apache License, Version 2.0 (the ""License"");<<NEWL>># you may not use this file except in compliance with the License.<<NEWL>># You may obtain a copy of the License at<<NEWL>>#<<NEWL>>#     http://www.apache.org/licenses/LICENSE-2.0<<NEWL>>#<<NEWL>># Unless required by applicable law or agreed to in writing, software<<NEWL>># distributed under the License is distributed on an ""AS IS"" BASIS,<<NEWL>># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.<<NEWL>># See the License for the specific language governing permissions and<<NEWL>># limitations under the License.<<NEWL>><<NEWL>># [START privateca_disable_ca]<<NEWL>>import google.cloud.security.privateca_v1 as privateca_v1<<NEWL>><<NEWL>><<NEWL>>def disable_certificate_authority(<<NEWL>>    project_id: str, location: str, ca_pool_name: str, ca_name: str<<NEWL>>) -> None:<<NEWL>>    """"""<<NEWL>>    Disable a Certificate Authority which is present in the given CA pool.<<NEWL>><<NEWL>>    Args:<<NEWL>>        project_id: project ID or project number of the Cloud project you want to use.<<NEWL>>        location: location you want to use. For a list of locations, see: https://cloud.google.com/certificate-authority-service/docs/locations.<<NEWL>>        ca_pool_name: the name of the CA pool under which the CA is present.<<NEWL>>        ca_name: the name of the CA to be disabled.<<NEWL>>    """"""<<NEWL>><<NEWL>>    caServiceClient = privateca_v1.CertificateAuthorityServiceClient()<<NEWL>>    ca_path = caServiceClient.certificate_authority_path(<<NEWL>>        project_id, location, ca_pool_name, ca_name<<NEWL>>    )<<NEWL>><<NEWL>>    # Create the Disable Certificate Authority Request.<<NEWL>>    request = privateca_v1.DisableCertificateAuthorityRequest(name=ca_path)<<NEWL>><<NEWL>>    # Disable the Certificate Authority.<<NEWL>>    operation = caServiceClient.disable_certificate_authority(request=request)<<NEWL>>    result = operation.result()<<NEWL>><<NEWL>>    print(""Operation result:"", result)<<NEWL>><<NEWL>>    # Get the current CA state.<<NEWL>>    ca_state = caServiceClient.get_certificate_authority(name=ca_path).state<<NEWL>><<NEWL>>    # Check if the CA is disabled.<<NEWL>>    if ca_state == privateca_v1.CertificateAuthority.State.DISABLED:<<NEWL>>        print(""Disabled Certificate Authority:"", ca_name)<<NEWL>>    else:<<NEWL>>        print(""Cannot disable the Certificate Authority ! Current CA State:"", ca_state)<<NEWL>><<NEWL>><<NEWL>># [END privateca_disable_ca]"
361	donghui	3	"#!/usr/bin/env python<<NEWL>><<NEWL>># Copyright 2021 Google LLC<<NEWL>>#<<NEWL>># Licensed under the Apache License, Version 2.0 (the ""License"");<<NEWL>># you may not use this file except in compliance with the License.<<NEWL>># You may obtain a copy of the License at<<NEWL>>#<<NEWL>>#     http://www.apache.org/licenses/LICENSE-2.0<<NEWL>>#<<NEWL>># Unless required by applicable law or agreed to in writing, software<<NEWL>># distributed under the License is distributed on an ""AS IS"" BASIS,<<NEWL>># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.<<NEWL>># See the License for the specific language governing permissions and<<NEWL>># limitations under the License.<<NEWL>><<NEWL>># [START privateca_disable_ca]<<NEWL>>import google.cloud.security.privateca_v1 as privateca_v1<<NEWL>><<NEWL>><<NEWL>>def disable_certificate_authority(<<NEWL>>    project_id: str, location: str, ca_pool_name: str, ca_name: str<<NEWL>>) -> None:<<NEWL>>    """"""<<NEWL>>    Disable a Certificate Authority which is present in the given CA pool.<<NEWL>><<NEWL>>    Args:<<NEWL>>        project_id: project ID or project number of the Cloud project you want to use.<<NEWL>>        location: location you want to use. For a list of locations, see: https://cloud.google.com/certificate-authority-service/docs/locations.<<NEWL>>        ca_pool_name: the name of the CA pool under which the CA is present.<<NEWL>>        ca_name: the name of the CA to be disabled.<<NEWL>>    """"""<<NEWL>><<NEWL>>    caServiceClient = privateca_v1.CertificateAuthorityServiceClient()<<NEWL>>    ca_path = caServiceClient.certificate_authority_path(<<NEWL>>        project_id, location, ca_pool_name, ca_name<<NEWL>>    )<<NEWL>><<NEWL>>    # Create the Disable Certificate Authority Request.<<NEWL>>    request = privateca_v1.DisableCertificateAuthorityRequest(name=ca_path)<<NEWL>><<NEWL>>    # Disable the Certificate Authority.<<NEWL>>    operation = caServiceClient.disable_certificate_authority(request=request)<<NEWL>>    result = operation.result()<<NEWL>><<NEWL>>    print(""Operation result:"", result)<<NEWL>><<NEWL>>    # Get the current CA state.<<NEWL>>    ca_state = caServiceClient.get_certificate_authority(name=ca_path).state<<NEWL>><<NEWL>>    # Check if the CA is disabled.<<NEWL>>    if ca_state == privateca_v1.CertificateAuthority.State.DISABLED:<<NEWL>>        print(""Disabled Certificate Authority:"", ca_name)<<NEWL>>    else:<<NEWL>>        print(""Cannot disable the Certificate Authority ! Current CA State:"", ca_state)<<NEWL>><<NEWL>><<NEWL>># [END privateca_disable_ca]"
392	jackson	1	"from django.core import checks<<NEWL>><<NEWL>>NOT_PROVIDED = object()<<NEWL>><<NEWL>><<NEWL>>class FieldCacheMixin:<<NEWL>>    """"""Provide an API for working with the model's fields value cache.""""""<<NEWL>><<NEWL>>    def get_cache_name(self):<<NEWL>>        raise NotImplementedError<<NEWL>><<NEWL>>    def get_cached_value(self, instance, default=NOT_PROVIDED):<<NEWL>>        cache_name = self.get_cache_name()<<NEWL>>        try:<<NEWL>>            return instance._state.fields_cache[cache_name]<<NEWL>>        except KeyError:<<NEWL>>            if default is NOT_PROVIDED:<<NEWL>>                raise<<NEWL>>            return default<<NEWL>><<NEWL>>    def is_cached(self, instance):<<NEWL>>        return self.get_cache_name() in instance._state.fields_cache<<NEWL>><<NEWL>>    def set_cached_value(self, instance, value):<<NEWL>>        instance._state.fields_cache[self.get_cache_name()] = value<<NEWL>><<NEWL>>    def delete_cached_value(self, instance):<<NEWL>>        del instance._state.fields_cache[self.get_cache_name()]<<NEWL>><<NEWL>><<NEWL>>class CheckFieldDefaultMixin:<<NEWL>>    _default_hint = (""<valid default>"", ""<invalid default>"")<<NEWL>><<NEWL>>    def _check_default(self):<<NEWL>>        if (<<NEWL>>            self.has_default()<<NEWL>>            and self.default is not None<<NEWL>>            and not callable(self.default)<<NEWL>>        ):<<NEWL>>            return [<<NEWL>>                checks.Warning(<<NEWL>>                    ""%s default should be a callable instead of an instance ""<<NEWL>>                    ""so that it's not shared between all field instances.""<<NEWL>>                    % (self.__class__.__name__,),<<NEWL>>                    hint=(<<NEWL>>                        ""Use a callable instead, e.g., use `%s` instead of ""<<NEWL>>                        ""`%s`."" % self._default_hint<<NEWL>>                    ),<<NEWL>>                    obj=self,<<NEWL>>                    id=""fields.E010"",<<NEWL>>                )<<NEWL>>            ]<<NEWL>>        else:<<NEWL>>            return []<<NEWL>><<NEWL>>    def check(self, **kwargs):<<NEWL>>        errors = super().check(**kwargs)<<NEWL>>        errors.extend(self._check_default())<<NEWL>>        return errors"
392	donghui	1	"from django.core import checks<<NEWL>><<NEWL>>NOT_PROVIDED = object()<<NEWL>><<NEWL>><<NEWL>>class FieldCacheMixin:<<NEWL>>    """"""Provide an API for working with the model's fields value cache.""""""<<NEWL>><<NEWL>>    def get_cache_name(self):<<NEWL>>        raise NotImplementedError<<NEWL>><<NEWL>>    def get_cached_value(self, instance, default=NOT_PROVIDED):<<NEWL>>        cache_name = self.get_cache_name()<<NEWL>>        try:<<NEWL>>            return instance._state.fields_cache[cache_name]<<NEWL>>        except KeyError:<<NEWL>>            if default is NOT_PROVIDED:<<NEWL>>                raise<<NEWL>>            return default<<NEWL>><<NEWL>>    def is_cached(self, instance):<<NEWL>>        return self.get_cache_name() in instance._state.fields_cache<<NEWL>><<NEWL>>    def set_cached_value(self, instance, value):<<NEWL>>        instance._state.fields_cache[self.get_cache_name()] = value<<NEWL>><<NEWL>>    def delete_cached_value(self, instance):<<NEWL>>        del instance._state.fields_cache[self.get_cache_name()]<<NEWL>><<NEWL>><<NEWL>>class CheckFieldDefaultMixin:<<NEWL>>    _default_hint = (""<valid default>"", ""<invalid default>"")<<NEWL>><<NEWL>>    def _check_default(self):<<NEWL>>        if (<<NEWL>>            self.has_default()<<NEWL>>            and self.default is not None<<NEWL>>            and not callable(self.default)<<NEWL>>        ):<<NEWL>>            return [<<NEWL>>                checks.Warning(<<NEWL>>                    ""%s default should be a callable instead of an instance ""<<NEWL>>                    ""so that it's not shared between all field instances.""<<NEWL>>                    % (self.__class__.__name__,),<<NEWL>>                    hint=(<<NEWL>>                        ""Use a callable instead, e.g., use `%s` instead of ""<<NEWL>>                        ""`%s`."" % self._default_hint<<NEWL>>                    ),<<NEWL>>                    obj=self,<<NEWL>>                    id=""fields.E010"",<<NEWL>>                )<<NEWL>>            ]<<NEWL>>        else:<<NEWL>>            return []<<NEWL>><<NEWL>>    def check(self, **kwargs):<<NEWL>>        errors = super().check(**kwargs)<<NEWL>>        errors.extend(self._check_default())<<NEWL>>        return errors"
283	jackson	2	"alphabet = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']<<NEWL>><<NEWL>>def caesar(start_text, shift_amount, cipher_direction):<<NEWL>>  end_text = """"<<NEWL>>  if cipher_direction == ""decode"":<<NEWL>>    shift_amount *= -1<<NEWL>>  for char in start_text:<<NEWL>>    #TODO-3: What happens if the user enters a number/symbol/space?<<NEWL>>    #Can you fix the code to keep the number/symbol/space when the text is encoded/decoded?<<NEWL>>    #e.g. start_text = ""meet me at 3""<<NEWL>>    #end_text = ""•••• •• •• 3""<<NEWL>>    if char in alphabet:<<NEWL>>      position = alphabet.index(char)<<NEWL>>      new_position = position + shift_amount<<NEWL>>      end_text += alphabet[new_position]<<NEWL>>    else:<<NEWL>>      end_text += char<<NEWL>>  print(f""Here's the {cipher_direction}d result: {end_text}"")<<NEWL>><<NEWL>>#TODO-1: Import and print the logo from art.py when the program starts.<<NEWL>>from art import logo<<NEWL>>print(logo)<<NEWL>><<NEWL>>#TODO-4: Can you figure out a way to ask the user if they want to restart the cipher program?<<NEWL>>#e.g. Type 'yes' if you want to go again. Otherwise type 'no'.<<NEWL>>#If they type 'yes' then ask them for the direction/text/shift again and call the caesar() function again?<<NEWL>>#Hint: Try creating a while loop that continues to execute the program if the user types 'yes'.<<NEWL>>should_end = False<<NEWL>>while not should_end:<<NEWL>><<NEWL>>  direction = input(""Type 'encode' to encrypt, type 'decode' to decrypt:\n"")<<NEWL>>  text = input(""Type your message:\n"").lower()<<NEWL>>  shift = int(input(""Type the shift number:\n""))<<NEWL>>  #TODO-2: What if the user enters a shift that is greater than the number of letters in the alphabet?<<NEWL>>  #Try running the program and entering a shift number of 45.<<NEWL>>  #Add some code so that the program continues to work even if the user enters a shift number greater than 26. <<NEWL>>  #Hint: Think about how you can use the modulus (%).<<NEWL>>  shift = shift % 26<<NEWL>><<NEWL>>  caesar(start_text=text, shift_amount=shift, cipher_direction=direction)<<NEWL>><<NEWL>>  restart = input(""Type 'yes' if you want to go again. Otherwise type 'no'.\n"")<<NEWL>>  if restart == ""no"":<<NEWL>>    should_end = True<<NEWL>>    print(""Goodbye"")<<NEWL>>    "
283	donghui	3	"alphabet = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']<<NEWL>><<NEWL>>def caesar(start_text, shift_amount, cipher_direction):<<NEWL>>  end_text = """"<<NEWL>>  if cipher_direction == ""decode"":<<NEWL>>    shift_amount *= -1<<NEWL>>  for char in start_text:<<NEWL>>    #TODO-3: What happens if the user enters a number/symbol/space?<<NEWL>>    #Can you fix the code to keep the number/symbol/space when the text is encoded/decoded?<<NEWL>>    #e.g. start_text = ""meet me at 3""<<NEWL>>    #end_text = ""•••• •• •• 3""<<NEWL>>    if char in alphabet:<<NEWL>>      position = alphabet.index(char)<<NEWL>>      new_position = position + shift_amount<<NEWL>>      end_text += alphabet[new_position]<<NEWL>>    else:<<NEWL>>      end_text += char<<NEWL>>  print(f""Here's the {cipher_direction}d result: {end_text}"")<<NEWL>><<NEWL>>#TODO-1: Import and print the logo from art.py when the program starts.<<NEWL>>from art import logo<<NEWL>>print(logo)<<NEWL>><<NEWL>>#TODO-4: Can you figure out a way to ask the user if they want to restart the cipher program?<<NEWL>>#e.g. Type 'yes' if you want to go again. Otherwise type 'no'.<<NEWL>>#If they type 'yes' then ask them for the direction/text/shift again and call the caesar() function again?<<NEWL>>#Hint: Try creating a while loop that continues to execute the program if the user types 'yes'.<<NEWL>>should_end = False<<NEWL>>while not should_end:<<NEWL>><<NEWL>>  direction = input(""Type 'encode' to encrypt, type 'decode' to decrypt:\n"")<<NEWL>>  text = input(""Type your message:\n"").lower()<<NEWL>>  shift = int(input(""Type the shift number:\n""))<<NEWL>>  #TODO-2: What if the user enters a shift that is greater than the number of letters in the alphabet?<<NEWL>>  #Try running the program and entering a shift number of 45.<<NEWL>>  #Add some code so that the program continues to work even if the user enters a shift number greater than 26. <<NEWL>>  #Hint: Think about how you can use the modulus (%).<<NEWL>>  shift = shift % 26<<NEWL>><<NEWL>>  caesar(start_text=text, shift_amount=shift, cipher_direction=direction)<<NEWL>><<NEWL>>  restart = input(""Type 'yes' if you want to go again. Otherwise type 'no'.\n"")<<NEWL>>  if restart == ""no"":<<NEWL>>    should_end = True<<NEWL>>    print(""Goodbye"")<<NEWL>>    "
355	jackson	2	import lightly.data as data<<NEWL>><<NEWL>># the collate function applies random transforms to the input images<<NEWL>>collate_fn = data.ImageCollateFunction(input_size=32, cj_prob=0.5)<<NEWL>>import torch<<NEWL>><<NEWL>># create a dataset from your image folder<<NEWL>>dataset = data.LightlyDataset(input_dir='./my/cute/cats/dataset/')<<NEWL>><<NEWL>># build a PyTorch dataloader<<NEWL>>dataloader = torch.utils.data.DataLoader(<<NEWL>>    dataset,                # pass the dataset to the dataloader<<NEWL>>    batch_size=128,         # a large batch size helps with the learning<<NEWL>>    shuffle=True,           # shuffling is important!<<NEWL>>    collate_fn=collate_fn)  # apply transformations to the input images<<NEWL>>import torchvision<<NEWL>><<NEWL>>from lightly.loss import NTXentLoss<<NEWL>>from lightly.models.modules.heads import SimCLRProjectionHead<<NEWL>><<NEWL>># use a resnet backbone<<NEWL>>resnet = torchvision.models.resnext101_64x4d()  # or efficientnet0_b0<<NEWL>>resnet = torch.nn.Sequential(*list(resnet.children())[:-1])<<NEWL>><<NEWL>># build a SimCLR model<<NEWL>>class SimCLR(torch.nn.Module):<<NEWL>>    def __init__(self, backbone, hidden_dim, out_dim):<<NEWL>>        super().__init__()<<NEWL>>        self.backbone = backbone<<NEWL>>        self.projection_head = SimCLRProjectionHead(hidden_dim, hidden_dim, out_dim)<<NEWL>><<NEWL>>    def forward(self, x):<<NEWL>>        h = self.backbone(x).flatten(start_dim=1)<<NEWL>>        z = self.projection_head(h)<<NEWL>>        return z<<NEWL>><<NEWL>>model = SimCLR(resnet, hidden_dim=512, out_dim=128)<<NEWL>><<NEWL>># use a criterion for self-supervised learning<<NEWL>># (normalized temperature-scaled cross entropy loss)<<NEWL>>criterion = NTXentLoss(temperature=0.5)<<NEWL>><<NEWL>># get a PyTorch optimizer<<NEWL>>optimizer = torch.optim.SGD(model.parameters(), lr=1e-0, weight_decay=1e-5)<<NEWL>>device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')<<NEWL>>max_epochs = 10<<NEWL>>for epoch in range(max_epochs):<<NEWL>>    for (x0, x1), _, _ in dataloader:<<NEWL>><<NEWL>>        x0 = x0.to(device)<<NEWL>>        x1 = x1.to(device)<<NEWL>><<NEWL>>        z0 = model(x0)<<NEWL>>        z1 = model(x1)<<NEWL>><<NEWL>>        loss = criterion(z0, z1)<<NEWL>>        loss.backward()<<NEWL>><<NEWL>>        optimizer.step()<<NEWL>>        optimizer.zero_grad()
355	donghui	2	import lightly.data as data<<NEWL>><<NEWL>># the collate function applies random transforms to the input images<<NEWL>>collate_fn = data.ImageCollateFunction(input_size=32, cj_prob=0.5)<<NEWL>>import torch<<NEWL>><<NEWL>># create a dataset from your image folder<<NEWL>>dataset = data.LightlyDataset(input_dir='./my/cute/cats/dataset/')<<NEWL>><<NEWL>># build a PyTorch dataloader<<NEWL>>dataloader = torch.utils.data.DataLoader(<<NEWL>>    dataset,                # pass the dataset to the dataloader<<NEWL>>    batch_size=128,         # a large batch size helps with the learning<<NEWL>>    shuffle=True,           # shuffling is important!<<NEWL>>    collate_fn=collate_fn)  # apply transformations to the input images<<NEWL>>import torchvision<<NEWL>><<NEWL>>from lightly.loss import NTXentLoss<<NEWL>>from lightly.models.modules.heads import SimCLRProjectionHead<<NEWL>><<NEWL>># use a resnet backbone<<NEWL>>resnet = torchvision.models.resnext101_64x4d()  # or efficientnet0_b0<<NEWL>>resnet = torch.nn.Sequential(*list(resnet.children())[:-1])<<NEWL>><<NEWL>># build a SimCLR model<<NEWL>>class SimCLR(torch.nn.Module):<<NEWL>>    def __init__(self, backbone, hidden_dim, out_dim):<<NEWL>>        super().__init__()<<NEWL>>        self.backbone = backbone<<NEWL>>        self.projection_head = SimCLRProjectionHead(hidden_dim, hidden_dim, out_dim)<<NEWL>><<NEWL>>    def forward(self, x):<<NEWL>>        h = self.backbone(x).flatten(start_dim=1)<<NEWL>>        z = self.projection_head(h)<<NEWL>>        return z<<NEWL>><<NEWL>>model = SimCLR(resnet, hidden_dim=512, out_dim=128)<<NEWL>><<NEWL>># use a criterion for self-supervised learning<<NEWL>># (normalized temperature-scaled cross entropy loss)<<NEWL>>criterion = NTXentLoss(temperature=0.5)<<NEWL>><<NEWL>># get a PyTorch optimizer<<NEWL>>optimizer = torch.optim.SGD(model.parameters(), lr=1e-0, weight_decay=1e-5)<<NEWL>>device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')<<NEWL>>max_epochs = 10<<NEWL>>for epoch in range(max_epochs):<<NEWL>>    for (x0, x1), _, _ in dataloader:<<NEWL>><<NEWL>>        x0 = x0.to(device)<<NEWL>>        x1 = x1.to(device)<<NEWL>><<NEWL>>        z0 = model(x0)<<NEWL>>        z1 = model(x1)<<NEWL>><<NEWL>>        loss = criterion(z0, z1)<<NEWL>>        loss.backward()<<NEWL>><<NEWL>>        optimizer.step()<<NEWL>>        optimizer.zero_grad()
304	jackson	3	"# Copyright (c) Facebook, Inc. and its affiliates.<<NEWL>>#<<NEWL>># This source code is licensed under the MIT license found in the<<NEWL>># LICENSE file in the root directory of this source tree.<<NEWL>>""""""isort:skip_file""""""<<NEWL>><<NEWL>>import functools<<NEWL>>import importlib<<NEWL>><<NEWL>><<NEWL>>dependencies = [<<NEWL>>    ""dataclasses"",<<NEWL>>    ""hydra"",<<NEWL>>    ""numpy"",<<NEWL>>    ""omegaconf"",<<NEWL>>    ""regex"",<<NEWL>>    ""requests"",<<NEWL>>    ""torch"",<<NEWL>>]<<NEWL>><<NEWL>><<NEWL>># Check for required dependencies and raise a RuntimeError if any are missing.<<NEWL>>missing_deps = []<<NEWL>>for dep in dependencies:<<NEWL>>    try:<<NEWL>>        importlib.import_module(dep)<<NEWL>>    except ImportError:<<NEWL>>        # Hack: the hydra package is provided under the ""hydra-core"" name in<<NEWL>>        # pypi. We don't want the user mistakenly calling `pip install hydra`<<NEWL>>        # since that will install an unrelated package.<<NEWL>>        if dep == ""hydra"":<<NEWL>>            dep = ""hydra-core""<<NEWL>>        missing_deps.append(dep)<<NEWL>>if len(missing_deps) > 0:<<NEWL>>    raise RuntimeError(""Missing dependencies: {}"".format("", "".join(missing_deps)))<<NEWL>><<NEWL>><<NEWL>># only do fairseq imports after checking for dependencies<<NEWL>>from fairseq.hub_utils import (  # noqa; noqa<<NEWL>>    BPEHubInterface as bpe,<<NEWL>>    TokenizerHubInterface as tokenizer,<<NEWL>>)<<NEWL>>from fairseq.models import MODEL_REGISTRY  # noqa<<NEWL>><<NEWL>><<NEWL>># torch.hub doesn't build Cython components, so if they are not found then try<<NEWL>># to build them here<<NEWL>>try:<<NEWL>>    import fairseq.data.token_block_utils_fast  # noqa<<NEWL>>except ImportError:<<NEWL>>    try:<<NEWL>>        import cython  # noqa<<NEWL>>        import os<<NEWL>>        from setuptools import sandbox<<NEWL>><<NEWL>>        sandbox.run_setup(<<NEWL>>            os.path.join(os.path.dirname(__file__), ""setup.py""),<<NEWL>>            [""build_ext"", ""--inplace""],<<NEWL>>        )<<NEWL>>    except ImportError:<<NEWL>>        print(<<NEWL>>            ""Unable to build Cython components. Please make sure Cython is ""<<NEWL>>            ""installed if the torch.hub model you are loading depends on it.""<<NEWL>>        )<<NEWL>><<NEWL>><<NEWL>># automatically expose models defined in FairseqModel::hub_models<<NEWL>>for _model_type, _cls in MODEL_REGISTRY.items():<<NEWL>>    for model_name in _cls.hub_models().keys():<<NEWL>>        globals()[model_name] = functools.partial(<<NEWL>>            _cls.from_pretrained,<<NEWL>>            model_name,<<NEWL>>        )"
304	donghui	3	"# Copyright (c) Facebook, Inc. and its affiliates.<<NEWL>>#<<NEWL>># This source code is licensed under the MIT license found in the<<NEWL>># LICENSE file in the root directory of this source tree.<<NEWL>>""""""isort:skip_file""""""<<NEWL>><<NEWL>>import functools<<NEWL>>import importlib<<NEWL>><<NEWL>><<NEWL>>dependencies = [<<NEWL>>    ""dataclasses"",<<NEWL>>    ""hydra"",<<NEWL>>    ""numpy"",<<NEWL>>    ""omegaconf"",<<NEWL>>    ""regex"",<<NEWL>>    ""requests"",<<NEWL>>    ""torch"",<<NEWL>>]<<NEWL>><<NEWL>><<NEWL>># Check for required dependencies and raise a RuntimeError if any are missing.<<NEWL>>missing_deps = []<<NEWL>>for dep in dependencies:<<NEWL>>    try:<<NEWL>>        importlib.import_module(dep)<<NEWL>>    except ImportError:<<NEWL>>        # Hack: the hydra package is provided under the ""hydra-core"" name in<<NEWL>>        # pypi. We don't want the user mistakenly calling `pip install hydra`<<NEWL>>        # since that will install an unrelated package.<<NEWL>>        if dep == ""hydra"":<<NEWL>>            dep = ""hydra-core""<<NEWL>>        missing_deps.append(dep)<<NEWL>>if len(missing_deps) > 0:<<NEWL>>    raise RuntimeError(""Missing dependencies: {}"".format("", "".join(missing_deps)))<<NEWL>><<NEWL>><<NEWL>># only do fairseq imports after checking for dependencies<<NEWL>>from fairseq.hub_utils import (  # noqa; noqa<<NEWL>>    BPEHubInterface as bpe,<<NEWL>>    TokenizerHubInterface as tokenizer,<<NEWL>>)<<NEWL>>from fairseq.models import MODEL_REGISTRY  # noqa<<NEWL>><<NEWL>><<NEWL>># torch.hub doesn't build Cython components, so if they are not found then try<<NEWL>># to build them here<<NEWL>>try:<<NEWL>>    import fairseq.data.token_block_utils_fast  # noqa<<NEWL>>except ImportError:<<NEWL>>    try:<<NEWL>>        import cython  # noqa<<NEWL>>        import os<<NEWL>>        from setuptools import sandbox<<NEWL>><<NEWL>>        sandbox.run_setup(<<NEWL>>            os.path.join(os.path.dirname(__file__), ""setup.py""),<<NEWL>>            [""build_ext"", ""--inplace""],<<NEWL>>        )<<NEWL>>    except ImportError:<<NEWL>>        print(<<NEWL>>            ""Unable to build Cython components. Please make sure Cython is ""<<NEWL>>            ""installed if the torch.hub model you are loading depends on it.""<<NEWL>>        )<<NEWL>><<NEWL>><<NEWL>># automatically expose models defined in FairseqModel::hub_models<<NEWL>>for _model_type, _cls in MODEL_REGISTRY.items():<<NEWL>>    for model_name in _cls.hub_models().keys():<<NEWL>>        globals()[model_name] = functools.partial(<<NEWL>>            _cls.from_pretrained,<<NEWL>>            model_name,<<NEWL>>        )"
314	jackson	3	"from abc import ABCMeta, abstractmethod<<NEWL>><<NEWL>><<NEWL>>class CacheAdapter:<<NEWL>>    """"""<<NEWL>>    CacheAdapter Abstract Base Class<<NEWL>>    """"""<<NEWL>>    __metaclass__ = ABCMeta<<NEWL>><<NEWL>>    @abstractmethod<<NEWL>>    def get(self, public_id, type, resource_type, transformation, format):<<NEWL>>        """"""<<NEWL>>        Gets value specified by parameters<<NEWL>><<NEWL>>        :param public_id:       The public ID of the resource<<NEWL>>        :param type:            The storage type<<NEWL>>        :param resource_type:   The type of the resource<<NEWL>>        :param transformation:  The transformation string<<NEWL>>        :param format:          The format of the resource<<NEWL>><<NEWL>>        :return: None|mixed value, None if not found<<NEWL>>        """"""<<NEWL>>        raise NotImplementedError<<NEWL>><<NEWL>>    @abstractmethod<<NEWL>>    def set(self, public_id, type, resource_type, transformation, format, value):<<NEWL>>        """"""<<NEWL>>        Sets value specified by parameters<<NEWL>><<NEWL>>        :param public_id:       The public ID of the resource<<NEWL>>        :param type:            The storage type<<NEWL>>        :param resource_type:   The type of the resource<<NEWL>>        :param transformation:  The transformation string<<NEWL>>        :param format:          The format of the resource<<NEWL>>        :param value:           The value to set<<NEWL>><<NEWL>>        :return: bool True on success or False on failure<<NEWL>>        """"""<<NEWL>>        raise NotImplementedError<<NEWL>><<NEWL>>    @abstractmethod<<NEWL>>    def delete(self, public_id, type, resource_type, transformation, format):<<NEWL>>        """"""<<NEWL>>        Deletes entry specified by parameters<<NEWL>><<NEWL>>        :param public_id:       The public ID of the resource<<NEWL>>        :param type:            The storage type<<NEWL>>        :param resource_type:   The type of the resource<<NEWL>>        :param transformation:  The transformation string<<NEWL>>        :param format:          The format of the resource<<NEWL>><<NEWL>>        :return: bool True on success or False on failure<<NEWL>>        """"""<<NEWL>>        raise NotImplementedError<<NEWL>><<NEWL>>    @abstractmethod<<NEWL>>    def flush_all(self):<<NEWL>>        """"""<<NEWL>>        Flushes all entries from cache<<NEWL>><<NEWL>>        :return: bool True on success or False on failure<<NEWL>>        """"""<<NEWL>>        raise NotImplementedError"
314	donghui	3	"from abc import ABCMeta, abstractmethod<<NEWL>><<NEWL>><<NEWL>>class CacheAdapter:<<NEWL>>    """"""<<NEWL>>    CacheAdapter Abstract Base Class<<NEWL>>    """"""<<NEWL>>    __metaclass__ = ABCMeta<<NEWL>><<NEWL>>    @abstractmethod<<NEWL>>    def get(self, public_id, type, resource_type, transformation, format):<<NEWL>>        """"""<<NEWL>>        Gets value specified by parameters<<NEWL>><<NEWL>>        :param public_id:       The public ID of the resource<<NEWL>>        :param type:            The storage type<<NEWL>>        :param resource_type:   The type of the resource<<NEWL>>        :param transformation:  The transformation string<<NEWL>>        :param format:          The format of the resource<<NEWL>><<NEWL>>        :return: None|mixed value, None if not found<<NEWL>>        """"""<<NEWL>>        raise NotImplementedError<<NEWL>><<NEWL>>    @abstractmethod<<NEWL>>    def set(self, public_id, type, resource_type, transformation, format, value):<<NEWL>>        """"""<<NEWL>>        Sets value specified by parameters<<NEWL>><<NEWL>>        :param public_id:       The public ID of the resource<<NEWL>>        :param type:            The storage type<<NEWL>>        :param resource_type:   The type of the resource<<NEWL>>        :param transformation:  The transformation string<<NEWL>>        :param format:          The format of the resource<<NEWL>>        :param value:           The value to set<<NEWL>><<NEWL>>        :return: bool True on success or False on failure<<NEWL>>        """"""<<NEWL>>        raise NotImplementedError<<NEWL>><<NEWL>>    @abstractmethod<<NEWL>>    def delete(self, public_id, type, resource_type, transformation, format):<<NEWL>>        """"""<<NEWL>>        Deletes entry specified by parameters<<NEWL>><<NEWL>>        :param public_id:       The public ID of the resource<<NEWL>>        :param type:            The storage type<<NEWL>>        :param resource_type:   The type of the resource<<NEWL>>        :param transformation:  The transformation string<<NEWL>>        :param format:          The format of the resource<<NEWL>><<NEWL>>        :return: bool True on success or False on failure<<NEWL>>        """"""<<NEWL>>        raise NotImplementedError<<NEWL>><<NEWL>>    @abstractmethod<<NEWL>>    def flush_all(self):<<NEWL>>        """"""<<NEWL>>        Flushes all entries from cache<<NEWL>><<NEWL>>        :return: bool True on success or False on failure<<NEWL>>        """"""<<NEWL>>        raise NotImplementedError"
254	jackson	3	"import pytest<<NEWL>><<NEWL>>from pandas import (<<NEWL>>    CategoricalIndex,<<NEWL>>    Index,<<NEWL>>)<<NEWL>>import pandas._testing as tm<<NEWL>><<NEWL>><<NEWL>>class TestAppend:<<NEWL>>    @pytest.fixture<<NEWL>>    def ci(self):<<NEWL>>        categories = list(""cab"")<<NEWL>>        return CategoricalIndex(list(""aabbca""), categories=categories, ordered=False)<<NEWL>><<NEWL>>    def test_append(self, ci):<<NEWL>>        # append cats with the same categories<<NEWL>>        result = ci[:3].append(ci[3:])<<NEWL>>        tm.assert_index_equal(result, ci, exact=True)<<NEWL>><<NEWL>>        foos = [ci[:1], ci[1:3], ci[3:]]<<NEWL>>        result = foos[0].append(foos[1:])<<NEWL>>        tm.assert_index_equal(result, ci, exact=True)<<NEWL>><<NEWL>>    def test_append_empty(self, ci):<<NEWL>>        # empty<<NEWL>>        result = ci.append([])<<NEWL>>        tm.assert_index_equal(result, ci, exact=True)<<NEWL>><<NEWL>>    def test_append_mismatched_categories(self, ci):<<NEWL>>        # appending with different categories or reordered is not ok<<NEWL>>        msg = ""all inputs must be Index""<<NEWL>>        with pytest.raises(TypeError, match=msg):<<NEWL>>            ci.append(ci.values.set_categories(list(""abcd"")))<<NEWL>>        with pytest.raises(TypeError, match=msg):<<NEWL>>            ci.append(ci.values.reorder_categories(list(""abc"")))<<NEWL>><<NEWL>>    def test_append_category_objects(self, ci):<<NEWL>>        # with objects<<NEWL>>        result = ci.append(Index([""c"", ""a""]))<<NEWL>>        expected = CategoricalIndex(list(""aabbcaca""), categories=ci.categories)<<NEWL>>        tm.assert_index_equal(result, expected, exact=True)<<NEWL>><<NEWL>>    def test_append_non_categories(self, ci):<<NEWL>>        # invalid objects -> cast to object via concat_compat<<NEWL>>        result = ci.append(Index([""a"", ""d""]))<<NEWL>>        expected = Index([""a"", ""a"", ""b"", ""b"", ""c"", ""a"", ""a"", ""d""])<<NEWL>>        tm.assert_index_equal(result, expected, exact=True)<<NEWL>><<NEWL>>    def test_append_object(self, ci):<<NEWL>>        # GH#14298 - if base object is not categorical -> coerce to object<<NEWL>>        result = Index([""c"", ""a""]).append(ci)<<NEWL>>        expected = Index(list(""caaabbca""))<<NEWL>>        tm.assert_index_equal(result, expected, exact=True)<<NEWL>><<NEWL>>    def test_append_to_another(self):<<NEWL>>        # hits Index._concat<<NEWL>>        fst = Index([""a"", ""b""])<<NEWL>>        snd = CategoricalIndex([""d"", ""e""])<<NEWL>>        result = fst.append(snd)<<NEWL>>        expected = Index([""a"", ""b"", ""d"", ""e""])<<NEWL>>        tm.assert_index_equal(result, expected)"
254	donghui	2	"import pytest<<NEWL>><<NEWL>>from pandas import (<<NEWL>>    CategoricalIndex,<<NEWL>>    Index,<<NEWL>>)<<NEWL>>import pandas._testing as tm<<NEWL>><<NEWL>><<NEWL>>class TestAppend:<<NEWL>>    @pytest.fixture<<NEWL>>    def ci(self):<<NEWL>>        categories = list(""cab"")<<NEWL>>        return CategoricalIndex(list(""aabbca""), categories=categories, ordered=False)<<NEWL>><<NEWL>>    def test_append(self, ci):<<NEWL>>        # append cats with the same categories<<NEWL>>        result = ci[:3].append(ci[3:])<<NEWL>>        tm.assert_index_equal(result, ci, exact=True)<<NEWL>><<NEWL>>        foos = [ci[:1], ci[1:3], ci[3:]]<<NEWL>>        result = foos[0].append(foos[1:])<<NEWL>>        tm.assert_index_equal(result, ci, exact=True)<<NEWL>><<NEWL>>    def test_append_empty(self, ci):<<NEWL>>        # empty<<NEWL>>        result = ci.append([])<<NEWL>>        tm.assert_index_equal(result, ci, exact=True)<<NEWL>><<NEWL>>    def test_append_mismatched_categories(self, ci):<<NEWL>>        # appending with different categories or reordered is not ok<<NEWL>>        msg = ""all inputs must be Index""<<NEWL>>        with pytest.raises(TypeError, match=msg):<<NEWL>>            ci.append(ci.values.set_categories(list(""abcd"")))<<NEWL>>        with pytest.raises(TypeError, match=msg):<<NEWL>>            ci.append(ci.values.reorder_categories(list(""abc"")))<<NEWL>><<NEWL>>    def test_append_category_objects(self, ci):<<NEWL>>        # with objects<<NEWL>>        result = ci.append(Index([""c"", ""a""]))<<NEWL>>        expected = CategoricalIndex(list(""aabbcaca""), categories=ci.categories)<<NEWL>>        tm.assert_index_equal(result, expected, exact=True)<<NEWL>><<NEWL>>    def test_append_non_categories(self, ci):<<NEWL>>        # invalid objects -> cast to object via concat_compat<<NEWL>>        result = ci.append(Index([""a"", ""d""]))<<NEWL>>        expected = Index([""a"", ""a"", ""b"", ""b"", ""c"", ""a"", ""a"", ""d""])<<NEWL>>        tm.assert_index_equal(result, expected, exact=True)<<NEWL>><<NEWL>>    def test_append_object(self, ci):<<NEWL>>        # GH#14298 - if base object is not categorical -> coerce to object<<NEWL>>        result = Index([""c"", ""a""]).append(ci)<<NEWL>>        expected = Index(list(""caaabbca""))<<NEWL>>        tm.assert_index_equal(result, expected, exact=True)<<NEWL>><<NEWL>>    def test_append_to_another(self):<<NEWL>>        # hits Index._concat<<NEWL>>        fst = Index([""a"", ""b""])<<NEWL>>        snd = CategoricalIndex([""d"", ""e""])<<NEWL>>        result = fst.append(snd)<<NEWL>>        expected = Index([""a"", ""b"", ""d"", ""e""])<<NEWL>>        tm.assert_index_equal(result, expected)"
345	jackson	0	"import pygame as pg<<NEWL>><<NEWL>>class Launch:<<NEWL>><<NEWL>>    def __init__(self, game):<<NEWL>>        self.game = game<<NEWL>>        self.screen = game.screen<<NEWL>>        self.settings = game.settings<<NEWL>>        self.screen_rect = self.screen.get_rect()<<NEWL>>        self.images = []<<NEWL>>        self.default_color = (255, 255, 255)<<NEWL>>        self.prep_strings()<<NEWL>>        self.prep_aliens()<<NEWL>><<NEWL>>    <<NEWL>>    def prep_strings(self):<<NEWL>>        self.prep_Text(""SPACE"", 170, offsetY=40)<<NEWL>>        self.prep_Text(""INVADERS"", 90, color=(0,210,0), offsetY=140)<<NEWL>>        self.prep_Text(""= 10 PTS"", 40, offsetX=600, offsetY=300)<<NEWL>>        self.prep_Text(""= 20 PTS"", 40, offsetX=600, offsetY=350)<<NEWL>>        self.prep_Text(""= 40 PTS"", 40, offsetX=600, offsetY=400)<<NEWL>>        self.prep_Text(""= ???"", 40, offsetX=610, offsetY=450)<<NEWL>>    <<NEWL>>    def prep_aliens(self):<<NEWL>>        alien1 = pg.transform.rotozoom(pg.image.load(f'images/alien_03-0.png'), 0, 1.5)<<NEWL>>        self.images.append((alien1, (540, 290)))<<NEWL>>        alien1 = pg.transform.rotozoom(pg.image.load(f'images/alien__10.png'), 0, .5)<<NEWL>>        self.images.append((alien1, (525, 340)))<<NEWL>>        alien1 = pg.transform.rotozoom(pg.image.load(f'images/alien__20.png'), 0, .5)<<NEWL>>        self.images.append((alien1, (525, 390)))<<NEWL>>        alien1 = pg.transform.rotozoom(pg.image.load(f'images/ufo.png'), 0, 1.2)<<NEWL>>        self.images.append((alien1, (500, 410)))<<NEWL>><<NEWL>>    def prep_Text(self, msg, size, color=(255,255,255), offsetX=0, offsetY=0):<<NEWL>>        font = pg.font.SysFont(None, size)<<NEWL>>        text_image = font.render(msg, True, color, self.settings.bg_color)<<NEWL>>        rect = text_image.get_rect()<<NEWL>>        if offsetY == 0:<<NEWL>>            rect.centery = self.screen_rect.centery<<NEWL>>        else:<<NEWL>>            rect.top = offsetY<<NEWL>>        if offsetX == 0:<<NEWL>>            rect.centerx = self.screen_rect.centerx<<NEWL>>        else:<<NEWL>>            rect.left = offsetX<<NEWL>><<NEWL>>        self.images.append((text_image,rect))<<NEWL>><<NEWL>>    def draw(self):<<NEWL>>        for image in self.images:<<NEWL>>            self.screen.blit(image[0], image[1])"
345	donghui	0	"import pygame as pg<<NEWL>><<NEWL>>class Launch:<<NEWL>><<NEWL>>    def __init__(self, game):<<NEWL>>        self.game = game<<NEWL>>        self.screen = game.screen<<NEWL>>        self.settings = game.settings<<NEWL>>        self.screen_rect = self.screen.get_rect()<<NEWL>>        self.images = []<<NEWL>>        self.default_color = (255, 255, 255)<<NEWL>>        self.prep_strings()<<NEWL>>        self.prep_aliens()<<NEWL>><<NEWL>>    <<NEWL>>    def prep_strings(self):<<NEWL>>        self.prep_Text(""SPACE"", 170, offsetY=40)<<NEWL>>        self.prep_Text(""INVADERS"", 90, color=(0,210,0), offsetY=140)<<NEWL>>        self.prep_Text(""= 10 PTS"", 40, offsetX=600, offsetY=300)<<NEWL>>        self.prep_Text(""= 20 PTS"", 40, offsetX=600, offsetY=350)<<NEWL>>        self.prep_Text(""= 40 PTS"", 40, offsetX=600, offsetY=400)<<NEWL>>        self.prep_Text(""= ???"", 40, offsetX=610, offsetY=450)<<NEWL>>    <<NEWL>>    def prep_aliens(self):<<NEWL>>        alien1 = pg.transform.rotozoom(pg.image.load(f'images/alien_03-0.png'), 0, 1.5)<<NEWL>>        self.images.append((alien1, (540, 290)))<<NEWL>>        alien1 = pg.transform.rotozoom(pg.image.load(f'images/alien__10.png'), 0, .5)<<NEWL>>        self.images.append((alien1, (525, 340)))<<NEWL>>        alien1 = pg.transform.rotozoom(pg.image.load(f'images/alien__20.png'), 0, .5)<<NEWL>>        self.images.append((alien1, (525, 390)))<<NEWL>>        alien1 = pg.transform.rotozoom(pg.image.load(f'images/ufo.png'), 0, 1.2)<<NEWL>>        self.images.append((alien1, (500, 410)))<<NEWL>><<NEWL>>    def prep_Text(self, msg, size, color=(255,255,255), offsetX=0, offsetY=0):<<NEWL>>        font = pg.font.SysFont(None, size)<<NEWL>>        text_image = font.render(msg, True, color, self.settings.bg_color)<<NEWL>>        rect = text_image.get_rect()<<NEWL>>        if offsetY == 0:<<NEWL>>            rect.centery = self.screen_rect.centery<<NEWL>>        else:<<NEWL>>            rect.top = offsetY<<NEWL>>        if offsetX == 0:<<NEWL>>            rect.centerx = self.screen_rect.centerx<<NEWL>>        else:<<NEWL>>            rect.left = offsetX<<NEWL>><<NEWL>>        self.images.append((text_image,rect))<<NEWL>><<NEWL>>    def draw(self):<<NEWL>>        for image in self.images:<<NEWL>>            self.screen.blit(image[0], image[1])"
293	jackson	4	"""""""eCommerce URL Configuration<<NEWL>><<NEWL>>The `urlpatterns` list routes URLs to views. For more information please see:<<NEWL>>    https://docs.djangoproject.com/en/4.1/topics/http/urls/<<NEWL>>Examples:<<NEWL>>Function views<<NEWL>>    1. Add an import:  from my_app import views<<NEWL>>    2. Add a URL to urlpatterns:  path('', views.home, name='home')<<NEWL>>Class-based views<<NEWL>>    1. Add an import:  from other_app.views import Home<<NEWL>>    2. Add a URL to urlpatterns:  path('', Home.as_view(), name='home')<<NEWL>>Including another URLconf<<NEWL>>    1. Import the include() function: from django.urls import include, path<<NEWL>>    2. Add a URL to urlpatterns:  path('blog/', include('blog.urls'))<<NEWL>>""""""<<NEWL>>from django.contrib import admin<<NEWL>>from django.urls import path,include<<NEWL>>from crud import views<<NEWL>><<NEWL>>from django.conf import settings<<NEWL>>from django.conf.urls.static import static<<NEWL>><<NEWL>>urlpatterns = [<<NEWL>>    path('ckeditor/',include('ckeditor_uploader.urls')),<<NEWL>>    path('admin/', admin.site.urls),<<NEWL>>    path('',views.homepage,name=""homepage""),<<NEWL>>    path('home/',views.homepage,name=""homepage""),<<NEWL>>    path('about/',views.about,name=""about""),<<NEWL>>    path('contact/',views.contact,name=""contact""),<<NEWL>>    path('product/',views.product,name=""product""),<<NEWL>>    path('signup/',views.user_signup,name=""user_signup""),<<NEWL>>    path('signin/',views.user_login,name=""user_login""),<<NEWL>>    path('dashboard/',views.dashboard,name=""dashboard""),<<NEWL>>    path('cart/',views.cart,name=""cart""),<<NEWL>>    path('addpost/',views.addpost,name=""addpost""),<<NEWL>>    path('addcart/<str:title>',views.addcart,name=""addcart""),<<NEWL>>    path('updatepost/<int:id>',views.updatepost,name=""updatepost""),<<NEWL>>    path('deletepost/<int:id>',views.deletepost,name=""deletepost""),<<NEWL>>    path('logout/',views.user_logout,name=""logout""),<<NEWL>>    path('oauth/', include('social_django.urls', namespace='social')),<<NEWL>>]<<NEWL>><<NEWL>>if settings.DEBUG:<<NEWL>>    urlpatterns += static(settings.MEDIA_URL,document_root=settings.MEDIA_ROOT)"
293	donghui	3	"""""""eCommerce URL Configuration<<NEWL>><<NEWL>>The `urlpatterns` list routes URLs to views. For more information please see:<<NEWL>>    https://docs.djangoproject.com/en/4.1/topics/http/urls/<<NEWL>>Examples:<<NEWL>>Function views<<NEWL>>    1. Add an import:  from my_app import views<<NEWL>>    2. Add a URL to urlpatterns:  path('', views.home, name='home')<<NEWL>>Class-based views<<NEWL>>    1. Add an import:  from other_app.views import Home<<NEWL>>    2. Add a URL to urlpatterns:  path('', Home.as_view(), name='home')<<NEWL>>Including another URLconf<<NEWL>>    1. Import the include() function: from django.urls import include, path<<NEWL>>    2. Add a URL to urlpatterns:  path('blog/', include('blog.urls'))<<NEWL>>""""""<<NEWL>>from django.contrib import admin<<NEWL>>from django.urls import path,include<<NEWL>>from crud import views<<NEWL>><<NEWL>>from django.conf import settings<<NEWL>>from django.conf.urls.static import static<<NEWL>><<NEWL>>urlpatterns = [<<NEWL>>    path('ckeditor/',include('ckeditor_uploader.urls')),<<NEWL>>    path('admin/', admin.site.urls),<<NEWL>>    path('',views.homepage,name=""homepage""),<<NEWL>>    path('home/',views.homepage,name=""homepage""),<<NEWL>>    path('about/',views.about,name=""about""),<<NEWL>>    path('contact/',views.contact,name=""contact""),<<NEWL>>    path('product/',views.product,name=""product""),<<NEWL>>    path('signup/',views.user_signup,name=""user_signup""),<<NEWL>>    path('signin/',views.user_login,name=""user_login""),<<NEWL>>    path('dashboard/',views.dashboard,name=""dashboard""),<<NEWL>>    path('cart/',views.cart,name=""cart""),<<NEWL>>    path('addpost/',views.addpost,name=""addpost""),<<NEWL>>    path('addcart/<str:title>',views.addcart,name=""addcart""),<<NEWL>>    path('updatepost/<int:id>',views.updatepost,name=""updatepost""),<<NEWL>>    path('deletepost/<int:id>',views.deletepost,name=""deletepost""),<<NEWL>>    path('logout/',views.user_logout,name=""logout""),<<NEWL>>    path('oauth/', include('social_django.urls', namespace='social')),<<NEWL>>]<<NEWL>><<NEWL>>if settings.DEBUG:<<NEWL>>    urlpatterns += static(settings.MEDIA_URL,document_root=settings.MEDIA_ROOT)"
382	jackson	1	"def _fix_contents(filename, contents):<<NEWL>>    import re<<NEWL>><<NEWL>>    contents = re.sub(<<NEWL>>        r""from bytecode"", r'from _pydevd_frame_eval.vendored.bytecode', contents, flags=re.MULTILINE<<NEWL>>    )<<NEWL>><<NEWL>>    contents = re.sub(<<NEWL>>        r""import bytecode"", r'from _pydevd_frame_eval.vendored import bytecode', contents, flags=re.MULTILINE<<NEWL>>    )<<NEWL>><<NEWL>>    # This test will import the wrong setup (we're not interested in it).<<NEWL>>    contents = re.sub(<<NEWL>>        r""def test_version\(self\):"", r'def skip_test_version(self):', contents, flags=re.MULTILINE<<NEWL>>    )<<NEWL>><<NEWL>>    if filename.startswith('test_'):<<NEWL>>        if 'pytestmark' not in contents:<<NEWL>>            pytest_mark = '''<<NEWL>>import pytest<<NEWL>>from tests_python.debugger_unittest import IS_PY36_OR_GREATER, IS_CPYTHON<<NEWL>>from tests_python.debug_constants import TEST_CYTHON<<NEWL>>pytestmark = pytest.mark.skipif(not IS_PY36_OR_GREATER or not IS_CPYTHON or not TEST_CYTHON, reason='Requires CPython >= 3.6')<<NEWL>>'''<<NEWL>>            contents = pytest_mark + contents<<NEWL>>    return contents<<NEWL>><<NEWL>><<NEWL>>def main():<<NEWL>>    import os<<NEWL>><<NEWL>>    # traverse root directory, and list directories as dirs and files as files<<NEWL>>    for root, dirs, files in os.walk(os.path.dirname(__file__)):<<NEWL>>        path = root.split(os.sep)<<NEWL>>        for filename in files:<<NEWL>>            if filename.endswith('.py') and filename != 'pydevd_fix_code.py':<<NEWL>>                with open(os.path.join(root, filename), 'r') as stream:<<NEWL>>                    contents = stream.read()<<NEWL>><<NEWL>>                new_contents = _fix_contents(filename, contents)<<NEWL>>                if contents != new_contents:<<NEWL>>                    print('fixed ', os.path.join(root, filename))<<NEWL>>                    with open(os.path.join(root, filename), 'w') as stream:<<NEWL>>                        stream.write(new_contents)<<NEWL>><<NEWL>>#             print(len(path) * '---', filename)<<NEWL>><<NEWL>><<NEWL>>if __name__ == '__main__':<<NEWL>>    main()"
382	donghui	2	"def _fix_contents(filename, contents):<<NEWL>>    import re<<NEWL>><<NEWL>>    contents = re.sub(<<NEWL>>        r""from bytecode"", r'from _pydevd_frame_eval.vendored.bytecode', contents, flags=re.MULTILINE<<NEWL>>    )<<NEWL>><<NEWL>>    contents = re.sub(<<NEWL>>        r""import bytecode"", r'from _pydevd_frame_eval.vendored import bytecode', contents, flags=re.MULTILINE<<NEWL>>    )<<NEWL>><<NEWL>>    # This test will import the wrong setup (we're not interested in it).<<NEWL>>    contents = re.sub(<<NEWL>>        r""def test_version\(self\):"", r'def skip_test_version(self):', contents, flags=re.MULTILINE<<NEWL>>    )<<NEWL>><<NEWL>>    if filename.startswith('test_'):<<NEWL>>        if 'pytestmark' not in contents:<<NEWL>>            pytest_mark = '''<<NEWL>>import pytest<<NEWL>>from tests_python.debugger_unittest import IS_PY36_OR_GREATER, IS_CPYTHON<<NEWL>>from tests_python.debug_constants import TEST_CYTHON<<NEWL>>pytestmark = pytest.mark.skipif(not IS_PY36_OR_GREATER or not IS_CPYTHON or not TEST_CYTHON, reason='Requires CPython >= 3.6')<<NEWL>>'''<<NEWL>>            contents = pytest_mark + contents<<NEWL>>    return contents<<NEWL>><<NEWL>><<NEWL>>def main():<<NEWL>>    import os<<NEWL>><<NEWL>>    # traverse root directory, and list directories as dirs and files as files<<NEWL>>    for root, dirs, files in os.walk(os.path.dirname(__file__)):<<NEWL>>        path = root.split(os.sep)<<NEWL>>        for filename in files:<<NEWL>>            if filename.endswith('.py') and filename != 'pydevd_fix_code.py':<<NEWL>>                with open(os.path.join(root, filename), 'r') as stream:<<NEWL>>                    contents = stream.read()<<NEWL>><<NEWL>>                new_contents = _fix_contents(filename, contents)<<NEWL>>                if contents != new_contents:<<NEWL>>                    print('fixed ', os.path.join(root, filename))<<NEWL>>                    with open(os.path.join(root, filename), 'w') as stream:<<NEWL>>                        stream.write(new_contents)<<NEWL>><<NEWL>>#             print(len(path) * '---', filename)<<NEWL>><<NEWL>><<NEWL>>if __name__ == '__main__':<<NEWL>>    main()"
371	jackson	3	"from functools import wraps<<NEWL>><<NEWL>>from django.middleware.csrf import CsrfViewMiddleware, get_token<<NEWL>>from django.utils.decorators import decorator_from_middleware<<NEWL>><<NEWL>>csrf_protect = decorator_from_middleware(CsrfViewMiddleware)<<NEWL>>csrf_protect.__name__ = ""csrf_protect""<<NEWL>>csrf_protect.__doc__ = """"""<<NEWL>>This decorator adds CSRF protection in exactly the same way as<<NEWL>>CsrfViewMiddleware, but it can be used on a per view basis.  Using both, or<<NEWL>>using the decorator multiple times, is harmless and efficient.<<NEWL>>""""""<<NEWL>><<NEWL>><<NEWL>>class _EnsureCsrfToken(CsrfViewMiddleware):<<NEWL>>    # Behave like CsrfViewMiddleware but don't reject requests or log warnings.<<NEWL>>    def _reject(self, request, reason):<<NEWL>>        return None<<NEWL>><<NEWL>><<NEWL>>requires_csrf_token = decorator_from_middleware(_EnsureCsrfToken)<<NEWL>>requires_csrf_token.__name__ = 'requires_csrf_token'<<NEWL>>requires_csrf_token.__doc__ = """"""<<NEWL>>Use this decorator on views that need a correct csrf_token available to<<NEWL>>RequestContext, but without the CSRF protection that csrf_protect<<NEWL>>enforces.<<NEWL>>""""""<<NEWL>><<NEWL>><<NEWL>>class _EnsureCsrfCookie(CsrfViewMiddleware):<<NEWL>>    def _reject(self, request, reason):<<NEWL>>        return None<<NEWL>><<NEWL>>    def process_view(self, request, callback, callback_args, callback_kwargs):<<NEWL>>        retval = super().process_view(request, callback, callback_args, callback_kwargs)<<NEWL>>        # Force process_response to send the cookie<<NEWL>>        get_token(request)<<NEWL>>        return retval<<NEWL>><<NEWL>><<NEWL>>ensure_csrf_cookie = decorator_from_middleware(_EnsureCsrfCookie)<<NEWL>>ensure_csrf_cookie.__name__ = 'ensure_csrf_cookie'<<NEWL>>ensure_csrf_cookie.__doc__ = """"""<<NEWL>>Use this decorator to ensure that a view sets a CSRF cookie, whether or not it<<NEWL>>uses the csrf_token template tag, or the CsrfViewMiddleware is used.<<NEWL>>""""""<<NEWL>><<NEWL>><<NEWL>>def csrf_exempt(view_func):<<NEWL>>    """"""Mark a view function as being exempt from the CSRF view protection.""""""<<NEWL>>    # view_func.csrf_exempt = True would also work, but decorators are nicer<<NEWL>>    # if they don't have side effects, so return a new function.<<NEWL>>    def wrapped_view(*args, **kwargs):<<NEWL>>        return view_func(*args, **kwargs)<<NEWL>>    wrapped_view.csrf_exempt = True<<NEWL>>    return wraps(view_func)(wrapped_view)"
371	donghui	2	"from functools import wraps<<NEWL>><<NEWL>>from django.middleware.csrf import CsrfViewMiddleware, get_token<<NEWL>>from django.utils.decorators import decorator_from_middleware<<NEWL>><<NEWL>>csrf_protect = decorator_from_middleware(CsrfViewMiddleware)<<NEWL>>csrf_protect.__name__ = ""csrf_protect""<<NEWL>>csrf_protect.__doc__ = """"""<<NEWL>>This decorator adds CSRF protection in exactly the same way as<<NEWL>>CsrfViewMiddleware, but it can be used on a per view basis.  Using both, or<<NEWL>>using the decorator multiple times, is harmless and efficient.<<NEWL>>""""""<<NEWL>><<NEWL>><<NEWL>>class _EnsureCsrfToken(CsrfViewMiddleware):<<NEWL>>    # Behave like CsrfViewMiddleware but don't reject requests or log warnings.<<NEWL>>    def _reject(self, request, reason):<<NEWL>>        return None<<NEWL>><<NEWL>><<NEWL>>requires_csrf_token = decorator_from_middleware(_EnsureCsrfToken)<<NEWL>>requires_csrf_token.__name__ = 'requires_csrf_token'<<NEWL>>requires_csrf_token.__doc__ = """"""<<NEWL>>Use this decorator on views that need a correct csrf_token available to<<NEWL>>RequestContext, but without the CSRF protection that csrf_protect<<NEWL>>enforces.<<NEWL>>""""""<<NEWL>><<NEWL>><<NEWL>>class _EnsureCsrfCookie(CsrfViewMiddleware):<<NEWL>>    def _reject(self, request, reason):<<NEWL>>        return None<<NEWL>><<NEWL>>    def process_view(self, request, callback, callback_args, callback_kwargs):<<NEWL>>        retval = super().process_view(request, callback, callback_args, callback_kwargs)<<NEWL>>        # Force process_response to send the cookie<<NEWL>>        get_token(request)<<NEWL>>        return retval<<NEWL>><<NEWL>><<NEWL>>ensure_csrf_cookie = decorator_from_middleware(_EnsureCsrfCookie)<<NEWL>>ensure_csrf_cookie.__name__ = 'ensure_csrf_cookie'<<NEWL>>ensure_csrf_cookie.__doc__ = """"""<<NEWL>>Use this decorator to ensure that a view sets a CSRF cookie, whether or not it<<NEWL>>uses the csrf_token template tag, or the CsrfViewMiddleware is used.<<NEWL>>""""""<<NEWL>><<NEWL>><<NEWL>>def csrf_exempt(view_func):<<NEWL>>    """"""Mark a view function as being exempt from the CSRF view protection.""""""<<NEWL>>    # view_func.csrf_exempt = True would also work, but decorators are nicer<<NEWL>>    # if they don't have side effects, so return a new function.<<NEWL>>    def wrapped_view(*args, **kwargs):<<NEWL>>        return view_func(*args, **kwargs)<<NEWL>>    wrapped_view.csrf_exempt = True<<NEWL>>    return wraps(view_func)(wrapped_view)"
260	jackson	1	"# -*- coding: utf-8 -*-<<NEWL>># Generated by the protocol buffer compiler.  DO NOT EDIT!<<NEWL>># source: streamlit/proto/PageNotFound.proto<<NEWL>><<NEWL>>from google.protobuf import descriptor as _descriptor<<NEWL>>from google.protobuf import message as _message<<NEWL>>from google.protobuf import reflection as _reflection<<NEWL>>from google.protobuf import symbol_database as _symbol_database<<NEWL>># @@protoc_insertion_point(imports)<<NEWL>><<NEWL>>_sym_db = _symbol_database.Default()<<NEWL>><<NEWL>><<NEWL>><<NEWL>><<NEWL>>DESCRIPTOR = _descriptor.FileDescriptor(<<NEWL>>  name='streamlit/proto/PageNotFound.proto',<<NEWL>>  package='',<<NEWL>>  syntax='proto3',<<NEWL>>  serialized_options=None,<<NEWL>>  create_key=_descriptor._internal_create_key,<<NEWL>>  serialized_pb=b'\n\""streamlit/proto/PageNotFound.proto\""!\n\x0cPageNotFound\x12\x11\n\tpage_name\x18\x01 \x01(\tb\x06proto3'<<NEWL>>)<<NEWL>><<NEWL>><<NEWL>><<NEWL>><<NEWL>>_PAGENOTFOUND = _descriptor.Descriptor(<<NEWL>>  name='PageNotFound',<<NEWL>>  full_name='PageNotFound',<<NEWL>>  filename=None,<<NEWL>>  file=DESCRIPTOR,<<NEWL>>  containing_type=None,<<NEWL>>  create_key=_descriptor._internal_create_key,<<NEWL>>  fields=[<<NEWL>>    _descriptor.FieldDescriptor(<<NEWL>>      name='page_name', full_name='PageNotFound.page_name', index=0,<<NEWL>>      number=1, type=9, cpp_type=9, label=1,<<NEWL>>      has_default_value=False, default_value=b"""".decode('utf-8'),<<NEWL>>      message_type=None, enum_type=None, containing_type=None,<<NEWL>>      is_extension=False, extension_scope=None,<<NEWL>>      serialized_options=None, file=DESCRIPTOR,  create_key=_descriptor._internal_create_key),<<NEWL>>  ],<<NEWL>>  extensions=[<<NEWL>>  ],<<NEWL>>  nested_types=[],<<NEWL>>  enum_types=[<<NEWL>>  ],<<NEWL>>  serialized_options=None,<<NEWL>>  is_extendable=False,<<NEWL>>  syntax='proto3',<<NEWL>>  extension_ranges=[],<<NEWL>>  oneofs=[<<NEWL>>  ],<<NEWL>>  serialized_start=38,<<NEWL>>  serialized_end=71,<<NEWL>>)<<NEWL>><<NEWL>>DESCRIPTOR.message_types_by_name['PageNotFound'] = _PAGENOTFOUND<<NEWL>>_sym_db.RegisterFileDescriptor(DESCRIPTOR)<<NEWL>><<NEWL>>PageNotFound = _reflection.GeneratedProtocolMessageType('PageNotFound', (_message.Message,), {<<NEWL>>  'DESCRIPTOR' : _PAGENOTFOUND,<<NEWL>>  '__module__' : 'streamlit.proto.PageNotFound_pb2'<<NEWL>>  # @@protoc_insertion_point(class_scope:PageNotFound)<<NEWL>>  })<<NEWL>>_sym_db.RegisterMessage(PageNotFound)<<NEWL>><<NEWL>><<NEWL>># @@protoc_insertion_point(module_scope)"
260	donghui	0	"# -*- coding: utf-8 -*-<<NEWL>># Generated by the protocol buffer compiler.  DO NOT EDIT!<<NEWL>># source: streamlit/proto/PageNotFound.proto<<NEWL>><<NEWL>>from google.protobuf import descriptor as _descriptor<<NEWL>>from google.protobuf import message as _message<<NEWL>>from google.protobuf import reflection as _reflection<<NEWL>>from google.protobuf import symbol_database as _symbol_database<<NEWL>># @@protoc_insertion_point(imports)<<NEWL>><<NEWL>>_sym_db = _symbol_database.Default()<<NEWL>><<NEWL>><<NEWL>><<NEWL>><<NEWL>>DESCRIPTOR = _descriptor.FileDescriptor(<<NEWL>>  name='streamlit/proto/PageNotFound.proto',<<NEWL>>  package='',<<NEWL>>  syntax='proto3',<<NEWL>>  serialized_options=None,<<NEWL>>  create_key=_descriptor._internal_create_key,<<NEWL>>  serialized_pb=b'\n\""streamlit/proto/PageNotFound.proto\""!\n\x0cPageNotFound\x12\x11\n\tpage_name\x18\x01 \x01(\tb\x06proto3'<<NEWL>>)<<NEWL>><<NEWL>><<NEWL>><<NEWL>><<NEWL>>_PAGENOTFOUND = _descriptor.Descriptor(<<NEWL>>  name='PageNotFound',<<NEWL>>  full_name='PageNotFound',<<NEWL>>  filename=None,<<NEWL>>  file=DESCRIPTOR,<<NEWL>>  containing_type=None,<<NEWL>>  create_key=_descriptor._internal_create_key,<<NEWL>>  fields=[<<NEWL>>    _descriptor.FieldDescriptor(<<NEWL>>      name='page_name', full_name='PageNotFound.page_name', index=0,<<NEWL>>      number=1, type=9, cpp_type=9, label=1,<<NEWL>>      has_default_value=False, default_value=b"""".decode('utf-8'),<<NEWL>>      message_type=None, enum_type=None, containing_type=None,<<NEWL>>      is_extension=False, extension_scope=None,<<NEWL>>      serialized_options=None, file=DESCRIPTOR,  create_key=_descriptor._internal_create_key),<<NEWL>>  ],<<NEWL>>  extensions=[<<NEWL>>  ],<<NEWL>>  nested_types=[],<<NEWL>>  enum_types=[<<NEWL>>  ],<<NEWL>>  serialized_options=None,<<NEWL>>  is_extendable=False,<<NEWL>>  syntax='proto3',<<NEWL>>  extension_ranges=[],<<NEWL>>  oneofs=[<<NEWL>>  ],<<NEWL>>  serialized_start=38,<<NEWL>>  serialized_end=71,<<NEWL>>)<<NEWL>><<NEWL>>DESCRIPTOR.message_types_by_name['PageNotFound'] = _PAGENOTFOUND<<NEWL>>_sym_db.RegisterFileDescriptor(DESCRIPTOR)<<NEWL>><<NEWL>>PageNotFound = _reflection.GeneratedProtocolMessageType('PageNotFound', (_message.Message,), {<<NEWL>>  'DESCRIPTOR' : _PAGENOTFOUND,<<NEWL>>  '__module__' : 'streamlit.proto.PageNotFound_pb2'<<NEWL>>  # @@protoc_insertion_point(class_scope:PageNotFound)<<NEWL>>  })<<NEWL>>_sym_db.RegisterMessage(PageNotFound)<<NEWL>><<NEWL>><<NEWL>># @@protoc_insertion_point(module_scope)"
300	jackson	1	"# Copyright 2020 Google LLC<<NEWL>>#<<NEWL>># Licensed under the Apache License, Version 2.0 (the ""License"");<<NEWL>># you may not use this file except in compliance with the License.<<NEWL>># You may obtain a copy of the License at<<NEWL>>#<<NEWL>>#    http://www.apache.org/licenses/LICENSE-2.0<<NEWL>>#<<NEWL>># Unless required by applicable law or agreed to in writing, software<<NEWL>># distributed under the License is distributed on an ""AS IS"" BASIS,<<NEWL>># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.<<NEWL>># See the License for the specific language governing permissions and<<NEWL>># limitations under the License.<<NEWL>><<NEWL>><<NEWL>># [START automl_batch_predict_beta]<<NEWL>>from google.cloud import automl_v1beta1 as automl<<NEWL>><<NEWL>><<NEWL>>def batch_predict(<<NEWL>>    project_id=""YOUR_PROJECT_ID"",<<NEWL>>    model_id=""YOUR_MODEL_ID"",<<NEWL>>    input_uri=""gs://YOUR_BUCKET_ID/path/to/your/input/csv_or_jsonl"",<<NEWL>>    output_uri=""gs://YOUR_BUCKET_ID/path/to/save/results/"",<<NEWL>>):<<NEWL>>    """"""Batch predict""""""<<NEWL>>    prediction_client = automl.PredictionServiceClient()<<NEWL>><<NEWL>>    # Get the full path of the model.<<NEWL>>    model_full_id = automl.AutoMlClient.model_path(<<NEWL>>        project_id, ""us-central1"", model_id<<NEWL>>    )<<NEWL>><<NEWL>>    gcs_source = automl.GcsSource(input_uris=[input_uri])<<NEWL>><<NEWL>>    input_config = automl.BatchPredictInputConfig(gcs_source=gcs_source)<<NEWL>>    gcs_destination = automl.GcsDestination(output_uri_prefix=output_uri)<<NEWL>>    output_config = automl.BatchPredictOutputConfig(<<NEWL>>        gcs_destination=gcs_destination<<NEWL>>    )<<NEWL>>    params = {}<<NEWL>><<NEWL>>    request = automl.BatchPredictRequest(<<NEWL>>        name=model_full_id,<<NEWL>>        input_config=input_config,<<NEWL>>        output_config=output_config,<<NEWL>>        params=params<<NEWL>>    )<<NEWL>>    response = prediction_client.batch_predict(<<NEWL>>        request=request<<NEWL>>    )<<NEWL>><<NEWL>>    print(""Waiting for operation to complete..."")<<NEWL>>    print(<<NEWL>>        ""Batch Prediction results saved to Cloud Storage bucket. {}"".format(<<NEWL>>            response.result()<<NEWL>>        )<<NEWL>>    )<<NEWL>># [END automl_batch_predict_beta]"
300	donghui	1	"# Copyright 2020 Google LLC<<NEWL>>#<<NEWL>># Licensed under the Apache License, Version 2.0 (the ""License"");<<NEWL>># you may not use this file except in compliance with the License.<<NEWL>># You may obtain a copy of the License at<<NEWL>>#<<NEWL>>#    http://www.apache.org/licenses/LICENSE-2.0<<NEWL>>#<<NEWL>># Unless required by applicable law or agreed to in writing, software<<NEWL>># distributed under the License is distributed on an ""AS IS"" BASIS,<<NEWL>># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.<<NEWL>># See the License for the specific language governing permissions and<<NEWL>># limitations under the License.<<NEWL>><<NEWL>><<NEWL>># [START automl_batch_predict_beta]<<NEWL>>from google.cloud import automl_v1beta1 as automl<<NEWL>><<NEWL>><<NEWL>>def batch_predict(<<NEWL>>    project_id=""YOUR_PROJECT_ID"",<<NEWL>>    model_id=""YOUR_MODEL_ID"",<<NEWL>>    input_uri=""gs://YOUR_BUCKET_ID/path/to/your/input/csv_or_jsonl"",<<NEWL>>    output_uri=""gs://YOUR_BUCKET_ID/path/to/save/results/"",<<NEWL>>):<<NEWL>>    """"""Batch predict""""""<<NEWL>>    prediction_client = automl.PredictionServiceClient()<<NEWL>><<NEWL>>    # Get the full path of the model.<<NEWL>>    model_full_id = automl.AutoMlClient.model_path(<<NEWL>>        project_id, ""us-central1"", model_id<<NEWL>>    )<<NEWL>><<NEWL>>    gcs_source = automl.GcsSource(input_uris=[input_uri])<<NEWL>><<NEWL>>    input_config = automl.BatchPredictInputConfig(gcs_source=gcs_source)<<NEWL>>    gcs_destination = automl.GcsDestination(output_uri_prefix=output_uri)<<NEWL>>    output_config = automl.BatchPredictOutputConfig(<<NEWL>>        gcs_destination=gcs_destination<<NEWL>>    )<<NEWL>>    params = {}<<NEWL>><<NEWL>>    request = automl.BatchPredictRequest(<<NEWL>>        name=model_full_id,<<NEWL>>        input_config=input_config,<<NEWL>>        output_config=output_config,<<NEWL>>        params=params<<NEWL>>    )<<NEWL>>    response = prediction_client.batch_predict(<<NEWL>>        request=request<<NEWL>>    )<<NEWL>><<NEWL>>    print(""Waiting for operation to complete..."")<<NEWL>>    print(<<NEWL>>        ""Batch Prediction results saved to Cloud Storage bucket. {}"".format(<<NEWL>>            response.result()<<NEWL>>        )<<NEWL>>    )<<NEWL>># [END automl_batch_predict_beta]"
351	jackson	3	"from django.contrib.messages.storage.base import BaseStorage<<NEWL>>from django.contrib.messages.storage.cookie import CookieStorage<<NEWL>>from django.contrib.messages.storage.session import SessionStorage<<NEWL>><<NEWL>><<NEWL>>class FallbackStorage(BaseStorage):<<NEWL>>    """"""<<NEWL>>    Try to store all messages in the first backend. Store any unstored<<NEWL>>    messages in each subsequent backend.<<NEWL>>    """"""<<NEWL>><<NEWL>>    storage_classes = (CookieStorage, SessionStorage)<<NEWL>><<NEWL>>    def __init__(self, *args, **kwargs):<<NEWL>>        super().__init__(*args, **kwargs)<<NEWL>>        self.storages = [<<NEWL>>            storage_class(*args, **kwargs) for storage_class in self.storage_classes<<NEWL>>        ]<<NEWL>>        self._used_storages = set()<<NEWL>><<NEWL>>    def _get(self, *args, **kwargs):<<NEWL>>        """"""<<NEWL>>        Get a single list of messages from all storage backends.<<NEWL>>        """"""<<NEWL>>        all_messages = []<<NEWL>>        for storage in self.storages:<<NEWL>>            messages, all_retrieved = storage._get()<<NEWL>>            # If the backend hasn't been used, no more retrieval is necessary.<<NEWL>>            if messages is None:<<NEWL>>                break<<NEWL>>            if messages:<<NEWL>>                self._used_storages.add(storage)<<NEWL>>            all_messages.extend(messages)<<NEWL>>            # If this storage class contained all the messages, no further<<NEWL>>            # retrieval is necessary<<NEWL>>            if all_retrieved:<<NEWL>>                break<<NEWL>>        return all_messages, all_retrieved<<NEWL>><<NEWL>>    def _store(self, messages, response, *args, **kwargs):<<NEWL>>        """"""<<NEWL>>        Store the messages and return any unstored messages after trying all<<NEWL>>        backends.<<NEWL>><<NEWL>>        For each storage backend, any messages not stored are passed on to the<<NEWL>>        next backend.<<NEWL>>        """"""<<NEWL>>        for storage in self.storages:<<NEWL>>            if messages:<<NEWL>>                messages = storage._store(messages, response, remove_oldest=False)<<NEWL>>            # Even if there are no more messages, continue iterating to ensure<<NEWL>>            # storages which contained messages are flushed.<<NEWL>>            elif storage in self._used_storages:<<NEWL>>                storage._store([], response)<<NEWL>>                self._used_storages.remove(storage)<<NEWL>>        return messages"
351	donghui	2	"from django.contrib.messages.storage.base import BaseStorage<<NEWL>>from django.contrib.messages.storage.cookie import CookieStorage<<NEWL>>from django.contrib.messages.storage.session import SessionStorage<<NEWL>><<NEWL>><<NEWL>>class FallbackStorage(BaseStorage):<<NEWL>>    """"""<<NEWL>>    Try to store all messages in the first backend. Store any unstored<<NEWL>>    messages in each subsequent backend.<<NEWL>>    """"""<<NEWL>><<NEWL>>    storage_classes = (CookieStorage, SessionStorage)<<NEWL>><<NEWL>>    def __init__(self, *args, **kwargs):<<NEWL>>        super().__init__(*args, **kwargs)<<NEWL>>        self.storages = [<<NEWL>>            storage_class(*args, **kwargs) for storage_class in self.storage_classes<<NEWL>>        ]<<NEWL>>        self._used_storages = set()<<NEWL>><<NEWL>>    def _get(self, *args, **kwargs):<<NEWL>>        """"""<<NEWL>>        Get a single list of messages from all storage backends.<<NEWL>>        """"""<<NEWL>>        all_messages = []<<NEWL>>        for storage in self.storages:<<NEWL>>            messages, all_retrieved = storage._get()<<NEWL>>            # If the backend hasn't been used, no more retrieval is necessary.<<NEWL>>            if messages is None:<<NEWL>>                break<<NEWL>>            if messages:<<NEWL>>                self._used_storages.add(storage)<<NEWL>>            all_messages.extend(messages)<<NEWL>>            # If this storage class contained all the messages, no further<<NEWL>>            # retrieval is necessary<<NEWL>>            if all_retrieved:<<NEWL>>                break<<NEWL>>        return all_messages, all_retrieved<<NEWL>><<NEWL>>    def _store(self, messages, response, *args, **kwargs):<<NEWL>>        """"""<<NEWL>>        Store the messages and return any unstored messages after trying all<<NEWL>>        backends.<<NEWL>><<NEWL>>        For each storage backend, any messages not stored are passed on to the<<NEWL>>        next backend.<<NEWL>>        """"""<<NEWL>>        for storage in self.storages:<<NEWL>>            if messages:<<NEWL>>                messages = storage._store(messages, response, remove_oldest=False)<<NEWL>>            # Even if there are no more messages, continue iterating to ensure<<NEWL>>            # storages which contained messages are flushed.<<NEWL>>            elif storage in self._used_storages:<<NEWL>>                storage._store([], response)<<NEWL>>                self._used_storages.remove(storage)<<NEWL>>        return messages"
365	jackson	0	"from sqlalchemy import create_engine<<NEWL>>import pandas as pd<<NEWL>><<NEWL>>class Manager:<<NEWL>>    __instance = None<<NEWL>><<NEWL>>    def __init__(self, engine=None, username=None, password=None, database=None, host=None, port=None):<<NEWL>>        if Manager.__instance is None:<<NEWL>>            self.engine_type = engine<<NEWL>>            self.username = username<<NEWL>>            self.password = password<<NEWL>>            self.database = database<<NEWL>>            self.host = host<<NEWL>>            self.port = port<<NEWL>>            self.url = self._generate_url()<<NEWL>>            self.engine = create_engine(self.url)<<NEWL>>            Manager.__instance = self<<NEWL>>        else:<<NEWL>>            raise Exception(""Cannot create multiple instances of Database class"")<<NEWL>><<NEWL>>    @staticmethod<<NEWL>>    def get_instance(engine=None, username=None, password=None, database=None, host=None, port=None):<<NEWL>>        if Manager.__instance is None:<<NEWL>>            Manager(engine, username, password, database, host, port)<<NEWL>>        return Manager.__instance<<NEWL>><<NEWL>>    def _generate_url(self):<<NEWL>>        if self.engine_type == 'postgresql':<<NEWL>>            return f""postgresql://{self.username}:{self.password}@{self.host}:{self.port}/{self.database}""<<NEWL>>        elif self.engine_type == 'mysql':<<NEWL>>            return f""mysql+pymysql://{self.username}:{self.password}@{self.host}:{self.port}/{self.database}""<<NEWL>>        elif self.engine_type == 'sqlite':<<NEWL>>            return f""sqlite:///{self.database}""<<NEWL>>        elif self.engine_type == 'oracle':<<NEWL>>            return f""oracle://{self.username}:{self.password}@{self.host}:{self.port}/{self.database}""<<NEWL>>        elif self.engine_type == 'mssql':<<NEWL>>            return f""mssql+pymssql://{self.username}:{self.password}@{self.host}:{self.port}/{self.database}""<<NEWL>>        else:<<NEWL>>            raise Exception(""Unsupported engine type"")<<NEWL>><<NEWL>>    def execute_query(self, query):<<NEWL>>        with self.engine.connect() as conn:<<NEWL>>            result = conn.execute(query)<<NEWL>>            data = pd.DataFrame(result.fetchall(), columns=result.keys())<<NEWL>>        return data"
365	donghui	0	"from sqlalchemy import create_engine<<NEWL>>import pandas as pd<<NEWL>><<NEWL>>class Manager:<<NEWL>>    __instance = None<<NEWL>><<NEWL>>    def __init__(self, engine=None, username=None, password=None, database=None, host=None, port=None):<<NEWL>>        if Manager.__instance is None:<<NEWL>>            self.engine_type = engine<<NEWL>>            self.username = username<<NEWL>>            self.password = password<<NEWL>>            self.database = database<<NEWL>>            self.host = host<<NEWL>>            self.port = port<<NEWL>>            self.url = self._generate_url()<<NEWL>>            self.engine = create_engine(self.url)<<NEWL>>            Manager.__instance = self<<NEWL>>        else:<<NEWL>>            raise Exception(""Cannot create multiple instances of Database class"")<<NEWL>><<NEWL>>    @staticmethod<<NEWL>>    def get_instance(engine=None, username=None, password=None, database=None, host=None, port=None):<<NEWL>>        if Manager.__instance is None:<<NEWL>>            Manager(engine, username, password, database, host, port)<<NEWL>>        return Manager.__instance<<NEWL>><<NEWL>>    def _generate_url(self):<<NEWL>>        if self.engine_type == 'postgresql':<<NEWL>>            return f""postgresql://{self.username}:{self.password}@{self.host}:{self.port}/{self.database}""<<NEWL>>        elif self.engine_type == 'mysql':<<NEWL>>            return f""mysql+pymysql://{self.username}:{self.password}@{self.host}:{self.port}/{self.database}""<<NEWL>>        elif self.engine_type == 'sqlite':<<NEWL>>            return f""sqlite:///{self.database}""<<NEWL>>        elif self.engine_type == 'oracle':<<NEWL>>            return f""oracle://{self.username}:{self.password}@{self.host}:{self.port}/{self.database}""<<NEWL>>        elif self.engine_type == 'mssql':<<NEWL>>            return f""mssql+pymssql://{self.username}:{self.password}@{self.host}:{self.port}/{self.database}""<<NEWL>>        else:<<NEWL>>            raise Exception(""Unsupported engine type"")<<NEWL>><<NEWL>>    def execute_query(self, query):<<NEWL>>        with self.engine.connect() as conn:<<NEWL>>            result = conn.execute(query)<<NEWL>>            data = pd.DataFrame(result.fetchall(), columns=result.keys())<<NEWL>>        return data"
334	jackson	0	"# This file is dual licensed under the terms of the Apache License, Version<<NEWL>># 2.0, and the BSD License. See the LICENSE file in the root of this repository<<NEWL>># for complete details.<<NEWL>><<NEWL>>import typing<<NEWL>><<NEWL>>from cryptography.hazmat.primitives import hashes<<NEWL>>from cryptography.hazmat.primitives.asymmetric.utils import Prehashed<<NEWL>><<NEWL>>if typing.TYPE_CHECKING:<<NEWL>>    from cryptography.hazmat.backends.openssl.backend import Backend<<NEWL>><<NEWL>><<NEWL>>def _evp_pkey_derive(backend: ""Backend"", evp_pkey, peer_public_key) -> bytes:<<NEWL>>    ctx = backend._lib.EVP_PKEY_CTX_new(evp_pkey, backend._ffi.NULL)<<NEWL>>    backend.openssl_assert(ctx != backend._ffi.NULL)<<NEWL>>    ctx = backend._ffi.gc(ctx, backend._lib.EVP_PKEY_CTX_free)<<NEWL>>    res = backend._lib.EVP_PKEY_derive_init(ctx)<<NEWL>>    backend.openssl_assert(res == 1)<<NEWL>>    res = backend._lib.EVP_PKEY_derive_set_peer(ctx, peer_public_key._evp_pkey)<<NEWL>>    if res != 1:<<NEWL>>        errors_with_text = backend._consume_errors_with_text()<<NEWL>>        raise ValueError(""Error computing shared key."", errors_with_text)<<NEWL>><<NEWL>>    keylen = backend._ffi.new(""size_t *"")<<NEWL>>    res = backend._lib.EVP_PKEY_derive(ctx, backend._ffi.NULL, keylen)<<NEWL>>    backend.openssl_assert(res == 1)<<NEWL>>    backend.openssl_assert(keylen[0] > 0)<<NEWL>>    buf = backend._ffi.new(""unsigned char[]"", keylen[0])<<NEWL>>    res = backend._lib.EVP_PKEY_derive(ctx, buf, keylen)<<NEWL>>    if res != 1:<<NEWL>>        errors_with_text = backend._consume_errors_with_text()<<NEWL>>        raise ValueError(""Error computing shared key."", errors_with_text)<<NEWL>><<NEWL>>    return backend._ffi.buffer(buf, keylen[0])[:]<<NEWL>><<NEWL>><<NEWL>>def _calculate_digest_and_algorithm(<<NEWL>>    data: bytes,<<NEWL>>    algorithm: typing.Union[Prehashed, hashes.HashAlgorithm],<<NEWL>>) -> typing.Tuple[bytes, hashes.HashAlgorithm]:<<NEWL>>    if not isinstance(algorithm, Prehashed):<<NEWL>>        hash_ctx = hashes.Hash(algorithm)<<NEWL>>        hash_ctx.update(data)<<NEWL>>        data = hash_ctx.finalize()<<NEWL>>    else:<<NEWL>>        algorithm = algorithm._algorithm<<NEWL>><<NEWL>>    if len(data) != algorithm.digest_size:<<NEWL>>        raise ValueError(<<NEWL>>            ""The provided data must be the same length as the hash ""<<NEWL>>            ""algorithm's digest size.""<<NEWL>>        )<<NEWL>><<NEWL>>    return (data, algorithm)"
334	donghui	0	"# This file is dual licensed under the terms of the Apache License, Version<<NEWL>># 2.0, and the BSD License. See the LICENSE file in the root of this repository<<NEWL>># for complete details.<<NEWL>><<NEWL>>import typing<<NEWL>><<NEWL>>from cryptography.hazmat.primitives import hashes<<NEWL>>from cryptography.hazmat.primitives.asymmetric.utils import Prehashed<<NEWL>><<NEWL>>if typing.TYPE_CHECKING:<<NEWL>>    from cryptography.hazmat.backends.openssl.backend import Backend<<NEWL>><<NEWL>><<NEWL>>def _evp_pkey_derive(backend: ""Backend"", evp_pkey, peer_public_key) -> bytes:<<NEWL>>    ctx = backend._lib.EVP_PKEY_CTX_new(evp_pkey, backend._ffi.NULL)<<NEWL>>    backend.openssl_assert(ctx != backend._ffi.NULL)<<NEWL>>    ctx = backend._ffi.gc(ctx, backend._lib.EVP_PKEY_CTX_free)<<NEWL>>    res = backend._lib.EVP_PKEY_derive_init(ctx)<<NEWL>>    backend.openssl_assert(res == 1)<<NEWL>>    res = backend._lib.EVP_PKEY_derive_set_peer(ctx, peer_public_key._evp_pkey)<<NEWL>>    if res != 1:<<NEWL>>        errors_with_text = backend._consume_errors_with_text()<<NEWL>>        raise ValueError(""Error computing shared key."", errors_with_text)<<NEWL>><<NEWL>>    keylen = backend._ffi.new(""size_t *"")<<NEWL>>    res = backend._lib.EVP_PKEY_derive(ctx, backend._ffi.NULL, keylen)<<NEWL>>    backend.openssl_assert(res == 1)<<NEWL>>    backend.openssl_assert(keylen[0] > 0)<<NEWL>>    buf = backend._ffi.new(""unsigned char[]"", keylen[0])<<NEWL>>    res = backend._lib.EVP_PKEY_derive(ctx, buf, keylen)<<NEWL>>    if res != 1:<<NEWL>>        errors_with_text = backend._consume_errors_with_text()<<NEWL>>        raise ValueError(""Error computing shared key."", errors_with_text)<<NEWL>><<NEWL>>    return backend._ffi.buffer(buf, keylen[0])[:]<<NEWL>><<NEWL>><<NEWL>>def _calculate_digest_and_algorithm(<<NEWL>>    data: bytes,<<NEWL>>    algorithm: typing.Union[Prehashed, hashes.HashAlgorithm],<<NEWL>>) -> typing.Tuple[bytes, hashes.HashAlgorithm]:<<NEWL>>    if not isinstance(algorithm, Prehashed):<<NEWL>>        hash_ctx = hashes.Hash(algorithm)<<NEWL>>        hash_ctx.update(data)<<NEWL>>        data = hash_ctx.finalize()<<NEWL>>    else:<<NEWL>>        algorithm = algorithm._algorithm<<NEWL>><<NEWL>>    if len(data) != algorithm.digest_size:<<NEWL>>        raise ValueError(<<NEWL>>            ""The provided data must be the same length as the hash ""<<NEWL>>            ""algorithm's digest size.""<<NEWL>>        )<<NEWL>><<NEWL>>    return (data, algorithm)"
274	jackson	0	"import threading<<NEWL>>from dataclasses import dataclass<<NEWL>><<NEWL>>from prowler.lib.logger import logger<<NEWL>>from prowler.providers.aws.aws_provider import generate_regional_clients<<NEWL>><<NEWL>><<NEWL>>################## Macie<<NEWL>>class Macie:<<NEWL>>    def __init__(self, audit_info):<<NEWL>>        self.service = ""macie2""<<NEWL>>        self.session = audit_info.audit_session<<NEWL>>        self.audited_account = audit_info.audited_account<<NEWL>>        self.regional_clients = generate_regional_clients(self.service, audit_info)<<NEWL>>        self.sessions = []<<NEWL>>        self.__threading_call__(self.__get_macie_session__)<<NEWL>><<NEWL>>    def __get_session__(self):<<NEWL>>        return self.session<<NEWL>><<NEWL>>    def __threading_call__(self, call):<<NEWL>>        threads = []<<NEWL>>        for regional_client in self.regional_clients.values():<<NEWL>>            threads.append(threading.Thread(target=call, args=(regional_client,)))<<NEWL>>        for t in threads:<<NEWL>>            t.start()<<NEWL>>        for t in threads:<<NEWL>>            t.join()<<NEWL>><<NEWL>>    def __get_macie_session__(self, regional_client):<<NEWL>>        logger.info(""Macie - Get Macie Session..."")<<NEWL>>        try:<<NEWL>>            self.sessions.append(<<NEWL>>                Session(<<NEWL>>                    regional_client.get_macie_session()[""status""],<<NEWL>>                    regional_client.region,<<NEWL>>                )<<NEWL>>            )<<NEWL>><<NEWL>>        except Exception as error:<<NEWL>>            if ""Macie is not enabled"" in str(error):<<NEWL>>                self.sessions.append(<<NEWL>>                    Session(<<NEWL>>                        ""DISABLED"",<<NEWL>>                        regional_client.region,<<NEWL>>                    )<<NEWL>>                )<<NEWL>>            else:<<NEWL>>                logger.error(<<NEWL>>                    f""{regional_client.region} -- {error.__class__.__name__}[{error.__traceback__.tb_lineno}]: {error}""<<NEWL>>                )<<NEWL>><<NEWL>><<NEWL>>@dataclass<<NEWL>>class Session:<<NEWL>>    status: str<<NEWL>>    region: str<<NEWL>><<NEWL>>    def __init__(<<NEWL>>        self,<<NEWL>>        status,<<NEWL>>        region,<<NEWL>>    ):<<NEWL>>        self.status = status<<NEWL>>        self.region = region"
274	donghui	0	"import threading<<NEWL>>from dataclasses import dataclass<<NEWL>><<NEWL>>from prowler.lib.logger import logger<<NEWL>>from prowler.providers.aws.aws_provider import generate_regional_clients<<NEWL>><<NEWL>><<NEWL>>################## Macie<<NEWL>>class Macie:<<NEWL>>    def __init__(self, audit_info):<<NEWL>>        self.service = ""macie2""<<NEWL>>        self.session = audit_info.audit_session<<NEWL>>        self.audited_account = audit_info.audited_account<<NEWL>>        self.regional_clients = generate_regional_clients(self.service, audit_info)<<NEWL>>        self.sessions = []<<NEWL>>        self.__threading_call__(self.__get_macie_session__)<<NEWL>><<NEWL>>    def __get_session__(self):<<NEWL>>        return self.session<<NEWL>><<NEWL>>    def __threading_call__(self, call):<<NEWL>>        threads = []<<NEWL>>        for regional_client in self.regional_clients.values():<<NEWL>>            threads.append(threading.Thread(target=call, args=(regional_client,)))<<NEWL>>        for t in threads:<<NEWL>>            t.start()<<NEWL>>        for t in threads:<<NEWL>>            t.join()<<NEWL>><<NEWL>>    def __get_macie_session__(self, regional_client):<<NEWL>>        logger.info(""Macie - Get Macie Session..."")<<NEWL>>        try:<<NEWL>>            self.sessions.append(<<NEWL>>                Session(<<NEWL>>                    regional_client.get_macie_session()[""status""],<<NEWL>>                    regional_client.region,<<NEWL>>                )<<NEWL>>            )<<NEWL>><<NEWL>>        except Exception as error:<<NEWL>>            if ""Macie is not enabled"" in str(error):<<NEWL>>                self.sessions.append(<<NEWL>>                    Session(<<NEWL>>                        ""DISABLED"",<<NEWL>>                        regional_client.region,<<NEWL>>                    )<<NEWL>>                )<<NEWL>>            else:<<NEWL>>                logger.error(<<NEWL>>                    f""{regional_client.region} -- {error.__class__.__name__}[{error.__traceback__.tb_lineno}]: {error}""<<NEWL>>                )<<NEWL>><<NEWL>><<NEWL>>@dataclass<<NEWL>>class Session:<<NEWL>>    status: str<<NEWL>>    region: str<<NEWL>><<NEWL>>    def __init__(<<NEWL>>        self,<<NEWL>>        status,<<NEWL>>        region,<<NEWL>>    ):<<NEWL>>        self.status = status<<NEWL>>        self.region = region"
287	jackson	0	_base_ = [<<NEWL>>    '../_base_/datasets/coco_detection.py',<<NEWL>>    '../_base_/schedules/schedule_1x.py', '../_base_/default_runtime.py'<<NEWL>>]<<NEWL>>model = dict(<<NEWL>>    type='ATSS',<<NEWL>>    backbone=dict(<<NEWL>>        type='ResNet',<<NEWL>>        depth=50,<<NEWL>>        num_stages=4,<<NEWL>>        out_indices=(0, 1, 2, 3),<<NEWL>>        frozen_stages=1,<<NEWL>>        norm_cfg=dict(type='BN', requires_grad=True),<<NEWL>>        norm_eval=True,<<NEWL>>        style='pytorch',<<NEWL>>        init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet50')),<<NEWL>>    neck=[<<NEWL>>        dict(<<NEWL>>            type='FPN',<<NEWL>>            in_channels=[256, 512, 1024, 2048],<<NEWL>>            out_channels=256,<<NEWL>>            start_level=1,<<NEWL>>            add_extra_convs='on_output',<<NEWL>>            num_outs=5),<<NEWL>>        dict(type='DyHead', in_channels=256, out_channels=256, num_blocks=6)<<NEWL>>    ],<<NEWL>>    bbox_head=dict(<<NEWL>>        type='ATSSHead',<<NEWL>>        num_classes=80,<<NEWL>>        in_channels=256,<<NEWL>>        stacked_convs=0,<<NEWL>>        feat_channels=256,<<NEWL>>        anchor_generator=dict(<<NEWL>>            type='AnchorGenerator',<<NEWL>>            ratios=[1.0],<<NEWL>>            octave_base_scale=8,<<NEWL>>            scales_per_octave=1,<<NEWL>>            strides=[8, 16, 32, 64, 128]),<<NEWL>>        bbox_coder=dict(<<NEWL>>            type='DeltaXYWHBBoxCoder',<<NEWL>>            target_means=[.0, .0, .0, .0],<<NEWL>>            target_stds=[0.1, 0.1, 0.2, 0.2]),<<NEWL>>        loss_cls=dict(<<NEWL>>            type='FocalLoss',<<NEWL>>            use_sigmoid=True,<<NEWL>>            gamma=2.0,<<NEWL>>            alpha=0.25,<<NEWL>>            loss_weight=1.0),<<NEWL>>        loss_bbox=dict(type='GIoULoss', loss_weight=2.0),<<NEWL>>        loss_centerness=dict(<<NEWL>>            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0)),<<NEWL>>    # training and testing settings<<NEWL>>    train_cfg=dict(<<NEWL>>        assigner=dict(type='ATSSAssigner', topk=9),<<NEWL>>        allowed_border=-1,<<NEWL>>        pos_weight=-1,<<NEWL>>        debug=False),<<NEWL>>    test_cfg=dict(<<NEWL>>        nms_pre=1000,<<NEWL>>        min_bbox_size=0,<<NEWL>>        score_thr=0.05,<<NEWL>>        nms=dict(type='nms', iou_threshold=0.6),<<NEWL>>        max_per_img=100))<<NEWL>># optimizer<<NEWL>>optimizer = dict(type='SGD', lr=0.01, momentum=0.9, weight_decay=0.0001)
287	donghui	0	_base_ = [<<NEWL>>    '../_base_/datasets/coco_detection.py',<<NEWL>>    '../_base_/schedules/schedule_1x.py', '../_base_/default_runtime.py'<<NEWL>>]<<NEWL>>model = dict(<<NEWL>>    type='ATSS',<<NEWL>>    backbone=dict(<<NEWL>>        type='ResNet',<<NEWL>>        depth=50,<<NEWL>>        num_stages=4,<<NEWL>>        out_indices=(0, 1, 2, 3),<<NEWL>>        frozen_stages=1,<<NEWL>>        norm_cfg=dict(type='BN', requires_grad=True),<<NEWL>>        norm_eval=True,<<NEWL>>        style='pytorch',<<NEWL>>        init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet50')),<<NEWL>>    neck=[<<NEWL>>        dict(<<NEWL>>            type='FPN',<<NEWL>>            in_channels=[256, 512, 1024, 2048],<<NEWL>>            out_channels=256,<<NEWL>>            start_level=1,<<NEWL>>            add_extra_convs='on_output',<<NEWL>>            num_outs=5),<<NEWL>>        dict(type='DyHead', in_channels=256, out_channels=256, num_blocks=6)<<NEWL>>    ],<<NEWL>>    bbox_head=dict(<<NEWL>>        type='ATSSHead',<<NEWL>>        num_classes=80,<<NEWL>>        in_channels=256,<<NEWL>>        stacked_convs=0,<<NEWL>>        feat_channels=256,<<NEWL>>        anchor_generator=dict(<<NEWL>>            type='AnchorGenerator',<<NEWL>>            ratios=[1.0],<<NEWL>>            octave_base_scale=8,<<NEWL>>            scales_per_octave=1,<<NEWL>>            strides=[8, 16, 32, 64, 128]),<<NEWL>>        bbox_coder=dict(<<NEWL>>            type='DeltaXYWHBBoxCoder',<<NEWL>>            target_means=[.0, .0, .0, .0],<<NEWL>>            target_stds=[0.1, 0.1, 0.2, 0.2]),<<NEWL>>        loss_cls=dict(<<NEWL>>            type='FocalLoss',<<NEWL>>            use_sigmoid=True,<<NEWL>>            gamma=2.0,<<NEWL>>            alpha=0.25,<<NEWL>>            loss_weight=1.0),<<NEWL>>        loss_bbox=dict(type='GIoULoss', loss_weight=2.0),<<NEWL>>        loss_centerness=dict(<<NEWL>>            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0)),<<NEWL>>    # training and testing settings<<NEWL>>    train_cfg=dict(<<NEWL>>        assigner=dict(type='ATSSAssigner', topk=9),<<NEWL>>        allowed_border=-1,<<NEWL>>        pos_weight=-1,<<NEWL>>        debug=False),<<NEWL>>    test_cfg=dict(<<NEWL>>        nms_pre=1000,<<NEWL>>        min_bbox_size=0,<<NEWL>>        score_thr=0.05,<<NEWL>>        nms=dict(type='nms', iou_threshold=0.6),<<NEWL>>        max_per_img=100))<<NEWL>># optimizer<<NEWL>>optimizer = dict(type='SGD', lr=0.01, momentum=0.9, weight_decay=0.0001)
396	jackson	0	# Generated by Django 3.2.18 on 2023-03-09 11:26<<NEWL>><<NEWL>>from django.db import migrations, models<<NEWL>>import django.db.models.deletion<<NEWL>>import multiselectfield.db.fields<<NEWL>><<NEWL>><<NEWL>>class Migration(migrations.Migration):<<NEWL>><<NEWL>>    initial = True<<NEWL>><<NEWL>>    dependencies = [<<NEWL>>    ]<<NEWL>><<NEWL>>    operations = [<<NEWL>>        migrations.CreateModel(<<NEWL>>            name='Event',<<NEWL>>            fields=[<<NEWL>>                ('id', models.BigAutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),<<NEWL>>                ('event_name', models.CharField(max_length=200, unique=True)),<<NEWL>>                ('event_date', models.DateField()),<<NEWL>>                ('event_time', models.TimeField()),<<NEWL>>                ('created_on', models.DateTimeField(auto_now_add=True)),<<NEWL>>            ],<<NEWL>>        ),<<NEWL>>        migrations.CreateModel(<<NEWL>>            name='Guest',<<NEWL>>            fields=[<<NEWL>>                ('id', models.BigAutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),<<NEWL>>                ('guest_name', models.CharField(max_length=200, unique=True)),<<NEWL>>                ('slug', models.SlugField(max_length=200, unique=True)),<<NEWL>>                ('email', models.EmailField(max_length=254)),<<NEWL>>                ('is_attending', models.BooleanField(choices=[(False, 'No'), (True, 'Yes')], default='', verbose_name='Attending?')),<<NEWL>>                ('message', models.TextField(blank=True)),<<NEWL>>                ('dietary_requirements', multiselectfield.db.fields.MultiSelectField(choices=[(1, 'none'), (2, 'coeliac'), (3, 'food allergy'), (4, 'food intolerance'), (5, 'vegetarian'), (6, 'vegan'), (7, 'pescatarian'), (8, 'teetotal')], max_length=15)),<<NEWL>>                ('plus_one_attending', models.BooleanField(choices=[(False, 'No'), (True, 'Yes')], default='', verbose_name='Attending?')),<<NEWL>>                ('invited', models.IntegerField(choices=[(0, 'Draft'), (1, 'Invited')], default=0)),<<NEWL>>                ('event', models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, related_name='guests', to='weddingapp.event')),<<NEWL>>            ],<<NEWL>>            options={<<NEWL>>                'ordering': ['-guest_name'],<<NEWL>>            },<<NEWL>>        ),<<NEWL>>    ]
396	donghui	0	# Generated by Django 3.2.18 on 2023-03-09 11:26<<NEWL>><<NEWL>>from django.db import migrations, models<<NEWL>>import django.db.models.deletion<<NEWL>>import multiselectfield.db.fields<<NEWL>><<NEWL>><<NEWL>>class Migration(migrations.Migration):<<NEWL>><<NEWL>>    initial = True<<NEWL>><<NEWL>>    dependencies = [<<NEWL>>    ]<<NEWL>><<NEWL>>    operations = [<<NEWL>>        migrations.CreateModel(<<NEWL>>            name='Event',<<NEWL>>            fields=[<<NEWL>>                ('id', models.BigAutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),<<NEWL>>                ('event_name', models.CharField(max_length=200, unique=True)),<<NEWL>>                ('event_date', models.DateField()),<<NEWL>>                ('event_time', models.TimeField()),<<NEWL>>                ('created_on', models.DateTimeField(auto_now_add=True)),<<NEWL>>            ],<<NEWL>>        ),<<NEWL>>        migrations.CreateModel(<<NEWL>>            name='Guest',<<NEWL>>            fields=[<<NEWL>>                ('id', models.BigAutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),<<NEWL>>                ('guest_name', models.CharField(max_length=200, unique=True)),<<NEWL>>                ('slug', models.SlugField(max_length=200, unique=True)),<<NEWL>>                ('email', models.EmailField(max_length=254)),<<NEWL>>                ('is_attending', models.BooleanField(choices=[(False, 'No'), (True, 'Yes')], default='', verbose_name='Attending?')),<<NEWL>>                ('message', models.TextField(blank=True)),<<NEWL>>                ('dietary_requirements', multiselectfield.db.fields.MultiSelectField(choices=[(1, 'none'), (2, 'coeliac'), (3, 'food allergy'), (4, 'food intolerance'), (5, 'vegetarian'), (6, 'vegan'), (7, 'pescatarian'), (8, 'teetotal')], max_length=15)),<<NEWL>>                ('plus_one_attending', models.BooleanField(choices=[(False, 'No'), (True, 'Yes')], default='', verbose_name='Attending?')),<<NEWL>>                ('invited', models.IntegerField(choices=[(0, 'Draft'), (1, 'Invited')], default=0)),<<NEWL>>                ('event', models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, related_name='guests', to='weddingapp.event')),<<NEWL>>            ],<<NEWL>>            options={<<NEWL>>                'ordering': ['-guest_name'],<<NEWL>>            },<<NEWL>>        ),<<NEWL>>    ]
489	jackson	3	"from django.contrib.messages.storage.base import BaseStorage<<NEWL>>from django.contrib.messages.storage.cookie import CookieStorage<<NEWL>>from django.contrib.messages.storage.session import SessionStorage<<NEWL>><<NEWL>><<NEWL>>class FallbackStorage(BaseStorage):<<NEWL>>    """"""<<NEWL>>    Try to store all messages in the first backend. Store any unstored<<NEWL>>    messages in each subsequent backend.<<NEWL>>    """"""<<NEWL>>    storage_classes = (CookieStorage, SessionStorage)<<NEWL>><<NEWL>>    def __init__(self, *args, **kwargs):<<NEWL>>        super().__init__(*args, **kwargs)<<NEWL>>        self.storages = [storage_class(*args, **kwargs)<<NEWL>>                         for storage_class in self.storage_classes]<<NEWL>>        self._used_storages = set()<<NEWL>><<NEWL>>    def _get(self, *args, **kwargs):<<NEWL>>        """"""<<NEWL>>        Get a single list of messages from all storage backends.<<NEWL>>        """"""<<NEWL>>        all_messages = []<<NEWL>>        for storage in self.storages:<<NEWL>>            messages, all_retrieved = storage._get()<<NEWL>>            # If the backend hasn't been used, no more retrieval is necessary.<<NEWL>>            if messages is None:<<NEWL>>                break<<NEWL>>            if messages:<<NEWL>>                self._used_storages.add(storage)<<NEWL>>            all_messages.extend(messages)<<NEWL>>            # If this storage class contained all the messages, no further<<NEWL>>            # retrieval is necessary<<NEWL>>            if all_retrieved:<<NEWL>>                break<<NEWL>>        return all_messages, all_retrieved<<NEWL>><<NEWL>>    def _store(self, messages, response, *args, **kwargs):<<NEWL>>        """"""<<NEWL>>        Store the messages and return any unstored messages after trying all<<NEWL>>        backends.<<NEWL>><<NEWL>>        For each storage backend, any messages not stored are passed on to the<<NEWL>>        next backend.<<NEWL>>        """"""<<NEWL>>        for storage in self.storages:<<NEWL>>            if messages:<<NEWL>>                messages = storage._store(messages, response, remove_oldest=False)<<NEWL>>            # Even if there are no more messages, continue iterating to ensure<<NEWL>>            # storages which contained messages are flushed.<<NEWL>>            elif storage in self._used_storages:<<NEWL>>                storage._store([], response)<<NEWL>>                self._used_storages.remove(storage)<<NEWL>>        return messages"
489	donghui	3	"from django.contrib.messages.storage.base import BaseStorage<<NEWL>>from django.contrib.messages.storage.cookie import CookieStorage<<NEWL>>from django.contrib.messages.storage.session import SessionStorage<<NEWL>><<NEWL>><<NEWL>>class FallbackStorage(BaseStorage):<<NEWL>>    """"""<<NEWL>>    Try to store all messages in the first backend. Store any unstored<<NEWL>>    messages in each subsequent backend.<<NEWL>>    """"""<<NEWL>>    storage_classes = (CookieStorage, SessionStorage)<<NEWL>><<NEWL>>    def __init__(self, *args, **kwargs):<<NEWL>>        super().__init__(*args, **kwargs)<<NEWL>>        self.storages = [storage_class(*args, **kwargs)<<NEWL>>                         for storage_class in self.storage_classes]<<NEWL>>        self._used_storages = set()<<NEWL>><<NEWL>>    def _get(self, *args, **kwargs):<<NEWL>>        """"""<<NEWL>>        Get a single list of messages from all storage backends.<<NEWL>>        """"""<<NEWL>>        all_messages = []<<NEWL>>        for storage in self.storages:<<NEWL>>            messages, all_retrieved = storage._get()<<NEWL>>            # If the backend hasn't been used, no more retrieval is necessary.<<NEWL>>            if messages is None:<<NEWL>>                break<<NEWL>>            if messages:<<NEWL>>                self._used_storages.add(storage)<<NEWL>>            all_messages.extend(messages)<<NEWL>>            # If this storage class contained all the messages, no further<<NEWL>>            # retrieval is necessary<<NEWL>>            if all_retrieved:<<NEWL>>                break<<NEWL>>        return all_messages, all_retrieved<<NEWL>><<NEWL>>    def _store(self, messages, response, *args, **kwargs):<<NEWL>>        """"""<<NEWL>>        Store the messages and return any unstored messages after trying all<<NEWL>>        backends.<<NEWL>><<NEWL>>        For each storage backend, any messages not stored are passed on to the<<NEWL>>        next backend.<<NEWL>>        """"""<<NEWL>>        for storage in self.storages:<<NEWL>>            if messages:<<NEWL>>                messages = storage._store(messages, response, remove_oldest=False)<<NEWL>>            # Even if there are no more messages, continue iterating to ensure<<NEWL>>            # storages which contained messages are flushed.<<NEWL>>            elif storage in self._used_storages:<<NEWL>>                storage._store([], response)<<NEWL>>                self._used_storages.remove(storage)<<NEWL>>        return messages"
499	jackson	4	"""""""Object Utilities.""""""<<NEWL>>from __future__ import absolute_import, unicode_literals<<NEWL>><<NEWL>><<NEWL>>class cached_property(object):<<NEWL>>    """"""Cached property descriptor.<<NEWL>><<NEWL>>    Caches the return value of the get method on first call.<<NEWL>><<NEWL>>    Examples:<<NEWL>>        .. code-block:: python<<NEWL>><<NEWL>>            @cached_property<<NEWL>>            def connection(self):<<NEWL>>                return Connection()<<NEWL>><<NEWL>>            @connection.setter  # Prepares stored value<<NEWL>>            def connection(self, value):<<NEWL>>                if value is None:<<NEWL>>                    raise TypeError('Connection must be a connection')<<NEWL>>                return value<<NEWL>><<NEWL>>            @connection.deleter<<NEWL>>            def connection(self, value):<<NEWL>>                # Additional action to do at del(self.attr)<<NEWL>>                if value is not None:<<NEWL>>                    print('Connection {0!r} deleted'.format(value)<<NEWL>>    """"""<<NEWL>><<NEWL>>    def __init__(self, fget=None, fset=None, fdel=None, doc=None):<<NEWL>>        self.__get = fget<<NEWL>>        self.__set = fset<<NEWL>>        self.__del = fdel<<NEWL>>        self.__doc__ = doc or fget.__doc__<<NEWL>>        self.__name__ = fget.__name__<<NEWL>>        self.__module__ = fget.__module__<<NEWL>><<NEWL>>    def __get__(self, obj, type=None):<<NEWL>>        if obj is None:<<NEWL>>            return self<<NEWL>>        try:<<NEWL>>            return obj.__dict__[self.__name__]<<NEWL>>        except KeyError:<<NEWL>>            value = obj.__dict__[self.__name__] = self.__get(obj)<<NEWL>>            return value<<NEWL>><<NEWL>>    def __set__(self, obj, value):<<NEWL>>        if obj is None:<<NEWL>>            return self<<NEWL>>        if self.__set is not None:<<NEWL>>            value = self.__set(obj, value)<<NEWL>>        obj.__dict__[self.__name__] = value<<NEWL>><<NEWL>>    def __delete__(self, obj, _sentinel=object()):<<NEWL>>        if obj is None:<<NEWL>>            return self<<NEWL>>        value = obj.__dict__.pop(self.__name__, _sentinel)<<NEWL>>        if self.__del is not None and value is not _sentinel:<<NEWL>>            self.__del(obj, value)<<NEWL>><<NEWL>>    def setter(self, fset):<<NEWL>>        return self.__class__(self.__get, fset, self.__del)<<NEWL>><<NEWL>>    def deleter(self, fdel):<<NEWL>>        return self.__class__(self.__get, self.__set, fdel)"
499	donghui	4	"""""""Object Utilities.""""""<<NEWL>>from __future__ import absolute_import, unicode_literals<<NEWL>><<NEWL>><<NEWL>>class cached_property(object):<<NEWL>>    """"""Cached property descriptor.<<NEWL>><<NEWL>>    Caches the return value of the get method on first call.<<NEWL>><<NEWL>>    Examples:<<NEWL>>        .. code-block:: python<<NEWL>><<NEWL>>            @cached_property<<NEWL>>            def connection(self):<<NEWL>>                return Connection()<<NEWL>><<NEWL>>            @connection.setter  # Prepares stored value<<NEWL>>            def connection(self, value):<<NEWL>>                if value is None:<<NEWL>>                    raise TypeError('Connection must be a connection')<<NEWL>>                return value<<NEWL>><<NEWL>>            @connection.deleter<<NEWL>>            def connection(self, value):<<NEWL>>                # Additional action to do at del(self.attr)<<NEWL>>                if value is not None:<<NEWL>>                    print('Connection {0!r} deleted'.format(value)<<NEWL>>    """"""<<NEWL>><<NEWL>>    def __init__(self, fget=None, fset=None, fdel=None, doc=None):<<NEWL>>        self.__get = fget<<NEWL>>        self.__set = fset<<NEWL>>        self.__del = fdel<<NEWL>>        self.__doc__ = doc or fget.__doc__<<NEWL>>        self.__name__ = fget.__name__<<NEWL>>        self.__module__ = fget.__module__<<NEWL>><<NEWL>>    def __get__(self, obj, type=None):<<NEWL>>        if obj is None:<<NEWL>>            return self<<NEWL>>        try:<<NEWL>>            return obj.__dict__[self.__name__]<<NEWL>>        except KeyError:<<NEWL>>            value = obj.__dict__[self.__name__] = self.__get(obj)<<NEWL>>            return value<<NEWL>><<NEWL>>    def __set__(self, obj, value):<<NEWL>>        if obj is None:<<NEWL>>            return self<<NEWL>>        if self.__set is not None:<<NEWL>>            value = self.__set(obj, value)<<NEWL>>        obj.__dict__[self.__name__] = value<<NEWL>><<NEWL>>    def __delete__(self, obj, _sentinel=object()):<<NEWL>>        if obj is None:<<NEWL>>            return self<<NEWL>>        value = obj.__dict__.pop(self.__name__, _sentinel)<<NEWL>>        if self.__del is not None and value is not _sentinel:<<NEWL>>            self.__del(obj, value)<<NEWL>><<NEWL>>    def setter(self, fset):<<NEWL>>        return self.__class__(self.__get, fset, self.__del)<<NEWL>><<NEWL>>    def deleter(self, fdel):<<NEWL>>        return self.__class__(self.__get, self.__set, fdel)"
386	jackson	2	"""""""<<NEWL>>rest_framework.schemas<<NEWL>><<NEWL>>schemas:<<NEWL>>    __init__.py<<NEWL>>    generators.py   # Top-down schema generation<<NEWL>>    inspectors.py   # Per-endpoint view introspection<<NEWL>>    utils.py        # Shared helper functions<<NEWL>>    views.py        # Houses `SchemaView`, `APIView` subclass.<<NEWL>><<NEWL>>We expose a minimal ""public"" API directly from `schemas`. This covers the<<NEWL>>basic use-cases:<<NEWL>><<NEWL>>    from rest_framework.schemas import (<<NEWL>>        AutoSchema,<<NEWL>>        ManualSchema,<<NEWL>>        get_schema_view,<<NEWL>>        SchemaGenerator,<<NEWL>>    )<<NEWL>><<NEWL>>Other access should target the submodules directly<<NEWL>>""""""<<NEWL>>from rest_framework.settings import api_settings<<NEWL>><<NEWL>>from . import coreapi, openapi<<NEWL>>from .coreapi import AutoSchema, ManualSchema, SchemaGenerator  # noqa<<NEWL>>from .inspectors import DefaultSchema  # noqa<<NEWL>><<NEWL>><<NEWL>>def get_schema_view(<<NEWL>>    title=None,<<NEWL>>    url=None,<<NEWL>>    description=None,<<NEWL>>    urlconf=None,<<NEWL>>    renderer_classes=None,<<NEWL>>    public=False,<<NEWL>>    patterns=None,<<NEWL>>    generator_class=None,<<NEWL>>    authentication_classes=api_settings.DEFAULT_AUTHENTICATION_CLASSES,<<NEWL>>    permission_classes=api_settings.DEFAULT_PERMISSION_CLASSES,<<NEWL>>    version=None,<<NEWL>>):<<NEWL>>    """"""<<NEWL>>    Return a schema view.<<NEWL>>    """"""<<NEWL>>    if generator_class is None:<<NEWL>>        if coreapi.is_enabled():<<NEWL>>            generator_class = coreapi.SchemaGenerator<<NEWL>>        else:<<NEWL>>            generator_class = openapi.SchemaGenerator<<NEWL>><<NEWL>>    generator = generator_class(<<NEWL>>        title=title,<<NEWL>>        url=url,<<NEWL>>        description=description,<<NEWL>>        urlconf=urlconf,<<NEWL>>        patterns=patterns,<<NEWL>>        version=version,<<NEWL>>    )<<NEWL>><<NEWL>>    # Avoid import cycle on APIView<<NEWL>>    from .views import SchemaView<<NEWL>><<NEWL>>    return SchemaView.as_view(<<NEWL>>        renderer_classes=renderer_classes,<<NEWL>>        schema_generator=generator,<<NEWL>>        public=public,<<NEWL>>        authentication_classes=authentication_classes,<<NEWL>>        permission_classes=permission_classes,<<NEWL>>    )"
386	donghui	2	"""""""<<NEWL>>rest_framework.schemas<<NEWL>><<NEWL>>schemas:<<NEWL>>    __init__.py<<NEWL>>    generators.py   # Top-down schema generation<<NEWL>>    inspectors.py   # Per-endpoint view introspection<<NEWL>>    utils.py        # Shared helper functions<<NEWL>>    views.py        # Houses `SchemaView`, `APIView` subclass.<<NEWL>><<NEWL>>We expose a minimal ""public"" API directly from `schemas`. This covers the<<NEWL>>basic use-cases:<<NEWL>><<NEWL>>    from rest_framework.schemas import (<<NEWL>>        AutoSchema,<<NEWL>>        ManualSchema,<<NEWL>>        get_schema_view,<<NEWL>>        SchemaGenerator,<<NEWL>>    )<<NEWL>><<NEWL>>Other access should target the submodules directly<<NEWL>>""""""<<NEWL>>from rest_framework.settings import api_settings<<NEWL>><<NEWL>>from . import coreapi, openapi<<NEWL>>from .coreapi import AutoSchema, ManualSchema, SchemaGenerator  # noqa<<NEWL>>from .inspectors import DefaultSchema  # noqa<<NEWL>><<NEWL>><<NEWL>>def get_schema_view(<<NEWL>>    title=None,<<NEWL>>    url=None,<<NEWL>>    description=None,<<NEWL>>    urlconf=None,<<NEWL>>    renderer_classes=None,<<NEWL>>    public=False,<<NEWL>>    patterns=None,<<NEWL>>    generator_class=None,<<NEWL>>    authentication_classes=api_settings.DEFAULT_AUTHENTICATION_CLASSES,<<NEWL>>    permission_classes=api_settings.DEFAULT_PERMISSION_CLASSES,<<NEWL>>    version=None,<<NEWL>>):<<NEWL>>    """"""<<NEWL>>    Return a schema view.<<NEWL>>    """"""<<NEWL>>    if generator_class is None:<<NEWL>>        if coreapi.is_enabled():<<NEWL>>            generator_class = coreapi.SchemaGenerator<<NEWL>>        else:<<NEWL>>            generator_class = openapi.SchemaGenerator<<NEWL>><<NEWL>>    generator = generator_class(<<NEWL>>        title=title,<<NEWL>>        url=url,<<NEWL>>        description=description,<<NEWL>>        urlconf=urlconf,<<NEWL>>        patterns=patterns,<<NEWL>>        version=version,<<NEWL>>    )<<NEWL>><<NEWL>>    # Avoid import cycle on APIView<<NEWL>>    from .views import SchemaView<<NEWL>><<NEWL>>    return SchemaView.as_view(<<NEWL>>        renderer_classes=renderer_classes,<<NEWL>>        schema_generator=generator,<<NEWL>>        public=public,<<NEWL>>        authentication_classes=authentication_classes,<<NEWL>>        permission_classes=permission_classes,<<NEWL>>    )"
297	jackson	0	"#  Copyright 2022 Google LLC<<NEWL>>#<<NEWL>>#  Licensed under the Apache License, Version 2.0 (the ""License"");<<NEWL>>#  you may not use this file except in compliance with the License.<<NEWL>>#  You may obtain a copy of the License at<<NEWL>>#<<NEWL>>#      http://www.apache.org/licenses/LICENSE-2.0<<NEWL>>#<<NEWL>>#  Unless required by applicable law or agreed to in writing, software<<NEWL>>#  distributed under the License is distributed on an ""AS IS"" BASIS,<<NEWL>>#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.<<NEWL>>#  See the License for the specific language governing permissions and<<NEWL>>#  limitations under the License.<<NEWL>>import uuid<<NEWL>><<NEWL>><<NEWL>>import google.auth<<NEWL>>from google.cloud import batch_v1<<NEWL>>from google.cloud import storage<<NEWL>>import pytest<<NEWL>><<NEWL>>from .test_basics import _test_body<<NEWL>>from ..create.create_with_mounted_bucket import create_script_job_with_bucket<<NEWL>><<NEWL>>PROJECT = google.auth.default()[1]<<NEWL>>REGION = 'europe-north1'<<NEWL>><<NEWL>>TIMEOUT = 600  # 10 minutes<<NEWL>><<NEWL>>WAIT_STATES = {<<NEWL>>    batch_v1.JobStatus.State.STATE_UNSPECIFIED,<<NEWL>>    batch_v1.JobStatus.State.QUEUED,<<NEWL>>    batch_v1.JobStatus.State.RUNNING,<<NEWL>>    batch_v1.JobStatus.State.SCHEDULED,<<NEWL>>}<<NEWL>><<NEWL>><<NEWL>>@pytest.fixture<<NEWL>>def job_name():<<NEWL>>    return f""test-job-{uuid.uuid4().hex[:10]}""<<NEWL>><<NEWL>><<NEWL>>@pytest.fixture()<<NEWL>>def test_bucket():<<NEWL>>    bucket_name = f""test-bucket-{uuid.uuid4().hex[:8]}""<<NEWL>>    client = storage.Client()<<NEWL>>    client.create_bucket(bucket_name, location=""eu"")<<NEWL>><<NEWL>>    yield bucket_name<<NEWL>><<NEWL>>    bucket = client.get_bucket(bucket_name)<<NEWL>>    bucket.delete(force=True)<<NEWL>><<NEWL>><<NEWL>>def _test_bucket_content(test_bucket):<<NEWL>>    client = storage.Client()<<NEWL>>    bucket = client.get_bucket(test_bucket)<<NEWL>><<NEWL>>    file_name_template = ""output_task_{task_number}.txt""<<NEWL>>    file_content_template = ""Hello world from task {task_number}.\n""<<NEWL>><<NEWL>>    for i in range(4):<<NEWL>>        blob = bucket.blob(file_name_template.format(task_number=i))<<NEWL>>        content = blob.download_as_bytes().decode()<<NEWL>>        assert content == file_content_template.format(task_number=i)<<NEWL>><<NEWL>><<NEWL>>def test_bucket_job(job_name, test_bucket):<<NEWL>>    job = create_script_job_with_bucket(PROJECT, REGION, job_name, test_bucket)<<NEWL>>    _test_body(job, lambda: _test_bucket_content(test_bucket))"
297	donghui	0	"#  Copyright 2022 Google LLC<<NEWL>>#<<NEWL>>#  Licensed under the Apache License, Version 2.0 (the ""License"");<<NEWL>>#  you may not use this file except in compliance with the License.<<NEWL>>#  You may obtain a copy of the License at<<NEWL>>#<<NEWL>>#      http://www.apache.org/licenses/LICENSE-2.0<<NEWL>>#<<NEWL>>#  Unless required by applicable law or agreed to in writing, software<<NEWL>>#  distributed under the License is distributed on an ""AS IS"" BASIS,<<NEWL>>#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.<<NEWL>>#  See the License for the specific language governing permissions and<<NEWL>>#  limitations under the License.<<NEWL>>import uuid<<NEWL>><<NEWL>><<NEWL>>import google.auth<<NEWL>>from google.cloud import batch_v1<<NEWL>>from google.cloud import storage<<NEWL>>import pytest<<NEWL>><<NEWL>>from .test_basics import _test_body<<NEWL>>from ..create.create_with_mounted_bucket import create_script_job_with_bucket<<NEWL>><<NEWL>>PROJECT = google.auth.default()[1]<<NEWL>>REGION = 'europe-north1'<<NEWL>><<NEWL>>TIMEOUT = 600  # 10 minutes<<NEWL>><<NEWL>>WAIT_STATES = {<<NEWL>>    batch_v1.JobStatus.State.STATE_UNSPECIFIED,<<NEWL>>    batch_v1.JobStatus.State.QUEUED,<<NEWL>>    batch_v1.JobStatus.State.RUNNING,<<NEWL>>    batch_v1.JobStatus.State.SCHEDULED,<<NEWL>>}<<NEWL>><<NEWL>><<NEWL>>@pytest.fixture<<NEWL>>def job_name():<<NEWL>>    return f""test-job-{uuid.uuid4().hex[:10]}""<<NEWL>><<NEWL>><<NEWL>>@pytest.fixture()<<NEWL>>def test_bucket():<<NEWL>>    bucket_name = f""test-bucket-{uuid.uuid4().hex[:8]}""<<NEWL>>    client = storage.Client()<<NEWL>>    client.create_bucket(bucket_name, location=""eu"")<<NEWL>><<NEWL>>    yield bucket_name<<NEWL>><<NEWL>>    bucket = client.get_bucket(bucket_name)<<NEWL>>    bucket.delete(force=True)<<NEWL>><<NEWL>><<NEWL>>def _test_bucket_content(test_bucket):<<NEWL>>    client = storage.Client()<<NEWL>>    bucket = client.get_bucket(test_bucket)<<NEWL>><<NEWL>>    file_name_template = ""output_task_{task_number}.txt""<<NEWL>>    file_content_template = ""Hello world from task {task_number}.\n""<<NEWL>><<NEWL>>    for i in range(4):<<NEWL>>        blob = bucket.blob(file_name_template.format(task_number=i))<<NEWL>>        content = blob.download_as_bytes().decode()<<NEWL>>        assert content == file_content_template.format(task_number=i)<<NEWL>><<NEWL>><<NEWL>>def test_bucket_job(job_name, test_bucket):<<NEWL>>    job = create_script_job_with_bucket(PROJECT, REGION, job_name, test_bucket)<<NEWL>>    _test_body(job, lambda: _test_bucket_content(test_bucket))"
264	jackson	1	"# automatically generated by the FlatBuffers compiler, do not modify<<NEWL>><<NEWL>># namespace: proto<<NEWL>><<NEWL>>import flatbuffers<<NEWL>>from flatbuffers.compat import import_numpy<<NEWL>>np = import_numpy()<<NEWL>><<NEWL>>class RouterRoles(object):<<NEWL>>    __slots__ = ['_tab']<<NEWL>><<NEWL>>    @classmethod<<NEWL>>    def GetRootAs(cls, buf, offset=0):<<NEWL>>        n = flatbuffers.encode.Get(flatbuffers.packer.uoffset, buf, offset)<<NEWL>>        x = RouterRoles()<<NEWL>>        x.Init(buf, n + offset)<<NEWL>>        return x<<NEWL>><<NEWL>>    @classmethod<<NEWL>>    def GetRootAsRouterRoles(cls, buf, offset=0):<<NEWL>>        """"""This method is deprecated. Please switch to GetRootAs.""""""<<NEWL>>        return cls.GetRootAs(buf, offset)<<NEWL>>    # RouterRoles<<NEWL>>    def Init(self, buf, pos):<<NEWL>>        self._tab = flatbuffers.table.Table(buf, pos)<<NEWL>><<NEWL>>    # RouterRoles<<NEWL>>    def Broker(self):<<NEWL>>        o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(4))<<NEWL>>        if o != 0:<<NEWL>>            x = self._tab.Indirect(o + self._tab.Pos)<<NEWL>>            from wamp.proto.BrokerFeatures import BrokerFeatures<<NEWL>>            obj = BrokerFeatures()<<NEWL>>            obj.Init(self._tab.Bytes, x)<<NEWL>>            return obj<<NEWL>>        return None<<NEWL>><<NEWL>>    # RouterRoles<<NEWL>>    def Dealer(self):<<NEWL>>        o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(6))<<NEWL>>        if o != 0:<<NEWL>>            x = self._tab.Indirect(o + self._tab.Pos)<<NEWL>>            from wamp.proto.DealerFeatures import DealerFeatures<<NEWL>>            obj = DealerFeatures()<<NEWL>>            obj.Init(self._tab.Bytes, x)<<NEWL>>            return obj<<NEWL>>        return None<<NEWL>><<NEWL>>def RouterRolesStart(builder): builder.StartObject(2)<<NEWL>>def Start(builder):<<NEWL>>    return RouterRolesStart(builder)<<NEWL>>def RouterRolesAddBroker(builder, broker): builder.PrependUOffsetTRelativeSlot(0, flatbuffers.number_types.UOffsetTFlags.py_type(broker), 0)<<NEWL>>def AddBroker(builder, broker):<<NEWL>>    return RouterRolesAddBroker(builder, broker)<<NEWL>>def RouterRolesAddDealer(builder, dealer): builder.PrependUOffsetTRelativeSlot(1, flatbuffers.number_types.UOffsetTFlags.py_type(dealer), 0)<<NEWL>>def AddDealer(builder, dealer):<<NEWL>>    return RouterRolesAddDealer(builder, dealer)<<NEWL>>def RouterRolesEnd(builder): return builder.EndObject()<<NEWL>>def End(builder):<<NEWL>>    return RouterRolesEnd(builder)"
264	donghui	1	"# automatically generated by the FlatBuffers compiler, do not modify<<NEWL>><<NEWL>># namespace: proto<<NEWL>><<NEWL>>import flatbuffers<<NEWL>>from flatbuffers.compat import import_numpy<<NEWL>>np = import_numpy()<<NEWL>><<NEWL>>class RouterRoles(object):<<NEWL>>    __slots__ = ['_tab']<<NEWL>><<NEWL>>    @classmethod<<NEWL>>    def GetRootAs(cls, buf, offset=0):<<NEWL>>        n = flatbuffers.encode.Get(flatbuffers.packer.uoffset, buf, offset)<<NEWL>>        x = RouterRoles()<<NEWL>>        x.Init(buf, n + offset)<<NEWL>>        return x<<NEWL>><<NEWL>>    @classmethod<<NEWL>>    def GetRootAsRouterRoles(cls, buf, offset=0):<<NEWL>>        """"""This method is deprecated. Please switch to GetRootAs.""""""<<NEWL>>        return cls.GetRootAs(buf, offset)<<NEWL>>    # RouterRoles<<NEWL>>    def Init(self, buf, pos):<<NEWL>>        self._tab = flatbuffers.table.Table(buf, pos)<<NEWL>><<NEWL>>    # RouterRoles<<NEWL>>    def Broker(self):<<NEWL>>        o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(4))<<NEWL>>        if o != 0:<<NEWL>>            x = self._tab.Indirect(o + self._tab.Pos)<<NEWL>>            from wamp.proto.BrokerFeatures import BrokerFeatures<<NEWL>>            obj = BrokerFeatures()<<NEWL>>            obj.Init(self._tab.Bytes, x)<<NEWL>>            return obj<<NEWL>>        return None<<NEWL>><<NEWL>>    # RouterRoles<<NEWL>>    def Dealer(self):<<NEWL>>        o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(6))<<NEWL>>        if o != 0:<<NEWL>>            x = self._tab.Indirect(o + self._tab.Pos)<<NEWL>>            from wamp.proto.DealerFeatures import DealerFeatures<<NEWL>>            obj = DealerFeatures()<<NEWL>>            obj.Init(self._tab.Bytes, x)<<NEWL>>            return obj<<NEWL>>        return None<<NEWL>><<NEWL>>def RouterRolesStart(builder): builder.StartObject(2)<<NEWL>>def Start(builder):<<NEWL>>    return RouterRolesStart(builder)<<NEWL>>def RouterRolesAddBroker(builder, broker): builder.PrependUOffsetTRelativeSlot(0, flatbuffers.number_types.UOffsetTFlags.py_type(broker), 0)<<NEWL>>def AddBroker(builder, broker):<<NEWL>>    return RouterRolesAddBroker(builder, broker)<<NEWL>>def RouterRolesAddDealer(builder, dealer): builder.PrependUOffsetTRelativeSlot(1, flatbuffers.number_types.UOffsetTFlags.py_type(dealer), 0)<<NEWL>>def AddDealer(builder, dealer):<<NEWL>>    return RouterRolesAddDealer(builder, dealer)<<NEWL>>def RouterRolesEnd(builder): return builder.EndObject()<<NEWL>>def End(builder):<<NEWL>>    return RouterRolesEnd(builder)"
324	jackson	0	from urllib.parse import parse_qsl, unquote, urlparse, urlunparse<<NEWL>><<NEWL>>from django import template<<NEWL>>from django.contrib.admin.utils import quote<<NEWL>>from django.urls import Resolver404, get_script_prefix, resolve<<NEWL>>from django.utils.http import urlencode<<NEWL>><<NEWL>>register = template.Library()<<NEWL>><<NEWL>><<NEWL>>@register.filter<<NEWL>>def admin_urlname(value, arg):<<NEWL>>    return 'admin:%s_%s_%s' % (value.app_label, value.model_name, arg)<<NEWL>><<NEWL>><<NEWL>>@register.filter<<NEWL>>def admin_urlquote(value):<<NEWL>>    return quote(value)<<NEWL>><<NEWL>><<NEWL>>@register.simple_tag(takes_context=True)<<NEWL>>def add_preserved_filters(context, url, popup=False, to_field=None):<<NEWL>>    opts = context.get('opts')<<NEWL>>    preserved_filters = context.get('preserved_filters')<<NEWL>><<NEWL>>    parsed_url = list(urlparse(url))<<NEWL>>    parsed_qs = dict(parse_qsl(parsed_url[4]))<<NEWL>>    merged_qs = {}<<NEWL>><<NEWL>>    if opts and preserved_filters:<<NEWL>>        preserved_filters = dict(parse_qsl(preserved_filters))<<NEWL>><<NEWL>>        match_url = '/%s' % unquote(url).partition(get_script_prefix())[2]<<NEWL>>        try:<<NEWL>>            match = resolve(match_url)<<NEWL>>        except Resolver404:<<NEWL>>            pass<<NEWL>>        else:<<NEWL>>            current_url = '%s:%s' % (match.app_name, match.url_name)<<NEWL>>            changelist_url = 'admin:%s_%s_changelist' % (opts.app_label, opts.model_name)<<NEWL>>            if changelist_url == current_url and '_changelist_filters' in preserved_filters:<<NEWL>>                preserved_filters = dict(parse_qsl(preserved_filters['_changelist_filters']))<<NEWL>><<NEWL>>        merged_qs.update(preserved_filters)<<NEWL>><<NEWL>>    if popup:<<NEWL>>        from django.contrib.admin.options import IS_POPUP_VAR<<NEWL>>        merged_qs[IS_POPUP_VAR] = 1<<NEWL>>    if to_field:<<NEWL>>        from django.contrib.admin.options import TO_FIELD_VAR<<NEWL>>        merged_qs[TO_FIELD_VAR] = to_field<<NEWL>><<NEWL>>    merged_qs.update(parsed_qs)<<NEWL>><<NEWL>>    parsed_url[4] = urlencode(merged_qs)<<NEWL>>    return urlunparse(parsed_url)
324	donghui	0	from urllib.parse import parse_qsl, unquote, urlparse, urlunparse<<NEWL>><<NEWL>>from django import template<<NEWL>>from django.contrib.admin.utils import quote<<NEWL>>from django.urls import Resolver404, get_script_prefix, resolve<<NEWL>>from django.utils.http import urlencode<<NEWL>><<NEWL>>register = template.Library()<<NEWL>><<NEWL>><<NEWL>>@register.filter<<NEWL>>def admin_urlname(value, arg):<<NEWL>>    return 'admin:%s_%s_%s' % (value.app_label, value.model_name, arg)<<NEWL>><<NEWL>><<NEWL>>@register.filter<<NEWL>>def admin_urlquote(value):<<NEWL>>    return quote(value)<<NEWL>><<NEWL>><<NEWL>>@register.simple_tag(takes_context=True)<<NEWL>>def add_preserved_filters(context, url, popup=False, to_field=None):<<NEWL>>    opts = context.get('opts')<<NEWL>>    preserved_filters = context.get('preserved_filters')<<NEWL>><<NEWL>>    parsed_url = list(urlparse(url))<<NEWL>>    parsed_qs = dict(parse_qsl(parsed_url[4]))<<NEWL>>    merged_qs = {}<<NEWL>><<NEWL>>    if opts and preserved_filters:<<NEWL>>        preserved_filters = dict(parse_qsl(preserved_filters))<<NEWL>><<NEWL>>        match_url = '/%s' % unquote(url).partition(get_script_prefix())[2]<<NEWL>>        try:<<NEWL>>            match = resolve(match_url)<<NEWL>>        except Resolver404:<<NEWL>>            pass<<NEWL>>        else:<<NEWL>>            current_url = '%s:%s' % (match.app_name, match.url_name)<<NEWL>>            changelist_url = 'admin:%s_%s_changelist' % (opts.app_label, opts.model_name)<<NEWL>>            if changelist_url == current_url and '_changelist_filters' in preserved_filters:<<NEWL>>                preserved_filters = dict(parse_qsl(preserved_filters['_changelist_filters']))<<NEWL>><<NEWL>>        merged_qs.update(preserved_filters)<<NEWL>><<NEWL>>    if popup:<<NEWL>>        from django.contrib.admin.options import IS_POPUP_VAR<<NEWL>>        merged_qs[IS_POPUP_VAR] = 1<<NEWL>>    if to_field:<<NEWL>>        from django.contrib.admin.options import TO_FIELD_VAR<<NEWL>>        merged_qs[TO_FIELD_VAR] = to_field<<NEWL>><<NEWL>>    merged_qs.update(parsed_qs)<<NEWL>><<NEWL>>    parsed_url[4] = urlencode(merged_qs)<<NEWL>>    return urlunparse(parsed_url)
375	jackson	3	"#<<NEWL>># The Python Imaging Library.<<NEWL>># $Id$<<NEWL>>#<<NEWL>># Binary input/output support routines.<<NEWL>>#<<NEWL>># Copyright (c) 1997-2003 by Secret Labs AB<<NEWL>># Copyright (c) 1995-2003 by Fredrik Lundh<<NEWL>># Copyright (c) 2012 by Brian Crowell<<NEWL>>#<<NEWL>># See the README file for information on usage and redistribution.<<NEWL>>#<<NEWL>><<NEWL>><<NEWL>>""""""Binary input/output support routines.""""""<<NEWL>><<NEWL>><<NEWL>>from struct import pack, unpack_from<<NEWL>><<NEWL>><<NEWL>>def i8(c):<<NEWL>>    return c if c.__class__ is int else c[0]<<NEWL>><<NEWL>><<NEWL>>def o8(i):<<NEWL>>    return bytes((i & 255,))<<NEWL>><<NEWL>><<NEWL>># Input, le = little endian, be = big endian<<NEWL>>def i16le(c, o=0):<<NEWL>>    """"""<<NEWL>>    Converts a 2-bytes (16 bits) string to an unsigned integer.<<NEWL>><<NEWL>>    :param c: string containing bytes to convert<<NEWL>>    :param o: offset of bytes to convert in string<<NEWL>>    """"""<<NEWL>>    return unpack_from(""<H"", c, o)[0]<<NEWL>><<NEWL>><<NEWL>>def si16le(c, o=0):<<NEWL>>    """"""<<NEWL>>    Converts a 2-bytes (16 bits) string to a signed integer.<<NEWL>><<NEWL>>    :param c: string containing bytes to convert<<NEWL>>    :param o: offset of bytes to convert in string<<NEWL>>    """"""<<NEWL>>    return unpack_from(""<h"", c, o)[0]<<NEWL>><<NEWL>><<NEWL>>def si16be(c, o=0):<<NEWL>>    """"""<<NEWL>>    Converts a 2-bytes (16 bits) string to a signed integer, big endian.<<NEWL>><<NEWL>>    :param c: string containing bytes to convert<<NEWL>>    :param o: offset of bytes to convert in string<<NEWL>>    """"""<<NEWL>>    return unpack_from("">h"", c, o)[0]<<NEWL>><<NEWL>><<NEWL>>def i32le(c, o=0):<<NEWL>>    """"""<<NEWL>>    Converts a 4-bytes (32 bits) string to an unsigned integer.<<NEWL>><<NEWL>>    :param c: string containing bytes to convert<<NEWL>>    :param o: offset of bytes to convert in string<<NEWL>>    """"""<<NEWL>>    return unpack_from(""<I"", c, o)[0]<<NEWL>><<NEWL>><<NEWL>>def si32le(c, o=0):<<NEWL>>    """"""<<NEWL>>    Converts a 4-bytes (32 bits) string to a signed integer.<<NEWL>><<NEWL>>    :param c: string containing bytes to convert<<NEWL>>    :param o: offset of bytes to convert in string<<NEWL>>    """"""<<NEWL>>    return unpack_from(""<i"", c, o)[0]<<NEWL>><<NEWL>><<NEWL>>def i16be(c, o=0):<<NEWL>>    return unpack_from("">H"", c, o)[0]<<NEWL>><<NEWL>><<NEWL>>def i32be(c, o=0):<<NEWL>>    return unpack_from("">I"", c, o)[0]<<NEWL>><<NEWL>><<NEWL>># Output, le = little endian, be = big endian<<NEWL>>def o16le(i):<<NEWL>>    return pack(""<H"", i)<<NEWL>><<NEWL>><<NEWL>>def o32le(i):<<NEWL>>    return pack(""<I"", i)<<NEWL>><<NEWL>><<NEWL>>def o16be(i):<<NEWL>>    return pack("">H"", i)<<NEWL>><<NEWL>><<NEWL>>def o32be(i):<<NEWL>>    return pack("">I"", i)"
375	donghui	2	"#<<NEWL>># The Python Imaging Library.<<NEWL>># $Id$<<NEWL>>#<<NEWL>># Binary input/output support routines.<<NEWL>>#<<NEWL>># Copyright (c) 1997-2003 by Secret Labs AB<<NEWL>># Copyright (c) 1995-2003 by Fredrik Lundh<<NEWL>># Copyright (c) 2012 by Brian Crowell<<NEWL>>#<<NEWL>># See the README file for information on usage and redistribution.<<NEWL>>#<<NEWL>><<NEWL>><<NEWL>>""""""Binary input/output support routines.""""""<<NEWL>><<NEWL>><<NEWL>>from struct import pack, unpack_from<<NEWL>><<NEWL>><<NEWL>>def i8(c):<<NEWL>>    return c if c.__class__ is int else c[0]<<NEWL>><<NEWL>><<NEWL>>def o8(i):<<NEWL>>    return bytes((i & 255,))<<NEWL>><<NEWL>><<NEWL>># Input, le = little endian, be = big endian<<NEWL>>def i16le(c, o=0):<<NEWL>>    """"""<<NEWL>>    Converts a 2-bytes (16 bits) string to an unsigned integer.<<NEWL>><<NEWL>>    :param c: string containing bytes to convert<<NEWL>>    :param o: offset of bytes to convert in string<<NEWL>>    """"""<<NEWL>>    return unpack_from(""<H"", c, o)[0]<<NEWL>><<NEWL>><<NEWL>>def si16le(c, o=0):<<NEWL>>    """"""<<NEWL>>    Converts a 2-bytes (16 bits) string to a signed integer.<<NEWL>><<NEWL>>    :param c: string containing bytes to convert<<NEWL>>    :param o: offset of bytes to convert in string<<NEWL>>    """"""<<NEWL>>    return unpack_from(""<h"", c, o)[0]<<NEWL>><<NEWL>><<NEWL>>def si16be(c, o=0):<<NEWL>>    """"""<<NEWL>>    Converts a 2-bytes (16 bits) string to a signed integer, big endian.<<NEWL>><<NEWL>>    :param c: string containing bytes to convert<<NEWL>>    :param o: offset of bytes to convert in string<<NEWL>>    """"""<<NEWL>>    return unpack_from("">h"", c, o)[0]<<NEWL>><<NEWL>><<NEWL>>def i32le(c, o=0):<<NEWL>>    """"""<<NEWL>>    Converts a 4-bytes (32 bits) string to an unsigned integer.<<NEWL>><<NEWL>>    :param c: string containing bytes to convert<<NEWL>>    :param o: offset of bytes to convert in string<<NEWL>>    """"""<<NEWL>>    return unpack_from(""<I"", c, o)[0]<<NEWL>><<NEWL>><<NEWL>>def si32le(c, o=0):<<NEWL>>    """"""<<NEWL>>    Converts a 4-bytes (32 bits) string to a signed integer.<<NEWL>><<NEWL>>    :param c: string containing bytes to convert<<NEWL>>    :param o: offset of bytes to convert in string<<NEWL>>    """"""<<NEWL>>    return unpack_from(""<i"", c, o)[0]<<NEWL>><<NEWL>><<NEWL>>def i16be(c, o=0):<<NEWL>>    return unpack_from("">H"", c, o)[0]<<NEWL>><<NEWL>><<NEWL>>def i32be(c, o=0):<<NEWL>>    return unpack_from("">I"", c, o)[0]<<NEWL>><<NEWL>><<NEWL>># Output, le = little endian, be = big endian<<NEWL>>def o16le(i):<<NEWL>>    return pack(""<H"", i)<<NEWL>><<NEWL>><<NEWL>>def o32le(i):<<NEWL>>    return pack(""<I"", i)<<NEWL>><<NEWL>><<NEWL>>def o16be(i):<<NEWL>>    return pack("">H"", i)<<NEWL>><<NEWL>><<NEWL>>def o32be(i):<<NEWL>>    return pack("">I"", i)"
341	jackson	1	"from django.db import models<<NEWL>>from django.contrib.auth.models import User<<NEWL>>from io import BytesIO<<NEWL>>import sys<<NEWL>>print(sys.path)<<NEWL>>from PIL import Image<<NEWL>>from django.core.files.uploadedfile import InMemoryUploadedFile<<NEWL>><<NEWL>><<NEWL>># Create your models here.<<NEWL>><<NEWL>>class Blog_Post(models.Model, object):<<NEWL>>    image = models.ImageField(blank= True, upload_to='get_upload_file_name')<<NEWL>>    title = models.CharField(blank=True, max_length = 100)<<NEWL>>    summary = models.TextField(blank= True, max_length =30)<<NEWL>>    body = models.TextField(blank=True)<<NEWL>>    slug = models.SlugField( unique=True)<<NEWL>>    writer = models.ForeignKey(User,on_delete= models.CASCADE)<<NEWL>>    created_on = models.DateTimeField(auto_now_add=True)<<NEWL>><<NEWL>>    def __str__(self) -> str:<<NEWL>>        return self.title<<NEWL>>    def save(self):<<NEWL>>        # Opening the uploaded image<<NEWL>>        im = Image.open(self.image)<<NEWL>><<NEWL>>        output = BytesIO()<<NEWL>><<NEWL>>        # Resize/modify the image<<NEWL>>        im = im.resize((1024, 720))<<NEWL>><<NEWL>>        # after modifications, save it to the output<<NEWL>>        im.save(output, format='png', quality=300)<<NEWL>>        output.seek(0)<<NEWL>><<NEWL>>        # change the imagefield value to be the newley modifed image value<<NEWL>>        self.image = InMemoryUploadedFile(output, 'ImageField', ""%s.webp"" % self.image.name.split('.')[0], 'image/webp',<<NEWL>>                                        sys.getsizeof(output), None)<<NEWL>><<NEWL>>        super(Blog_Post, self).save()<<NEWL>><<NEWL>>class Comment(models.Model):<<NEWL>>        commenter = models.CharField(max_length=15)<<NEWL>>        body = models.TextField(max_length=30, blank=True)<<NEWL>>        post = models.ForeignKey(Blog_Post, on_delete=models.CASCADE, related_name='comments')<<NEWL>>        date = models.DateField(auto_now_add=True)<<NEWL>>        like = models.BooleanField(default=True)<<NEWL>>        def __str__(self) -> str:<<NEWL>>             return self.commenter<<NEWL>><<NEWL>><<NEWL>><<NEWL>>class Meta:<<NEWL>>    ordering = ('-created_at',)<<NEWL>><<NEWL>>   "
341	donghui	2	"from django.db import models<<NEWL>>from django.contrib.auth.models import User<<NEWL>>from io import BytesIO<<NEWL>>import sys<<NEWL>>print(sys.path)<<NEWL>>from PIL import Image<<NEWL>>from django.core.files.uploadedfile import InMemoryUploadedFile<<NEWL>><<NEWL>><<NEWL>># Create your models here.<<NEWL>><<NEWL>>class Blog_Post(models.Model, object):<<NEWL>>    image = models.ImageField(blank= True, upload_to='get_upload_file_name')<<NEWL>>    title = models.CharField(blank=True, max_length = 100)<<NEWL>>    summary = models.TextField(blank= True, max_length =30)<<NEWL>>    body = models.TextField(blank=True)<<NEWL>>    slug = models.SlugField( unique=True)<<NEWL>>    writer = models.ForeignKey(User,on_delete= models.CASCADE)<<NEWL>>    created_on = models.DateTimeField(auto_now_add=True)<<NEWL>><<NEWL>>    def __str__(self) -> str:<<NEWL>>        return self.title<<NEWL>>    def save(self):<<NEWL>>        # Opening the uploaded image<<NEWL>>        im = Image.open(self.image)<<NEWL>><<NEWL>>        output = BytesIO()<<NEWL>><<NEWL>>        # Resize/modify the image<<NEWL>>        im = im.resize((1024, 720))<<NEWL>><<NEWL>>        # after modifications, save it to the output<<NEWL>>        im.save(output, format='png', quality=300)<<NEWL>>        output.seek(0)<<NEWL>><<NEWL>>        # change the imagefield value to be the newley modifed image value<<NEWL>>        self.image = InMemoryUploadedFile(output, 'ImageField', ""%s.webp"" % self.image.name.split('.')[0], 'image/webp',<<NEWL>>                                        sys.getsizeof(output), None)<<NEWL>><<NEWL>>        super(Blog_Post, self).save()<<NEWL>><<NEWL>>class Comment(models.Model):<<NEWL>>        commenter = models.CharField(max_length=15)<<NEWL>>        body = models.TextField(max_length=30, blank=True)<<NEWL>>        post = models.ForeignKey(Blog_Post, on_delete=models.CASCADE, related_name='comments')<<NEWL>>        date = models.DateField(auto_now_add=True)<<NEWL>>        like = models.BooleanField(default=True)<<NEWL>>        def __str__(self) -> str:<<NEWL>>             return self.commenter<<NEWL>><<NEWL>><<NEWL>><<NEWL>>class Meta:<<NEWL>>    ordering = ('-created_at',)<<NEWL>><<NEWL>>   "
310	jackson	1	"from datetime import datetime<<NEWL>><<NEWL>>import pytest<<NEWL>><<NEWL>>from pandas._libs import tslib<<NEWL>><<NEWL>><<NEWL>>@pytest.mark.parametrize(<<NEWL>>    ""date_str, exp"",<<NEWL>>    [<<NEWL>>        (""2011-01-02"", datetime(2011, 1, 2)),<<NEWL>>        (""2011-1-2"", datetime(2011, 1, 2)),<<NEWL>>        (""2011-01"", datetime(2011, 1, 1)),<<NEWL>>        (""2011-1"", datetime(2011, 1, 1)),<<NEWL>>        (""2011 01 02"", datetime(2011, 1, 2)),<<NEWL>>        (""2011.01.02"", datetime(2011, 1, 2)),<<NEWL>>        (""2011/01/02"", datetime(2011, 1, 2)),<<NEWL>>        (""2011\\01\\02"", datetime(2011, 1, 2)),<<NEWL>>        (""2013-01-01 05:30:00"", datetime(2013, 1, 1, 5, 30)),<<NEWL>>        (""2013-1-1 5:30:00"", datetime(2013, 1, 1, 5, 30)),<<NEWL>>    ],<<NEWL>>)<<NEWL>>def test_parsers_iso8601(date_str, exp):<<NEWL>>    # see gh-12060<<NEWL>>    #<<NEWL>>    # Test only the ISO parser - flexibility to<<NEWL>>    # different separators and leading zero's.<<NEWL>>    actual = tslib._test_parse_iso8601(date_str)<<NEWL>>    assert actual == exp<<NEWL>><<NEWL>><<NEWL>>@pytest.mark.parametrize(<<NEWL>>    ""date_str"",<<NEWL>>    [<<NEWL>>        ""2011-01/02"",<<NEWL>>        ""2011=11=11"",<<NEWL>>        ""201401"",<<NEWL>>        ""201111"",<<NEWL>>        ""200101"",<<NEWL>>        # Mixed separated and unseparated.<<NEWL>>        ""2005-0101"",<<NEWL>>        ""200501-01"",<<NEWL>>        ""20010101 12:3456"",<<NEWL>>        ""20010101 1234:56"",<<NEWL>>        # HHMMSS must have two digits in<<NEWL>>        # each component if unseparated.<<NEWL>>        ""20010101 1"",<<NEWL>>        ""20010101 123"",<<NEWL>>        ""20010101 12345"",<<NEWL>>        ""20010101 12345Z"",<<NEWL>>    ],<<NEWL>>)<<NEWL>>def test_parsers_iso8601_invalid(date_str):<<NEWL>>    msg = f'Error parsing datetime string ""{date_str}""'<<NEWL>><<NEWL>>    with pytest.raises(ValueError, match=msg):<<NEWL>>        tslib._test_parse_iso8601(date_str)<<NEWL>><<NEWL>><<NEWL>>def test_parsers_iso8601_invalid_offset_invalid():<<NEWL>>    date_str = ""2001-01-01 12-34-56""<<NEWL>>    msg = f'Timezone hours offset out of range in datetime string ""{date_str}""'<<NEWL>><<NEWL>>    with pytest.raises(ValueError, match=msg):<<NEWL>>        tslib._test_parse_iso8601(date_str)<<NEWL>><<NEWL>><<NEWL>>def test_parsers_iso8601_leading_space():<<NEWL>>    # GH#25895 make sure isoparser doesn't overflow with long input<<NEWL>>    date_str, expected = (""2013-1-1 5:30:00"", datetime(2013, 1, 1, 5, 30))<<NEWL>>    actual = tslib._test_parse_iso8601("" "" * 200 + date_str)<<NEWL>>    assert actual == expected"
310	donghui	1	"from datetime import datetime<<NEWL>><<NEWL>>import pytest<<NEWL>><<NEWL>>from pandas._libs import tslib<<NEWL>><<NEWL>><<NEWL>>@pytest.mark.parametrize(<<NEWL>>    ""date_str, exp"",<<NEWL>>    [<<NEWL>>        (""2011-01-02"", datetime(2011, 1, 2)),<<NEWL>>        (""2011-1-2"", datetime(2011, 1, 2)),<<NEWL>>        (""2011-01"", datetime(2011, 1, 1)),<<NEWL>>        (""2011-1"", datetime(2011, 1, 1)),<<NEWL>>        (""2011 01 02"", datetime(2011, 1, 2)),<<NEWL>>        (""2011.01.02"", datetime(2011, 1, 2)),<<NEWL>>        (""2011/01/02"", datetime(2011, 1, 2)),<<NEWL>>        (""2011\\01\\02"", datetime(2011, 1, 2)),<<NEWL>>        (""2013-01-01 05:30:00"", datetime(2013, 1, 1, 5, 30)),<<NEWL>>        (""2013-1-1 5:30:00"", datetime(2013, 1, 1, 5, 30)),<<NEWL>>    ],<<NEWL>>)<<NEWL>>def test_parsers_iso8601(date_str, exp):<<NEWL>>    # see gh-12060<<NEWL>>    #<<NEWL>>    # Test only the ISO parser - flexibility to<<NEWL>>    # different separators and leading zero's.<<NEWL>>    actual = tslib._test_parse_iso8601(date_str)<<NEWL>>    assert actual == exp<<NEWL>><<NEWL>><<NEWL>>@pytest.mark.parametrize(<<NEWL>>    ""date_str"",<<NEWL>>    [<<NEWL>>        ""2011-01/02"",<<NEWL>>        ""2011=11=11"",<<NEWL>>        ""201401"",<<NEWL>>        ""201111"",<<NEWL>>        ""200101"",<<NEWL>>        # Mixed separated and unseparated.<<NEWL>>        ""2005-0101"",<<NEWL>>        ""200501-01"",<<NEWL>>        ""20010101 12:3456"",<<NEWL>>        ""20010101 1234:56"",<<NEWL>>        # HHMMSS must have two digits in<<NEWL>>        # each component if unseparated.<<NEWL>>        ""20010101 1"",<<NEWL>>        ""20010101 123"",<<NEWL>>        ""20010101 12345"",<<NEWL>>        ""20010101 12345Z"",<<NEWL>>    ],<<NEWL>>)<<NEWL>>def test_parsers_iso8601_invalid(date_str):<<NEWL>>    msg = f'Error parsing datetime string ""{date_str}""'<<NEWL>><<NEWL>>    with pytest.raises(ValueError, match=msg):<<NEWL>>        tslib._test_parse_iso8601(date_str)<<NEWL>><<NEWL>><<NEWL>>def test_parsers_iso8601_invalid_offset_invalid():<<NEWL>>    date_str = ""2001-01-01 12-34-56""<<NEWL>>    msg = f'Timezone hours offset out of range in datetime string ""{date_str}""'<<NEWL>><<NEWL>>    with pytest.raises(ValueError, match=msg):<<NEWL>>        tslib._test_parse_iso8601(date_str)<<NEWL>><<NEWL>><<NEWL>>def test_parsers_iso8601_leading_space():<<NEWL>>    # GH#25895 make sure isoparser doesn't overflow with long input<<NEWL>>    date_str, expected = (""2013-1-1 5:30:00"", datetime(2013, 1, 1, 5, 30))<<NEWL>>    actual = tslib._test_parse_iso8601("" "" * 200 + date_str)<<NEWL>>    assert actual == expected"
471	jackson	3	"""""""Metadata generation logic for legacy source distributions.<<NEWL>>""""""<<NEWL>><<NEWL>>import logging<<NEWL>>import os<<NEWL>><<NEWL>>from pip._internal.build_env import BuildEnvironment<<NEWL>>from pip._internal.exceptions import InstallationError<<NEWL>>from pip._internal.utils.setuptools_build import make_setuptools_egg_info_args<<NEWL>>from pip._internal.utils.subprocess import call_subprocess<<NEWL>>from pip._internal.utils.temp_dir import TempDirectory<<NEWL>><<NEWL>>logger = logging.getLogger(__name__)<<NEWL>><<NEWL>><<NEWL>>def _find_egg_info(directory):<<NEWL>>    # type: (str) -> str<<NEWL>>    """"""Find an .egg-info subdirectory in `directory`.<<NEWL>>    """"""<<NEWL>>    filenames = [<<NEWL>>        f for f in os.listdir(directory) if f.endswith("".egg-info"")<<NEWL>>    ]<<NEWL>><<NEWL>>    if not filenames:<<NEWL>>        raise InstallationError(<<NEWL>>            f""No .egg-info directory found in {directory}""<<NEWL>>        )<<NEWL>><<NEWL>>    if len(filenames) > 1:<<NEWL>>        raise InstallationError(<<NEWL>>            ""More than one .egg-info directory found in {}"".format(<<NEWL>>                directory<<NEWL>>            )<<NEWL>>        )<<NEWL>><<NEWL>>    return os.path.join(directory, filenames[0])<<NEWL>><<NEWL>><<NEWL>>def generate_metadata(<<NEWL>>    build_env,  # type: BuildEnvironment<<NEWL>>    setup_py_path,  # type: str<<NEWL>>    source_dir,  # type: str<<NEWL>>    isolated,  # type: bool<<NEWL>>    details,  # type: str<<NEWL>>):<<NEWL>>    # type: (...) -> str<<NEWL>>    """"""Generate metadata using setup.py-based defacto mechanisms.<<NEWL>><<NEWL>>    Returns the generated metadata directory.<<NEWL>>    """"""<<NEWL>>    logger.debug(<<NEWL>>        'Running setup.py (path:%s) egg_info for package %s',<<NEWL>>        setup_py_path, details,<<NEWL>>    )<<NEWL>><<NEWL>>    egg_info_dir = TempDirectory(<<NEWL>>        kind=""pip-egg-info"", globally_managed=True<<NEWL>>    ).path<<NEWL>><<NEWL>>    args = make_setuptools_egg_info_args(<<NEWL>>        setup_py_path,<<NEWL>>        egg_info_dir=egg_info_dir,<<NEWL>>        no_user_config=isolated,<<NEWL>>    )<<NEWL>><<NEWL>>    with build_env:<<NEWL>>        call_subprocess(<<NEWL>>            args,<<NEWL>>            cwd=source_dir,<<NEWL>>            command_desc='python setup.py egg_info',<<NEWL>>        )<<NEWL>><<NEWL>>    # Return the .egg-info directory.<<NEWL>>    return _find_egg_info(egg_info_dir)"
471	donghui	2	"""""""Metadata generation logic for legacy source distributions.<<NEWL>>""""""<<NEWL>><<NEWL>>import logging<<NEWL>>import os<<NEWL>><<NEWL>>from pip._internal.build_env import BuildEnvironment<<NEWL>>from pip._internal.exceptions import InstallationError<<NEWL>>from pip._internal.utils.setuptools_build import make_setuptools_egg_info_args<<NEWL>>from pip._internal.utils.subprocess import call_subprocess<<NEWL>>from pip._internal.utils.temp_dir import TempDirectory<<NEWL>><<NEWL>>logger = logging.getLogger(__name__)<<NEWL>><<NEWL>><<NEWL>>def _find_egg_info(directory):<<NEWL>>    # type: (str) -> str<<NEWL>>    """"""Find an .egg-info subdirectory in `directory`.<<NEWL>>    """"""<<NEWL>>    filenames = [<<NEWL>>        f for f in os.listdir(directory) if f.endswith("".egg-info"")<<NEWL>>    ]<<NEWL>><<NEWL>>    if not filenames:<<NEWL>>        raise InstallationError(<<NEWL>>            f""No .egg-info directory found in {directory}""<<NEWL>>        )<<NEWL>><<NEWL>>    if len(filenames) > 1:<<NEWL>>        raise InstallationError(<<NEWL>>            ""More than one .egg-info directory found in {}"".format(<<NEWL>>                directory<<NEWL>>            )<<NEWL>>        )<<NEWL>><<NEWL>>    return os.path.join(directory, filenames[0])<<NEWL>><<NEWL>><<NEWL>>def generate_metadata(<<NEWL>>    build_env,  # type: BuildEnvironment<<NEWL>>    setup_py_path,  # type: str<<NEWL>>    source_dir,  # type: str<<NEWL>>    isolated,  # type: bool<<NEWL>>    details,  # type: str<<NEWL>>):<<NEWL>>    # type: (...) -> str<<NEWL>>    """"""Generate metadata using setup.py-based defacto mechanisms.<<NEWL>><<NEWL>>    Returns the generated metadata directory.<<NEWL>>    """"""<<NEWL>>    logger.debug(<<NEWL>>        'Running setup.py (path:%s) egg_info for package %s',<<NEWL>>        setup_py_path, details,<<NEWL>>    )<<NEWL>><<NEWL>>    egg_info_dir = TempDirectory(<<NEWL>>        kind=""pip-egg-info"", globally_managed=True<<NEWL>>    ).path<<NEWL>><<NEWL>>    args = make_setuptools_egg_info_args(<<NEWL>>        setup_py_path,<<NEWL>>        egg_info_dir=egg_info_dir,<<NEWL>>        no_user_config=isolated,<<NEWL>>    )<<NEWL>><<NEWL>>    with build_env:<<NEWL>>        call_subprocess(<<NEWL>>            args,<<NEWL>>            cwd=source_dir,<<NEWL>>            command_desc='python setup.py egg_info',<<NEWL>>        )<<NEWL>><<NEWL>>    # Return the .egg-info directory.<<NEWL>>    return _find_egg_info(egg_info_dir)"
420	jackson	1	"from distutils.util import convert_path<<NEWL>>from distutils import log<<NEWL>>from distutils.errors import DistutilsOptionError<<NEWL>>import os<<NEWL>>import shutil<<NEWL>><<NEWL>>from setuptools import Command<<NEWL>><<NEWL>><<NEWL>>class rotate(Command):<<NEWL>>    """"""Delete older distributions""""""<<NEWL>><<NEWL>>    description = ""delete older distributions, keeping N newest files""<<NEWL>>    user_options = [<<NEWL>>        ('match=', 'm', ""patterns to match (required)""),<<NEWL>>        ('dist-dir=', 'd', ""directory where the distributions are""),<<NEWL>>        ('keep=', 'k', ""number of matching distributions to keep""),<<NEWL>>    ]<<NEWL>><<NEWL>>    boolean_options = []<<NEWL>><<NEWL>>    def initialize_options(self):<<NEWL>>        self.match = None<<NEWL>>        self.dist_dir = None<<NEWL>>        self.keep = None<<NEWL>><<NEWL>>    def finalize_options(self):<<NEWL>>        if self.match is None:<<NEWL>>            raise DistutilsOptionError(<<NEWL>>                ""Must specify one or more (comma-separated) match patterns ""<<NEWL>>                ""(e.g. '.zip' or '.egg')""<<NEWL>>            )<<NEWL>>        if self.keep is None:<<NEWL>>            raise DistutilsOptionError(""Must specify number of files to keep"")<<NEWL>>        try:<<NEWL>>            self.keep = int(self.keep)<<NEWL>>        except ValueError as e:<<NEWL>>            raise DistutilsOptionError(""--keep must be an integer"") from e<<NEWL>>        if isinstance(self.match, str):<<NEWL>>            self.match = [<<NEWL>>                convert_path(p.strip()) for p in self.match.split(',')<<NEWL>>            ]<<NEWL>>        self.set_undefined_options('bdist', ('dist_dir', 'dist_dir'))<<NEWL>><<NEWL>>    def run(self):<<NEWL>>        self.run_command(""egg_info"")<<NEWL>>        from glob import glob<<NEWL>><<NEWL>>        for pattern in self.match:<<NEWL>>            pattern = self.distribution.get_name() + '*' + pattern<<NEWL>>            files = glob(os.path.join(self.dist_dir, pattern))<<NEWL>>            files = [(os.path.getmtime(f), f) for f in files]<<NEWL>>            files.sort()<<NEWL>>            files.reverse()<<NEWL>><<NEWL>>            log.info(""%d file(s) matching %s"", len(files), pattern)<<NEWL>>            files = files[self.keep:]<<NEWL>>            for (t, f) in files:<<NEWL>>                log.info(""Deleting %s"", f)<<NEWL>>                if not self.dry_run:<<NEWL>>                    if os.path.isdir(f):<<NEWL>>                        shutil.rmtree(f)<<NEWL>>                    else:<<NEWL>>                        os.unlink(f)"
420	donghui	1	"from distutils.util import convert_path<<NEWL>>from distutils import log<<NEWL>>from distutils.errors import DistutilsOptionError<<NEWL>>import os<<NEWL>>import shutil<<NEWL>><<NEWL>>from setuptools import Command<<NEWL>><<NEWL>><<NEWL>>class rotate(Command):<<NEWL>>    """"""Delete older distributions""""""<<NEWL>><<NEWL>>    description = ""delete older distributions, keeping N newest files""<<NEWL>>    user_options = [<<NEWL>>        ('match=', 'm', ""patterns to match (required)""),<<NEWL>>        ('dist-dir=', 'd', ""directory where the distributions are""),<<NEWL>>        ('keep=', 'k', ""number of matching distributions to keep""),<<NEWL>>    ]<<NEWL>><<NEWL>>    boolean_options = []<<NEWL>><<NEWL>>    def initialize_options(self):<<NEWL>>        self.match = None<<NEWL>>        self.dist_dir = None<<NEWL>>        self.keep = None<<NEWL>><<NEWL>>    def finalize_options(self):<<NEWL>>        if self.match is None:<<NEWL>>            raise DistutilsOptionError(<<NEWL>>                ""Must specify one or more (comma-separated) match patterns ""<<NEWL>>                ""(e.g. '.zip' or '.egg')""<<NEWL>>            )<<NEWL>>        if self.keep is None:<<NEWL>>            raise DistutilsOptionError(""Must specify number of files to keep"")<<NEWL>>        try:<<NEWL>>            self.keep = int(self.keep)<<NEWL>>        except ValueError as e:<<NEWL>>            raise DistutilsOptionError(""--keep must be an integer"") from e<<NEWL>>        if isinstance(self.match, str):<<NEWL>>            self.match = [<<NEWL>>                convert_path(p.strip()) for p in self.match.split(',')<<NEWL>>            ]<<NEWL>>        self.set_undefined_options('bdist', ('dist_dir', 'dist_dir'))<<NEWL>><<NEWL>>    def run(self):<<NEWL>>        self.run_command(""egg_info"")<<NEWL>>        from glob import glob<<NEWL>><<NEWL>>        for pattern in self.match:<<NEWL>>            pattern = self.distribution.get_name() + '*' + pattern<<NEWL>>            files = glob(os.path.join(self.dist_dir, pattern))<<NEWL>>            files = [(os.path.getmtime(f), f) for f in files]<<NEWL>>            files.sort()<<NEWL>>            files.reverse()<<NEWL>><<NEWL>>            log.info(""%d file(s) matching %s"", len(files), pattern)<<NEWL>>            files = files[self.keep:]<<NEWL>>            for (t, f) in files:<<NEWL>>                log.info(""Deleting %s"", f)<<NEWL>>                if not self.dry_run:<<NEWL>>                    if os.path.isdir(f):<<NEWL>>                        shutil.rmtree(f)<<NEWL>>                    else:<<NEWL>>                        os.unlink(f)"
482	jackson	3	"from urllib.parse import unquote, urlparse<<NEWL>><<NEWL>>from asgiref.testing import ApplicationCommunicator<<NEWL>><<NEWL>><<NEWL>>class HttpCommunicator(ApplicationCommunicator):<<NEWL>>    """"""<<NEWL>>    ApplicationCommunicator subclass that has HTTP shortcut methods.<<NEWL>><<NEWL>>    It will construct the scope for you, so you need to pass the application<<NEWL>>    (uninstantiated) along with HTTP parameters.<<NEWL>><<NEWL>>    This does not support full chunking - for that, just use ApplicationCommunicator<<NEWL>>    directly.<<NEWL>>    """"""<<NEWL>><<NEWL>>    def __init__(self, application, method, path, body=b"""", headers=None):<<NEWL>>        parsed = urlparse(path)<<NEWL>>        self.scope = {<<NEWL>>            ""type"": ""http"",<<NEWL>>            ""http_version"": ""1.1"",<<NEWL>>            ""method"": method.upper(),<<NEWL>>            ""path"": unquote(parsed.path),<<NEWL>>            ""query_string"": parsed.query.encode(""utf-8""),<<NEWL>>            ""headers"": headers or [],<<NEWL>>        }<<NEWL>>        assert isinstance(body, bytes)<<NEWL>>        self.body = body<<NEWL>>        self.sent_request = False<<NEWL>>        super().__init__(application, self.scope)<<NEWL>><<NEWL>>    async def get_response(self, timeout=1):<<NEWL>>        """"""<<NEWL>>        Get the application's response. Returns a dict with keys of<<NEWL>>        ""body"", ""headers"" and ""status"".<<NEWL>>        """"""<<NEWL>>        # If we've not sent the request yet, do so<<NEWL>>        if not self.sent_request:<<NEWL>>            self.sent_request = True<<NEWL>>            await self.send_input({""type"": ""http.request"", ""body"": self.body})<<NEWL>>        # Get the response start<<NEWL>>        response_start = await self.receive_output(timeout)<<NEWL>>        assert response_start[""type""] == ""http.response.start""<<NEWL>>        # Get all body parts<<NEWL>>        response_start[""body""] = b""""<<NEWL>>        while True:<<NEWL>>            chunk = await self.receive_output(timeout)<<NEWL>>            assert chunk[""type""] == ""http.response.body""<<NEWL>>            assert isinstance(chunk[""body""], bytes)<<NEWL>>            response_start[""body""] += chunk[""body""]<<NEWL>>            if not chunk.get(""more_body"", False):<<NEWL>>                break<<NEWL>>        # Return structured info<<NEWL>>        del response_start[""type""]<<NEWL>>        response_start.setdefault(""headers"", [])<<NEWL>>        return response_start"
482	donghui	3	"from urllib.parse import unquote, urlparse<<NEWL>><<NEWL>>from asgiref.testing import ApplicationCommunicator<<NEWL>><<NEWL>><<NEWL>>class HttpCommunicator(ApplicationCommunicator):<<NEWL>>    """"""<<NEWL>>    ApplicationCommunicator subclass that has HTTP shortcut methods.<<NEWL>><<NEWL>>    It will construct the scope for you, so you need to pass the application<<NEWL>>    (uninstantiated) along with HTTP parameters.<<NEWL>><<NEWL>>    This does not support full chunking - for that, just use ApplicationCommunicator<<NEWL>>    directly.<<NEWL>>    """"""<<NEWL>><<NEWL>>    def __init__(self, application, method, path, body=b"""", headers=None):<<NEWL>>        parsed = urlparse(path)<<NEWL>>        self.scope = {<<NEWL>>            ""type"": ""http"",<<NEWL>>            ""http_version"": ""1.1"",<<NEWL>>            ""method"": method.upper(),<<NEWL>>            ""path"": unquote(parsed.path),<<NEWL>>            ""query_string"": parsed.query.encode(""utf-8""),<<NEWL>>            ""headers"": headers or [],<<NEWL>>        }<<NEWL>>        assert isinstance(body, bytes)<<NEWL>>        self.body = body<<NEWL>>        self.sent_request = False<<NEWL>>        super().__init__(application, self.scope)<<NEWL>><<NEWL>>    async def get_response(self, timeout=1):<<NEWL>>        """"""<<NEWL>>        Get the application's response. Returns a dict with keys of<<NEWL>>        ""body"", ""headers"" and ""status"".<<NEWL>>        """"""<<NEWL>>        # If we've not sent the request yet, do so<<NEWL>>        if not self.sent_request:<<NEWL>>            self.sent_request = True<<NEWL>>            await self.send_input({""type"": ""http.request"", ""body"": self.body})<<NEWL>>        # Get the response start<<NEWL>>        response_start = await self.receive_output(timeout)<<NEWL>>        assert response_start[""type""] == ""http.response.start""<<NEWL>>        # Get all body parts<<NEWL>>        response_start[""body""] = b""""<<NEWL>>        while True:<<NEWL>>            chunk = await self.receive_output(timeout)<<NEWL>>            assert chunk[""type""] == ""http.response.body""<<NEWL>>            assert isinstance(chunk[""body""], bytes)<<NEWL>>            response_start[""body""] += chunk[""body""]<<NEWL>>            if not chunk.get(""more_body"", False):<<NEWL>>                break<<NEWL>>        # Return structured info<<NEWL>>        del response_start[""type""]<<NEWL>>        response_start.setdefault(""headers"", [])<<NEWL>>        return response_start"
414	jackson	3	"# -*- coding: utf-8 -*-<<NEWL>>#<<NEWL>># Copyright (C) 2019 Radim Rehurek <me@radimrehurek.com><<NEWL>>#<<NEWL>># This code is distributed under the terms and conditions<<NEWL>># from the MIT License (MIT).<<NEWL>>#<<NEWL>><<NEWL>>""""""<<NEWL>>Utilities for streaming to/from several file-like data storages: S3 / HDFS / local<<NEWL>>filesystem / compressed files, and many more, using a simple, Pythonic API.<<NEWL>><<NEWL>>The streaming makes heavy use of generators and pipes, to avoid loading<<NEWL>>full file contents into memory, allowing work with arbitrarily large files.<<NEWL>><<NEWL>>The main functions are:<<NEWL>><<NEWL>>* `open()`, which opens the given file for reading/writing<<NEWL>>* `parse_uri()`<<NEWL>>* `s3_iter_bucket()`, which goes over all keys in an S3 bucket in parallel<<NEWL>>* `register_compressor()`, which registers callbacks for transparent compressor handling<<NEWL>><<NEWL>>""""""<<NEWL>><<NEWL>>import logging<<NEWL>><<NEWL>>#<<NEWL>># Prevent regression of #474 and #475<<NEWL>>#<<NEWL>>logger = logging.getLogger(__name__)<<NEWL>>logger.addHandler(logging.NullHandler())<<NEWL>><<NEWL>>from smart_open import version  # noqa: E402<<NEWL>>from .smart_open_lib import open, parse_uri, smart_open, register_compressor  # noqa: E402<<NEWL>><<NEWL>>_WARNING = """"""smart_open.s3_iter_bucket is deprecated and will stop functioning<<NEWL>>in a future version. Please import iter_bucket from the smart_open.s3 module instead:<<NEWL>><<NEWL>>    from smart_open.s3 import iter_bucket as s3_iter_bucket<<NEWL>><<NEWL>>""""""<<NEWL>>_WARNED = False<<NEWL>><<NEWL>><<NEWL>>def s3_iter_bucket(<<NEWL>>        bucket_name,<<NEWL>>        prefix='',<<NEWL>>        accept_key=None,<<NEWL>>        key_limit=None,<<NEWL>>        workers=16,<<NEWL>>        retries=3,<<NEWL>>        **session_kwargs<<NEWL>>):<<NEWL>>    """"""Deprecated.  Use smart_open.s3.iter_bucket instead.""""""<<NEWL>>    global _WARNED<<NEWL>>    from .s3 import iter_bucket<<NEWL>>    if not _WARNED:<<NEWL>>        logger.warning(_WARNING)<<NEWL>>        _WARNED = True<<NEWL>>    return iter_bucket(<<NEWL>>        bucket_name=bucket_name,<<NEWL>>        prefix=prefix,<<NEWL>>        accept_key=accept_key,<<NEWL>>        key_limit=key_limit,<<NEWL>>        workers=workers,<<NEWL>>        retries=retries,<<NEWL>>        session_kwargs=session_kwargs<<NEWL>>    )<<NEWL>><<NEWL>><<NEWL>>__all__ = [<<NEWL>>    'open',<<NEWL>>    'parse_uri',<<NEWL>>    'register_compressor',<<NEWL>>    's3_iter_bucket',<<NEWL>>    'smart_open',<<NEWL>>]<<NEWL>><<NEWL>>__version__ = version.__version__"
414	donghui	2	"# -*- coding: utf-8 -*-<<NEWL>>#<<NEWL>># Copyright (C) 2019 Radim Rehurek <me@radimrehurek.com><<NEWL>>#<<NEWL>># This code is distributed under the terms and conditions<<NEWL>># from the MIT License (MIT).<<NEWL>>#<<NEWL>><<NEWL>>""""""<<NEWL>>Utilities for streaming to/from several file-like data storages: S3 / HDFS / local<<NEWL>>filesystem / compressed files, and many more, using a simple, Pythonic API.<<NEWL>><<NEWL>>The streaming makes heavy use of generators and pipes, to avoid loading<<NEWL>>full file contents into memory, allowing work with arbitrarily large files.<<NEWL>><<NEWL>>The main functions are:<<NEWL>><<NEWL>>* `open()`, which opens the given file for reading/writing<<NEWL>>* `parse_uri()`<<NEWL>>* `s3_iter_bucket()`, which goes over all keys in an S3 bucket in parallel<<NEWL>>* `register_compressor()`, which registers callbacks for transparent compressor handling<<NEWL>><<NEWL>>""""""<<NEWL>><<NEWL>>import logging<<NEWL>><<NEWL>>#<<NEWL>># Prevent regression of #474 and #475<<NEWL>>#<<NEWL>>logger = logging.getLogger(__name__)<<NEWL>>logger.addHandler(logging.NullHandler())<<NEWL>><<NEWL>>from smart_open import version  # noqa: E402<<NEWL>>from .smart_open_lib import open, parse_uri, smart_open, register_compressor  # noqa: E402<<NEWL>><<NEWL>>_WARNING = """"""smart_open.s3_iter_bucket is deprecated and will stop functioning<<NEWL>>in a future version. Please import iter_bucket from the smart_open.s3 module instead:<<NEWL>><<NEWL>>    from smart_open.s3 import iter_bucket as s3_iter_bucket<<NEWL>><<NEWL>>""""""<<NEWL>>_WARNED = False<<NEWL>><<NEWL>><<NEWL>>def s3_iter_bucket(<<NEWL>>        bucket_name,<<NEWL>>        prefix='',<<NEWL>>        accept_key=None,<<NEWL>>        key_limit=None,<<NEWL>>        workers=16,<<NEWL>>        retries=3,<<NEWL>>        **session_kwargs<<NEWL>>):<<NEWL>>    """"""Deprecated.  Use smart_open.s3.iter_bucket instead.""""""<<NEWL>>    global _WARNED<<NEWL>>    from .s3 import iter_bucket<<NEWL>>    if not _WARNED:<<NEWL>>        logger.warning(_WARNING)<<NEWL>>        _WARNED = True<<NEWL>>    return iter_bucket(<<NEWL>>        bucket_name=bucket_name,<<NEWL>>        prefix=prefix,<<NEWL>>        accept_key=accept_key,<<NEWL>>        key_limit=key_limit,<<NEWL>>        workers=workers,<<NEWL>>        retries=retries,<<NEWL>>        session_kwargs=session_kwargs<<NEWL>>    )<<NEWL>><<NEWL>><<NEWL>>__all__ = [<<NEWL>>    'open',<<NEWL>>    'parse_uri',<<NEWL>>    'register_compressor',<<NEWL>>    's3_iter_bucket',<<NEWL>>    'smart_open',<<NEWL>>]<<NEWL>><<NEWL>>__version__ = version.__version__"
505	jackson	3	"""""""<<NEWL>>    pygments.styles.vim<<NEWL>>    ~~~~~~~~~~~~~~~~~~~<<NEWL>><<NEWL>>    A highlighting style for Pygments, inspired by vim.<<NEWL>><<NEWL>>    :copyright: Copyright 2006-2022 by the Pygments team, see AUTHORS.<<NEWL>>    :license: BSD, see LICENSE for details.<<NEWL>>""""""<<NEWL>><<NEWL>>from pygments.style import Style<<NEWL>>from pygments.token import Keyword, Name, Comment, String, Error, \<<NEWL>>     Number, Operator, Generic, Whitespace, Token<<NEWL>><<NEWL>><<NEWL>>class VimStyle(Style):<<NEWL>>    """"""<<NEWL>>    Styles somewhat like vim 7.0<<NEWL>>    """"""<<NEWL>><<NEWL>>    background_color = ""#000000""<<NEWL>>    highlight_color = ""#222222""<<NEWL>><<NEWL>>    styles = {<<NEWL>>        Token:                     ""#cccccc"",<<NEWL>>        Whitespace:                """",<<NEWL>>        Comment:                   ""#000080"",<<NEWL>>        Comment.Preproc:           """",<<NEWL>>        Comment.Special:           ""bold #cd0000"",<<NEWL>><<NEWL>>        Keyword:                   ""#cdcd00"",<<NEWL>>        Keyword.Declaration:       ""#00cd00"",<<NEWL>>        Keyword.Namespace:         ""#cd00cd"",<<NEWL>>        Keyword.Pseudo:            """",<<NEWL>>        Keyword.Type:              ""#00cd00"",<<NEWL>><<NEWL>>        Operator:                  ""#3399cc"",<<NEWL>>        Operator.Word:             ""#cdcd00"",<<NEWL>><<NEWL>>        Name:                      """",<<NEWL>>        Name.Class:                ""#00cdcd"",<<NEWL>>        Name.Builtin:              ""#cd00cd"",<<NEWL>>        Name.Exception:            ""bold #666699"",<<NEWL>>        Name.Variable:             ""#00cdcd"",<<NEWL>><<NEWL>>        String:                    ""#cd0000"",<<NEWL>>        Number:                    ""#cd00cd"",<<NEWL>><<NEWL>>        Generic.Heading:           ""bold #000080"",<<NEWL>>        Generic.Subheading:        ""bold #800080"",<<NEWL>>        Generic.Deleted:           ""#cd0000"",<<NEWL>>        Generic.Inserted:          ""#00cd00"",<<NEWL>>        Generic.Error:             ""#FF0000"",<<NEWL>>        Generic.Emph:              ""italic"",<<NEWL>>        Generic.Strong:            ""bold"",<<NEWL>>        Generic.Prompt:            ""bold #000080"",<<NEWL>>        Generic.Output:            ""#888"",<<NEWL>>        Generic.Traceback:         ""#04D"",<<NEWL>><<NEWL>>        Error:                     ""border:#FF0000""<<NEWL>>    }"
505	donghui	1	"""""""<<NEWL>>    pygments.styles.vim<<NEWL>>    ~~~~~~~~~~~~~~~~~~~<<NEWL>><<NEWL>>    A highlighting style for Pygments, inspired by vim.<<NEWL>><<NEWL>>    :copyright: Copyright 2006-2022 by the Pygments team, see AUTHORS.<<NEWL>>    :license: BSD, see LICENSE for details.<<NEWL>>""""""<<NEWL>><<NEWL>>from pygments.style import Style<<NEWL>>from pygments.token import Keyword, Name, Comment, String, Error, \<<NEWL>>     Number, Operator, Generic, Whitespace, Token<<NEWL>><<NEWL>><<NEWL>>class VimStyle(Style):<<NEWL>>    """"""<<NEWL>>    Styles somewhat like vim 7.0<<NEWL>>    """"""<<NEWL>><<NEWL>>    background_color = ""#000000""<<NEWL>>    highlight_color = ""#222222""<<NEWL>><<NEWL>>    styles = {<<NEWL>>        Token:                     ""#cccccc"",<<NEWL>>        Whitespace:                """",<<NEWL>>        Comment:                   ""#000080"",<<NEWL>>        Comment.Preproc:           """",<<NEWL>>        Comment.Special:           ""bold #cd0000"",<<NEWL>><<NEWL>>        Keyword:                   ""#cdcd00"",<<NEWL>>        Keyword.Declaration:       ""#00cd00"",<<NEWL>>        Keyword.Namespace:         ""#cd00cd"",<<NEWL>>        Keyword.Pseudo:            """",<<NEWL>>        Keyword.Type:              ""#00cd00"",<<NEWL>><<NEWL>>        Operator:                  ""#3399cc"",<<NEWL>>        Operator.Word:             ""#cdcd00"",<<NEWL>><<NEWL>>        Name:                      """",<<NEWL>>        Name.Class:                ""#00cdcd"",<<NEWL>>        Name.Builtin:              ""#cd00cd"",<<NEWL>>        Name.Exception:            ""bold #666699"",<<NEWL>>        Name.Variable:             ""#00cdcd"",<<NEWL>><<NEWL>>        String:                    ""#cd0000"",<<NEWL>>        Number:                    ""#cd00cd"",<<NEWL>><<NEWL>>        Generic.Heading:           ""bold #000080"",<<NEWL>>        Generic.Subheading:        ""bold #800080"",<<NEWL>>        Generic.Deleted:           ""#cd0000"",<<NEWL>>        Generic.Inserted:          ""#00cd00"",<<NEWL>>        Generic.Error:             ""#FF0000"",<<NEWL>>        Generic.Emph:              ""italic"",<<NEWL>>        Generic.Strong:            ""bold"",<<NEWL>>        Generic.Prompt:            ""bold #000080"",<<NEWL>>        Generic.Output:            ""#888"",<<NEWL>>        Generic.Traceback:         ""#04D"",<<NEWL>><<NEWL>>        Error:                     ""border:#FF0000""<<NEWL>>    }"
445	jackson	0	from rest_framework.views import APIView<<NEWL>>from rest_framework.response import Response<<NEWL>>from rest_framework.exceptions import AuthenticationFailed<<NEWL>>from rest_framework.parsers import JSONParser<<NEWL>>from .serializers import UserSerializer<<NEWL>>from .models import User<<NEWL>>import jwt<<NEWL>>import datetime<<NEWL>>from jwt import decode<<NEWL>>from bson.objectid import ObjectId<<NEWL>><<NEWL>>import sys<<NEWL>><<NEWL>>from helpers.permissions import isUser<<NEWL>><<NEWL>>class RegisterView(APIView):<<NEWL>>    def post(self, request):<<NEWL>>        serializer = UserSerializer(data=request.data)<<NEWL>>        serializer.is_valid(raise_exception=True)<<NEWL>>        serializer.save()<<NEWL>>        return Response(serializer.data)<<NEWL>><<NEWL>><<NEWL>>class LoginView(APIView):<<NEWL>>    def post(self, request):<<NEWL>>        email = request.data['email']<<NEWL>>        password = request.data['password']<<NEWL>><<NEWL>>        user = User.objects.filter(email__exact=email).first()<<NEWL>><<NEWL>>        if user is None:<<NEWL>>            raise AuthenticationFailed('User not found!')<<NEWL>><<NEWL>>        if not user.check_password(password):<<NEWL>>            raise AuthenticationFailed('Incorrect password!')<<NEWL>><<NEWL>>        payload = {<<NEWL>>            'id': str(user._id),<<NEWL>>            'admin': user.is_superAdmin,<<NEWL>>            'exp': datetime.datetime.utcnow() + datetime.timedelta(minutes=60),<<NEWL>>            'iat': datetime.datetime.utcnow()<<NEWL>>        }<<NEWL>>        <<NEWL>><<NEWL>>        token = jwt.encode(payload, 'secret',<<NEWL>>                           algorithm='HS256').decode('utf-8')<<NEWL>><<NEWL>>        response = Response()<<NEWL>><<NEWL>>        response.set_cookie(key='jwt', value=token, httponly=True)<<NEWL>>        response.data = {<<NEWL>>            'jwt': token<<NEWL>>        }<<NEWL>><<NEWL>>        return response<<NEWL>><<NEWL>><<NEWL>>class UserView(APIView):<<NEWL>>    <<NEWL>>    permission_classes = [isUser]<<NEWL>><<NEWL>>    def get(self, request):<<NEWL>>        user = User.objects.filter(_id=ObjectId(request.account['id'])).first()<<NEWL>>        serializer = UserSerializer(user)<<NEWL>>        <<NEWL>>        return Response(serializer.data)<<NEWL>><<NEWL>><<NEWL>><<NEWL>>class LogoutView(APIView):<<NEWL>>    def post(self, request):<<NEWL>>        response = Response()<<NEWL>>        response.delete_cookie('jwt')<<NEWL>>        response.data = {<<NEWL>>            'message': 'success'<<NEWL>>        }<<NEWL>>        return response
445	donghui	0	from rest_framework.views import APIView<<NEWL>>from rest_framework.response import Response<<NEWL>>from rest_framework.exceptions import AuthenticationFailed<<NEWL>>from rest_framework.parsers import JSONParser<<NEWL>>from .serializers import UserSerializer<<NEWL>>from .models import User<<NEWL>>import jwt<<NEWL>>import datetime<<NEWL>>from jwt import decode<<NEWL>>from bson.objectid import ObjectId<<NEWL>><<NEWL>>import sys<<NEWL>><<NEWL>>from helpers.permissions import isUser<<NEWL>><<NEWL>>class RegisterView(APIView):<<NEWL>>    def post(self, request):<<NEWL>>        serializer = UserSerializer(data=request.data)<<NEWL>>        serializer.is_valid(raise_exception=True)<<NEWL>>        serializer.save()<<NEWL>>        return Response(serializer.data)<<NEWL>><<NEWL>><<NEWL>>class LoginView(APIView):<<NEWL>>    def post(self, request):<<NEWL>>        email = request.data['email']<<NEWL>>        password = request.data['password']<<NEWL>><<NEWL>>        user = User.objects.filter(email__exact=email).first()<<NEWL>><<NEWL>>        if user is None:<<NEWL>>            raise AuthenticationFailed('User not found!')<<NEWL>><<NEWL>>        if not user.check_password(password):<<NEWL>>            raise AuthenticationFailed('Incorrect password!')<<NEWL>><<NEWL>>        payload = {<<NEWL>>            'id': str(user._id),<<NEWL>>            'admin': user.is_superAdmin,<<NEWL>>            'exp': datetime.datetime.utcnow() + datetime.timedelta(minutes=60),<<NEWL>>            'iat': datetime.datetime.utcnow()<<NEWL>>        }<<NEWL>>        <<NEWL>><<NEWL>>        token = jwt.encode(payload, 'secret',<<NEWL>>                           algorithm='HS256').decode('utf-8')<<NEWL>><<NEWL>>        response = Response()<<NEWL>><<NEWL>>        response.set_cookie(key='jwt', value=token, httponly=True)<<NEWL>>        response.data = {<<NEWL>>            'jwt': token<<NEWL>>        }<<NEWL>><<NEWL>>        return response<<NEWL>><<NEWL>><<NEWL>>class UserView(APIView):<<NEWL>>    <<NEWL>>    permission_classes = [isUser]<<NEWL>><<NEWL>>    def get(self, request):<<NEWL>>        user = User.objects.filter(_id=ObjectId(request.account['id'])).first()<<NEWL>>        serializer = UserSerializer(user)<<NEWL>>        <<NEWL>>        return Response(serializer.data)<<NEWL>><<NEWL>><<NEWL>><<NEWL>>class LogoutView(APIView):<<NEWL>>    def post(self, request):<<NEWL>>        response = Response()<<NEWL>>        response.delete_cookie('jwt')<<NEWL>>        response.data = {<<NEWL>>            'message': 'success'<<NEWL>>        }<<NEWL>>        return response
455	jackson	3	"# Copyright 2016 Google LLC.<<NEWL>>#<<NEWL>># Licensed under the Apache License, Version 2.0 (the ""License"");<<NEWL>># you may not use this file except in compliance with the License.<<NEWL>># You may obtain a copy of the License at<<NEWL>>#<<NEWL>>#     http://www.apache.org/licenses/LICENSE-2.0<<NEWL>>#<<NEWL>># Unless required by applicable law or agreed to in writing, software<<NEWL>># distributed under the License is distributed on an ""AS IS"" BASIS,<<NEWL>># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.<<NEWL>># See the License for the specific language governing permissions and<<NEWL>># limitations under the License.<<NEWL>><<NEWL>>from flask import Flask<<NEWL>>import requests<<NEWL>><<NEWL>>import services_config<<NEWL>><<NEWL>>app = Flask(__name__)<<NEWL>>services_config.init_app(app)<<NEWL>><<NEWL>><<NEWL>>@app.route('/')<<NEWL>>def root():<<NEWL>>    """"""Gets index.html from the static file server""""""<<NEWL>>    res = requests.get(app.config['SERVICE_MAP']['static'])<<NEWL>>    return res.content<<NEWL>><<NEWL>><<NEWL>>@app.route('/hello/<service>')<<NEWL>>def say_hello(service):<<NEWL>>    """"""Recieves requests from buttons on the front end and resopnds<<NEWL>>    or sends request to the static file server""""""<<NEWL>>    # If 'gateway' is specified return immediate<<NEWL>>    if service == 'gateway':<<NEWL>>        return 'Gateway says hello'<<NEWL>><<NEWL>>    # Otherwise send request to service indicated by URL param<<NEWL>>    responses = []<<NEWL>>    url = app.config['SERVICE_MAP'][service]<<NEWL>>    res = requests.get(url + '/hello')<<NEWL>>    responses.append(res.content)<<NEWL>>    return '\n'.encode().join(responses)<<NEWL>><<NEWL>><<NEWL>>@app.route('/<path>')<<NEWL>>def static_file(path):<<NEWL>>    """"""Gets static files required by index.html to static file server""""""<<NEWL>>    url = app.config['SERVICE_MAP']['static']<<NEWL>>    res = requests.get(url + '/' + path)<<NEWL>>    return res.content, 200, {'Content-Type': res.headers['Content-Type']}<<NEWL>><<NEWL>><<NEWL>>if __name__ == '__main__':<<NEWL>>    # This is used when running locally. Gunicorn is used to run the<<NEWL>>    # application on Google App Engine. See entrypoint in app.yaml.<<NEWL>>    app.run(host='127.0.0.1', port=8000, debug=True)"
455	donghui	2	"# Copyright 2016 Google LLC.<<NEWL>>#<<NEWL>># Licensed under the Apache License, Version 2.0 (the ""License"");<<NEWL>># you may not use this file except in compliance with the License.<<NEWL>># You may obtain a copy of the License at<<NEWL>>#<<NEWL>>#     http://www.apache.org/licenses/LICENSE-2.0<<NEWL>>#<<NEWL>># Unless required by applicable law or agreed to in writing, software<<NEWL>># distributed under the License is distributed on an ""AS IS"" BASIS,<<NEWL>># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.<<NEWL>># See the License for the specific language governing permissions and<<NEWL>># limitations under the License.<<NEWL>><<NEWL>>from flask import Flask<<NEWL>>import requests<<NEWL>><<NEWL>>import services_config<<NEWL>><<NEWL>>app = Flask(__name__)<<NEWL>>services_config.init_app(app)<<NEWL>><<NEWL>><<NEWL>>@app.route('/')<<NEWL>>def root():<<NEWL>>    """"""Gets index.html from the static file server""""""<<NEWL>>    res = requests.get(app.config['SERVICE_MAP']['static'])<<NEWL>>    return res.content<<NEWL>><<NEWL>><<NEWL>>@app.route('/hello/<service>')<<NEWL>>def say_hello(service):<<NEWL>>    """"""Recieves requests from buttons on the front end and resopnds<<NEWL>>    or sends request to the static file server""""""<<NEWL>>    # If 'gateway' is specified return immediate<<NEWL>>    if service == 'gateway':<<NEWL>>        return 'Gateway says hello'<<NEWL>><<NEWL>>    # Otherwise send request to service indicated by URL param<<NEWL>>    responses = []<<NEWL>>    url = app.config['SERVICE_MAP'][service]<<NEWL>>    res = requests.get(url + '/hello')<<NEWL>>    responses.append(res.content)<<NEWL>>    return '\n'.encode().join(responses)<<NEWL>><<NEWL>><<NEWL>>@app.route('/<path>')<<NEWL>>def static_file(path):<<NEWL>>    """"""Gets static files required by index.html to static file server""""""<<NEWL>>    url = app.config['SERVICE_MAP']['static']<<NEWL>>    res = requests.get(url + '/' + path)<<NEWL>>    return res.content, 200, {'Content-Type': res.headers['Content-Type']}<<NEWL>><<NEWL>><<NEWL>>if __name__ == '__main__':<<NEWL>>    # This is used when running locally. Gunicorn is used to run the<<NEWL>>    # application on Google App Engine. See entrypoint in app.yaml.<<NEWL>>    app.run(host='127.0.0.1', port=8000, debug=True)"
515	jackson	0	"import sys<<NEWL>>from typing import TYPE_CHECKING<<NEWL>><<NEWL>>if sys.version_info < (3, 7) or TYPE_CHECKING:<<NEWL>>    from ._visible import VisibleValidator<<NEWL>>    from ._type import TypeValidator<<NEWL>>    from ._templateitemname import TemplateitemnameValidator<<NEWL>>    from ._symbol import SymbolValidator<<NEWL>>    from ._sourcetype import SourcetypeValidator<<NEWL>>    from ._sourcelayer import SourcelayerValidator<<NEWL>>    from ._sourceattribution import SourceattributionValidator<<NEWL>>    from ._source import SourceValidator<<NEWL>>    from ._opacity import OpacityValidator<<NEWL>>    from ._name import NameValidator<<NEWL>>    from ._minzoom import MinzoomValidator<<NEWL>>    from ._maxzoom import MaxzoomValidator<<NEWL>>    from ._line import LineValidator<<NEWL>>    from ._fill import FillValidator<<NEWL>>    from ._coordinates import CoordinatesValidator<<NEWL>>    from ._color import ColorValidator<<NEWL>>    from ._circle import CircleValidator<<NEWL>>    from ._below import BelowValidator<<NEWL>>else:<<NEWL>>    from _plotly_utils.importers import relative_import<<NEWL>><<NEWL>>    __all__, __getattr__, __dir__ = relative_import(<<NEWL>>        __name__,<<NEWL>>        [],<<NEWL>>        [<<NEWL>>            ""._visible.VisibleValidator"",<<NEWL>>            ""._type.TypeValidator"",<<NEWL>>            ""._templateitemname.TemplateitemnameValidator"",<<NEWL>>            ""._symbol.SymbolValidator"",<<NEWL>>            ""._sourcetype.SourcetypeValidator"",<<NEWL>>            ""._sourcelayer.SourcelayerValidator"",<<NEWL>>            ""._sourceattribution.SourceattributionValidator"",<<NEWL>>            ""._source.SourceValidator"",<<NEWL>>            ""._opacity.OpacityValidator"",<<NEWL>>            ""._name.NameValidator"",<<NEWL>>            ""._minzoom.MinzoomValidator"",<<NEWL>>            ""._maxzoom.MaxzoomValidator"",<<NEWL>>            ""._line.LineValidator"",<<NEWL>>            ""._fill.FillValidator"",<<NEWL>>            ""._coordinates.CoordinatesValidator"",<<NEWL>>            ""._color.ColorValidator"",<<NEWL>>            ""._circle.CircleValidator"",<<NEWL>>            ""._below.BelowValidator"",<<NEWL>>        ],<<NEWL>>    )"
515	donghui	0	"import sys<<NEWL>>from typing import TYPE_CHECKING<<NEWL>><<NEWL>>if sys.version_info < (3, 7) or TYPE_CHECKING:<<NEWL>>    from ._visible import VisibleValidator<<NEWL>>    from ._type import TypeValidator<<NEWL>>    from ._templateitemname import TemplateitemnameValidator<<NEWL>>    from ._symbol import SymbolValidator<<NEWL>>    from ._sourcetype import SourcetypeValidator<<NEWL>>    from ._sourcelayer import SourcelayerValidator<<NEWL>>    from ._sourceattribution import SourceattributionValidator<<NEWL>>    from ._source import SourceValidator<<NEWL>>    from ._opacity import OpacityValidator<<NEWL>>    from ._name import NameValidator<<NEWL>>    from ._minzoom import MinzoomValidator<<NEWL>>    from ._maxzoom import MaxzoomValidator<<NEWL>>    from ._line import LineValidator<<NEWL>>    from ._fill import FillValidator<<NEWL>>    from ._coordinates import CoordinatesValidator<<NEWL>>    from ._color import ColorValidator<<NEWL>>    from ._circle import CircleValidator<<NEWL>>    from ._below import BelowValidator<<NEWL>>else:<<NEWL>>    from _plotly_utils.importers import relative_import<<NEWL>><<NEWL>>    __all__, __getattr__, __dir__ = relative_import(<<NEWL>>        __name__,<<NEWL>>        [],<<NEWL>>        [<<NEWL>>            ""._visible.VisibleValidator"",<<NEWL>>            ""._type.TypeValidator"",<<NEWL>>            ""._templateitemname.TemplateitemnameValidator"",<<NEWL>>            ""._symbol.SymbolValidator"",<<NEWL>>            ""._sourcetype.SourcetypeValidator"",<<NEWL>>            ""._sourcelayer.SourcelayerValidator"",<<NEWL>>            ""._sourceattribution.SourceattributionValidator"",<<NEWL>>            ""._source.SourceValidator"",<<NEWL>>            ""._opacity.OpacityValidator"",<<NEWL>>            ""._name.NameValidator"",<<NEWL>>            ""._minzoom.MinzoomValidator"",<<NEWL>>            ""._maxzoom.MaxzoomValidator"",<<NEWL>>            ""._line.LineValidator"",<<NEWL>>            ""._fill.FillValidator"",<<NEWL>>            ""._coordinates.CoordinatesValidator"",<<NEWL>>            ""._color.ColorValidator"",<<NEWL>>            ""._circle.CircleValidator"",<<NEWL>>            ""._below.BelowValidator"",<<NEWL>>        ],<<NEWL>>    )"
404	jackson	3	"#<<NEWL>># The Python Imaging Library.<<NEWL>># $Id$<<NEWL>>#<<NEWL>># sequence support classes<<NEWL>>#<<NEWL>># history:<<NEWL>># 1997-02-20 fl     Created<<NEWL>>#<<NEWL>># Copyright (c) 1997 by Secret Labs AB.<<NEWL>># Copyright (c) 1997 by Fredrik Lundh.<<NEWL>>#<<NEWL>># See the README file for information on usage and redistribution.<<NEWL>>#<<NEWL>><<NEWL>>##<<NEWL>><<NEWL>><<NEWL>>class Iterator:<<NEWL>>    """"""<<NEWL>>    This class implements an iterator object that can be used to loop<<NEWL>>    over an image sequence.<<NEWL>><<NEWL>>    You can use the ``[]`` operator to access elements by index. This operator<<NEWL>>    will raise an :py:exc:`IndexError` if you try to access a nonexistent<<NEWL>>    frame.<<NEWL>><<NEWL>>    :param im: An image object.<<NEWL>>    """"""<<NEWL>><<NEWL>>    def __init__(self, im):<<NEWL>>        if not hasattr(im, ""seek""):<<NEWL>>            msg = ""im must have seek method""<<NEWL>>            raise AttributeError(msg)<<NEWL>>        self.im = im<<NEWL>>        self.position = getattr(self.im, ""_min_frame"", 0)<<NEWL>><<NEWL>>    def __getitem__(self, ix):<<NEWL>>        try:<<NEWL>>            self.im.seek(ix)<<NEWL>>            return self.im<<NEWL>>        except EOFError as e:<<NEWL>>            raise IndexError from e  # end of sequence<<NEWL>><<NEWL>>    def __iter__(self):<<NEWL>>        return self<<NEWL>><<NEWL>>    def __next__(self):<<NEWL>>        try:<<NEWL>>            self.im.seek(self.position)<<NEWL>>            self.position += 1<<NEWL>>            return self.im<<NEWL>>        except EOFError as e:<<NEWL>>            raise StopIteration from e<<NEWL>><<NEWL>><<NEWL>>def all_frames(im, func=None):<<NEWL>>    """"""<<NEWL>>    Applies a given function to all frames in an image or a list of images.<<NEWL>>    The frames are returned as a list of separate images.<<NEWL>><<NEWL>>    :param im: An image, or a list of images.<<NEWL>>    :param func: The function to apply to all of the image frames.<<NEWL>>    :returns: A list of images.<<NEWL>>    """"""<<NEWL>>    if not isinstance(im, list):<<NEWL>>        im = [im]<<NEWL>><<NEWL>>    ims = []<<NEWL>>    for imSequence in im:<<NEWL>>        current = imSequence.tell()<<NEWL>><<NEWL>>        ims += [im_frame.copy() for im_frame in Iterator(imSequence)]<<NEWL>><<NEWL>>        imSequence.seek(current)<<NEWL>>    return [func(im) for im in ims] if func else ims"
404	donghui	3	"#<<NEWL>># The Python Imaging Library.<<NEWL>># $Id$<<NEWL>>#<<NEWL>># sequence support classes<<NEWL>>#<<NEWL>># history:<<NEWL>># 1997-02-20 fl     Created<<NEWL>>#<<NEWL>># Copyright (c) 1997 by Secret Labs AB.<<NEWL>># Copyright (c) 1997 by Fredrik Lundh.<<NEWL>>#<<NEWL>># See the README file for information on usage and redistribution.<<NEWL>>#<<NEWL>><<NEWL>>##<<NEWL>><<NEWL>><<NEWL>>class Iterator:<<NEWL>>    """"""<<NEWL>>    This class implements an iterator object that can be used to loop<<NEWL>>    over an image sequence.<<NEWL>><<NEWL>>    You can use the ``[]`` operator to access elements by index. This operator<<NEWL>>    will raise an :py:exc:`IndexError` if you try to access a nonexistent<<NEWL>>    frame.<<NEWL>><<NEWL>>    :param im: An image object.<<NEWL>>    """"""<<NEWL>><<NEWL>>    def __init__(self, im):<<NEWL>>        if not hasattr(im, ""seek""):<<NEWL>>            msg = ""im must have seek method""<<NEWL>>            raise AttributeError(msg)<<NEWL>>        self.im = im<<NEWL>>        self.position = getattr(self.im, ""_min_frame"", 0)<<NEWL>><<NEWL>>    def __getitem__(self, ix):<<NEWL>>        try:<<NEWL>>            self.im.seek(ix)<<NEWL>>            return self.im<<NEWL>>        except EOFError as e:<<NEWL>>            raise IndexError from e  # end of sequence<<NEWL>><<NEWL>>    def __iter__(self):<<NEWL>>        return self<<NEWL>><<NEWL>>    def __next__(self):<<NEWL>>        try:<<NEWL>>            self.im.seek(self.position)<<NEWL>>            self.position += 1<<NEWL>>            return self.im<<NEWL>>        except EOFError as e:<<NEWL>>            raise StopIteration from e<<NEWL>><<NEWL>><<NEWL>>def all_frames(im, func=None):<<NEWL>>    """"""<<NEWL>>    Applies a given function to all frames in an image or a list of images.<<NEWL>>    The frames are returned as a list of separate images.<<NEWL>><<NEWL>>    :param im: An image, or a list of images.<<NEWL>>    :param func: The function to apply to all of the image frames.<<NEWL>>    :returns: A list of images.<<NEWL>>    """"""<<NEWL>>    if not isinstance(im, list):<<NEWL>>        im = [im]<<NEWL>><<NEWL>>    ims = []<<NEWL>>    for imSequence in im:<<NEWL>>        current = imSequence.tell()<<NEWL>><<NEWL>>        ims += [im_frame.copy() for im_frame in Iterator(imSequence)]<<NEWL>><<NEWL>>        imSequence.seek(current)<<NEWL>>    return [func(im) for im in ims] if func else ims"
492	jackson	1	"#<<NEWL>># download_mks_assets.py<<NEWL>># Added by HAS_TFT_LVGL_UI to download assets from Makerbase repo<<NEWL>>#<<NEWL>>import pioutil<<NEWL>>if pioutil.is_pio_build():<<NEWL>>    Import(""env"")<<NEWL>>    import requests,zipfile,tempfile,shutil<<NEWL>>    from pathlib import Path<<NEWL>><<NEWL>>    url = ""https://github.com/makerbase-mks/Mks-Robin-Nano-Marlin2.0-Firmware/archive/0263cdaccf.zip""<<NEWL>>    deps_path = Path(env.Dictionary(""PROJECT_LIBDEPS_DIR""))<<NEWL>>    zip_path = deps_path / ""mks-assets.zip""<<NEWL>>    assets_path = Path(env.Dictionary(""PROJECT_BUILD_DIR""), env.Dictionary(""PIOENV""), ""assets"")<<NEWL>><<NEWL>>    def download_mks_assets():<<NEWL>>        print(""Downloading MKS Assets"")<<NEWL>>        r = requests.get(url, stream=True)<<NEWL>>        # the user may have a very clean workspace,<<NEWL>>        # so create the PROJECT_LIBDEPS_DIR directory if not exits<<NEWL>>        if not deps_path.exists():<<NEWL>>            deps_path.mkdir()<<NEWL>>        with zip_path.open('wb') as fd:<<NEWL>>            for chunk in r.iter_content(chunk_size=128):<<NEWL>>                fd.write(chunk)<<NEWL>><<NEWL>>    def copy_mks_assets():<<NEWL>>        print(""Copying MKS Assets"")<<NEWL>>        output_path = Path(tempfile.mkdtemp())<<NEWL>>        zip_obj = zipfile.ZipFile(zip_path, 'r')<<NEWL>>        zip_obj.extractall(output_path)<<NEWL>>        zip_obj.close()<<NEWL>>        if assets_path.exists() and not assets_path.is_dir():<<NEWL>>            assets_path.unlink()<<NEWL>>        if not assets_path.exists():<<NEWL>>            assets_path.mkdir()<<NEWL>>        base_path = ''<<NEWL>>        for filename in output_path.iterdir():<<NEWL>>            base_path = filename<<NEWL>>        fw_path = (output_path / base_path / 'Firmware')<<NEWL>>        font_path = fw_path / 'mks_font'<<NEWL>>        for filename in font_path.iterdir():<<NEWL>>            shutil.copy(font_path / filename, assets_path)<<NEWL>>        pic_path = fw_path / 'mks_pic'<<NEWL>>        for filename in pic_path.iterdir():<<NEWL>>            shutil.copy(pic_path / filename, assets_path)<<NEWL>>        shutil.rmtree(output_path, ignore_errors=True)<<NEWL>><<NEWL>>    if not zip_path.exists():<<NEWL>>        download_mks_assets()<<NEWL>><<NEWL>>    if not assets_path.exists():<<NEWL>>        copy_mks_assets()"
492	donghui	1	"#<<NEWL>># download_mks_assets.py<<NEWL>># Added by HAS_TFT_LVGL_UI to download assets from Makerbase repo<<NEWL>>#<<NEWL>>import pioutil<<NEWL>>if pioutil.is_pio_build():<<NEWL>>    Import(""env"")<<NEWL>>    import requests,zipfile,tempfile,shutil<<NEWL>>    from pathlib import Path<<NEWL>><<NEWL>>    url = ""https://github.com/makerbase-mks/Mks-Robin-Nano-Marlin2.0-Firmware/archive/0263cdaccf.zip""<<NEWL>>    deps_path = Path(env.Dictionary(""PROJECT_LIBDEPS_DIR""))<<NEWL>>    zip_path = deps_path / ""mks-assets.zip""<<NEWL>>    assets_path = Path(env.Dictionary(""PROJECT_BUILD_DIR""), env.Dictionary(""PIOENV""), ""assets"")<<NEWL>><<NEWL>>    def download_mks_assets():<<NEWL>>        print(""Downloading MKS Assets"")<<NEWL>>        r = requests.get(url, stream=True)<<NEWL>>        # the user may have a very clean workspace,<<NEWL>>        # so create the PROJECT_LIBDEPS_DIR directory if not exits<<NEWL>>        if not deps_path.exists():<<NEWL>>            deps_path.mkdir()<<NEWL>>        with zip_path.open('wb') as fd:<<NEWL>>            for chunk in r.iter_content(chunk_size=128):<<NEWL>>                fd.write(chunk)<<NEWL>><<NEWL>>    def copy_mks_assets():<<NEWL>>        print(""Copying MKS Assets"")<<NEWL>>        output_path = Path(tempfile.mkdtemp())<<NEWL>>        zip_obj = zipfile.ZipFile(zip_path, 'r')<<NEWL>>        zip_obj.extractall(output_path)<<NEWL>>        zip_obj.close()<<NEWL>>        if assets_path.exists() and not assets_path.is_dir():<<NEWL>>            assets_path.unlink()<<NEWL>>        if not assets_path.exists():<<NEWL>>            assets_path.mkdir()<<NEWL>>        base_path = ''<<NEWL>>        for filename in output_path.iterdir():<<NEWL>>            base_path = filename<<NEWL>>        fw_path = (output_path / base_path / 'Firmware')<<NEWL>>        font_path = fw_path / 'mks_font'<<NEWL>>        for filename in font_path.iterdir():<<NEWL>>            shutil.copy(font_path / filename, assets_path)<<NEWL>>        pic_path = fw_path / 'mks_pic'<<NEWL>>        for filename in pic_path.iterdir():<<NEWL>>            shutil.copy(pic_path / filename, assets_path)<<NEWL>>        shutil.rmtree(output_path, ignore_errors=True)<<NEWL>><<NEWL>>    if not zip_path.exists():<<NEWL>>        download_mks_assets()<<NEWL>><<NEWL>>    if not assets_path.exists():<<NEWL>>        copy_mks_assets()"
430	jackson	4	"import os<<NEWL>><<NEWL>>from _pydev_bundle import pydev_log<<NEWL>>from _pydevd_bundle.pydevd_trace_dispatch import USING_CYTHON<<NEWL>>from _pydevd_bundle.pydevd_constants import USE_CYTHON_FLAG, ENV_FALSE_LOWER_VALUES, \<<NEWL>>    ENV_TRUE_LOWER_VALUES, IS_PY36_OR_GREATER, IS_PY38_OR_GREATER, SUPPORT_GEVENT, IS_PYTHON_STACKLESS, \<<NEWL>>    PYDEVD_USE_FRAME_EVAL, PYDEVD_IPYTHON_COMPATIBLE_DEBUGGING<<NEWL>><<NEWL>>frame_eval_func = None<<NEWL>>stop_frame_eval = None<<NEWL>>dummy_trace_dispatch = None<<NEWL>>clear_thread_local_info = None<<NEWL>><<NEWL>># ""NO"" means we should not use frame evaluation, 'YES' we should use it (and fail if not there) and unspecified uses if possible.<<NEWL>>if (<<NEWL>>        PYDEVD_USE_FRAME_EVAL in ENV_FALSE_LOWER_VALUES or<<NEWL>>        USE_CYTHON_FLAG in ENV_FALSE_LOWER_VALUES or<<NEWL>>        not USING_CYTHON or<<NEWL>><<NEWL>>        # Frame eval mode does not work with ipython compatible debugging (this happens because the<<NEWL>>        # way that frame eval works is run untraced and set tracing only for the frames with<<NEWL>>        # breakpoints, but ipython compatible debugging creates separate frames for what's logically<<NEWL>>        # the same frame).<<NEWL>>        PYDEVD_IPYTHON_COMPATIBLE_DEBUGGING<<NEWL>>    ):<<NEWL>>    USING_FRAME_EVAL = False<<NEWL>><<NEWL>>elif SUPPORT_GEVENT or (IS_PYTHON_STACKLESS and not IS_PY38_OR_GREATER):<<NEWL>>    USING_FRAME_EVAL = False<<NEWL>>    # i.e gevent and frame eval mode don't get along very well.<<NEWL>>    # https://github.com/microsoft/debugpy/issues/189<<NEWL>>    # Same problem with Stackless.<<NEWL>>    # https://github.com/stackless-dev/stackless/issues/240<<NEWL>><<NEWL>>elif PYDEVD_USE_FRAME_EVAL in ENV_TRUE_LOWER_VALUES:<<NEWL>>    # Fail if unable to use<<NEWL>>    from _pydevd_frame_eval.pydevd_frame_eval_cython_wrapper import frame_eval_func, stop_frame_eval, dummy_trace_dispatch, clear_thread_local_info<<NEWL>>    USING_FRAME_EVAL = True<<NEWL>><<NEWL>>else:<<NEWL>>    USING_FRAME_EVAL = False<<NEWL>>    # Try to use if possible<<NEWL>>    if IS_PY36_OR_GREATER:<<NEWL>>        try:<<NEWL>>            from _pydevd_frame_eval.pydevd_frame_eval_cython_wrapper import frame_eval_func, stop_frame_eval, dummy_trace_dispatch, clear_thread_local_info<<NEWL>>            USING_FRAME_EVAL = True<<NEWL>>        except ImportError:<<NEWL>>            pydev_log.show_compile_cython_command_line()"
430	donghui	2	"import os<<NEWL>><<NEWL>>from _pydev_bundle import pydev_log<<NEWL>>from _pydevd_bundle.pydevd_trace_dispatch import USING_CYTHON<<NEWL>>from _pydevd_bundle.pydevd_constants import USE_CYTHON_FLAG, ENV_FALSE_LOWER_VALUES, \<<NEWL>>    ENV_TRUE_LOWER_VALUES, IS_PY36_OR_GREATER, IS_PY38_OR_GREATER, SUPPORT_GEVENT, IS_PYTHON_STACKLESS, \<<NEWL>>    PYDEVD_USE_FRAME_EVAL, PYDEVD_IPYTHON_COMPATIBLE_DEBUGGING<<NEWL>><<NEWL>>frame_eval_func = None<<NEWL>>stop_frame_eval = None<<NEWL>>dummy_trace_dispatch = None<<NEWL>>clear_thread_local_info = None<<NEWL>><<NEWL>># ""NO"" means we should not use frame evaluation, 'YES' we should use it (and fail if not there) and unspecified uses if possible.<<NEWL>>if (<<NEWL>>        PYDEVD_USE_FRAME_EVAL in ENV_FALSE_LOWER_VALUES or<<NEWL>>        USE_CYTHON_FLAG in ENV_FALSE_LOWER_VALUES or<<NEWL>>        not USING_CYTHON or<<NEWL>><<NEWL>>        # Frame eval mode does not work with ipython compatible debugging (this happens because the<<NEWL>>        # way that frame eval works is run untraced and set tracing only for the frames with<<NEWL>>        # breakpoints, but ipython compatible debugging creates separate frames for what's logically<<NEWL>>        # the same frame).<<NEWL>>        PYDEVD_IPYTHON_COMPATIBLE_DEBUGGING<<NEWL>>    ):<<NEWL>>    USING_FRAME_EVAL = False<<NEWL>><<NEWL>>elif SUPPORT_GEVENT or (IS_PYTHON_STACKLESS and not IS_PY38_OR_GREATER):<<NEWL>>    USING_FRAME_EVAL = False<<NEWL>>    # i.e gevent and frame eval mode don't get along very well.<<NEWL>>    # https://github.com/microsoft/debugpy/issues/189<<NEWL>>    # Same problem with Stackless.<<NEWL>>    # https://github.com/stackless-dev/stackless/issues/240<<NEWL>><<NEWL>>elif PYDEVD_USE_FRAME_EVAL in ENV_TRUE_LOWER_VALUES:<<NEWL>>    # Fail if unable to use<<NEWL>>    from _pydevd_frame_eval.pydevd_frame_eval_cython_wrapper import frame_eval_func, stop_frame_eval, dummy_trace_dispatch, clear_thread_local_info<<NEWL>>    USING_FRAME_EVAL = True<<NEWL>><<NEWL>>else:<<NEWL>>    USING_FRAME_EVAL = False<<NEWL>>    # Try to use if possible<<NEWL>>    if IS_PY36_OR_GREATER:<<NEWL>>        try:<<NEWL>>            from _pydevd_frame_eval.pydevd_frame_eval_cython_wrapper import frame_eval_func, stop_frame_eval, dummy_trace_dispatch, clear_thread_local_info<<NEWL>>            USING_FRAME_EVAL = True<<NEWL>>        except ImportError:<<NEWL>>            pydev_log.show_compile_cython_command_line()"
461	jackson	1	"#!/usr/bin/env python3<<NEWL>><<NEWL>># Copyright (c) 2012 Google Inc. All rights reserved.<<NEWL>># Use of this source code is governed by a BSD-style license that can be<<NEWL>># found in the LICENSE file.<<NEWL>><<NEWL>>"""""" Unit tests for the ninja.py file. """"""<<NEWL>><<NEWL>>import sys<<NEWL>>import unittest<<NEWL>><<NEWL>>import gyp.generator.ninja as ninja<<NEWL>><<NEWL>><<NEWL>>class TestPrefixesAndSuffixes(unittest.TestCase):<<NEWL>>    def test_BinaryNamesWindows(self):<<NEWL>>        # These cannot run on non-Windows as they require a VS installation to<<NEWL>>        # correctly handle variable expansion.<<NEWL>>        if sys.platform.startswith(""win""):<<NEWL>>            writer = ninja.NinjaWriter(<<NEWL>>                ""foo"", ""wee"", ""."", ""."", ""build.ninja"", ""."", ""build.ninja"", ""win""<<NEWL>>            )<<NEWL>>            spec = {""target_name"": ""wee""}<<NEWL>>            self.assertTrue(<<NEWL>>                writer.ComputeOutputFileName(spec, ""executable"").endswith("".exe"")<<NEWL>>            )<<NEWL>>            self.assertTrue(<<NEWL>>                writer.ComputeOutputFileName(spec, ""shared_library"").endswith("".dll"")<<NEWL>>            )<<NEWL>>            self.assertTrue(<<NEWL>>                writer.ComputeOutputFileName(spec, ""static_library"").endswith("".lib"")<<NEWL>>            )<<NEWL>><<NEWL>>    def test_BinaryNamesLinux(self):<<NEWL>>        writer = ninja.NinjaWriter(<<NEWL>>            ""foo"", ""wee"", ""."", ""."", ""build.ninja"", ""."", ""build.ninja"", ""linux""<<NEWL>>        )<<NEWL>>        spec = {""target_name"": ""wee""}<<NEWL>>        self.assertTrue(""."" not in writer.ComputeOutputFileName(spec, ""executable""))<<NEWL>>        self.assertTrue(<<NEWL>>            writer.ComputeOutputFileName(spec, ""shared_library"").startswith(""lib"")<<NEWL>>        )<<NEWL>>        self.assertTrue(<<NEWL>>            writer.ComputeOutputFileName(spec, ""static_library"").startswith(""lib"")<<NEWL>>        )<<NEWL>>        self.assertTrue(<<NEWL>>            writer.ComputeOutputFileName(spec, ""shared_library"").endswith("".so"")<<NEWL>>        )<<NEWL>>        self.assertTrue(<<NEWL>>            writer.ComputeOutputFileName(spec, ""static_library"").endswith("".a"")<<NEWL>>        )<<NEWL>><<NEWL>><<NEWL>>if __name__ == ""__main__"":<<NEWL>>    unittest.main()"
461	donghui	2	"#!/usr/bin/env python3<<NEWL>><<NEWL>># Copyright (c) 2012 Google Inc. All rights reserved.<<NEWL>># Use of this source code is governed by a BSD-style license that can be<<NEWL>># found in the LICENSE file.<<NEWL>><<NEWL>>"""""" Unit tests for the ninja.py file. """"""<<NEWL>><<NEWL>>import sys<<NEWL>>import unittest<<NEWL>><<NEWL>>import gyp.generator.ninja as ninja<<NEWL>><<NEWL>><<NEWL>>class TestPrefixesAndSuffixes(unittest.TestCase):<<NEWL>>    def test_BinaryNamesWindows(self):<<NEWL>>        # These cannot run on non-Windows as they require a VS installation to<<NEWL>>        # correctly handle variable expansion.<<NEWL>>        if sys.platform.startswith(""win""):<<NEWL>>            writer = ninja.NinjaWriter(<<NEWL>>                ""foo"", ""wee"", ""."", ""."", ""build.ninja"", ""."", ""build.ninja"", ""win""<<NEWL>>            )<<NEWL>>            spec = {""target_name"": ""wee""}<<NEWL>>            self.assertTrue(<<NEWL>>                writer.ComputeOutputFileName(spec, ""executable"").endswith("".exe"")<<NEWL>>            )<<NEWL>>            self.assertTrue(<<NEWL>>                writer.ComputeOutputFileName(spec, ""shared_library"").endswith("".dll"")<<NEWL>>            )<<NEWL>>            self.assertTrue(<<NEWL>>                writer.ComputeOutputFileName(spec, ""static_library"").endswith("".lib"")<<NEWL>>            )<<NEWL>><<NEWL>>    def test_BinaryNamesLinux(self):<<NEWL>>        writer = ninja.NinjaWriter(<<NEWL>>            ""foo"", ""wee"", ""."", ""."", ""build.ninja"", ""."", ""build.ninja"", ""linux""<<NEWL>>        )<<NEWL>>        spec = {""target_name"": ""wee""}<<NEWL>>        self.assertTrue(""."" not in writer.ComputeOutputFileName(spec, ""executable""))<<NEWL>>        self.assertTrue(<<NEWL>>            writer.ComputeOutputFileName(spec, ""shared_library"").startswith(""lib"")<<NEWL>>        )<<NEWL>>        self.assertTrue(<<NEWL>>            writer.ComputeOutputFileName(spec, ""static_library"").startswith(""lib"")<<NEWL>>        )<<NEWL>>        self.assertTrue(<<NEWL>>            writer.ComputeOutputFileName(spec, ""shared_library"").endswith("".so"")<<NEWL>>        )<<NEWL>>        self.assertTrue(<<NEWL>>            writer.ComputeOutputFileName(spec, ""static_library"").endswith("".a"")<<NEWL>>        )<<NEWL>><<NEWL>><<NEWL>>if __name__ == ""__main__"":<<NEWL>>    unittest.main()"
441	jackson	4	"""""""<<NEWL>>PostGIS to GDAL conversion constant definitions<<NEWL>>""""""<<NEWL>># Lookup to convert pixel type values from GDAL to PostGIS<<NEWL>>GDAL_TO_POSTGIS = [None, 4, 6, 5, 8, 7, 10, 11, None, None, None, None]<<NEWL>><<NEWL>># Lookup to convert pixel type values from PostGIS to GDAL<<NEWL>>POSTGIS_TO_GDAL = [1, 1, 1, 3, 1, 3, 2, 5, 4, None, 6, 7, None, None]<<NEWL>><<NEWL>># Struct pack structure for raster header, the raster header has the<<NEWL>># following structure:<<NEWL>>#<<NEWL>># Endianness, PostGIS raster version, number of bands, scale, origin,<<NEWL>># skew, srid, width, and height.<<NEWL>>#<<NEWL>># Scale, origin, and skew have x and y values. PostGIS currently uses<<NEWL>># a fixed endianness (1) and there is only one version (0).<<NEWL>>POSTGIS_HEADER_STRUCTURE = ""B H H d d d d d d i H H""<<NEWL>><<NEWL>># Lookup values to convert GDAL pixel types to struct characters. This is<<NEWL>># used to pack and unpack the pixel values of PostGIS raster bands.<<NEWL>>GDAL_TO_STRUCT = [<<NEWL>>    None,<<NEWL>>    ""B"",<<NEWL>>    ""H"",<<NEWL>>    ""h"",<<NEWL>>    ""L"",<<NEWL>>    ""l"",<<NEWL>>    ""f"",<<NEWL>>    ""d"",<<NEWL>>    None,<<NEWL>>    None,<<NEWL>>    None,<<NEWL>>    None,<<NEWL>>]<<NEWL>><<NEWL>># Size of the packed value in bytes for different numerical types.<<NEWL>># This is needed to cut chunks of band data out of PostGIS raster strings<<NEWL>># when decomposing them into GDALRasters.<<NEWL>># See https://docs.python.org/library/struct.html#format-characters<<NEWL>>STRUCT_SIZE = {<<NEWL>>    ""b"": 1,  # Signed char<<NEWL>>    ""B"": 1,  # Unsigned char<<NEWL>>    ""?"": 1,  # _Bool<<NEWL>>    ""h"": 2,  # Short<<NEWL>>    ""H"": 2,  # Unsigned short<<NEWL>>    ""i"": 4,  # Integer<<NEWL>>    ""I"": 4,  # Unsigned Integer<<NEWL>>    ""l"": 4,  # Long<<NEWL>>    ""L"": 4,  # Unsigned Long<<NEWL>>    ""f"": 4,  # Float<<NEWL>>    ""d"": 8,  # Double<<NEWL>>}<<NEWL>><<NEWL>># Pixel type specifies type of pixel values in a band. Storage flag specifies<<NEWL>># whether the band data is stored as part of the datum or is to be found on the<<NEWL>># server's filesystem. There are currently 11 supported pixel value types, so 4<<NEWL>># bits are enough to account for all. Reserve the upper 4 bits for generic<<NEWL>># flags. See<<NEWL>># https://trac.osgeo.org/postgis/wiki/WKTRaster/RFC/RFC1_V0SerialFormat#Pixeltypeandstorageflag<<NEWL>>BANDTYPE_PIXTYPE_MASK = 0x0F<<NEWL>>BANDTYPE_FLAG_HASNODATA = 1 << 6"
441	donghui	3	"""""""<<NEWL>>PostGIS to GDAL conversion constant definitions<<NEWL>>""""""<<NEWL>># Lookup to convert pixel type values from GDAL to PostGIS<<NEWL>>GDAL_TO_POSTGIS = [None, 4, 6, 5, 8, 7, 10, 11, None, None, None, None]<<NEWL>><<NEWL>># Lookup to convert pixel type values from PostGIS to GDAL<<NEWL>>POSTGIS_TO_GDAL = [1, 1, 1, 3, 1, 3, 2, 5, 4, None, 6, 7, None, None]<<NEWL>><<NEWL>># Struct pack structure for raster header, the raster header has the<<NEWL>># following structure:<<NEWL>>#<<NEWL>># Endianness, PostGIS raster version, number of bands, scale, origin,<<NEWL>># skew, srid, width, and height.<<NEWL>>#<<NEWL>># Scale, origin, and skew have x and y values. PostGIS currently uses<<NEWL>># a fixed endianness (1) and there is only one version (0).<<NEWL>>POSTGIS_HEADER_STRUCTURE = ""B H H d d d d d d i H H""<<NEWL>><<NEWL>># Lookup values to convert GDAL pixel types to struct characters. This is<<NEWL>># used to pack and unpack the pixel values of PostGIS raster bands.<<NEWL>>GDAL_TO_STRUCT = [<<NEWL>>    None,<<NEWL>>    ""B"",<<NEWL>>    ""H"",<<NEWL>>    ""h"",<<NEWL>>    ""L"",<<NEWL>>    ""l"",<<NEWL>>    ""f"",<<NEWL>>    ""d"",<<NEWL>>    None,<<NEWL>>    None,<<NEWL>>    None,<<NEWL>>    None,<<NEWL>>]<<NEWL>><<NEWL>># Size of the packed value in bytes for different numerical types.<<NEWL>># This is needed to cut chunks of band data out of PostGIS raster strings<<NEWL>># when decomposing them into GDALRasters.<<NEWL>># See https://docs.python.org/library/struct.html#format-characters<<NEWL>>STRUCT_SIZE = {<<NEWL>>    ""b"": 1,  # Signed char<<NEWL>>    ""B"": 1,  # Unsigned char<<NEWL>>    ""?"": 1,  # _Bool<<NEWL>>    ""h"": 2,  # Short<<NEWL>>    ""H"": 2,  # Unsigned short<<NEWL>>    ""i"": 4,  # Integer<<NEWL>>    ""I"": 4,  # Unsigned Integer<<NEWL>>    ""l"": 4,  # Long<<NEWL>>    ""L"": 4,  # Unsigned Long<<NEWL>>    ""f"": 4,  # Float<<NEWL>>    ""d"": 8,  # Double<<NEWL>>}<<NEWL>><<NEWL>># Pixel type specifies type of pixel values in a band. Storage flag specifies<<NEWL>># whether the band data is stored as part of the datum or is to be found on the<<NEWL>># server's filesystem. There are currently 11 supported pixel value types, so 4<<NEWL>># bits are enough to account for all. Reserve the upper 4 bits for generic<<NEWL>># flags. See<<NEWL>># https://trac.osgeo.org/postgis/wiki/WKTRaster/RFC/RFC1_V0SerialFormat#Pixeltypeandstorageflag<<NEWL>>BANDTYPE_PIXTYPE_MASK = 0x0F<<NEWL>>BANDTYPE_FLAG_HASNODATA = 1 << 6"
410	jackson	1	"# -*- coding: utf-8 -*-<<NEWL>># Generated by the protocol buffer compiler.  DO NOT EDIT!<<NEWL>># source: streamlit/proto/Spinner.proto<<NEWL>><<NEWL>>from google.protobuf import descriptor as _descriptor<<NEWL>>from google.protobuf import message as _message<<NEWL>>from google.protobuf import reflection as _reflection<<NEWL>>from google.protobuf import symbol_database as _symbol_database<<NEWL>># @@protoc_insertion_point(imports)<<NEWL>><<NEWL>>_sym_db = _symbol_database.Default()<<NEWL>><<NEWL>><<NEWL>><<NEWL>><<NEWL>>DESCRIPTOR = _descriptor.FileDescriptor(<<NEWL>>  name='streamlit/proto/Spinner.proto',<<NEWL>>  package='',<<NEWL>>  syntax='proto3',<<NEWL>>  serialized_options=None,<<NEWL>>  create_key=_descriptor._internal_create_key,<<NEWL>>  serialized_pb=b'\n\x1dstreamlit/proto/Spinner.proto\""\x17\n\x07Spinner\x12\x0c\n\x04text\x18\x01 \x01(\tb\x06proto3'<<NEWL>>)<<NEWL>><<NEWL>><<NEWL>><<NEWL>><<NEWL>>_SPINNER = _descriptor.Descriptor(<<NEWL>>  name='Spinner',<<NEWL>>  full_name='Spinner',<<NEWL>>  filename=None,<<NEWL>>  file=DESCRIPTOR,<<NEWL>>  containing_type=None,<<NEWL>>  create_key=_descriptor._internal_create_key,<<NEWL>>  fields=[<<NEWL>>    _descriptor.FieldDescriptor(<<NEWL>>      name='text', full_name='Spinner.text', index=0,<<NEWL>>      number=1, type=9, cpp_type=9, label=1,<<NEWL>>      has_default_value=False, default_value=b"""".decode('utf-8'),<<NEWL>>      message_type=None, enum_type=None, containing_type=None,<<NEWL>>      is_extension=False, extension_scope=None,<<NEWL>>      serialized_options=None, file=DESCRIPTOR,  create_key=_descriptor._internal_create_key),<<NEWL>>  ],<<NEWL>>  extensions=[<<NEWL>>  ],<<NEWL>>  nested_types=[],<<NEWL>>  enum_types=[<<NEWL>>  ],<<NEWL>>  serialized_options=None,<<NEWL>>  is_extendable=False,<<NEWL>>  syntax='proto3',<<NEWL>>  extension_ranges=[],<<NEWL>>  oneofs=[<<NEWL>>  ],<<NEWL>>  serialized_start=33,<<NEWL>>  serialized_end=56,<<NEWL>>)<<NEWL>><<NEWL>>DESCRIPTOR.message_types_by_name['Spinner'] = _SPINNER<<NEWL>>_sym_db.RegisterFileDescriptor(DESCRIPTOR)<<NEWL>><<NEWL>>Spinner = _reflection.GeneratedProtocolMessageType('Spinner', (_message.Message,), {<<NEWL>>  'DESCRIPTOR' : _SPINNER,<<NEWL>>  '__module__' : 'streamlit.proto.Spinner_pb2'<<NEWL>>  # @@protoc_insertion_point(class_scope:Spinner)<<NEWL>>  })<<NEWL>>_sym_db.RegisterMessage(Spinner)<<NEWL>><<NEWL>><<NEWL>># @@protoc_insertion_point(module_scope)"
410	donghui	1	"# -*- coding: utf-8 -*-<<NEWL>># Generated by the protocol buffer compiler.  DO NOT EDIT!<<NEWL>># source: streamlit/proto/Spinner.proto<<NEWL>><<NEWL>>from google.protobuf import descriptor as _descriptor<<NEWL>>from google.protobuf import message as _message<<NEWL>>from google.protobuf import reflection as _reflection<<NEWL>>from google.protobuf import symbol_database as _symbol_database<<NEWL>># @@protoc_insertion_point(imports)<<NEWL>><<NEWL>>_sym_db = _symbol_database.Default()<<NEWL>><<NEWL>><<NEWL>><<NEWL>><<NEWL>>DESCRIPTOR = _descriptor.FileDescriptor(<<NEWL>>  name='streamlit/proto/Spinner.proto',<<NEWL>>  package='',<<NEWL>>  syntax='proto3',<<NEWL>>  serialized_options=None,<<NEWL>>  create_key=_descriptor._internal_create_key,<<NEWL>>  serialized_pb=b'\n\x1dstreamlit/proto/Spinner.proto\""\x17\n\x07Spinner\x12\x0c\n\x04text\x18\x01 \x01(\tb\x06proto3'<<NEWL>>)<<NEWL>><<NEWL>><<NEWL>><<NEWL>><<NEWL>>_SPINNER = _descriptor.Descriptor(<<NEWL>>  name='Spinner',<<NEWL>>  full_name='Spinner',<<NEWL>>  filename=None,<<NEWL>>  file=DESCRIPTOR,<<NEWL>>  containing_type=None,<<NEWL>>  create_key=_descriptor._internal_create_key,<<NEWL>>  fields=[<<NEWL>>    _descriptor.FieldDescriptor(<<NEWL>>      name='text', full_name='Spinner.text', index=0,<<NEWL>>      number=1, type=9, cpp_type=9, label=1,<<NEWL>>      has_default_value=False, default_value=b"""".decode('utf-8'),<<NEWL>>      message_type=None, enum_type=None, containing_type=None,<<NEWL>>      is_extension=False, extension_scope=None,<<NEWL>>      serialized_options=None, file=DESCRIPTOR,  create_key=_descriptor._internal_create_key),<<NEWL>>  ],<<NEWL>>  extensions=[<<NEWL>>  ],<<NEWL>>  nested_types=[],<<NEWL>>  enum_types=[<<NEWL>>  ],<<NEWL>>  serialized_options=None,<<NEWL>>  is_extendable=False,<<NEWL>>  syntax='proto3',<<NEWL>>  extension_ranges=[],<<NEWL>>  oneofs=[<<NEWL>>  ],<<NEWL>>  serialized_start=33,<<NEWL>>  serialized_end=56,<<NEWL>>)<<NEWL>><<NEWL>>DESCRIPTOR.message_types_by_name['Spinner'] = _SPINNER<<NEWL>>_sym_db.RegisterFileDescriptor(DESCRIPTOR)<<NEWL>><<NEWL>>Spinner = _reflection.GeneratedProtocolMessageType('Spinner', (_message.Message,), {<<NEWL>>  'DESCRIPTOR' : _SPINNER,<<NEWL>>  '__module__' : 'streamlit.proto.Spinner_pb2'<<NEWL>>  # @@protoc_insertion_point(class_scope:Spinner)<<NEWL>>  })<<NEWL>>_sym_db.RegisterMessage(Spinner)<<NEWL>><<NEWL>><<NEWL>># @@protoc_insertion_point(module_scope)"
424	jackson	1	"""""""Test markdown rendering""""""<<NEWL>><<NEWL>><<NEWL>>from nbformat.v4 import new_markdown_cell<<NEWL>><<NEWL>>from .utils import EDITOR_PAGE<<NEWL>><<NEWL>><<NEWL>>def get_rendered_contents(nb):<<NEWL>>    # TODO: Encapsulate element access/refactor so we're not accessing playwright element objects<<NEWL>>    cl = [""text_cell"", ""render""]<<NEWL>>    rendered_cells = [cell.locate("".text_cell_render"")<<NEWL>>                      for cell in nb.cells<<NEWL>>                      if all([c in cell.get_attribute(""class"") for c in cl])]<<NEWL>>    return [x.get_inner_html().strip()<<NEWL>>            for x in rendered_cells<<NEWL>>            if x is not None]<<NEWL>><<NEWL>><<NEWL>>def test_markdown_cell(prefill_notebook):<<NEWL>>    notebook_frontend = prefill_notebook([new_markdown_cell(md) for md in [<<NEWL>>        '# Foo', '**Bar**', '*Baz*', '```\nx = 1\n```', '```aaaa\nx = 1\n```',<<NEWL>>        '```python\ns = ""$""\nt = ""$""\n```'<<NEWL>>    ]])<<NEWL>><<NEWL>>    assert get_rendered_contents(notebook_frontend) == [<<NEWL>>        '<h1 id=""Foo"">Foo<a class=""anchor-link"" href=""#Foo"">¶</a></h1>',<<NEWL>>        '<p><strong>Bar</strong></p>',<<NEWL>>        '<p><em>Baz</em></p>',<<NEWL>>        '<pre><code>x = 1</code></pre>',<<NEWL>>        '<pre><code class=""cm-s-ipython language-aaaa"">x = 1</code></pre>',<<NEWL>>        '<pre><code class=""cm-s-ipython language-python"">' +<<NEWL>>        '<span class=""cm-variable"">s</span> <span class=""cm-operator"">=</span> <span class=""cm-string"">""$""</span>\n' +<<NEWL>>        '<span class=""cm-variable"">t</span> <span class=""cm-operator"">=</span> <span class=""cm-string"">""$""</span></code></pre>'<<NEWL>>    ]<<NEWL>><<NEWL>><<NEWL>>def test_markdown_headings(notebook_frontend):<<NEWL>>    for i in [1, 2, 3, 4, 5, 6, 2, 1]:<<NEWL>>        notebook_frontend.add_markdown_cell()<<NEWL>>        cell_text = notebook_frontend.evaluate(f""""""<<NEWL>>            var cell = IPython.notebook.get_cell(1);<<NEWL>>            cell.set_heading_level({i});<<NEWL>>            cell.get_text();<<NEWL>>        """""", page=EDITOR_PAGE)<<NEWL>>        assert notebook_frontend.get_cell_contents(1) == ""#"" * i + "" ""<<NEWL>>        notebook_frontend.delete_cell(1)"
424	donghui	1	"""""""Test markdown rendering""""""<<NEWL>><<NEWL>><<NEWL>>from nbformat.v4 import new_markdown_cell<<NEWL>><<NEWL>>from .utils import EDITOR_PAGE<<NEWL>><<NEWL>><<NEWL>>def get_rendered_contents(nb):<<NEWL>>    # TODO: Encapsulate element access/refactor so we're not accessing playwright element objects<<NEWL>>    cl = [""text_cell"", ""render""]<<NEWL>>    rendered_cells = [cell.locate("".text_cell_render"")<<NEWL>>                      for cell in nb.cells<<NEWL>>                      if all([c in cell.get_attribute(""class"") for c in cl])]<<NEWL>>    return [x.get_inner_html().strip()<<NEWL>>            for x in rendered_cells<<NEWL>>            if x is not None]<<NEWL>><<NEWL>><<NEWL>>def test_markdown_cell(prefill_notebook):<<NEWL>>    notebook_frontend = prefill_notebook([new_markdown_cell(md) for md in [<<NEWL>>        '# Foo', '**Bar**', '*Baz*', '```\nx = 1\n```', '```aaaa\nx = 1\n```',<<NEWL>>        '```python\ns = ""$""\nt = ""$""\n```'<<NEWL>>    ]])<<NEWL>><<NEWL>>    assert get_rendered_contents(notebook_frontend) == [<<NEWL>>        '<h1 id=""Foo"">Foo<a class=""anchor-link"" href=""#Foo"">¶</a></h1>',<<NEWL>>        '<p><strong>Bar</strong></p>',<<NEWL>>        '<p><em>Baz</em></p>',<<NEWL>>        '<pre><code>x = 1</code></pre>',<<NEWL>>        '<pre><code class=""cm-s-ipython language-aaaa"">x = 1</code></pre>',<<NEWL>>        '<pre><code class=""cm-s-ipython language-python"">' +<<NEWL>>        '<span class=""cm-variable"">s</span> <span class=""cm-operator"">=</span> <span class=""cm-string"">""$""</span>\n' +<<NEWL>>        '<span class=""cm-variable"">t</span> <span class=""cm-operator"">=</span> <span class=""cm-string"">""$""</span></code></pre>'<<NEWL>>    ]<<NEWL>><<NEWL>><<NEWL>>def test_markdown_headings(notebook_frontend):<<NEWL>>    for i in [1, 2, 3, 4, 5, 6, 2, 1]:<<NEWL>>        notebook_frontend.add_markdown_cell()<<NEWL>>        cell_text = notebook_frontend.evaluate(f""""""<<NEWL>>            var cell = IPython.notebook.get_cell(1);<<NEWL>>            cell.set_heading_level({i});<<NEWL>>            cell.get_text();<<NEWL>>        """""", page=EDITOR_PAGE)<<NEWL>>        assert notebook_frontend.get_cell_contents(1) == ""#"" * i + "" ""<<NEWL>>        notebook_frontend.delete_cell(1)"
475	jackson	1	from rx.core import ObservableBase, Observer, AnonymousObserver, Disposable<<NEWL>>from rx.disposables import CompositeDisposable<<NEWL>><<NEWL>>from .subscription import Subscription<<NEWL>>from .reactive_assert import AssertList<<NEWL>><<NEWL>><<NEWL>>class ColdObservable(ObservableBase):<<NEWL>>    def __init__(self, scheduler, messages):<<NEWL>>        super(ColdObservable, self).__init__()<<NEWL>><<NEWL>>        self.scheduler = scheduler<<NEWL>>        self.messages = messages<<NEWL>>        self.subscriptions = AssertList()<<NEWL>><<NEWL>>    def subscribe(self, on_next=None, on_error=None, on_completed=None, observer=None):<<NEWL>>        # Be forgiving and accept an un-named observer as first parameter<<NEWL>>        if isinstance(on_next, Observer):<<NEWL>>            observer = on_next<<NEWL>>        elif not observer:<<NEWL>>            observer = AnonymousObserver(on_next, on_error, on_completed)<<NEWL>><<NEWL>>        return self._subscribe_core(observer)<<NEWL>><<NEWL>>    def _subscribe_core(self, observer):<<NEWL>>        clock = self.scheduler.to_relative(self.scheduler.now)<<NEWL>>        self.subscriptions.append(Subscription(clock))<<NEWL>>        index = len(self.subscriptions) - 1<<NEWL>>        disposable = CompositeDisposable()<<NEWL>><<NEWL>>        def get_action(notification):<<NEWL>>            def action(scheduler, state):<<NEWL>>                notification.accept(observer)<<NEWL>>                return Disposable.empty()<<NEWL>>            return action<<NEWL>><<NEWL>>        for message in self.messages:<<NEWL>>            notification = message.value<<NEWL>><<NEWL>>            # Don't make closures within a loop<<NEWL>>            action = get_action(notification)<<NEWL>>            disposable.add(self.scheduler.schedule_relative(message.time, action))<<NEWL>><<NEWL>>        def dispose():<<NEWL>>            start = self.subscriptions[index].subscribe<<NEWL>>            end = self.scheduler.to_relative(self.scheduler.now)<<NEWL>>            self.subscriptions[index] = Subscription(start, end)<<NEWL>>            disposable.dispose()<<NEWL>><<NEWL>>        return Disposable.create(dispose)
475	donghui	0	from rx.core import ObservableBase, Observer, AnonymousObserver, Disposable<<NEWL>>from rx.disposables import CompositeDisposable<<NEWL>><<NEWL>>from .subscription import Subscription<<NEWL>>from .reactive_assert import AssertList<<NEWL>><<NEWL>><<NEWL>>class ColdObservable(ObservableBase):<<NEWL>>    def __init__(self, scheduler, messages):<<NEWL>>        super(ColdObservable, self).__init__()<<NEWL>><<NEWL>>        self.scheduler = scheduler<<NEWL>>        self.messages = messages<<NEWL>>        self.subscriptions = AssertList()<<NEWL>><<NEWL>>    def subscribe(self, on_next=None, on_error=None, on_completed=None, observer=None):<<NEWL>>        # Be forgiving and accept an un-named observer as first parameter<<NEWL>>        if isinstance(on_next, Observer):<<NEWL>>            observer = on_next<<NEWL>>        elif not observer:<<NEWL>>            observer = AnonymousObserver(on_next, on_error, on_completed)<<NEWL>><<NEWL>>        return self._subscribe_core(observer)<<NEWL>><<NEWL>>    def _subscribe_core(self, observer):<<NEWL>>        clock = self.scheduler.to_relative(self.scheduler.now)<<NEWL>>        self.subscriptions.append(Subscription(clock))<<NEWL>>        index = len(self.subscriptions) - 1<<NEWL>>        disposable = CompositeDisposable()<<NEWL>><<NEWL>>        def get_action(notification):<<NEWL>>            def action(scheduler, state):<<NEWL>>                notification.accept(observer)<<NEWL>>                return Disposable.empty()<<NEWL>>            return action<<NEWL>><<NEWL>>        for message in self.messages:<<NEWL>>            notification = message.value<<NEWL>><<NEWL>>            # Don't make closures within a loop<<NEWL>>            action = get_action(notification)<<NEWL>>            disposable.add(self.scheduler.schedule_relative(message.time, action))<<NEWL>><<NEWL>>        def dispose():<<NEWL>>            start = self.subscriptions[index].subscribe<<NEWL>>            end = self.scheduler.to_relative(self.scheduler.now)<<NEWL>>            self.subscriptions[index] = Subscription(start, end)<<NEWL>>            disposable.dispose()<<NEWL>><<NEWL>>        return Disposable.create(dispose)
486	jackson	1	"import socket<<NEWL>>import typing<<NEWL>><<NEWL>>from tornado.http1connection import HTTP1Connection<<NEWL>>from tornado.httputil import HTTPMessageDelegate<<NEWL>>from tornado.iostream import IOStream<<NEWL>>from tornado.locks import Event<<NEWL>>from tornado.netutil import add_accept_handler<<NEWL>>from tornado.testing import AsyncTestCase, bind_unused_port, gen_test<<NEWL>><<NEWL>><<NEWL>>class HTTP1ConnectionTest(AsyncTestCase):<<NEWL>>    code = None  # type: typing.Optional[int]<<NEWL>><<NEWL>>    def setUp(self):<<NEWL>>        super().setUp()<<NEWL>>        self.asyncSetUp()<<NEWL>><<NEWL>>    @gen_test<<NEWL>>    def asyncSetUp(self):<<NEWL>>        listener, port = bind_unused_port()<<NEWL>>        event = Event()<<NEWL>><<NEWL>>        def accept_callback(conn, addr):<<NEWL>>            self.server_stream = IOStream(conn)<<NEWL>>            self.addCleanup(self.server_stream.close)<<NEWL>>            event.set()<<NEWL>><<NEWL>>        add_accept_handler(listener, accept_callback)<<NEWL>>        self.client_stream = IOStream(socket.socket())<<NEWL>>        self.addCleanup(self.client_stream.close)<<NEWL>>        yield [self.client_stream.connect((""127.0.0.1"", port)), event.wait()]<<NEWL>>        self.io_loop.remove_handler(listener)<<NEWL>>        listener.close()<<NEWL>><<NEWL>>    @gen_test<<NEWL>>    def test_http10_no_content_length(self):<<NEWL>>        # Regression test for a bug in which can_keep_alive would crash<<NEWL>>        # for an HTTP/1.0 (not 1.1) response with no content-length.<<NEWL>>        conn = HTTP1Connection(self.client_stream, True)<<NEWL>>        self.server_stream.write(b""HTTP/1.0 200 Not Modified\r\n\r\nhello"")<<NEWL>>        self.server_stream.close()<<NEWL>><<NEWL>>        event = Event()<<NEWL>>        test = self<<NEWL>>        body = []<<NEWL>><<NEWL>>        class Delegate(HTTPMessageDelegate):<<NEWL>>            def headers_received(self, start_line, headers):<<NEWL>>                test.code = start_line.code<<NEWL>><<NEWL>>            def data_received(self, data):<<NEWL>>                body.append(data)<<NEWL>><<NEWL>>            def finish(self):<<NEWL>>                event.set()<<NEWL>><<NEWL>>        yield conn.read_response(Delegate())<<NEWL>>        yield event.wait()<<NEWL>>        self.assertEqual(self.code, 200)<<NEWL>>        self.assertEqual(b"""".join(body), b""hello"")"
486	donghui	1	"import socket<<NEWL>>import typing<<NEWL>><<NEWL>>from tornado.http1connection import HTTP1Connection<<NEWL>>from tornado.httputil import HTTPMessageDelegate<<NEWL>>from tornado.iostream import IOStream<<NEWL>>from tornado.locks import Event<<NEWL>>from tornado.netutil import add_accept_handler<<NEWL>>from tornado.testing import AsyncTestCase, bind_unused_port, gen_test<<NEWL>><<NEWL>><<NEWL>>class HTTP1ConnectionTest(AsyncTestCase):<<NEWL>>    code = None  # type: typing.Optional[int]<<NEWL>><<NEWL>>    def setUp(self):<<NEWL>>        super().setUp()<<NEWL>>        self.asyncSetUp()<<NEWL>><<NEWL>>    @gen_test<<NEWL>>    def asyncSetUp(self):<<NEWL>>        listener, port = bind_unused_port()<<NEWL>>        event = Event()<<NEWL>><<NEWL>>        def accept_callback(conn, addr):<<NEWL>>            self.server_stream = IOStream(conn)<<NEWL>>            self.addCleanup(self.server_stream.close)<<NEWL>>            event.set()<<NEWL>><<NEWL>>        add_accept_handler(listener, accept_callback)<<NEWL>>        self.client_stream = IOStream(socket.socket())<<NEWL>>        self.addCleanup(self.client_stream.close)<<NEWL>>        yield [self.client_stream.connect((""127.0.0.1"", port)), event.wait()]<<NEWL>>        self.io_loop.remove_handler(listener)<<NEWL>>        listener.close()<<NEWL>><<NEWL>>    @gen_test<<NEWL>>    def test_http10_no_content_length(self):<<NEWL>>        # Regression test for a bug in which can_keep_alive would crash<<NEWL>>        # for an HTTP/1.0 (not 1.1) response with no content-length.<<NEWL>>        conn = HTTP1Connection(self.client_stream, True)<<NEWL>>        self.server_stream.write(b""HTTP/1.0 200 Not Modified\r\n\r\nhello"")<<NEWL>>        self.server_stream.close()<<NEWL>><<NEWL>>        event = Event()<<NEWL>>        test = self<<NEWL>>        body = []<<NEWL>><<NEWL>>        class Delegate(HTTPMessageDelegate):<<NEWL>>            def headers_received(self, start_line, headers):<<NEWL>>                test.code = start_line.code<<NEWL>><<NEWL>>            def data_received(self, data):<<NEWL>>                body.append(data)<<NEWL>><<NEWL>>            def finish(self):<<NEWL>>                event.set()<<NEWL>><<NEWL>>        yield conn.read_response(Delegate())<<NEWL>>        yield event.wait()<<NEWL>>        self.assertEqual(self.code, 200)<<NEWL>>        self.assertEqual(b"""".join(body), b""hello"")"
399	jackson	3	"# PermWrapper and PermLookupDict proxy the permissions system into objects that<<NEWL>># the template system can understand.<<NEWL>><<NEWL>><<NEWL>>class PermLookupDict:<<NEWL>>    def __init__(self, user, app_label):<<NEWL>>        self.user, self.app_label = user, app_label<<NEWL>><<NEWL>>    def __repr__(self):<<NEWL>>        return str(self.user.get_all_permissions())<<NEWL>><<NEWL>>    def __getitem__(self, perm_name):<<NEWL>>        return self.user.has_perm(""%s.%s"" % (self.app_label, perm_name))<<NEWL>><<NEWL>>    def __iter__(self):<<NEWL>>        # To fix 'item in perms.someapp' and __getitem__ interaction we need to<<NEWL>>        # define __iter__. See #18979 for details.<<NEWL>>        raise TypeError(""PermLookupDict is not iterable."")<<NEWL>><<NEWL>>    def __bool__(self):<<NEWL>>        return self.user.has_module_perms(self.app_label)<<NEWL>><<NEWL>><<NEWL>>class PermWrapper:<<NEWL>>    def __init__(self, user):<<NEWL>>        self.user = user<<NEWL>><<NEWL>>    def __getitem__(self, app_label):<<NEWL>>        return PermLookupDict(self.user, app_label)<<NEWL>><<NEWL>>    def __iter__(self):<<NEWL>>        # I am large, I contain multitudes.<<NEWL>>        raise TypeError(""PermWrapper is not iterable."")<<NEWL>><<NEWL>>    def __contains__(self, perm_name):<<NEWL>>        """"""<<NEWL>>        Lookup by ""someapp"" or ""someapp.someperm"" in perms.<<NEWL>>        """"""<<NEWL>>        if '.' not in perm_name:<<NEWL>>            # The name refers to module.<<NEWL>>            return bool(self[perm_name])<<NEWL>>        app_label, perm_name = perm_name.split('.', 1)<<NEWL>>        return self[app_label][perm_name]<<NEWL>><<NEWL>><<NEWL>>def auth(request):<<NEWL>>    """"""<<NEWL>>    Return context variables required by apps that use Django's authentication<<NEWL>>    system.<<NEWL>><<NEWL>>    If there is no 'user' attribute in the request, use AnonymousUser (from<<NEWL>>    django.contrib.auth).<<NEWL>>    """"""<<NEWL>>    if hasattr(request, 'user'):<<NEWL>>        user = request.user<<NEWL>>    else:<<NEWL>>        from django.contrib.auth.models import AnonymousUser<<NEWL>>        user = AnonymousUser()<<NEWL>><<NEWL>>    return {<<NEWL>>        'user': user,<<NEWL>>        'perms': PermWrapper(user),<<NEWL>>    }"
399	donghui	2	"# PermWrapper and PermLookupDict proxy the permissions system into objects that<<NEWL>># the template system can understand.<<NEWL>><<NEWL>><<NEWL>>class PermLookupDict:<<NEWL>>    def __init__(self, user, app_label):<<NEWL>>        self.user, self.app_label = user, app_label<<NEWL>><<NEWL>>    def __repr__(self):<<NEWL>>        return str(self.user.get_all_permissions())<<NEWL>><<NEWL>>    def __getitem__(self, perm_name):<<NEWL>>        return self.user.has_perm(""%s.%s"" % (self.app_label, perm_name))<<NEWL>><<NEWL>>    def __iter__(self):<<NEWL>>        # To fix 'item in perms.someapp' and __getitem__ interaction we need to<<NEWL>>        # define __iter__. See #18979 for details.<<NEWL>>        raise TypeError(""PermLookupDict is not iterable."")<<NEWL>><<NEWL>>    def __bool__(self):<<NEWL>>        return self.user.has_module_perms(self.app_label)<<NEWL>><<NEWL>><<NEWL>>class PermWrapper:<<NEWL>>    def __init__(self, user):<<NEWL>>        self.user = user<<NEWL>><<NEWL>>    def __getitem__(self, app_label):<<NEWL>>        return PermLookupDict(self.user, app_label)<<NEWL>><<NEWL>>    def __iter__(self):<<NEWL>>        # I am large, I contain multitudes.<<NEWL>>        raise TypeError(""PermWrapper is not iterable."")<<NEWL>><<NEWL>>    def __contains__(self, perm_name):<<NEWL>>        """"""<<NEWL>>        Lookup by ""someapp"" or ""someapp.someperm"" in perms.<<NEWL>>        """"""<<NEWL>>        if '.' not in perm_name:<<NEWL>>            # The name refers to module.<<NEWL>>            return bool(self[perm_name])<<NEWL>>        app_label, perm_name = perm_name.split('.', 1)<<NEWL>>        return self[app_label][perm_name]<<NEWL>><<NEWL>><<NEWL>>def auth(request):<<NEWL>>    """"""<<NEWL>>    Return context variables required by apps that use Django's authentication<<NEWL>>    system.<<NEWL>><<NEWL>>    If there is no 'user' attribute in the request, use AnonymousUser (from<<NEWL>>    django.contrib.auth).<<NEWL>>    """"""<<NEWL>>    if hasattr(request, 'user'):<<NEWL>>        user = request.user<<NEWL>>    else:<<NEWL>>        from django.contrib.auth.models import AnonymousUser<<NEWL>>        user = AnonymousUser()<<NEWL>><<NEWL>>    return {<<NEWL>>        'user': user,<<NEWL>>        'perms': PermWrapper(user),<<NEWL>>    }"
288	jackson	0	"import esphome.codegen as cg<<NEWL>>import esphome.config_validation as cv<<NEWL>>from esphome import pins<<NEWL>>from esphome.components import display<<NEWL>>from esphome.const import (<<NEWL>>    CONF_CLK_PIN,<<NEWL>>    CONF_DIO_PIN,<<NEWL>>    CONF_ID,<<NEWL>>    CONF_LAMBDA,<<NEWL>>    CONF_INTENSITY,<<NEWL>>    CONF_INVERTED,<<NEWL>>    CONF_LENGTH,<<NEWL>>)<<NEWL>><<NEWL>>CODEOWNERS = [""@glmnet""]<<NEWL>><<NEWL>>tm1637_ns = cg.esphome_ns.namespace(""tm1637"")<<NEWL>>TM1637Display = tm1637_ns.class_(""TM1637Display"", cg.PollingComponent)<<NEWL>>TM1637DisplayRef = TM1637Display.operator(""ref"")<<NEWL>><<NEWL>>CONFIG_SCHEMA = display.BASIC_DISPLAY_SCHEMA.extend(<<NEWL>>    {<<NEWL>>        cv.GenerateID(): cv.declare_id(TM1637Display),<<NEWL>>        cv.Optional(CONF_INTENSITY, default=7): cv.All(<<NEWL>>            cv.uint8_t, cv.Range(min=0, max=7)<<NEWL>>        ),<<NEWL>>        cv.Optional(CONF_INVERTED, default=False): cv.boolean,<<NEWL>>        cv.Optional(CONF_LENGTH, default=6): cv.All(cv.uint8_t, cv.Range(min=1, max=6)),<<NEWL>>        cv.Required(CONF_CLK_PIN): pins.gpio_output_pin_schema,<<NEWL>>        cv.Required(CONF_DIO_PIN): pins.gpio_output_pin_schema,<<NEWL>>    }<<NEWL>>).extend(cv.polling_component_schema(""1s""))<<NEWL>><<NEWL>><<NEWL>>async def to_code(config):<<NEWL>>    var = cg.new_Pvariable(config[CONF_ID])<<NEWL>>    await cg.register_component(var, config)<<NEWL>>    await display.register_display(var, config)<<NEWL>><<NEWL>>    clk = await cg.gpio_pin_expression(config[CONF_CLK_PIN])<<NEWL>>    cg.add(var.set_clk_pin(clk))<<NEWL>>    dio = await cg.gpio_pin_expression(config[CONF_DIO_PIN])<<NEWL>>    cg.add(var.set_dio_pin(dio))<<NEWL>><<NEWL>>    cg.add(var.set_intensity(config[CONF_INTENSITY]))<<NEWL>>    cg.add(var.set_inverted(config[CONF_INVERTED]))<<NEWL>>    cg.add(var.set_length(config[CONF_LENGTH]))<<NEWL>><<NEWL>>    if CONF_LAMBDA in config:<<NEWL>>        lambda_ = await cg.process_lambda(<<NEWL>>            config[CONF_LAMBDA], [(TM1637DisplayRef, ""it"")], return_type=cg.void<<NEWL>>        )<<NEWL>>        cg.add(var.set_writer(lambda_))"
288	donghui	0	"import esphome.codegen as cg<<NEWL>>import esphome.config_validation as cv<<NEWL>>from esphome import pins<<NEWL>>from esphome.components import display<<NEWL>>from esphome.const import (<<NEWL>>    CONF_CLK_PIN,<<NEWL>>    CONF_DIO_PIN,<<NEWL>>    CONF_ID,<<NEWL>>    CONF_LAMBDA,<<NEWL>>    CONF_INTENSITY,<<NEWL>>    CONF_INVERTED,<<NEWL>>    CONF_LENGTH,<<NEWL>>)<<NEWL>><<NEWL>>CODEOWNERS = [""@glmnet""]<<NEWL>><<NEWL>>tm1637_ns = cg.esphome_ns.namespace(""tm1637"")<<NEWL>>TM1637Display = tm1637_ns.class_(""TM1637Display"", cg.PollingComponent)<<NEWL>>TM1637DisplayRef = TM1637Display.operator(""ref"")<<NEWL>><<NEWL>>CONFIG_SCHEMA = display.BASIC_DISPLAY_SCHEMA.extend(<<NEWL>>    {<<NEWL>>        cv.GenerateID(): cv.declare_id(TM1637Display),<<NEWL>>        cv.Optional(CONF_INTENSITY, default=7): cv.All(<<NEWL>>            cv.uint8_t, cv.Range(min=0, max=7)<<NEWL>>        ),<<NEWL>>        cv.Optional(CONF_INVERTED, default=False): cv.boolean,<<NEWL>>        cv.Optional(CONF_LENGTH, default=6): cv.All(cv.uint8_t, cv.Range(min=1, max=6)),<<NEWL>>        cv.Required(CONF_CLK_PIN): pins.gpio_output_pin_schema,<<NEWL>>        cv.Required(CONF_DIO_PIN): pins.gpio_output_pin_schema,<<NEWL>>    }<<NEWL>>).extend(cv.polling_component_schema(""1s""))<<NEWL>><<NEWL>><<NEWL>>async def to_code(config):<<NEWL>>    var = cg.new_Pvariable(config[CONF_ID])<<NEWL>>    await cg.register_component(var, config)<<NEWL>>    await display.register_display(var, config)<<NEWL>><<NEWL>>    clk = await cg.gpio_pin_expression(config[CONF_CLK_PIN])<<NEWL>>    cg.add(var.set_clk_pin(clk))<<NEWL>>    dio = await cg.gpio_pin_expression(config[CONF_DIO_PIN])<<NEWL>>    cg.add(var.set_dio_pin(dio))<<NEWL>><<NEWL>>    cg.add(var.set_intensity(config[CONF_INTENSITY]))<<NEWL>>    cg.add(var.set_inverted(config[CONF_INVERTED]))<<NEWL>>    cg.add(var.set_length(config[CONF_LENGTH]))<<NEWL>><<NEWL>>    if CONF_LAMBDA in config:<<NEWL>>        lambda_ = await cg.process_lambda(<<NEWL>>            config[CONF_LAMBDA], [(TM1637DisplayRef, ""it"")], return_type=cg.void<<NEWL>>        )<<NEWL>>        cg.add(var.set_writer(lambda_))"
298	jackson	2	"import _plotly_utils.basevalidators<<NEWL>><<NEWL>><<NEWL>>class InsidetextfontValidator(_plotly_utils.basevalidators.CompoundValidator):<<NEWL>>    def __init__(self, plotly_name=""insidetextfont"", parent_name=""bar"", **kwargs):<<NEWL>>        super(InsidetextfontValidator, self).__init__(<<NEWL>>            plotly_name=plotly_name,<<NEWL>>            parent_name=parent_name,<<NEWL>>            data_class_str=kwargs.pop(""data_class_str"", ""Insidetextfont""),<<NEWL>>            data_docs=kwargs.pop(<<NEWL>>                ""data_docs"",<<NEWL>>                """"""<<NEWL>>            color<<NEWL>><<NEWL>>            colorsrc<<NEWL>>                Sets the source reference on Chart Studio Cloud<<NEWL>>                for `color`.<<NEWL>>            family<<NEWL>>                HTML font family - the typeface that will be<<NEWL>>                applied by the web browser. The web browser<<NEWL>>                will only be able to apply a font if it is<<NEWL>>                available on the system which it operates.<<NEWL>>                Provide multiple font families, separated by<<NEWL>>                commas, to indicate the preference in which to<<NEWL>>                apply fonts if they aren't available on the<<NEWL>>                system. The Chart Studio Cloud (at<<NEWL>>                https://chart-studio.plotly.com or on-premise)<<NEWL>>                generates images on a server, where only a<<NEWL>>                select number of fonts are installed and<<NEWL>>                supported. These include ""Arial"", ""Balto"",<<NEWL>>                ""Courier New"", ""Droid Sans"",, ""Droid Serif"",<<NEWL>>                ""Droid Sans Mono"", ""Gravitas One"", ""Old<<NEWL>>                Standard TT"", ""Open Sans"", ""Overpass"", ""PT Sans<<NEWL>>                Narrow"", ""Raleway"", ""Times New Roman"".<<NEWL>>            familysrc<<NEWL>>                Sets the source reference on Chart Studio Cloud<<NEWL>>                for `family`.<<NEWL>>            size<<NEWL>><<NEWL>>            sizesrc<<NEWL>>                Sets the source reference on Chart Studio Cloud<<NEWL>>                for `size`.<<NEWL>>"""""",<<NEWL>>            ),<<NEWL>>            **kwargs,<<NEWL>>        )"
298	donghui	1	"import _plotly_utils.basevalidators<<NEWL>><<NEWL>><<NEWL>>class InsidetextfontValidator(_plotly_utils.basevalidators.CompoundValidator):<<NEWL>>    def __init__(self, plotly_name=""insidetextfont"", parent_name=""bar"", **kwargs):<<NEWL>>        super(InsidetextfontValidator, self).__init__(<<NEWL>>            plotly_name=plotly_name,<<NEWL>>            parent_name=parent_name,<<NEWL>>            data_class_str=kwargs.pop(""data_class_str"", ""Insidetextfont""),<<NEWL>>            data_docs=kwargs.pop(<<NEWL>>                ""data_docs"",<<NEWL>>                """"""<<NEWL>>            color<<NEWL>><<NEWL>>            colorsrc<<NEWL>>                Sets the source reference on Chart Studio Cloud<<NEWL>>                for `color`.<<NEWL>>            family<<NEWL>>                HTML font family - the typeface that will be<<NEWL>>                applied by the web browser. The web browser<<NEWL>>                will only be able to apply a font if it is<<NEWL>>                available on the system which it operates.<<NEWL>>                Provide multiple font families, separated by<<NEWL>>                commas, to indicate the preference in which to<<NEWL>>                apply fonts if they aren't available on the<<NEWL>>                system. The Chart Studio Cloud (at<<NEWL>>                https://chart-studio.plotly.com or on-premise)<<NEWL>>                generates images on a server, where only a<<NEWL>>                select number of fonts are installed and<<NEWL>>                supported. These include ""Arial"", ""Balto"",<<NEWL>>                ""Courier New"", ""Droid Sans"",, ""Droid Serif"",<<NEWL>>                ""Droid Sans Mono"", ""Gravitas One"", ""Old<<NEWL>>                Standard TT"", ""Open Sans"", ""Overpass"", ""PT Sans<<NEWL>>                Narrow"", ""Raleway"", ""Times New Roman"".<<NEWL>>            familysrc<<NEWL>>                Sets the source reference on Chart Studio Cloud<<NEWL>>                for `family`.<<NEWL>>            size<<NEWL>><<NEWL>>            sizesrc<<NEWL>>                Sets the source reference on Chart Studio Cloud<<NEWL>>                for `size`.<<NEWL>>"""""",<<NEWL>>            ),<<NEWL>>            **kwargs,<<NEWL>>        )"
389	jackson	0	"import cv2<<NEWL>>from cvzone.HandTrackingModule import HandDetector<<NEWL>>from cvzone.ClassificationModule import Classifier<<NEWL>>import numpy as np<<NEWL>>import math<<NEWL>>cap = cv2.VideoCapture(0)<<NEWL>>detector = HandDetector(maxHands=1)<<NEWL>>classifier = Classifier(""keras_model.h5"", ""labels.txt"")<<NEWL>><<NEWL>>offset = 20<<NEWL>>imgSize = 300<<NEWL>><<NEWL>>folder = ""Data/C""<<NEWL>>counter = 0<<NEWL>><<NEWL>>labels = [""A"", ""B"", ""C""]<<NEWL>><<NEWL>>while True:<<NEWL>>    success, img = cap.read()<<NEWL>>    imgOutput = img.copy()<<NEWL>>    hands, img = detector.findHands(img)<<NEWL>>    if hands:<<NEWL>>        hand = hands[0]<<NEWL>>        x, y, w, h = hand['bbox']<<NEWL>><<NEWL>>        imgWhite = np.ones((imgSize, imgSize, 3), np.uint8) * 255<<NEWL>>        imgCrop = img[y - offset:y + h + offset, x - offset:x + w + offset]<<NEWL>><<NEWL>>        imgCropShape = imgCrop.shape<<NEWL>><<NEWL>>        aspectRatio = h / w<<NEWL>><<NEWL>>        if aspectRatio > 1:<<NEWL>>            k = imgSize / h<<NEWL>>            wCal = math.ceil(k * w)<<NEWL>>            imgResize = cv2.resize(imgCrop, (wCal, imgSize))<<NEWL>>            imgResizeShape = imgResize.shape<<NEWL>>            wGap = math.ceil((imgSize - wCal) / 2)<<NEWL>>            imgWhite[:, wGap:wCal + wGap] = imgResize<<NEWL>>            prediction, index = classifier.getPrediction(imgWhite, draw=False)<<NEWL>>            print(prediction, index)<<NEWL>><<NEWL>>        else:<<NEWL>>            k = imgSize / w<<NEWL>>            hCal = math.ceil(k * h)<<NEWL>>            imgResize = cv2.resize(imgCrop, (imgSize, hCal))<<NEWL>>            imgResizeShape = imgResize.shape<<NEWL>>            hGap = math.ceil((imgSize - hCal) / 2)<<NEWL>>            imgWhite[hGap:hCal + hGap, :] = imgResize<<NEWL>>            prediction, index = classifier.getPrediction(imgWhite, draw=False)<<NEWL>><<NEWL>>        cv2.rectangle(imgOutput, (x - offset, y - offset-50),<<NEWL>>                      (x - offset+90, y - offset-50+50), (255, 0, 255), cv2.FILLED)<<NEWL>>        cv2.putText(imgOutput, labels[index], (x, y -26), cv2.FONT_HERSHEY_COMPLEX, 1.7, (255, 255, 255), 2)<<NEWL>>        cv2.rectangle(imgOutput, (x-offset, y-offset),<<NEWL>>                      (x + w+offset, y + h+offset), (255, 0, 255), 4)<<NEWL>><<NEWL>><<NEWL>>        cv2.imshow(""ImageCrop"", imgCrop)<<NEWL>>        cv2.imshow(""ImageWhite"", imgWhite)<<NEWL>><<NEWL>>    cv2.imshow(""Image"", imgOutput)<<NEWL>>    cv2.waitKey(1)"
389	donghui	0	"import cv2<<NEWL>>from cvzone.HandTrackingModule import HandDetector<<NEWL>>from cvzone.ClassificationModule import Classifier<<NEWL>>import numpy as np<<NEWL>>import math<<NEWL>>cap = cv2.VideoCapture(0)<<NEWL>>detector = HandDetector(maxHands=1)<<NEWL>>classifier = Classifier(""keras_model.h5"", ""labels.txt"")<<NEWL>><<NEWL>>offset = 20<<NEWL>>imgSize = 300<<NEWL>><<NEWL>>folder = ""Data/C""<<NEWL>>counter = 0<<NEWL>><<NEWL>>labels = [""A"", ""B"", ""C""]<<NEWL>><<NEWL>>while True:<<NEWL>>    success, img = cap.read()<<NEWL>>    imgOutput = img.copy()<<NEWL>>    hands, img = detector.findHands(img)<<NEWL>>    if hands:<<NEWL>>        hand = hands[0]<<NEWL>>        x, y, w, h = hand['bbox']<<NEWL>><<NEWL>>        imgWhite = np.ones((imgSize, imgSize, 3), np.uint8) * 255<<NEWL>>        imgCrop = img[y - offset:y + h + offset, x - offset:x + w + offset]<<NEWL>><<NEWL>>        imgCropShape = imgCrop.shape<<NEWL>><<NEWL>>        aspectRatio = h / w<<NEWL>><<NEWL>>        if aspectRatio > 1:<<NEWL>>            k = imgSize / h<<NEWL>>            wCal = math.ceil(k * w)<<NEWL>>            imgResize = cv2.resize(imgCrop, (wCal, imgSize))<<NEWL>>            imgResizeShape = imgResize.shape<<NEWL>>            wGap = math.ceil((imgSize - wCal) / 2)<<NEWL>>            imgWhite[:, wGap:wCal + wGap] = imgResize<<NEWL>>            prediction, index = classifier.getPrediction(imgWhite, draw=False)<<NEWL>>            print(prediction, index)<<NEWL>><<NEWL>>        else:<<NEWL>>            k = imgSize / w<<NEWL>>            hCal = math.ceil(k * h)<<NEWL>>            imgResize = cv2.resize(imgCrop, (imgSize, hCal))<<NEWL>>            imgResizeShape = imgResize.shape<<NEWL>>            hGap = math.ceil((imgSize - hCal) / 2)<<NEWL>>            imgWhite[hGap:hCal + hGap, :] = imgResize<<NEWL>>            prediction, index = classifier.getPrediction(imgWhite, draw=False)<<NEWL>><<NEWL>>        cv2.rectangle(imgOutput, (x - offset, y - offset-50),<<NEWL>>                      (x - offset+90, y - offset-50+50), (255, 0, 255), cv2.FILLED)<<NEWL>>        cv2.putText(imgOutput, labels[index], (x, y -26), cv2.FONT_HERSHEY_COMPLEX, 1.7, (255, 255, 255), 2)<<NEWL>>        cv2.rectangle(imgOutput, (x-offset, y-offset),<<NEWL>>                      (x + w+offset, y + h+offset), (255, 0, 255), 4)<<NEWL>><<NEWL>><<NEWL>>        cv2.imshow(""ImageCrop"", imgCrop)<<NEWL>>        cv2.imshow(""ImageWhite"", imgWhite)<<NEWL>><<NEWL>>    cv2.imshow(""Image"", imgOutput)<<NEWL>>    cv2.waitKey(1)"
496	jackson	4	"# Copyright 2021 Google LLC<<NEWL>>#<<NEWL>># Licensed under the Apache License, Version 2.0 (the ""License"");<<NEWL>># you may not use this file except in compliance with the License.<<NEWL>># You may obtain a copy of the License at<<NEWL>>#<<NEWL>>#      http://www.apache.org/licenses/LICENSE-2.0<<NEWL>>#<<NEWL>># Unless required by applicable law or agreed to in writing, software<<NEWL>># distributed under the License is distributed on an ""AS IS"" BASIS,<<NEWL>># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.<<NEWL>># See the License for the specific language governing permissions and<<NEWL>># limitations under the License.<<NEWL>><<NEWL>># Default TEST_CONFIG_OVERRIDE for python repos.<<NEWL>><<NEWL>># You can copy this file into your directory, then it will be imported from<<NEWL>># the noxfile.py.<<NEWL>><<NEWL>># The source of truth:<<NEWL>># https://github.com/GoogleCloudPlatform/python-docs-samples/blob/main/noxfile_config.py<<NEWL>><<NEWL>>TEST_CONFIG_OVERRIDE = {<<NEWL>>    # You can opt out from the test for specific Python versions.<<NEWL>>    ""ignored_versions"": [""2.7"", ""3.6"", ""3.9"", ""3.10"", ""3.11""],<<NEWL>>    # Old samples are opted out of enforcing Python type hints<<NEWL>>    # All new samples should feature them<<NEWL>>    ""enforce_type_hints"": False,<<NEWL>>    # An envvar key for determining the project id to use. Change it<<NEWL>>    # to 'BUILD_SPECIFIC_GCLOUD_PROJECT' if you want to opt in using a<<NEWL>>    # build specific Cloud project. You can also use your own string<<NEWL>>    # to use your own Cloud project.<<NEWL>>    ""gcloud_project_env"": ""GOOGLE_CLOUD_PROJECT"",<<NEWL>>    # 'gcloud_project_env': 'BUILD_SPECIFIC_GCLOUD_PROJECT',<<NEWL>>    # If you need to use a specific version of pip,<<NEWL>>    # change pip_version_override to the string representation<<NEWL>>    # of the version number, for example, ""20.2.4""<<NEWL>>    ""pip_version_override"": None,<<NEWL>>    # A dictionary you want to inject into your test. Don't put any<<NEWL>>    # secrets here. These values will override predefined values.<<NEWL>>    ""envs"": {},<<NEWL>>}"
496	donghui	3	"# Copyright 2021 Google LLC<<NEWL>>#<<NEWL>># Licensed under the Apache License, Version 2.0 (the ""License"");<<NEWL>># you may not use this file except in compliance with the License.<<NEWL>># You may obtain a copy of the License at<<NEWL>>#<<NEWL>>#      http://www.apache.org/licenses/LICENSE-2.0<<NEWL>>#<<NEWL>># Unless required by applicable law or agreed to in writing, software<<NEWL>># distributed under the License is distributed on an ""AS IS"" BASIS,<<NEWL>># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.<<NEWL>># See the License for the specific language governing permissions and<<NEWL>># limitations under the License.<<NEWL>><<NEWL>># Default TEST_CONFIG_OVERRIDE for python repos.<<NEWL>><<NEWL>># You can copy this file into your directory, then it will be imported from<<NEWL>># the noxfile.py.<<NEWL>><<NEWL>># The source of truth:<<NEWL>># https://github.com/GoogleCloudPlatform/python-docs-samples/blob/main/noxfile_config.py<<NEWL>><<NEWL>>TEST_CONFIG_OVERRIDE = {<<NEWL>>    # You can opt out from the test for specific Python versions.<<NEWL>>    ""ignored_versions"": [""2.7"", ""3.6"", ""3.9"", ""3.10"", ""3.11""],<<NEWL>>    # Old samples are opted out of enforcing Python type hints<<NEWL>>    # All new samples should feature them<<NEWL>>    ""enforce_type_hints"": False,<<NEWL>>    # An envvar key for determining the project id to use. Change it<<NEWL>>    # to 'BUILD_SPECIFIC_GCLOUD_PROJECT' if you want to opt in using a<<NEWL>>    # build specific Cloud project. You can also use your own string<<NEWL>>    # to use your own Cloud project.<<NEWL>>    ""gcloud_project_env"": ""GOOGLE_CLOUD_PROJECT"",<<NEWL>>    # 'gcloud_project_env': 'BUILD_SPECIFIC_GCLOUD_PROJECT',<<NEWL>>    # If you need to use a specific version of pip,<<NEWL>>    # change pip_version_override to the string representation<<NEWL>>    # of the version number, for example, ""20.2.4""<<NEWL>>    ""pip_version_override"": None,<<NEWL>>    # A dictionary you want to inject into your test. Don't put any<<NEWL>>    # secrets here. These values will override predefined values.<<NEWL>>    ""envs"": {},<<NEWL>>}"
465	jackson	0	"# Copyright (c) Meta Platforms, Inc. and affiliates.<<NEWL>>#<<NEWL>># This source code is licensed under the MIT license found in the<<NEWL>># LICENSE file in the root directory of this source tree.<<NEWL>><<NEWL>>import dataclasses<<NEWL>><<NEWL>>from textwrap import dedent<<NEWL>><<NEWL>>import viktor._vendor.libcst as cst<<NEWL>>from viktor._vendor.libcst.metadata import AccessorProvider, MetadataWrapper<<NEWL>>from viktor._vendor.libcst.testing.utils import data_provider, UnitTest<<NEWL>><<NEWL>><<NEWL>>class DependentVisitor(cst.CSTVisitor):<<NEWL>>    METADATA_DEPENDENCIES = (AccessorProvider,)<<NEWL>><<NEWL>>    def __init__(self, *, test: UnitTest) -> None:<<NEWL>>        self.test = test<<NEWL>><<NEWL>>    def on_visit(self, node: cst.CSTNode) -> bool:<<NEWL>>        for f in dataclasses.fields(node):<<NEWL>>            child = getattr(node, f.name)<<NEWL>>            if type(child) is cst.CSTNode:<<NEWL>>                accessor = self.get_metadata(AccessorProvider, child)<<NEWL>>                self.test.assertEqual(accessor, f.name)<<NEWL>><<NEWL>>        return True<<NEWL>><<NEWL>><<NEWL>>class AccessorProviderTest(UnitTest):<<NEWL>>    @data_provider(<<NEWL>>        (<<NEWL>>            (<<NEWL>>                """"""<<NEWL>>                foo = 'toplevel'<<NEWL>>                fn1(foo)<<NEWL>>                fn2(foo)<<NEWL>>                def fn_def():<<NEWL>>                    foo = 'shadow'<<NEWL>>                    fn3(foo)<<NEWL>>                """""",<<NEWL>>            ),<<NEWL>>            (<<NEWL>>                """"""<<NEWL>>                global_var = None<<NEWL>>                @cls_attr<<NEWL>>                class Cls(cls_attr, kwarg=cls_attr):<<NEWL>>                    cls_attr = 5<<NEWL>>                    def f():<<NEWL>>                        pass<<NEWL>>                """""",<<NEWL>>            ),<<NEWL>>            (<<NEWL>>                """"""<<NEWL>>                iterator = None<<NEWL>>                condition = None<<NEWL>>                [elt for target in iterator if condition]<<NEWL>>                {elt for target in iterator if condition}<<NEWL>>                {elt: target for target in iterator if condition}<<NEWL>>                (elt for target in iterator if condition)<<NEWL>>                """""",<<NEWL>>            ),<<NEWL>>        )<<NEWL>>    )<<NEWL>>    def test_accessor_provier(self, code: str) -> None:<<NEWL>>        wrapper = MetadataWrapper(cst.parse_module(dedent(code)))<<NEWL>>        wrapper.visit(DependentVisitor(test=self))"
465	donghui	0	"# Copyright (c) Meta Platforms, Inc. and affiliates.<<NEWL>>#<<NEWL>># This source code is licensed under the MIT license found in the<<NEWL>># LICENSE file in the root directory of this source tree.<<NEWL>><<NEWL>>import dataclasses<<NEWL>><<NEWL>>from textwrap import dedent<<NEWL>><<NEWL>>import viktor._vendor.libcst as cst<<NEWL>>from viktor._vendor.libcst.metadata import AccessorProvider, MetadataWrapper<<NEWL>>from viktor._vendor.libcst.testing.utils import data_provider, UnitTest<<NEWL>><<NEWL>><<NEWL>>class DependentVisitor(cst.CSTVisitor):<<NEWL>>    METADATA_DEPENDENCIES = (AccessorProvider,)<<NEWL>><<NEWL>>    def __init__(self, *, test: UnitTest) -> None:<<NEWL>>        self.test = test<<NEWL>><<NEWL>>    def on_visit(self, node: cst.CSTNode) -> bool:<<NEWL>>        for f in dataclasses.fields(node):<<NEWL>>            child = getattr(node, f.name)<<NEWL>>            if type(child) is cst.CSTNode:<<NEWL>>                accessor = self.get_metadata(AccessorProvider, child)<<NEWL>>                self.test.assertEqual(accessor, f.name)<<NEWL>><<NEWL>>        return True<<NEWL>><<NEWL>><<NEWL>>class AccessorProviderTest(UnitTest):<<NEWL>>    @data_provider(<<NEWL>>        (<<NEWL>>            (<<NEWL>>                """"""<<NEWL>>                foo = 'toplevel'<<NEWL>>                fn1(foo)<<NEWL>>                fn2(foo)<<NEWL>>                def fn_def():<<NEWL>>                    foo = 'shadow'<<NEWL>>                    fn3(foo)<<NEWL>>                """""",<<NEWL>>            ),<<NEWL>>            (<<NEWL>>                """"""<<NEWL>>                global_var = None<<NEWL>>                @cls_attr<<NEWL>>                class Cls(cls_attr, kwarg=cls_attr):<<NEWL>>                    cls_attr = 5<<NEWL>>                    def f():<<NEWL>>                        pass<<NEWL>>                """""",<<NEWL>>            ),<<NEWL>>            (<<NEWL>>                """"""<<NEWL>>                iterator = None<<NEWL>>                condition = None<<NEWL>>                [elt for target in iterator if condition]<<NEWL>>                {elt for target in iterator if condition}<<NEWL>>                {elt: target for target in iterator if condition}<<NEWL>>                (elt for target in iterator if condition)<<NEWL>>                """""",<<NEWL>>            ),<<NEWL>>        )<<NEWL>>    )<<NEWL>>    def test_accessor_provier(self, code: str) -> None:<<NEWL>>        wrapper = MetadataWrapper(cst.parse_module(dedent(code)))<<NEWL>>        wrapper.visit(DependentVisitor(test=self))"
434	jackson	4	"# This file is distributed under the same license as the Django package.<<NEWL>>#<<NEWL>># The *_FORMAT strings use the Django date format syntax,<<NEWL>># see https://docs.djangoproject.com/en/dev/ref/templates/builtins/#date<<NEWL>>DATE_FORMAT = ""d F Y""  # 25 Ottobre 2006<<NEWL>>TIME_FORMAT = ""H:i""  # 14:30<<NEWL>>DATETIME_FORMAT = ""l d F Y H:i""  # Mercoledì 25 Ottobre 2006 14:30<<NEWL>>YEAR_MONTH_FORMAT = ""F Y""  # Ottobre 2006<<NEWL>>MONTH_DAY_FORMAT = ""j F""  # 25 Ottobre<<NEWL>>SHORT_DATE_FORMAT = ""d/m/Y""  # 25/12/2009<<NEWL>>SHORT_DATETIME_FORMAT = ""d/m/Y H:i""  # 25/10/2009 14:30<<NEWL>>FIRST_DAY_OF_WEEK = 1  # Lunedì<<NEWL>><<NEWL>># The *_INPUT_FORMATS strings use the Python strftime format syntax,<<NEWL>># see https://docs.python.org/library/datetime.html#strftime-strptime-behavior<<NEWL>>DATE_INPUT_FORMATS = [<<NEWL>>    ""%d/%m/%Y"",  # '25/10/2006'<<NEWL>>    ""%Y/%m/%d"",  # '2006/10/25'<<NEWL>>    ""%d-%m-%Y"",  # '25-10-2006'<<NEWL>>    ""%Y-%m-%d"",  # '2006-10-25'<<NEWL>>    ""%d-%m-%y"",  # '25-10-06'<<NEWL>>    ""%d/%m/%y"",  # '25/10/06'<<NEWL>>]<<NEWL>>DATETIME_INPUT_FORMATS = [<<NEWL>>    ""%d/%m/%Y %H:%M:%S"",  # '25/10/2006 14:30:59'<<NEWL>>    ""%d/%m/%Y %H:%M:%S.%f"",  # '25/10/2006 14:30:59.000200'<<NEWL>>    ""%d/%m/%Y %H:%M"",  # '25/10/2006 14:30'<<NEWL>>    ""%d/%m/%y %H:%M:%S"",  # '25/10/06 14:30:59'<<NEWL>>    ""%d/%m/%y %H:%M:%S.%f"",  # '25/10/06 14:30:59.000200'<<NEWL>>    ""%d/%m/%y %H:%M"",  # '25/10/06 14:30'<<NEWL>>    ""%Y-%m-%d %H:%M:%S"",  # '2006-10-25 14:30:59'<<NEWL>>    ""%Y-%m-%d %H:%M:%S.%f"",  # '2006-10-25 14:30:59.000200'<<NEWL>>    ""%Y-%m-%d %H:%M"",  # '2006-10-25 14:30'<<NEWL>>    ""%d-%m-%Y %H:%M:%S"",  # '25-10-2006 14:30:59'<<NEWL>>    ""%d-%m-%Y %H:%M:%S.%f"",  # '25-10-2006 14:30:59.000200'<<NEWL>>    ""%d-%m-%Y %H:%M"",  # '25-10-2006 14:30'<<NEWL>>    ""%d-%m-%y %H:%M:%S"",  # '25-10-06 14:30:59'<<NEWL>>    ""%d-%m-%y %H:%M:%S.%f"",  # '25-10-06 14:30:59.000200'<<NEWL>>    ""%d-%m-%y %H:%M"",  # '25-10-06 14:30'<<NEWL>>]<<NEWL>>DECIMAL_SEPARATOR = "",""<<NEWL>>THOUSAND_SEPARATOR = "".""<<NEWL>>NUMBER_GROUPING = 3"
434	donghui	1	"# This file is distributed under the same license as the Django package.<<NEWL>>#<<NEWL>># The *_FORMAT strings use the Django date format syntax,<<NEWL>># see https://docs.djangoproject.com/en/dev/ref/templates/builtins/#date<<NEWL>>DATE_FORMAT = ""d F Y""  # 25 Ottobre 2006<<NEWL>>TIME_FORMAT = ""H:i""  # 14:30<<NEWL>>DATETIME_FORMAT = ""l d F Y H:i""  # Mercoledì 25 Ottobre 2006 14:30<<NEWL>>YEAR_MONTH_FORMAT = ""F Y""  # Ottobre 2006<<NEWL>>MONTH_DAY_FORMAT = ""j F""  # 25 Ottobre<<NEWL>>SHORT_DATE_FORMAT = ""d/m/Y""  # 25/12/2009<<NEWL>>SHORT_DATETIME_FORMAT = ""d/m/Y H:i""  # 25/10/2009 14:30<<NEWL>>FIRST_DAY_OF_WEEK = 1  # Lunedì<<NEWL>><<NEWL>># The *_INPUT_FORMATS strings use the Python strftime format syntax,<<NEWL>># see https://docs.python.org/library/datetime.html#strftime-strptime-behavior<<NEWL>>DATE_INPUT_FORMATS = [<<NEWL>>    ""%d/%m/%Y"",  # '25/10/2006'<<NEWL>>    ""%Y/%m/%d"",  # '2006/10/25'<<NEWL>>    ""%d-%m-%Y"",  # '25-10-2006'<<NEWL>>    ""%Y-%m-%d"",  # '2006-10-25'<<NEWL>>    ""%d-%m-%y"",  # '25-10-06'<<NEWL>>    ""%d/%m/%y"",  # '25/10/06'<<NEWL>>]<<NEWL>>DATETIME_INPUT_FORMATS = [<<NEWL>>    ""%d/%m/%Y %H:%M:%S"",  # '25/10/2006 14:30:59'<<NEWL>>    ""%d/%m/%Y %H:%M:%S.%f"",  # '25/10/2006 14:30:59.000200'<<NEWL>>    ""%d/%m/%Y %H:%M"",  # '25/10/2006 14:30'<<NEWL>>    ""%d/%m/%y %H:%M:%S"",  # '25/10/06 14:30:59'<<NEWL>>    ""%d/%m/%y %H:%M:%S.%f"",  # '25/10/06 14:30:59.000200'<<NEWL>>    ""%d/%m/%y %H:%M"",  # '25/10/06 14:30'<<NEWL>>    ""%Y-%m-%d %H:%M:%S"",  # '2006-10-25 14:30:59'<<NEWL>>    ""%Y-%m-%d %H:%M:%S.%f"",  # '2006-10-25 14:30:59.000200'<<NEWL>>    ""%Y-%m-%d %H:%M"",  # '2006-10-25 14:30'<<NEWL>>    ""%d-%m-%Y %H:%M:%S"",  # '25-10-2006 14:30:59'<<NEWL>>    ""%d-%m-%Y %H:%M:%S.%f"",  # '25-10-2006 14:30:59.000200'<<NEWL>>    ""%d-%m-%Y %H:%M"",  # '25-10-2006 14:30'<<NEWL>>    ""%d-%m-%y %H:%M:%S"",  # '25-10-06 14:30:59'<<NEWL>>    ""%d-%m-%y %H:%M:%S.%f"",  # '25-10-06 14:30:59.000200'<<NEWL>>    ""%d-%m-%y %H:%M"",  # '25-10-06 14:30'<<NEWL>>]<<NEWL>>DECIMAL_SEPARATOR = "",""<<NEWL>>THOUSAND_SEPARATOR = "".""<<NEWL>>NUMBER_GROUPING = 3"
400	jackson	4	"""""""Stuff that differs in different Python versions and platform<<NEWL>>distributions.""""""<<NEWL>><<NEWL>>import logging<<NEWL>>import os<<NEWL>>import sys<<NEWL>><<NEWL>>__all__ = [""get_path_uid"", ""stdlib_pkgs"", ""WINDOWS""]<<NEWL>><<NEWL>><<NEWL>>logger = logging.getLogger(__name__)<<NEWL>><<NEWL>><<NEWL>>def has_tls() -> bool:<<NEWL>>    try:<<NEWL>>        import _ssl  # noqa: F401  # ignore unused<<NEWL>><<NEWL>>        return True<<NEWL>>    except ImportError:<<NEWL>>        pass<<NEWL>><<NEWL>>    from pip._vendor.urllib3.util import IS_PYOPENSSL<<NEWL>><<NEWL>>    return IS_PYOPENSSL<<NEWL>><<NEWL>><<NEWL>>def get_path_uid(path: str) -> int:<<NEWL>>    """"""<<NEWL>>    Return path's uid.<<NEWL>><<NEWL>>    Does not follow symlinks:<<NEWL>>        https://github.com/pypa/pip/pull/935#discussion_r5307003<<NEWL>><<NEWL>>    Placed this function in compat due to differences on AIX and<<NEWL>>    Jython, that should eventually go away.<<NEWL>><<NEWL>>    :raises OSError: When path is a symlink or can't be read.<<NEWL>>    """"""<<NEWL>>    if hasattr(os, ""O_NOFOLLOW""):<<NEWL>>        fd = os.open(path, os.O_RDONLY | os.O_NOFOLLOW)<<NEWL>>        file_uid = os.fstat(fd).st_uid<<NEWL>>        os.close(fd)<<NEWL>>    else:  # AIX and Jython<<NEWL>>        # WARNING: time of check vulnerability, but best we can do w/o NOFOLLOW<<NEWL>>        if not os.path.islink(path):<<NEWL>>            # older versions of Jython don't have `os.fstat`<<NEWL>>            file_uid = os.stat(path).st_uid<<NEWL>>        else:<<NEWL>>            # raise OSError for parity with os.O_NOFOLLOW above<<NEWL>>            raise OSError(f""{path} is a symlink; Will not return uid for symlinks"")<<NEWL>>    return file_uid<<NEWL>><<NEWL>><<NEWL>># packages in the stdlib that may have installation metadata, but should not be<<NEWL>># considered 'installed'.  this theoretically could be determined based on<<NEWL>># dist.location (py27:`sysconfig.get_paths()['stdlib']`,<<NEWL>># py26:sysconfig.get_config_vars('LIBDEST')), but fear platform variation may<<NEWL>># make this ineffective, so hard-coding<<NEWL>>stdlib_pkgs = {""python"", ""wsgiref"", ""argparse""}<<NEWL>><<NEWL>><<NEWL>># windows detection, covers cpython and ironpython<<NEWL>>WINDOWS = sys.platform.startswith(""win"") or (sys.platform == ""cli"" and os.name == ""nt"")"
400	donghui	3	"""""""Stuff that differs in different Python versions and platform<<NEWL>>distributions.""""""<<NEWL>><<NEWL>>import logging<<NEWL>>import os<<NEWL>>import sys<<NEWL>><<NEWL>>__all__ = [""get_path_uid"", ""stdlib_pkgs"", ""WINDOWS""]<<NEWL>><<NEWL>><<NEWL>>logger = logging.getLogger(__name__)<<NEWL>><<NEWL>><<NEWL>>def has_tls() -> bool:<<NEWL>>    try:<<NEWL>>        import _ssl  # noqa: F401  # ignore unused<<NEWL>><<NEWL>>        return True<<NEWL>>    except ImportError:<<NEWL>>        pass<<NEWL>><<NEWL>>    from pip._vendor.urllib3.util import IS_PYOPENSSL<<NEWL>><<NEWL>>    return IS_PYOPENSSL<<NEWL>><<NEWL>><<NEWL>>def get_path_uid(path: str) -> int:<<NEWL>>    """"""<<NEWL>>    Return path's uid.<<NEWL>><<NEWL>>    Does not follow symlinks:<<NEWL>>        https://github.com/pypa/pip/pull/935#discussion_r5307003<<NEWL>><<NEWL>>    Placed this function in compat due to differences on AIX and<<NEWL>>    Jython, that should eventually go away.<<NEWL>><<NEWL>>    :raises OSError: When path is a symlink or can't be read.<<NEWL>>    """"""<<NEWL>>    if hasattr(os, ""O_NOFOLLOW""):<<NEWL>>        fd = os.open(path, os.O_RDONLY | os.O_NOFOLLOW)<<NEWL>>        file_uid = os.fstat(fd).st_uid<<NEWL>>        os.close(fd)<<NEWL>>    else:  # AIX and Jython<<NEWL>>        # WARNING: time of check vulnerability, but best we can do w/o NOFOLLOW<<NEWL>>        if not os.path.islink(path):<<NEWL>>            # older versions of Jython don't have `os.fstat`<<NEWL>>            file_uid = os.stat(path).st_uid<<NEWL>>        else:<<NEWL>>            # raise OSError for parity with os.O_NOFOLLOW above<<NEWL>>            raise OSError(f""{path} is a symlink; Will not return uid for symlinks"")<<NEWL>>    return file_uid<<NEWL>><<NEWL>><<NEWL>># packages in the stdlib that may have installation metadata, but should not be<<NEWL>># considered 'installed'.  this theoretically could be determined based on<<NEWL>># dist.location (py27:`sysconfig.get_paths()['stdlib']`,<<NEWL>># py26:sysconfig.get_config_vars('LIBDEST')), but fear platform variation may<<NEWL>># make this ineffective, so hard-coding<<NEWL>>stdlib_pkgs = {""python"", ""wsgiref"", ""argparse""}<<NEWL>><<NEWL>><<NEWL>># windows detection, covers cpython and ironpython<<NEWL>>WINDOWS = sys.platform.startswith(""win"") or (sys.platform == ""cli"" and os.name == ""nt"")"
451	jackson	2	"import _plotly_utils.basevalidators<<NEWL>><<NEWL>><<NEWL>>class HoverlabelValidator(_plotly_utils.basevalidators.CompoundValidator):<<NEWL>>    def __init__(self, plotly_name=""hoverlabel"", parent_name=""contour"", **kwargs):<<NEWL>>        super(HoverlabelValidator, self).__init__(<<NEWL>>            plotly_name=plotly_name,<<NEWL>>            parent_name=parent_name,<<NEWL>>            data_class_str=kwargs.pop(""data_class_str"", ""Hoverlabel""),<<NEWL>>            data_docs=kwargs.pop(<<NEWL>>                ""data_docs"",<<NEWL>>                """"""<<NEWL>>            align<<NEWL>>                Sets the horizontal alignment of the text<<NEWL>>                content within hover label box. Has an effect<<NEWL>>                only if the hover label text spans more two or<<NEWL>>                more lines<<NEWL>>            alignsrc<<NEWL>>                Sets the source reference on Chart Studio Cloud<<NEWL>>                for `align`.<<NEWL>>            bgcolor<<NEWL>>                Sets the background color of the hover labels<<NEWL>>                for this trace<<NEWL>>            bgcolorsrc<<NEWL>>                Sets the source reference on Chart Studio Cloud<<NEWL>>                for `bgcolor`.<<NEWL>>            bordercolor<<NEWL>>                Sets the border color of the hover labels for<<NEWL>>                this trace.<<NEWL>>            bordercolorsrc<<NEWL>>                Sets the source reference on Chart Studio Cloud<<NEWL>>                for `bordercolor`.<<NEWL>>            font<<NEWL>>                Sets the font used in hover labels.<<NEWL>>            namelength<<NEWL>>                Sets the default length (in number of<<NEWL>>                characters) of the trace name in the hover<<NEWL>>                labels for all traces. -1 shows the whole name<<NEWL>>                regardless of length. 0-3 shows the first 0-3<<NEWL>>                characters, and an integer >3 will show the<<NEWL>>                whole name if it is less than that many<<NEWL>>                characters, but if it is longer, will truncate<<NEWL>>                to `namelength - 3` characters and add an<<NEWL>>                ellipsis.<<NEWL>>            namelengthsrc<<NEWL>>                Sets the source reference on Chart Studio Cloud<<NEWL>>                for `namelength`.<<NEWL>>"""""",<<NEWL>>            ),<<NEWL>>            **kwargs,<<NEWL>>        )"
451	donghui	1	"import _plotly_utils.basevalidators<<NEWL>><<NEWL>><<NEWL>>class HoverlabelValidator(_plotly_utils.basevalidators.CompoundValidator):<<NEWL>>    def __init__(self, plotly_name=""hoverlabel"", parent_name=""contour"", **kwargs):<<NEWL>>        super(HoverlabelValidator, self).__init__(<<NEWL>>            plotly_name=plotly_name,<<NEWL>>            parent_name=parent_name,<<NEWL>>            data_class_str=kwargs.pop(""data_class_str"", ""Hoverlabel""),<<NEWL>>            data_docs=kwargs.pop(<<NEWL>>                ""data_docs"",<<NEWL>>                """"""<<NEWL>>            align<<NEWL>>                Sets the horizontal alignment of the text<<NEWL>>                content within hover label box. Has an effect<<NEWL>>                only if the hover label text spans more two or<<NEWL>>                more lines<<NEWL>>            alignsrc<<NEWL>>                Sets the source reference on Chart Studio Cloud<<NEWL>>                for `align`.<<NEWL>>            bgcolor<<NEWL>>                Sets the background color of the hover labels<<NEWL>>                for this trace<<NEWL>>            bgcolorsrc<<NEWL>>                Sets the source reference on Chart Studio Cloud<<NEWL>>                for `bgcolor`.<<NEWL>>            bordercolor<<NEWL>>                Sets the border color of the hover labels for<<NEWL>>                this trace.<<NEWL>>            bordercolorsrc<<NEWL>>                Sets the source reference on Chart Studio Cloud<<NEWL>>                for `bordercolor`.<<NEWL>>            font<<NEWL>>                Sets the font used in hover labels.<<NEWL>>            namelength<<NEWL>>                Sets the default length (in number of<<NEWL>>                characters) of the trace name in the hover<<NEWL>>                labels for all traces. -1 shows the whole name<<NEWL>>                regardless of length. 0-3 shows the first 0-3<<NEWL>>                characters, and an integer >3 will show the<<NEWL>>                whole name if it is less than that many<<NEWL>>                characters, but if it is longer, will truncate<<NEWL>>                to `namelength - 3` characters and add an<<NEWL>>                ellipsis.<<NEWL>>            namelengthsrc<<NEWL>>                Sets the source reference on Chart Studio Cloud<<NEWL>>                for `namelength`.<<NEWL>>"""""",<<NEWL>>            ),<<NEWL>>            **kwargs,<<NEWL>>        )"
511	jackson	3	"#!/usr/bin/env python3<<NEWL>>""""""Session authentication with expiration<<NEWL>>and storage support module for the API.<<NEWL>>""""""<<NEWL>>from flask import request<<NEWL>>from datetime import datetime, timedelta<<NEWL>><<NEWL>>from models.user_session import UserSession<<NEWL>>from .session_exp_auth import SessionExpAuth<<NEWL>><<NEWL>><<NEWL>>class SessionDBAuth(SessionExpAuth):<<NEWL>>    """"""Session authentication class with expiration and storage support.<<NEWL>>    """"""<<NEWL>><<NEWL>>    def create_session(self, user_id=None) -> str:<<NEWL>>        """"""Creates and stores a session id for the user.<<NEWL>>        """"""<<NEWL>>        session_id = super().create_session(user_id)<<NEWL>>        if type(session_id) == str:<<NEWL>>            kwargs = {<<NEWL>>                'user_id': user_id,<<NEWL>>                'session_id': session_id,<<NEWL>>            }<<NEWL>>            user_session = UserSession(**kwargs)<<NEWL>>            user_session.save()<<NEWL>>            return session_id<<NEWL>><<NEWL>>    def user_id_for_session_id(self, session_id=None):<<NEWL>>        """"""Retrieves the user id of the user associated with<<NEWL>>        a given session id.<<NEWL>>        """"""<<NEWL>>        try:<<NEWL>>            sessions = UserSession.search({'session_id': session_id})<<NEWL>>        except Exception:<<NEWL>>            return None<<NEWL>>        if len(sessions) <= 0:<<NEWL>>            return None<<NEWL>>        cur_time = datetime.now()<<NEWL>>        time_span = timedelta(seconds=self.session_duration)<<NEWL>>        exp_time = sessions[0].created_at + time_span<<NEWL>>        if exp_time < cur_time:<<NEWL>>            return None<<NEWL>>        return sessions[0].user_id<<NEWL>><<NEWL>>    def destroy_session(self, request=None) -> bool:<<NEWL>>        """"""Destroys an authenticated session.<<NEWL>>        """"""<<NEWL>>        session_id = self.session_cookie(request)<<NEWL>>        try:<<NEWL>>            sessions = UserSession.search({'session_id': session_id})<<NEWL>>        except Exception:<<NEWL>>            return False<<NEWL>>        if len(sessions) <= 0:<<NEWL>>            return False<<NEWL>>        sessions[0].remove()<<NEWL>>        return True"
511	donghui	3	"#!/usr/bin/env python3<<NEWL>>""""""Session authentication with expiration<<NEWL>>and storage support module for the API.<<NEWL>>""""""<<NEWL>>from flask import request<<NEWL>>from datetime import datetime, timedelta<<NEWL>><<NEWL>>from models.user_session import UserSession<<NEWL>>from .session_exp_auth import SessionExpAuth<<NEWL>><<NEWL>><<NEWL>>class SessionDBAuth(SessionExpAuth):<<NEWL>>    """"""Session authentication class with expiration and storage support.<<NEWL>>    """"""<<NEWL>><<NEWL>>    def create_session(self, user_id=None) -> str:<<NEWL>>        """"""Creates and stores a session id for the user.<<NEWL>>        """"""<<NEWL>>        session_id = super().create_session(user_id)<<NEWL>>        if type(session_id) == str:<<NEWL>>            kwargs = {<<NEWL>>                'user_id': user_id,<<NEWL>>                'session_id': session_id,<<NEWL>>            }<<NEWL>>            user_session = UserSession(**kwargs)<<NEWL>>            user_session.save()<<NEWL>>            return session_id<<NEWL>><<NEWL>>    def user_id_for_session_id(self, session_id=None):<<NEWL>>        """"""Retrieves the user id of the user associated with<<NEWL>>        a given session id.<<NEWL>>        """"""<<NEWL>>        try:<<NEWL>>            sessions = UserSession.search({'session_id': session_id})<<NEWL>>        except Exception:<<NEWL>>            return None<<NEWL>>        if len(sessions) <= 0:<<NEWL>>            return None<<NEWL>>        cur_time = datetime.now()<<NEWL>>        time_span = timedelta(seconds=self.session_duration)<<NEWL>>        exp_time = sessions[0].created_at + time_span<<NEWL>>        if exp_time < cur_time:<<NEWL>>            return None<<NEWL>>        return sessions[0].user_id<<NEWL>><<NEWL>>    def destroy_session(self, request=None) -> bool:<<NEWL>>        """"""Destroys an authenticated session.<<NEWL>>        """"""<<NEWL>>        session_id = self.session_cookie(request)<<NEWL>>        try:<<NEWL>>            sessions = UserSession.search({'session_id': session_id})<<NEWL>>        except Exception:<<NEWL>>            return False<<NEWL>>        if len(sessions) <= 0:<<NEWL>>            return False<<NEWL>>        sessions[0].remove()<<NEWL>>        return True"
500	jackson	3	from MySQLdb.constants import FIELD_TYPE<<NEWL>><<NEWL>>from django.contrib.gis.gdal import OGRGeomType<<NEWL>>from django.db.backends.mysql.introspection import DatabaseIntrospection<<NEWL>><<NEWL>><<NEWL>>class MySQLIntrospection(DatabaseIntrospection):<<NEWL>>    # Updating the data_types_reverse dictionary with the appropriate<<NEWL>>    # type for Geometry fields.<<NEWL>>    data_types_reverse = DatabaseIntrospection.data_types_reverse.copy()<<NEWL>>    data_types_reverse[FIELD_TYPE.GEOMETRY] = 'GeometryField'<<NEWL>><<NEWL>>    def get_geometry_type(self, table_name, description):<<NEWL>>        with self.connection.cursor() as cursor:<<NEWL>>            # In order to get the specific geometry type of the field,<<NEWL>>            # we introspect on the table definition using `DESCRIBE`.<<NEWL>>            cursor.execute('DESCRIBE %s' %<<NEWL>>                           self.connection.ops.quote_name(table_name))<<NEWL>>            # Increment over description info until we get to the geometry<<NEWL>>            # column.<<NEWL>>            for column, typ, null, key, default, extra in cursor.fetchall():<<NEWL>>                if column == description.name:<<NEWL>>                    # Using OGRGeomType to convert from OGC name to Django field.<<NEWL>>                    # MySQL does not support 3D or SRIDs, so the field params<<NEWL>>                    # are empty.<<NEWL>>                    field_type = OGRGeomType(typ).django<<NEWL>>                    field_params = {}<<NEWL>>                    break<<NEWL>>        return field_type, field_params<<NEWL>><<NEWL>>    def supports_spatial_index(self, cursor, table_name):<<NEWL>>        # Supported with MyISAM/Aria, or InnoDB on MySQL 5.7.5+/MariaDB 10.2.2+<<NEWL>>        storage_engine = self.get_storage_engine(cursor, table_name)<<NEWL>>        if storage_engine == 'InnoDB':<<NEWL>>            return self.connection.mysql_version >= (<<NEWL>>                (10, 2, 2) if self.connection.mysql_is_mariadb else (5, 7, 5)<<NEWL>>            )<<NEWL>>        return storage_engine in ('MyISAM', 'Aria')
500	donghui	3	from MySQLdb.constants import FIELD_TYPE<<NEWL>><<NEWL>>from django.contrib.gis.gdal import OGRGeomType<<NEWL>>from django.db.backends.mysql.introspection import DatabaseIntrospection<<NEWL>><<NEWL>><<NEWL>>class MySQLIntrospection(DatabaseIntrospection):<<NEWL>>    # Updating the data_types_reverse dictionary with the appropriate<<NEWL>>    # type for Geometry fields.<<NEWL>>    data_types_reverse = DatabaseIntrospection.data_types_reverse.copy()<<NEWL>>    data_types_reverse[FIELD_TYPE.GEOMETRY] = 'GeometryField'<<NEWL>><<NEWL>>    def get_geometry_type(self, table_name, description):<<NEWL>>        with self.connection.cursor() as cursor:<<NEWL>>            # In order to get the specific geometry type of the field,<<NEWL>>            # we introspect on the table definition using `DESCRIBE`.<<NEWL>>            cursor.execute('DESCRIBE %s' %<<NEWL>>                           self.connection.ops.quote_name(table_name))<<NEWL>>            # Increment over description info until we get to the geometry<<NEWL>>            # column.<<NEWL>>            for column, typ, null, key, default, extra in cursor.fetchall():<<NEWL>>                if column == description.name:<<NEWL>>                    # Using OGRGeomType to convert from OGC name to Django field.<<NEWL>>                    # MySQL does not support 3D or SRIDs, so the field params<<NEWL>>                    # are empty.<<NEWL>>                    field_type = OGRGeomType(typ).django<<NEWL>>                    field_params = {}<<NEWL>>                    break<<NEWL>>        return field_type, field_params<<NEWL>><<NEWL>>    def supports_spatial_index(self, cursor, table_name):<<NEWL>>        # Supported with MyISAM/Aria, or InnoDB on MySQL 5.7.5+/MariaDB 10.2.2+<<NEWL>>        storage_engine = self.get_storage_engine(cursor, table_name)<<NEWL>>        if storage_engine == 'InnoDB':<<NEWL>>            return self.connection.mysql_version >= (<<NEWL>>                (10, 2, 2) if self.connection.mysql_is_mariadb else (5, 7, 5)<<NEWL>>            )<<NEWL>>        return storage_engine in ('MyISAM', 'Aria')
440	jackson	3	"from typing import List, Optional<<NEWL>><<NEWL>>from .base import BaseDistribution, BaseEnvironment, FilesystemWheel, MemoryWheel, Wheel<<NEWL>><<NEWL>>__all__ = [<<NEWL>>    ""BaseDistribution"",<<NEWL>>    ""BaseEnvironment"",<<NEWL>>    ""FilesystemWheel"",<<NEWL>>    ""MemoryWheel"",<<NEWL>>    ""Wheel"",<<NEWL>>    ""get_default_environment"",<<NEWL>>    ""get_environment"",<<NEWL>>    ""get_wheel_distribution"",<<NEWL>>]<<NEWL>><<NEWL>><<NEWL>>def get_default_environment() -> BaseEnvironment:<<NEWL>>    """"""Get the default representation for the current environment.<<NEWL>><<NEWL>>    This returns an Environment instance from the chosen backend. The default<<NEWL>>    Environment instance should be built from ``sys.path`` and may use caching<<NEWL>>    to share instance state accorss calls.<<NEWL>>    """"""<<NEWL>>    from .pkg_resources import Environment<<NEWL>><<NEWL>>    return Environment.default()<<NEWL>><<NEWL>><<NEWL>>def get_environment(paths: Optional[List[str]]) -> BaseEnvironment:<<NEWL>>    """"""Get a representation of the environment specified by ``paths``.<<NEWL>><<NEWL>>    This returns an Environment instance from the chosen backend based on the<<NEWL>>    given import paths. The backend must build a fresh instance representing<<NEWL>>    the state of installed distributions when this function is called.<<NEWL>>    """"""<<NEWL>>    from .pkg_resources import Environment<<NEWL>><<NEWL>>    return Environment.from_paths(paths)<<NEWL>><<NEWL>><<NEWL>>def get_directory_distribution(directory: str) -> BaseDistribution:<<NEWL>>    """"""Get the distribution metadata representation in the specified directory.<<NEWL>><<NEWL>>    This returns a Distribution instance from the chosen backend based on<<NEWL>>    the given on-disk ``.dist-info`` directory.<<NEWL>>    """"""<<NEWL>>    from .pkg_resources import Distribution<<NEWL>><<NEWL>>    return Distribution.from_directory(directory)<<NEWL>><<NEWL>><<NEWL>>def get_wheel_distribution(wheel: Wheel, canonical_name: str) -> BaseDistribution:<<NEWL>>    """"""Get the representation of the specified wheel's distribution metadata.<<NEWL>><<NEWL>>    This returns a Distribution instance from the chosen backend based on<<NEWL>>    the given wheel's ``.dist-info`` directory.<<NEWL>><<NEWL>>    :param canonical_name: Normalized project name of the given wheel.<<NEWL>>    """"""<<NEWL>>    from .pkg_resources import Distribution<<NEWL>><<NEWL>>    return Distribution.from_wheel(wheel, canonical_name)"
440	donghui	3	"from typing import List, Optional<<NEWL>><<NEWL>>from .base import BaseDistribution, BaseEnvironment, FilesystemWheel, MemoryWheel, Wheel<<NEWL>><<NEWL>>__all__ = [<<NEWL>>    ""BaseDistribution"",<<NEWL>>    ""BaseEnvironment"",<<NEWL>>    ""FilesystemWheel"",<<NEWL>>    ""MemoryWheel"",<<NEWL>>    ""Wheel"",<<NEWL>>    ""get_default_environment"",<<NEWL>>    ""get_environment"",<<NEWL>>    ""get_wheel_distribution"",<<NEWL>>]<<NEWL>><<NEWL>><<NEWL>>def get_default_environment() -> BaseEnvironment:<<NEWL>>    """"""Get the default representation for the current environment.<<NEWL>><<NEWL>>    This returns an Environment instance from the chosen backend. The default<<NEWL>>    Environment instance should be built from ``sys.path`` and may use caching<<NEWL>>    to share instance state accorss calls.<<NEWL>>    """"""<<NEWL>>    from .pkg_resources import Environment<<NEWL>><<NEWL>>    return Environment.default()<<NEWL>><<NEWL>><<NEWL>>def get_environment(paths: Optional[List[str]]) -> BaseEnvironment:<<NEWL>>    """"""Get a representation of the environment specified by ``paths``.<<NEWL>><<NEWL>>    This returns an Environment instance from the chosen backend based on the<<NEWL>>    given import paths. The backend must build a fresh instance representing<<NEWL>>    the state of installed distributions when this function is called.<<NEWL>>    """"""<<NEWL>>    from .pkg_resources import Environment<<NEWL>><<NEWL>>    return Environment.from_paths(paths)<<NEWL>><<NEWL>><<NEWL>>def get_directory_distribution(directory: str) -> BaseDistribution:<<NEWL>>    """"""Get the distribution metadata representation in the specified directory.<<NEWL>><<NEWL>>    This returns a Distribution instance from the chosen backend based on<<NEWL>>    the given on-disk ``.dist-info`` directory.<<NEWL>>    """"""<<NEWL>>    from .pkg_resources import Distribution<<NEWL>><<NEWL>>    return Distribution.from_directory(directory)<<NEWL>><<NEWL>><<NEWL>>def get_wheel_distribution(wheel: Wheel, canonical_name: str) -> BaseDistribution:<<NEWL>>    """"""Get the representation of the specified wheel's distribution metadata.<<NEWL>><<NEWL>>    This returns a Distribution instance from the chosen backend based on<<NEWL>>    the given wheel's ``.dist-info`` directory.<<NEWL>><<NEWL>>    :param canonical_name: Normalized project name of the given wheel.<<NEWL>>    """"""<<NEWL>>    from .pkg_resources import Distribution<<NEWL>><<NEWL>>    return Distribution.from_wheel(wheel, canonical_name)"
411	jackson	0	"from . import DefaultTable<<NEWL>>import sys<<NEWL>>import array<<NEWL>>import logging<<NEWL>><<NEWL>><<NEWL>>log = logging.getLogger(__name__)<<NEWL>><<NEWL>><<NEWL>>class table__l_o_c_a(DefaultTable.DefaultTable):<<NEWL>><<NEWL>>    dependencies = [""glyf""]<<NEWL>><<NEWL>>    def decompile(self, data, ttFont):<<NEWL>>        longFormat = ttFont[""head""].indexToLocFormat<<NEWL>>        if longFormat:<<NEWL>>            format = ""I""<<NEWL>>        else:<<NEWL>>            format = ""H""<<NEWL>>        locations = array.array(format)<<NEWL>>        locations.frombytes(data)<<NEWL>>        if sys.byteorder != ""big"":<<NEWL>>            locations.byteswap()<<NEWL>>        if not longFormat:<<NEWL>>            l = array.array(""I"")<<NEWL>>            for i in range(len(locations)):<<NEWL>>                l.append(locations[i] * 2)<<NEWL>>            locations = l<<NEWL>>        if len(locations) < (ttFont[""maxp""].numGlyphs + 1):<<NEWL>>            log.warning(<<NEWL>>                ""corrupt 'loca' table, or wrong numGlyphs in 'maxp': %d %d"",<<NEWL>>                len(locations) - 1,<<NEWL>>                ttFont[""maxp""].numGlyphs,<<NEWL>>            )<<NEWL>>        self.locations = locations<<NEWL>><<NEWL>>    def compile(self, ttFont):<<NEWL>>        try:<<NEWL>>            max_location = max(self.locations)<<NEWL>>        except AttributeError:<<NEWL>>            self.set([])<<NEWL>>            max_location = 0<<NEWL>>        if max_location < 0x20000 and all(l % 2 == 0 for l in self.locations):<<NEWL>>            locations = array.array(""H"")<<NEWL>>            for i in range(len(self.locations)):<<NEWL>>                locations.append(self.locations[i] // 2)<<NEWL>>            ttFont[""head""].indexToLocFormat = 0<<NEWL>>        else:<<NEWL>>            locations = array.array(""I"", self.locations)<<NEWL>>            ttFont[""head""].indexToLocFormat = 1<<NEWL>>        if sys.byteorder != ""big"":<<NEWL>>            locations.byteswap()<<NEWL>>        return locations.tobytes()<<NEWL>><<NEWL>>    def set(self, locations):<<NEWL>>        self.locations = array.array(""I"", locations)<<NEWL>><<NEWL>>    def toXML(self, writer, ttFont):<<NEWL>>        writer.comment(""The 'loca' table will be calculated by the compiler"")<<NEWL>>        writer.newline()<<NEWL>><<NEWL>>    def __getitem__(self, index):<<NEWL>>        return self.locations[index]<<NEWL>><<NEWL>>    def __len__(self):<<NEWL>>        return len(self.locations)"
411	donghui	0	"from . import DefaultTable<<NEWL>>import sys<<NEWL>>import array<<NEWL>>import logging<<NEWL>><<NEWL>><<NEWL>>log = logging.getLogger(__name__)<<NEWL>><<NEWL>><<NEWL>>class table__l_o_c_a(DefaultTable.DefaultTable):<<NEWL>><<NEWL>>    dependencies = [""glyf""]<<NEWL>><<NEWL>>    def decompile(self, data, ttFont):<<NEWL>>        longFormat = ttFont[""head""].indexToLocFormat<<NEWL>>        if longFormat:<<NEWL>>            format = ""I""<<NEWL>>        else:<<NEWL>>            format = ""H""<<NEWL>>        locations = array.array(format)<<NEWL>>        locations.frombytes(data)<<NEWL>>        if sys.byteorder != ""big"":<<NEWL>>            locations.byteswap()<<NEWL>>        if not longFormat:<<NEWL>>            l = array.array(""I"")<<NEWL>>            for i in range(len(locations)):<<NEWL>>                l.append(locations[i] * 2)<<NEWL>>            locations = l<<NEWL>>        if len(locations) < (ttFont[""maxp""].numGlyphs + 1):<<NEWL>>            log.warning(<<NEWL>>                ""corrupt 'loca' table, or wrong numGlyphs in 'maxp': %d %d"",<<NEWL>>                len(locations) - 1,<<NEWL>>                ttFont[""maxp""].numGlyphs,<<NEWL>>            )<<NEWL>>        self.locations = locations<<NEWL>><<NEWL>>    def compile(self, ttFont):<<NEWL>>        try:<<NEWL>>            max_location = max(self.locations)<<NEWL>>        except AttributeError:<<NEWL>>            self.set([])<<NEWL>>            max_location = 0<<NEWL>>        if max_location < 0x20000 and all(l % 2 == 0 for l in self.locations):<<NEWL>>            locations = array.array(""H"")<<NEWL>>            for i in range(len(self.locations)):<<NEWL>>                locations.append(self.locations[i] // 2)<<NEWL>>            ttFont[""head""].indexToLocFormat = 0<<NEWL>>        else:<<NEWL>>            locations = array.array(""I"", self.locations)<<NEWL>>            ttFont[""head""].indexToLocFormat = 1<<NEWL>>        if sys.byteorder != ""big"":<<NEWL>>            locations.byteswap()<<NEWL>>        return locations.tobytes()<<NEWL>><<NEWL>>    def set(self, locations):<<NEWL>>        self.locations = array.array(""I"", locations)<<NEWL>><<NEWL>>    def toXML(self, writer, ttFont):<<NEWL>>        writer.comment(""The 'loca' table will be calculated by the compiler"")<<NEWL>>        writer.newline()<<NEWL>><<NEWL>>    def __getitem__(self, index):<<NEWL>>        return self.locations[index]<<NEWL>><<NEWL>>    def __len__(self):<<NEWL>>        return len(self.locations)"
425	jackson	0	"from datetime import (<<NEWL>>    datetime,<<NEWL>>    timedelta,<<NEWL>>)<<NEWL>><<NEWL>>from pandas import (<<NEWL>>    DatetimeIndex,<<NEWL>>    NaT,<<NEWL>>    Timestamp,<<NEWL>>)<<NEWL>>import pandas._testing as tm<<NEWL>><<NEWL>><<NEWL>>def test_unique(tz_naive_fixture):<<NEWL>><<NEWL>>    idx = DatetimeIndex([""2017""] * 2, tz=tz_naive_fixture)<<NEWL>>    expected = idx[:1]<<NEWL>><<NEWL>>    result = idx.unique()<<NEWL>>    tm.assert_index_equal(result, expected)<<NEWL>>    # GH#21737<<NEWL>>    # Ensure the underlying data is consistent<<NEWL>>    assert result[0] == expected[0]<<NEWL>><<NEWL>><<NEWL>>def test_index_unique(rand_series_with_duplicate_datetimeindex):<<NEWL>>    dups = rand_series_with_duplicate_datetimeindex<<NEWL>>    index = dups.index<<NEWL>><<NEWL>>    uniques = index.unique()<<NEWL>>    expected = DatetimeIndex(<<NEWL>>        [<<NEWL>>            datetime(2000, 1, 2),<<NEWL>>            datetime(2000, 1, 3),<<NEWL>>            datetime(2000, 1, 4),<<NEWL>>            datetime(2000, 1, 5),<<NEWL>>        ]<<NEWL>>    )<<NEWL>>    assert uniques.dtype == ""M8[ns]""  # sanity<<NEWL>>    tm.assert_index_equal(uniques, expected)<<NEWL>>    assert index.nunique() == 4<<NEWL>><<NEWL>>    # GH#2563<<NEWL>>    assert isinstance(uniques, DatetimeIndex)<<NEWL>><<NEWL>>    dups_local = index.tz_localize(""US/Eastern"")<<NEWL>>    dups_local.name = ""foo""<<NEWL>>    result = dups_local.unique()<<NEWL>>    expected = DatetimeIndex(expected, name=""foo"")<<NEWL>>    expected = expected.tz_localize(""US/Eastern"")<<NEWL>>    assert result.tz is not None<<NEWL>>    assert result.name == ""foo""<<NEWL>>    tm.assert_index_equal(result, expected)<<NEWL>><<NEWL>><<NEWL>>def test_index_unique2():<<NEWL>>    # NaT, note this is excluded<<NEWL>>    arr = [1370745748 + t for t in range(20)] + [NaT.value]<<NEWL>>    idx = DatetimeIndex(arr * 3)<<NEWL>>    tm.assert_index_equal(idx.unique(), DatetimeIndex(arr))<<NEWL>>    assert idx.nunique() == 20<<NEWL>>    assert idx.nunique(dropna=False) == 21<<NEWL>><<NEWL>><<NEWL>>def test_index_unique3():<<NEWL>>    arr = [<<NEWL>>        Timestamp(""2013-06-09 02:42:28"") + timedelta(seconds=t) for t in range(20)<<NEWL>>    ] + [NaT]<<NEWL>>    idx = DatetimeIndex(arr * 3)<<NEWL>>    tm.assert_index_equal(idx.unique(), DatetimeIndex(arr))<<NEWL>>    assert idx.nunique() == 20<<NEWL>>    assert idx.nunique(dropna=False) == 21<<NEWL>><<NEWL>><<NEWL>>def test_is_unique_monotonic(rand_series_with_duplicate_datetimeindex):<<NEWL>>    index = rand_series_with_duplicate_datetimeindex.index<<NEWL>>    assert not index.is_unique"
425	donghui	1	"from datetime import (<<NEWL>>    datetime,<<NEWL>>    timedelta,<<NEWL>>)<<NEWL>><<NEWL>>from pandas import (<<NEWL>>    DatetimeIndex,<<NEWL>>    NaT,<<NEWL>>    Timestamp,<<NEWL>>)<<NEWL>>import pandas._testing as tm<<NEWL>><<NEWL>><<NEWL>>def test_unique(tz_naive_fixture):<<NEWL>><<NEWL>>    idx = DatetimeIndex([""2017""] * 2, tz=tz_naive_fixture)<<NEWL>>    expected = idx[:1]<<NEWL>><<NEWL>>    result = idx.unique()<<NEWL>>    tm.assert_index_equal(result, expected)<<NEWL>>    # GH#21737<<NEWL>>    # Ensure the underlying data is consistent<<NEWL>>    assert result[0] == expected[0]<<NEWL>><<NEWL>><<NEWL>>def test_index_unique(rand_series_with_duplicate_datetimeindex):<<NEWL>>    dups = rand_series_with_duplicate_datetimeindex<<NEWL>>    index = dups.index<<NEWL>><<NEWL>>    uniques = index.unique()<<NEWL>>    expected = DatetimeIndex(<<NEWL>>        [<<NEWL>>            datetime(2000, 1, 2),<<NEWL>>            datetime(2000, 1, 3),<<NEWL>>            datetime(2000, 1, 4),<<NEWL>>            datetime(2000, 1, 5),<<NEWL>>        ]<<NEWL>>    )<<NEWL>>    assert uniques.dtype == ""M8[ns]""  # sanity<<NEWL>>    tm.assert_index_equal(uniques, expected)<<NEWL>>    assert index.nunique() == 4<<NEWL>><<NEWL>>    # GH#2563<<NEWL>>    assert isinstance(uniques, DatetimeIndex)<<NEWL>><<NEWL>>    dups_local = index.tz_localize(""US/Eastern"")<<NEWL>>    dups_local.name = ""foo""<<NEWL>>    result = dups_local.unique()<<NEWL>>    expected = DatetimeIndex(expected, name=""foo"")<<NEWL>>    expected = expected.tz_localize(""US/Eastern"")<<NEWL>>    assert result.tz is not None<<NEWL>>    assert result.name == ""foo""<<NEWL>>    tm.assert_index_equal(result, expected)<<NEWL>><<NEWL>><<NEWL>>def test_index_unique2():<<NEWL>>    # NaT, note this is excluded<<NEWL>>    arr = [1370745748 + t for t in range(20)] + [NaT.value]<<NEWL>>    idx = DatetimeIndex(arr * 3)<<NEWL>>    tm.assert_index_equal(idx.unique(), DatetimeIndex(arr))<<NEWL>>    assert idx.nunique() == 20<<NEWL>>    assert idx.nunique(dropna=False) == 21<<NEWL>><<NEWL>><<NEWL>>def test_index_unique3():<<NEWL>>    arr = [<<NEWL>>        Timestamp(""2013-06-09 02:42:28"") + timedelta(seconds=t) for t in range(20)<<NEWL>>    ] + [NaT]<<NEWL>>    idx = DatetimeIndex(arr * 3)<<NEWL>>    tm.assert_index_equal(idx.unique(), DatetimeIndex(arr))<<NEWL>>    assert idx.nunique() == 20<<NEWL>>    assert idx.nunique(dropna=False) == 21<<NEWL>><<NEWL>><<NEWL>>def test_is_unique_monotonic(rand_series_with_duplicate_datetimeindex):<<NEWL>>    index = rand_series_with_duplicate_datetimeindex.index<<NEWL>>    assert not index.is_unique"
474	jackson	1	"# Copyright 2018-present Tellabs, Inc.<<NEWL>>#<<NEWL>># Licensed under the Apache License, Version 2.0 (the ""License"");<<NEWL>># you may not use this file except in compliance with the License.<<NEWL>># You may obtain a copy of the License at<<NEWL>>#<<NEWL>>#      http://www.apache.org/licenses/LICENSE-2.0<<NEWL>>#<<NEWL>># Unless required by applicable law or agreed to in writing, software<<NEWL>># distributed under the License is distributed on an ""AS IS"" BASIS,<<NEWL>># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.<<NEWL>># See the License for the specific language governing permissions and<<NEWL>># limitations under the License.<<NEWL>>#<<NEWL>>"""""" Tellabs vendor-specific OMCI Entities""""""<<NEWL>><<NEWL>>import inspect<<NEWL>>import structlog<<NEWL>>import sys<<NEWL>><<NEWL>>from scapy.fields import ShortField, IntField, ByteField, StrFixedLenField<<NEWL>>from voltha.extensions.omci.omci_entities import EntityClassAttribute, \<<NEWL>>    AttributeAccess, OmciNullPointer, EntityOperations, EntityClass<<NEWL>><<NEWL>>log = structlog.get_logger()<<NEWL>><<NEWL>># abbreviations<<NEWL>>ECA = EntityClassAttribute<<NEWL>>AA = AttributeAccess<<NEWL>>OP = EntityOperations<<NEWL>><<NEWL>>#################################################################################<<NEWL>># entity class lookup table from entity_class values<<NEWL>>_onu_entity_classes_name_map = dict(<<NEWL>>    inspect.getmembers(sys.modules[__name__], lambda o:<<NEWL>>    inspect.isclass(o) and issubclass(o, EntityClass) and o is not EntityClass)<<NEWL>>)<<NEWL>>_onu_custom_entity_classes = [c for c in _onu_entity_classes_name_map.itervalues()]<<NEWL>>_onu_custom_entity_id_to_class_map = dict()<<NEWL>><<NEWL>><<NEWL>>def onu_custom_me_entities():<<NEWL>>    log.info('onu_custom_me_entities')<<NEWL>><<NEWL>>    if len(_onu_custom_entity_id_to_class_map) == 0:<<NEWL>>        for entity_class in _onu_custom_entity_classes:<<NEWL>>            log.info('adding-custom-me', class_id=entity_class.class_id)<<NEWL>>            assert entity_class.class_id not in _onu_custom_entity_id_to_class_map, \<<NEWL>>                ""Class ID '{}' already exists in the class map"".format(entity_class.class_id)<<NEWL>>            _onu_custom_entity_id_to_class_map[entity_class.class_id] = entity_class<<NEWL>><<NEWL>>    log.info('onu_custom_me_entities', map=_onu_custom_entity_id_to_class_map)<<NEWL>>    return _onu_custom_entity_id_to_class_map<<NEWL>>"
474	donghui	1	"# Copyright 2018-present Tellabs, Inc.<<NEWL>>#<<NEWL>># Licensed under the Apache License, Version 2.0 (the ""License"");<<NEWL>># you may not use this file except in compliance with the License.<<NEWL>># You may obtain a copy of the License at<<NEWL>>#<<NEWL>>#      http://www.apache.org/licenses/LICENSE-2.0<<NEWL>>#<<NEWL>># Unless required by applicable law or agreed to in writing, software<<NEWL>># distributed under the License is distributed on an ""AS IS"" BASIS,<<NEWL>># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.<<NEWL>># See the License for the specific language governing permissions and<<NEWL>># limitations under the License.<<NEWL>>#<<NEWL>>"""""" Tellabs vendor-specific OMCI Entities""""""<<NEWL>><<NEWL>>import inspect<<NEWL>>import structlog<<NEWL>>import sys<<NEWL>><<NEWL>>from scapy.fields import ShortField, IntField, ByteField, StrFixedLenField<<NEWL>>from voltha.extensions.omci.omci_entities import EntityClassAttribute, \<<NEWL>>    AttributeAccess, OmciNullPointer, EntityOperations, EntityClass<<NEWL>><<NEWL>>log = structlog.get_logger()<<NEWL>><<NEWL>># abbreviations<<NEWL>>ECA = EntityClassAttribute<<NEWL>>AA = AttributeAccess<<NEWL>>OP = EntityOperations<<NEWL>><<NEWL>>#################################################################################<<NEWL>># entity class lookup table from entity_class values<<NEWL>>_onu_entity_classes_name_map = dict(<<NEWL>>    inspect.getmembers(sys.modules[__name__], lambda o:<<NEWL>>    inspect.isclass(o) and issubclass(o, EntityClass) and o is not EntityClass)<<NEWL>>)<<NEWL>>_onu_custom_entity_classes = [c for c in _onu_entity_classes_name_map.itervalues()]<<NEWL>>_onu_custom_entity_id_to_class_map = dict()<<NEWL>><<NEWL>><<NEWL>>def onu_custom_me_entities():<<NEWL>>    log.info('onu_custom_me_entities')<<NEWL>><<NEWL>>    if len(_onu_custom_entity_id_to_class_map) == 0:<<NEWL>>        for entity_class in _onu_custom_entity_classes:<<NEWL>>            log.info('adding-custom-me', class_id=entity_class.class_id)<<NEWL>>            assert entity_class.class_id not in _onu_custom_entity_id_to_class_map, \<<NEWL>>                ""Class ID '{}' already exists in the class map"".format(entity_class.class_id)<<NEWL>>            _onu_custom_entity_id_to_class_map[entity_class.class_id] = entity_class<<NEWL>><<NEWL>>    log.info('onu_custom_me_entities', map=_onu_custom_entity_id_to_class_map)<<NEWL>>    return _onu_custom_entity_id_to_class_map<<NEWL>>"
487	jackson	0	"import numpy as np<<NEWL>><<NEWL>>from pandas import (<<NEWL>>    Series,<<NEWL>>    Timestamp,<<NEWL>>    date_range,<<NEWL>>)<<NEWL>>import pandas._testing as tm<<NEWL>>from pandas.api.types import is_scalar<<NEWL>><<NEWL>><<NEWL>>class TestSeriesSearchSorted:<<NEWL>>    def test_searchsorted(self):<<NEWL>>        ser = Series([1, 2, 3])<<NEWL>><<NEWL>>        result = ser.searchsorted(1, side=""left"")<<NEWL>>        assert is_scalar(result)<<NEWL>>        assert result == 0<<NEWL>><<NEWL>>        result = ser.searchsorted(1, side=""right"")<<NEWL>>        assert is_scalar(result)<<NEWL>>        assert result == 1<<NEWL>><<NEWL>>    def test_searchsorted_numeric_dtypes_scalar(self):<<NEWL>>        ser = Series([1, 2, 90, 1000, 3e9])<<NEWL>>        res = ser.searchsorted(30)<<NEWL>>        assert is_scalar(res)<<NEWL>>        assert res == 2<<NEWL>><<NEWL>>        res = ser.searchsorted([30])<<NEWL>>        exp = np.array([2], dtype=np.intp)<<NEWL>>        tm.assert_numpy_array_equal(res, exp)<<NEWL>><<NEWL>>    def test_searchsorted_numeric_dtypes_vector(self):<<NEWL>>        ser = Series([1, 2, 90, 1000, 3e9])<<NEWL>>        res = ser.searchsorted([91, 2e6])<<NEWL>>        exp = np.array([3, 4], dtype=np.intp)<<NEWL>>        tm.assert_numpy_array_equal(res, exp)<<NEWL>><<NEWL>>    def test_searchsorted_datetime64_scalar(self):<<NEWL>>        ser = Series(date_range(""20120101"", periods=10, freq=""2D""))<<NEWL>>        val = Timestamp(""20120102"")<<NEWL>>        res = ser.searchsorted(val)<<NEWL>>        assert is_scalar(res)<<NEWL>>        assert res == 1<<NEWL>><<NEWL>>    def test_searchsorted_datetime64_scalar_mixed_timezones(self):<<NEWL>>        # GH 30086<<NEWL>>        ser = Series(date_range(""20120101"", periods=10, freq=""2D"", tz=""UTC""))<<NEWL>>        val = Timestamp(""20120102"", tz=""America/New_York"")<<NEWL>>        res = ser.searchsorted(val)<<NEWL>>        assert is_scalar(res)<<NEWL>>        assert res == 1<<NEWL>><<NEWL>>    def test_searchsorted_datetime64_list(self):<<NEWL>>        ser = Series(date_range(""20120101"", periods=10, freq=""2D""))<<NEWL>>        vals = [Timestamp(""20120102""), Timestamp(""20120104"")]<<NEWL>>        res = ser.searchsorted(vals)<<NEWL>>        exp = np.array([1, 2], dtype=np.intp)<<NEWL>>        tm.assert_numpy_array_equal(res, exp)<<NEWL>><<NEWL>>    def test_searchsorted_sorter(self):<<NEWL>>        # GH8490<<NEWL>>        ser = Series([3, 1, 2])<<NEWL>>        res = ser.searchsorted([0, 3], sorter=np.argsort(ser))<<NEWL>>        exp = np.array([0, 2], dtype=np.intp)<<NEWL>>        tm.assert_numpy_array_equal(res, exp)"
487	donghui	0	"import numpy as np<<NEWL>><<NEWL>>from pandas import (<<NEWL>>    Series,<<NEWL>>    Timestamp,<<NEWL>>    date_range,<<NEWL>>)<<NEWL>>import pandas._testing as tm<<NEWL>>from pandas.api.types import is_scalar<<NEWL>><<NEWL>><<NEWL>>class TestSeriesSearchSorted:<<NEWL>>    def test_searchsorted(self):<<NEWL>>        ser = Series([1, 2, 3])<<NEWL>><<NEWL>>        result = ser.searchsorted(1, side=""left"")<<NEWL>>        assert is_scalar(result)<<NEWL>>        assert result == 0<<NEWL>><<NEWL>>        result = ser.searchsorted(1, side=""right"")<<NEWL>>        assert is_scalar(result)<<NEWL>>        assert result == 1<<NEWL>><<NEWL>>    def test_searchsorted_numeric_dtypes_scalar(self):<<NEWL>>        ser = Series([1, 2, 90, 1000, 3e9])<<NEWL>>        res = ser.searchsorted(30)<<NEWL>>        assert is_scalar(res)<<NEWL>>        assert res == 2<<NEWL>><<NEWL>>        res = ser.searchsorted([30])<<NEWL>>        exp = np.array([2], dtype=np.intp)<<NEWL>>        tm.assert_numpy_array_equal(res, exp)<<NEWL>><<NEWL>>    def test_searchsorted_numeric_dtypes_vector(self):<<NEWL>>        ser = Series([1, 2, 90, 1000, 3e9])<<NEWL>>        res = ser.searchsorted([91, 2e6])<<NEWL>>        exp = np.array([3, 4], dtype=np.intp)<<NEWL>>        tm.assert_numpy_array_equal(res, exp)<<NEWL>><<NEWL>>    def test_searchsorted_datetime64_scalar(self):<<NEWL>>        ser = Series(date_range(""20120101"", periods=10, freq=""2D""))<<NEWL>>        val = Timestamp(""20120102"")<<NEWL>>        res = ser.searchsorted(val)<<NEWL>>        assert is_scalar(res)<<NEWL>>        assert res == 1<<NEWL>><<NEWL>>    def test_searchsorted_datetime64_scalar_mixed_timezones(self):<<NEWL>>        # GH 30086<<NEWL>>        ser = Series(date_range(""20120101"", periods=10, freq=""2D"", tz=""UTC""))<<NEWL>>        val = Timestamp(""20120102"", tz=""America/New_York"")<<NEWL>>        res = ser.searchsorted(val)<<NEWL>>        assert is_scalar(res)<<NEWL>>        assert res == 1<<NEWL>><<NEWL>>    def test_searchsorted_datetime64_list(self):<<NEWL>>        ser = Series(date_range(""20120101"", periods=10, freq=""2D""))<<NEWL>>        vals = [Timestamp(""20120102""), Timestamp(""20120104"")]<<NEWL>>        res = ser.searchsorted(vals)<<NEWL>>        exp = np.array([1, 2], dtype=np.intp)<<NEWL>>        tm.assert_numpy_array_equal(res, exp)<<NEWL>><<NEWL>>    def test_searchsorted_sorter(self):<<NEWL>>        # GH8490<<NEWL>>        ser = Series([3, 1, 2])<<NEWL>>        res = ser.searchsorted([0, 3], sorter=np.argsort(ser))<<NEWL>>        exp = np.array([0, 2], dtype=np.intp)<<NEWL>>        tm.assert_numpy_array_equal(res, exp)"
398	jackson	3	"""""""<<NEWL>>    pygments.styles.vim<<NEWL>>    ~~~~~~~~~~~~~~~~~~~<<NEWL>><<NEWL>>    A highlighting style for Pygments, inspired by vim.<<NEWL>><<NEWL>>    :copyright: Copyright 2006-2022 by the Pygments team, see AUTHORS.<<NEWL>>    :license: BSD, see LICENSE for details.<<NEWL>>""""""<<NEWL>><<NEWL>>from pygments.style import Style<<NEWL>>from pygments.token import Keyword, Name, Comment, String, Error, \<<NEWL>>     Number, Operator, Generic, Whitespace, Token<<NEWL>><<NEWL>><<NEWL>>class VimStyle(Style):<<NEWL>>    """"""<<NEWL>>    Styles somewhat like vim 7.0<<NEWL>>    """"""<<NEWL>><<NEWL>>    background_color = ""#000000""<<NEWL>>    highlight_color = ""#222222""<<NEWL>><<NEWL>>    styles = {<<NEWL>>        Token:                     ""#cccccc"",<<NEWL>>        Whitespace:                """",<<NEWL>>        Comment:                   ""#000080"",<<NEWL>>        Comment.Preproc:           """",<<NEWL>>        Comment.Special:           ""bold #cd0000"",<<NEWL>><<NEWL>>        Keyword:                   ""#cdcd00"",<<NEWL>>        Keyword.Declaration:       ""#00cd00"",<<NEWL>>        Keyword.Namespace:         ""#cd00cd"",<<NEWL>>        Keyword.Pseudo:            """",<<NEWL>>        Keyword.Type:              ""#00cd00"",<<NEWL>><<NEWL>>        Operator:                  ""#3399cc"",<<NEWL>>        Operator.Word:             ""#cdcd00"",<<NEWL>><<NEWL>>        Name:                      """",<<NEWL>>        Name.Class:                ""#00cdcd"",<<NEWL>>        Name.Builtin:              ""#cd00cd"",<<NEWL>>        Name.Exception:            ""bold #666699"",<<NEWL>>        Name.Variable:             ""#00cdcd"",<<NEWL>><<NEWL>>        String:                    ""#cd0000"",<<NEWL>>        Number:                    ""#cd00cd"",<<NEWL>><<NEWL>>        Generic.Heading:           ""bold #000080"",<<NEWL>>        Generic.Subheading:        ""bold #800080"",<<NEWL>>        Generic.Deleted:           ""#cd0000"",<<NEWL>>        Generic.Inserted:          ""#00cd00"",<<NEWL>>        Generic.Error:             ""#FF0000"",<<NEWL>>        Generic.Emph:              ""italic"",<<NEWL>>        Generic.Strong:            ""bold"",<<NEWL>>        Generic.Prompt:            ""bold #000080"",<<NEWL>>        Generic.Output:            ""#888"",<<NEWL>>        Generic.Traceback:         ""#04D"",<<NEWL>><<NEWL>>        Error:                     ""border:#FF0000""<<NEWL>>    }"
398	donghui	1	"""""""<<NEWL>>    pygments.styles.vim<<NEWL>>    ~~~~~~~~~~~~~~~~~~~<<NEWL>><<NEWL>>    A highlighting style for Pygments, inspired by vim.<<NEWL>><<NEWL>>    :copyright: Copyright 2006-2022 by the Pygments team, see AUTHORS.<<NEWL>>    :license: BSD, see LICENSE for details.<<NEWL>>""""""<<NEWL>><<NEWL>>from pygments.style import Style<<NEWL>>from pygments.token import Keyword, Name, Comment, String, Error, \<<NEWL>>     Number, Operator, Generic, Whitespace, Token<<NEWL>><<NEWL>><<NEWL>>class VimStyle(Style):<<NEWL>>    """"""<<NEWL>>    Styles somewhat like vim 7.0<<NEWL>>    """"""<<NEWL>><<NEWL>>    background_color = ""#000000""<<NEWL>>    highlight_color = ""#222222""<<NEWL>><<NEWL>>    styles = {<<NEWL>>        Token:                     ""#cccccc"",<<NEWL>>        Whitespace:                """",<<NEWL>>        Comment:                   ""#000080"",<<NEWL>>        Comment.Preproc:           """",<<NEWL>>        Comment.Special:           ""bold #cd0000"",<<NEWL>><<NEWL>>        Keyword:                   ""#cdcd00"",<<NEWL>>        Keyword.Declaration:       ""#00cd00"",<<NEWL>>        Keyword.Namespace:         ""#cd00cd"",<<NEWL>>        Keyword.Pseudo:            """",<<NEWL>>        Keyword.Type:              ""#00cd00"",<<NEWL>><<NEWL>>        Operator:                  ""#3399cc"",<<NEWL>>        Operator.Word:             ""#cdcd00"",<<NEWL>><<NEWL>>        Name:                      """",<<NEWL>>        Name.Class:                ""#00cdcd"",<<NEWL>>        Name.Builtin:              ""#cd00cd"",<<NEWL>>        Name.Exception:            ""bold #666699"",<<NEWL>>        Name.Variable:             ""#00cdcd"",<<NEWL>><<NEWL>>        String:                    ""#cd0000"",<<NEWL>>        Number:                    ""#cd00cd"",<<NEWL>><<NEWL>>        Generic.Heading:           ""bold #000080"",<<NEWL>>        Generic.Subheading:        ""bold #800080"",<<NEWL>>        Generic.Deleted:           ""#cd0000"",<<NEWL>>        Generic.Inserted:          ""#00cd00"",<<NEWL>>        Generic.Error:             ""#FF0000"",<<NEWL>>        Generic.Emph:              ""italic"",<<NEWL>>        Generic.Strong:            ""bold"",<<NEWL>>        Generic.Prompt:            ""bold #000080"",<<NEWL>>        Generic.Output:            ""#888"",<<NEWL>>        Generic.Traceback:         ""#04D"",<<NEWL>><<NEWL>>        Error:                     ""border:#FF0000""<<NEWL>>    }"
388	jackson	3	"class BaseInstanceLoader:<<NEWL>>    """"""<<NEWL>>    Base abstract implementation of instance loader.<<NEWL>>    """"""<<NEWL>><<NEWL>>    def __init__(self, resource, dataset=None):<<NEWL>>        self.resource = resource<<NEWL>>        self.dataset = dataset<<NEWL>><<NEWL>>    def get_instance(self, row):<<NEWL>>        raise NotImplementedError<<NEWL>><<NEWL>><<NEWL>>class ModelInstanceLoader(BaseInstanceLoader):<<NEWL>>    """"""<<NEWL>>    Instance loader for Django model.<<NEWL>><<NEWL>>    Lookup for model instance by ``import_id_fields``.<<NEWL>>    """"""<<NEWL>><<NEWL>>    def get_queryset(self):<<NEWL>>        return self.resource.get_queryset()<<NEWL>><<NEWL>>    def get_instance(self, row):<<NEWL>>        try:<<NEWL>>            params = {}<<NEWL>>            for key in self.resource.get_import_id_fields():<<NEWL>>                field = self.resource.fields[key]<<NEWL>>                params[field.attribute] = field.clean(row)<<NEWL>>            if params:<<NEWL>>                return self.get_queryset().get(**params)<<NEWL>>            else:<<NEWL>>                return None<<NEWL>>        except self.resource._meta.model.DoesNotExist:<<NEWL>>            return None<<NEWL>><<NEWL>><<NEWL>>class CachedInstanceLoader(ModelInstanceLoader):<<NEWL>>    """"""<<NEWL>>    Loads all possible model instances in dataset avoid hitting database for<<NEWL>>    every ``get_instance`` call.<<NEWL>><<NEWL>>    This instance loader work only when there is one ``import_id_fields``<<NEWL>>    field.<<NEWL>>    """"""<<NEWL>><<NEWL>>    def __init__(self, *args, **kwargs):<<NEWL>>        super().__init__(*args, **kwargs)<<NEWL>><<NEWL>>        pk_field_name = self.resource.get_import_id_fields()[0]<<NEWL>>        self.pk_field = self.resource.fields[pk_field_name]<<NEWL>><<NEWL>>        ids = [self.pk_field.clean(row) for row in self.dataset.dict]<<NEWL>>        qs = self.get_queryset().filter(**{<<NEWL>>            ""%s__in"" % self.pk_field.attribute: ids<<NEWL>>            })<<NEWL>><<NEWL>>        self.all_instances = {<<NEWL>>            self.pk_field.get_value(instance): instance<<NEWL>>            for instance in qs<<NEWL>>        }<<NEWL>><<NEWL>>    def get_instance(self, row):<<NEWL>>        return self.all_instances.get(self.pk_field.clean(row))"
388	donghui	2	"class BaseInstanceLoader:<<NEWL>>    """"""<<NEWL>>    Base abstract implementation of instance loader.<<NEWL>>    """"""<<NEWL>><<NEWL>>    def __init__(self, resource, dataset=None):<<NEWL>>        self.resource = resource<<NEWL>>        self.dataset = dataset<<NEWL>><<NEWL>>    def get_instance(self, row):<<NEWL>>        raise NotImplementedError<<NEWL>><<NEWL>><<NEWL>>class ModelInstanceLoader(BaseInstanceLoader):<<NEWL>>    """"""<<NEWL>>    Instance loader for Django model.<<NEWL>><<NEWL>>    Lookup for model instance by ``import_id_fields``.<<NEWL>>    """"""<<NEWL>><<NEWL>>    def get_queryset(self):<<NEWL>>        return self.resource.get_queryset()<<NEWL>><<NEWL>>    def get_instance(self, row):<<NEWL>>        try:<<NEWL>>            params = {}<<NEWL>>            for key in self.resource.get_import_id_fields():<<NEWL>>                field = self.resource.fields[key]<<NEWL>>                params[field.attribute] = field.clean(row)<<NEWL>>            if params:<<NEWL>>                return self.get_queryset().get(**params)<<NEWL>>            else:<<NEWL>>                return None<<NEWL>>        except self.resource._meta.model.DoesNotExist:<<NEWL>>            return None<<NEWL>><<NEWL>><<NEWL>>class CachedInstanceLoader(ModelInstanceLoader):<<NEWL>>    """"""<<NEWL>>    Loads all possible model instances in dataset avoid hitting database for<<NEWL>>    every ``get_instance`` call.<<NEWL>><<NEWL>>    This instance loader work only when there is one ``import_id_fields``<<NEWL>>    field.<<NEWL>>    """"""<<NEWL>><<NEWL>>    def __init__(self, *args, **kwargs):<<NEWL>>        super().__init__(*args, **kwargs)<<NEWL>><<NEWL>>        pk_field_name = self.resource.get_import_id_fields()[0]<<NEWL>>        self.pk_field = self.resource.fields[pk_field_name]<<NEWL>><<NEWL>>        ids = [self.pk_field.clean(row) for row in self.dataset.dict]<<NEWL>>        qs = self.get_queryset().filter(**{<<NEWL>>            ""%s__in"" % self.pk_field.attribute: ids<<NEWL>>            })<<NEWL>><<NEWL>>        self.all_instances = {<<NEWL>>            self.pk_field.get_value(instance): instance<<NEWL>>            for instance in qs<<NEWL>>        }<<NEWL>><<NEWL>>    def get_instance(self, row):<<NEWL>>        return self.all_instances.get(self.pk_field.clean(row))"
497	jackson	3	"""""""miscellaneous zmq_utils wrapping""""""<<NEWL>><<NEWL>># Copyright (C) PyZMQ Developers<<NEWL>># Distributed under the terms of the Modified BSD License.<<NEWL>><<NEWL>>from zmq.error import InterruptedSystemCall, _check_rc, _check_version<<NEWL>><<NEWL>>from ._cffi import ffi<<NEWL>>from ._cffi import lib as C<<NEWL>><<NEWL>><<NEWL>>def has(capability):<<NEWL>>    """"""Check for zmq capability by name (e.g. 'ipc', 'curve')<<NEWL>><<NEWL>>    .. versionadded:: libzmq-4.1<<NEWL>>    .. versionadded:: 14.1<<NEWL>>    """"""<<NEWL>>    _check_version((4, 1), 'zmq.has')<<NEWL>>    if isinstance(capability, str):<<NEWL>>        capability = capability.encode('utf8')<<NEWL>>    return bool(C.zmq_has(capability))<<NEWL>><<NEWL>><<NEWL>>def curve_keypair():<<NEWL>>    """"""generate a Z85 key pair for use with zmq.CURVE security<<NEWL>><<NEWL>>    Requires libzmq (≥ 4.0) to have been built with CURVE support.<<NEWL>><<NEWL>>    Returns<<NEWL>>    -------<<NEWL>>    (public, secret) : two bytestrings<<NEWL>>        The public and private key pair as 40 byte z85-encoded bytestrings.<<NEWL>>    """"""<<NEWL>>    _check_version((3, 2), ""curve_keypair"")<<NEWL>>    public = ffi.new('char[64]')<<NEWL>>    private = ffi.new('char[64]')<<NEWL>>    rc = C.zmq_curve_keypair(public, private)<<NEWL>>    _check_rc(rc)<<NEWL>>    return ffi.buffer(public)[:40], ffi.buffer(private)[:40]<<NEWL>><<NEWL>><<NEWL>>def curve_public(private):<<NEWL>>    """"""Compute the public key corresponding to a private key for use<<NEWL>>    with zmq.CURVE security<<NEWL>><<NEWL>>    Requires libzmq (≥ 4.2) to have been built with CURVE support.<<NEWL>><<NEWL>>    Parameters<<NEWL>>    ----------<<NEWL>>    private<<NEWL>>        The private key as a 40 byte z85-encoded bytestring<<NEWL>>    Returns<<NEWL>>    -------<<NEWL>>    bytestring<<NEWL>>        The public key as a 40 byte z85-encoded bytestring.<<NEWL>>    """"""<<NEWL>>    if isinstance(private, str):<<NEWL>>        private = private.encode('utf8')<<NEWL>>    _check_version((4, 2), ""curve_public"")<<NEWL>>    public = ffi.new('char[64]')<<NEWL>>    rc = C.zmq_curve_public(public, private)<<NEWL>>    _check_rc(rc)<<NEWL>>    return ffi.buffer(public)[:40]<<NEWL>><<NEWL>><<NEWL>>def _retry_sys_call(f, *args, **kwargs):<<NEWL>>    """"""make a call, retrying if interrupted with EINTR""""""<<NEWL>>    while True:<<NEWL>>        rc = f(*args)<<NEWL>>        try:<<NEWL>>            _check_rc(rc)<<NEWL>>        except InterruptedSystemCall:<<NEWL>>            continue<<NEWL>>        else:<<NEWL>>            break<<NEWL>><<NEWL>><<NEWL>>__all__ = ['has', 'curve_keypair', 'curve_public']"
497	donghui	3	"""""""miscellaneous zmq_utils wrapping""""""<<NEWL>><<NEWL>># Copyright (C) PyZMQ Developers<<NEWL>># Distributed under the terms of the Modified BSD License.<<NEWL>><<NEWL>>from zmq.error import InterruptedSystemCall, _check_rc, _check_version<<NEWL>><<NEWL>>from ._cffi import ffi<<NEWL>>from ._cffi import lib as C<<NEWL>><<NEWL>><<NEWL>>def has(capability):<<NEWL>>    """"""Check for zmq capability by name (e.g. 'ipc', 'curve')<<NEWL>><<NEWL>>    .. versionadded:: libzmq-4.1<<NEWL>>    .. versionadded:: 14.1<<NEWL>>    """"""<<NEWL>>    _check_version((4, 1), 'zmq.has')<<NEWL>>    if isinstance(capability, str):<<NEWL>>        capability = capability.encode('utf8')<<NEWL>>    return bool(C.zmq_has(capability))<<NEWL>><<NEWL>><<NEWL>>def curve_keypair():<<NEWL>>    """"""generate a Z85 key pair for use with zmq.CURVE security<<NEWL>><<NEWL>>    Requires libzmq (≥ 4.0) to have been built with CURVE support.<<NEWL>><<NEWL>>    Returns<<NEWL>>    -------<<NEWL>>    (public, secret) : two bytestrings<<NEWL>>        The public and private key pair as 40 byte z85-encoded bytestrings.<<NEWL>>    """"""<<NEWL>>    _check_version((3, 2), ""curve_keypair"")<<NEWL>>    public = ffi.new('char[64]')<<NEWL>>    private = ffi.new('char[64]')<<NEWL>>    rc = C.zmq_curve_keypair(public, private)<<NEWL>>    _check_rc(rc)<<NEWL>>    return ffi.buffer(public)[:40], ffi.buffer(private)[:40]<<NEWL>><<NEWL>><<NEWL>>def curve_public(private):<<NEWL>>    """"""Compute the public key corresponding to a private key for use<<NEWL>>    with zmq.CURVE security<<NEWL>><<NEWL>>    Requires libzmq (≥ 4.2) to have been built with CURVE support.<<NEWL>><<NEWL>>    Parameters<<NEWL>>    ----------<<NEWL>>    private<<NEWL>>        The private key as a 40 byte z85-encoded bytestring<<NEWL>>    Returns<<NEWL>>    -------<<NEWL>>    bytestring<<NEWL>>        The public key as a 40 byte z85-encoded bytestring.<<NEWL>>    """"""<<NEWL>>    if isinstance(private, str):<<NEWL>>        private = private.encode('utf8')<<NEWL>>    _check_version((4, 2), ""curve_public"")<<NEWL>>    public = ffi.new('char[64]')<<NEWL>>    rc = C.zmq_curve_public(public, private)<<NEWL>>    _check_rc(rc)<<NEWL>>    return ffi.buffer(public)[:40]<<NEWL>><<NEWL>><<NEWL>>def _retry_sys_call(f, *args, **kwargs):<<NEWL>>    """"""make a call, retrying if interrupted with EINTR""""""<<NEWL>>    while True:<<NEWL>>        rc = f(*args)<<NEWL>>        try:<<NEWL>>            _check_rc(rc)<<NEWL>>        except InterruptedSystemCall:<<NEWL>>            continue<<NEWL>>        else:<<NEWL>>            break<<NEWL>><<NEWL>><<NEWL>>__all__ = ['has', 'curve_keypair', 'curve_public']"
464	jackson	1	"import os<<NEWL>><<NEWL>>import pytest<<NEWL>><<NEWL>>import pandas.compat as compat<<NEWL>><<NEWL>>import pandas._testing as tm<<NEWL>><<NEWL>><<NEWL>>def test_rands():<<NEWL>>    r = tm.rands(10)<<NEWL>>    assert len(r) == 10<<NEWL>><<NEWL>><<NEWL>>def test_rands_array_1d():<<NEWL>>    arr = tm.rands_array(5, size=10)<<NEWL>>    assert arr.shape == (10,)<<NEWL>>    assert len(arr[0]) == 5<<NEWL>><<NEWL>><<NEWL>>def test_rands_array_2d():<<NEWL>>    arr = tm.rands_array(7, size=(10, 10))<<NEWL>>    assert arr.shape == (10, 10)<<NEWL>>    assert len(arr[1, 1]) == 7<<NEWL>><<NEWL>><<NEWL>>def test_numpy_err_state_is_default():<<NEWL>>    expected = {""over"": ""warn"", ""divide"": ""warn"", ""invalid"": ""warn"", ""under"": ""ignore""}<<NEWL>>    import numpy as np<<NEWL>><<NEWL>>    # The error state should be unchanged after that import.<<NEWL>>    assert np.geterr() == expected<<NEWL>><<NEWL>><<NEWL>>def test_convert_rows_list_to_csv_str():<<NEWL>>    rows_list = [""aaa"", ""bbb"", ""ccc""]<<NEWL>>    ret = tm.convert_rows_list_to_csv_str(rows_list)<<NEWL>><<NEWL>>    if compat.is_platform_windows():<<NEWL>>        expected = ""aaa\r\nbbb\r\nccc\r\n""<<NEWL>>    else:<<NEWL>>        expected = ""aaa\nbbb\nccc\n""<<NEWL>><<NEWL>>    assert ret == expected<<NEWL>><<NEWL>><<NEWL>>def test_create_temp_directory():<<NEWL>>    with tm.ensure_clean_dir() as path:<<NEWL>>        assert os.path.exists(path)<<NEWL>>        assert os.path.isdir(path)<<NEWL>>    assert not os.path.exists(path)<<NEWL>><<NEWL>><<NEWL>>@pytest.mark.parametrize(""strict_data_files"", [True, False])<<NEWL>>def test_datapath_missing(datapath):<<NEWL>>    with pytest.raises(ValueError, match=""Could not find file""):<<NEWL>>        datapath(""not_a_file"")<<NEWL>><<NEWL>><<NEWL>>def test_datapath(datapath):<<NEWL>>    args = (""io"", ""data"", ""csv"", ""iris.csv"")<<NEWL>><<NEWL>>    result = datapath(*args)<<NEWL>>    expected = os.path.join(os.path.dirname(os.path.dirname(__file__)), *args)<<NEWL>><<NEWL>>    assert result == expected<<NEWL>><<NEWL>><<NEWL>>def test_rng_context():<<NEWL>>    import numpy as np<<NEWL>><<NEWL>>    expected0 = 1.764052345967664<<NEWL>>    expected1 = 1.6243453636632417<<NEWL>><<NEWL>>    with tm.RNGContext(0):<<NEWL>>        with tm.RNGContext(1):<<NEWL>>            assert np.random.randn() == expected1<<NEWL>>        assert np.random.randn() == expected0<<NEWL>><<NEWL>><<NEWL>>def test_external_error_raised():<<NEWL>>    with tm.external_error_raised(TypeError):<<NEWL>>        raise TypeError(""Should not check this error message, so it will pass"")"
464	donghui	1	"import os<<NEWL>><<NEWL>>import pytest<<NEWL>><<NEWL>>import pandas.compat as compat<<NEWL>><<NEWL>>import pandas._testing as tm<<NEWL>><<NEWL>><<NEWL>>def test_rands():<<NEWL>>    r = tm.rands(10)<<NEWL>>    assert len(r) == 10<<NEWL>><<NEWL>><<NEWL>>def test_rands_array_1d():<<NEWL>>    arr = tm.rands_array(5, size=10)<<NEWL>>    assert arr.shape == (10,)<<NEWL>>    assert len(arr[0]) == 5<<NEWL>><<NEWL>><<NEWL>>def test_rands_array_2d():<<NEWL>>    arr = tm.rands_array(7, size=(10, 10))<<NEWL>>    assert arr.shape == (10, 10)<<NEWL>>    assert len(arr[1, 1]) == 7<<NEWL>><<NEWL>><<NEWL>>def test_numpy_err_state_is_default():<<NEWL>>    expected = {""over"": ""warn"", ""divide"": ""warn"", ""invalid"": ""warn"", ""under"": ""ignore""}<<NEWL>>    import numpy as np<<NEWL>><<NEWL>>    # The error state should be unchanged after that import.<<NEWL>>    assert np.geterr() == expected<<NEWL>><<NEWL>><<NEWL>>def test_convert_rows_list_to_csv_str():<<NEWL>>    rows_list = [""aaa"", ""bbb"", ""ccc""]<<NEWL>>    ret = tm.convert_rows_list_to_csv_str(rows_list)<<NEWL>><<NEWL>>    if compat.is_platform_windows():<<NEWL>>        expected = ""aaa\r\nbbb\r\nccc\r\n""<<NEWL>>    else:<<NEWL>>        expected = ""aaa\nbbb\nccc\n""<<NEWL>><<NEWL>>    assert ret == expected<<NEWL>><<NEWL>><<NEWL>>def test_create_temp_directory():<<NEWL>>    with tm.ensure_clean_dir() as path:<<NEWL>>        assert os.path.exists(path)<<NEWL>>        assert os.path.isdir(path)<<NEWL>>    assert not os.path.exists(path)<<NEWL>><<NEWL>><<NEWL>>@pytest.mark.parametrize(""strict_data_files"", [True, False])<<NEWL>>def test_datapath_missing(datapath):<<NEWL>>    with pytest.raises(ValueError, match=""Could not find file""):<<NEWL>>        datapath(""not_a_file"")<<NEWL>><<NEWL>><<NEWL>>def test_datapath(datapath):<<NEWL>>    args = (""io"", ""data"", ""csv"", ""iris.csv"")<<NEWL>><<NEWL>>    result = datapath(*args)<<NEWL>>    expected = os.path.join(os.path.dirname(os.path.dirname(__file__)), *args)<<NEWL>><<NEWL>>    assert result == expected<<NEWL>><<NEWL>><<NEWL>>def test_rng_context():<<NEWL>>    import numpy as np<<NEWL>><<NEWL>>    expected0 = 1.764052345967664<<NEWL>>    expected1 = 1.6243453636632417<<NEWL>><<NEWL>>    with tm.RNGContext(0):<<NEWL>>        with tm.RNGContext(1):<<NEWL>>            assert np.random.randn() == expected1<<NEWL>>        assert np.random.randn() == expected0<<NEWL>><<NEWL>><<NEWL>>def test_external_error_raised():<<NEWL>>    with tm.external_error_raised(TypeError):<<NEWL>>        raise TypeError(""Should not check this error message, so it will pass"")"
435	jackson	3	"""""""<<NEWL>>    pygments.lexers.sgf<<NEWL>>    ~~~~~~~~~~~~~~~~~~~<<NEWL>><<NEWL>>    Lexer for Smart Game Format (sgf) file format.<<NEWL>><<NEWL>>    :copyright: Copyright 2006-2022 by the Pygments team, see AUTHORS.<<NEWL>>    :license: BSD, see LICENSE for details.<<NEWL>>""""""<<NEWL>><<NEWL>>from pygments.lexer import RegexLexer, bygroups<<NEWL>>from pygments.token import Name, Literal, String, Text, Punctuation, Whitespace<<NEWL>><<NEWL>>__all__ = [""SmartGameFormatLexer""]<<NEWL>><<NEWL>><<NEWL>>class SmartGameFormatLexer(RegexLexer):<<NEWL>>    """"""<<NEWL>>    Lexer for Smart Game Format (sgf) file format.<<NEWL>><<NEWL>>    The format is used to store game records of board games for two players<<NEWL>>    (mainly Go game).<<NEWL>><<NEWL>>    .. versionadded:: 2.4<<NEWL>>    """"""<<NEWL>>    name = 'SmartGameFormat'<<NEWL>>    url = 'https://www.red-bean.com/sgf/'<<NEWL>>    aliases = ['sgf']<<NEWL>>    filenames = ['*.sgf']<<NEWL>><<NEWL>>    tokens = {<<NEWL>>        'root': [<<NEWL>>            (r'[():;]+', Punctuation),<<NEWL>>            # tokens:<<NEWL>>            (r'(A[BW]|AE|AN|AP|AR|AS|[BW]L|BM|[BW]R|[BW]S|[BW]T|CA|CH|CP|CR|'<<NEWL>>             r'DD|DM|DO|DT|EL|EV|EX|FF|FG|G[BW]|GC|GM|GN|HA|HO|ID|IP|IT|IY|KM|'<<NEWL>>             r'KO|LB|LN|LT|L|MA|MN|M|N|OB|OM|ON|OP|OT|OV|P[BW]|PC|PL|PM|RE|RG|'<<NEWL>>             r'RO|RU|SO|SC|SE|SI|SL|SO|SQ|ST|SU|SZ|T[BW]|TC|TE|TM|TR|UC|US|VW|'<<NEWL>>             r'V|[BW]|C)',<<NEWL>>             Name.Builtin),<<NEWL>>            # number:<<NEWL>>            (r'(\[)([0-9.]+)(\])',<<NEWL>>             bygroups(Punctuation, Literal.Number, Punctuation)),<<NEWL>>            # date:<<NEWL>>            (r'(\[)([0-9]{4}-[0-9]{2}-[0-9]{2})(\])',<<NEWL>>             bygroups(Punctuation, Literal.Date, Punctuation)),<<NEWL>>            # point:<<NEWL>>            (r'(\[)([a-z]{2})(\])',<<NEWL>>             bygroups(Punctuation, String, Punctuation)),<<NEWL>>            # double points:<<NEWL>>            (r'(\[)([a-z]{2})(:)([a-z]{2})(\])',<<NEWL>>             bygroups(Punctuation, String, Punctuation, String, Punctuation)),<<NEWL>><<NEWL>>            (r'(\[)([\w\s#()+,\-.:?]+)(\])',<<NEWL>>             bygroups(Punctuation, String, Punctuation)),<<NEWL>>            (r'(\[)(\s.*)(\])',<<NEWL>>             bygroups(Punctuation, Whitespace, Punctuation)),<<NEWL>>            (r'\s+', Whitespace)<<NEWL>>        ],<<NEWL>>    }"
435	donghui	2	"""""""<<NEWL>>    pygments.lexers.sgf<<NEWL>>    ~~~~~~~~~~~~~~~~~~~<<NEWL>><<NEWL>>    Lexer for Smart Game Format (sgf) file format.<<NEWL>><<NEWL>>    :copyright: Copyright 2006-2022 by the Pygments team, see AUTHORS.<<NEWL>>    :license: BSD, see LICENSE for details.<<NEWL>>""""""<<NEWL>><<NEWL>>from pygments.lexer import RegexLexer, bygroups<<NEWL>>from pygments.token import Name, Literal, String, Text, Punctuation, Whitespace<<NEWL>><<NEWL>>__all__ = [""SmartGameFormatLexer""]<<NEWL>><<NEWL>><<NEWL>>class SmartGameFormatLexer(RegexLexer):<<NEWL>>    """"""<<NEWL>>    Lexer for Smart Game Format (sgf) file format.<<NEWL>><<NEWL>>    The format is used to store game records of board games for two players<<NEWL>>    (mainly Go game).<<NEWL>><<NEWL>>    .. versionadded:: 2.4<<NEWL>>    """"""<<NEWL>>    name = 'SmartGameFormat'<<NEWL>>    url = 'https://www.red-bean.com/sgf/'<<NEWL>>    aliases = ['sgf']<<NEWL>>    filenames = ['*.sgf']<<NEWL>><<NEWL>>    tokens = {<<NEWL>>        'root': [<<NEWL>>            (r'[():;]+', Punctuation),<<NEWL>>            # tokens:<<NEWL>>            (r'(A[BW]|AE|AN|AP|AR|AS|[BW]L|BM|[BW]R|[BW]S|[BW]T|CA|CH|CP|CR|'<<NEWL>>             r'DD|DM|DO|DT|EL|EV|EX|FF|FG|G[BW]|GC|GM|GN|HA|HO|ID|IP|IT|IY|KM|'<<NEWL>>             r'KO|LB|LN|LT|L|MA|MN|M|N|OB|OM|ON|OP|OT|OV|P[BW]|PC|PL|PM|RE|RG|'<<NEWL>>             r'RO|RU|SO|SC|SE|SI|SL|SO|SQ|ST|SU|SZ|T[BW]|TC|TE|TM|TR|UC|US|VW|'<<NEWL>>             r'V|[BW]|C)',<<NEWL>>             Name.Builtin),<<NEWL>>            # number:<<NEWL>>            (r'(\[)([0-9.]+)(\])',<<NEWL>>             bygroups(Punctuation, Literal.Number, Punctuation)),<<NEWL>>            # date:<<NEWL>>            (r'(\[)([0-9]{4}-[0-9]{2}-[0-9]{2})(\])',<<NEWL>>             bygroups(Punctuation, Literal.Date, Punctuation)),<<NEWL>>            # point:<<NEWL>>            (r'(\[)([a-z]{2})(\])',<<NEWL>>             bygroups(Punctuation, String, Punctuation)),<<NEWL>>            # double points:<<NEWL>>            (r'(\[)([a-z]{2})(:)([a-z]{2})(\])',<<NEWL>>             bygroups(Punctuation, String, Punctuation, String, Punctuation)),<<NEWL>><<NEWL>>            (r'(\[)([\w\s#()+,\-.:?]+)(\])',<<NEWL>>             bygroups(Punctuation, String, Punctuation)),<<NEWL>>            (r'(\[)(\s.*)(\])',<<NEWL>>             bygroups(Punctuation, Whitespace, Punctuation)),<<NEWL>>            (r'\s+', Whitespace)<<NEWL>>        ],<<NEWL>>    }"
401	jackson	4	"import hashlib<<NEWL>>import hmac<<NEWL>>from operator import itemgetter<<NEWL>>from typing import Callable, Any, Dict<<NEWL>>from urllib.parse import parse_qsl<<NEWL>><<NEWL>><<NEWL>>def check_webapp_signature(token: str, init_data: str) -> bool:<<NEWL>>    """"""<<NEWL>>    Check incoming WebApp init data signature<<NEWL>><<NEWL>>    Source: https://core.telegram.org/bots/webapps#validating-data-received-via-the-web-app<<NEWL>><<NEWL>>    :param token:<<NEWL>>    :param init_data:<<NEWL>>    :return:<<NEWL>>    """"""<<NEWL>>    try:<<NEWL>>        parsed_data = dict(parse_qsl(init_data))<<NEWL>>    except ValueError:<<NEWL>>        # Init data is not a valid query string<<NEWL>>        return False<<NEWL>>    if ""hash"" not in parsed_data:<<NEWL>>        # Hash is not present in init data<<NEWL>>        return False<<NEWL>><<NEWL>>    hash_ = parsed_data.pop('hash')<<NEWL>>    data_check_string = ""\n"".join(<<NEWL>>        f""{k}={v}"" for k, v in sorted(parsed_data.items(), key=itemgetter(0))<<NEWL>>    )<<NEWL>>    secret_key = hmac.new(<<NEWL>>        key=b""WebAppData"", msg=token.encode(), digestmod=hashlib.sha256<<NEWL>>    )<<NEWL>>    calculated_hash = hmac.new(<<NEWL>>        key=secret_key.digest(), msg=data_check_string.encode(), digestmod=hashlib.sha256<<NEWL>>    ).hexdigest()<<NEWL>>    return calculated_hash == hash_<<NEWL>><<NEWL>><<NEWL>>def parse_init_data(init_data: str, _loads: Callable[..., Any]) -> Dict[str, Any]:<<NEWL>>    """"""<<NEWL>>    Parse WebApp init data and return it as dict<<NEWL>><<NEWL>>    :param init_data:<<NEWL>>    :param _loads:<<NEWL>>    :return:<<NEWL>>    """"""<<NEWL>>    result = {}<<NEWL>>    for key, value in parse_qsl(init_data):<<NEWL>>        if (value.startswith('[') and value.endswith(']')) or (value.startswith('{') and value.endswith('}')):<<NEWL>>            value = _loads(value)<<NEWL>>        result[key] = value<<NEWL>>    return result<<NEWL>><<NEWL>><<NEWL>>def safe_parse_webapp_init_data(token: str, init_data: str, _loads: Callable[..., Any]) -> Dict[str, Any]:<<NEWL>>    """"""<<NEWL>>    Validate WebApp init data and return it as dict<<NEWL>><<NEWL>>    :param token:<<NEWL>>    :param init_data:<<NEWL>>    :param _loads:<<NEWL>>    :return:<<NEWL>>    """"""<<NEWL>>    if check_webapp_signature(token, init_data):<<NEWL>>        return parse_init_data(init_data, _loads)<<NEWL>>    raise ValueError(""Invalid init data signature"")"
401	donghui	3	"import hashlib<<NEWL>>import hmac<<NEWL>>from operator import itemgetter<<NEWL>>from typing import Callable, Any, Dict<<NEWL>>from urllib.parse import parse_qsl<<NEWL>><<NEWL>><<NEWL>>def check_webapp_signature(token: str, init_data: str) -> bool:<<NEWL>>    """"""<<NEWL>>    Check incoming WebApp init data signature<<NEWL>><<NEWL>>    Source: https://core.telegram.org/bots/webapps#validating-data-received-via-the-web-app<<NEWL>><<NEWL>>    :param token:<<NEWL>>    :param init_data:<<NEWL>>    :return:<<NEWL>>    """"""<<NEWL>>    try:<<NEWL>>        parsed_data = dict(parse_qsl(init_data))<<NEWL>>    except ValueError:<<NEWL>>        # Init data is not a valid query string<<NEWL>>        return False<<NEWL>>    if ""hash"" not in parsed_data:<<NEWL>>        # Hash is not present in init data<<NEWL>>        return False<<NEWL>><<NEWL>>    hash_ = parsed_data.pop('hash')<<NEWL>>    data_check_string = ""\n"".join(<<NEWL>>        f""{k}={v}"" for k, v in sorted(parsed_data.items(), key=itemgetter(0))<<NEWL>>    )<<NEWL>>    secret_key = hmac.new(<<NEWL>>        key=b""WebAppData"", msg=token.encode(), digestmod=hashlib.sha256<<NEWL>>    )<<NEWL>>    calculated_hash = hmac.new(<<NEWL>>        key=secret_key.digest(), msg=data_check_string.encode(), digestmod=hashlib.sha256<<NEWL>>    ).hexdigest()<<NEWL>>    return calculated_hash == hash_<<NEWL>><<NEWL>><<NEWL>>def parse_init_data(init_data: str, _loads: Callable[..., Any]) -> Dict[str, Any]:<<NEWL>>    """"""<<NEWL>>    Parse WebApp init data and return it as dict<<NEWL>><<NEWL>>    :param init_data:<<NEWL>>    :param _loads:<<NEWL>>    :return:<<NEWL>>    """"""<<NEWL>>    result = {}<<NEWL>>    for key, value in parse_qsl(init_data):<<NEWL>>        if (value.startswith('[') and value.endswith(']')) or (value.startswith('{') and value.endswith('}')):<<NEWL>>            value = _loads(value)<<NEWL>>        result[key] = value<<NEWL>>    return result<<NEWL>><<NEWL>><<NEWL>>def safe_parse_webapp_init_data(token: str, init_data: str, _loads: Callable[..., Any]) -> Dict[str, Any]:<<NEWL>>    """"""<<NEWL>>    Validate WebApp init data and return it as dict<<NEWL>><<NEWL>>    :param token:<<NEWL>>    :param init_data:<<NEWL>>    :param _loads:<<NEWL>>    :return:<<NEWL>>    """"""<<NEWL>>    if check_webapp_signature(token, init_data):<<NEWL>>        return parse_init_data(init_data, _loads)<<NEWL>>    raise ValueError(""Invalid init data signature"")"
450	jackson	3	"# flake8: noqa<<NEWL>>import subprocess<<NEWL>>import sys<<NEWL>>import unittest<<NEWL>><<NEWL>>_import_everything = b""""""<<NEWL>># The event loop is not fork-safe, and it's easy to initialize an asyncio.Future<<NEWL>># at startup, which in turn creates the default event loop and prevents forking.<<NEWL>># Explicitly disallow the default event loop so that an error will be raised<<NEWL>># if something tries to touch it.<<NEWL>>import asyncio<<NEWL>>asyncio.set_event_loop(None)<<NEWL>><<NEWL>>import tornado.auth<<NEWL>>import tornado.autoreload<<NEWL>>import tornado.concurrent<<NEWL>>import tornado.escape<<NEWL>>import tornado.gen<<NEWL>>import tornado.http1connection<<NEWL>>import tornado.httpclient<<NEWL>>import tornado.httpserver<<NEWL>>import tornado.httputil<<NEWL>>import tornado.ioloop<<NEWL>>import tornado.iostream<<NEWL>>import tornado.locale<<NEWL>>import tornado.log<<NEWL>>import tornado.netutil<<NEWL>>import tornado.options<<NEWL>>import tornado.process<<NEWL>>import tornado.simple_httpclient<<NEWL>>import tornado.tcpserver<<NEWL>>import tornado.tcpclient<<NEWL>>import tornado.template<<NEWL>>import tornado.testing<<NEWL>>import tornado.util<<NEWL>>import tornado.web<<NEWL>>import tornado.websocket<<NEWL>>import tornado.wsgi<<NEWL>><<NEWL>>try:<<NEWL>>    import pycurl<<NEWL>>except ImportError:<<NEWL>>    pass<<NEWL>>else:<<NEWL>>    import tornado.curl_httpclient<<NEWL>>""""""<<NEWL>><<NEWL>><<NEWL>>class ImportTest(unittest.TestCase):<<NEWL>>    def test_import_everything(self):<<NEWL>>        # Test that all Tornado modules can be imported without side effects,<<NEWL>>        # specifically without initializing the default asyncio event loop.<<NEWL>>        # Since we can't tell which modules may have already beein imported<<NEWL>>        # in our process, do it in a subprocess for a clean slate.<<NEWL>>        proc = subprocess.Popen([sys.executable], stdin=subprocess.PIPE)<<NEWL>>        proc.communicate(_import_everything)<<NEWL>>        self.assertEqual(proc.returncode, 0)<<NEWL>><<NEWL>>    def test_import_aliases(self):<<NEWL>>        # Ensure we don't delete formerly-documented aliases accidentally.<<NEWL>>        import tornado.ioloop<<NEWL>>        import tornado.gen<<NEWL>>        import tornado.util<<NEWL>>        import asyncio<<NEWL>><<NEWL>>        self.assertIs(tornado.ioloop.TimeoutError, tornado.util.TimeoutError)<<NEWL>>        self.assertIs(tornado.gen.TimeoutError, tornado.util.TimeoutError)<<NEWL>>        self.assertIs(tornado.util.TimeoutError, asyncio.TimeoutError)"
450	donghui	2	"# flake8: noqa<<NEWL>>import subprocess<<NEWL>>import sys<<NEWL>>import unittest<<NEWL>><<NEWL>>_import_everything = b""""""<<NEWL>># The event loop is not fork-safe, and it's easy to initialize an asyncio.Future<<NEWL>># at startup, which in turn creates the default event loop and prevents forking.<<NEWL>># Explicitly disallow the default event loop so that an error will be raised<<NEWL>># if something tries to touch it.<<NEWL>>import asyncio<<NEWL>>asyncio.set_event_loop(None)<<NEWL>><<NEWL>>import tornado.auth<<NEWL>>import tornado.autoreload<<NEWL>>import tornado.concurrent<<NEWL>>import tornado.escape<<NEWL>>import tornado.gen<<NEWL>>import tornado.http1connection<<NEWL>>import tornado.httpclient<<NEWL>>import tornado.httpserver<<NEWL>>import tornado.httputil<<NEWL>>import tornado.ioloop<<NEWL>>import tornado.iostream<<NEWL>>import tornado.locale<<NEWL>>import tornado.log<<NEWL>>import tornado.netutil<<NEWL>>import tornado.options<<NEWL>>import tornado.process<<NEWL>>import tornado.simple_httpclient<<NEWL>>import tornado.tcpserver<<NEWL>>import tornado.tcpclient<<NEWL>>import tornado.template<<NEWL>>import tornado.testing<<NEWL>>import tornado.util<<NEWL>>import tornado.web<<NEWL>>import tornado.websocket<<NEWL>>import tornado.wsgi<<NEWL>><<NEWL>>try:<<NEWL>>    import pycurl<<NEWL>>except ImportError:<<NEWL>>    pass<<NEWL>>else:<<NEWL>>    import tornado.curl_httpclient<<NEWL>>""""""<<NEWL>><<NEWL>><<NEWL>>class ImportTest(unittest.TestCase):<<NEWL>>    def test_import_everything(self):<<NEWL>>        # Test that all Tornado modules can be imported without side effects,<<NEWL>>        # specifically without initializing the default asyncio event loop.<<NEWL>>        # Since we can't tell which modules may have already beein imported<<NEWL>>        # in our process, do it in a subprocess for a clean slate.<<NEWL>>        proc = subprocess.Popen([sys.executable], stdin=subprocess.PIPE)<<NEWL>>        proc.communicate(_import_everything)<<NEWL>>        self.assertEqual(proc.returncode, 0)<<NEWL>><<NEWL>>    def test_import_aliases(self):<<NEWL>>        # Ensure we don't delete formerly-documented aliases accidentally.<<NEWL>>        import tornado.ioloop<<NEWL>>        import tornado.gen<<NEWL>>        import tornado.util<<NEWL>>        import asyncio<<NEWL>><<NEWL>>        self.assertIs(tornado.ioloop.TimeoutError, tornado.util.TimeoutError)<<NEWL>>        self.assertIs(tornado.gen.TimeoutError, tornado.util.TimeoutError)<<NEWL>>        self.assertIs(tornado.util.TimeoutError, asyncio.TimeoutError)"
510	jackson	1	"import argparse<<NEWL>>from typing import Tuple<<NEWL>><<NEWL>><<NEWL>>def get_next_version(release_type) -> Tuple[Tuple[int, int, int], str, str]:<<NEWL>>    current_ver = find_version(""fairseq/version.txt"")<<NEWL>>    version_list = [int(x) for x in current_ver.strip(""'"").split(""."")]<<NEWL>>    major, minor, patch = version_list[0], version_list[1], version_list[2]<<NEWL>>    if release_type == ""patch"":<<NEWL>>        patch += 1<<NEWL>>    elif release_type == ""minor"":<<NEWL>>        minor += 1<<NEWL>>        patch = 0<<NEWL>>    elif release_type == ""major"":<<NEWL>>        major += 1<<NEWL>>        minor = patch = 0<<NEWL>>    else:<<NEWL>>        raise ValueError(<<NEWL>>            ""Incorrect release type specified. Acceptable types are major, minor and patch.""<<NEWL>>        )<<NEWL>><<NEWL>>    new_version_tuple = (major, minor, patch)<<NEWL>>    new_version_str = ""."".join([str(x) for x in new_version_tuple])<<NEWL>>    new_tag_str = ""v"" + new_version_str<<NEWL>>    return new_version_tuple, new_version_str, new_tag_str<<NEWL>><<NEWL>><<NEWL>>def find_version(version_file_path) -> str:<<NEWL>>    with open(version_file_path) as f:<<NEWL>>        version = f.read().strip()<<NEWL>>        return version<<NEWL>><<NEWL>><<NEWL>>def update_version(new_version_str) -> None:<<NEWL>>    """"""<<NEWL>>    given the current version, update the version to the<<NEWL>>    next version depending on the type of release.<<NEWL>>    """"""<<NEWL>><<NEWL>>    with open(""fairseq/version.txt"", ""w"") as writer:<<NEWL>>        writer.write(new_version_str)<<NEWL>><<NEWL>><<NEWL>>def main(args):<<NEWL>>    if args.release_type in [""major"", ""minor"", ""patch""]:<<NEWL>>        new_version_tuple, new_version, new_tag = get_next_version(args.release_type)<<NEWL>>    else:<<NEWL>>        raise ValueError(""Incorrect release type specified"")<<NEWL>><<NEWL>>    if args.update_version:<<NEWL>>        update_version(new_version)<<NEWL>><<NEWL>>    print(new_version, new_tag)<<NEWL>><<NEWL>><<NEWL>>if __name__ == ""__main__"":<<NEWL>>    parser = argparse.ArgumentParser(description=""Versioning utils"")<<NEWL>>    parser.add_argument(<<NEWL>>        ""--release-type"",<<NEWL>>        type=str,<<NEWL>>        required=True,<<NEWL>>        help=""type of release = major/minor/patch"",<<NEWL>>    )<<NEWL>>    parser.add_argument(<<NEWL>>        ""--update-version"",<<NEWL>>        action=""store_true"",<<NEWL>>        required=False,<<NEWL>>        help=""updates the version in fairseq/version.txt"",<<NEWL>>    )<<NEWL>><<NEWL>>    args = parser.parse_args()<<NEWL>>    main(args)"
510	donghui	1	"import argparse<<NEWL>>from typing import Tuple<<NEWL>><<NEWL>><<NEWL>>def get_next_version(release_type) -> Tuple[Tuple[int, int, int], str, str]:<<NEWL>>    current_ver = find_version(""fairseq/version.txt"")<<NEWL>>    version_list = [int(x) for x in current_ver.strip(""'"").split(""."")]<<NEWL>>    major, minor, patch = version_list[0], version_list[1], version_list[2]<<NEWL>>    if release_type == ""patch"":<<NEWL>>        patch += 1<<NEWL>>    elif release_type == ""minor"":<<NEWL>>        minor += 1<<NEWL>>        patch = 0<<NEWL>>    elif release_type == ""major"":<<NEWL>>        major += 1<<NEWL>>        minor = patch = 0<<NEWL>>    else:<<NEWL>>        raise ValueError(<<NEWL>>            ""Incorrect release type specified. Acceptable types are major, minor and patch.""<<NEWL>>        )<<NEWL>><<NEWL>>    new_version_tuple = (major, minor, patch)<<NEWL>>    new_version_str = ""."".join([str(x) for x in new_version_tuple])<<NEWL>>    new_tag_str = ""v"" + new_version_str<<NEWL>>    return new_version_tuple, new_version_str, new_tag_str<<NEWL>><<NEWL>><<NEWL>>def find_version(version_file_path) -> str:<<NEWL>>    with open(version_file_path) as f:<<NEWL>>        version = f.read().strip()<<NEWL>>        return version<<NEWL>><<NEWL>><<NEWL>>def update_version(new_version_str) -> None:<<NEWL>>    """"""<<NEWL>>    given the current version, update the version to the<<NEWL>>    next version depending on the type of release.<<NEWL>>    """"""<<NEWL>><<NEWL>>    with open(""fairseq/version.txt"", ""w"") as writer:<<NEWL>>        writer.write(new_version_str)<<NEWL>><<NEWL>><<NEWL>>def main(args):<<NEWL>>    if args.release_type in [""major"", ""minor"", ""patch""]:<<NEWL>>        new_version_tuple, new_version, new_tag = get_next_version(args.release_type)<<NEWL>>    else:<<NEWL>>        raise ValueError(""Incorrect release type specified"")<<NEWL>><<NEWL>>    if args.update_version:<<NEWL>>        update_version(new_version)<<NEWL>><<NEWL>>    print(new_version, new_tag)<<NEWL>><<NEWL>><<NEWL>>if __name__ == ""__main__"":<<NEWL>>    parser = argparse.ArgumentParser(description=""Versioning utils"")<<NEWL>>    parser.add_argument(<<NEWL>>        ""--release-type"",<<NEWL>>        type=str,<<NEWL>>        required=True,<<NEWL>>        help=""type of release = major/minor/patch"",<<NEWL>>    )<<NEWL>>    parser.add_argument(<<NEWL>>        ""--update-version"",<<NEWL>>        action=""store_true"",<<NEWL>>        required=False,<<NEWL>>        help=""updates the version in fairseq/version.txt"",<<NEWL>>    )<<NEWL>><<NEWL>>    args = parser.parse_args()<<NEWL>>    main(args)"
470	jackson	1	"from selenium.webdriver.common.by import By<<NEWL>><<NEWL>><<NEWL>>class BasePageLocators():<<NEWL>>    LOGIN_LINK = (By.CSS_SELECTOR, ""#login_link"")<<NEWL>>    LOGIN_LINK_INVALID = (By.CSS_SELECTOR, ""#login_link_inc"")  # for checking the correct error message<<NEWL>><<NEWL>><<NEWL>>class MainPageLocators():<<NEWL>>    pass<<NEWL>><<NEWL>><<NEWL>>class LoginPageLocators():<<NEWL>>    LOGIN_EMAIL = (By.CSS_SELECTOR, ""[name='login-username']"")<<NEWL>>    LOGIN_PASSWORD = (By.CSS_SELECTOR, ""[name='login-password']"")<<NEWL>>    FORGOT_PASSWORD_BUTTON = (By.CSS_SELECTOR, 'a[href*=""password-reset""]')<<NEWL>>    LOGIN_BUTTON = (By.CSS_SELECTOR, ""[name='login_submit']"")<<NEWL>>    SIGN_UP_EMAIL = (By.CSS_SELECTOR, ""[name='registration-email']"")<<NEWL>>    SIGN_UP_PASSWORD = (By.CSS_SELECTOR, ""[name='registration-password1']"")<<NEWL>>    SIGN_UP_PASSWORD_REPETITION = (By.CSS_SELECTOR, ""[name='registration-password2']"")<<NEWL>>    SIGN_UP_BUTTON = (By.CSS_SELECTOR, ""[name='registration_submit']"")<<NEWL>><<NEWL>><<NEWL>>class ProductPageLocators():<<NEWL>>    PRODUCT_NAME = (By.CSS_SELECTOR, "".product_main > h1"")<<NEWL>>    ADD_TO_BASKET_BUTTON = (By. CSS_SELECTOR, "".btn-add-to-basket"")<<NEWL>>    ADD_TO_WISHLIST_BUTTON = (By.CSS_SELECTOR, "".btn-wishlist"")<<NEWL>>    PRODUCT_GALLERY = (By.CSS_SELECTOR, ""#product_gallery"")<<NEWL>>    PRODUCT_DESCRIPTION = (By.CSS_SELECTOR, ""#product_description"")<<NEWL>>    PRICE = (By.CSS_SELECTOR, "".product_main > .price_color"")<<NEWL>>    AVAILABILITY = (By.CSS_SELECTOR, "".product_main > .availability"")<<NEWL>>    WRITE_REVIEW = (By.CSS_SELECTOR, ""#write_review"")<<NEWL>>    PRODUCT_INFO_TABLE = (By.CSS_SELECTOR, "".table-striped"")<<NEWL>>    SUCCESS_MESSAGE = (By.CSS_SELECTOR, ""#messages > .alert-success:nth-child(1)"")<<NEWL>>    NAME_OF_ADDED_PRODUCT = (By.CSS_SELECTOR, ""div.alert:nth-child(1) strong"")<<NEWL>>    TOTAL_PRICE = (By.CSS_SELECTOR, "".alertinner p strong"")<<NEWL>><<NEWL>><<NEWL>>class BasketPageLocators():<<NEWL>>    VIEW_BASKET = (By.CSS_SELECTOR, "".btn-group > a[href*='basket']"")<<NEWL>>    VIEW_BASKET_INVALID = (By.CSS_SELECTOR, "".btn-group > a[href*='basket']"")  # for checking the correct error message<<NEWL>>    EMPTY_BASKET_MESSAGE = (By.CSS_SELECTOR, ""#content_inner > p"")<<NEWL>>    FILLED_BASKET = (By.CSS_SELECTOR, "".basket-items"")"
470	donghui	1	"from selenium.webdriver.common.by import By<<NEWL>><<NEWL>><<NEWL>>class BasePageLocators():<<NEWL>>    LOGIN_LINK = (By.CSS_SELECTOR, ""#login_link"")<<NEWL>>    LOGIN_LINK_INVALID = (By.CSS_SELECTOR, ""#login_link_inc"")  # for checking the correct error message<<NEWL>><<NEWL>><<NEWL>>class MainPageLocators():<<NEWL>>    pass<<NEWL>><<NEWL>><<NEWL>>class LoginPageLocators():<<NEWL>>    LOGIN_EMAIL = (By.CSS_SELECTOR, ""[name='login-username']"")<<NEWL>>    LOGIN_PASSWORD = (By.CSS_SELECTOR, ""[name='login-password']"")<<NEWL>>    FORGOT_PASSWORD_BUTTON = (By.CSS_SELECTOR, 'a[href*=""password-reset""]')<<NEWL>>    LOGIN_BUTTON = (By.CSS_SELECTOR, ""[name='login_submit']"")<<NEWL>>    SIGN_UP_EMAIL = (By.CSS_SELECTOR, ""[name='registration-email']"")<<NEWL>>    SIGN_UP_PASSWORD = (By.CSS_SELECTOR, ""[name='registration-password1']"")<<NEWL>>    SIGN_UP_PASSWORD_REPETITION = (By.CSS_SELECTOR, ""[name='registration-password2']"")<<NEWL>>    SIGN_UP_BUTTON = (By.CSS_SELECTOR, ""[name='registration_submit']"")<<NEWL>><<NEWL>><<NEWL>>class ProductPageLocators():<<NEWL>>    PRODUCT_NAME = (By.CSS_SELECTOR, "".product_main > h1"")<<NEWL>>    ADD_TO_BASKET_BUTTON = (By. CSS_SELECTOR, "".btn-add-to-basket"")<<NEWL>>    ADD_TO_WISHLIST_BUTTON = (By.CSS_SELECTOR, "".btn-wishlist"")<<NEWL>>    PRODUCT_GALLERY = (By.CSS_SELECTOR, ""#product_gallery"")<<NEWL>>    PRODUCT_DESCRIPTION = (By.CSS_SELECTOR, ""#product_description"")<<NEWL>>    PRICE = (By.CSS_SELECTOR, "".product_main > .price_color"")<<NEWL>>    AVAILABILITY = (By.CSS_SELECTOR, "".product_main > .availability"")<<NEWL>>    WRITE_REVIEW = (By.CSS_SELECTOR, ""#write_review"")<<NEWL>>    PRODUCT_INFO_TABLE = (By.CSS_SELECTOR, "".table-striped"")<<NEWL>>    SUCCESS_MESSAGE = (By.CSS_SELECTOR, ""#messages > .alert-success:nth-child(1)"")<<NEWL>>    NAME_OF_ADDED_PRODUCT = (By.CSS_SELECTOR, ""div.alert:nth-child(1) strong"")<<NEWL>>    TOTAL_PRICE = (By.CSS_SELECTOR, "".alertinner p strong"")<<NEWL>><<NEWL>><<NEWL>>class BasketPageLocators():<<NEWL>>    VIEW_BASKET = (By.CSS_SELECTOR, "".btn-group > a[href*='basket']"")<<NEWL>>    VIEW_BASKET_INVALID = (By.CSS_SELECTOR, "".btn-group > a[href*='basket']"")  # for checking the correct error message<<NEWL>>    EMPTY_BASKET_MESSAGE = (By.CSS_SELECTOR, ""#content_inner > p"")<<NEWL>>    FILLED_BASKET = (By.CSS_SELECTOR, "".basket-items"")"
421	jackson	0	import asyncio<<NEWL>>import json<<NEWL>><<NEWL>>from openpyxl import load_workbook<<NEWL>>from rest_framework.response import Response<<NEWL>>from rest_framework.views import APIView<<NEWL>><<NEWL>>from wildberries.models import Product<<NEWL>>from wildberries.pydantic import CardPydantic<<NEWL>>from wildberries.utils import make_request<<NEWL>><<NEWL>><<NEWL>>class CardView(APIView):<<NEWL>>    @staticmethod<<NEWL>>    def get_card_info(value):<<NEWL>>        page = asyncio.run(make_request(value))<<NEWL>>        return CardView.get_objects(page, value)<<NEWL>><<NEWL>>    @staticmethod<<NEWL>>    def get_cards_info(file):<<NEWL>>        values = []<<NEWL>>        wb = load_workbook(file)<<NEWL>>        for sheet in wb.sheetnames:<<NEWL>>            for row in wb[sheet].iter_rows(values_only=True):<<NEWL>>                values.append(row[0])<<NEWL>>        cards_info = [CardView.get_card_info(i) for i in values]<<NEWL>>        return cards_info<<NEWL>><<NEWL>>    @staticmethod<<NEWL>>    def get_objects(page, value):<<NEWL>>        card = None<<NEWL>>        try:<<NEWL>>            products = json.dumps(page['data']['products'][0])<<NEWL>>            card = CardPydantic.parse_raw(products)<<NEWL>>            Product.objects.create(**card.dict())<<NEWL>>        except IndexError:<<NEWL>>            print(f'id {value} отсутствует на сайте wildberries.ru')<<NEWL>>        if card:<<NEWL>>            return card.dict()<<NEWL>>        else:<<NEWL>>            return {'error': f'id {value} отсутствует на сайте wildberries.ru'}<<NEWL>><<NEWL>>    def post(self, request, *args, **kwargs):<<NEWL>>        data = None<<NEWL>>        if 'file' in request.data and 'value' in request.data:<<NEWL>>            return Response({'error': 'Одновременно отправлять поля '<<NEWL>>                                      'file и value запрещено!'})<<NEWL>>        elif 'file' in request.data:<<NEWL>>            file = request.data['file']<<NEWL>>            data = CardView.get_cards_info(file)<<NEWL>>        elif 'value' in request.data:<<NEWL>>            value = request.data['value']<<NEWL>>            data = CardView.get_card_info(value)<<NEWL>>        return Response(data)
421	donghui	0	import asyncio<<NEWL>>import json<<NEWL>><<NEWL>>from openpyxl import load_workbook<<NEWL>>from rest_framework.response import Response<<NEWL>>from rest_framework.views import APIView<<NEWL>><<NEWL>>from wildberries.models import Product<<NEWL>>from wildberries.pydantic import CardPydantic<<NEWL>>from wildberries.utils import make_request<<NEWL>><<NEWL>><<NEWL>>class CardView(APIView):<<NEWL>>    @staticmethod<<NEWL>>    def get_card_info(value):<<NEWL>>        page = asyncio.run(make_request(value))<<NEWL>>        return CardView.get_objects(page, value)<<NEWL>><<NEWL>>    @staticmethod<<NEWL>>    def get_cards_info(file):<<NEWL>>        values = []<<NEWL>>        wb = load_workbook(file)<<NEWL>>        for sheet in wb.sheetnames:<<NEWL>>            for row in wb[sheet].iter_rows(values_only=True):<<NEWL>>                values.append(row[0])<<NEWL>>        cards_info = [CardView.get_card_info(i) for i in values]<<NEWL>>        return cards_info<<NEWL>><<NEWL>>    @staticmethod<<NEWL>>    def get_objects(page, value):<<NEWL>>        card = None<<NEWL>>        try:<<NEWL>>            products = json.dumps(page['data']['products'][0])<<NEWL>>            card = CardPydantic.parse_raw(products)<<NEWL>>            Product.objects.create(**card.dict())<<NEWL>>        except IndexError:<<NEWL>>            print(f'id {value} отсутствует на сайте wildberries.ru')<<NEWL>>        if card:<<NEWL>>            return card.dict()<<NEWL>>        else:<<NEWL>>            return {'error': f'id {value} отсутствует на сайте wildberries.ru'}<<NEWL>><<NEWL>>    def post(self, request, *args, **kwargs):<<NEWL>>        data = None<<NEWL>>        if 'file' in request.data and 'value' in request.data:<<NEWL>>            return Response({'error': 'Одновременно отправлять поля '<<NEWL>>                                      'file и value запрещено!'})<<NEWL>>        elif 'file' in request.data:<<NEWL>>            file = request.data['file']<<NEWL>>            data = CardView.get_cards_info(file)<<NEWL>>        elif 'value' in request.data:<<NEWL>>            value = request.data['value']<<NEWL>>            data = CardView.get_card_info(value)<<NEWL>>        return Response(data)
483	jackson	4	"from . import base<<NEWL>>from . import fields<<NEWL>>from . import mixins<<NEWL>>from .mask_position import MaskPosition<<NEWL>>from .photo_size import PhotoSize<<NEWL>>from .file import File<<NEWL>><<NEWL>><<NEWL>>class Sticker(base.TelegramObject, mixins.Downloadable):<<NEWL>>    """"""<<NEWL>>    This object represents a sticker.<<NEWL>><<NEWL>>    https://core.telegram.org/bots/api#sticker<<NEWL>>    """"""<<NEWL>>    file_id: base.String = fields.Field()<<NEWL>>    file_unique_id: base.String = fields.Field()<<NEWL>>    type: base.String = fields.Field()<<NEWL>>    width: base.Integer = fields.Field()<<NEWL>>    height: base.Integer = fields.Field()<<NEWL>>    is_animated: base.Boolean = fields.Field()<<NEWL>>    is_video: base.Boolean = fields.Field()<<NEWL>>    thumb: PhotoSize = fields.Field(base=PhotoSize)<<NEWL>>    emoji: base.String = fields.Field()<<NEWL>>    set_name: base.String = fields.Field()<<NEWL>>    premium_animation: File = fields.Field(base=File)<<NEWL>>    mask_position: MaskPosition = fields.Field(base=MaskPosition)<<NEWL>>    custom_emoji_id: base.String = fields.Field()<<NEWL>>    file_size: base.Integer = fields.Field()<<NEWL>><<NEWL>>    async def set_position_in_set(self, position: base.Integer) -> base.Boolean:<<NEWL>>        """"""<<NEWL>>        Use this method to move a sticker in a set created by the bot to a specific position.<<NEWL>><<NEWL>>        Source: https://core.telegram.org/bots/api#setstickerpositioninset<<NEWL>><<NEWL>>        :param position: New sticker position in the set, zero-based<<NEWL>>        :type position: :obj:`base.Integer`<<NEWL>>        :return: Returns True on success<<NEWL>>        :rtype: :obj:`base.Boolean`<<NEWL>>        """"""<<NEWL>>        return await self.bot.set_sticker_position_in_set(self.file_id, position=position)<<NEWL>><<NEWL>>    async def delete_from_set(self) -> base.Boolean:<<NEWL>>        """"""<<NEWL>>        Use this method to delete a sticker from a set created by the bot.<<NEWL>><<NEWL>>        Source: https://core.telegram.org/bots/api#deletestickerfromset<<NEWL>><<NEWL>>        :return: Returns True on success<<NEWL>>        :rtype: :obj:`base.Boolean`<<NEWL>>        """"""<<NEWL>>        return await self.bot.delete_sticker_from_set(self.file_id)"
483	donghui	3	"from . import base<<NEWL>>from . import fields<<NEWL>>from . import mixins<<NEWL>>from .mask_position import MaskPosition<<NEWL>>from .photo_size import PhotoSize<<NEWL>>from .file import File<<NEWL>><<NEWL>><<NEWL>>class Sticker(base.TelegramObject, mixins.Downloadable):<<NEWL>>    """"""<<NEWL>>    This object represents a sticker.<<NEWL>><<NEWL>>    https://core.telegram.org/bots/api#sticker<<NEWL>>    """"""<<NEWL>>    file_id: base.String = fields.Field()<<NEWL>>    file_unique_id: base.String = fields.Field()<<NEWL>>    type: base.String = fields.Field()<<NEWL>>    width: base.Integer = fields.Field()<<NEWL>>    height: base.Integer = fields.Field()<<NEWL>>    is_animated: base.Boolean = fields.Field()<<NEWL>>    is_video: base.Boolean = fields.Field()<<NEWL>>    thumb: PhotoSize = fields.Field(base=PhotoSize)<<NEWL>>    emoji: base.String = fields.Field()<<NEWL>>    set_name: base.String = fields.Field()<<NEWL>>    premium_animation: File = fields.Field(base=File)<<NEWL>>    mask_position: MaskPosition = fields.Field(base=MaskPosition)<<NEWL>>    custom_emoji_id: base.String = fields.Field()<<NEWL>>    file_size: base.Integer = fields.Field()<<NEWL>><<NEWL>>    async def set_position_in_set(self, position: base.Integer) -> base.Boolean:<<NEWL>>        """"""<<NEWL>>        Use this method to move a sticker in a set created by the bot to a specific position.<<NEWL>><<NEWL>>        Source: https://core.telegram.org/bots/api#setstickerpositioninset<<NEWL>><<NEWL>>        :param position: New sticker position in the set, zero-based<<NEWL>>        :type position: :obj:`base.Integer`<<NEWL>>        :return: Returns True on success<<NEWL>>        :rtype: :obj:`base.Boolean`<<NEWL>>        """"""<<NEWL>>        return await self.bot.set_sticker_position_in_set(self.file_id, position=position)<<NEWL>><<NEWL>>    async def delete_from_set(self) -> base.Boolean:<<NEWL>>        """"""<<NEWL>>        Use this method to delete a sticker from a set created by the bot.<<NEWL>><<NEWL>>        Source: https://core.telegram.org/bots/api#deletestickerfromset<<NEWL>><<NEWL>>        :return: Returns True on success<<NEWL>>        :rtype: :obj:`base.Boolean`<<NEWL>>        """"""<<NEWL>>        return await self.bot.delete_sticker_from_set(self.file_id)"
415	jackson	0	"import pandas as pd<<NEWL>>import numpy as np<<NEWL>>import random<<NEWL>><<NEWL>>import matplotlib.pyplot as plt<<NEWL>>import seaborn as sns<<NEWL>><<NEWL>>from plotly import graph_objs as go<<NEWL>>from plotly import express as px<<NEWL>>from plotly.subplots import make_subplots<<NEWL>><<NEWL>>import pickle<<NEWL>>import lightgbm as lgb<<NEWL>><<NEWL>><<NEWL>><<NEWL>>colorarr = ['#0592D0','#Cd7f32', '#E97451', '#Bdb76b', '#954535', '#C2b280', '#808000','#C2b280', '#E4d008', '#9acd32', '#Eedc82', '#E4d96f',<<NEWL>>           '#32cd32','#39ff14','#00ff7f', '#008080', '#36454f', '#F88379', '#Ff4500', '#Ffb347', '#A94064', '#E75480', '#Ffb6c1', '#E5e4e2',<<NEWL>>           '#Faf0e6', '#8c92ac', '#Dbd7d2','#A7a6ba', '#B38b6d']<<NEWL>><<NEWL>><<NEWL>>cropdf = pd.read_csv(""D:/Suyash College Files/Semester 6/22060 - Capstone Project Execution and Report Writing/Datasets/Crop_recommendation.csv"")<<NEWL>><<NEWL>>X = cropdf.drop('label', axis=1)<<NEWL>>y = cropdf['label']<<NEWL>><<NEWL>>from sklearn.model_selection import train_test_split<<NEWL>>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3,<<NEWL>>                                                    shuffle = True, random_state = 0)<<NEWL>><<NEWL>>model = lgb.LGBMClassifier()<<NEWL>>model.fit(X_train, y_train)<<NEWL>><<NEWL>>y_pred=model.predict(X_test)<<NEWL>><<NEWL>><<NEWL>>from sklearn.metrics import accuracy_score<<NEWL>><<NEWL>>accuracy=accuracy_score(y_pred, y_test)<<NEWL>>print('LightGBM Model accuracy score: {0:0.4f}'.format(accuracy_score(y_test, y_pred)))<<NEWL>><<NEWL>><<NEWL>>y_pred_train = model.predict(X_train)<<NEWL>>print('Training-set accuracy score: {0:0.4f}'. format(accuracy_score(y_train, y_pred_train)))<<NEWL>><<NEWL>>print('Training set score: {:.4f}'.format(model.score(X_train, y_train)))<<NEWL>>print('Test set score: {:.4f}'.format(model.score(X_test, y_test)))<<NEWL>><<NEWL>><<NEWL>>model.booster_.save_model('crop_predict1.h5')<<NEWL>><<NEWL>>        #        my_model.booster_.save_model('mode.txt')<<NEWL>>        #        #load from model:<<NEWL>>        #        #bst = lgb.Booster(model_file='mode.txt')<<NEWL>><<NEWL>><<NEWL>><<NEWL>>filename = ""trained_model.pkl""<<NEWL>>pickle.dump(model,open(filename,'wb'))<<NEWL>><<NEWL>>print(""done with all"")<<NEWL>><<NEWL>>"
415	donghui	1	"import pandas as pd<<NEWL>>import numpy as np<<NEWL>>import random<<NEWL>><<NEWL>>import matplotlib.pyplot as plt<<NEWL>>import seaborn as sns<<NEWL>><<NEWL>>from plotly import graph_objs as go<<NEWL>>from plotly import express as px<<NEWL>>from plotly.subplots import make_subplots<<NEWL>><<NEWL>>import pickle<<NEWL>>import lightgbm as lgb<<NEWL>><<NEWL>><<NEWL>><<NEWL>>colorarr = ['#0592D0','#Cd7f32', '#E97451', '#Bdb76b', '#954535', '#C2b280', '#808000','#C2b280', '#E4d008', '#9acd32', '#Eedc82', '#E4d96f',<<NEWL>>           '#32cd32','#39ff14','#00ff7f', '#008080', '#36454f', '#F88379', '#Ff4500', '#Ffb347', '#A94064', '#E75480', '#Ffb6c1', '#E5e4e2',<<NEWL>>           '#Faf0e6', '#8c92ac', '#Dbd7d2','#A7a6ba', '#B38b6d']<<NEWL>><<NEWL>><<NEWL>>cropdf = pd.read_csv(""D:/Suyash College Files/Semester 6/22060 - Capstone Project Execution and Report Writing/Datasets/Crop_recommendation.csv"")<<NEWL>><<NEWL>>X = cropdf.drop('label', axis=1)<<NEWL>>y = cropdf['label']<<NEWL>><<NEWL>>from sklearn.model_selection import train_test_split<<NEWL>>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3,<<NEWL>>                                                    shuffle = True, random_state = 0)<<NEWL>><<NEWL>>model = lgb.LGBMClassifier()<<NEWL>>model.fit(X_train, y_train)<<NEWL>><<NEWL>>y_pred=model.predict(X_test)<<NEWL>><<NEWL>><<NEWL>>from sklearn.metrics import accuracy_score<<NEWL>><<NEWL>>accuracy=accuracy_score(y_pred, y_test)<<NEWL>>print('LightGBM Model accuracy score: {0:0.4f}'.format(accuracy_score(y_test, y_pred)))<<NEWL>><<NEWL>><<NEWL>>y_pred_train = model.predict(X_train)<<NEWL>>print('Training-set accuracy score: {0:0.4f}'. format(accuracy_score(y_train, y_pred_train)))<<NEWL>><<NEWL>>print('Training set score: {:.4f}'.format(model.score(X_train, y_train)))<<NEWL>>print('Test set score: {:.4f}'.format(model.score(X_test, y_test)))<<NEWL>><<NEWL>><<NEWL>>model.booster_.save_model('crop_predict1.h5')<<NEWL>><<NEWL>>        #        my_model.booster_.save_model('mode.txt')<<NEWL>>        #        #load from model:<<NEWL>>        #        #bst = lgb.Booster(model_file='mode.txt')<<NEWL>><<NEWL>><<NEWL>><<NEWL>>filename = ""trained_model.pkl""<<NEWL>>pickle.dump(model,open(filename,'wb'))<<NEWL>><<NEWL>>print(""done with all"")<<NEWL>><<NEWL>>"
444	jackson	0	"# Licensed to the Apache Software Foundation (ASF) under one<<NEWL>># or more contributor license agreements.  See the NOTICE file<<NEWL>># distributed with this work for additional information<<NEWL>># regarding copyright ownership.  The ASF licenses this file<<NEWL>># to you under the Apache License, Version 2.0 (the<<NEWL>># ""License""); you may not use this file except in compliance<<NEWL>># with the License.  You may obtain a copy of the License at<<NEWL>>#<<NEWL>>#   http://www.apache.org/licenses/LICENSE-2.0<<NEWL>>#<<NEWL>># Unless required by applicable law or agreed to in writing,<<NEWL>># software distributed under the License is distributed on an<<NEWL>># ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY<<NEWL>># KIND, either express or implied.  See the License for the<<NEWL>># specific language governing permissions and limitations<<NEWL>># under the License.<<NEWL>><<NEWL>>from __future__ import absolute_import<<NEWL>><<NEWL>>import cffi<<NEWL>><<NEWL>>c_source = """"""<<NEWL>>    struct ArrowSchema {<<NEWL>>      // Array type description<<NEWL>>      const char* format;<<NEWL>>      const char* name;<<NEWL>>      const char* metadata;<<NEWL>>      int64_t flags;<<NEWL>>      int64_t n_children;<<NEWL>>      struct ArrowSchema** children;<<NEWL>>      struct ArrowSchema* dictionary;<<NEWL>><<NEWL>>      // Release callback<<NEWL>>      void (*release)(struct ArrowSchema*);<<NEWL>>      // Opaque producer-specific data<<NEWL>>      void* private_data;<<NEWL>>    };<<NEWL>><<NEWL>>    struct ArrowArray {<<NEWL>>      // Array data description<<NEWL>>      int64_t length;<<NEWL>>      int64_t null_count;<<NEWL>>      int64_t offset;<<NEWL>>      int64_t n_buffers;<<NEWL>>      int64_t n_children;<<NEWL>>      const void** buffers;<<NEWL>>      struct ArrowArray** children;<<NEWL>>      struct ArrowArray* dictionary;<<NEWL>><<NEWL>>      // Release callback<<NEWL>>      void (*release)(struct ArrowArray*);<<NEWL>>      // Opaque producer-specific data<<NEWL>>      void* private_data;<<NEWL>>    };<<NEWL>><<NEWL>>    struct ArrowArrayStream {<<NEWL>>      int (*get_schema)(struct ArrowArrayStream*, struct ArrowSchema* out);<<NEWL>>      int (*get_next)(struct ArrowArrayStream*, struct ArrowArray* out);<<NEWL>><<NEWL>>      const char* (*get_last_error)(struct ArrowArrayStream*);<<NEWL>><<NEWL>>      // Release callback<<NEWL>>      void (*release)(struct ArrowArrayStream*);<<NEWL>>      // Opaque producer-specific data<<NEWL>>      void* private_data;<<NEWL>>    };<<NEWL>>    """"""<<NEWL>><<NEWL>># TODO use out-of-line mode for faster import and avoid C parsing<<NEWL>>ffi = cffi.FFI()<<NEWL>>ffi.cdef(c_source)"
444	donghui	1	"# Licensed to the Apache Software Foundation (ASF) under one<<NEWL>># or more contributor license agreements.  See the NOTICE file<<NEWL>># distributed with this work for additional information<<NEWL>># regarding copyright ownership.  The ASF licenses this file<<NEWL>># to you under the Apache License, Version 2.0 (the<<NEWL>># ""License""); you may not use this file except in compliance<<NEWL>># with the License.  You may obtain a copy of the License at<<NEWL>>#<<NEWL>>#   http://www.apache.org/licenses/LICENSE-2.0<<NEWL>>#<<NEWL>># Unless required by applicable law or agreed to in writing,<<NEWL>># software distributed under the License is distributed on an<<NEWL>># ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY<<NEWL>># KIND, either express or implied.  See the License for the<<NEWL>># specific language governing permissions and limitations<<NEWL>># under the License.<<NEWL>><<NEWL>>from __future__ import absolute_import<<NEWL>><<NEWL>>import cffi<<NEWL>><<NEWL>>c_source = """"""<<NEWL>>    struct ArrowSchema {<<NEWL>>      // Array type description<<NEWL>>      const char* format;<<NEWL>>      const char* name;<<NEWL>>      const char* metadata;<<NEWL>>      int64_t flags;<<NEWL>>      int64_t n_children;<<NEWL>>      struct ArrowSchema** children;<<NEWL>>      struct ArrowSchema* dictionary;<<NEWL>><<NEWL>>      // Release callback<<NEWL>>      void (*release)(struct ArrowSchema*);<<NEWL>>      // Opaque producer-specific data<<NEWL>>      void* private_data;<<NEWL>>    };<<NEWL>><<NEWL>>    struct ArrowArray {<<NEWL>>      // Array data description<<NEWL>>      int64_t length;<<NEWL>>      int64_t null_count;<<NEWL>>      int64_t offset;<<NEWL>>      int64_t n_buffers;<<NEWL>>      int64_t n_children;<<NEWL>>      const void** buffers;<<NEWL>>      struct ArrowArray** children;<<NEWL>>      struct ArrowArray* dictionary;<<NEWL>><<NEWL>>      // Release callback<<NEWL>>      void (*release)(struct ArrowArray*);<<NEWL>>      // Opaque producer-specific data<<NEWL>>      void* private_data;<<NEWL>>    };<<NEWL>><<NEWL>>    struct ArrowArrayStream {<<NEWL>>      int (*get_schema)(struct ArrowArrayStream*, struct ArrowSchema* out);<<NEWL>>      int (*get_next)(struct ArrowArrayStream*, struct ArrowArray* out);<<NEWL>><<NEWL>>      const char* (*get_last_error)(struct ArrowArrayStream*);<<NEWL>><<NEWL>>      // Release callback<<NEWL>>      void (*release)(struct ArrowArrayStream*);<<NEWL>>      // Opaque producer-specific data<<NEWL>>      void* private_data;<<NEWL>>    };<<NEWL>>    """"""<<NEWL>><<NEWL>># TODO use out-of-line mode for faster import and avoid C parsing<<NEWL>>ffi = cffi.FFI()<<NEWL>>ffi.cdef(c_source)"
454	jackson	1	"""""""<<NEWL>>views.py        # Houses `SchemaView`, `APIView` subclass.<<NEWL>><<NEWL>>See schemas.__init__.py for package overview.<<NEWL>>""""""<<NEWL>>from rest_framework import exceptions, renderers<<NEWL>>from rest_framework.response import Response<<NEWL>>from rest_framework.schemas import coreapi<<NEWL>>from rest_framework.settings import api_settings<<NEWL>>from rest_framework.views import APIView<<NEWL>><<NEWL>><<NEWL>>class SchemaView(APIView):<<NEWL>>    _ignore_model_permissions = True<<NEWL>>    schema = None  # exclude from schema<<NEWL>>    renderer_classes = None<<NEWL>>    schema_generator = None<<NEWL>>    public = False<<NEWL>><<NEWL>>    def __init__(self, *args, **kwargs):<<NEWL>>        super().__init__(*args, **kwargs)<<NEWL>>        if self.renderer_classes is None:<<NEWL>>            if coreapi.is_enabled():<<NEWL>>                self.renderer_classes = [<<NEWL>>                    renderers.CoreAPIOpenAPIRenderer,<<NEWL>>                    renderers.CoreJSONRenderer<<NEWL>>                ]<<NEWL>>            else:<<NEWL>>                self.renderer_classes = [<<NEWL>>                    renderers.OpenAPIRenderer,<<NEWL>>                    renderers.JSONOpenAPIRenderer,<<NEWL>>                ]<<NEWL>>            if renderers.BrowsableAPIRenderer in api_settings.DEFAULT_RENDERER_CLASSES:<<NEWL>>                self.renderer_classes += [renderers.BrowsableAPIRenderer]<<NEWL>><<NEWL>>    def get(self, request, *args, **kwargs):<<NEWL>>        schema = self.schema_generator.get_schema(request, self.public)<<NEWL>>        if schema is None:<<NEWL>>            raise exceptions.PermissionDenied()<<NEWL>>        return Response(schema)<<NEWL>><<NEWL>>    def handle_exception(self, exc):<<NEWL>>        # Schema renderers do not render exceptions, so re-perform content<<NEWL>>        # negotiation with default renderers.<<NEWL>>        self.renderer_classes = api_settings.DEFAULT_RENDERER_CLASSES<<NEWL>>        neg = self.perform_content_negotiation(self.request, force=True)<<NEWL>>        self.request.accepted_renderer, self.request.accepted_media_type = neg<<NEWL>>        return super().handle_exception(exc)"
454	donghui	1	"""""""<<NEWL>>views.py        # Houses `SchemaView`, `APIView` subclass.<<NEWL>><<NEWL>>See schemas.__init__.py for package overview.<<NEWL>>""""""<<NEWL>>from rest_framework import exceptions, renderers<<NEWL>>from rest_framework.response import Response<<NEWL>>from rest_framework.schemas import coreapi<<NEWL>>from rest_framework.settings import api_settings<<NEWL>>from rest_framework.views import APIView<<NEWL>><<NEWL>><<NEWL>>class SchemaView(APIView):<<NEWL>>    _ignore_model_permissions = True<<NEWL>>    schema = None  # exclude from schema<<NEWL>>    renderer_classes = None<<NEWL>>    schema_generator = None<<NEWL>>    public = False<<NEWL>><<NEWL>>    def __init__(self, *args, **kwargs):<<NEWL>>        super().__init__(*args, **kwargs)<<NEWL>>        if self.renderer_classes is None:<<NEWL>>            if coreapi.is_enabled():<<NEWL>>                self.renderer_classes = [<<NEWL>>                    renderers.CoreAPIOpenAPIRenderer,<<NEWL>>                    renderers.CoreJSONRenderer<<NEWL>>                ]<<NEWL>>            else:<<NEWL>>                self.renderer_classes = [<<NEWL>>                    renderers.OpenAPIRenderer,<<NEWL>>                    renderers.JSONOpenAPIRenderer,<<NEWL>>                ]<<NEWL>>            if renderers.BrowsableAPIRenderer in api_settings.DEFAULT_RENDERER_CLASSES:<<NEWL>>                self.renderer_classes += [renderers.BrowsableAPIRenderer]<<NEWL>><<NEWL>>    def get(self, request, *args, **kwargs):<<NEWL>>        schema = self.schema_generator.get_schema(request, self.public)<<NEWL>>        if schema is None:<<NEWL>>            raise exceptions.PermissionDenied()<<NEWL>>        return Response(schema)<<NEWL>><<NEWL>>    def handle_exception(self, exc):<<NEWL>>        # Schema renderers do not render exceptions, so re-perform content<<NEWL>>        # negotiation with default renderers.<<NEWL>>        self.renderer_classes = api_settings.DEFAULT_RENDERER_CLASSES<<NEWL>>        neg = self.perform_content_negotiation(self.request, force=True)<<NEWL>>        self.request.accepted_renderer, self.request.accepted_media_type = neg<<NEWL>>        return super().handle_exception(exc)"
514	jackson	3	"""""""<<NEWL>> The GeometryColumns and SpatialRefSys models for the SpatiaLite backend.<<NEWL>>""""""<<NEWL>>from django.contrib.gis.db.backends.base.models import SpatialRefSysMixin<<NEWL>>from django.db import models<<NEWL>><<NEWL>><<NEWL>>class SpatialiteGeometryColumns(models.Model):<<NEWL>>    """"""<<NEWL>>    The 'geometry_columns' table from SpatiaLite.<<NEWL>>    """"""<<NEWL>>    f_table_name = models.CharField(max_length=256)<<NEWL>>    f_geometry_column = models.CharField(max_length=256)<<NEWL>>    coord_dimension = models.IntegerField()<<NEWL>>    srid = models.IntegerField(primary_key=True)<<NEWL>>    spatial_index_enabled = models.IntegerField()<<NEWL>>    type = models.IntegerField(db_column='geometry_type')<<NEWL>><<NEWL>>    class Meta:<<NEWL>>        app_label = 'gis'<<NEWL>>        db_table = 'geometry_columns'<<NEWL>>        managed = False<<NEWL>><<NEWL>>    def __str__(self):<<NEWL>>        return '%s.%s - %dD %s field (SRID: %d)' % (<<NEWL>>            self.f_table_name,<<NEWL>>            self.f_geometry_column,<<NEWL>>            self.coord_dimension,<<NEWL>>            self.type,<<NEWL>>            self.srid,<<NEWL>>        )<<NEWL>><<NEWL>>    @classmethod<<NEWL>>    def table_name_col(cls):<<NEWL>>        """"""<<NEWL>>        Return the name of the metadata column used to store the feature table<<NEWL>>        name.<<NEWL>>        """"""<<NEWL>>        return 'f_table_name'<<NEWL>><<NEWL>>    @classmethod<<NEWL>>    def geom_col_name(cls):<<NEWL>>        """"""<<NEWL>>        Return the name of the metadata column used to store the feature<<NEWL>>        geometry column.<<NEWL>>        """"""<<NEWL>>        return 'f_geometry_column'<<NEWL>><<NEWL>><<NEWL>>class SpatialiteSpatialRefSys(models.Model, SpatialRefSysMixin):<<NEWL>>    """"""<<NEWL>>    The 'spatial_ref_sys' table from SpatiaLite.<<NEWL>>    """"""<<NEWL>>    srid = models.IntegerField(primary_key=True)<<NEWL>>    auth_name = models.CharField(max_length=256)<<NEWL>>    auth_srid = models.IntegerField()<<NEWL>>    ref_sys_name = models.CharField(max_length=256)<<NEWL>>    proj4text = models.CharField(max_length=2048)<<NEWL>>    srtext = models.CharField(max_length=2048)<<NEWL>><<NEWL>>    class Meta:<<NEWL>>        app_label = 'gis'<<NEWL>>        db_table = 'spatial_ref_sys'<<NEWL>>        managed = False<<NEWL>><<NEWL>>    @property<<NEWL>>    def wkt(self):<<NEWL>>        return self.srtext"
514	donghui	3	"""""""<<NEWL>> The GeometryColumns and SpatialRefSys models for the SpatiaLite backend.<<NEWL>>""""""<<NEWL>>from django.contrib.gis.db.backends.base.models import SpatialRefSysMixin<<NEWL>>from django.db import models<<NEWL>><<NEWL>><<NEWL>>class SpatialiteGeometryColumns(models.Model):<<NEWL>>    """"""<<NEWL>>    The 'geometry_columns' table from SpatiaLite.<<NEWL>>    """"""<<NEWL>>    f_table_name = models.CharField(max_length=256)<<NEWL>>    f_geometry_column = models.CharField(max_length=256)<<NEWL>>    coord_dimension = models.IntegerField()<<NEWL>>    srid = models.IntegerField(primary_key=True)<<NEWL>>    spatial_index_enabled = models.IntegerField()<<NEWL>>    type = models.IntegerField(db_column='geometry_type')<<NEWL>><<NEWL>>    class Meta:<<NEWL>>        app_label = 'gis'<<NEWL>>        db_table = 'geometry_columns'<<NEWL>>        managed = False<<NEWL>><<NEWL>>    def __str__(self):<<NEWL>>        return '%s.%s - %dD %s field (SRID: %d)' % (<<NEWL>>            self.f_table_name,<<NEWL>>            self.f_geometry_column,<<NEWL>>            self.coord_dimension,<<NEWL>>            self.type,<<NEWL>>            self.srid,<<NEWL>>        )<<NEWL>><<NEWL>>    @classmethod<<NEWL>>    def table_name_col(cls):<<NEWL>>        """"""<<NEWL>>        Return the name of the metadata column used to store the feature table<<NEWL>>        name.<<NEWL>>        """"""<<NEWL>>        return 'f_table_name'<<NEWL>><<NEWL>>    @classmethod<<NEWL>>    def geom_col_name(cls):<<NEWL>>        """"""<<NEWL>>        Return the name of the metadata column used to store the feature<<NEWL>>        geometry column.<<NEWL>>        """"""<<NEWL>>        return 'f_geometry_column'<<NEWL>><<NEWL>><<NEWL>>class SpatialiteSpatialRefSys(models.Model, SpatialRefSysMixin):<<NEWL>>    """"""<<NEWL>>    The 'spatial_ref_sys' table from SpatiaLite.<<NEWL>>    """"""<<NEWL>>    srid = models.IntegerField(primary_key=True)<<NEWL>>    auth_name = models.CharField(max_length=256)<<NEWL>>    auth_srid = models.IntegerField()<<NEWL>>    ref_sys_name = models.CharField(max_length=256)<<NEWL>>    proj4text = models.CharField(max_length=2048)<<NEWL>>    srtext = models.CharField(max_length=2048)<<NEWL>><<NEWL>>    class Meta:<<NEWL>>        app_label = 'gis'<<NEWL>>        db_table = 'spatial_ref_sys'<<NEWL>>        managed = False<<NEWL>><<NEWL>>    @property<<NEWL>>    def wkt(self):<<NEWL>>        return self.srtext"
405	jackson	4	"from itertools import filterfalse<<NEWL>><<NEWL>><<NEWL>>def unique_everseen(iterable, key=None):<<NEWL>>    ""List unique elements, preserving order. Remember all elements ever seen.""<<NEWL>>    # unique_everseen('AAAABBBCCDAABBB') --> A B C D<<NEWL>>    # unique_everseen('ABBCcAD', str.lower) --> A B C D<<NEWL>>    seen = set()<<NEWL>>    seen_add = seen.add<<NEWL>>    if key is None:<<NEWL>>        for element in filterfalse(seen.__contains__, iterable):<<NEWL>>            seen_add(element)<<NEWL>>            yield element<<NEWL>>    else:<<NEWL>>        for element in iterable:<<NEWL>>            k = key(element)<<NEWL>>            if k not in seen:<<NEWL>>                seen_add(k)<<NEWL>>                yield element<<NEWL>><<NEWL>><<NEWL>># copied from more_itertools 8.8<<NEWL>>def always_iterable(obj, base_type=(str, bytes)):<<NEWL>>    """"""If *obj* is iterable, return an iterator over its items::<<NEWL>><<NEWL>>        >>> obj = (1, 2, 3)<<NEWL>>        >>> list(always_iterable(obj))<<NEWL>>        [1, 2, 3]<<NEWL>><<NEWL>>    If *obj* is not iterable, return a one-item iterable containing *obj*::<<NEWL>><<NEWL>>        >>> obj = 1<<NEWL>>        >>> list(always_iterable(obj))<<NEWL>>        [1]<<NEWL>><<NEWL>>    If *obj* is ``None``, return an empty iterable:<<NEWL>><<NEWL>>        >>> obj = None<<NEWL>>        >>> list(always_iterable(None))<<NEWL>>        []<<NEWL>><<NEWL>>    By default, binary and text strings are not considered iterable::<<NEWL>><<NEWL>>        >>> obj = 'foo'<<NEWL>>        >>> list(always_iterable(obj))<<NEWL>>        ['foo']<<NEWL>><<NEWL>>    If *base_type* is set, objects for which ``isinstance(obj, base_type)``<<NEWL>>    returns ``True`` won't be considered iterable.<<NEWL>><<NEWL>>        >>> obj = {'a': 1}<<NEWL>>        >>> list(always_iterable(obj))  # Iterate over the dict's keys<<NEWL>>        ['a']<<NEWL>>        >>> list(always_iterable(obj, base_type=dict))  # Treat dicts as a unit<<NEWL>>        [{'a': 1}]<<NEWL>><<NEWL>>    Set *base_type* to ``None`` to avoid any special handling and treat objects<<NEWL>>    Python considers iterable as iterable:<<NEWL>><<NEWL>>        >>> obj = 'foo'<<NEWL>>        >>> list(always_iterable(obj, base_type=None))<<NEWL>>        ['f', 'o', 'o']<<NEWL>>    """"""<<NEWL>>    if obj is None:<<NEWL>>        return iter(())<<NEWL>><<NEWL>>    if (base_type is not None) and isinstance(obj, base_type):<<NEWL>>        return iter((obj,))<<NEWL>><<NEWL>>    try:<<NEWL>>        return iter(obj)<<NEWL>>    except TypeError:<<NEWL>>        return iter((obj,))"
405	donghui	4	"from itertools import filterfalse<<NEWL>><<NEWL>><<NEWL>>def unique_everseen(iterable, key=None):<<NEWL>>    ""List unique elements, preserving order. Remember all elements ever seen.""<<NEWL>>    # unique_everseen('AAAABBBCCDAABBB') --> A B C D<<NEWL>>    # unique_everseen('ABBCcAD', str.lower) --> A B C D<<NEWL>>    seen = set()<<NEWL>>    seen_add = seen.add<<NEWL>>    if key is None:<<NEWL>>        for element in filterfalse(seen.__contains__, iterable):<<NEWL>>            seen_add(element)<<NEWL>>            yield element<<NEWL>>    else:<<NEWL>>        for element in iterable:<<NEWL>>            k = key(element)<<NEWL>>            if k not in seen:<<NEWL>>                seen_add(k)<<NEWL>>                yield element<<NEWL>><<NEWL>><<NEWL>># copied from more_itertools 8.8<<NEWL>>def always_iterable(obj, base_type=(str, bytes)):<<NEWL>>    """"""If *obj* is iterable, return an iterator over its items::<<NEWL>><<NEWL>>        >>> obj = (1, 2, 3)<<NEWL>>        >>> list(always_iterable(obj))<<NEWL>>        [1, 2, 3]<<NEWL>><<NEWL>>    If *obj* is not iterable, return a one-item iterable containing *obj*::<<NEWL>><<NEWL>>        >>> obj = 1<<NEWL>>        >>> list(always_iterable(obj))<<NEWL>>        [1]<<NEWL>><<NEWL>>    If *obj* is ``None``, return an empty iterable:<<NEWL>><<NEWL>>        >>> obj = None<<NEWL>>        >>> list(always_iterable(None))<<NEWL>>        []<<NEWL>><<NEWL>>    By default, binary and text strings are not considered iterable::<<NEWL>><<NEWL>>        >>> obj = 'foo'<<NEWL>>        >>> list(always_iterable(obj))<<NEWL>>        ['foo']<<NEWL>><<NEWL>>    If *base_type* is set, objects for which ``isinstance(obj, base_type)``<<NEWL>>    returns ``True`` won't be considered iterable.<<NEWL>><<NEWL>>        >>> obj = {'a': 1}<<NEWL>>        >>> list(always_iterable(obj))  # Iterate over the dict's keys<<NEWL>>        ['a']<<NEWL>>        >>> list(always_iterable(obj, base_type=dict))  # Treat dicts as a unit<<NEWL>>        [{'a': 1}]<<NEWL>><<NEWL>>    Set *base_type* to ``None`` to avoid any special handling and treat objects<<NEWL>>    Python considers iterable as iterable:<<NEWL>><<NEWL>>        >>> obj = 'foo'<<NEWL>>        >>> list(always_iterable(obj, base_type=None))<<NEWL>>        ['f', 'o', 'o']<<NEWL>>    """"""<<NEWL>>    if obj is None:<<NEWL>>        return iter(())<<NEWL>><<NEWL>>    if (base_type is not None) and isinstance(obj, base_type):<<NEWL>>        return iter((obj,))<<NEWL>><<NEWL>>    try:<<NEWL>>        return iter(obj)<<NEWL>>    except TypeError:<<NEWL>>        return iter((obj,))"
493	jackson	1	"#<<NEWL>># Licensed to the Apache Software Foundation (ASF) under one<<NEWL>># or more contributor license agreements.  See the NOTICE file<<NEWL>># distributed with this work for additional information<<NEWL>># regarding copyright ownership.  The ASF licenses this file<<NEWL>># to you under the Apache License, Version 2.0 (the<<NEWL>># ""License""); you may not use this file except in compliance<<NEWL>># with the License.  You may obtain a copy of the License at<<NEWL>>#<<NEWL>>#   http://www.apache.org/licenses/LICENSE-2.0<<NEWL>>#<<NEWL>># Unless required by applicable law or agreed to in writing,<<NEWL>># software distributed under the License is distributed on an<<NEWL>># ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY<<NEWL>># KIND, either express or implied.  See the License for the<<NEWL>># specific language governing permissions and limitations<<NEWL>># under the License.<<NEWL>>from __future__ import annotations<<NEWL>><<NEWL>>from unittest import mock<<NEWL>><<NEWL>>import pytest<<NEWL>><<NEWL>>from airflow.cli import cli_parser<<NEWL>>from airflow.cli.commands import dag_processor_command<<NEWL>>from airflow.configuration import conf<<NEWL>>from tests.test_utils.config import conf_vars<<NEWL>><<NEWL>><<NEWL>>class TestDagProcessorCommand:<<NEWL>>    """"""<<NEWL>>    Tests the CLI interface and that it correctly calls the DagProcessor<<NEWL>>    """"""<<NEWL>><<NEWL>>    @classmethod<<NEWL>>    def setup_class(cls):<<NEWL>>        cls.parser = cli_parser.get_parser()<<NEWL>><<NEWL>>    @conf_vars(<<NEWL>>        {<<NEWL>>            (""scheduler"", ""standalone_dag_processor""): ""True"",<<NEWL>>            (""core"", ""load_examples""): ""False"",<<NEWL>>        }<<NEWL>>    )<<NEWL>>    @mock.patch(""airflow.cli.commands.dag_processor_command.DagProcessorJob"")<<NEWL>>    @pytest.mark.skipif(<<NEWL>>        conf.get_mandatory_value(""database"", ""sql_alchemy_conn"").lower().startswith(""sqlite""),<<NEWL>>        reason=""Standalone Dag Processor doesn't support sqlite."",<<NEWL>>    )<<NEWL>>    def test_start_job(<<NEWL>>        self,<<NEWL>>        mock_dag_job,<<NEWL>>    ):<<NEWL>>        """"""Ensure that DagFileProcessorManager is started""""""<<NEWL>>        with conf_vars({(""scheduler"", ""standalone_dag_processor""): ""True""}):<<NEWL>>            args = self.parser.parse_args([""dag-processor""])<<NEWL>>            dag_processor_command.dag_processor(args)<<NEWL>>            mock_dag_job.return_value.run.assert_called()"
493	donghui	2	"#<<NEWL>># Licensed to the Apache Software Foundation (ASF) under one<<NEWL>># or more contributor license agreements.  See the NOTICE file<<NEWL>># distributed with this work for additional information<<NEWL>># regarding copyright ownership.  The ASF licenses this file<<NEWL>># to you under the Apache License, Version 2.0 (the<<NEWL>># ""License""); you may not use this file except in compliance<<NEWL>># with the License.  You may obtain a copy of the License at<<NEWL>>#<<NEWL>>#   http://www.apache.org/licenses/LICENSE-2.0<<NEWL>>#<<NEWL>># Unless required by applicable law or agreed to in writing,<<NEWL>># software distributed under the License is distributed on an<<NEWL>># ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY<<NEWL>># KIND, either express or implied.  See the License for the<<NEWL>># specific language governing permissions and limitations<<NEWL>># under the License.<<NEWL>>from __future__ import annotations<<NEWL>><<NEWL>>from unittest import mock<<NEWL>><<NEWL>>import pytest<<NEWL>><<NEWL>>from airflow.cli import cli_parser<<NEWL>>from airflow.cli.commands import dag_processor_command<<NEWL>>from airflow.configuration import conf<<NEWL>>from tests.test_utils.config import conf_vars<<NEWL>><<NEWL>><<NEWL>>class TestDagProcessorCommand:<<NEWL>>    """"""<<NEWL>>    Tests the CLI interface and that it correctly calls the DagProcessor<<NEWL>>    """"""<<NEWL>><<NEWL>>    @classmethod<<NEWL>>    def setup_class(cls):<<NEWL>>        cls.parser = cli_parser.get_parser()<<NEWL>><<NEWL>>    @conf_vars(<<NEWL>>        {<<NEWL>>            (""scheduler"", ""standalone_dag_processor""): ""True"",<<NEWL>>            (""core"", ""load_examples""): ""False"",<<NEWL>>        }<<NEWL>>    )<<NEWL>>    @mock.patch(""airflow.cli.commands.dag_processor_command.DagProcessorJob"")<<NEWL>>    @pytest.mark.skipif(<<NEWL>>        conf.get_mandatory_value(""database"", ""sql_alchemy_conn"").lower().startswith(""sqlite""),<<NEWL>>        reason=""Standalone Dag Processor doesn't support sqlite."",<<NEWL>>    )<<NEWL>>    def test_start_job(<<NEWL>>        self,<<NEWL>>        mock_dag_job,<<NEWL>>    ):<<NEWL>>        """"""Ensure that DagFileProcessorManager is started""""""<<NEWL>>        with conf_vars({(""scheduler"", ""standalone_dag_processor""): ""True""}):<<NEWL>>            args = self.parser.parse_args([""dag-processor""])<<NEWL>>            dag_processor_command.dag_processor(args)<<NEWL>>            mock_dag_job.return_value.run.assert_called()"
431	jackson	3	"import os<<NEWL>>import string<<NEWL>>import urllib.parse<<NEWL>>import urllib.request<<NEWL>>from typing import Optional<<NEWL>><<NEWL>>from .compat import WINDOWS<<NEWL>><<NEWL>><<NEWL>>def get_url_scheme(url: str) -> Optional[str]:<<NEWL>>    if "":"" not in url:<<NEWL>>        return None<<NEWL>>    return url.split("":"", 1)[0].lower()<<NEWL>><<NEWL>><<NEWL>>def path_to_url(path: str) -> str:<<NEWL>>    """"""<<NEWL>>    Convert a path to a file: URL.  The path will be made absolute and have<<NEWL>>    quoted path parts.<<NEWL>>    """"""<<NEWL>>    path = os.path.normpath(os.path.abspath(path))<<NEWL>>    url = urllib.parse.urljoin(""file:"", urllib.request.pathname2url(path))<<NEWL>>    return url<<NEWL>><<NEWL>><<NEWL>>def url_to_path(url: str) -> str:<<NEWL>>    """"""<<NEWL>>    Convert a file: URL to a path.<<NEWL>>    """"""<<NEWL>>    assert url.startswith(<<NEWL>>        ""file:""<<NEWL>>    ), f""You can only turn file: urls into filenames (not {url!r})""<<NEWL>><<NEWL>>    _, netloc, path, _, _ = urllib.parse.urlsplit(url)<<NEWL>><<NEWL>>    if not netloc or netloc == ""localhost"":<<NEWL>>        # According to RFC 8089, same as empty authority.<<NEWL>>        netloc = """"<<NEWL>>    elif WINDOWS:<<NEWL>>        # If we have a UNC path, prepend UNC share notation.<<NEWL>>        netloc = ""\\\\"" + netloc<<NEWL>>    else:<<NEWL>>        raise ValueError(<<NEWL>>            f""non-local file URIs are not supported on this platform: {url!r}""<<NEWL>>        )<<NEWL>><<NEWL>>    path = urllib.request.url2pathname(netloc + path)<<NEWL>><<NEWL>>    # On Windows, urlsplit parses the path as something like ""/C:/Users/foo"".<<NEWL>>    # This creates issues for path-related functions like io.open(), so we try<<NEWL>>    # to detect and strip the leading slash.<<NEWL>>    if (<<NEWL>>        WINDOWS<<NEWL>>        and not netloc  # Not UNC.<<NEWL>>        and len(path) >= 3<<NEWL>>        and path[0] == ""/""  # Leading slash to strip.<<NEWL>>        and path[1] in string.ascii_letters  # Drive letter.<<NEWL>>        and path[2:4] in ("":"", "":/"")  # Colon + end of string, or colon + absolute path.<<NEWL>>    ):<<NEWL>>        path = path[1:]<<NEWL>><<NEWL>>    return path"
431	donghui	3	"import os<<NEWL>>import string<<NEWL>>import urllib.parse<<NEWL>>import urllib.request<<NEWL>>from typing import Optional<<NEWL>><<NEWL>>from .compat import WINDOWS<<NEWL>><<NEWL>><<NEWL>>def get_url_scheme(url: str) -> Optional[str]:<<NEWL>>    if "":"" not in url:<<NEWL>>        return None<<NEWL>>    return url.split("":"", 1)[0].lower()<<NEWL>><<NEWL>><<NEWL>>def path_to_url(path: str) -> str:<<NEWL>>    """"""<<NEWL>>    Convert a path to a file: URL.  The path will be made absolute and have<<NEWL>>    quoted path parts.<<NEWL>>    """"""<<NEWL>>    path = os.path.normpath(os.path.abspath(path))<<NEWL>>    url = urllib.parse.urljoin(""file:"", urllib.request.pathname2url(path))<<NEWL>>    return url<<NEWL>><<NEWL>><<NEWL>>def url_to_path(url: str) -> str:<<NEWL>>    """"""<<NEWL>>    Convert a file: URL to a path.<<NEWL>>    """"""<<NEWL>>    assert url.startswith(<<NEWL>>        ""file:""<<NEWL>>    ), f""You can only turn file: urls into filenames (not {url!r})""<<NEWL>><<NEWL>>    _, netloc, path, _, _ = urllib.parse.urlsplit(url)<<NEWL>><<NEWL>>    if not netloc or netloc == ""localhost"":<<NEWL>>        # According to RFC 8089, same as empty authority.<<NEWL>>        netloc = """"<<NEWL>>    elif WINDOWS:<<NEWL>>        # If we have a UNC path, prepend UNC share notation.<<NEWL>>        netloc = ""\\\\"" + netloc<<NEWL>>    else:<<NEWL>>        raise ValueError(<<NEWL>>            f""non-local file URIs are not supported on this platform: {url!r}""<<NEWL>>        )<<NEWL>><<NEWL>>    path = urllib.request.url2pathname(netloc + path)<<NEWL>><<NEWL>>    # On Windows, urlsplit parses the path as something like ""/C:/Users/foo"".<<NEWL>>    # This creates issues for path-related functions like io.open(), so we try<<NEWL>>    # to detect and strip the leading slash.<<NEWL>>    if (<<NEWL>>        WINDOWS<<NEWL>>        and not netloc  # Not UNC.<<NEWL>>        and len(path) >= 3<<NEWL>>        and path[0] == ""/""  # Leading slash to strip.<<NEWL>>        and path[1] in string.ascii_letters  # Drive letter.<<NEWL>>        and path[2:4] in ("":"", "":/"")  # Colon + end of string, or colon + absolute path.<<NEWL>>    ):<<NEWL>>        path = path[1:]<<NEWL>><<NEWL>>    return path"
460	jackson	1	"#!/usr/bin/env python<<NEWL>>"""""" pygame.examples.setmodescale<<NEWL>><<NEWL>>On high resolution displays(4k, 1080p) and tiny graphics games (640x480)<<NEWL>>show up very small so that they are unplayable. SCALED scales up the window<<NEWL>>for you. The game thinks it's a 640x480 window, but really it can be bigger.<<NEWL>>Mouse events are scaled for you, so your game doesn't need to do it.<<NEWL>><<NEWL>>Passing SCALED to pygame.display.set_mode means the resolution depends<<NEWL>>on desktop size and the graphics are scaled.<<NEWL>>""""""<<NEWL>><<NEWL>>import pygame as pg<<NEWL>><<NEWL>>pg.init()<<NEWL>><<NEWL>>RES = (160, 120)<<NEWL>>FPS = 30<<NEWL>>clock = pg.time.Clock()<<NEWL>><<NEWL>>print(""desktops"", pg.display.get_desktop_sizes())<<NEWL>>screen = pg.display.set_mode(RES, pg.SCALED | pg.RESIZABLE)<<NEWL>><<NEWL>># MAIN LOOP<<NEWL>><<NEWL>>done = False<<NEWL>><<NEWL>>i = 0<<NEWL>>j = 0<<NEWL>><<NEWL>>r_name, r_flags = pg.display._get_renderer_info()<<NEWL>>print(""renderer:"", r_name, ""flags:"", bin(r_flags))<<NEWL>>for flag, name in [<<NEWL>>    (1, ""software""),<<NEWL>>    (2, ""accelerated""),<<NEWL>>    (4, ""VSync""),<<NEWL>>    (8, ""render to texture""),<<NEWL>>]:<<NEWL>>    if flag & r_flags:<<NEWL>>        print(name)<<NEWL>><<NEWL>>while not done:<<NEWL>>    for event in pg.event.get():<<NEWL>>        if event.type == pg.KEYDOWN and event.key == pg.K_q:<<NEWL>>            done = True<<NEWL>>        if event.type == pg.QUIT:<<NEWL>>            done = True<<NEWL>>        if event.type == pg.KEYDOWN and event.key == pg.K_f:<<NEWL>>            pg.display.toggle_fullscreen()<<NEWL>>        if event.type == pg.VIDEORESIZE:<<NEWL>>            pg.display._resize_event(event)<<NEWL>><<NEWL>>    i += 1<<NEWL>>    i = i % screen.get_width()<<NEWL>>    j += i % 2<<NEWL>>    j = j % screen.get_height()<<NEWL>><<NEWL>>    screen.fill((255, 0, 255))<<NEWL>>    pg.draw.circle(screen, (0, 0, 0), (100, 100), 20)<<NEWL>>    pg.draw.circle(screen, (0, 0, 200), (0, 0), 10)<<NEWL>>    pg.draw.circle(screen, (200, 0, 0), (160, 120), 30)<<NEWL>>    pg.draw.line(screen, (250, 250, 0), (0, 120), (160, 0))<<NEWL>>    pg.draw.circle(screen, (255, 255, 255), (i, j), 5)<<NEWL>><<NEWL>>    pg.display.flip()<<NEWL>>    clock.tick(FPS)<<NEWL>>pg.quit()"
460	donghui	1	"#!/usr/bin/env python<<NEWL>>"""""" pygame.examples.setmodescale<<NEWL>><<NEWL>>On high resolution displays(4k, 1080p) and tiny graphics games (640x480)<<NEWL>>show up very small so that they are unplayable. SCALED scales up the window<<NEWL>>for you. The game thinks it's a 640x480 window, but really it can be bigger.<<NEWL>>Mouse events are scaled for you, so your game doesn't need to do it.<<NEWL>><<NEWL>>Passing SCALED to pygame.display.set_mode means the resolution depends<<NEWL>>on desktop size and the graphics are scaled.<<NEWL>>""""""<<NEWL>><<NEWL>>import pygame as pg<<NEWL>><<NEWL>>pg.init()<<NEWL>><<NEWL>>RES = (160, 120)<<NEWL>>FPS = 30<<NEWL>>clock = pg.time.Clock()<<NEWL>><<NEWL>>print(""desktops"", pg.display.get_desktop_sizes())<<NEWL>>screen = pg.display.set_mode(RES, pg.SCALED | pg.RESIZABLE)<<NEWL>><<NEWL>># MAIN LOOP<<NEWL>><<NEWL>>done = False<<NEWL>><<NEWL>>i = 0<<NEWL>>j = 0<<NEWL>><<NEWL>>r_name, r_flags = pg.display._get_renderer_info()<<NEWL>>print(""renderer:"", r_name, ""flags:"", bin(r_flags))<<NEWL>>for flag, name in [<<NEWL>>    (1, ""software""),<<NEWL>>    (2, ""accelerated""),<<NEWL>>    (4, ""VSync""),<<NEWL>>    (8, ""render to texture""),<<NEWL>>]:<<NEWL>>    if flag & r_flags:<<NEWL>>        print(name)<<NEWL>><<NEWL>>while not done:<<NEWL>>    for event in pg.event.get():<<NEWL>>        if event.type == pg.KEYDOWN and event.key == pg.K_q:<<NEWL>>            done = True<<NEWL>>        if event.type == pg.QUIT:<<NEWL>>            done = True<<NEWL>>        if event.type == pg.KEYDOWN and event.key == pg.K_f:<<NEWL>>            pg.display.toggle_fullscreen()<<NEWL>>        if event.type == pg.VIDEORESIZE:<<NEWL>>            pg.display._resize_event(event)<<NEWL>><<NEWL>>    i += 1<<NEWL>>    i = i % screen.get_width()<<NEWL>>    j += i % 2<<NEWL>>    j = j % screen.get_height()<<NEWL>><<NEWL>>    screen.fill((255, 0, 255))<<NEWL>>    pg.draw.circle(screen, (0, 0, 0), (100, 100), 20)<<NEWL>>    pg.draw.circle(screen, (0, 0, 200), (0, 0), 10)<<NEWL>>    pg.draw.circle(screen, (200, 0, 0), (160, 120), 30)<<NEWL>>    pg.draw.line(screen, (250, 250, 0), (0, 120), (160, 0))<<NEWL>>    pg.draw.circle(screen, (255, 255, 255), (i, j), 5)<<NEWL>><<NEWL>>    pg.display.flip()<<NEWL>>    clock.tick(FPS)<<NEWL>>pg.quit()"
412	jackson	2	"import numpy as np<<NEWL>>import pytest<<NEWL>><<NEWL>>import pandas as pd<<NEWL>>from pandas import (<<NEWL>>    Index,<<NEWL>>    MultiIndex,<<NEWL>>)<<NEWL>><<NEWL>><<NEWL>># Note: identical the ""multi"" entry in the top-level ""index"" fixture<<NEWL>>@pytest.fixture<<NEWL>>def idx():<<NEWL>>    # a MultiIndex used to test the general functionality of the<<NEWL>>    # general functionality of this object<<NEWL>>    major_axis = Index([""foo"", ""bar"", ""baz"", ""qux""])<<NEWL>>    minor_axis = Index([""one"", ""two""])<<NEWL>><<NEWL>>    major_codes = np.array([0, 0, 1, 2, 3, 3])<<NEWL>>    minor_codes = np.array([0, 1, 0, 1, 0, 1])<<NEWL>>    index_names = [""first"", ""second""]<<NEWL>>    mi = MultiIndex(<<NEWL>>        levels=[major_axis, minor_axis],<<NEWL>>        codes=[major_codes, minor_codes],<<NEWL>>        names=index_names,<<NEWL>>        verify_integrity=False,<<NEWL>>    )<<NEWL>>    return mi<<NEWL>><<NEWL>><<NEWL>>@pytest.fixture<<NEWL>>def idx_dup():<<NEWL>>    # compare tests/indexes/multi/conftest.py<<NEWL>>    major_axis = Index([""foo"", ""bar"", ""baz"", ""qux""])<<NEWL>>    minor_axis = Index([""one"", ""two""])<<NEWL>><<NEWL>>    major_codes = np.array([0, 0, 1, 0, 1, 1])<<NEWL>>    minor_codes = np.array([0, 1, 0, 1, 0, 1])<<NEWL>>    index_names = [""first"", ""second""]<<NEWL>>    mi = MultiIndex(<<NEWL>>        levels=[major_axis, minor_axis],<<NEWL>>        codes=[major_codes, minor_codes],<<NEWL>>        names=index_names,<<NEWL>>        verify_integrity=False,<<NEWL>>    )<<NEWL>>    return mi<<NEWL>><<NEWL>><<NEWL>>@pytest.fixture<<NEWL>>def index_names():<<NEWL>>    # names that match those in the idx fixture for testing equality of<<NEWL>>    # names assigned to the idx<<NEWL>>    return [""first"", ""second""]<<NEWL>><<NEWL>><<NEWL>>@pytest.fixture<<NEWL>>def narrow_multi_index():<<NEWL>>    """"""<<NEWL>>    Return a MultiIndex that is narrower than the display (<80 characters).<<NEWL>>    """"""<<NEWL>>    n = 1000<<NEWL>>    ci = pd.CategoricalIndex(list(""a"" * n) + ([""abc""] * n))<<NEWL>>    dti = pd.date_range(""2000-01-01"", freq=""s"", periods=n * 2)<<NEWL>>    return MultiIndex.from_arrays([ci, ci.codes + 9, dti], names=[""a"", ""b"", ""dti""])<<NEWL>><<NEWL>><<NEWL>>@pytest.fixture<<NEWL>>def wide_multi_index():<<NEWL>>    """"""<<NEWL>>    Return a MultiIndex that is wider than the display (>80 characters).<<NEWL>>    """"""<<NEWL>>    n = 1000<<NEWL>>    ci = pd.CategoricalIndex(list(""a"" * n) + ([""abc""] * n))<<NEWL>>    dti = pd.date_range(""2000-01-01"", freq=""s"", periods=n * 2)<<NEWL>>    levels = [ci, ci.codes + 9, dti, dti, dti]<<NEWL>>    names = [""a"", ""b"", ""dti_1"", ""dti_2"", ""dti_3""]<<NEWL>>    return MultiIndex.from_arrays(levels, names=names)"
412	donghui	2	"import numpy as np<<NEWL>>import pytest<<NEWL>><<NEWL>>import pandas as pd<<NEWL>>from pandas import (<<NEWL>>    Index,<<NEWL>>    MultiIndex,<<NEWL>>)<<NEWL>><<NEWL>><<NEWL>># Note: identical the ""multi"" entry in the top-level ""index"" fixture<<NEWL>>@pytest.fixture<<NEWL>>def idx():<<NEWL>>    # a MultiIndex used to test the general functionality of the<<NEWL>>    # general functionality of this object<<NEWL>>    major_axis = Index([""foo"", ""bar"", ""baz"", ""qux""])<<NEWL>>    minor_axis = Index([""one"", ""two""])<<NEWL>><<NEWL>>    major_codes = np.array([0, 0, 1, 2, 3, 3])<<NEWL>>    minor_codes = np.array([0, 1, 0, 1, 0, 1])<<NEWL>>    index_names = [""first"", ""second""]<<NEWL>>    mi = MultiIndex(<<NEWL>>        levels=[major_axis, minor_axis],<<NEWL>>        codes=[major_codes, minor_codes],<<NEWL>>        names=index_names,<<NEWL>>        verify_integrity=False,<<NEWL>>    )<<NEWL>>    return mi<<NEWL>><<NEWL>><<NEWL>>@pytest.fixture<<NEWL>>def idx_dup():<<NEWL>>    # compare tests/indexes/multi/conftest.py<<NEWL>>    major_axis = Index([""foo"", ""bar"", ""baz"", ""qux""])<<NEWL>>    minor_axis = Index([""one"", ""two""])<<NEWL>><<NEWL>>    major_codes = np.array([0, 0, 1, 0, 1, 1])<<NEWL>>    minor_codes = np.array([0, 1, 0, 1, 0, 1])<<NEWL>>    index_names = [""first"", ""second""]<<NEWL>>    mi = MultiIndex(<<NEWL>>        levels=[major_axis, minor_axis],<<NEWL>>        codes=[major_codes, minor_codes],<<NEWL>>        names=index_names,<<NEWL>>        verify_integrity=False,<<NEWL>>    )<<NEWL>>    return mi<<NEWL>><<NEWL>><<NEWL>>@pytest.fixture<<NEWL>>def index_names():<<NEWL>>    # names that match those in the idx fixture for testing equality of<<NEWL>>    # names assigned to the idx<<NEWL>>    return [""first"", ""second""]<<NEWL>><<NEWL>><<NEWL>>@pytest.fixture<<NEWL>>def narrow_multi_index():<<NEWL>>    """"""<<NEWL>>    Return a MultiIndex that is narrower than the display (<80 characters).<<NEWL>>    """"""<<NEWL>>    n = 1000<<NEWL>>    ci = pd.CategoricalIndex(list(""a"" * n) + ([""abc""] * n))<<NEWL>>    dti = pd.date_range(""2000-01-01"", freq=""s"", periods=n * 2)<<NEWL>>    return MultiIndex.from_arrays([ci, ci.codes + 9, dti], names=[""a"", ""b"", ""dti""])<<NEWL>><<NEWL>><<NEWL>>@pytest.fixture<<NEWL>>def wide_multi_index():<<NEWL>>    """"""<<NEWL>>    Return a MultiIndex that is wider than the display (>80 characters).<<NEWL>>    """"""<<NEWL>>    n = 1000<<NEWL>>    ci = pd.CategoricalIndex(list(""a"" * n) + ([""abc""] * n))<<NEWL>>    dti = pd.date_range(""2000-01-01"", freq=""s"", periods=n * 2)<<NEWL>>    levels = [ci, ci.codes + 9, dti, dti, dti]<<NEWL>>    names = [""a"", ""b"", ""dti_1"", ""dti_2"", ""dti_3""]<<NEWL>>    return MultiIndex.from_arrays(levels, names=names)"
443	jackson	0	"# This file is dual licensed under the terms of the Apache License, Version<<NEWL>># 2.0, and the BSD License. See the LICENSE file in the root of this repository<<NEWL>># for complete details.<<NEWL>><<NEWL>><<NEWL>>import typing<<NEWL>><<NEWL>>from cryptography.hazmat.primitives.asymmetric import dh<<NEWL>>from cryptography.hazmat.primitives.asymmetric.types import (<<NEWL>>    PRIVATE_KEY_TYPES,<<NEWL>>    PUBLIC_KEY_TYPES,<<NEWL>>)<<NEWL>><<NEWL>><<NEWL>>def load_pem_private_key(<<NEWL>>    data: bytes,<<NEWL>>    password: typing.Optional[bytes],<<NEWL>>    backend: typing.Any = None,<<NEWL>>    *,<<NEWL>>    unsafe_skip_rsa_key_validation: bool = False,<<NEWL>>) -> PRIVATE_KEY_TYPES:<<NEWL>>    from cryptography.hazmat.backends.openssl.backend import backend as ossl<<NEWL>><<NEWL>>    return ossl.load_pem_private_key(<<NEWL>>        data, password, unsafe_skip_rsa_key_validation<<NEWL>>    )<<NEWL>><<NEWL>><<NEWL>>def load_pem_public_key(<<NEWL>>    data: bytes, backend: typing.Any = None<<NEWL>>) -> PUBLIC_KEY_TYPES:<<NEWL>>    from cryptography.hazmat.backends.openssl.backend import backend as ossl<<NEWL>><<NEWL>>    return ossl.load_pem_public_key(data)<<NEWL>><<NEWL>><<NEWL>>def load_pem_parameters(<<NEWL>>    data: bytes, backend: typing.Any = None<<NEWL>>) -> ""dh.DHParameters"":<<NEWL>>    from cryptography.hazmat.backends.openssl.backend import backend as ossl<<NEWL>><<NEWL>>    return ossl.load_pem_parameters(data)<<NEWL>><<NEWL>><<NEWL>>def load_der_private_key(<<NEWL>>    data: bytes,<<NEWL>>    password: typing.Optional[bytes],<<NEWL>>    backend: typing.Any = None,<<NEWL>>    *,<<NEWL>>    unsafe_skip_rsa_key_validation: bool = False,<<NEWL>>) -> PRIVATE_KEY_TYPES:<<NEWL>>    from cryptography.hazmat.backends.openssl.backend import backend as ossl<<NEWL>><<NEWL>>    return ossl.load_der_private_key(<<NEWL>>        data, password, unsafe_skip_rsa_key_validation<<NEWL>>    )<<NEWL>><<NEWL>><<NEWL>>def load_der_public_key(<<NEWL>>    data: bytes, backend: typing.Any = None<<NEWL>>) -> PUBLIC_KEY_TYPES:<<NEWL>>    from cryptography.hazmat.backends.openssl.backend import backend as ossl<<NEWL>><<NEWL>>    return ossl.load_der_public_key(data)<<NEWL>><<NEWL>><<NEWL>>def load_der_parameters(<<NEWL>>    data: bytes, backend: typing.Any = None<<NEWL>>) -> ""dh.DHParameters"":<<NEWL>>    from cryptography.hazmat.backends.openssl.backend import backend as ossl<<NEWL>><<NEWL>>    return ossl.load_der_parameters(data)"
443	donghui	0	"# This file is dual licensed under the terms of the Apache License, Version<<NEWL>># 2.0, and the BSD License. See the LICENSE file in the root of this repository<<NEWL>># for complete details.<<NEWL>><<NEWL>><<NEWL>>import typing<<NEWL>><<NEWL>>from cryptography.hazmat.primitives.asymmetric import dh<<NEWL>>from cryptography.hazmat.primitives.asymmetric.types import (<<NEWL>>    PRIVATE_KEY_TYPES,<<NEWL>>    PUBLIC_KEY_TYPES,<<NEWL>>)<<NEWL>><<NEWL>><<NEWL>>def load_pem_private_key(<<NEWL>>    data: bytes,<<NEWL>>    password: typing.Optional[bytes],<<NEWL>>    backend: typing.Any = None,<<NEWL>>    *,<<NEWL>>    unsafe_skip_rsa_key_validation: bool = False,<<NEWL>>) -> PRIVATE_KEY_TYPES:<<NEWL>>    from cryptography.hazmat.backends.openssl.backend import backend as ossl<<NEWL>><<NEWL>>    return ossl.load_pem_private_key(<<NEWL>>        data, password, unsafe_skip_rsa_key_validation<<NEWL>>    )<<NEWL>><<NEWL>><<NEWL>>def load_pem_public_key(<<NEWL>>    data: bytes, backend: typing.Any = None<<NEWL>>) -> PUBLIC_KEY_TYPES:<<NEWL>>    from cryptography.hazmat.backends.openssl.backend import backend as ossl<<NEWL>><<NEWL>>    return ossl.load_pem_public_key(data)<<NEWL>><<NEWL>><<NEWL>>def load_pem_parameters(<<NEWL>>    data: bytes, backend: typing.Any = None<<NEWL>>) -> ""dh.DHParameters"":<<NEWL>>    from cryptography.hazmat.backends.openssl.backend import backend as ossl<<NEWL>><<NEWL>>    return ossl.load_pem_parameters(data)<<NEWL>><<NEWL>><<NEWL>>def load_der_private_key(<<NEWL>>    data: bytes,<<NEWL>>    password: typing.Optional[bytes],<<NEWL>>    backend: typing.Any = None,<<NEWL>>    *,<<NEWL>>    unsafe_skip_rsa_key_validation: bool = False,<<NEWL>>) -> PRIVATE_KEY_TYPES:<<NEWL>>    from cryptography.hazmat.backends.openssl.backend import backend as ossl<<NEWL>><<NEWL>>    return ossl.load_der_private_key(<<NEWL>>        data, password, unsafe_skip_rsa_key_validation<<NEWL>>    )<<NEWL>><<NEWL>><<NEWL>>def load_der_public_key(<<NEWL>>    data: bytes, backend: typing.Any = None<<NEWL>>) -> PUBLIC_KEY_TYPES:<<NEWL>>    from cryptography.hazmat.backends.openssl.backend import backend as ossl<<NEWL>><<NEWL>>    return ossl.load_der_public_key(data)<<NEWL>><<NEWL>><<NEWL>>def load_der_parameters(<<NEWL>>    data: bytes, backend: typing.Any = None<<NEWL>>) -> ""dh.DHParameters"":<<NEWL>>    from cryptography.hazmat.backends.openssl.backend import backend as ossl<<NEWL>><<NEWL>>    return ossl.load_der_parameters(data)"
484	jackson	1	"""""""<<NEWL>>views.py        # Houses `SchemaView`, `APIView` subclass.<<NEWL>><<NEWL>>See schemas.__init__.py for package overview.<<NEWL>>""""""<<NEWL>>from rest_framework import exceptions, renderers<<NEWL>>from rest_framework.response import Response<<NEWL>>from rest_framework.schemas import coreapi<<NEWL>>from rest_framework.settings import api_settings<<NEWL>>from rest_framework.views import APIView<<NEWL>><<NEWL>><<NEWL>>class SchemaView(APIView):<<NEWL>>    _ignore_model_permissions = True<<NEWL>>    schema = None  # exclude from schema<<NEWL>>    renderer_classes = None<<NEWL>>    schema_generator = None<<NEWL>>    public = False<<NEWL>><<NEWL>>    def __init__(self, *args, **kwargs):<<NEWL>>        super().__init__(*args, **kwargs)<<NEWL>>        if self.renderer_classes is None:<<NEWL>>            if coreapi.is_enabled():<<NEWL>>                self.renderer_classes = [<<NEWL>>                    renderers.CoreAPIOpenAPIRenderer,<<NEWL>>                    renderers.CoreJSONRenderer,<<NEWL>>                ]<<NEWL>>            else:<<NEWL>>                self.renderer_classes = [<<NEWL>>                    renderers.OpenAPIRenderer,<<NEWL>>                    renderers.JSONOpenAPIRenderer,<<NEWL>>                ]<<NEWL>>            if renderers.BrowsableAPIRenderer in api_settings.DEFAULT_RENDERER_CLASSES:<<NEWL>>                self.renderer_classes += [renderers.BrowsableAPIRenderer]<<NEWL>><<NEWL>>    def get(self, request, *args, **kwargs):<<NEWL>>        schema = self.schema_generator.get_schema(request, self.public)<<NEWL>>        if schema is None:<<NEWL>>            raise exceptions.PermissionDenied()<<NEWL>>        return Response(schema)<<NEWL>><<NEWL>>    def handle_exception(self, exc):<<NEWL>>        # Schema renderers do not render exceptions, so re-perform content<<NEWL>>        # negotiation with default renderers.<<NEWL>>        self.renderer_classes = api_settings.DEFAULT_RENDERER_CLASSES<<NEWL>>        neg = self.perform_content_negotiation(self.request, force=True)<<NEWL>>        self.request.accepted_renderer, self.request.accepted_media_type = neg<<NEWL>>        return super().handle_exception(exc)"
484	donghui	1	"""""""<<NEWL>>views.py        # Houses `SchemaView`, `APIView` subclass.<<NEWL>><<NEWL>>See schemas.__init__.py for package overview.<<NEWL>>""""""<<NEWL>>from rest_framework import exceptions, renderers<<NEWL>>from rest_framework.response import Response<<NEWL>>from rest_framework.schemas import coreapi<<NEWL>>from rest_framework.settings import api_settings<<NEWL>>from rest_framework.views import APIView<<NEWL>><<NEWL>><<NEWL>>class SchemaView(APIView):<<NEWL>>    _ignore_model_permissions = True<<NEWL>>    schema = None  # exclude from schema<<NEWL>>    renderer_classes = None<<NEWL>>    schema_generator = None<<NEWL>>    public = False<<NEWL>><<NEWL>>    def __init__(self, *args, **kwargs):<<NEWL>>        super().__init__(*args, **kwargs)<<NEWL>>        if self.renderer_classes is None:<<NEWL>>            if coreapi.is_enabled():<<NEWL>>                self.renderer_classes = [<<NEWL>>                    renderers.CoreAPIOpenAPIRenderer,<<NEWL>>                    renderers.CoreJSONRenderer,<<NEWL>>                ]<<NEWL>>            else:<<NEWL>>                self.renderer_classes = [<<NEWL>>                    renderers.OpenAPIRenderer,<<NEWL>>                    renderers.JSONOpenAPIRenderer,<<NEWL>>                ]<<NEWL>>            if renderers.BrowsableAPIRenderer in api_settings.DEFAULT_RENDERER_CLASSES:<<NEWL>>                self.renderer_classes += [renderers.BrowsableAPIRenderer]<<NEWL>><<NEWL>>    def get(self, request, *args, **kwargs):<<NEWL>>        schema = self.schema_generator.get_schema(request, self.public)<<NEWL>>        if schema is None:<<NEWL>>            raise exceptions.PermissionDenied()<<NEWL>>        return Response(schema)<<NEWL>><<NEWL>>    def handle_exception(self, exc):<<NEWL>>        # Schema renderers do not render exceptions, so re-perform content<<NEWL>>        # negotiation with default renderers.<<NEWL>>        self.renderer_classes = api_settings.DEFAULT_RENDERER_CLASSES<<NEWL>>        neg = self.perform_content_negotiation(self.request, force=True)<<NEWL>>        self.request.accepted_renderer, self.request.accepted_media_type = neg<<NEWL>>        return super().handle_exception(exc)"
477	jackson	3	"""""""<<NEWL>>    pygments.styles.autumn<<NEWL>>    ~~~~~~~~~~~~~~~~~~~~~~<<NEWL>><<NEWL>>    A colorful style, inspired by the terminal highlighting style.<<NEWL>><<NEWL>>    :copyright: Copyright 2006-2022 by the Pygments team, see AUTHORS.<<NEWL>>    :license: BSD, see LICENSE for details.<<NEWL>>""""""<<NEWL>><<NEWL>>from pygments.style import Style<<NEWL>>from pygments.token import Keyword, Name, Comment, String, Error, \<<NEWL>>     Number, Operator, Generic, Whitespace<<NEWL>><<NEWL>><<NEWL>>class AutumnStyle(Style):<<NEWL>>    """"""<<NEWL>>    A colorful style, inspired by the terminal highlighting style.<<NEWL>>    """"""<<NEWL>><<NEWL>>    styles = {<<NEWL>>        Whitespace:                 '#bbbbbb',<<NEWL>><<NEWL>>        Comment:                    'italic #aaaaaa',<<NEWL>>        Comment.Preproc:            'noitalic #4c8317',<<NEWL>>        Comment.Special:            'italic #0000aa',<<NEWL>><<NEWL>>        Keyword:                    '#0000aa',<<NEWL>>        Keyword.Type:               '#00aaaa',<<NEWL>><<NEWL>>        Operator.Word:              '#0000aa',<<NEWL>><<NEWL>>        Name.Builtin:               '#00aaaa',<<NEWL>>        Name.Function:              '#00aa00',<<NEWL>>        Name.Class:                 'underline #00aa00',<<NEWL>>        Name.Namespace:             'underline #00aaaa',<<NEWL>>        Name.Variable:              '#aa0000',<<NEWL>>        Name.Constant:              '#aa0000',<<NEWL>>        Name.Entity:                'bold #800',<<NEWL>>        Name.Attribute:             '#1e90ff',<<NEWL>>        Name.Tag:                   'bold #1e90ff',<<NEWL>>        Name.Decorator:             '#888888',<<NEWL>><<NEWL>>        String:                     '#aa5500',<<NEWL>>        String.Symbol:              '#0000aa',<<NEWL>>        String.Regex:               '#009999',<<NEWL>><<NEWL>>        Number:                     '#009999',<<NEWL>><<NEWL>>        Generic.Heading:            'bold #000080',<<NEWL>>        Generic.Subheading:         'bold #800080',<<NEWL>>        Generic.Deleted:            '#aa0000',<<NEWL>>        Generic.Inserted:           '#00aa00',<<NEWL>>        Generic.Error:              '#aa0000',<<NEWL>>        Generic.Emph:               'italic',<<NEWL>>        Generic.Strong:             'bold',<<NEWL>>        Generic.Prompt:             '#555555',<<NEWL>>        Generic.Output:             '#888888',<<NEWL>>        Generic.Traceback:          '#aa0000',<<NEWL>><<NEWL>>        Error:                      '#F00 bg:#FAA'<<NEWL>>    }"
477	donghui	2	"""""""<<NEWL>>    pygments.styles.autumn<<NEWL>>    ~~~~~~~~~~~~~~~~~~~~~~<<NEWL>><<NEWL>>    A colorful style, inspired by the terminal highlighting style.<<NEWL>><<NEWL>>    :copyright: Copyright 2006-2022 by the Pygments team, see AUTHORS.<<NEWL>>    :license: BSD, see LICENSE for details.<<NEWL>>""""""<<NEWL>><<NEWL>>from pygments.style import Style<<NEWL>>from pygments.token import Keyword, Name, Comment, String, Error, \<<NEWL>>     Number, Operator, Generic, Whitespace<<NEWL>><<NEWL>><<NEWL>>class AutumnStyle(Style):<<NEWL>>    """"""<<NEWL>>    A colorful style, inspired by the terminal highlighting style.<<NEWL>>    """"""<<NEWL>><<NEWL>>    styles = {<<NEWL>>        Whitespace:                 '#bbbbbb',<<NEWL>><<NEWL>>        Comment:                    'italic #aaaaaa',<<NEWL>>        Comment.Preproc:            'noitalic #4c8317',<<NEWL>>        Comment.Special:            'italic #0000aa',<<NEWL>><<NEWL>>        Keyword:                    '#0000aa',<<NEWL>>        Keyword.Type:               '#00aaaa',<<NEWL>><<NEWL>>        Operator.Word:              '#0000aa',<<NEWL>><<NEWL>>        Name.Builtin:               '#00aaaa',<<NEWL>>        Name.Function:              '#00aa00',<<NEWL>>        Name.Class:                 'underline #00aa00',<<NEWL>>        Name.Namespace:             'underline #00aaaa',<<NEWL>>        Name.Variable:              '#aa0000',<<NEWL>>        Name.Constant:              '#aa0000',<<NEWL>>        Name.Entity:                'bold #800',<<NEWL>>        Name.Attribute:             '#1e90ff',<<NEWL>>        Name.Tag:                   'bold #1e90ff',<<NEWL>>        Name.Decorator:             '#888888',<<NEWL>><<NEWL>>        String:                     '#aa5500',<<NEWL>>        String.Symbol:              '#0000aa',<<NEWL>>        String.Regex:               '#009999',<<NEWL>><<NEWL>>        Number:                     '#009999',<<NEWL>><<NEWL>>        Generic.Heading:            'bold #000080',<<NEWL>>        Generic.Subheading:         'bold #800080',<<NEWL>>        Generic.Deleted:            '#aa0000',<<NEWL>>        Generic.Inserted:           '#00aa00',<<NEWL>>        Generic.Error:              '#aa0000',<<NEWL>>        Generic.Emph:               'italic',<<NEWL>>        Generic.Strong:             'bold',<<NEWL>>        Generic.Prompt:             '#555555',<<NEWL>>        Generic.Output:             '#888888',<<NEWL>>        Generic.Traceback:          '#aa0000',<<NEWL>><<NEWL>>        Error:                      '#F00 bg:#FAA'<<NEWL>>    }"
426	jackson	2	"""""""<<NEWL>>Aliases for functions which may be accelerated by Scipy.<<NEWL>><<NEWL>>Scipy_ can be built to use accelerated or otherwise improved libraries<<NEWL>>for FFTs, linear algebra, and special functions. This module allows<<NEWL>>developers to transparently support these accelerated functions when<<NEWL>>scipy is available but still support users who have only installed<<NEWL>>NumPy.<<NEWL>><<NEWL>>.. _Scipy : https://www.scipy.org<<NEWL>><<NEWL>>""""""<<NEWL>># This module should be used for functions both in numpy and scipy if<<NEWL>>#  you want to use the numpy version if available but the scipy version<<NEWL>>#  otherwise.<<NEWL>>#  Usage  --- from numpy.dual import fft, inv<<NEWL>><<NEWL>>__all__ = ['fft', 'ifft', 'fftn', 'ifftn', 'fft2', 'ifft2',<<NEWL>>           'norm', 'inv', 'svd', 'solve', 'det', 'eig', 'eigvals',<<NEWL>>           'eigh', 'eigvalsh', 'lstsq', 'pinv', 'cholesky', 'i0']<<NEWL>><<NEWL>>import numpy.linalg as linpkg<<NEWL>>import numpy.fft as fftpkg<<NEWL>>from numpy.lib import i0<<NEWL>>import sys<<NEWL>><<NEWL>><<NEWL>>fft = fftpkg.fft<<NEWL>>ifft = fftpkg.ifft<<NEWL>>fftn = fftpkg.fftn<<NEWL>>ifftn = fftpkg.ifftn<<NEWL>>fft2 = fftpkg.fft2<<NEWL>>ifft2 = fftpkg.ifft2<<NEWL>><<NEWL>>norm = linpkg.norm<<NEWL>>inv = linpkg.inv<<NEWL>>svd = linpkg.svd<<NEWL>>solve = linpkg.solve<<NEWL>>det = linpkg.det<<NEWL>>eig = linpkg.eig<<NEWL>>eigvals = linpkg.eigvals<<NEWL>>eigh = linpkg.eigh<<NEWL>>eigvalsh = linpkg.eigvalsh<<NEWL>>lstsq = linpkg.lstsq<<NEWL>>pinv = linpkg.pinv<<NEWL>>cholesky = linpkg.cholesky<<NEWL>><<NEWL>>_restore_dict = {}<<NEWL>><<NEWL>>def register_func(name, func):<<NEWL>>    if name not in __all__:<<NEWL>>        raise ValueError(""{} not a dual function."".format(name))<<NEWL>>    f = sys._getframe(0).f_globals<<NEWL>>    _restore_dict[name] = f[name]<<NEWL>>    f[name] = func<<NEWL>><<NEWL>>def restore_func(name):<<NEWL>>    if name not in __all__:<<NEWL>>        raise ValueError(""{} not a dual function."".format(name))<<NEWL>>    try:<<NEWL>>        val = _restore_dict[name]<<NEWL>>    except KeyError:<<NEWL>>        return<<NEWL>>    else:<<NEWL>>        sys._getframe(0).f_globals[name] = val<<NEWL>><<NEWL>>def restore_all():<<NEWL>>    for name in _restore_dict.keys():<<NEWL>>        restore_func(name)"
426	donghui	1	"""""""<<NEWL>>Aliases for functions which may be accelerated by Scipy.<<NEWL>><<NEWL>>Scipy_ can be built to use accelerated or otherwise improved libraries<<NEWL>>for FFTs, linear algebra, and special functions. This module allows<<NEWL>>developers to transparently support these accelerated functions when<<NEWL>>scipy is available but still support users who have only installed<<NEWL>>NumPy.<<NEWL>><<NEWL>>.. _Scipy : https://www.scipy.org<<NEWL>><<NEWL>>""""""<<NEWL>># This module should be used for functions both in numpy and scipy if<<NEWL>>#  you want to use the numpy version if available but the scipy version<<NEWL>>#  otherwise.<<NEWL>>#  Usage  --- from numpy.dual import fft, inv<<NEWL>><<NEWL>>__all__ = ['fft', 'ifft', 'fftn', 'ifftn', 'fft2', 'ifft2',<<NEWL>>           'norm', 'inv', 'svd', 'solve', 'det', 'eig', 'eigvals',<<NEWL>>           'eigh', 'eigvalsh', 'lstsq', 'pinv', 'cholesky', 'i0']<<NEWL>><<NEWL>>import numpy.linalg as linpkg<<NEWL>>import numpy.fft as fftpkg<<NEWL>>from numpy.lib import i0<<NEWL>>import sys<<NEWL>><<NEWL>><<NEWL>>fft = fftpkg.fft<<NEWL>>ifft = fftpkg.ifft<<NEWL>>fftn = fftpkg.fftn<<NEWL>>ifftn = fftpkg.ifftn<<NEWL>>fft2 = fftpkg.fft2<<NEWL>>ifft2 = fftpkg.ifft2<<NEWL>><<NEWL>>norm = linpkg.norm<<NEWL>>inv = linpkg.inv<<NEWL>>svd = linpkg.svd<<NEWL>>solve = linpkg.solve<<NEWL>>det = linpkg.det<<NEWL>>eig = linpkg.eig<<NEWL>>eigvals = linpkg.eigvals<<NEWL>>eigh = linpkg.eigh<<NEWL>>eigvalsh = linpkg.eigvalsh<<NEWL>>lstsq = linpkg.lstsq<<NEWL>>pinv = linpkg.pinv<<NEWL>>cholesky = linpkg.cholesky<<NEWL>><<NEWL>>_restore_dict = {}<<NEWL>><<NEWL>>def register_func(name, func):<<NEWL>>    if name not in __all__:<<NEWL>>        raise ValueError(""{} not a dual function."".format(name))<<NEWL>>    f = sys._getframe(0).f_globals<<NEWL>>    _restore_dict[name] = f[name]<<NEWL>>    f[name] = func<<NEWL>><<NEWL>>def restore_func(name):<<NEWL>>    if name not in __all__:<<NEWL>>        raise ValueError(""{} not a dual function."".format(name))<<NEWL>>    try:<<NEWL>>        val = _restore_dict[name]<<NEWL>>    except KeyError:<<NEWL>>        return<<NEWL>>    else:<<NEWL>>        sys._getframe(0).f_globals[name] = val<<NEWL>><<NEWL>>def restore_all():<<NEWL>>    for name in _restore_dict.keys():<<NEWL>>        restore_func(name)"
368	jackson	0	"# mssql/__init__.py<<NEWL>># Copyright (C) 2005-2023 the SQLAlchemy authors and contributors<<NEWL>># <see AUTHORS file><<NEWL>>#<<NEWL>># This module is part of SQLAlchemy and is released under<<NEWL>># the MIT License: https://www.opensource.org/licenses/mit-license.php<<NEWL>># mypy: ignore-errors<<NEWL>><<NEWL>><<NEWL>>from . import base  # noqa<<NEWL>>from . import pymssql  # noqa<<NEWL>>from . import pyodbc  # noqa<<NEWL>>from .base import BIGINT<<NEWL>>from .base import BINARY<<NEWL>>from .base import BIT<<NEWL>>from .base import CHAR<<NEWL>>from .base import DATE<<NEWL>>from .base import DATETIME<<NEWL>>from .base import DATETIME2<<NEWL>>from .base import DATETIMEOFFSET<<NEWL>>from .base import DECIMAL<<NEWL>>from .base import FLOAT<<NEWL>>from .base import IMAGE<<NEWL>>from .base import INTEGER<<NEWL>>from .base import JSON<<NEWL>>from .base import MONEY<<NEWL>>from .base import NCHAR<<NEWL>>from .base import NTEXT<<NEWL>>from .base import NUMERIC<<NEWL>>from .base import NVARCHAR<<NEWL>>from .base import REAL<<NEWL>>from .base import ROWVERSION<<NEWL>>from .base import SMALLDATETIME<<NEWL>>from .base import SMALLINT<<NEWL>>from .base import SMALLMONEY<<NEWL>>from .base import SQL_VARIANT<<NEWL>>from .base import TEXT<<NEWL>>from .base import TIME<<NEWL>>from .base import TIMESTAMP<<NEWL>>from .base import TINYINT<<NEWL>>from .base import try_cast<<NEWL>>from .base import UNIQUEIDENTIFIER<<NEWL>>from .base import VARBINARY<<NEWL>>from .base import VARCHAR<<NEWL>>from .base import XML<<NEWL>><<NEWL>><<NEWL>>base.dialect = dialect = pyodbc.dialect<<NEWL>><<NEWL>><<NEWL>>__all__ = (<<NEWL>>    ""JSON"",<<NEWL>>    ""INTEGER"",<<NEWL>>    ""BIGINT"",<<NEWL>>    ""SMALLINT"",<<NEWL>>    ""TINYINT"",<<NEWL>>    ""VARCHAR"",<<NEWL>>    ""NVARCHAR"",<<NEWL>>    ""CHAR"",<<NEWL>>    ""NCHAR"",<<NEWL>>    ""TEXT"",<<NEWL>>    ""NTEXT"",<<NEWL>>    ""DECIMAL"",<<NEWL>>    ""NUMERIC"",<<NEWL>>    ""FLOAT"",<<NEWL>>    ""DATETIME"",<<NEWL>>    ""DATETIME2"",<<NEWL>>    ""DATETIMEOFFSET"",<<NEWL>>    ""DATE"",<<NEWL>>    ""TIME"",<<NEWL>>    ""SMALLDATETIME"",<<NEWL>>    ""BINARY"",<<NEWL>>    ""VARBINARY"",<<NEWL>>    ""BIT"",<<NEWL>>    ""REAL"",<<NEWL>>    ""IMAGE"",<<NEWL>>    ""TIMESTAMP"",<<NEWL>>    ""ROWVERSION"",<<NEWL>>    ""MONEY"",<<NEWL>>    ""SMALLMONEY"",<<NEWL>>    ""UNIQUEIDENTIFIER"",<<NEWL>>    ""SQL_VARIANT"",<<NEWL>>    ""XML"",<<NEWL>>    ""dialect"",<<NEWL>>    ""try_cast"",<<NEWL>>)"
368	donghui	0	"# mssql/__init__.py<<NEWL>># Copyright (C) 2005-2023 the SQLAlchemy authors and contributors<<NEWL>># <see AUTHORS file><<NEWL>>#<<NEWL>># This module is part of SQLAlchemy and is released under<<NEWL>># the MIT License: https://www.opensource.org/licenses/mit-license.php<<NEWL>># mypy: ignore-errors<<NEWL>><<NEWL>><<NEWL>>from . import base  # noqa<<NEWL>>from . import pymssql  # noqa<<NEWL>>from . import pyodbc  # noqa<<NEWL>>from .base import BIGINT<<NEWL>>from .base import BINARY<<NEWL>>from .base import BIT<<NEWL>>from .base import CHAR<<NEWL>>from .base import DATE<<NEWL>>from .base import DATETIME<<NEWL>>from .base import DATETIME2<<NEWL>>from .base import DATETIMEOFFSET<<NEWL>>from .base import DECIMAL<<NEWL>>from .base import FLOAT<<NEWL>>from .base import IMAGE<<NEWL>>from .base import INTEGER<<NEWL>>from .base import JSON<<NEWL>>from .base import MONEY<<NEWL>>from .base import NCHAR<<NEWL>>from .base import NTEXT<<NEWL>>from .base import NUMERIC<<NEWL>>from .base import NVARCHAR<<NEWL>>from .base import REAL<<NEWL>>from .base import ROWVERSION<<NEWL>>from .base import SMALLDATETIME<<NEWL>>from .base import SMALLINT<<NEWL>>from .base import SMALLMONEY<<NEWL>>from .base import SQL_VARIANT<<NEWL>>from .base import TEXT<<NEWL>>from .base import TIME<<NEWL>>from .base import TIMESTAMP<<NEWL>>from .base import TINYINT<<NEWL>>from .base import try_cast<<NEWL>>from .base import UNIQUEIDENTIFIER<<NEWL>>from .base import VARBINARY<<NEWL>>from .base import VARCHAR<<NEWL>>from .base import XML<<NEWL>><<NEWL>><<NEWL>>base.dialect = dialect = pyodbc.dialect<<NEWL>><<NEWL>><<NEWL>>__all__ = (<<NEWL>>    ""JSON"",<<NEWL>>    ""INTEGER"",<<NEWL>>    ""BIGINT"",<<NEWL>>    ""SMALLINT"",<<NEWL>>    ""TINYINT"",<<NEWL>>    ""VARCHAR"",<<NEWL>>    ""NVARCHAR"",<<NEWL>>    ""CHAR"",<<NEWL>>    ""NCHAR"",<<NEWL>>    ""TEXT"",<<NEWL>>    ""NTEXT"",<<NEWL>>    ""DECIMAL"",<<NEWL>>    ""NUMERIC"",<<NEWL>>    ""FLOAT"",<<NEWL>>    ""DATETIME"",<<NEWL>>    ""DATETIME2"",<<NEWL>>    ""DATETIMEOFFSET"",<<NEWL>>    ""DATE"",<<NEWL>>    ""TIME"",<<NEWL>>    ""SMALLDATETIME"",<<NEWL>>    ""BINARY"",<<NEWL>>    ""VARBINARY"",<<NEWL>>    ""BIT"",<<NEWL>>    ""REAL"",<<NEWL>>    ""IMAGE"",<<NEWL>>    ""TIMESTAMP"",<<NEWL>>    ""ROWVERSION"",<<NEWL>>    ""MONEY"",<<NEWL>>    ""SMALLMONEY"",<<NEWL>>    ""UNIQUEIDENTIFIER"",<<NEWL>>    ""SQL_VARIANT"",<<NEWL>>    ""XML"",<<NEWL>>    ""dialect"",<<NEWL>>    ""try_cast"",<<NEWL>>)"
339	jackson	0	"from fontTools.misc.textTools import safeEval<<NEWL>>from . import DefaultTable<<NEWL>>import struct<<NEWL>><<NEWL>><<NEWL>>GASP_SYMMETRIC_GRIDFIT = 0x0004<<NEWL>>GASP_SYMMETRIC_SMOOTHING = 0x0008<<NEWL>>GASP_DOGRAY = 0x0002<<NEWL>>GASP_GRIDFIT = 0x0001<<NEWL>><<NEWL>><<NEWL>>class table__g_a_s_p(DefaultTable.DefaultTable):<<NEWL>>    def decompile(self, data, ttFont):<<NEWL>>        self.version, numRanges = struct.unpack("">HH"", data[:4])<<NEWL>>        assert 0 <= self.version <= 1, ""unknown 'gasp' format: %s"" % self.version<<NEWL>>        data = data[4:]<<NEWL>>        self.gaspRange = {}<<NEWL>>        for i in range(numRanges):<<NEWL>>            rangeMaxPPEM, rangeGaspBehavior = struct.unpack("">HH"", data[:4])<<NEWL>>            self.gaspRange[int(rangeMaxPPEM)] = int(rangeGaspBehavior)<<NEWL>>            data = data[4:]<<NEWL>>        assert not data, ""too much data""<<NEWL>><<NEWL>>    def compile(self, ttFont):<<NEWL>>        version = 0  # ignore self.version<<NEWL>>        numRanges = len(self.gaspRange)<<NEWL>>        data = b""""<<NEWL>>        items = sorted(self.gaspRange.items())<<NEWL>>        for rangeMaxPPEM, rangeGaspBehavior in items:<<NEWL>>            data = data + struct.pack("">HH"", rangeMaxPPEM, rangeGaspBehavior)<<NEWL>>            if rangeGaspBehavior & ~(GASP_GRIDFIT | GASP_DOGRAY):<<NEWL>>                version = 1<<NEWL>>        data = struct.pack("">HH"", version, numRanges) + data<<NEWL>>        return data<<NEWL>><<NEWL>>    def toXML(self, writer, ttFont):<<NEWL>>        items = sorted(self.gaspRange.items())<<NEWL>>        for rangeMaxPPEM, rangeGaspBehavior in items:<<NEWL>>            writer.simpletag(<<NEWL>>                ""gaspRange"",<<NEWL>>                [<<NEWL>>                    (""rangeMaxPPEM"", rangeMaxPPEM),<<NEWL>>                    (""rangeGaspBehavior"", rangeGaspBehavior),<<NEWL>>                ],<<NEWL>>            )<<NEWL>>            writer.newline()<<NEWL>><<NEWL>>    def fromXML(self, name, attrs, content, ttFont):<<NEWL>>        if name != ""gaspRange"":<<NEWL>>            return<<NEWL>>        if not hasattr(self, ""gaspRange""):<<NEWL>>            self.gaspRange = {}<<NEWL>>        self.gaspRange[safeEval(attrs[""rangeMaxPPEM""])] = safeEval(<<NEWL>>            attrs[""rangeGaspBehavior""]<<NEWL>>        )"
339	donghui	0	"from fontTools.misc.textTools import safeEval<<NEWL>>from . import DefaultTable<<NEWL>>import struct<<NEWL>><<NEWL>><<NEWL>>GASP_SYMMETRIC_GRIDFIT = 0x0004<<NEWL>>GASP_SYMMETRIC_SMOOTHING = 0x0008<<NEWL>>GASP_DOGRAY = 0x0002<<NEWL>>GASP_GRIDFIT = 0x0001<<NEWL>><<NEWL>><<NEWL>>class table__g_a_s_p(DefaultTable.DefaultTable):<<NEWL>>    def decompile(self, data, ttFont):<<NEWL>>        self.version, numRanges = struct.unpack("">HH"", data[:4])<<NEWL>>        assert 0 <= self.version <= 1, ""unknown 'gasp' format: %s"" % self.version<<NEWL>>        data = data[4:]<<NEWL>>        self.gaspRange = {}<<NEWL>>        for i in range(numRanges):<<NEWL>>            rangeMaxPPEM, rangeGaspBehavior = struct.unpack("">HH"", data[:4])<<NEWL>>            self.gaspRange[int(rangeMaxPPEM)] = int(rangeGaspBehavior)<<NEWL>>            data = data[4:]<<NEWL>>        assert not data, ""too much data""<<NEWL>><<NEWL>>    def compile(self, ttFont):<<NEWL>>        version = 0  # ignore self.version<<NEWL>>        numRanges = len(self.gaspRange)<<NEWL>>        data = b""""<<NEWL>>        items = sorted(self.gaspRange.items())<<NEWL>>        for rangeMaxPPEM, rangeGaspBehavior in items:<<NEWL>>            data = data + struct.pack("">HH"", rangeMaxPPEM, rangeGaspBehavior)<<NEWL>>            if rangeGaspBehavior & ~(GASP_GRIDFIT | GASP_DOGRAY):<<NEWL>>                version = 1<<NEWL>>        data = struct.pack("">HH"", version, numRanges) + data<<NEWL>>        return data<<NEWL>><<NEWL>>    def toXML(self, writer, ttFont):<<NEWL>>        items = sorted(self.gaspRange.items())<<NEWL>>        for rangeMaxPPEM, rangeGaspBehavior in items:<<NEWL>>            writer.simpletag(<<NEWL>>                ""gaspRange"",<<NEWL>>                [<<NEWL>>                    (""rangeMaxPPEM"", rangeMaxPPEM),<<NEWL>>                    (""rangeGaspBehavior"", rangeGaspBehavior),<<NEWL>>                ],<<NEWL>>            )<<NEWL>>            writer.newline()<<NEWL>><<NEWL>>    def fromXML(self, name, attrs, content, ttFont):<<NEWL>>        if name != ""gaspRange"":<<NEWL>>            return<<NEWL>>        if not hasattr(self, ""gaspRange""):<<NEWL>>            self.gaspRange = {}<<NEWL>>        self.gaspRange[safeEval(attrs[""rangeMaxPPEM""])] = safeEval(<<NEWL>>            attrs[""rangeGaspBehavior""]<<NEWL>>        )"
279	jackson	4	"""""""Pillow (Fork of the Python Imaging Library)<<NEWL>><<NEWL>>Pillow is the friendly PIL fork by Alex Clark and Contributors.<<NEWL>>    https://github.com/python-pillow/Pillow/<<NEWL>><<NEWL>>Pillow is forked from PIL 1.1.7.<<NEWL>><<NEWL>>PIL is the Python Imaging Library by Fredrik Lundh and Contributors.<<NEWL>>Copyright (c) 1999 by Secret Labs AB.<<NEWL>><<NEWL>>Use PIL.__version__ for this Pillow version.<<NEWL>><<NEWL>>;-)<<NEWL>>""""""<<NEWL>><<NEWL>>from . import _version<<NEWL>><<NEWL>># VERSION was removed in Pillow 6.0.0.<<NEWL>># PILLOW_VERSION was removed in Pillow 9.0.0.<<NEWL>># Use __version__ instead.<<NEWL>>__version__ = _version.__version__<<NEWL>>del _version<<NEWL>><<NEWL>><<NEWL>>_plugins = [<<NEWL>>    ""BlpImagePlugin"",<<NEWL>>    ""BmpImagePlugin"",<<NEWL>>    ""BufrStubImagePlugin"",<<NEWL>>    ""CurImagePlugin"",<<NEWL>>    ""DcxImagePlugin"",<<NEWL>>    ""DdsImagePlugin"",<<NEWL>>    ""EpsImagePlugin"",<<NEWL>>    ""FitsImagePlugin"",<<NEWL>>    ""FitsStubImagePlugin"",<<NEWL>>    ""FliImagePlugin"",<<NEWL>>    ""FpxImagePlugin"",<<NEWL>>    ""FtexImagePlugin"",<<NEWL>>    ""GbrImagePlugin"",<<NEWL>>    ""GifImagePlugin"",<<NEWL>>    ""GribStubImagePlugin"",<<NEWL>>    ""Hdf5StubImagePlugin"",<<NEWL>>    ""IcnsImagePlugin"",<<NEWL>>    ""IcoImagePlugin"",<<NEWL>>    ""ImImagePlugin"",<<NEWL>>    ""ImtImagePlugin"",<<NEWL>>    ""IptcImagePlugin"",<<NEWL>>    ""JpegImagePlugin"",<<NEWL>>    ""Jpeg2KImagePlugin"",<<NEWL>>    ""McIdasImagePlugin"",<<NEWL>>    ""MicImagePlugin"",<<NEWL>>    ""MpegImagePlugin"",<<NEWL>>    ""MpoImagePlugin"",<<NEWL>>    ""MspImagePlugin"",<<NEWL>>    ""PalmImagePlugin"",<<NEWL>>    ""PcdImagePlugin"",<<NEWL>>    ""PcxImagePlugin"",<<NEWL>>    ""PdfImagePlugin"",<<NEWL>>    ""PixarImagePlugin"",<<NEWL>>    ""PngImagePlugin"",<<NEWL>>    ""PpmImagePlugin"",<<NEWL>>    ""PsdImagePlugin"",<<NEWL>>    ""SgiImagePlugin"",<<NEWL>>    ""SpiderImagePlugin"",<<NEWL>>    ""SunImagePlugin"",<<NEWL>>    ""TgaImagePlugin"",<<NEWL>>    ""TiffImagePlugin"",<<NEWL>>    ""WebPImagePlugin"",<<NEWL>>    ""WmfImagePlugin"",<<NEWL>>    ""XbmImagePlugin"",<<NEWL>>    ""XpmImagePlugin"",<<NEWL>>    ""XVThumbImagePlugin"",<<NEWL>>]<<NEWL>><<NEWL>><<NEWL>>class UnidentifiedImageError(OSError):<<NEWL>>    """"""<<NEWL>>    Raised in :py:meth:`PIL.Image.open` if an image cannot be opened and identified.<<NEWL>>    """"""<<NEWL>><<NEWL>>    pass"
279	donghui	2	"""""""Pillow (Fork of the Python Imaging Library)<<NEWL>><<NEWL>>Pillow is the friendly PIL fork by Alex Clark and Contributors.<<NEWL>>    https://github.com/python-pillow/Pillow/<<NEWL>><<NEWL>>Pillow is forked from PIL 1.1.7.<<NEWL>><<NEWL>>PIL is the Python Imaging Library by Fredrik Lundh and Contributors.<<NEWL>>Copyright (c) 1999 by Secret Labs AB.<<NEWL>><<NEWL>>Use PIL.__version__ for this Pillow version.<<NEWL>><<NEWL>>;-)<<NEWL>>""""""<<NEWL>><<NEWL>>from . import _version<<NEWL>><<NEWL>># VERSION was removed in Pillow 6.0.0.<<NEWL>># PILLOW_VERSION was removed in Pillow 9.0.0.<<NEWL>># Use __version__ instead.<<NEWL>>__version__ = _version.__version__<<NEWL>>del _version<<NEWL>><<NEWL>><<NEWL>>_plugins = [<<NEWL>>    ""BlpImagePlugin"",<<NEWL>>    ""BmpImagePlugin"",<<NEWL>>    ""BufrStubImagePlugin"",<<NEWL>>    ""CurImagePlugin"",<<NEWL>>    ""DcxImagePlugin"",<<NEWL>>    ""DdsImagePlugin"",<<NEWL>>    ""EpsImagePlugin"",<<NEWL>>    ""FitsImagePlugin"",<<NEWL>>    ""FitsStubImagePlugin"",<<NEWL>>    ""FliImagePlugin"",<<NEWL>>    ""FpxImagePlugin"",<<NEWL>>    ""FtexImagePlugin"",<<NEWL>>    ""GbrImagePlugin"",<<NEWL>>    ""GifImagePlugin"",<<NEWL>>    ""GribStubImagePlugin"",<<NEWL>>    ""Hdf5StubImagePlugin"",<<NEWL>>    ""IcnsImagePlugin"",<<NEWL>>    ""IcoImagePlugin"",<<NEWL>>    ""ImImagePlugin"",<<NEWL>>    ""ImtImagePlugin"",<<NEWL>>    ""IptcImagePlugin"",<<NEWL>>    ""JpegImagePlugin"",<<NEWL>>    ""Jpeg2KImagePlugin"",<<NEWL>>    ""McIdasImagePlugin"",<<NEWL>>    ""MicImagePlugin"",<<NEWL>>    ""MpegImagePlugin"",<<NEWL>>    ""MpoImagePlugin"",<<NEWL>>    ""MspImagePlugin"",<<NEWL>>    ""PalmImagePlugin"",<<NEWL>>    ""PcdImagePlugin"",<<NEWL>>    ""PcxImagePlugin"",<<NEWL>>    ""PdfImagePlugin"",<<NEWL>>    ""PixarImagePlugin"",<<NEWL>>    ""PngImagePlugin"",<<NEWL>>    ""PpmImagePlugin"",<<NEWL>>    ""PsdImagePlugin"",<<NEWL>>    ""SgiImagePlugin"",<<NEWL>>    ""SpiderImagePlugin"",<<NEWL>>    ""SunImagePlugin"",<<NEWL>>    ""TgaImagePlugin"",<<NEWL>>    ""TiffImagePlugin"",<<NEWL>>    ""WebPImagePlugin"",<<NEWL>>    ""WmfImagePlugin"",<<NEWL>>    ""XbmImagePlugin"",<<NEWL>>    ""XpmImagePlugin"",<<NEWL>>    ""XVThumbImagePlugin"",<<NEWL>>]<<NEWL>><<NEWL>><<NEWL>>class UnidentifiedImageError(OSError):<<NEWL>>    """"""<<NEWL>>    Raised in :py:meth:`PIL.Image.open` if an image cannot be opened and identified.<<NEWL>>    """"""<<NEWL>><<NEWL>>    pass"
269	jackson	3	"# -*- coding: utf-8 -*-<<NEWL>>""""""Payload system for IPython.<<NEWL>><<NEWL>>Authors:<<NEWL>><<NEWL>>* Fernando Perez<<NEWL>>* Brian Granger<<NEWL>>""""""<<NEWL>><<NEWL>>#-----------------------------------------------------------------------------<<NEWL>>#       Copyright (C) 2008-2011 The IPython Development Team<<NEWL>>#<<NEWL>>#  Distributed under the terms of the BSD License.  The full license is in<<NEWL>>#  the file COPYING, distributed as part of this software.<<NEWL>>#-----------------------------------------------------------------------------<<NEWL>><<NEWL>>#-----------------------------------------------------------------------------<<NEWL>># Imports<<NEWL>>#-----------------------------------------------------------------------------<<NEWL>><<NEWL>>from traitlets.config.configurable import Configurable<<NEWL>>from traitlets import List<<NEWL>><<NEWL>>#-----------------------------------------------------------------------------<<NEWL>># Main payload class<<NEWL>>#-----------------------------------------------------------------------------<<NEWL>><<NEWL>>class PayloadManager(Configurable):<<NEWL>><<NEWL>>    _payload = List([])<<NEWL>><<NEWL>>    def write_payload(self, data, single=True):<<NEWL>>        """"""Include or update the specified `data` payload in the PayloadManager.<<NEWL>><<NEWL>>        If a previous payload with the same source exists and `single` is True,<<NEWL>>        it will be overwritten with the new one.<<NEWL>>        """"""<<NEWL>><<NEWL>>        if not isinstance(data, dict):<<NEWL>>            raise TypeError('Each payload write must be a dict, got: %r' % data)<<NEWL>><<NEWL>>        if single and 'source' in data:<<NEWL>>            source = data['source']<<NEWL>>            for i, pl in enumerate(self._payload):<<NEWL>>                if 'source' in pl and pl['source'] == source:<<NEWL>>                    self._payload[i] = data<<NEWL>>                    return<<NEWL>><<NEWL>>        self._payload.append(data)<<NEWL>><<NEWL>>    def read_payload(self):<<NEWL>>        return self._payload<<NEWL>><<NEWL>>    def clear_payload(self):<<NEWL>>        self._payload = []"
269	donghui	1	"# -*- coding: utf-8 -*-<<NEWL>>""""""Payload system for IPython.<<NEWL>><<NEWL>>Authors:<<NEWL>><<NEWL>>* Fernando Perez<<NEWL>>* Brian Granger<<NEWL>>""""""<<NEWL>><<NEWL>>#-----------------------------------------------------------------------------<<NEWL>>#       Copyright (C) 2008-2011 The IPython Development Team<<NEWL>>#<<NEWL>>#  Distributed under the terms of the BSD License.  The full license is in<<NEWL>>#  the file COPYING, distributed as part of this software.<<NEWL>>#-----------------------------------------------------------------------------<<NEWL>><<NEWL>>#-----------------------------------------------------------------------------<<NEWL>># Imports<<NEWL>>#-----------------------------------------------------------------------------<<NEWL>><<NEWL>>from traitlets.config.configurable import Configurable<<NEWL>>from traitlets import List<<NEWL>><<NEWL>>#-----------------------------------------------------------------------------<<NEWL>># Main payload class<<NEWL>>#-----------------------------------------------------------------------------<<NEWL>><<NEWL>>class PayloadManager(Configurable):<<NEWL>><<NEWL>>    _payload = List([])<<NEWL>><<NEWL>>    def write_payload(self, data, single=True):<<NEWL>>        """"""Include or update the specified `data` payload in the PayloadManager.<<NEWL>><<NEWL>>        If a previous payload with the same source exists and `single` is True,<<NEWL>>        it will be overwritten with the new one.<<NEWL>>        """"""<<NEWL>><<NEWL>>        if not isinstance(data, dict):<<NEWL>>            raise TypeError('Each payload write must be a dict, got: %r' % data)<<NEWL>><<NEWL>>        if single and 'source' in data:<<NEWL>>            source = data['source']<<NEWL>>            for i, pl in enumerate(self._payload):<<NEWL>>                if 'source' in pl and pl['source'] == source:<<NEWL>>                    self._payload[i] = data<<NEWL>>                    return<<NEWL>><<NEWL>>        self._payload.append(data)<<NEWL>><<NEWL>>    def read_payload(self):<<NEWL>>        return self._payload<<NEWL>><<NEWL>>    def clear_payload(self):<<NEWL>>        self._payload = []"
329	jackson	1	"from splunk.persistconn.application import PersistentServerConnectionApplication<<NEWL>>import json<<NEWL>>import requests<<NEWL>>import logging<<NEWL>><<NEWL>><<NEWL>>class request(PersistentServerConnectionApplication):<<NEWL>>    def __init__(self, command_line, command_arg, logger=None):<<NEWL>>        super(PersistentServerConnectionApplication, self).__init__()<<NEWL>>        self.logger = logger<<NEWL>>        if self.logger == None:<<NEWL>>            self.logger = logging.getLogger(f""splunk.appserver.badmsc"")<<NEWL>><<NEWL>>        PersistentServerConnectionApplication.__init__(self)<<NEWL>><<NEWL>>    def handle(self, in_string):<<NEWL>>        args = json.loads(in_string)<<NEWL>><<NEWL>>        if args[""method""] != ""POST"":<<NEWL>>            self.logger.info(f""Method {args['method']} not allowed"")<<NEWL>>            return {<<NEWL>>                ""payload"": ""Method Not Allowed"",<<NEWL>>                ""status"": 405,<<NEWL>>                ""headers"": {""Allow"": ""POST""},<<NEWL>>            }<<NEWL>><<NEWL>>        try:<<NEWL>>            options = json.loads(args[""payload""])<<NEWL>>        except Exception as e:<<NEWL>>            self.logger.info(f""Invalid payload. {e}"")<<NEWL>>            return {""payload"": ""Invalid JSON payload"", ""status"": 400}<<NEWL>><<NEWL>>        self.logger.info(args[""payload""])<<NEWL>><<NEWL>>        # Handle local requests by adding FQDN and auth token<<NEWL>>        if options[""url""].startswith(""/services""):<<NEWL>>            options[""verify""] = False<<NEWL>>            options[""url""] = f""{args['server']['rest_uri']}{options['url']}""<<NEWL>>            options[""headers""][<<NEWL>>                ""Authorization""<<NEWL>>            ] = f""Splunk {args['session']['authtoken']}""<<NEWL>>        elif not (<<NEWL>>            options[""url""].startswith(""https://"")<<NEWL>>            or options[""url""].startswith(""http://"")<<NEWL>>        ):<<NEWL>>            options[""url""] = f""https://{options['url']}""<<NEWL>><<NEWL>>        try:<<NEWL>>            r = requests.request(**options)<<NEWL>>            self.logger.info(f""{r.status_code} {r.text}"")<<NEWL>>            return {""payload"": r.text, ""status"": r.status_code}<<NEWL>>        except Exception as e:<<NEWL>>            self.logger.info(f""Request failed. {e}"")<<NEWL>>            return {""payload"": str(e), ""status"": 500}"
329	donghui	1	"from splunk.persistconn.application import PersistentServerConnectionApplication<<NEWL>>import json<<NEWL>>import requests<<NEWL>>import logging<<NEWL>><<NEWL>><<NEWL>>class request(PersistentServerConnectionApplication):<<NEWL>>    def __init__(self, command_line, command_arg, logger=None):<<NEWL>>        super(PersistentServerConnectionApplication, self).__init__()<<NEWL>>        self.logger = logger<<NEWL>>        if self.logger == None:<<NEWL>>            self.logger = logging.getLogger(f""splunk.appserver.badmsc"")<<NEWL>><<NEWL>>        PersistentServerConnectionApplication.__init__(self)<<NEWL>><<NEWL>>    def handle(self, in_string):<<NEWL>>        args = json.loads(in_string)<<NEWL>><<NEWL>>        if args[""method""] != ""POST"":<<NEWL>>            self.logger.info(f""Method {args['method']} not allowed"")<<NEWL>>            return {<<NEWL>>                ""payload"": ""Method Not Allowed"",<<NEWL>>                ""status"": 405,<<NEWL>>                ""headers"": {""Allow"": ""POST""},<<NEWL>>            }<<NEWL>><<NEWL>>        try:<<NEWL>>            options = json.loads(args[""payload""])<<NEWL>>        except Exception as e:<<NEWL>>            self.logger.info(f""Invalid payload. {e}"")<<NEWL>>            return {""payload"": ""Invalid JSON payload"", ""status"": 400}<<NEWL>><<NEWL>>        self.logger.info(args[""payload""])<<NEWL>><<NEWL>>        # Handle local requests by adding FQDN and auth token<<NEWL>>        if options[""url""].startswith(""/services""):<<NEWL>>            options[""verify""] = False<<NEWL>>            options[""url""] = f""{args['server']['rest_uri']}{options['url']}""<<NEWL>>            options[""headers""][<<NEWL>>                ""Authorization""<<NEWL>>            ] = f""Splunk {args['session']['authtoken']}""<<NEWL>>        elif not (<<NEWL>>            options[""url""].startswith(""https://"")<<NEWL>>            or options[""url""].startswith(""http://"")<<NEWL>>        ):<<NEWL>>            options[""url""] = f""https://{options['url']}""<<NEWL>><<NEWL>>        try:<<NEWL>>            r = requests.request(**options)<<NEWL>>            self.logger.info(f""{r.status_code} {r.text}"")<<NEWL>>            return {""payload"": r.text, ""status"": r.status_code}<<NEWL>>        except Exception as e:<<NEWL>>            self.logger.info(f""Request failed. {e}"")<<NEWL>>            return {""payload"": str(e), ""status"": 500}"
378	jackson	0	"######################## BEGIN LICENSE BLOCK ########################<<NEWL>># The Original Code is mozilla.org code.<<NEWL>>#<<NEWL>># The Initial Developer of the Original Code is<<NEWL>># Netscape Communications Corporation.<<NEWL>># Portions created by the Initial Developer are Copyright (C) 1998<<NEWL>># the Initial Developer. All Rights Reserved.<<NEWL>>#<<NEWL>># Contributor(s):<<NEWL>>#   Mark Pilgrim - port to Python<<NEWL>>#<<NEWL>># This library is free software; you can redistribute it and/or<<NEWL>># modify it under the terms of the GNU Lesser General Public<<NEWL>># License as published by the Free Software Foundation; either<<NEWL>># version 2.1 of the License, or (at your option) any later version.<<NEWL>>#<<NEWL>># This library is distributed in the hope that it will be useful,<<NEWL>># but WITHOUT ANY WARRANTY; without even the implied warranty of<<NEWL>># MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU<<NEWL>># Lesser General Public License for more details.<<NEWL>>#<<NEWL>># You should have received a copy of the GNU Lesser General Public<<NEWL>># License along with this library; if not, write to the Free Software<<NEWL>># Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA<<NEWL>># 02110-1301  USA<<NEWL>>######################### END LICENSE BLOCK #########################<<NEWL>><<NEWL>>from .chardistribution import EUCTWDistributionAnalysis<<NEWL>>from .codingstatemachine import CodingStateMachine<<NEWL>>from .mbcharsetprober import MultiByteCharSetProber<<NEWL>>from .mbcssm import EUCTW_SM_MODEL<<NEWL>><<NEWL>><<NEWL>>class EUCTWProber(MultiByteCharSetProber):<<NEWL>>    def __init__(self) -> None:<<NEWL>>        super().__init__()<<NEWL>>        self.coding_sm = CodingStateMachine(EUCTW_SM_MODEL)<<NEWL>>        self.distribution_analyzer = EUCTWDistributionAnalysis()<<NEWL>>        self.reset()<<NEWL>><<NEWL>>    @property<<NEWL>>    def charset_name(self) -> str:<<NEWL>>        return ""EUC-TW""<<NEWL>><<NEWL>>    @property<<NEWL>>    def language(self) -> str:<<NEWL>>        return ""Taiwan"""
378	donghui	0	"######################## BEGIN LICENSE BLOCK ########################<<NEWL>># The Original Code is mozilla.org code.<<NEWL>>#<<NEWL>># The Initial Developer of the Original Code is<<NEWL>># Netscape Communications Corporation.<<NEWL>># Portions created by the Initial Developer are Copyright (C) 1998<<NEWL>># the Initial Developer. All Rights Reserved.<<NEWL>>#<<NEWL>># Contributor(s):<<NEWL>>#   Mark Pilgrim - port to Python<<NEWL>>#<<NEWL>># This library is free software; you can redistribute it and/or<<NEWL>># modify it under the terms of the GNU Lesser General Public<<NEWL>># License as published by the Free Software Foundation; either<<NEWL>># version 2.1 of the License, or (at your option) any later version.<<NEWL>>#<<NEWL>># This library is distributed in the hope that it will be useful,<<NEWL>># but WITHOUT ANY WARRANTY; without even the implied warranty of<<NEWL>># MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU<<NEWL>># Lesser General Public License for more details.<<NEWL>>#<<NEWL>># You should have received a copy of the GNU Lesser General Public<<NEWL>># License along with this library; if not, write to the Free Software<<NEWL>># Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA<<NEWL>># 02110-1301  USA<<NEWL>>######################### END LICENSE BLOCK #########################<<NEWL>><<NEWL>>from .chardistribution import EUCTWDistributionAnalysis<<NEWL>>from .codingstatemachine import CodingStateMachine<<NEWL>>from .mbcharsetprober import MultiByteCharSetProber<<NEWL>>from .mbcssm import EUCTW_SM_MODEL<<NEWL>><<NEWL>><<NEWL>>class EUCTWProber(MultiByteCharSetProber):<<NEWL>>    def __init__(self) -> None:<<NEWL>>        super().__init__()<<NEWL>>        self.coding_sm = CodingStateMachine(EUCTW_SM_MODEL)<<NEWL>>        self.distribution_analyzer = EUCTWDistributionAnalysis()<<NEWL>>        self.reset()<<NEWL>><<NEWL>>    @property<<NEWL>>    def charset_name(self) -> str:<<NEWL>>        return ""EUC-TW""<<NEWL>><<NEWL>>    @property<<NEWL>>    def language(self) -> str:<<NEWL>>        return ""Taiwan"""
436	jackson	1	"""""""xmlrpclib.Transport implementation<<NEWL>>""""""<<NEWL>><<NEWL>>import logging<<NEWL>>import urllib.parse<<NEWL>>import xmlrpc.client<<NEWL>>from typing import TYPE_CHECKING, Tuple<<NEWL>><<NEWL>>from pip._internal.exceptions import NetworkConnectionError<<NEWL>>from pip._internal.network.session import PipSession<<NEWL>>from pip._internal.network.utils import raise_for_status<<NEWL>><<NEWL>>if TYPE_CHECKING:<<NEWL>>    from xmlrpc.client import _HostType, _Marshallable<<NEWL>><<NEWL>>logger = logging.getLogger(__name__)<<NEWL>><<NEWL>><<NEWL>>class PipXmlrpcTransport(xmlrpc.client.Transport):<<NEWL>>    """"""Provide a `xmlrpclib.Transport` implementation via a `PipSession`<<NEWL>>    object.<<NEWL>>    """"""<<NEWL>><<NEWL>>    def __init__(<<NEWL>>        self, index_url: str, session: PipSession, use_datetime: bool = False<<NEWL>>    ) -> None:<<NEWL>>        super().__init__(use_datetime)<<NEWL>>        index_parts = urllib.parse.urlparse(index_url)<<NEWL>>        self._scheme = index_parts.scheme<<NEWL>>        self._session = session<<NEWL>><<NEWL>>    def request(<<NEWL>>        self,<<NEWL>>        host: ""_HostType"",<<NEWL>>        handler: str,<<NEWL>>        request_body: bytes,<<NEWL>>        verbose: bool = False,<<NEWL>>    ) -> Tuple[""_Marshallable"", ...]:<<NEWL>>        assert isinstance(host, str)<<NEWL>>        parts = (self._scheme, host, handler, None, None, None)<<NEWL>>        url = urllib.parse.urlunparse(parts)<<NEWL>>        try:<<NEWL>>            headers = {""Content-Type"": ""text/xml""}<<NEWL>>            response = self._session.post(<<NEWL>>                url,<<NEWL>>                data=request_body,<<NEWL>>                headers=headers,<<NEWL>>                stream=True,<<NEWL>>            )<<NEWL>>            raise_for_status(response)<<NEWL>>            self.verbose = verbose<<NEWL>>            return self.parse_response(response.raw)<<NEWL>>        except NetworkConnectionError as exc:<<NEWL>>            assert exc.response<<NEWL>>            logger.critical(<<NEWL>>                ""HTTP error %s while getting %s"",<<NEWL>>                exc.response.status_code,<<NEWL>>                url,<<NEWL>>            )<<NEWL>>            raise"
436	donghui	1	"""""""xmlrpclib.Transport implementation<<NEWL>>""""""<<NEWL>><<NEWL>>import logging<<NEWL>>import urllib.parse<<NEWL>>import xmlrpc.client<<NEWL>>from typing import TYPE_CHECKING, Tuple<<NEWL>><<NEWL>>from pip._internal.exceptions import NetworkConnectionError<<NEWL>>from pip._internal.network.session import PipSession<<NEWL>>from pip._internal.network.utils import raise_for_status<<NEWL>><<NEWL>>if TYPE_CHECKING:<<NEWL>>    from xmlrpc.client import _HostType, _Marshallable<<NEWL>><<NEWL>>logger = logging.getLogger(__name__)<<NEWL>><<NEWL>><<NEWL>>class PipXmlrpcTransport(xmlrpc.client.Transport):<<NEWL>>    """"""Provide a `xmlrpclib.Transport` implementation via a `PipSession`<<NEWL>>    object.<<NEWL>>    """"""<<NEWL>><<NEWL>>    def __init__(<<NEWL>>        self, index_url: str, session: PipSession, use_datetime: bool = False<<NEWL>>    ) -> None:<<NEWL>>        super().__init__(use_datetime)<<NEWL>>        index_parts = urllib.parse.urlparse(index_url)<<NEWL>>        self._scheme = index_parts.scheme<<NEWL>>        self._session = session<<NEWL>><<NEWL>>    def request(<<NEWL>>        self,<<NEWL>>        host: ""_HostType"",<<NEWL>>        handler: str,<<NEWL>>        request_body: bytes,<<NEWL>>        verbose: bool = False,<<NEWL>>    ) -> Tuple[""_Marshallable"", ...]:<<NEWL>>        assert isinstance(host, str)<<NEWL>>        parts = (self._scheme, host, handler, None, None, None)<<NEWL>>        url = urllib.parse.urlunparse(parts)<<NEWL>>        try:<<NEWL>>            headers = {""Content-Type"": ""text/xml""}<<NEWL>>            response = self._session.post(<<NEWL>>                url,<<NEWL>>                data=request_body,<<NEWL>>                headers=headers,<<NEWL>>                stream=True,<<NEWL>>            )<<NEWL>>            raise_for_status(response)<<NEWL>>            self.verbose = verbose<<NEWL>>            return self.parse_response(response.raw)<<NEWL>>        except NetworkConnectionError as exc:<<NEWL>>            assert exc.response<<NEWL>>            logger.critical(<<NEWL>>                ""HTTP error %s while getting %s"",<<NEWL>>                exc.response.status_code,<<NEWL>>                url,<<NEWL>>            )<<NEWL>>            raise"
467	jackson	2	"import json<<NEWL>><<NEWL>>from django import forms<<NEWL>>from django.core.exceptions import ValidationError<<NEWL>>from django.utils.translation import gettext_lazy as _<<NEWL>><<NEWL>>__all__ = [""HStoreField""]<<NEWL>><<NEWL>><<NEWL>>class HStoreField(forms.CharField):<<NEWL>>    """"""<<NEWL>>    A field for HStore data which accepts dictionary JSON input.<<NEWL>>    """"""<<NEWL>><<NEWL>>    widget = forms.Textarea<<NEWL>>    default_error_messages = {<<NEWL>>        ""invalid_json"": _(""Could not load JSON data.""),<<NEWL>>        ""invalid_format"": _(""Input must be a JSON dictionary.""),<<NEWL>>    }<<NEWL>><<NEWL>>    def prepare_value(self, value):<<NEWL>>        if isinstance(value, dict):<<NEWL>>            return json.dumps(value)<<NEWL>>        return value<<NEWL>><<NEWL>>    def to_python(self, value):<<NEWL>>        if not value:<<NEWL>>            return {}<<NEWL>>        if not isinstance(value, dict):<<NEWL>>            try:<<NEWL>>                value = json.loads(value)<<NEWL>>            except json.JSONDecodeError:<<NEWL>>                raise ValidationError(<<NEWL>>                    self.error_messages[""invalid_json""],<<NEWL>>                    code=""invalid_json"",<<NEWL>>                )<<NEWL>><<NEWL>>        if not isinstance(value, dict):<<NEWL>>            raise ValidationError(<<NEWL>>                self.error_messages[""invalid_format""],<<NEWL>>                code=""invalid_format"",<<NEWL>>            )<<NEWL>><<NEWL>>        # Cast everything to strings for ease.<<NEWL>>        for key, val in value.items():<<NEWL>>            if val is not None:<<NEWL>>                val = str(val)<<NEWL>>            value[key] = val<<NEWL>>        return value<<NEWL>><<NEWL>>    def has_changed(self, initial, data):<<NEWL>>        """"""<<NEWL>>        Return True if data differs from initial.<<NEWL>>        """"""<<NEWL>>        # For purposes of seeing whether something has changed, None is<<NEWL>>        # the same as an empty dict, if the data or initial value we get<<NEWL>>        # is None, replace it w/ {}.<<NEWL>>        initial_value = self.to_python(initial)<<NEWL>>        return super().has_changed(initial_value, data)"
467	donghui	2	"import json<<NEWL>><<NEWL>>from django import forms<<NEWL>>from django.core.exceptions import ValidationError<<NEWL>>from django.utils.translation import gettext_lazy as _<<NEWL>><<NEWL>>__all__ = [""HStoreField""]<<NEWL>><<NEWL>><<NEWL>>class HStoreField(forms.CharField):<<NEWL>>    """"""<<NEWL>>    A field for HStore data which accepts dictionary JSON input.<<NEWL>>    """"""<<NEWL>><<NEWL>>    widget = forms.Textarea<<NEWL>>    default_error_messages = {<<NEWL>>        ""invalid_json"": _(""Could not load JSON data.""),<<NEWL>>        ""invalid_format"": _(""Input must be a JSON dictionary.""),<<NEWL>>    }<<NEWL>><<NEWL>>    def prepare_value(self, value):<<NEWL>>        if isinstance(value, dict):<<NEWL>>            return json.dumps(value)<<NEWL>>        return value<<NEWL>><<NEWL>>    def to_python(self, value):<<NEWL>>        if not value:<<NEWL>>            return {}<<NEWL>>        if not isinstance(value, dict):<<NEWL>>            try:<<NEWL>>                value = json.loads(value)<<NEWL>>            except json.JSONDecodeError:<<NEWL>>                raise ValidationError(<<NEWL>>                    self.error_messages[""invalid_json""],<<NEWL>>                    code=""invalid_json"",<<NEWL>>                )<<NEWL>><<NEWL>>        if not isinstance(value, dict):<<NEWL>>            raise ValidationError(<<NEWL>>                self.error_messages[""invalid_format""],<<NEWL>>                code=""invalid_format"",<<NEWL>>            )<<NEWL>><<NEWL>>        # Cast everything to strings for ease.<<NEWL>>        for key, val in value.items():<<NEWL>>            if val is not None:<<NEWL>>                val = str(val)<<NEWL>>            value[key] = val<<NEWL>>        return value<<NEWL>><<NEWL>>    def has_changed(self, initial, data):<<NEWL>>        """"""<<NEWL>>        Return True if data differs from initial.<<NEWL>>        """"""<<NEWL>>        # For purposes of seeing whether something has changed, None is<<NEWL>>        # the same as an empty dict, if the data or initial value we get<<NEWL>>        # is None, replace it w/ {}.<<NEWL>>        initial_value = self.to_python(initial)<<NEWL>>        return super().has_changed(initial_value, data)"
494	jackson	2	"import _plotly_utils.basevalidators<<NEWL>><<NEWL>><<NEWL>>class InsidetextfontValidator(_plotly_utils.basevalidators.CompoundValidator):<<NEWL>>    def __init__(self, plotly_name=""insidetextfont"", parent_name=""funnel"", **kwargs):<<NEWL>>        super(InsidetextfontValidator, self).__init__(<<NEWL>>            plotly_name=plotly_name,<<NEWL>>            parent_name=parent_name,<<NEWL>>            data_class_str=kwargs.pop(""data_class_str"", ""Insidetextfont""),<<NEWL>>            data_docs=kwargs.pop(<<NEWL>>                ""data_docs"",<<NEWL>>                """"""<<NEWL>>            color<<NEWL>><<NEWL>>            colorsrc<<NEWL>>                Sets the source reference on Chart Studio Cloud<<NEWL>>                for `color`.<<NEWL>>            family<<NEWL>>                HTML font family - the typeface that will be<<NEWL>>                applied by the web browser. The web browser<<NEWL>>                will only be able to apply a font if it is<<NEWL>>                available on the system which it operates.<<NEWL>>                Provide multiple font families, separated by<<NEWL>>                commas, to indicate the preference in which to<<NEWL>>                apply fonts if they aren't available on the<<NEWL>>                system. The Chart Studio Cloud (at<<NEWL>>                https://chart-studio.plotly.com or on-premise)<<NEWL>>                generates images on a server, where only a<<NEWL>>                select number of fonts are installed and<<NEWL>>                supported. These include ""Arial"", ""Balto"",<<NEWL>>                ""Courier New"", ""Droid Sans"",, ""Droid Serif"",<<NEWL>>                ""Droid Sans Mono"", ""Gravitas One"", ""Old<<NEWL>>                Standard TT"", ""Open Sans"", ""Overpass"", ""PT Sans<<NEWL>>                Narrow"", ""Raleway"", ""Times New Roman"".<<NEWL>>            familysrc<<NEWL>>                Sets the source reference on Chart Studio Cloud<<NEWL>>                for `family`.<<NEWL>>            size<<NEWL>><<NEWL>>            sizesrc<<NEWL>>                Sets the source reference on Chart Studio Cloud<<NEWL>>                for `size`.<<NEWL>>"""""",<<NEWL>>            ),<<NEWL>>            **kwargs,<<NEWL>>        )"
494	donghui	1	"import _plotly_utils.basevalidators<<NEWL>><<NEWL>><<NEWL>>class InsidetextfontValidator(_plotly_utils.basevalidators.CompoundValidator):<<NEWL>>    def __init__(self, plotly_name=""insidetextfont"", parent_name=""funnel"", **kwargs):<<NEWL>>        super(InsidetextfontValidator, self).__init__(<<NEWL>>            plotly_name=plotly_name,<<NEWL>>            parent_name=parent_name,<<NEWL>>            data_class_str=kwargs.pop(""data_class_str"", ""Insidetextfont""),<<NEWL>>            data_docs=kwargs.pop(<<NEWL>>                ""data_docs"",<<NEWL>>                """"""<<NEWL>>            color<<NEWL>><<NEWL>>            colorsrc<<NEWL>>                Sets the source reference on Chart Studio Cloud<<NEWL>>                for `color`.<<NEWL>>            family<<NEWL>>                HTML font family - the typeface that will be<<NEWL>>                applied by the web browser. The web browser<<NEWL>>                will only be able to apply a font if it is<<NEWL>>                available on the system which it operates.<<NEWL>>                Provide multiple font families, separated by<<NEWL>>                commas, to indicate the preference in which to<<NEWL>>                apply fonts if they aren't available on the<<NEWL>>                system. The Chart Studio Cloud (at<<NEWL>>                https://chart-studio.plotly.com or on-premise)<<NEWL>>                generates images on a server, where only a<<NEWL>>                select number of fonts are installed and<<NEWL>>                supported. These include ""Arial"", ""Balto"",<<NEWL>>                ""Courier New"", ""Droid Sans"",, ""Droid Serif"",<<NEWL>>                ""Droid Sans Mono"", ""Gravitas One"", ""Old<<NEWL>>                Standard TT"", ""Open Sans"", ""Overpass"", ""PT Sans<<NEWL>>                Narrow"", ""Raleway"", ""Times New Roman"".<<NEWL>>            familysrc<<NEWL>>                Sets the source reference on Chart Studio Cloud<<NEWL>>                for `family`.<<NEWL>>            size<<NEWL>><<NEWL>>            sizesrc<<NEWL>>                Sets the source reference on Chart Studio Cloud<<NEWL>>                for `size`.<<NEWL>>"""""",<<NEWL>>            ),<<NEWL>>            **kwargs,<<NEWL>>        )"
453	jackson	0	"from node import HuffmanNode<<NEWL>><<NEWL>><<NEWL>>class Huffman():<<NEWL>><<NEWL>>    def __init__(self, s:str) -> None:<<NEWL>>        self.__weights = self.__get_weights(s)<<NEWL>>        self.__buffer = [b' ' for _ in range(round(len(self.__weights)))]<<NEWL>>        self.__tree = self.__build_huffman_tree(self.__weights)<<NEWL>>        self.__code = self.__build_code(self.__tree)<<NEWL>><<NEWL>>    @property<<NEWL>>    def weights(self):<<NEWL>>        return self.__weights<<NEWL>><<NEWL>>    @property<<NEWL>>    def tree(self):<<NEWL>>        return self.__tree<<NEWL>><<NEWL>>    @property<<NEWL>>    def code(self):<<NEWL>>        return self.__code<<NEWL>><<NEWL>>    def __get_weights(self, s:str) -> dict:<<NEWL>>        weights = dict()<<NEWL>>        for i in s:<<NEWL>>            weights[i] = weights.get(i, 0)+1<<NEWL>>        return weights<<NEWL>>    <<NEWL>>    def __build_huffman_tree(self, weights:dict):<<NEWL>>        nodes = [HuffmanNode(value, weight) for value,weight in weights.items()]<<NEWL>>        while len(nodes) > 1:<<NEWL>>            nodes.sort(key=lambda node:node.weight, reverse=True)<<NEWL>>            c = HuffmanNode(value=None, weight=(nodes[-1].weight+nodes[-2].weight))<<NEWL>>            c.left_node = nodes.pop()<<NEWL>>            c.right_node = nodes.pop()<<NEWL>>            nodes.append(c)<<NEWL>>        return nodes[0]<<NEWL>><<NEWL>>    def __build_code(self, tree):<<NEWL>>        <<NEWL>>        def func(tree:HuffmanNode, length:int):<<NEWL>>            nonlocal code, self<<NEWL>>            node = tree<<NEWL>>            if not node:<<NEWL>>                return<<NEWL>>            elif node.value:<<NEWL>>                code[node.value] = b''.join( self.__buffer[:length] )<<NEWL>>                return<<NEWL>>            self.__buffer[length] = b'0'<<NEWL>>            func(node.left_node, length+1)<<NEWL>>            self.__buffer[length] = b'1'<<NEWL>>            func(node.right_node, length+1)<<NEWL>>        <<NEWL>>        code = dict()<<NEWL>>        func(tree, 0)<<NEWL>>        return code<<NEWL>><<NEWL>><<NEWL>>if __name__ == ""__main__"":<<NEWL>>    s = ""aabbccdddeefgenajojfonadkjfwqnioaerweggrefdsfassasdfgr""<<NEWL>><<NEWL>>    huffman = Huffman(s)<<NEWL>><<NEWL>>    print(huffman.weights)<<NEWL>>    print(huffman.code)"
453	donghui	0	"from node import HuffmanNode<<NEWL>><<NEWL>><<NEWL>>class Huffman():<<NEWL>><<NEWL>>    def __init__(self, s:str) -> None:<<NEWL>>        self.__weights = self.__get_weights(s)<<NEWL>>        self.__buffer = [b' ' for _ in range(round(len(self.__weights)))]<<NEWL>>        self.__tree = self.__build_huffman_tree(self.__weights)<<NEWL>>        self.__code = self.__build_code(self.__tree)<<NEWL>><<NEWL>>    @property<<NEWL>>    def weights(self):<<NEWL>>        return self.__weights<<NEWL>><<NEWL>>    @property<<NEWL>>    def tree(self):<<NEWL>>        return self.__tree<<NEWL>><<NEWL>>    @property<<NEWL>>    def code(self):<<NEWL>>        return self.__code<<NEWL>><<NEWL>>    def __get_weights(self, s:str) -> dict:<<NEWL>>        weights = dict()<<NEWL>>        for i in s:<<NEWL>>            weights[i] = weights.get(i, 0)+1<<NEWL>>        return weights<<NEWL>>    <<NEWL>>    def __build_huffman_tree(self, weights:dict):<<NEWL>>        nodes = [HuffmanNode(value, weight) for value,weight in weights.items()]<<NEWL>>        while len(nodes) > 1:<<NEWL>>            nodes.sort(key=lambda node:node.weight, reverse=True)<<NEWL>>            c = HuffmanNode(value=None, weight=(nodes[-1].weight+nodes[-2].weight))<<NEWL>>            c.left_node = nodes.pop()<<NEWL>>            c.right_node = nodes.pop()<<NEWL>>            nodes.append(c)<<NEWL>>        return nodes[0]<<NEWL>><<NEWL>>    def __build_code(self, tree):<<NEWL>>        <<NEWL>>        def func(tree:HuffmanNode, length:int):<<NEWL>>            nonlocal code, self<<NEWL>>            node = tree<<NEWL>>            if not node:<<NEWL>>                return<<NEWL>>            elif node.value:<<NEWL>>                code[node.value] = b''.join( self.__buffer[:length] )<<NEWL>>                return<<NEWL>>            self.__buffer[length] = b'0'<<NEWL>>            func(node.left_node, length+1)<<NEWL>>            self.__buffer[length] = b'1'<<NEWL>>            func(node.right_node, length+1)<<NEWL>>        <<NEWL>>        code = dict()<<NEWL>>        func(tree, 0)<<NEWL>>        return code<<NEWL>><<NEWL>><<NEWL>>if __name__ == ""__main__"":<<NEWL>>    s = ""aabbccdddeefgenajojfonadkjfwqnioaerweggrefdsfassasdfgr""<<NEWL>><<NEWL>>    huffman = Huffman(s)<<NEWL>><<NEWL>>    print(huffman.weights)<<NEWL>>    print(huffman.code)"
513	jackson	1	"# coding: utf8<<NEWL>>from __future__ import unicode_literals<<NEWL>><<NEWL>>from ...attrs import LIKE_NUM<<NEWL>><<NEWL>>_num_words = [<<NEWL>>    ""אפס"",<<NEWL>>    ""אחד"",<<NEWL>>    ""אחת"",<<NEWL>>    ""שתיים"",<<NEWL>>    ""שתים"",<<NEWL>>    ""שניים"",<<NEWL>>    ""שנים"",<<NEWL>>    ""שלוש"",<<NEWL>>    ""שלושה"",<<NEWL>>    ""ארבע"",<<NEWL>>    ""ארבעה"",<<NEWL>>    ""חמש"",<<NEWL>>    ""חמישה"",<<NEWL>>    ""שש"",<<NEWL>>    ""שישה"",<<NEWL>>    ""שבע"",<<NEWL>>    ""שבעה"",<<NEWL>>    ""שמונה"",<<NEWL>>    ""תשע"",<<NEWL>>    ""תשעה"",<<NEWL>>    ""עשר"",<<NEWL>>    ""עשרה"",<<NEWL>>    ""אחד עשר"",<<NEWL>>    ""אחת עשרה"",<<NEWL>>    ""שנים עשר"",<<NEWL>>    ""שתים עשרה"",<<NEWL>>    ""שלושה עשר"",<<NEWL>>    ""שלוש עשרה"",<<NEWL>>    ""ארבעה עשר"",<<NEWL>>    ""ארבע עשרה"",<<NEWL>>    ""חמישה עשר"",<<NEWL>>    ""חמש עשרה"",<<NEWL>>    ""ששה עשר"",<<NEWL>>    ""שש עשרה"",<<NEWL>>    ""שבעה עשר"",<<NEWL>>    ""שבע עשרה"",<<NEWL>>    ""שמונה עשר"",<<NEWL>>    ""שמונה עשרה"",<<NEWL>>    ""תשעה עשר"",<<NEWL>>    ""תשע עשרה"",<<NEWL>>    ""עשרים"",<<NEWL>>    ""שלושים"",<<NEWL>>    ""ארבעים"",<<NEWL>>    ""חמישים"",<<NEWL>>    ""שישים"",<<NEWL>>    ""שבעים"",<<NEWL>>    ""שמונים"",<<NEWL>>    ""תשעים"",<<NEWL>>    ""מאה"",<<NEWL>>    ""אלף"",<<NEWL>>    ""מליון"",<<NEWL>>    ""מליארד"",<<NEWL>>    ""טריליון"",<<NEWL>>]<<NEWL>><<NEWL>><<NEWL>>_ordinal_words = [<<NEWL>>    ""ראשון"",<<NEWL>>    ""שני"",<<NEWL>>    ""שלישי"",<<NEWL>>    ""רביעי"",<<NEWL>>    ""חמישי"",<<NEWL>>    ""שישי"",<<NEWL>>    ""שביעי"",<<NEWL>>    ""שמיני"",<<NEWL>>    ""תשיעי"",<<NEWL>>    ""עשירי"",<<NEWL>>]<<NEWL>><<NEWL>>def like_num(text):<<NEWL>>    if text.startswith((""+"", ""-"", ""±"", ""~"")):<<NEWL>>        text = text[1:]<<NEWL>>    text = text.replace("","", """").replace(""."", """")<<NEWL>>    if text.isdigit():<<NEWL>>        return True<<NEWL>><<NEWL>>    if text.count(""/"") == 1:<<NEWL>>        num, denom = text.split(""/"")<<NEWL>>        if num.isdigit() and denom.isdigit():<<NEWL>>            return True<<NEWL>>    <<NEWL>>    if text in _num_words:<<NEWL>>        return True<<NEWL>><<NEWL>>    # CHeck ordinal number<<NEWL>>    if text in _ordinal_words:<<NEWL>>        return True<<NEWL>>    return False<<NEWL>><<NEWL>><<NEWL>>LEX_ATTRS = {LIKE_NUM: like_num}"
513	donghui	1	"# coding: utf8<<NEWL>>from __future__ import unicode_literals<<NEWL>><<NEWL>>from ...attrs import LIKE_NUM<<NEWL>><<NEWL>>_num_words = [<<NEWL>>    ""אפס"",<<NEWL>>    ""אחד"",<<NEWL>>    ""אחת"",<<NEWL>>    ""שתיים"",<<NEWL>>    ""שתים"",<<NEWL>>    ""שניים"",<<NEWL>>    ""שנים"",<<NEWL>>    ""שלוש"",<<NEWL>>    ""שלושה"",<<NEWL>>    ""ארבע"",<<NEWL>>    ""ארבעה"",<<NEWL>>    ""חמש"",<<NEWL>>    ""חמישה"",<<NEWL>>    ""שש"",<<NEWL>>    ""שישה"",<<NEWL>>    ""שבע"",<<NEWL>>    ""שבעה"",<<NEWL>>    ""שמונה"",<<NEWL>>    ""תשע"",<<NEWL>>    ""תשעה"",<<NEWL>>    ""עשר"",<<NEWL>>    ""עשרה"",<<NEWL>>    ""אחד עשר"",<<NEWL>>    ""אחת עשרה"",<<NEWL>>    ""שנים עשר"",<<NEWL>>    ""שתים עשרה"",<<NEWL>>    ""שלושה עשר"",<<NEWL>>    ""שלוש עשרה"",<<NEWL>>    ""ארבעה עשר"",<<NEWL>>    ""ארבע עשרה"",<<NEWL>>    ""חמישה עשר"",<<NEWL>>    ""חמש עשרה"",<<NEWL>>    ""ששה עשר"",<<NEWL>>    ""שש עשרה"",<<NEWL>>    ""שבעה עשר"",<<NEWL>>    ""שבע עשרה"",<<NEWL>>    ""שמונה עשר"",<<NEWL>>    ""שמונה עשרה"",<<NEWL>>    ""תשעה עשר"",<<NEWL>>    ""תשע עשרה"",<<NEWL>>    ""עשרים"",<<NEWL>>    ""שלושים"",<<NEWL>>    ""ארבעים"",<<NEWL>>    ""חמישים"",<<NEWL>>    ""שישים"",<<NEWL>>    ""שבעים"",<<NEWL>>    ""שמונים"",<<NEWL>>    ""תשעים"",<<NEWL>>    ""מאה"",<<NEWL>>    ""אלף"",<<NEWL>>    ""מליון"",<<NEWL>>    ""מליארד"",<<NEWL>>    ""טריליון"",<<NEWL>>]<<NEWL>><<NEWL>><<NEWL>>_ordinal_words = [<<NEWL>>    ""ראשון"",<<NEWL>>    ""שני"",<<NEWL>>    ""שלישי"",<<NEWL>>    ""רביעי"",<<NEWL>>    ""חמישי"",<<NEWL>>    ""שישי"",<<NEWL>>    ""שביעי"",<<NEWL>>    ""שמיני"",<<NEWL>>    ""תשיעי"",<<NEWL>>    ""עשירי"",<<NEWL>>]<<NEWL>><<NEWL>>def like_num(text):<<NEWL>>    if text.startswith((""+"", ""-"", ""±"", ""~"")):<<NEWL>>        text = text[1:]<<NEWL>>    text = text.replace("","", """").replace(""."", """")<<NEWL>>    if text.isdigit():<<NEWL>>        return True<<NEWL>><<NEWL>>    if text.count(""/"") == 1:<<NEWL>>        num, denom = text.split(""/"")<<NEWL>>        if num.isdigit() and denom.isdigit():<<NEWL>>            return True<<NEWL>>    <<NEWL>>    if text in _num_words:<<NEWL>>        return True<<NEWL>><<NEWL>>    # CHeck ordinal number<<NEWL>>    if text in _ordinal_words:<<NEWL>>        return True<<NEWL>>    return False<<NEWL>><<NEWL>><<NEWL>>LEX_ATTRS = {LIKE_NUM: like_num}"
402	jackson	4	"# This file is distributed under the same license as the Django package.<<NEWL>>#<<NEWL>># The *_FORMAT strings use the Django date format syntax,<<NEWL>># see https://docs.djangoproject.com/en/dev/ref/templates/builtins/#date<<NEWL>>DATE_FORMAT = ""l, j F, Y""<<NEWL>>TIME_FORMAT = ""h:i a""<<NEWL>>DATETIME_FORMAT = ""j F, Y h:i a""<<NEWL>>YEAR_MONTH_FORMAT = ""F, Y""<<NEWL>>MONTH_DAY_FORMAT = ""j F""<<NEWL>>SHORT_DATE_FORMAT = ""j.M.Y""<<NEWL>>SHORT_DATETIME_FORMAT = ""j.M.Y H:i""<<NEWL>>FIRST_DAY_OF_WEEK = 1  # (Monday)<<NEWL>><<NEWL>># The *_INPUT_FORMATS strings use the Python strftime format syntax,<<NEWL>># see https://docs.python.org/library/datetime.html#strftime-strptime-behavior<<NEWL>># Kept ISO formats as they are in first position<<NEWL>>DATE_INPUT_FORMATS = [<<NEWL>>    ""%Y-%m-%d"",  # '2006-10-25'<<NEWL>>    ""%m/%d/%Y"",  # '10/25/2006'<<NEWL>>    ""%m/%d/%y"",  # '10/25/06'<<NEWL>>    ""%d.%m.%Y"",  # '25.10.2006'<<NEWL>>    ""%d.%m.%y"",  # '25.10.06'<<NEWL>>    # ""%d %b %Y"",  # '25 Oct 2006'<<NEWL>>    # ""%d %b, %Y"",  # '25 Oct, 2006'<<NEWL>>    # ""%d %b. %Y"",  # '25 Oct. 2006'<<NEWL>>    # ""%d %B %Y"",  # '25 October 2006'<<NEWL>>    # ""%d %B, %Y"",  # '25 October, 2006'<<NEWL>>]<<NEWL>>DATETIME_INPUT_FORMATS = [<<NEWL>>    ""%Y-%m-%d %H:%M:%S"",  # '2006-10-25 14:30:59'<<NEWL>>    ""%Y-%m-%d %H:%M:%S.%f"",  # '2006-10-25 14:30:59.000200'<<NEWL>>    ""%Y-%m-%d %H:%M"",  # '2006-10-25 14:30'<<NEWL>>    ""%d.%m.%Y %H:%M:%S"",  # '25.10.2006 14:30:59'<<NEWL>>    ""%d.%m.%Y %H:%M:%S.%f"",  # '25.10.2006 14:30:59.000200'<<NEWL>>    ""%d.%m.%Y %H:%M"",  # '25.10.2006 14:30'<<NEWL>>    ""%d.%m.%y %H:%M:%S"",  # '25.10.06 14:30:59'<<NEWL>>    ""%d.%m.%y %H:%M:%S.%f"",  # '25.10.06 14:30:59.000200'<<NEWL>>    ""%d.%m.%y %H:%M"",  # '25.10.06 14:30'<<NEWL>>    ""%m/%d/%Y %H:%M:%S"",  # '10/25/2006 14:30:59'<<NEWL>>    ""%m/%d/%Y %H:%M:%S.%f"",  # '10/25/2006 14:30:59.000200'<<NEWL>>    ""%m/%d/%Y %H:%M"",  # '10/25/2006 14:30'<<NEWL>>    ""%m/%d/%y %H:%M:%S"",  # '10/25/06 14:30:59'<<NEWL>>    ""%m/%d/%y %H:%M:%S.%f"",  # '10/25/06 14:30:59.000200'<<NEWL>>    ""%m/%d/%y %H:%M"",  # '10/25/06 14:30'<<NEWL>>]<<NEWL>>DECIMAL_SEPARATOR = "".""<<NEWL>>THOUSAND_SEPARATOR = "" ""<<NEWL>>NUMBER_GROUPING = 3"
402	donghui	1	"# This file is distributed under the same license as the Django package.<<NEWL>>#<<NEWL>># The *_FORMAT strings use the Django date format syntax,<<NEWL>># see https://docs.djangoproject.com/en/dev/ref/templates/builtins/#date<<NEWL>>DATE_FORMAT = ""l, j F, Y""<<NEWL>>TIME_FORMAT = ""h:i a""<<NEWL>>DATETIME_FORMAT = ""j F, Y h:i a""<<NEWL>>YEAR_MONTH_FORMAT = ""F, Y""<<NEWL>>MONTH_DAY_FORMAT = ""j F""<<NEWL>>SHORT_DATE_FORMAT = ""j.M.Y""<<NEWL>>SHORT_DATETIME_FORMAT = ""j.M.Y H:i""<<NEWL>>FIRST_DAY_OF_WEEK = 1  # (Monday)<<NEWL>><<NEWL>># The *_INPUT_FORMATS strings use the Python strftime format syntax,<<NEWL>># see https://docs.python.org/library/datetime.html#strftime-strptime-behavior<<NEWL>># Kept ISO formats as they are in first position<<NEWL>>DATE_INPUT_FORMATS = [<<NEWL>>    ""%Y-%m-%d"",  # '2006-10-25'<<NEWL>>    ""%m/%d/%Y"",  # '10/25/2006'<<NEWL>>    ""%m/%d/%y"",  # '10/25/06'<<NEWL>>    ""%d.%m.%Y"",  # '25.10.2006'<<NEWL>>    ""%d.%m.%y"",  # '25.10.06'<<NEWL>>    # ""%d %b %Y"",  # '25 Oct 2006'<<NEWL>>    # ""%d %b, %Y"",  # '25 Oct, 2006'<<NEWL>>    # ""%d %b. %Y"",  # '25 Oct. 2006'<<NEWL>>    # ""%d %B %Y"",  # '25 October 2006'<<NEWL>>    # ""%d %B, %Y"",  # '25 October, 2006'<<NEWL>>]<<NEWL>>DATETIME_INPUT_FORMATS = [<<NEWL>>    ""%Y-%m-%d %H:%M:%S"",  # '2006-10-25 14:30:59'<<NEWL>>    ""%Y-%m-%d %H:%M:%S.%f"",  # '2006-10-25 14:30:59.000200'<<NEWL>>    ""%Y-%m-%d %H:%M"",  # '2006-10-25 14:30'<<NEWL>>    ""%d.%m.%Y %H:%M:%S"",  # '25.10.2006 14:30:59'<<NEWL>>    ""%d.%m.%Y %H:%M:%S.%f"",  # '25.10.2006 14:30:59.000200'<<NEWL>>    ""%d.%m.%Y %H:%M"",  # '25.10.2006 14:30'<<NEWL>>    ""%d.%m.%y %H:%M:%S"",  # '25.10.06 14:30:59'<<NEWL>>    ""%d.%m.%y %H:%M:%S.%f"",  # '25.10.06 14:30:59.000200'<<NEWL>>    ""%d.%m.%y %H:%M"",  # '25.10.06 14:30'<<NEWL>>    ""%m/%d/%Y %H:%M:%S"",  # '10/25/2006 14:30:59'<<NEWL>>    ""%m/%d/%Y %H:%M:%S.%f"",  # '10/25/2006 14:30:59.000200'<<NEWL>>    ""%m/%d/%Y %H:%M"",  # '10/25/2006 14:30'<<NEWL>>    ""%m/%d/%y %H:%M:%S"",  # '10/25/06 14:30:59'<<NEWL>>    ""%m/%d/%y %H:%M:%S.%f"",  # '10/25/06 14:30:59.000200'<<NEWL>>    ""%m/%d/%y %H:%M"",  # '10/25/06 14:30'<<NEWL>>]<<NEWL>>DECIMAL_SEPARATOR = "".""<<NEWL>>THOUSAND_SEPARATOR = "" ""<<NEWL>>NUMBER_GROUPING = 3"
480	jackson	1	"""""""<<NEWL>>Customized Mixin2to3 support:<<NEWL>><<NEWL>> - adds support for converting doctests<<NEWL>><<NEWL>><<NEWL>>This module raises an ImportError on Python 2.<<NEWL>>""""""<<NEWL>><<NEWL>>from distutils.util import Mixin2to3 as _Mixin2to3<<NEWL>>from distutils import log<<NEWL>>from lib2to3.refactor import RefactoringTool, get_fixers_from_package<<NEWL>><<NEWL>>import setuptools<<NEWL>><<NEWL>><<NEWL>>class DistutilsRefactoringTool(RefactoringTool):<<NEWL>>    def log_error(self, msg, *args, **kw):<<NEWL>>        log.error(msg, *args)<<NEWL>><<NEWL>>    def log_message(self, msg, *args):<<NEWL>>        log.info(msg, *args)<<NEWL>><<NEWL>>    def log_debug(self, msg, *args):<<NEWL>>        log.debug(msg, *args)<<NEWL>><<NEWL>><<NEWL>>class Mixin2to3(_Mixin2to3):<<NEWL>>    def run_2to3(self, files, doctests=False):<<NEWL>>        # See of the distribution option has been set, otherwise check the<<NEWL>>        # setuptools default.<<NEWL>>        if self.distribution.use_2to3 is not True:<<NEWL>>            return<<NEWL>>        if not files:<<NEWL>>            return<<NEWL>>        log.info(""Fixing "" + "" "".join(files))<<NEWL>>        self.__build_fixer_names()<<NEWL>>        self.__exclude_fixers()<<NEWL>>        if doctests:<<NEWL>>            if setuptools.run_2to3_on_doctests:<<NEWL>>                r = DistutilsRefactoringTool(self.fixer_names)<<NEWL>>                r.refactor(files, write=True, doctests_only=True)<<NEWL>>        else:<<NEWL>>            _Mixin2to3.run_2to3(self, files)<<NEWL>><<NEWL>>    def __build_fixer_names(self):<<NEWL>>        if self.fixer_names:<<NEWL>>            return<<NEWL>>        self.fixer_names = []<<NEWL>>        for p in setuptools.lib2to3_fixer_packages:<<NEWL>>            self.fixer_names.extend(get_fixers_from_package(p))<<NEWL>>        if self.distribution.use_2to3_fixers is not None:<<NEWL>>            for p in self.distribution.use_2to3_fixers:<<NEWL>>                self.fixer_names.extend(get_fixers_from_package(p))<<NEWL>><<NEWL>>    def __exclude_fixers(self):<<NEWL>>        excluded_fixers = getattr(self, 'exclude_fixers', [])<<NEWL>>        if self.distribution.use_2to3_exclude_fixers is not None:<<NEWL>>            excluded_fixers.extend(self.distribution.use_2to3_exclude_fixers)<<NEWL>>        for fixer_name in excluded_fixers:<<NEWL>>            if fixer_name in self.fixer_names:<<NEWL>>                self.fixer_names.remove(fixer_name)"
480	donghui	1	"""""""<<NEWL>>Customized Mixin2to3 support:<<NEWL>><<NEWL>> - adds support for converting doctests<<NEWL>><<NEWL>><<NEWL>>This module raises an ImportError on Python 2.<<NEWL>>""""""<<NEWL>><<NEWL>>from distutils.util import Mixin2to3 as _Mixin2to3<<NEWL>>from distutils import log<<NEWL>>from lib2to3.refactor import RefactoringTool, get_fixers_from_package<<NEWL>><<NEWL>>import setuptools<<NEWL>><<NEWL>><<NEWL>>class DistutilsRefactoringTool(RefactoringTool):<<NEWL>>    def log_error(self, msg, *args, **kw):<<NEWL>>        log.error(msg, *args)<<NEWL>><<NEWL>>    def log_message(self, msg, *args):<<NEWL>>        log.info(msg, *args)<<NEWL>><<NEWL>>    def log_debug(self, msg, *args):<<NEWL>>        log.debug(msg, *args)<<NEWL>><<NEWL>><<NEWL>>class Mixin2to3(_Mixin2to3):<<NEWL>>    def run_2to3(self, files, doctests=False):<<NEWL>>        # See of the distribution option has been set, otherwise check the<<NEWL>>        # setuptools default.<<NEWL>>        if self.distribution.use_2to3 is not True:<<NEWL>>            return<<NEWL>>        if not files:<<NEWL>>            return<<NEWL>>        log.info(""Fixing "" + "" "".join(files))<<NEWL>>        self.__build_fixer_names()<<NEWL>>        self.__exclude_fixers()<<NEWL>>        if doctests:<<NEWL>>            if setuptools.run_2to3_on_doctests:<<NEWL>>                r = DistutilsRefactoringTool(self.fixer_names)<<NEWL>>                r.refactor(files, write=True, doctests_only=True)<<NEWL>>        else:<<NEWL>>            _Mixin2to3.run_2to3(self, files)<<NEWL>><<NEWL>>    def __build_fixer_names(self):<<NEWL>>        if self.fixer_names:<<NEWL>>            return<<NEWL>>        self.fixer_names = []<<NEWL>>        for p in setuptools.lib2to3_fixer_packages:<<NEWL>>            self.fixer_names.extend(get_fixers_from_package(p))<<NEWL>>        if self.distribution.use_2to3_fixers is not None:<<NEWL>>            for p in self.distribution.use_2to3_fixers:<<NEWL>>                self.fixer_names.extend(get_fixers_from_package(p))<<NEWL>><<NEWL>>    def __exclude_fixers(self):<<NEWL>>        excluded_fixers = getattr(self, 'exclude_fixers', [])<<NEWL>>        if self.distribution.use_2to3_exclude_fixers is not None:<<NEWL>>            excluded_fixers.extend(self.distribution.use_2to3_exclude_fixers)<<NEWL>>        for fixer_name in excluded_fixers:<<NEWL>>            if fixer_name in self.fixer_names:<<NEWL>>                self.fixer_names.remove(fixer_name)"
422	jackson	3	"import importlib.metadata<<NEWL>>from typing import Any, Optional, Protocol, cast<<NEWL>><<NEWL>><<NEWL>>class BadMetadata(ValueError):<<NEWL>>    def __init__(self, dist: importlib.metadata.Distribution, *, reason: str) -> None:<<NEWL>>        self.dist = dist<<NEWL>>        self.reason = reason<<NEWL>><<NEWL>>    def __str__(self) -> str:<<NEWL>>        return f""Bad metadata in {self.dist} ({self.reason})""<<NEWL>><<NEWL>><<NEWL>>class BasePath(Protocol):<<NEWL>>    """"""A protocol that various path objects conform.<<NEWL>><<NEWL>>    This exists because importlib.metadata uses both ``pathlib.Path`` and<<NEWL>>    ``zipfile.Path``, and we need a common base for type hints (Union does not<<NEWL>>    work well since ``zipfile.Path`` is too new for our linter setup).<<NEWL>><<NEWL>>    This does not mean to be exhaustive, but only contains things that present<<NEWL>>    in both classes *that we need*.<<NEWL>>    """"""<<NEWL>><<NEWL>>    @property<<NEWL>>    def name(self) -> str:<<NEWL>>        raise NotImplementedError()<<NEWL>><<NEWL>>    @property<<NEWL>>    def parent(self) -> ""BasePath"":<<NEWL>>        raise NotImplementedError()<<NEWL>><<NEWL>><<NEWL>>def get_info_location(d: importlib.metadata.Distribution) -> Optional[BasePath]:<<NEWL>>    """"""Find the path to the distribution's metadata directory.<<NEWL>><<NEWL>>    HACK: This relies on importlib.metadata's private ``_path`` attribute. Not<<NEWL>>    all distributions exist on disk, so importlib.metadata is correct to not<<NEWL>>    expose the attribute as public. But pip's code base is old and not as clean,<<NEWL>>    so we do this to avoid having to rewrite too many things. Hopefully we can<<NEWL>>    eliminate this some day.<<NEWL>>    """"""<<NEWL>>    return getattr(d, ""_path"", None)<<NEWL>><<NEWL>><<NEWL>>def get_dist_name(dist: importlib.metadata.Distribution) -> str:<<NEWL>>    """"""Get the distribution's project name.<<NEWL>><<NEWL>>    The ``name`` attribute is only available in Python 3.10 or later. We are<<NEWL>>    targeting exactly that, but Mypy does not know this.<<NEWL>>    """"""<<NEWL>>    name = cast(Any, dist).name<<NEWL>>    if not isinstance(name, str):<<NEWL>>        raise BadMetadata(dist, reason=""invalid metadata entry 'name'"")<<NEWL>>    return name"
422	donghui	3	"import importlib.metadata<<NEWL>>from typing import Any, Optional, Protocol, cast<<NEWL>><<NEWL>><<NEWL>>class BadMetadata(ValueError):<<NEWL>>    def __init__(self, dist: importlib.metadata.Distribution, *, reason: str) -> None:<<NEWL>>        self.dist = dist<<NEWL>>        self.reason = reason<<NEWL>><<NEWL>>    def __str__(self) -> str:<<NEWL>>        return f""Bad metadata in {self.dist} ({self.reason})""<<NEWL>><<NEWL>><<NEWL>>class BasePath(Protocol):<<NEWL>>    """"""A protocol that various path objects conform.<<NEWL>><<NEWL>>    This exists because importlib.metadata uses both ``pathlib.Path`` and<<NEWL>>    ``zipfile.Path``, and we need a common base for type hints (Union does not<<NEWL>>    work well since ``zipfile.Path`` is too new for our linter setup).<<NEWL>><<NEWL>>    This does not mean to be exhaustive, but only contains things that present<<NEWL>>    in both classes *that we need*.<<NEWL>>    """"""<<NEWL>><<NEWL>>    @property<<NEWL>>    def name(self) -> str:<<NEWL>>        raise NotImplementedError()<<NEWL>><<NEWL>>    @property<<NEWL>>    def parent(self) -> ""BasePath"":<<NEWL>>        raise NotImplementedError()<<NEWL>><<NEWL>><<NEWL>>def get_info_location(d: importlib.metadata.Distribution) -> Optional[BasePath]:<<NEWL>>    """"""Find the path to the distribution's metadata directory.<<NEWL>><<NEWL>>    HACK: This relies on importlib.metadata's private ``_path`` attribute. Not<<NEWL>>    all distributions exist on disk, so importlib.metadata is correct to not<<NEWL>>    expose the attribute as public. But pip's code base is old and not as clean,<<NEWL>>    so we do this to avoid having to rewrite too many things. Hopefully we can<<NEWL>>    eliminate this some day.<<NEWL>>    """"""<<NEWL>>    return getattr(d, ""_path"", None)<<NEWL>><<NEWL>><<NEWL>>def get_dist_name(dist: importlib.metadata.Distribution) -> str:<<NEWL>>    """"""Get the distribution's project name.<<NEWL>><<NEWL>>    The ``name`` attribute is only available in Python 3.10 or later. We are<<NEWL>>    targeting exactly that, but Mypy does not know this.<<NEWL>>    """"""<<NEWL>>    name = cast(Any, dist).name<<NEWL>>    if not isinstance(name, str):<<NEWL>>        raise BadMetadata(dist, reason=""invalid metadata entry 'name'"")<<NEWL>>    return name"
447	jackson	2	"import _plotly_utils.basevalidators<<NEWL>><<NEWL>><<NEWL>>class LineValidator(_plotly_utils.basevalidators.CompoundValidator):<<NEWL>>    def __init__(self, plotly_name=""line"", parent_name=""scatterpolar"", **kwargs):<<NEWL>>        super(LineValidator, self).__init__(<<NEWL>>            plotly_name=plotly_name,<<NEWL>>            parent_name=parent_name,<<NEWL>>            data_class_str=kwargs.pop(""data_class_str"", ""Line""),<<NEWL>>            data_docs=kwargs.pop(<<NEWL>>                ""data_docs"",<<NEWL>>                """"""<<NEWL>>            backoff<<NEWL>>                Sets the line back off from the end point of<<NEWL>>                the nth line segment (in px). This option is<<NEWL>>                useful e.g. to avoid overlap with arrowhead<<NEWL>>                markers. With ""auto"" the lines would trim<<NEWL>>                before markers if `marker.angleref` is set to<<NEWL>>                ""previous"".<<NEWL>>            backoffsrc<<NEWL>>                Sets the source reference on Chart Studio Cloud<<NEWL>>                for `backoff`.<<NEWL>>            color<<NEWL>>                Sets the line color.<<NEWL>>            dash<<NEWL>>                Sets the dash style of lines. Set to a dash<<NEWL>>                type string (""solid"", ""dot"", ""dash"",<<NEWL>>                ""longdash"", ""dashdot"", or ""longdashdot"") or a<<NEWL>>                dash length list in px (eg ""5px,10px,2px,2px"").<<NEWL>>            shape<<NEWL>>                Determines the line shape. With ""spline"" the<<NEWL>>                lines are drawn using spline interpolation. The<<NEWL>>                other available values correspond to step-wise<<NEWL>>                line shapes.<<NEWL>>            smoothing<<NEWL>>                Has an effect only if `shape` is set to<<NEWL>>                ""spline"" Sets the amount of smoothing. 0<<NEWL>>                corresponds to no smoothing (equivalent to a<<NEWL>>                ""linear"" shape).<<NEWL>>            width<<NEWL>>                Sets the line width (in px).<<NEWL>>"""""",<<NEWL>>            ),<<NEWL>>            **kwargs,<<NEWL>>        )"
447	donghui	1	"import _plotly_utils.basevalidators<<NEWL>><<NEWL>><<NEWL>>class LineValidator(_plotly_utils.basevalidators.CompoundValidator):<<NEWL>>    def __init__(self, plotly_name=""line"", parent_name=""scatterpolar"", **kwargs):<<NEWL>>        super(LineValidator, self).__init__(<<NEWL>>            plotly_name=plotly_name,<<NEWL>>            parent_name=parent_name,<<NEWL>>            data_class_str=kwargs.pop(""data_class_str"", ""Line""),<<NEWL>>            data_docs=kwargs.pop(<<NEWL>>                ""data_docs"",<<NEWL>>                """"""<<NEWL>>            backoff<<NEWL>>                Sets the line back off from the end point of<<NEWL>>                the nth line segment (in px). This option is<<NEWL>>                useful e.g. to avoid overlap with arrowhead<<NEWL>>                markers. With ""auto"" the lines would trim<<NEWL>>                before markers if `marker.angleref` is set to<<NEWL>>                ""previous"".<<NEWL>>            backoffsrc<<NEWL>>                Sets the source reference on Chart Studio Cloud<<NEWL>>                for `backoff`.<<NEWL>>            color<<NEWL>>                Sets the line color.<<NEWL>>            dash<<NEWL>>                Sets the dash style of lines. Set to a dash<<NEWL>>                type string (""solid"", ""dot"", ""dash"",<<NEWL>>                ""longdash"", ""dashdot"", or ""longdashdot"") or a<<NEWL>>                dash length list in px (eg ""5px,10px,2px,2px"").<<NEWL>>            shape<<NEWL>>                Determines the line shape. With ""spline"" the<<NEWL>>                lines are drawn using spline interpolation. The<<NEWL>>                other available values correspond to step-wise<<NEWL>>                line shapes.<<NEWL>>            smoothing<<NEWL>>                Has an effect only if `shape` is set to<<NEWL>>                ""spline"" Sets the amount of smoothing. 0<<NEWL>>                corresponds to no smoothing (equivalent to a<<NEWL>>                ""linear"" shape).<<NEWL>>            width<<NEWL>>                Sets the line width (in px).<<NEWL>>"""""",<<NEWL>>            ),<<NEWL>>            **kwargs,<<NEWL>>        )"
416	jackson	0	"import pytest<<NEWL>><<NEWL>>from pandas import (<<NEWL>>    DatetimeIndex,<<NEWL>>    date_range,<<NEWL>>)<<NEWL>>import pandas._testing as tm<<NEWL>><<NEWL>><<NEWL>>def astype_non_nano(dti_nano, unit):<<NEWL>>    # TODO(2.0): remove once DTI/DTA.astype supports non-nano<<NEWL>>    if unit == ""ns"":<<NEWL>>        return dti_nano<<NEWL>><<NEWL>>    dta_nano = dti_nano._data<<NEWL>>    arr_nano = dta_nano._ndarray<<NEWL>><<NEWL>>    arr = arr_nano.astype(f""M8[{unit}]"")<<NEWL>>    if dti_nano.tz is None:<<NEWL>>        dtype = arr.dtype<<NEWL>>    else:<<NEWL>>        dtype = type(dti_nano.dtype)(tz=dti_nano.tz, unit=unit)<<NEWL>>    dta = type(dta_nano)._simple_new(arr, dtype=dtype)<<NEWL>>    dti = DatetimeIndex(dta, name=dti_nano.name)<<NEWL>>    assert dti.dtype == dtype<<NEWL>>    return dti<<NEWL>><<NEWL>><<NEWL>>@pytest.mark.filterwarnings(""ignore::DeprecationWarning"")<<NEWL>>@pytest.mark.parametrize(""tz"", [None, ""Asia/Shanghai"", ""Europe/Berlin""])<<NEWL>>@pytest.mark.parametrize(""name"", [None, ""my_dti""])<<NEWL>>@pytest.mark.parametrize(""unit"", [""ns"", ""us"", ""ms"", ""s""])<<NEWL>>def test_dti_snap(name, tz, unit):<<NEWL>>    dti = DatetimeIndex(<<NEWL>>        [<<NEWL>>            ""1/1/2002"",<<NEWL>>            ""1/2/2002"",<<NEWL>>            ""1/3/2002"",<<NEWL>>            ""1/4/2002"",<<NEWL>>            ""1/5/2002"",<<NEWL>>            ""1/6/2002"",<<NEWL>>            ""1/7/2002"",<<NEWL>>        ],<<NEWL>>        name=name,<<NEWL>>        tz=tz,<<NEWL>>        freq=""D"",<<NEWL>>    )<<NEWL>>    dti = astype_non_nano(dti, unit)<<NEWL>><<NEWL>>    result = dti.snap(freq=""W-MON"")<<NEWL>>    expected = date_range(""12/31/2001"", ""1/7/2002"", name=name, tz=tz, freq=""w-mon"")<<NEWL>>    expected = expected.repeat([3, 4])<<NEWL>>    expected = astype_non_nano(expected, unit)<<NEWL>>    tm.assert_index_equal(result, expected)<<NEWL>>    assert result.tz == expected.tz<<NEWL>>    assert result.freq is None<<NEWL>>    assert expected.freq is None<<NEWL>><<NEWL>>    result = dti.snap(freq=""B"")<<NEWL>><<NEWL>>    expected = date_range(""1/1/2002"", ""1/7/2002"", name=name, tz=tz, freq=""b"")<<NEWL>>    expected = expected.repeat([1, 1, 1, 2, 2])<<NEWL>>    expected = astype_non_nano(expected, unit)<<NEWL>>    tm.assert_index_equal(result, expected)<<NEWL>>    assert result.tz == expected.tz<<NEWL>>    assert result.freq is None<<NEWL>>    assert expected.freq is None"
416	donghui	0	"import pytest<<NEWL>><<NEWL>>from pandas import (<<NEWL>>    DatetimeIndex,<<NEWL>>    date_range,<<NEWL>>)<<NEWL>>import pandas._testing as tm<<NEWL>><<NEWL>><<NEWL>>def astype_non_nano(dti_nano, unit):<<NEWL>>    # TODO(2.0): remove once DTI/DTA.astype supports non-nano<<NEWL>>    if unit == ""ns"":<<NEWL>>        return dti_nano<<NEWL>><<NEWL>>    dta_nano = dti_nano._data<<NEWL>>    arr_nano = dta_nano._ndarray<<NEWL>><<NEWL>>    arr = arr_nano.astype(f""M8[{unit}]"")<<NEWL>>    if dti_nano.tz is None:<<NEWL>>        dtype = arr.dtype<<NEWL>>    else:<<NEWL>>        dtype = type(dti_nano.dtype)(tz=dti_nano.tz, unit=unit)<<NEWL>>    dta = type(dta_nano)._simple_new(arr, dtype=dtype)<<NEWL>>    dti = DatetimeIndex(dta, name=dti_nano.name)<<NEWL>>    assert dti.dtype == dtype<<NEWL>>    return dti<<NEWL>><<NEWL>><<NEWL>>@pytest.mark.filterwarnings(""ignore::DeprecationWarning"")<<NEWL>>@pytest.mark.parametrize(""tz"", [None, ""Asia/Shanghai"", ""Europe/Berlin""])<<NEWL>>@pytest.mark.parametrize(""name"", [None, ""my_dti""])<<NEWL>>@pytest.mark.parametrize(""unit"", [""ns"", ""us"", ""ms"", ""s""])<<NEWL>>def test_dti_snap(name, tz, unit):<<NEWL>>    dti = DatetimeIndex(<<NEWL>>        [<<NEWL>>            ""1/1/2002"",<<NEWL>>            ""1/2/2002"",<<NEWL>>            ""1/3/2002"",<<NEWL>>            ""1/4/2002"",<<NEWL>>            ""1/5/2002"",<<NEWL>>            ""1/6/2002"",<<NEWL>>            ""1/7/2002"",<<NEWL>>        ],<<NEWL>>        name=name,<<NEWL>>        tz=tz,<<NEWL>>        freq=""D"",<<NEWL>>    )<<NEWL>>    dti = astype_non_nano(dti, unit)<<NEWL>><<NEWL>>    result = dti.snap(freq=""W-MON"")<<NEWL>>    expected = date_range(""12/31/2001"", ""1/7/2002"", name=name, tz=tz, freq=""w-mon"")<<NEWL>>    expected = expected.repeat([3, 4])<<NEWL>>    expected = astype_non_nano(expected, unit)<<NEWL>>    tm.assert_index_equal(result, expected)<<NEWL>>    assert result.tz == expected.tz<<NEWL>>    assert result.freq is None<<NEWL>>    assert expected.freq is None<<NEWL>><<NEWL>>    result = dti.snap(freq=""B"")<<NEWL>><<NEWL>>    expected = date_range(""1/1/2002"", ""1/7/2002"", name=name, tz=tz, freq=""b"")<<NEWL>>    expected = expected.repeat([1, 1, 1, 2, 2])<<NEWL>>    expected = astype_non_nano(expected, unit)<<NEWL>>    tm.assert_index_equal(result, expected)<<NEWL>>    assert result.tz == expected.tz<<NEWL>>    assert result.freq is None<<NEWL>>    assert expected.freq is None"
358	jackson	1	"# Copyright 2017-present Adtran, Inc.<<NEWL>>#<<NEWL>># Licensed under the Apache License, Version 2.0 (the ""License"");<<NEWL>># you may not use this file except in compliance with the License.<<NEWL>># You may obtain a copy of the License at<<NEWL>>#<<NEWL>># http://www.apache.org/licenses/LICENSE-2.0<<NEWL>>#<<NEWL>># Unless required by applicable law or agreed to in writing, software<<NEWL>># distributed under the License is distributed on an ""AS IS"" BASIS,<<NEWL>># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.<<NEWL>># See the License for the specific language governing permissions and<<NEWL>># limitations under the License.<<NEWL>><<NEWL>>from voltha.adapters.adtran_onu.pon_port import PonPort<<NEWL>>from mock import MagicMock<<NEWL>>import pytest<<NEWL>><<NEWL>>## Test class PonPort init settings  ###############<<NEWL>>def test_PonPort_inits():<<NEWL>>    handler = MagicMock()<<NEWL>>    handler.device_id = 100<<NEWL>>    portnum = 1<<NEWL>>    testponport = PonPort(handler, portnum)<<NEWL>><<NEWL>>    assert testponport._enabled is False<<NEWL>>    assert testponport._valid is True<<NEWL>>    assert testponport._handler is handler<<NEWL>>    assert testponport._deferred is None<<NEWL>>    assert testponport._port is None<<NEWL>>    assert testponport._port_number == 1<<NEWL>>    assert testponport._entity_id is None<<NEWL>>    assert testponport._next_entity_id == PonPort.MIN_GEM_ENTITY_ID<<NEWL>><<NEWL>><<NEWL>><<NEWL>><<NEWL>>## Test PonPort staticmethod #########<<NEWL>>def test_create():<<NEWL>>    handler = MagicMock()<<NEWL>>    handler.device_id = 200<<NEWL>>    port_no = 2<<NEWL>>    testcreate = PonPort.create(handler, port_no)<<NEWL>><<NEWL>>    assert isinstance(testcreate, PonPort)<<NEWL>>    assert testcreate._handler is handler<<NEWL>>    assert testcreate._port_number is port_no<<NEWL>><<NEWL>><<NEWL>><<NEWL>><<NEWL>><<NEWL>>## Test PonPort @property #########<<NEWL>>def test_PonPort_properties():<<NEWL>>    handler = MagicMock()<<NEWL>>    handler.device_id = 300<<NEWL>>    port_no = 3<<NEWL>>    testprop1 = PonPort(handler, port_no)<<NEWL>><<NEWL>>    assert testprop1.enabled is False<<NEWL>>    assert testprop1.port_number == 3<<NEWL>>    assert testprop1.entity_id is None<<NEWL>>    assert testprop1.next_gem_entity_id == PonPort.MIN_GEM_ENTITY_ID<<NEWL>>    assert testprop1.tconts == {}<<NEWL>>    assert testprop1.gem_ports == {}<<NEWL>><<NEWL>>"
358	donghui	1	"# Copyright 2017-present Adtran, Inc.<<NEWL>>#<<NEWL>># Licensed under the Apache License, Version 2.0 (the ""License"");<<NEWL>># you may not use this file except in compliance with the License.<<NEWL>># You may obtain a copy of the License at<<NEWL>>#<<NEWL>># http://www.apache.org/licenses/LICENSE-2.0<<NEWL>>#<<NEWL>># Unless required by applicable law or agreed to in writing, software<<NEWL>># distributed under the License is distributed on an ""AS IS"" BASIS,<<NEWL>># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.<<NEWL>># See the License for the specific language governing permissions and<<NEWL>># limitations under the License.<<NEWL>><<NEWL>>from voltha.adapters.adtran_onu.pon_port import PonPort<<NEWL>>from mock import MagicMock<<NEWL>>import pytest<<NEWL>><<NEWL>>## Test class PonPort init settings  ###############<<NEWL>>def test_PonPort_inits():<<NEWL>>    handler = MagicMock()<<NEWL>>    handler.device_id = 100<<NEWL>>    portnum = 1<<NEWL>>    testponport = PonPort(handler, portnum)<<NEWL>><<NEWL>>    assert testponport._enabled is False<<NEWL>>    assert testponport._valid is True<<NEWL>>    assert testponport._handler is handler<<NEWL>>    assert testponport._deferred is None<<NEWL>>    assert testponport._port is None<<NEWL>>    assert testponport._port_number == 1<<NEWL>>    assert testponport._entity_id is None<<NEWL>>    assert testponport._next_entity_id == PonPort.MIN_GEM_ENTITY_ID<<NEWL>><<NEWL>><<NEWL>><<NEWL>><<NEWL>>## Test PonPort staticmethod #########<<NEWL>>def test_create():<<NEWL>>    handler = MagicMock()<<NEWL>>    handler.device_id = 200<<NEWL>>    port_no = 2<<NEWL>>    testcreate = PonPort.create(handler, port_no)<<NEWL>><<NEWL>>    assert isinstance(testcreate, PonPort)<<NEWL>>    assert testcreate._handler is handler<<NEWL>>    assert testcreate._port_number is port_no<<NEWL>><<NEWL>><<NEWL>><<NEWL>><<NEWL>><<NEWL>>## Test PonPort @property #########<<NEWL>>def test_PonPort_properties():<<NEWL>>    handler = MagicMock()<<NEWL>>    handler.device_id = 300<<NEWL>>    port_no = 3<<NEWL>>    testprop1 = PonPort(handler, port_no)<<NEWL>><<NEWL>>    assert testprop1.enabled is False<<NEWL>>    assert testprop1.port_number == 3<<NEWL>>    assert testprop1.entity_id is None<<NEWL>>    assert testprop1.next_gem_entity_id == PonPort.MIN_GEM_ENTITY_ID<<NEWL>>    assert testprop1.tconts == {}<<NEWL>>    assert testprop1.gem_ports == {}<<NEWL>><<NEWL>>"
309	jackson	3	"from datetime import datetime<<NEWL>>from .virtualtimescheduler import VirtualTimeScheduler<<NEWL>><<NEWL>><<NEWL>>class HistoricalScheduler(VirtualTimeScheduler):<<NEWL>>    """"""Provides a virtual time scheduler that uses datetime for absolute time<<NEWL>>    and timedelta for relative time.""""""<<NEWL>><<NEWL>>    def __init__(self, initial_clock=None, comparer=None):<<NEWL>>        """"""Creates a new historical scheduler with the specified initial clock<<NEWL>>        value.<<NEWL>><<NEWL>>        Keyword arguments:<<NEWL>>        initial_clock -- {Number} Initial value for the clock.<<NEWL>>        comparer -- {Function} Comparer to determine causality of events based<<NEWL>>            on absolute time.""""""<<NEWL>><<NEWL>>        def compare_datetimes(a, b):<<NEWL>>            return (a > b) - (a < b)<<NEWL>><<NEWL>>        clock = initial_clock or datetime.fromtimestamp(0)<<NEWL>>        comparer = comparer or compare_datetimes<<NEWL>><<NEWL>>        super(HistoricalScheduler, self).__init__(clock)<<NEWL>><<NEWL>>    @property<<NEWL>>    def now(self):<<NEWL>>        """"""Represents a notion of time for this scheduler. Tasks being scheduled<<NEWL>>        on a scheduler will adhere to the time denoted by this property.""""""<<NEWL>><<NEWL>>        return self.clock<<NEWL>><<NEWL>>    @staticmethod<<NEWL>>    def add(absolute, relative):<<NEWL>>        """"""Adds a relative time value to an absolute time value.<<NEWL>><<NEWL>>        Keyword arguments:<<NEWL>>        absolute -- {datetime} Absolute virtual time value.<<NEWL>>        relative -- {timedelta} Relative virtual time value to add.<<NEWL>><<NEWL>>        Returns resulting absolute virtual time sum value.""""""<<NEWL>><<NEWL>>        return absolute + relative<<NEWL>><<NEWL>>    def to_datetime_offset(self, absolute):<<NEWL>>        """"""Converts the absolute time value to a datetime value.""""""<<NEWL>><<NEWL>>        # datetime -> datetime<<NEWL>>        return absolute<<NEWL>><<NEWL>>    def to_relative(self, timespan):<<NEWL>>        """"""Converts the timespan value to a relative virtual time value.<<NEWL>><<NEWL>>        Keyword arguments:<<NEWL>>        timespan -- {timedelta} Time_span value to convert.<<NEWL>><<NEWL>>        Returns corresponding relative virtual time value.""""""<<NEWL>><<NEWL>>        # timedelta -> timedelta<<NEWL>>        return timespan"
309	donghui	3	"from datetime import datetime<<NEWL>>from .virtualtimescheduler import VirtualTimeScheduler<<NEWL>><<NEWL>><<NEWL>>class HistoricalScheduler(VirtualTimeScheduler):<<NEWL>>    """"""Provides a virtual time scheduler that uses datetime for absolute time<<NEWL>>    and timedelta for relative time.""""""<<NEWL>><<NEWL>>    def __init__(self, initial_clock=None, comparer=None):<<NEWL>>        """"""Creates a new historical scheduler with the specified initial clock<<NEWL>>        value.<<NEWL>><<NEWL>>        Keyword arguments:<<NEWL>>        initial_clock -- {Number} Initial value for the clock.<<NEWL>>        comparer -- {Function} Comparer to determine causality of events based<<NEWL>>            on absolute time.""""""<<NEWL>><<NEWL>>        def compare_datetimes(a, b):<<NEWL>>            return (a > b) - (a < b)<<NEWL>><<NEWL>>        clock = initial_clock or datetime.fromtimestamp(0)<<NEWL>>        comparer = comparer or compare_datetimes<<NEWL>><<NEWL>>        super(HistoricalScheduler, self).__init__(clock)<<NEWL>><<NEWL>>    @property<<NEWL>>    def now(self):<<NEWL>>        """"""Represents a notion of time for this scheduler. Tasks being scheduled<<NEWL>>        on a scheduler will adhere to the time denoted by this property.""""""<<NEWL>><<NEWL>>        return self.clock<<NEWL>><<NEWL>>    @staticmethod<<NEWL>>    def add(absolute, relative):<<NEWL>>        """"""Adds a relative time value to an absolute time value.<<NEWL>><<NEWL>>        Keyword arguments:<<NEWL>>        absolute -- {datetime} Absolute virtual time value.<<NEWL>>        relative -- {timedelta} Relative virtual time value to add.<<NEWL>><<NEWL>>        Returns resulting absolute virtual time sum value.""""""<<NEWL>><<NEWL>>        return absolute + relative<<NEWL>><<NEWL>>    def to_datetime_offset(self, absolute):<<NEWL>>        """"""Converts the absolute time value to a datetime value.""""""<<NEWL>><<NEWL>>        # datetime -> datetime<<NEWL>>        return absolute<<NEWL>><<NEWL>>    def to_relative(self, timespan):<<NEWL>>        """"""Converts the timespan value to a relative virtual time value.<<NEWL>><<NEWL>>        Keyword arguments:<<NEWL>>        timespan -- {timedelta} Time_span value to convert.<<NEWL>><<NEWL>>        Returns corresponding relative virtual time value.""""""<<NEWL>><<NEWL>>        # timedelta -> timedelta<<NEWL>>        return timespan"
319	jackson	1	"from django.contrib.sites.models import Site<<NEWL>>from django.db import models<<NEWL>>from django.urls import NoReverseMatch, get_script_prefix, reverse<<NEWL>>from django.utils.encoding import iri_to_uri<<NEWL>>from django.utils.translation import gettext_lazy as _<<NEWL>><<NEWL>><<NEWL>>class FlatPage(models.Model):<<NEWL>>    url = models.CharField(_(""URL""), max_length=100, db_index=True)<<NEWL>>    title = models.CharField(_(""title""), max_length=200)<<NEWL>>    content = models.TextField(_(""content""), blank=True)<<NEWL>>    enable_comments = models.BooleanField(_(""enable comments""), default=False)<<NEWL>>    template_name = models.CharField(<<NEWL>>        _(""template name""),<<NEWL>>        max_length=70,<<NEWL>>        blank=True,<<NEWL>>        help_text=_(<<NEWL>>            ""Example: “flatpages/contact_page.html”. If this isn’t provided, ""<<NEWL>>            ""the system will use “flatpages/default.html”.""<<NEWL>>        ),<<NEWL>>    )<<NEWL>>    registration_required = models.BooleanField(<<NEWL>>        _(""registration required""),<<NEWL>>        help_text=_(<<NEWL>>            ""If this is checked, only logged-in users will be able to view the page.""<<NEWL>>        ),<<NEWL>>        default=False,<<NEWL>>    )<<NEWL>>    sites = models.ManyToManyField(Site, verbose_name=_(""sites""))<<NEWL>><<NEWL>>    class Meta:<<NEWL>>        db_table = ""django_flatpage""<<NEWL>>        verbose_name = _(""flat page"")<<NEWL>>        verbose_name_plural = _(""flat pages"")<<NEWL>>        ordering = [""url""]<<NEWL>><<NEWL>>    def __str__(self):<<NEWL>>        return ""%s -- %s"" % (self.url, self.title)<<NEWL>><<NEWL>>    def get_absolute_url(self):<<NEWL>>        from .views import flatpage<<NEWL>><<NEWL>>        for url in (self.url.lstrip(""/""), self.url):<<NEWL>>            try:<<NEWL>>                return reverse(flatpage, kwargs={""url"": url})<<NEWL>>            except NoReverseMatch:<<NEWL>>                pass<<NEWL>>        # Handle script prefix manually because we bypass reverse()<<NEWL>>        return iri_to_uri(get_script_prefix().rstrip(""/"") + self.url)"
319	donghui	1	"from django.contrib.sites.models import Site<<NEWL>>from django.db import models<<NEWL>>from django.urls import NoReverseMatch, get_script_prefix, reverse<<NEWL>>from django.utils.encoding import iri_to_uri<<NEWL>>from django.utils.translation import gettext_lazy as _<<NEWL>><<NEWL>><<NEWL>>class FlatPage(models.Model):<<NEWL>>    url = models.CharField(_(""URL""), max_length=100, db_index=True)<<NEWL>>    title = models.CharField(_(""title""), max_length=200)<<NEWL>>    content = models.TextField(_(""content""), blank=True)<<NEWL>>    enable_comments = models.BooleanField(_(""enable comments""), default=False)<<NEWL>>    template_name = models.CharField(<<NEWL>>        _(""template name""),<<NEWL>>        max_length=70,<<NEWL>>        blank=True,<<NEWL>>        help_text=_(<<NEWL>>            ""Example: “flatpages/contact_page.html”. If this isn’t provided, ""<<NEWL>>            ""the system will use “flatpages/default.html”.""<<NEWL>>        ),<<NEWL>>    )<<NEWL>>    registration_required = models.BooleanField(<<NEWL>>        _(""registration required""),<<NEWL>>        help_text=_(<<NEWL>>            ""If this is checked, only logged-in users will be able to view the page.""<<NEWL>>        ),<<NEWL>>        default=False,<<NEWL>>    )<<NEWL>>    sites = models.ManyToManyField(Site, verbose_name=_(""sites""))<<NEWL>><<NEWL>>    class Meta:<<NEWL>>        db_table = ""django_flatpage""<<NEWL>>        verbose_name = _(""flat page"")<<NEWL>>        verbose_name_plural = _(""flat pages"")<<NEWL>>        ordering = [""url""]<<NEWL>><<NEWL>>    def __str__(self):<<NEWL>>        return ""%s -- %s"" % (self.url, self.title)<<NEWL>><<NEWL>>    def get_absolute_url(self):<<NEWL>>        from .views import flatpage<<NEWL>><<NEWL>>        for url in (self.url.lstrip(""/""), self.url):<<NEWL>>            try:<<NEWL>>                return reverse(flatpage, kwargs={""url"": url})<<NEWL>>            except NoReverseMatch:<<NEWL>>                pass<<NEWL>>        # Handle script prefix manually because we bypass reverse()<<NEWL>>        return iri_to_uri(get_script_prefix().rstrip(""/"") + self.url)"
259	jackson	4	"""""""tst_tc1357_uxusylnz_68580 URL Configuration<<NEWL>><<NEWL>>The `urlpatterns` list routes URLs to views. For more information please see:<<NEWL>>    https://docs.djangoproject.com/en/2.2/topics/http/urls/<<NEWL>>Examples:<<NEWL>>Function views<<NEWL>>    1. Add an import:  from my_app import views<<NEWL>>    2. Add a URL to urlpatterns:  path('', views.home, name='home')<<NEWL>>Class-based views<<NEWL>>    1. Add an import:  from other_app.views import Home<<NEWL>>    2. Add a URL to urlpatterns:  path('', Home.as_view(), name='home')<<NEWL>>Including another URLconf<<NEWL>>    1. Import the include() function: from django.urls import include, path<<NEWL>>    2. Add a URL to urlpatterns:  path('blog/', include('blog.urls'))<<NEWL>>""""""<<NEWL>><<NEWL>>from django.contrib import admin<<NEWL>>from django.urls import path, include, re_path<<NEWL>>from django.views.generic.base import TemplateView<<NEWL>>from allauth.account.views import confirm_email<<NEWL>>from rest_framework import permissions<<NEWL>>from drf_spectacular.views import SpectacularJSONAPIView, SpectacularSwaggerView<<NEWL>><<NEWL>>urlpatterns = [<<NEWL>>    <<NEWL>>    path(""accounts/"", include(""allauth.urls"")),<<NEWL>>    path(""modules/"", include(""modules.urls"")),<<NEWL>>    path(""api/v1/"", include(""home.api.v1.urls"")),<<NEWL>>    path(""admin/"", admin.site.urls),<<NEWL>>    path(""users/"", include(""users.urls"", namespace=""users"")),<<NEWL>>    path(""rest-auth/"", include(""rest_auth.urls"")),<<NEWL>>    # Override email confirm to use allauth's HTML view instead of rest_auth's API view<<NEWL>>    path(""rest-auth/registration/account-confirm-email/<str:key>/"", confirm_email),<<NEWL>>    path(""rest-auth/registration/"", include(""rest_auth.registration.urls"")),<<NEWL>>]<<NEWL>><<NEWL>>admin.site.site_header = ""TST-TC1357-uxusylnzzz""<<NEWL>>admin.site.site_title = ""TST-TC1357-uxusylnzzz Admin Portal""<<NEWL>>admin.site.index_title = ""TST-TC1357-uxusylnzzz Admin""<<NEWL>><<NEWL>># swagger<<NEWL>>urlpatterns += [<<NEWL>>    path(""api-docs/schema/"", SpectacularJSONAPIView.as_view(), name=""schema""),<<NEWL>>    path(""api-docs/"", SpectacularSwaggerView.as_view(url_name='schema'), name=""api_docs"")<<NEWL>>]<<NEWL>><<NEWL>><<NEWL>>urlpatterns += [re_path(r"".*"",TemplateView.as_view(template_name='index.html'))]"
259	donghui	3	"""""""tst_tc1357_uxusylnz_68580 URL Configuration<<NEWL>><<NEWL>>The `urlpatterns` list routes URLs to views. For more information please see:<<NEWL>>    https://docs.djangoproject.com/en/2.2/topics/http/urls/<<NEWL>>Examples:<<NEWL>>Function views<<NEWL>>    1. Add an import:  from my_app import views<<NEWL>>    2. Add a URL to urlpatterns:  path('', views.home, name='home')<<NEWL>>Class-based views<<NEWL>>    1. Add an import:  from other_app.views import Home<<NEWL>>    2. Add a URL to urlpatterns:  path('', Home.as_view(), name='home')<<NEWL>>Including another URLconf<<NEWL>>    1. Import the include() function: from django.urls import include, path<<NEWL>>    2. Add a URL to urlpatterns:  path('blog/', include('blog.urls'))<<NEWL>>""""""<<NEWL>><<NEWL>>from django.contrib import admin<<NEWL>>from django.urls import path, include, re_path<<NEWL>>from django.views.generic.base import TemplateView<<NEWL>>from allauth.account.views import confirm_email<<NEWL>>from rest_framework import permissions<<NEWL>>from drf_spectacular.views import SpectacularJSONAPIView, SpectacularSwaggerView<<NEWL>><<NEWL>>urlpatterns = [<<NEWL>>    <<NEWL>>    path(""accounts/"", include(""allauth.urls"")),<<NEWL>>    path(""modules/"", include(""modules.urls"")),<<NEWL>>    path(""api/v1/"", include(""home.api.v1.urls"")),<<NEWL>>    path(""admin/"", admin.site.urls),<<NEWL>>    path(""users/"", include(""users.urls"", namespace=""users"")),<<NEWL>>    path(""rest-auth/"", include(""rest_auth.urls"")),<<NEWL>>    # Override email confirm to use allauth's HTML view instead of rest_auth's API view<<NEWL>>    path(""rest-auth/registration/account-confirm-email/<str:key>/"", confirm_email),<<NEWL>>    path(""rest-auth/registration/"", include(""rest_auth.registration.urls"")),<<NEWL>>]<<NEWL>><<NEWL>>admin.site.site_header = ""TST-TC1357-uxusylnzzz""<<NEWL>>admin.site.site_title = ""TST-TC1357-uxusylnzzz Admin Portal""<<NEWL>>admin.site.index_title = ""TST-TC1357-uxusylnzzz Admin""<<NEWL>><<NEWL>># swagger<<NEWL>>urlpatterns += [<<NEWL>>    path(""api-docs/schema/"", SpectacularJSONAPIView.as_view(), name=""schema""),<<NEWL>>    path(""api-docs/"", SpectacularSwaggerView.as_view(url_name='schema'), name=""api_docs"")<<NEWL>>]<<NEWL>><<NEWL>><<NEWL>>urlpatterns += [re_path(r"".*"",TemplateView.as_view(template_name='index.html'))]"
348	jackson	3	"""""""<<NEWL>>https://adventofcode.com/2016/day/15<<NEWL>>""""""<<NEWL>>from utils import extract_ints, read_data<<NEWL>><<NEWL>>USE_TEST_DATA = False<<NEWL>>SPLIT_BY_LINE = True<<NEWL>>data = read_data(USE_TEST_DATA, SPLIT_BY_LINE)<<NEWL>><<NEWL>><<NEWL>>def parse_data(data_in):<<NEWL>>    """""" Read the input data to retrieve the disc positions """"""<<NEWL>>    num_positions = []<<NEWL>>    starting_pos = []<<NEWL>><<NEWL>>    for line in data_in:<<NEWL>>        ints = extract_ints(line)<<NEWL>>        num_positions.append(ints[1])<<NEWL>>        starting_pos.append(ints[3])<<NEWL>><<NEWL>>    return num_positions, starting_pos<<NEWL>><<NEWL>><<NEWL>>def is_at_zero(num_positions, starting_pos, disc_index, time):<<NEWL>>    """""" Is the specified disc at the zero position at the given time? """"""<<NEWL>>    pos = (starting_pos[disc_index] + time) % num_positions[disc_index]<<NEWL>>    return pos == 0<<NEWL>><<NEWL>><<NEWL>>def find_time(num_positions, starting_pos):<<NEWL>>    """"""<<NEWL>>    Find the time at which to release a capsule so that it passes through all<<NEWL>>    discs successfully<<NEWL>>    """"""<<NEWL>><<NEWL>>    # What's the first time that we can release the disc where it reaches the<<NEWL>>    # first disc as it's at position 0?<<NEWL>>    candidate_time = num_positions[0] - starting_pos[0] - 1<<NEWL>><<NEWL>>    while True:<<NEWL>>        # Are all discs at position 0 when the capsule reaches them?<<NEWL>>        collision = False<<NEWL>>        for disc_index in range(len(num_positions)):<<NEWL>>            time_capsule_reaches_disc = candidate_time + disc_index + 1<<NEWL>>            if not is_at_zero(num_positions, starting_pos, disc_index, time_capsule_reaches_disc):<<NEWL>>                collision = True<<NEWL>>                break<<NEWL>><<NEWL>>        # There was no collision with any disc! candidate_time is the correct answer!<<NEWL>>        if not collision:<<NEWL>>            return candidate_time<<NEWL>><<NEWL>>        # There was a collision so candidate_time isn't a valid result.<<NEWL>>        # Increment it to the next time that disc 1 (index 0) is at the zero position.<<NEWL>>        candidate_time += num_positions[0]<<NEWL>><<NEWL>><<NEWL>>positions, starting = parse_data(data)<<NEWL>><<NEWL>># Part 1<<NEWL>># At what time can we release the capsule to pass through all of the discs?<<NEWL>>print(find_time(positions, starting))<<NEWL>><<NEWL>># Part 2<<NEWL>># What if we add another disc at the bottom?<<NEWL>>positions.append(11)<<NEWL>>starting.append(0)<<NEWL>>print(find_time(positions, starting))"
348	donghui	3	"""""""<<NEWL>>https://adventofcode.com/2016/day/15<<NEWL>>""""""<<NEWL>>from utils import extract_ints, read_data<<NEWL>><<NEWL>>USE_TEST_DATA = False<<NEWL>>SPLIT_BY_LINE = True<<NEWL>>data = read_data(USE_TEST_DATA, SPLIT_BY_LINE)<<NEWL>><<NEWL>><<NEWL>>def parse_data(data_in):<<NEWL>>    """""" Read the input data to retrieve the disc positions """"""<<NEWL>>    num_positions = []<<NEWL>>    starting_pos = []<<NEWL>><<NEWL>>    for line in data_in:<<NEWL>>        ints = extract_ints(line)<<NEWL>>        num_positions.append(ints[1])<<NEWL>>        starting_pos.append(ints[3])<<NEWL>><<NEWL>>    return num_positions, starting_pos<<NEWL>><<NEWL>><<NEWL>>def is_at_zero(num_positions, starting_pos, disc_index, time):<<NEWL>>    """""" Is the specified disc at the zero position at the given time? """"""<<NEWL>>    pos = (starting_pos[disc_index] + time) % num_positions[disc_index]<<NEWL>>    return pos == 0<<NEWL>><<NEWL>><<NEWL>>def find_time(num_positions, starting_pos):<<NEWL>>    """"""<<NEWL>>    Find the time at which to release a capsule so that it passes through all<<NEWL>>    discs successfully<<NEWL>>    """"""<<NEWL>><<NEWL>>    # What's the first time that we can release the disc where it reaches the<<NEWL>>    # first disc as it's at position 0?<<NEWL>>    candidate_time = num_positions[0] - starting_pos[0] - 1<<NEWL>><<NEWL>>    while True:<<NEWL>>        # Are all discs at position 0 when the capsule reaches them?<<NEWL>>        collision = False<<NEWL>>        for disc_index in range(len(num_positions)):<<NEWL>>            time_capsule_reaches_disc = candidate_time + disc_index + 1<<NEWL>>            if not is_at_zero(num_positions, starting_pos, disc_index, time_capsule_reaches_disc):<<NEWL>>                collision = True<<NEWL>>                break<<NEWL>><<NEWL>>        # There was no collision with any disc! candidate_time is the correct answer!<<NEWL>>        if not collision:<<NEWL>>            return candidate_time<<NEWL>><<NEWL>>        # There was a collision so candidate_time isn't a valid result.<<NEWL>>        # Increment it to the next time that disc 1 (index 0) is at the zero position.<<NEWL>>        candidate_time += num_positions[0]<<NEWL>><<NEWL>><<NEWL>>positions, starting = parse_data(data)<<NEWL>><<NEWL>># Part 1<<NEWL>># At what time can we release the capsule to pass through all of the discs?<<NEWL>>print(find_time(positions, starting))<<NEWL>><<NEWL>># Part 2<<NEWL>># What if we add another disc at the bottom?<<NEWL>>positions.append(11)<<NEWL>>starting.append(0)<<NEWL>>print(find_time(positions, starting))"
406	jackson	4	"""""""<<NEWL>>This module deals with interpreting the parse tree as Python<<NEWL>>would have done, in the compiler.<<NEWL>><<NEWL>>For now this only covers parse tree to value conversion of<<NEWL>>compile-time values.<<NEWL>>""""""<<NEWL>><<NEWL>>from __future__ import absolute_import<<NEWL>><<NEWL>>from .Nodes import *<<NEWL>>from .ExprNodes import *<<NEWL>>from .Errors import CompileError<<NEWL>><<NEWL>><<NEWL>>class EmptyScope(object):<<NEWL>>    def lookup(self, name):<<NEWL>>        return None<<NEWL>><<NEWL>>empty_scope = EmptyScope()<<NEWL>><<NEWL>>def interpret_compiletime_options(optlist, optdict, type_env=None, type_args=()):<<NEWL>>    """"""<<NEWL>>    Tries to interpret a list of compile time option nodes.<<NEWL>>    The result will be a tuple (optlist, optdict) but where<<NEWL>>    all expression nodes have been interpreted. The result is<<NEWL>>    in the form of tuples (value, pos).<<NEWL>><<NEWL>>    optlist is a list of nodes, while optdict is a DictNode (the<<NEWL>>    result optdict is a dict)<<NEWL>><<NEWL>>    If type_env is set, all type nodes will be analysed and the resulting<<NEWL>>    type set. Otherwise only interpretateable ExprNodes<<NEWL>>    are allowed, other nodes raises errors.<<NEWL>><<NEWL>>    A CompileError will be raised if there are problems.<<NEWL>>    """"""<<NEWL>><<NEWL>>    def interpret(node, ix):<<NEWL>>        if ix in type_args:<<NEWL>>            if type_env:<<NEWL>>                type = node.analyse_as_type(type_env)<<NEWL>>                if not type:<<NEWL>>                    raise CompileError(node.pos, ""Invalid type."")<<NEWL>>                return (type, node.pos)<<NEWL>>            else:<<NEWL>>                raise CompileError(node.pos, ""Type not allowed here."")<<NEWL>>        else:<<NEWL>>            if (sys.version_info[0] >=3 and<<NEWL>>                isinstance(node, StringNode) and<<NEWL>>                node.unicode_value is not None):<<NEWL>>                return (node.unicode_value, node.pos)<<NEWL>>            return (node.compile_time_value(empty_scope), node.pos)<<NEWL>><<NEWL>>    if optlist:<<NEWL>>        optlist = [interpret(x, ix) for ix, x in enumerate(optlist)]<<NEWL>>    if optdict:<<NEWL>>        assert isinstance(optdict, DictNode)<<NEWL>>        new_optdict = {}<<NEWL>>        for item in optdict.key_value_pairs:<<NEWL>>            new_key, dummy = interpret(item.key, None)<<NEWL>>            new_optdict[new_key] = interpret(item.value, item.key.value)<<NEWL>>        optdict = new_optdict<<NEWL>>    return (optlist, new_optdict)"
406	donghui	2	"""""""<<NEWL>>This module deals with interpreting the parse tree as Python<<NEWL>>would have done, in the compiler.<<NEWL>><<NEWL>>For now this only covers parse tree to value conversion of<<NEWL>>compile-time values.<<NEWL>>""""""<<NEWL>><<NEWL>>from __future__ import absolute_import<<NEWL>><<NEWL>>from .Nodes import *<<NEWL>>from .ExprNodes import *<<NEWL>>from .Errors import CompileError<<NEWL>><<NEWL>><<NEWL>>class EmptyScope(object):<<NEWL>>    def lookup(self, name):<<NEWL>>        return None<<NEWL>><<NEWL>>empty_scope = EmptyScope()<<NEWL>><<NEWL>>def interpret_compiletime_options(optlist, optdict, type_env=None, type_args=()):<<NEWL>>    """"""<<NEWL>>    Tries to interpret a list of compile time option nodes.<<NEWL>>    The result will be a tuple (optlist, optdict) but where<<NEWL>>    all expression nodes have been interpreted. The result is<<NEWL>>    in the form of tuples (value, pos).<<NEWL>><<NEWL>>    optlist is a list of nodes, while optdict is a DictNode (the<<NEWL>>    result optdict is a dict)<<NEWL>><<NEWL>>    If type_env is set, all type nodes will be analysed and the resulting<<NEWL>>    type set. Otherwise only interpretateable ExprNodes<<NEWL>>    are allowed, other nodes raises errors.<<NEWL>><<NEWL>>    A CompileError will be raised if there are problems.<<NEWL>>    """"""<<NEWL>><<NEWL>>    def interpret(node, ix):<<NEWL>>        if ix in type_args:<<NEWL>>            if type_env:<<NEWL>>                type = node.analyse_as_type(type_env)<<NEWL>>                if not type:<<NEWL>>                    raise CompileError(node.pos, ""Invalid type."")<<NEWL>>                return (type, node.pos)<<NEWL>>            else:<<NEWL>>                raise CompileError(node.pos, ""Type not allowed here."")<<NEWL>>        else:<<NEWL>>            if (sys.version_info[0] >=3 and<<NEWL>>                isinstance(node, StringNode) and<<NEWL>>                node.unicode_value is not None):<<NEWL>>                return (node.unicode_value, node.pos)<<NEWL>>            return (node.compile_time_value(empty_scope), node.pos)<<NEWL>><<NEWL>>    if optlist:<<NEWL>>        optlist = [interpret(x, ix) for ix, x in enumerate(optlist)]<<NEWL>>    if optdict:<<NEWL>>        assert isinstance(optdict, DictNode)<<NEWL>>        new_optdict = {}<<NEWL>>        for item in optdict.key_value_pairs:<<NEWL>>            new_key, dummy = interpret(item.key, None)<<NEWL>>            new_optdict[new_key] = interpret(item.value, item.key.value)<<NEWL>>        optdict = new_optdict<<NEWL>>    return (optlist, new_optdict)"
457	jackson	4	"""""""<<NEWL>>String utility functions.<<NEWL>>""""""<<NEWL>><<NEWL>>from typing import Any, Optional, Union<<NEWL>><<NEWL>><<NEWL>>def safe_repr(obj: Any, clip: Optional[int] = None) -> str:<<NEWL>>    """"""<<NEWL>>    Convert object to string representation, yielding the same result a `repr`<<NEWL>>    but catches all exceptions and returns 'N/A' instead of raising the<<NEWL>>    exception. Strings may be truncated by providing `clip`.<<NEWL>><<NEWL>>    >>> safe_repr(42)<<NEWL>>    '42'<<NEWL>>    >>> safe_repr('Clipped text', clip=8)<<NEWL>>    'Clip..xt'<<NEWL>>    >>> safe_repr([1,2,3,4], clip=8)<<NEWL>>    '[1,2..4]'<<NEWL>>    """"""<<NEWL>>    try:<<NEWL>>        s = repr(obj)<<NEWL>>        if not clip or len(s) <= clip:<<NEWL>>            return s<<NEWL>>        else:<<NEWL>>            return s[:clip - 4] + '..' + s[-2:]<<NEWL>>    except:<<NEWL>>        return 'N/A'<<NEWL>><<NEWL>><<NEWL>>def trunc(obj: str, max: int, left: bool = False) -> str:<<NEWL>>    """"""<<NEWL>>    Convert `obj` to string, eliminate newlines and truncate the string to<<NEWL>>    `max` characters. If there are more characters in the string add ``...`` to<<NEWL>>    the string. With `left=True`, the string can be truncated at the beginning.<<NEWL>><<NEWL>>    @note: Does not catch exceptions when converting `obj` to string with<<NEWL>>        `str`.<<NEWL>><<NEWL>>    >>> trunc('This is a long text.', 8)<<NEWL>>    This ...<<NEWL>>    >>> trunc('This is a long text.', 8, left=True)<<NEWL>>    ...text.<<NEWL>>    """"""<<NEWL>>    s = str(obj)<<NEWL>>    s = s.replace('\n', '|')<<NEWL>>    if len(s) > max:<<NEWL>>        if left:<<NEWL>>            return '...' + s[len(s) - max + 3:]<<NEWL>>        else:<<NEWL>>            return s[:(max - 3)] + '...'<<NEWL>>    else:<<NEWL>>        return s<<NEWL>><<NEWL>><<NEWL>>def pp(i: Union[int, float], base: int = 1024) -> str:<<NEWL>>    """"""<<NEWL>>    Pretty-print the integer `i` as a human-readable size representation.<<NEWL>>    """"""<<NEWL>>    degree = 0<<NEWL>>    pattern = ""%4d     %s""<<NEWL>>    while i > base:<<NEWL>>        pattern = ""%7.2f %s""<<NEWL>>        i = i / float(base)<<NEWL>>        degree += 1<<NEWL>>    scales = ['B', 'KB', 'MB', 'GB', 'TB', 'EB']<<NEWL>>    return pattern % (i, scales[degree])<<NEWL>><<NEWL>><<NEWL>>def pp_timestamp(t: Optional[float]) -> str:<<NEWL>>    """"""<<NEWL>>    Get a friendly timestamp represented as a string.<<NEWL>>    """"""<<NEWL>>    if t is None:<<NEWL>>        return ''<<NEWL>>    h, m, s = int(t / 3600), int(t / 60 % 60), t % 60<<NEWL>>    return ""%02d:%02d:%05.2f"" % (h, m, s)"
457	donghui	4	"""""""<<NEWL>>String utility functions.<<NEWL>>""""""<<NEWL>><<NEWL>>from typing import Any, Optional, Union<<NEWL>><<NEWL>><<NEWL>>def safe_repr(obj: Any, clip: Optional[int] = None) -> str:<<NEWL>>    """"""<<NEWL>>    Convert object to string representation, yielding the same result a `repr`<<NEWL>>    but catches all exceptions and returns 'N/A' instead of raising the<<NEWL>>    exception. Strings may be truncated by providing `clip`.<<NEWL>><<NEWL>>    >>> safe_repr(42)<<NEWL>>    '42'<<NEWL>>    >>> safe_repr('Clipped text', clip=8)<<NEWL>>    'Clip..xt'<<NEWL>>    >>> safe_repr([1,2,3,4], clip=8)<<NEWL>>    '[1,2..4]'<<NEWL>>    """"""<<NEWL>>    try:<<NEWL>>        s = repr(obj)<<NEWL>>        if not clip or len(s) <= clip:<<NEWL>>            return s<<NEWL>>        else:<<NEWL>>            return s[:clip - 4] + '..' + s[-2:]<<NEWL>>    except:<<NEWL>>        return 'N/A'<<NEWL>><<NEWL>><<NEWL>>def trunc(obj: str, max: int, left: bool = False) -> str:<<NEWL>>    """"""<<NEWL>>    Convert `obj` to string, eliminate newlines and truncate the string to<<NEWL>>    `max` characters. If there are more characters in the string add ``...`` to<<NEWL>>    the string. With `left=True`, the string can be truncated at the beginning.<<NEWL>><<NEWL>>    @note: Does not catch exceptions when converting `obj` to string with<<NEWL>>        `str`.<<NEWL>><<NEWL>>    >>> trunc('This is a long text.', 8)<<NEWL>>    This ...<<NEWL>>    >>> trunc('This is a long text.', 8, left=True)<<NEWL>>    ...text.<<NEWL>>    """"""<<NEWL>>    s = str(obj)<<NEWL>>    s = s.replace('\n', '|')<<NEWL>>    if len(s) > max:<<NEWL>>        if left:<<NEWL>>            return '...' + s[len(s) - max + 3:]<<NEWL>>        else:<<NEWL>>            return s[:(max - 3)] + '...'<<NEWL>>    else:<<NEWL>>        return s<<NEWL>><<NEWL>><<NEWL>>def pp(i: Union[int, float], base: int = 1024) -> str:<<NEWL>>    """"""<<NEWL>>    Pretty-print the integer `i` as a human-readable size representation.<<NEWL>>    """"""<<NEWL>>    degree = 0<<NEWL>>    pattern = ""%4d     %s""<<NEWL>>    while i > base:<<NEWL>>        pattern = ""%7.2f %s""<<NEWL>>        i = i / float(base)<<NEWL>>        degree += 1<<NEWL>>    scales = ['B', 'KB', 'MB', 'GB', 'TB', 'EB']<<NEWL>>    return pattern % (i, scales[degree])<<NEWL>><<NEWL>><<NEWL>>def pp_timestamp(t: Optional[float]) -> str:<<NEWL>>    """"""<<NEWL>>    Get a friendly timestamp represented as a string.<<NEWL>>    """"""<<NEWL>>    if t is None:<<NEWL>>        return ''<<NEWL>>    h, m, s = int(t / 3600), int(t / 60 % 60), t % 60<<NEWL>>    return ""%02d:%02d:%05.2f"" % (h, m, s)"
517	jackson	4	"""""""<<NEWL>>Kakao OAuth2 backend, docs at:<<NEWL>>    https://python-social-auth.readthedocs.io/en/latest/backends/kakao.html<<NEWL>>""""""<<NEWL>>from .oauth import BaseOAuth2<<NEWL>><<NEWL>><<NEWL>>class KakaoOAuth2(BaseOAuth2):<<NEWL>>    """"""Kakao OAuth authentication backend""""""<<NEWL>>    name = 'kakao'<<NEWL>>    AUTHORIZATION_URL = 'https://kauth.kakao.com/oauth/authorize'<<NEWL>>    ACCESS_TOKEN_URL = 'https://kauth.kakao.com/oauth/token'<<NEWL>>    ACCESS_TOKEN_METHOD = 'POST'<<NEWL>>    REDIRECT_STATE = False<<NEWL>>    EXTRA_DATA = [<<NEWL>>        ('properties', 'properties'),<<NEWL>>    ]<<NEWL>><<NEWL>>    def get_user_id(self, details, response):<<NEWL>>        return response['id']<<NEWL>><<NEWL>>    def get_user_details(self, response):<<NEWL>>        """"""Return user details from Kakao account""""""<<NEWL>><<NEWL>>        kakao_account = response.get('kakao_account', '')<<NEWL>>        kaccount_email = kakao_account.get('email', '')<<NEWL>>        properties = response.get('properties', '')<<NEWL>>        nickname = properties.get('nickname') if properties else ''<<NEWL>>        return {<<NEWL>>            'username': nickname,<<NEWL>>            'email': kaccount_email,<<NEWL>>            'fullname': nickname,<<NEWL>>            'first_name': nickname[1:] if nickname else '',<<NEWL>>            'last_name': nickname[0] if nickname else '',<<NEWL>>        }<<NEWL>><<NEWL>>    def user_data(self, access_token, *args, **kwargs):<<NEWL>>        """"""Loads user data from service""""""<<NEWL>>        return self.get_json(<<NEWL>>            'https://kapi.kakao.com/v2/user/me',<<NEWL>>            headers={<<NEWL>>                'Authorization': f'Bearer {access_token}',<<NEWL>>                'Content_Type': 'application/x-www-form-urlencoded;charset=utf-8',<<NEWL>>            },<<NEWL>>            params={'access_token': access_token}<<NEWL>>        )<<NEWL>><<NEWL>>    def auth_complete_params(self, state=None):<<NEWL>>        client_id, client_secret = self.get_key_and_secret()<<NEWL>>        return {<<NEWL>>            'grant_type': 'authorization_code',<<NEWL>>            'code': self.data.get('code', ''),<<NEWL>>            'client_id': client_id,<<NEWL>>            'client_secret': client_secret,<<NEWL>>        }"
517	donghui	4	"""""""<<NEWL>>Kakao OAuth2 backend, docs at:<<NEWL>>    https://python-social-auth.readthedocs.io/en/latest/backends/kakao.html<<NEWL>>""""""<<NEWL>>from .oauth import BaseOAuth2<<NEWL>><<NEWL>><<NEWL>>class KakaoOAuth2(BaseOAuth2):<<NEWL>>    """"""Kakao OAuth authentication backend""""""<<NEWL>>    name = 'kakao'<<NEWL>>    AUTHORIZATION_URL = 'https://kauth.kakao.com/oauth/authorize'<<NEWL>>    ACCESS_TOKEN_URL = 'https://kauth.kakao.com/oauth/token'<<NEWL>>    ACCESS_TOKEN_METHOD = 'POST'<<NEWL>>    REDIRECT_STATE = False<<NEWL>>    EXTRA_DATA = [<<NEWL>>        ('properties', 'properties'),<<NEWL>>    ]<<NEWL>><<NEWL>>    def get_user_id(self, details, response):<<NEWL>>        return response['id']<<NEWL>><<NEWL>>    def get_user_details(self, response):<<NEWL>>        """"""Return user details from Kakao account""""""<<NEWL>><<NEWL>>        kakao_account = response.get('kakao_account', '')<<NEWL>>        kaccount_email = kakao_account.get('email', '')<<NEWL>>        properties = response.get('properties', '')<<NEWL>>        nickname = properties.get('nickname') if properties else ''<<NEWL>>        return {<<NEWL>>            'username': nickname,<<NEWL>>            'email': kaccount_email,<<NEWL>>            'fullname': nickname,<<NEWL>>            'first_name': nickname[1:] if nickname else '',<<NEWL>>            'last_name': nickname[0] if nickname else '',<<NEWL>>        }<<NEWL>><<NEWL>>    def user_data(self, access_token, *args, **kwargs):<<NEWL>>        """"""Loads user data from service""""""<<NEWL>>        return self.get_json(<<NEWL>>            'https://kapi.kakao.com/v2/user/me',<<NEWL>>            headers={<<NEWL>>                'Authorization': f'Bearer {access_token}',<<NEWL>>                'Content_Type': 'application/x-www-form-urlencoded;charset=utf-8',<<NEWL>>            },<<NEWL>>            params={'access_token': access_token}<<NEWL>>        )<<NEWL>><<NEWL>>    def auth_complete_params(self, state=None):<<NEWL>>        client_id, client_secret = self.get_key_and_secret()<<NEWL>>        return {<<NEWL>>            'grant_type': 'authorization_code',<<NEWL>>            'code': self.data.get('code', ''),<<NEWL>>            'client_id': client_id,<<NEWL>>            'client_secret': client_secret,<<NEWL>>        }"
463	jackson	3	"from contextlib import contextmanager<<NEWL>>import os<<NEWL>>import tempfile<<NEWL>><<NEWL>>import pytest<<NEWL>><<NEWL>>from pandas.io.pytables import HDFStore<<NEWL>><<NEWL>>tables = pytest.importorskip(""tables"")<<NEWL>># set these parameters so we don't have file sharing<<NEWL>>tables.parameters.MAX_NUMEXPR_THREADS = 1<<NEWL>>tables.parameters.MAX_BLOSC_THREADS = 1<<NEWL>>tables.parameters.MAX_THREADS = 1<<NEWL>><<NEWL>><<NEWL>>def safe_remove(path):<<NEWL>>    if path is not None:<<NEWL>>        try:<<NEWL>>            os.remove(path)  # noqa: PDF008<<NEWL>>        except OSError:<<NEWL>>            pass<<NEWL>><<NEWL>><<NEWL>>def safe_close(store):<<NEWL>>    try:<<NEWL>>        if store is not None:<<NEWL>>            store.close()<<NEWL>>    except OSError:<<NEWL>>        pass<<NEWL>><<NEWL>><<NEWL>>def create_tempfile(path):<<NEWL>>    """"""create an unopened named temporary file""""""<<NEWL>>    return os.path.join(tempfile.gettempdir(), path)<<NEWL>><<NEWL>><<NEWL>># contextmanager to ensure the file cleanup<<NEWL>>@contextmanager<<NEWL>>def ensure_clean_store(path, mode=""a"", complevel=None, complib=None, fletcher32=False):<<NEWL>><<NEWL>>    try:<<NEWL>><<NEWL>>        # put in the temporary path if we don't have one already<<NEWL>>        if not len(os.path.dirname(path)):<<NEWL>>            path = create_tempfile(path)<<NEWL>><<NEWL>>        store = HDFStore(<<NEWL>>            path, mode=mode, complevel=complevel, complib=complib, fletcher32=False<<NEWL>>        )<<NEWL>>        yield store<<NEWL>>    finally:<<NEWL>>        safe_close(store)<<NEWL>>        if mode == ""w"" or mode == ""a"":<<NEWL>>            safe_remove(path)<<NEWL>><<NEWL>><<NEWL>>@contextmanager<<NEWL>>def ensure_clean_path(path):<<NEWL>>    """"""<<NEWL>>    return essentially a named temporary file that is not opened<<NEWL>>    and deleted on exiting; if path is a list, then create and<<NEWL>>    return list of filenames<<NEWL>>    """"""<<NEWL>>    try:<<NEWL>>        if isinstance(path, list):<<NEWL>>            filenames = [create_tempfile(p) for p in path]<<NEWL>>            yield filenames<<NEWL>>        else:<<NEWL>>            filenames = [create_tempfile(path)]<<NEWL>>            yield filenames[0]<<NEWL>>    finally:<<NEWL>>        for f in filenames:<<NEWL>>            safe_remove(f)<<NEWL>><<NEWL>><<NEWL>>def _maybe_remove(store, key):<<NEWL>>    """"""<<NEWL>>    For tests using tables, try removing the table to be sure there is<<NEWL>>    no content from previous tests using the same table name.<<NEWL>>    """"""<<NEWL>>    try:<<NEWL>>        store.remove(key)<<NEWL>>    except (ValueError, KeyError):<<NEWL>>        pass"
463	donghui	2	"from contextlib import contextmanager<<NEWL>>import os<<NEWL>>import tempfile<<NEWL>><<NEWL>>import pytest<<NEWL>><<NEWL>>from pandas.io.pytables import HDFStore<<NEWL>><<NEWL>>tables = pytest.importorskip(""tables"")<<NEWL>># set these parameters so we don't have file sharing<<NEWL>>tables.parameters.MAX_NUMEXPR_THREADS = 1<<NEWL>>tables.parameters.MAX_BLOSC_THREADS = 1<<NEWL>>tables.parameters.MAX_THREADS = 1<<NEWL>><<NEWL>><<NEWL>>def safe_remove(path):<<NEWL>>    if path is not None:<<NEWL>>        try:<<NEWL>>            os.remove(path)  # noqa: PDF008<<NEWL>>        except OSError:<<NEWL>>            pass<<NEWL>><<NEWL>><<NEWL>>def safe_close(store):<<NEWL>>    try:<<NEWL>>        if store is not None:<<NEWL>>            store.close()<<NEWL>>    except OSError:<<NEWL>>        pass<<NEWL>><<NEWL>><<NEWL>>def create_tempfile(path):<<NEWL>>    """"""create an unopened named temporary file""""""<<NEWL>>    return os.path.join(tempfile.gettempdir(), path)<<NEWL>><<NEWL>><<NEWL>># contextmanager to ensure the file cleanup<<NEWL>>@contextmanager<<NEWL>>def ensure_clean_store(path, mode=""a"", complevel=None, complib=None, fletcher32=False):<<NEWL>><<NEWL>>    try:<<NEWL>><<NEWL>>        # put in the temporary path if we don't have one already<<NEWL>>        if not len(os.path.dirname(path)):<<NEWL>>            path = create_tempfile(path)<<NEWL>><<NEWL>>        store = HDFStore(<<NEWL>>            path, mode=mode, complevel=complevel, complib=complib, fletcher32=False<<NEWL>>        )<<NEWL>>        yield store<<NEWL>>    finally:<<NEWL>>        safe_close(store)<<NEWL>>        if mode == ""w"" or mode == ""a"":<<NEWL>>            safe_remove(path)<<NEWL>><<NEWL>><<NEWL>>@contextmanager<<NEWL>>def ensure_clean_path(path):<<NEWL>>    """"""<<NEWL>>    return essentially a named temporary file that is not opened<<NEWL>>    and deleted on exiting; if path is a list, then create and<<NEWL>>    return list of filenames<<NEWL>>    """"""<<NEWL>>    try:<<NEWL>>        if isinstance(path, list):<<NEWL>>            filenames = [create_tempfile(p) for p in path]<<NEWL>>            yield filenames<<NEWL>>        else:<<NEWL>>            filenames = [create_tempfile(path)]<<NEWL>>            yield filenames[0]<<NEWL>>    finally:<<NEWL>>        for f in filenames:<<NEWL>>            safe_remove(f)<<NEWL>><<NEWL>><<NEWL>>def _maybe_remove(store, key):<<NEWL>>    """"""<<NEWL>>    For tests using tables, try removing the table to be sure there is<<NEWL>>    no content from previous tests using the same table name.<<NEWL>>    """"""<<NEWL>>    try:<<NEWL>>        store.remove(key)<<NEWL>>    except (ValueError, KeyError):<<NEWL>>        pass"
432	jackson	0	"from __future__ import absolute_import<<NEWL>><<NEWL>>from django import VERSION as django_version<<NEWL>>from django import forms<<NEWL>>from django.conf import settings<<NEWL>>from django.utils.encoding import force_text<<NEWL>>from django.utils.safestring import mark_safe<<NEWL>>from django.utils.html import format_html<<NEWL>><<NEWL>>from .utils import get_icon_choices<<NEWL>><<NEWL>>CHOICES = get_icon_choices()<<NEWL>><<NEWL>>class IconWidget(forms.Select):<<NEWL>><<NEWL>>    def __init__(self, attrs=None):<<NEWL>>        super(IconWidget, self).__init__(attrs, choices=CHOICES)<<NEWL>><<NEWL>>    if django_version >= (1, 11):<<NEWL>>        def create_option(self, name, value, label, selected, index, subindex=None, attrs=None):<<NEWL>>            option = super(IconWidget, self).create_option(name, value, label, selected, index, subindex=subindex, attrs=attrs)<<NEWL>>            option[""attrs""][""data-icon""] = value<<NEWL>>            return option<<NEWL>>    else:<<NEWL>>        def render_option(self, selected_choices, option_value, option_label):<<NEWL>>            if option_value is None:<<NEWL>>                option_value = ''<<NEWL>>            option_value = force_text(option_value)<<NEWL>>            if option_value in selected_choices:<<NEWL>>                selected_html = mark_safe(' selected=""selected""')<<NEWL>>                if not self.allow_multiple_selected:<<NEWL>>                    # Only allow for a single selection.<<NEWL>>                    selected_choices.remove(option_value)<<NEWL>>            else:<<NEWL>>                selected_html = ''<<NEWL>>            return format_html('<option data-icon=""{0}"" value=""{0}""{1}>{2}</option>',<<NEWL>>                option_value,<<NEWL>>                selected_html,<<NEWL>>                force_text(option_label),<<NEWL>>            )<<NEWL>><<NEWL>>    class Media:<<NEWL>><<NEWL>>        js = (<<NEWL>>            'fontawesome/js/django_fontawesome.js',<<NEWL>>            'fontawesome/select2/select2.min.js'<<NEWL>>        )<<NEWL>><<NEWL>>        css = {<<NEWL>>            'all': (<<NEWL>>                getattr(settings, 'FONTAWESOME_CSS_URL', 'fontawesome/css/font-awesome.min.css'),<<NEWL>>                'fontawesome/select2/select2.css',<<NEWL>>                'fontawesome/select2/select2-bootstrap.css'<<NEWL>>            )<<NEWL>>        }"
432	donghui	0	"from __future__ import absolute_import<<NEWL>><<NEWL>>from django import VERSION as django_version<<NEWL>>from django import forms<<NEWL>>from django.conf import settings<<NEWL>>from django.utils.encoding import force_text<<NEWL>>from django.utils.safestring import mark_safe<<NEWL>>from django.utils.html import format_html<<NEWL>><<NEWL>>from .utils import get_icon_choices<<NEWL>><<NEWL>>CHOICES = get_icon_choices()<<NEWL>><<NEWL>>class IconWidget(forms.Select):<<NEWL>><<NEWL>>    def __init__(self, attrs=None):<<NEWL>>        super(IconWidget, self).__init__(attrs, choices=CHOICES)<<NEWL>><<NEWL>>    if django_version >= (1, 11):<<NEWL>>        def create_option(self, name, value, label, selected, index, subindex=None, attrs=None):<<NEWL>>            option = super(IconWidget, self).create_option(name, value, label, selected, index, subindex=subindex, attrs=attrs)<<NEWL>>            option[""attrs""][""data-icon""] = value<<NEWL>>            return option<<NEWL>>    else:<<NEWL>>        def render_option(self, selected_choices, option_value, option_label):<<NEWL>>            if option_value is None:<<NEWL>>                option_value = ''<<NEWL>>            option_value = force_text(option_value)<<NEWL>>            if option_value in selected_choices:<<NEWL>>                selected_html = mark_safe(' selected=""selected""')<<NEWL>>                if not self.allow_multiple_selected:<<NEWL>>                    # Only allow for a single selection.<<NEWL>>                    selected_choices.remove(option_value)<<NEWL>>            else:<<NEWL>>                selected_html = ''<<NEWL>>            return format_html('<option data-icon=""{0}"" value=""{0}""{1}>{2}</option>',<<NEWL>>                option_value,<<NEWL>>                selected_html,<<NEWL>>                force_text(option_label),<<NEWL>>            )<<NEWL>><<NEWL>>    class Media:<<NEWL>><<NEWL>>        js = (<<NEWL>>            'fontawesome/js/django_fontawesome.js',<<NEWL>>            'fontawesome/select2/select2.min.js'<<NEWL>>        )<<NEWL>><<NEWL>>        css = {<<NEWL>>            'all': (<<NEWL>>                getattr(settings, 'FONTAWESOME_CSS_URL', 'fontawesome/css/font-awesome.min.css'),<<NEWL>>                'fontawesome/select2/select2.css',<<NEWL>>                'fontawesome/select2/select2-bootstrap.css'<<NEWL>>            )<<NEWL>>        }"
490	jackson	2	"import _plotly_utils.basevalidators<<NEWL>><<NEWL>><<NEWL>>class FontValidator(_plotly_utils.basevalidators.CompoundValidator):<<NEWL>>    def __init__(<<NEWL>>        self, plotly_name=""font"", parent_name=""sankey.node.hoverlabel"", **kwargs<<NEWL>>    ):<<NEWL>>        super(FontValidator, self).__init__(<<NEWL>>            plotly_name=plotly_name,<<NEWL>>            parent_name=parent_name,<<NEWL>>            data_class_str=kwargs.pop(""data_class_str"", ""Font""),<<NEWL>>            data_docs=kwargs.pop(<<NEWL>>                ""data_docs"",<<NEWL>>                """"""<<NEWL>>            color<<NEWL>><<NEWL>>            colorsrc<<NEWL>>                Sets the source reference on Chart Studio Cloud<<NEWL>>                for `color`.<<NEWL>>            family<<NEWL>>                HTML font family - the typeface that will be<<NEWL>>                applied by the web browser. The web browser<<NEWL>>                will only be able to apply a font if it is<<NEWL>>                available on the system which it operates.<<NEWL>>                Provide multiple font families, separated by<<NEWL>>                commas, to indicate the preference in which to<<NEWL>>                apply fonts if they aren't available on the<<NEWL>>                system. The Chart Studio Cloud (at<<NEWL>>                https://chart-studio.plotly.com or on-premise)<<NEWL>>                generates images on a server, where only a<<NEWL>>                select number of fonts are installed and<<NEWL>>                supported. These include ""Arial"", ""Balto"",<<NEWL>>                ""Courier New"", ""Droid Sans"",, ""Droid Serif"",<<NEWL>>                ""Droid Sans Mono"", ""Gravitas One"", ""Old<<NEWL>>                Standard TT"", ""Open Sans"", ""Overpass"", ""PT Sans<<NEWL>>                Narrow"", ""Raleway"", ""Times New Roman"".<<NEWL>>            familysrc<<NEWL>>                Sets the source reference on Chart Studio Cloud<<NEWL>>                for `family`.<<NEWL>>            size<<NEWL>><<NEWL>>            sizesrc<<NEWL>>                Sets the source reference on Chart Studio Cloud<<NEWL>>                for `size`.<<NEWL>>"""""",<<NEWL>>            ),<<NEWL>>            **kwargs,<<NEWL>>        )"
490	donghui	1	"import _plotly_utils.basevalidators<<NEWL>><<NEWL>><<NEWL>>class FontValidator(_plotly_utils.basevalidators.CompoundValidator):<<NEWL>>    def __init__(<<NEWL>>        self, plotly_name=""font"", parent_name=""sankey.node.hoverlabel"", **kwargs<<NEWL>>    ):<<NEWL>>        super(FontValidator, self).__init__(<<NEWL>>            plotly_name=plotly_name,<<NEWL>>            parent_name=parent_name,<<NEWL>>            data_class_str=kwargs.pop(""data_class_str"", ""Font""),<<NEWL>>            data_docs=kwargs.pop(<<NEWL>>                ""data_docs"",<<NEWL>>                """"""<<NEWL>>            color<<NEWL>><<NEWL>>            colorsrc<<NEWL>>                Sets the source reference on Chart Studio Cloud<<NEWL>>                for `color`.<<NEWL>>            family<<NEWL>>                HTML font family - the typeface that will be<<NEWL>>                applied by the web browser. The web browser<<NEWL>>                will only be able to apply a font if it is<<NEWL>>                available on the system which it operates.<<NEWL>>                Provide multiple font families, separated by<<NEWL>>                commas, to indicate the preference in which to<<NEWL>>                apply fonts if they aren't available on the<<NEWL>>                system. The Chart Studio Cloud (at<<NEWL>>                https://chart-studio.plotly.com or on-premise)<<NEWL>>                generates images on a server, where only a<<NEWL>>                select number of fonts are installed and<<NEWL>>                supported. These include ""Arial"", ""Balto"",<<NEWL>>                ""Courier New"", ""Droid Sans"",, ""Droid Serif"",<<NEWL>>                ""Droid Sans Mono"", ""Gravitas One"", ""Old<<NEWL>>                Standard TT"", ""Open Sans"", ""Overpass"", ""PT Sans<<NEWL>>                Narrow"", ""Raleway"", ""Times New Roman"".<<NEWL>>            familysrc<<NEWL>>                Sets the source reference on Chart Studio Cloud<<NEWL>>                for `family`.<<NEWL>>            size<<NEWL>><<NEWL>>            sizesrc<<NEWL>>                Sets the source reference on Chart Studio Cloud<<NEWL>>                for `size`.<<NEWL>>"""""",<<NEWL>>            ),<<NEWL>>            **kwargs,<<NEWL>>        )"
481	jackson	3	"""""""<<NEWL>>This module includes some utility functions for inspecting the layout<<NEWL>>of a GDAL data source -- the functionality is analogous to the output<<NEWL>>produced by the `ogrinfo` utility.<<NEWL>>""""""<<NEWL>><<NEWL>>from django.contrib.gis.gdal import DataSource<<NEWL>>from django.contrib.gis.gdal.geometries import GEO_CLASSES<<NEWL>><<NEWL>><<NEWL>>def ogrinfo(data_source, num_features=10):<<NEWL>>    """"""<<NEWL>>    Walk the available layers in the supplied `data_source`, displaying<<NEWL>>    the fields for the first `num_features` features.<<NEWL>>    """"""<<NEWL>><<NEWL>>    # Checking the parameters.<<NEWL>>    if isinstance(data_source, str):<<NEWL>>        data_source = DataSource(data_source)<<NEWL>>    elif isinstance(data_source, DataSource):<<NEWL>>        pass<<NEWL>>    else:<<NEWL>>        raise Exception(<<NEWL>>            ""Data source parameter must be a string or a DataSource object.""<<NEWL>>        )<<NEWL>><<NEWL>>    for i, layer in enumerate(data_source):<<NEWL>>        print(""data source : %s"" % data_source.name)<<NEWL>>        print(""==== layer %s"" % i)<<NEWL>>        print(""  shape type: %s"" % GEO_CLASSES[layer.geom_type.num].__name__)<<NEWL>>        print(""  # features: %s"" % len(layer))<<NEWL>>        print(""         srs: %s"" % layer.srs)<<NEWL>>        extent_tup = layer.extent.tuple<<NEWL>>        print(""      extent: %s - %s"" % (extent_tup[0:2], extent_tup[2:4]))<<NEWL>>        print(""Displaying the first %s features ===="" % num_features)<<NEWL>><<NEWL>>        width = max(*map(len, layer.fields))<<NEWL>>        fmt = "" %%%ss: %%s"" % width<<NEWL>>        for j, feature in enumerate(layer[:num_features]):<<NEWL>>            print(""=== Feature %s"" % j)<<NEWL>>            for fld_name in layer.fields:<<NEWL>>                type_name = feature[fld_name].type_name<<NEWL>>                output = fmt % (fld_name, type_name)<<NEWL>>                val = feature.get(fld_name)<<NEWL>>                if val:<<NEWL>>                    if isinstance(val, str):<<NEWL>>                        val_fmt = ' (""%s"")'<<NEWL>>                    else:<<NEWL>>                        val_fmt = "" (%s)""<<NEWL>>                    output += val_fmt % val<<NEWL>>                else:<<NEWL>>                    output += "" (None)""<<NEWL>>                print(output)"
481	donghui	2	"""""""<<NEWL>>This module includes some utility functions for inspecting the layout<<NEWL>>of a GDAL data source -- the functionality is analogous to the output<<NEWL>>produced by the `ogrinfo` utility.<<NEWL>>""""""<<NEWL>><<NEWL>>from django.contrib.gis.gdal import DataSource<<NEWL>>from django.contrib.gis.gdal.geometries import GEO_CLASSES<<NEWL>><<NEWL>><<NEWL>>def ogrinfo(data_source, num_features=10):<<NEWL>>    """"""<<NEWL>>    Walk the available layers in the supplied `data_source`, displaying<<NEWL>>    the fields for the first `num_features` features.<<NEWL>>    """"""<<NEWL>><<NEWL>>    # Checking the parameters.<<NEWL>>    if isinstance(data_source, str):<<NEWL>>        data_source = DataSource(data_source)<<NEWL>>    elif isinstance(data_source, DataSource):<<NEWL>>        pass<<NEWL>>    else:<<NEWL>>        raise Exception(<<NEWL>>            ""Data source parameter must be a string or a DataSource object.""<<NEWL>>        )<<NEWL>><<NEWL>>    for i, layer in enumerate(data_source):<<NEWL>>        print(""data source : %s"" % data_source.name)<<NEWL>>        print(""==== layer %s"" % i)<<NEWL>>        print(""  shape type: %s"" % GEO_CLASSES[layer.geom_type.num].__name__)<<NEWL>>        print(""  # features: %s"" % len(layer))<<NEWL>>        print(""         srs: %s"" % layer.srs)<<NEWL>>        extent_tup = layer.extent.tuple<<NEWL>>        print(""      extent: %s - %s"" % (extent_tup[0:2], extent_tup[2:4]))<<NEWL>>        print(""Displaying the first %s features ===="" % num_features)<<NEWL>><<NEWL>>        width = max(*map(len, layer.fields))<<NEWL>>        fmt = "" %%%ss: %%s"" % width<<NEWL>>        for j, feature in enumerate(layer[:num_features]):<<NEWL>>            print(""=== Feature %s"" % j)<<NEWL>>            for fld_name in layer.fields:<<NEWL>>                type_name = feature[fld_name].type_name<<NEWL>>                output = fmt % (fld_name, type_name)<<NEWL>>                val = feature.get(fld_name)<<NEWL>>                if val:<<NEWL>>                    if isinstance(val, str):<<NEWL>>                        val_fmt = ' (""%s"")'<<NEWL>>                    else:<<NEWL>>                        val_fmt = "" (%s)""<<NEWL>>                    output += val_fmt % val<<NEWL>>                else:<<NEWL>>                    output += "" (None)""<<NEWL>>                print(output)"
423	jackson	0	import os<<NEWL>>import asyncio<<NEWL>>import pytest<<NEWL>><<NEWL>>import txaio<<NEWL>><<NEWL>># because py.test tries to collect it as a test-case<<NEWL>>from unittest.mock import Mock<<NEWL>><<NEWL>>from autobahn.asyncio.websocket import WebSocketServerFactory<<NEWL>><<NEWL>><<NEWL>>async def echo_async(what, when):<<NEWL>>    await asyncio.sleep(when)<<NEWL>>    return what<<NEWL>><<NEWL>><<NEWL>>@pytest.mark.skipif(not os.environ.get('USE_ASYNCIO', False), reason='test runs on asyncio only')<<NEWL>>@pytest.mark.asyncio<<NEWL>>async def test_echo_async():<<NEWL>>    assert 'Hello!' == await echo_async('Hello!', 0)<<NEWL>><<NEWL>><<NEWL>># @pytest.mark.asyncio(forbid_global_loop=True)<<NEWL>>@pytest.mark.skipif(not os.environ.get('USE_ASYNCIO', False), reason='test runs on asyncio only')<<NEWL>>def test_websocket_custom_loop(event_loop):<<NEWL>>    factory = WebSocketServerFactory(loop=event_loop)<<NEWL>>    server = factory()<<NEWL>>    transport = Mock()<<NEWL>>    server.connection_made(transport)<<NEWL>><<NEWL>><<NEWL>>@pytest.mark.skipif(not os.environ.get('USE_ASYNCIO', False), reason='test runs on asyncio only')<<NEWL>>@pytest.mark.asyncio<<NEWL>>async def test_async_on_connect_server(event_loop):<<NEWL>><<NEWL>>    num = 42<<NEWL>>    done = txaio.create_future()<<NEWL>>    values = []<<NEWL>><<NEWL>>    async def foo(x):<<NEWL>>        await asyncio.sleep(1)<<NEWL>>        return x * x<<NEWL>><<NEWL>>    async def on_connect(req):<<NEWL>>        v = await foo(num)<<NEWL>>        values.append(v)<<NEWL>>        txaio.resolve(done, req)<<NEWL>><<NEWL>>    factory = WebSocketServerFactory()<<NEWL>>    server = factory()<<NEWL>>    server.onConnect = on_connect<<NEWL>>    transport = Mock()<<NEWL>><<NEWL>>    server.connection_made(transport)<<NEWL>>    server.data = b'\r\n'.join([<<NEWL>>        b'GET /ws HTTP/1.1',<<NEWL>>        b'Host: www.example.com',<<NEWL>>        b'Sec-WebSocket-Version: 13',<<NEWL>>        b'Origin: http://www.example.com.malicious.com',<<NEWL>>        b'Sec-WebSocket-Extensions: permessage-deflate',<<NEWL>>        b'Sec-WebSocket-Key: tXAxWFUqnhi86Ajj7dRY5g==',<<NEWL>>        b'Connection: keep-alive, Upgrade',<<NEWL>>        b'Upgrade: websocket',<<NEWL>>        b'\r\n',  # last string doesn't get a \r\n from join()<<NEWL>>    ])<<NEWL>>    server.processHandshake()<<NEWL>>    await done<<NEWL>><<NEWL>>    assert len(values) == 1<<NEWL>>    assert values[0] == num * num
423	donghui	1	import os<<NEWL>>import asyncio<<NEWL>>import pytest<<NEWL>><<NEWL>>import txaio<<NEWL>><<NEWL>># because py.test tries to collect it as a test-case<<NEWL>>from unittest.mock import Mock<<NEWL>><<NEWL>>from autobahn.asyncio.websocket import WebSocketServerFactory<<NEWL>><<NEWL>><<NEWL>>async def echo_async(what, when):<<NEWL>>    await asyncio.sleep(when)<<NEWL>>    return what<<NEWL>><<NEWL>><<NEWL>>@pytest.mark.skipif(not os.environ.get('USE_ASYNCIO', False), reason='test runs on asyncio only')<<NEWL>>@pytest.mark.asyncio<<NEWL>>async def test_echo_async():<<NEWL>>    assert 'Hello!' == await echo_async('Hello!', 0)<<NEWL>><<NEWL>><<NEWL>># @pytest.mark.asyncio(forbid_global_loop=True)<<NEWL>>@pytest.mark.skipif(not os.environ.get('USE_ASYNCIO', False), reason='test runs on asyncio only')<<NEWL>>def test_websocket_custom_loop(event_loop):<<NEWL>>    factory = WebSocketServerFactory(loop=event_loop)<<NEWL>>    server = factory()<<NEWL>>    transport = Mock()<<NEWL>>    server.connection_made(transport)<<NEWL>><<NEWL>><<NEWL>>@pytest.mark.skipif(not os.environ.get('USE_ASYNCIO', False), reason='test runs on asyncio only')<<NEWL>>@pytest.mark.asyncio<<NEWL>>async def test_async_on_connect_server(event_loop):<<NEWL>><<NEWL>>    num = 42<<NEWL>>    done = txaio.create_future()<<NEWL>>    values = []<<NEWL>><<NEWL>>    async def foo(x):<<NEWL>>        await asyncio.sleep(1)<<NEWL>>        return x * x<<NEWL>><<NEWL>>    async def on_connect(req):<<NEWL>>        v = await foo(num)<<NEWL>>        values.append(v)<<NEWL>>        txaio.resolve(done, req)<<NEWL>><<NEWL>>    factory = WebSocketServerFactory()<<NEWL>>    server = factory()<<NEWL>>    server.onConnect = on_connect<<NEWL>>    transport = Mock()<<NEWL>><<NEWL>>    server.connection_made(transport)<<NEWL>>    server.data = b'\r\n'.join([<<NEWL>>        b'GET /ws HTTP/1.1',<<NEWL>>        b'Host: www.example.com',<<NEWL>>        b'Sec-WebSocket-Version: 13',<<NEWL>>        b'Origin: http://www.example.com.malicious.com',<<NEWL>>        b'Sec-WebSocket-Extensions: permessage-deflate',<<NEWL>>        b'Sec-WebSocket-Key: tXAxWFUqnhi86Ajj7dRY5g==',<<NEWL>>        b'Connection: keep-alive, Upgrade',<<NEWL>>        b'Upgrade: websocket',<<NEWL>>        b'\r\n',  # last string doesn't get a \r\n from join()<<NEWL>>    ])<<NEWL>>    server.processHandshake()<<NEWL>>    await done<<NEWL>><<NEWL>>    assert len(values) == 1<<NEWL>>    assert values[0] == num * num
472	jackson	2	"""""""<<NEWL>> The GeometryColumns and SpatialRefSys models for the SpatiaLite backend.<<NEWL>>""""""<<NEWL>>from django.contrib.gis.db.backends.base.models import SpatialRefSysMixin<<NEWL>>from django.db import models<<NEWL>><<NEWL>><<NEWL>>class SpatialiteGeometryColumns(models.Model):<<NEWL>>    """"""<<NEWL>>    The 'geometry_columns' table from SpatiaLite.<<NEWL>>    """"""<<NEWL>><<NEWL>>    f_table_name = models.CharField(max_length=256)<<NEWL>>    f_geometry_column = models.CharField(max_length=256)<<NEWL>>    coord_dimension = models.IntegerField()<<NEWL>>    srid = models.IntegerField(primary_key=True)<<NEWL>>    spatial_index_enabled = models.IntegerField()<<NEWL>>    type = models.IntegerField(db_column=""geometry_type"")<<NEWL>><<NEWL>>    class Meta:<<NEWL>>        app_label = ""gis""<<NEWL>>        db_table = ""geometry_columns""<<NEWL>>        managed = False<<NEWL>><<NEWL>>    def __str__(self):<<NEWL>>        return ""%s.%s - %dD %s field (SRID: %d)"" % (<<NEWL>>            self.f_table_name,<<NEWL>>            self.f_geometry_column,<<NEWL>>            self.coord_dimension,<<NEWL>>            self.type,<<NEWL>>            self.srid,<<NEWL>>        )<<NEWL>><<NEWL>>    @classmethod<<NEWL>>    def table_name_col(cls):<<NEWL>>        """"""<<NEWL>>        Return the name of the metadata column used to store the feature table<<NEWL>>        name.<<NEWL>>        """"""<<NEWL>>        return ""f_table_name""<<NEWL>><<NEWL>>    @classmethod<<NEWL>>    def geom_col_name(cls):<<NEWL>>        """"""<<NEWL>>        Return the name of the metadata column used to store the feature<<NEWL>>        geometry column.<<NEWL>>        """"""<<NEWL>>        return ""f_geometry_column""<<NEWL>><<NEWL>><<NEWL>>class SpatialiteSpatialRefSys(models.Model, SpatialRefSysMixin):<<NEWL>>    """"""<<NEWL>>    The 'spatial_ref_sys' table from SpatiaLite.<<NEWL>>    """"""<<NEWL>><<NEWL>>    srid = models.IntegerField(primary_key=True)<<NEWL>>    auth_name = models.CharField(max_length=256)<<NEWL>>    auth_srid = models.IntegerField()<<NEWL>>    ref_sys_name = models.CharField(max_length=256)<<NEWL>>    proj4text = models.CharField(max_length=2048)<<NEWL>>    srtext = models.CharField(max_length=2048)<<NEWL>><<NEWL>>    class Meta:<<NEWL>>        app_label = ""gis""<<NEWL>>        db_table = ""spatial_ref_sys""<<NEWL>>        managed = False<<NEWL>><<NEWL>>    @property<<NEWL>>    def wkt(self):<<NEWL>>        return self.srtext"
472	donghui	2	"""""""<<NEWL>> The GeometryColumns and SpatialRefSys models for the SpatiaLite backend.<<NEWL>>""""""<<NEWL>>from django.contrib.gis.db.backends.base.models import SpatialRefSysMixin<<NEWL>>from django.db import models<<NEWL>><<NEWL>><<NEWL>>class SpatialiteGeometryColumns(models.Model):<<NEWL>>    """"""<<NEWL>>    The 'geometry_columns' table from SpatiaLite.<<NEWL>>    """"""<<NEWL>><<NEWL>>    f_table_name = models.CharField(max_length=256)<<NEWL>>    f_geometry_column = models.CharField(max_length=256)<<NEWL>>    coord_dimension = models.IntegerField()<<NEWL>>    srid = models.IntegerField(primary_key=True)<<NEWL>>    spatial_index_enabled = models.IntegerField()<<NEWL>>    type = models.IntegerField(db_column=""geometry_type"")<<NEWL>><<NEWL>>    class Meta:<<NEWL>>        app_label = ""gis""<<NEWL>>        db_table = ""geometry_columns""<<NEWL>>        managed = False<<NEWL>><<NEWL>>    def __str__(self):<<NEWL>>        return ""%s.%s - %dD %s field (SRID: %d)"" % (<<NEWL>>            self.f_table_name,<<NEWL>>            self.f_geometry_column,<<NEWL>>            self.coord_dimension,<<NEWL>>            self.type,<<NEWL>>            self.srid,<<NEWL>>        )<<NEWL>><<NEWL>>    @classmethod<<NEWL>>    def table_name_col(cls):<<NEWL>>        """"""<<NEWL>>        Return the name of the metadata column used to store the feature table<<NEWL>>        name.<<NEWL>>        """"""<<NEWL>>        return ""f_table_name""<<NEWL>><<NEWL>>    @classmethod<<NEWL>>    def geom_col_name(cls):<<NEWL>>        """"""<<NEWL>>        Return the name of the metadata column used to store the feature<<NEWL>>        geometry column.<<NEWL>>        """"""<<NEWL>>        return ""f_geometry_column""<<NEWL>><<NEWL>><<NEWL>>class SpatialiteSpatialRefSys(models.Model, SpatialRefSysMixin):<<NEWL>>    """"""<<NEWL>>    The 'spatial_ref_sys' table from SpatiaLite.<<NEWL>>    """"""<<NEWL>><<NEWL>>    srid = models.IntegerField(primary_key=True)<<NEWL>>    auth_name = models.CharField(max_length=256)<<NEWL>>    auth_srid = models.IntegerField()<<NEWL>>    ref_sys_name = models.CharField(max_length=256)<<NEWL>>    proj4text = models.CharField(max_length=2048)<<NEWL>>    srtext = models.CharField(max_length=2048)<<NEWL>><<NEWL>>    class Meta:<<NEWL>>        app_label = ""gis""<<NEWL>>        db_table = ""spatial_ref_sys""<<NEWL>>        managed = False<<NEWL>><<NEWL>>    @property<<NEWL>>    def wkt(self):<<NEWL>>        return self.srtext"
506	jackson	3	"""""""<<NEWL>>    pygments.styles.trac<<NEWL>>    ~~~~~~~~~~~~~~~~~~~~<<NEWL>><<NEWL>>    Port of the default trac highlighter design.<<NEWL>><<NEWL>>    :copyright: Copyright 2006-2022 by the Pygments team, see AUTHORS.<<NEWL>>    :license: BSD, see LICENSE for details.<<NEWL>>""""""<<NEWL>><<NEWL>>from pygments.style import Style<<NEWL>>from pygments.token import Keyword, Name, Comment, String, Error, \<<NEWL>>     Number, Operator, Generic, Whitespace<<NEWL>><<NEWL>><<NEWL>>class TracStyle(Style):<<NEWL>>    """"""<<NEWL>>    Port of the default trac highlighter design.<<NEWL>>    """"""<<NEWL>><<NEWL>>    styles = {<<NEWL>>        Whitespace:             '#bbbbbb',<<NEWL>>        Comment:                'italic #999988',<<NEWL>>        Comment.Preproc:        'bold noitalic #999999',<<NEWL>>        Comment.Special:        'bold #999999',<<NEWL>><<NEWL>>        Operator:               'bold',<<NEWL>><<NEWL>>        String:                 '#bb8844',<<NEWL>>        String.Regex:           '#808000',<<NEWL>><<NEWL>>        Number:                 '#009999',<<NEWL>><<NEWL>>        Keyword:                'bold',<<NEWL>>        Keyword.Type:           '#445588',<<NEWL>><<NEWL>>        Name.Builtin:           '#999999',<<NEWL>>        Name.Function:          'bold #990000',<<NEWL>>        Name.Class:             'bold #445588',<<NEWL>>        Name.Exception:         'bold #990000',<<NEWL>>        Name.Namespace:         '#555555',<<NEWL>>        Name.Variable:          '#008080',<<NEWL>>        Name.Constant:          '#008080',<<NEWL>>        Name.Tag:               '#000080',<<NEWL>>        Name.Attribute:         '#008080',<<NEWL>>        Name.Entity:            '#800080',<<NEWL>><<NEWL>>        Generic.Heading:        '#999999',<<NEWL>>        Generic.Subheading:     '#aaaaaa',<<NEWL>>        Generic.Deleted:        'bg:#ffdddd #000000',<<NEWL>>        Generic.Inserted:       'bg:#ddffdd #000000',<<NEWL>>        Generic.Error:          '#aa0000',<<NEWL>>        Generic.Emph:           'italic',<<NEWL>>        Generic.Strong:         'bold',<<NEWL>>        Generic.Prompt:         '#555555',<<NEWL>>        Generic.Output:         '#888888',<<NEWL>>        Generic.Traceback:      '#aa0000',<<NEWL>><<NEWL>>        Error:                  'bg:#e3d2d2 #a61717'<<NEWL>>    }"
506	donghui	1	"""""""<<NEWL>>    pygments.styles.trac<<NEWL>>    ~~~~~~~~~~~~~~~~~~~~<<NEWL>><<NEWL>>    Port of the default trac highlighter design.<<NEWL>><<NEWL>>    :copyright: Copyright 2006-2022 by the Pygments team, see AUTHORS.<<NEWL>>    :license: BSD, see LICENSE for details.<<NEWL>>""""""<<NEWL>><<NEWL>>from pygments.style import Style<<NEWL>>from pygments.token import Keyword, Name, Comment, String, Error, \<<NEWL>>     Number, Operator, Generic, Whitespace<<NEWL>><<NEWL>><<NEWL>>class TracStyle(Style):<<NEWL>>    """"""<<NEWL>>    Port of the default trac highlighter design.<<NEWL>>    """"""<<NEWL>><<NEWL>>    styles = {<<NEWL>>        Whitespace:             '#bbbbbb',<<NEWL>>        Comment:                'italic #999988',<<NEWL>>        Comment.Preproc:        'bold noitalic #999999',<<NEWL>>        Comment.Special:        'bold #999999',<<NEWL>><<NEWL>>        Operator:               'bold',<<NEWL>><<NEWL>>        String:                 '#bb8844',<<NEWL>>        String.Regex:           '#808000',<<NEWL>><<NEWL>>        Number:                 '#009999',<<NEWL>><<NEWL>>        Keyword:                'bold',<<NEWL>>        Keyword.Type:           '#445588',<<NEWL>><<NEWL>>        Name.Builtin:           '#999999',<<NEWL>>        Name.Function:          'bold #990000',<<NEWL>>        Name.Class:             'bold #445588',<<NEWL>>        Name.Exception:         'bold #990000',<<NEWL>>        Name.Namespace:         '#555555',<<NEWL>>        Name.Variable:          '#008080',<<NEWL>>        Name.Constant:          '#008080',<<NEWL>>        Name.Tag:               '#000080',<<NEWL>>        Name.Attribute:         '#008080',<<NEWL>>        Name.Entity:            '#800080',<<NEWL>><<NEWL>>        Generic.Heading:        '#999999',<<NEWL>>        Generic.Subheading:     '#aaaaaa',<<NEWL>>        Generic.Deleted:        'bg:#ffdddd #000000',<<NEWL>>        Generic.Inserted:       'bg:#ddffdd #000000',<<NEWL>>        Generic.Error:          '#aa0000',<<NEWL>>        Generic.Emph:           'italic',<<NEWL>>        Generic.Strong:         'bold',<<NEWL>>        Generic.Prompt:         '#555555',<<NEWL>>        Generic.Output:         '#888888',<<NEWL>>        Generic.Traceback:      '#aa0000',<<NEWL>><<NEWL>>        Error:                  'bg:#e3d2d2 #a61717'<<NEWL>>    }"
446	jackson	1	"#!/usr/bin/env python3<<NEWL>><<NEWL>>import re<<NEWL>><<NEWL>>from homeassistant.components.binary_sensor import BinarySensorDeviceClass<<NEWL>>from homeassistant.components.button import ButtonDeviceClass<<NEWL>>from homeassistant.components.cover import CoverDeviceClass<<NEWL>>from homeassistant.components.number import NumberDeviceClass<<NEWL>>from homeassistant.components.sensor import SensorDeviceClass<<NEWL>>from homeassistant.components.switch import SwitchDeviceClass<<NEWL>><<NEWL>>BLOCKLIST = (<<NEWL>>    # requires special support on HA side<<NEWL>>    ""enum"",<<NEWL>>)<<NEWL>><<NEWL>>DOMAINS = {<<NEWL>>    ""binary_sensor"": BinarySensorDeviceClass,<<NEWL>>    ""button"": ButtonDeviceClass,<<NEWL>>    ""cover"": CoverDeviceClass,<<NEWL>>    ""number"": NumberDeviceClass,<<NEWL>>    ""sensor"": SensorDeviceClass,<<NEWL>>    ""switch"": SwitchDeviceClass,<<NEWL>>}<<NEWL>><<NEWL>><<NEWL>>def sub(path, pattern, repl):<<NEWL>>    with open(path, ""r"") as handle:<<NEWL>>        content = handle.read()<<NEWL>>    content = re.sub(pattern, repl, content, flags=re.MULTILINE, count=1)<<NEWL>>    with open(path, ""w"") as handle:<<NEWL>>        handle.write(content)<<NEWL>><<NEWL>><<NEWL>>def main():<<NEWL>>    classes = {""EMPTY"": """"}<<NEWL>>    allowed = {}<<NEWL>><<NEWL>>    for domain, enum in DOMAINS.items():<<NEWL>>        available = {<<NEWL>>            cls.value.upper(): cls.value for cls in enum if cls.value not in BLOCKLIST<<NEWL>>        }<<NEWL>><<NEWL>>        classes.update(available)<<NEWL>>        allowed[domain] = list(available.keys()) + [""EMPTY""]<<NEWL>><<NEWL>>    # replace constant defines in const.py<<NEWL>>    out = """"<<NEWL>>    for cls in sorted(classes):<<NEWL>>        out += f'DEVICE_CLASS_{cls.upper()} = ""{classes[cls]}""\n'<<NEWL>>    sub(""esphome/const.py"", '(DEVICE_CLASS_\w+ = ""\w*""\r?\n)+', out)<<NEWL>><<NEWL>>    for domain in sorted(allowed):<<NEWL>>        # replace imports<<NEWL>>        out = """"<<NEWL>>        for item in sorted(allowed[domain]):<<NEWL>>            out += f""    DEVICE_CLASS_{item.upper()},\n""<<NEWL>><<NEWL>>        sub(<<NEWL>>            f""esphome/components/{domain}/__init__.py"",<<NEWL>>            ""(    DEVICE_CLASS_\w+,\r?\n)+"",<<NEWL>>            out,<<NEWL>>        )<<NEWL>><<NEWL>><<NEWL>>if __name__ == ""__main__"":<<NEWL>>    main()"
446	donghui	0	"#!/usr/bin/env python3<<NEWL>><<NEWL>>import re<<NEWL>><<NEWL>>from homeassistant.components.binary_sensor import BinarySensorDeviceClass<<NEWL>>from homeassistant.components.button import ButtonDeviceClass<<NEWL>>from homeassistant.components.cover import CoverDeviceClass<<NEWL>>from homeassistant.components.number import NumberDeviceClass<<NEWL>>from homeassistant.components.sensor import SensorDeviceClass<<NEWL>>from homeassistant.components.switch import SwitchDeviceClass<<NEWL>><<NEWL>>BLOCKLIST = (<<NEWL>>    # requires special support on HA side<<NEWL>>    ""enum"",<<NEWL>>)<<NEWL>><<NEWL>>DOMAINS = {<<NEWL>>    ""binary_sensor"": BinarySensorDeviceClass,<<NEWL>>    ""button"": ButtonDeviceClass,<<NEWL>>    ""cover"": CoverDeviceClass,<<NEWL>>    ""number"": NumberDeviceClass,<<NEWL>>    ""sensor"": SensorDeviceClass,<<NEWL>>    ""switch"": SwitchDeviceClass,<<NEWL>>}<<NEWL>><<NEWL>><<NEWL>>def sub(path, pattern, repl):<<NEWL>>    with open(path, ""r"") as handle:<<NEWL>>        content = handle.read()<<NEWL>>    content = re.sub(pattern, repl, content, flags=re.MULTILINE, count=1)<<NEWL>>    with open(path, ""w"") as handle:<<NEWL>>        handle.write(content)<<NEWL>><<NEWL>><<NEWL>>def main():<<NEWL>>    classes = {""EMPTY"": """"}<<NEWL>>    allowed = {}<<NEWL>><<NEWL>>    for domain, enum in DOMAINS.items():<<NEWL>>        available = {<<NEWL>>            cls.value.upper(): cls.value for cls in enum if cls.value not in BLOCKLIST<<NEWL>>        }<<NEWL>><<NEWL>>        classes.update(available)<<NEWL>>        allowed[domain] = list(available.keys()) + [""EMPTY""]<<NEWL>><<NEWL>>    # replace constant defines in const.py<<NEWL>>    out = """"<<NEWL>>    for cls in sorted(classes):<<NEWL>>        out += f'DEVICE_CLASS_{cls.upper()} = ""{classes[cls]}""\n'<<NEWL>>    sub(""esphome/const.py"", '(DEVICE_CLASS_\w+ = ""\w*""\r?\n)+', out)<<NEWL>><<NEWL>>    for domain in sorted(allowed):<<NEWL>>        # replace imports<<NEWL>>        out = """"<<NEWL>>        for item in sorted(allowed[domain]):<<NEWL>>            out += f""    DEVICE_CLASS_{item.upper()},\n""<<NEWL>><<NEWL>>        sub(<<NEWL>>            f""esphome/components/{domain}/__init__.py"",<<NEWL>>            ""(    DEVICE_CLASS_\w+,\r?\n)+"",<<NEWL>>            out,<<NEWL>>        )<<NEWL>><<NEWL>><<NEWL>>if __name__ == ""__main__"":<<NEWL>>    main()"
417	jackson	0	"import pytest<<NEWL>>from spacy.lang.en import English<<NEWL>>from spacy.training import Example<<NEWL>>from thinc.api import Config<<NEWL>><<NEWL>>default_tok2vec_config = """"""<<NEWL>>[model]<<NEWL>>@architectures = ""spacy-legacy.HashEmbedCNN.v1""<<NEWL>>pretrained_vectors = null<<NEWL>>width = 96<<NEWL>>depth = 4<<NEWL>>embed_size = 2000<<NEWL>>window_size = 1<<NEWL>>maxout_pieces = 3<<NEWL>>subword_features = true<<NEWL>>""""""<<NEWL>>DEFAULT_TOK2VEC_MODEL = Config().from_str(default_tok2vec_config)[""model""]<<NEWL>><<NEWL>>TRAIN_DATA = [<<NEWL>>    (<<NEWL>>        ""They trade mortgage-backed securities."",<<NEWL>>        {<<NEWL>>            ""heads"": [1, 1, 4, 4, 5, 1, 1],<<NEWL>>            ""deps"": [""nsubj"", ""ROOT"", ""compound"", ""punct"", ""nmod"", ""dobj"", ""punct""],<<NEWL>>        },<<NEWL>>    ),<<NEWL>>    (<<NEWL>>        ""I like London and Berlin."",<<NEWL>>        {<<NEWL>>            ""heads"": [1, 1, 1, 2, 2, 1],<<NEWL>>            ""deps"": [""nsubj"", ""ROOT"", ""dobj"", ""cc"", ""conj"", ""punct""],<<NEWL>>        },<<NEWL>>    ),<<NEWL>>]<<NEWL>><<NEWL>><<NEWL>>@pytest.mark.parametrize(<<NEWL>>    ""parser_config"",<<NEWL>>    [<<NEWL>>        {<<NEWL>>            ""@architectures"": ""spacy-legacy.TransitionBasedParser.v1"",<<NEWL>>            ""state_type"": ""parser"",<<NEWL>>            ""extra_state_tokens"": False,<<NEWL>>            ""hidden_width"": 66,<<NEWL>>            ""maxout_pieces"": 2,<<NEWL>>            ""use_upper"": True,<<NEWL>>            ""tok2vec"": DEFAULT_TOK2VEC_MODEL,<<NEWL>>        }<<NEWL>>    ],<<NEWL>>)<<NEWL>>def test_parser(parser_config):<<NEWL>>    pipe_config = {""model"": parser_config}<<NEWL>>    nlp = English()<<NEWL>>    parser = nlp.add_pipe(""parser"", config=pipe_config)<<NEWL>>    train_examples = []<<NEWL>>    for text, annotations in TRAIN_DATA:<<NEWL>>        train_examples.append(Example.from_dict(nlp.make_doc(text), annotations))<<NEWL>>        for dep in annotations.get(""deps"", []):<<NEWL>>            if dep is not None:<<NEWL>>                parser.add_label(dep)<<NEWL>>    optimizer = nlp.initialize(get_examples=lambda: train_examples)<<NEWL>>    for i in range(150):<<NEWL>>        losses = {}<<NEWL>>        nlp.update(train_examples, sgd=optimizer, losses=losses)<<NEWL>>    assert losses[""parser""] < 0.0001"
417	donghui	0	"import pytest<<NEWL>>from spacy.lang.en import English<<NEWL>>from spacy.training import Example<<NEWL>>from thinc.api import Config<<NEWL>><<NEWL>>default_tok2vec_config = """"""<<NEWL>>[model]<<NEWL>>@architectures = ""spacy-legacy.HashEmbedCNN.v1""<<NEWL>>pretrained_vectors = null<<NEWL>>width = 96<<NEWL>>depth = 4<<NEWL>>embed_size = 2000<<NEWL>>window_size = 1<<NEWL>>maxout_pieces = 3<<NEWL>>subword_features = true<<NEWL>>""""""<<NEWL>>DEFAULT_TOK2VEC_MODEL = Config().from_str(default_tok2vec_config)[""model""]<<NEWL>><<NEWL>>TRAIN_DATA = [<<NEWL>>    (<<NEWL>>        ""They trade mortgage-backed securities."",<<NEWL>>        {<<NEWL>>            ""heads"": [1, 1, 4, 4, 5, 1, 1],<<NEWL>>            ""deps"": [""nsubj"", ""ROOT"", ""compound"", ""punct"", ""nmod"", ""dobj"", ""punct""],<<NEWL>>        },<<NEWL>>    ),<<NEWL>>    (<<NEWL>>        ""I like London and Berlin."",<<NEWL>>        {<<NEWL>>            ""heads"": [1, 1, 1, 2, 2, 1],<<NEWL>>            ""deps"": [""nsubj"", ""ROOT"", ""dobj"", ""cc"", ""conj"", ""punct""],<<NEWL>>        },<<NEWL>>    ),<<NEWL>>]<<NEWL>><<NEWL>><<NEWL>>@pytest.mark.parametrize(<<NEWL>>    ""parser_config"",<<NEWL>>    [<<NEWL>>        {<<NEWL>>            ""@architectures"": ""spacy-legacy.TransitionBasedParser.v1"",<<NEWL>>            ""state_type"": ""parser"",<<NEWL>>            ""extra_state_tokens"": False,<<NEWL>>            ""hidden_width"": 66,<<NEWL>>            ""maxout_pieces"": 2,<<NEWL>>            ""use_upper"": True,<<NEWL>>            ""tok2vec"": DEFAULT_TOK2VEC_MODEL,<<NEWL>>        }<<NEWL>>    ],<<NEWL>>)<<NEWL>>def test_parser(parser_config):<<NEWL>>    pipe_config = {""model"": parser_config}<<NEWL>>    nlp = English()<<NEWL>>    parser = nlp.add_pipe(""parser"", config=pipe_config)<<NEWL>>    train_examples = []<<NEWL>>    for text, annotations in TRAIN_DATA:<<NEWL>>        train_examples.append(Example.from_dict(nlp.make_doc(text), annotations))<<NEWL>>        for dep in annotations.get(""deps"", []):<<NEWL>>            if dep is not None:<<NEWL>>                parser.add_label(dep)<<NEWL>>    optimizer = nlp.initialize(get_examples=lambda: train_examples)<<NEWL>>    for i in range(150):<<NEWL>>        losses = {}<<NEWL>>        nlp.update(train_examples, sgd=optimizer, losses=losses)<<NEWL>>    assert losses[""parser""] < 0.0001"
359	jackson	1	"######################## BEGIN LICENSE BLOCK ########################<<NEWL>># The Original Code is mozilla.org code.<<NEWL>>#<<NEWL>># The Initial Developer of the Original Code is<<NEWL>># Netscape Communications Corporation.<<NEWL>># Portions created by the Initial Developer are Copyright (C) 1998<<NEWL>># the Initial Developer. All Rights Reserved.<<NEWL>>#<<NEWL>># Contributor(s):<<NEWL>>#   Mark Pilgrim - port to Python<<NEWL>>#<<NEWL>># This library is free software; you can redistribute it and/or<<NEWL>># modify it under the terms of the GNU Lesser General Public<<NEWL>># License as published by the Free Software Foundation; either<<NEWL>># version 2.1 of the License, or (at your option) any later version.<<NEWL>>#<<NEWL>># This library is distributed in the hope that it will be useful,<<NEWL>># but WITHOUT ANY WARRANTY; without even the implied warranty of<<NEWL>># MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU<<NEWL>># Lesser General Public License for more details.<<NEWL>>#<<NEWL>># You should have received a copy of the GNU Lesser General Public<<NEWL>># License along with this library; if not, write to the Free Software<<NEWL>># Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA<<NEWL>># 02110-1301  USA<<NEWL>>######################### END LICENSE BLOCK #########################<<NEWL>><<NEWL>>from .chardistribution import EUCKRDistributionAnalysis<<NEWL>>from .codingstatemachine import CodingStateMachine<<NEWL>>from .mbcharsetprober import MultiByteCharSetProber<<NEWL>>from .mbcssm import CP949_SM_MODEL<<NEWL>><<NEWL>><<NEWL>>class CP949Prober(MultiByteCharSetProber):<<NEWL>>    def __init__(self):<<NEWL>>        super(CP949Prober, self).__init__()<<NEWL>>        self.coding_sm = CodingStateMachine(CP949_SM_MODEL)<<NEWL>>        # NOTE: CP949 is a superset of EUC-KR, so the distribution should be<<NEWL>>        #       not different.<<NEWL>>        self.distribution_analyzer = EUCKRDistributionAnalysis()<<NEWL>>        self.reset()<<NEWL>><<NEWL>>    @property<<NEWL>>    def charset_name(self):<<NEWL>>        return ""CP949""<<NEWL>><<NEWL>>    @property<<NEWL>>    def language(self):<<NEWL>>        return ""Korean"""
359	donghui	1	"######################## BEGIN LICENSE BLOCK ########################<<NEWL>># The Original Code is mozilla.org code.<<NEWL>>#<<NEWL>># The Initial Developer of the Original Code is<<NEWL>># Netscape Communications Corporation.<<NEWL>># Portions created by the Initial Developer are Copyright (C) 1998<<NEWL>># the Initial Developer. All Rights Reserved.<<NEWL>>#<<NEWL>># Contributor(s):<<NEWL>>#   Mark Pilgrim - port to Python<<NEWL>>#<<NEWL>># This library is free software; you can redistribute it and/or<<NEWL>># modify it under the terms of the GNU Lesser General Public<<NEWL>># License as published by the Free Software Foundation; either<<NEWL>># version 2.1 of the License, or (at your option) any later version.<<NEWL>>#<<NEWL>># This library is distributed in the hope that it will be useful,<<NEWL>># but WITHOUT ANY WARRANTY; without even the implied warranty of<<NEWL>># MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU<<NEWL>># Lesser General Public License for more details.<<NEWL>>#<<NEWL>># You should have received a copy of the GNU Lesser General Public<<NEWL>># License along with this library; if not, write to the Free Software<<NEWL>># Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA<<NEWL>># 02110-1301  USA<<NEWL>>######################### END LICENSE BLOCK #########################<<NEWL>><<NEWL>>from .chardistribution import EUCKRDistributionAnalysis<<NEWL>>from .codingstatemachine import CodingStateMachine<<NEWL>>from .mbcharsetprober import MultiByteCharSetProber<<NEWL>>from .mbcssm import CP949_SM_MODEL<<NEWL>><<NEWL>><<NEWL>>class CP949Prober(MultiByteCharSetProber):<<NEWL>>    def __init__(self):<<NEWL>>        super(CP949Prober, self).__init__()<<NEWL>>        self.coding_sm = CodingStateMachine(CP949_SM_MODEL)<<NEWL>>        # NOTE: CP949 is a superset of EUC-KR, so the distribution should be<<NEWL>>        #       not different.<<NEWL>>        self.distribution_analyzer = EUCKRDistributionAnalysis()<<NEWL>>        self.reset()<<NEWL>><<NEWL>>    @property<<NEWL>>    def charset_name(self):<<NEWL>>        return ""CP949""<<NEWL>><<NEWL>>    @property<<NEWL>>    def language(self):<<NEWL>>        return ""Korean"""
308	jackson	2	"""""""<<NEWL>>    pygments.lexers.jmespath<<NEWL>>    ~~~~~~~~~~~~~~~~~~~~~~~~<<NEWL>><<NEWL>>    Lexers for the JMESPath language<<NEWL>><<NEWL>>    :copyright: Copyright 2006-2022 by the Pygments team, see AUTHORS.<<NEWL>>    :license: BSD, see LICENSE for details.<<NEWL>>""""""<<NEWL>><<NEWL>>from pygments.lexer import RegexLexer, bygroups, include<<NEWL>>from pygments.token import String, Punctuation, Whitespace, Name, Operator, \<<NEWL>>    Number, Literal, Keyword<<NEWL>><<NEWL>>__all__ = ['JMESPathLexer']<<NEWL>><<NEWL>><<NEWL>>class JMESPathLexer(RegexLexer):<<NEWL>>    """"""<<NEWL>>    For JMESPath queries.<<NEWL>>    """"""<<NEWL>>    name = 'JMESPath'<<NEWL>>    url = 'https://jmespath.org'<<NEWL>>    filenames = ['*.jp']<<NEWL>>    aliases = ['jmespath', 'jp']<<NEWL>><<NEWL>>    tokens = {<<NEWL>>        'string': [<<NEWL>>            (r""'(\\(.|\n)|[^'\\])*'"", String),<<NEWL>>        ],<<NEWL>>        'punctuation': [<<NEWL>>            (r'(\[\?|[\.\*\[\],:\(\)\{\}\|])', Punctuation),<<NEWL>>        ],<<NEWL>>        'ws': [<<NEWL>>            (r"" |\t|\n|\r"", Whitespace)<<NEWL>>        ],<<NEWL>>        ""dq-identifier"": [<<NEWL>>            (r'[^\\""]+', Name.Variable),<<NEWL>>            (r'\\""', Name.Variable),<<NEWL>>            (r'.', Punctuation, '#pop'),<<NEWL>>        ],<<NEWL>>        'identifier': [<<NEWL>>            (r'(&)?("")', bygroups(Name.Variable, Punctuation), 'dq-identifier'),<<NEWL>>            (r'("")?(&?[A-Za-z][A-Za-z0-9_-]*)("")?', bygroups(Punctuation, Name.Variable, Punctuation)),<<NEWL>>        ],<<NEWL>>        'root': [<<NEWL>>            include('ws'),<<NEWL>>            include('string'),<<NEWL>>            (r'(==|!=|<=|>=|<|>|&&|\|\||!)', Operator),<<NEWL>>            include('punctuation'),<<NEWL>>            (r'@', Name.Variable.Global),<<NEWL>>            (r'(&?[A-Za-z][A-Za-z0-9_]*)(\()', bygroups(Name.Function, Punctuation)),<<NEWL>>            (r'(&)(\()', bygroups(Name.Variable, Punctuation)),<<NEWL>>            include('identifier'),<<NEWL>>            (r'-?\d+', Number),<<NEWL>>            (r'`', Literal, 'literal'),<<NEWL>>        ],<<NEWL>>        'literal': [<<NEWL>>            include('ws'),<<NEWL>>            include('string'),<<NEWL>>            include('punctuation'),<<NEWL>>            (r'(false|true|null)\b', Keyword.Constant),<<NEWL>>            include('identifier'),<<NEWL>>            (r'-?\d+\.?\d*([eE][-+]\d+)?', Number),<<NEWL>>            (r'\\`', Literal),<<NEWL>>            (r'`', Literal, '#pop'),<<NEWL>>        ]<<NEWL>>    }"
308	donghui	1	"""""""<<NEWL>>    pygments.lexers.jmespath<<NEWL>>    ~~~~~~~~~~~~~~~~~~~~~~~~<<NEWL>><<NEWL>>    Lexers for the JMESPath language<<NEWL>><<NEWL>>    :copyright: Copyright 2006-2022 by the Pygments team, see AUTHORS.<<NEWL>>    :license: BSD, see LICENSE for details.<<NEWL>>""""""<<NEWL>><<NEWL>>from pygments.lexer import RegexLexer, bygroups, include<<NEWL>>from pygments.token import String, Punctuation, Whitespace, Name, Operator, \<<NEWL>>    Number, Literal, Keyword<<NEWL>><<NEWL>>__all__ = ['JMESPathLexer']<<NEWL>><<NEWL>><<NEWL>>class JMESPathLexer(RegexLexer):<<NEWL>>    """"""<<NEWL>>    For JMESPath queries.<<NEWL>>    """"""<<NEWL>>    name = 'JMESPath'<<NEWL>>    url = 'https://jmespath.org'<<NEWL>>    filenames = ['*.jp']<<NEWL>>    aliases = ['jmespath', 'jp']<<NEWL>><<NEWL>>    tokens = {<<NEWL>>        'string': [<<NEWL>>            (r""'(\\(.|\n)|[^'\\])*'"", String),<<NEWL>>        ],<<NEWL>>        'punctuation': [<<NEWL>>            (r'(\[\?|[\.\*\[\],:\(\)\{\}\|])', Punctuation),<<NEWL>>        ],<<NEWL>>        'ws': [<<NEWL>>            (r"" |\t|\n|\r"", Whitespace)<<NEWL>>        ],<<NEWL>>        ""dq-identifier"": [<<NEWL>>            (r'[^\\""]+', Name.Variable),<<NEWL>>            (r'\\""', Name.Variable),<<NEWL>>            (r'.', Punctuation, '#pop'),<<NEWL>>        ],<<NEWL>>        'identifier': [<<NEWL>>            (r'(&)?("")', bygroups(Name.Variable, Punctuation), 'dq-identifier'),<<NEWL>>            (r'("")?(&?[A-Za-z][A-Za-z0-9_-]*)("")?', bygroups(Punctuation, Name.Variable, Punctuation)),<<NEWL>>        ],<<NEWL>>        'root': [<<NEWL>>            include('ws'),<<NEWL>>            include('string'),<<NEWL>>            (r'(==|!=|<=|>=|<|>|&&|\|\||!)', Operator),<<NEWL>>            include('punctuation'),<<NEWL>>            (r'@', Name.Variable.Global),<<NEWL>>            (r'(&?[A-Za-z][A-Za-z0-9_]*)(\()', bygroups(Name.Function, Punctuation)),<<NEWL>>            (r'(&)(\()', bygroups(Name.Variable, Punctuation)),<<NEWL>>            include('identifier'),<<NEWL>>            (r'-?\d+', Number),<<NEWL>>            (r'`', Literal, 'literal'),<<NEWL>>        ],<<NEWL>>        'literal': [<<NEWL>>            include('ws'),<<NEWL>>            include('string'),<<NEWL>>            include('punctuation'),<<NEWL>>            (r'(false|true|null)\b', Keyword.Constant),<<NEWL>>            include('identifier'),<<NEWL>>            (r'-?\d+\.?\d*([eE][-+]\d+)?', Number),<<NEWL>>            (r'\\`', Literal),<<NEWL>>            (r'`', Literal, '#pop'),<<NEWL>>        ]<<NEWL>>    }"
318	jackson	3	"""""""<<NEWL>>    pygments.lexers.graphviz<<NEWL>>    ~~~~~~~~~~~~~~~~~~~~~~~~<<NEWL>><<NEWL>>    Lexer for the DOT language (graphviz).<<NEWL>><<NEWL>>    :copyright: Copyright 2006-2022 by the Pygments team, see AUTHORS.<<NEWL>>    :license: BSD, see LICENSE for details.<<NEWL>>""""""<<NEWL>><<NEWL>>from pygments.lexer import RegexLexer, bygroups<<NEWL>>from pygments.token import Comment, Keyword, Operator, Name, String, Number, \<<NEWL>>    Punctuation, Whitespace<<NEWL>><<NEWL>><<NEWL>>__all__ = ['GraphvizLexer']<<NEWL>><<NEWL>><<NEWL>>class GraphvizLexer(RegexLexer):<<NEWL>>    """"""<<NEWL>>    For graphviz DOT graph description language.<<NEWL>><<NEWL>>    .. versionadded:: 2.8<<NEWL>>    """"""<<NEWL>>    name = 'Graphviz'<<NEWL>>    url = 'https://www.graphviz.org/doc/info/lang.html'<<NEWL>>    aliases = ['graphviz', 'dot']<<NEWL>>    filenames = ['*.gv', '*.dot']<<NEWL>>    mimetypes = ['text/x-graphviz', 'text/vnd.graphviz']<<NEWL>>    tokens = {<<NEWL>>        'root': [<<NEWL>>            (r'\s+', Whitespace),<<NEWL>>            (r'(#|//).*?$', Comment.Single),<<NEWL>>            (r'/(\\\n)?[*](.|\n)*?[*](\\\n)?/', Comment.Multiline),<<NEWL>>            (r'(?i)(node|edge|graph|digraph|subgraph|strict)\b', Keyword),<<NEWL>>            (r'--|->', Operator),<<NEWL>>            (r'[{}[\]:;,]', Punctuation),<<NEWL>>            (r'(\b\D\w*)(\s*)(=)(\s*)',<<NEWL>>                bygroups(Name.Attribute, Whitespace, Punctuation, Whitespace),<<NEWL>>                'attr_id'),<<NEWL>>            (r'\b(n|ne|e|se|s|sw|w|nw|c|_)\b', Name.Builtin),<<NEWL>>            (r'\b\D\w*', Name.Tag),  # node<<NEWL>>            (r'[-]?((\.[0-9]+)|([0-9]+(\.[0-9]*)?))', Number),<<NEWL>>            (r'""(\\""|[^""])*?""', Name.Tag),  # quoted node<<NEWL>>            (r'<', Punctuation, 'xml'),<<NEWL>>        ],<<NEWL>>        'attr_id': [<<NEWL>>            (r'\b\D\w*', String, '#pop'),<<NEWL>>            (r'[-]?((\.[0-9]+)|([0-9]+(\.[0-9]*)?))', Number, '#pop'),<<NEWL>>            (r'""(\\""|[^""])*?""', String.Double, '#pop'),<<NEWL>>            (r'<', Punctuation, ('#pop', 'xml')),<<NEWL>>        ],<<NEWL>>        'xml': [<<NEWL>>            (r'<', Punctuation, '#push'),<<NEWL>>            (r'>', Punctuation, '#pop'),<<NEWL>>            (r'\s+', Whitespace),<<NEWL>>            (r'[^<>\s]', Name.Tag),<<NEWL>>        ]<<NEWL>>    }"
318	donghui	1	"""""""<<NEWL>>    pygments.lexers.graphviz<<NEWL>>    ~~~~~~~~~~~~~~~~~~~~~~~~<<NEWL>><<NEWL>>    Lexer for the DOT language (graphviz).<<NEWL>><<NEWL>>    :copyright: Copyright 2006-2022 by the Pygments team, see AUTHORS.<<NEWL>>    :license: BSD, see LICENSE for details.<<NEWL>>""""""<<NEWL>><<NEWL>>from pygments.lexer import RegexLexer, bygroups<<NEWL>>from pygments.token import Comment, Keyword, Operator, Name, String, Number, \<<NEWL>>    Punctuation, Whitespace<<NEWL>><<NEWL>><<NEWL>>__all__ = ['GraphvizLexer']<<NEWL>><<NEWL>><<NEWL>>class GraphvizLexer(RegexLexer):<<NEWL>>    """"""<<NEWL>>    For graphviz DOT graph description language.<<NEWL>><<NEWL>>    .. versionadded:: 2.8<<NEWL>>    """"""<<NEWL>>    name = 'Graphviz'<<NEWL>>    url = 'https://www.graphviz.org/doc/info/lang.html'<<NEWL>>    aliases = ['graphviz', 'dot']<<NEWL>>    filenames = ['*.gv', '*.dot']<<NEWL>>    mimetypes = ['text/x-graphviz', 'text/vnd.graphviz']<<NEWL>>    tokens = {<<NEWL>>        'root': [<<NEWL>>            (r'\s+', Whitespace),<<NEWL>>            (r'(#|//).*?$', Comment.Single),<<NEWL>>            (r'/(\\\n)?[*](.|\n)*?[*](\\\n)?/', Comment.Multiline),<<NEWL>>            (r'(?i)(node|edge|graph|digraph|subgraph|strict)\b', Keyword),<<NEWL>>            (r'--|->', Operator),<<NEWL>>            (r'[{}[\]:;,]', Punctuation),<<NEWL>>            (r'(\b\D\w*)(\s*)(=)(\s*)',<<NEWL>>                bygroups(Name.Attribute, Whitespace, Punctuation, Whitespace),<<NEWL>>                'attr_id'),<<NEWL>>            (r'\b(n|ne|e|se|s|sw|w|nw|c|_)\b', Name.Builtin),<<NEWL>>            (r'\b\D\w*', Name.Tag),  # node<<NEWL>>            (r'[-]?((\.[0-9]+)|([0-9]+(\.[0-9]*)?))', Number),<<NEWL>>            (r'""(\\""|[^""])*?""', Name.Tag),  # quoted node<<NEWL>>            (r'<', Punctuation, 'xml'),<<NEWL>>        ],<<NEWL>>        'attr_id': [<<NEWL>>            (r'\b\D\w*', String, '#pop'),<<NEWL>>            (r'[-]?((\.[0-9]+)|([0-9]+(\.[0-9]*)?))', Number, '#pop'),<<NEWL>>            (r'""(\\""|[^""])*?""', String.Double, '#pop'),<<NEWL>>            (r'<', Punctuation, ('#pop', 'xml')),<<NEWL>>        ],<<NEWL>>        'xml': [<<NEWL>>            (r'<', Punctuation, '#push'),<<NEWL>>            (r'>', Punctuation, '#pop'),<<NEWL>>            (r'\s+', Whitespace),<<NEWL>>            (r'[^<>\s]', Name.Tag),<<NEWL>>        ]<<NEWL>>    }"
258	jackson	2	"import re<<NEWL>>import textwrap<<NEWL>>import email.message<<NEWL>><<NEWL>>from ._text import FoldedCase<<NEWL>><<NEWL>><<NEWL>>class Message(email.message.Message):<<NEWL>>    multiple_use_keys = set(<<NEWL>>        map(<<NEWL>>            FoldedCase,<<NEWL>>            [<<NEWL>>                'Classifier',<<NEWL>>                'Obsoletes-Dist',<<NEWL>>                'Platform',<<NEWL>>                'Project-URL',<<NEWL>>                'Provides-Dist',<<NEWL>>                'Provides-Extra',<<NEWL>>                'Requires-Dist',<<NEWL>>                'Requires-External',<<NEWL>>                'Supported-Platform',<<NEWL>>                'Dynamic',<<NEWL>>            ],<<NEWL>>        )<<NEWL>>    )<<NEWL>>    """"""<<NEWL>>    Keys that may be indicated multiple times per PEP 566.<<NEWL>>    """"""<<NEWL>><<NEWL>>    def __new__(cls, orig: email.message.Message):<<NEWL>>        res = super().__new__(cls)<<NEWL>>        vars(res).update(vars(orig))<<NEWL>>        return res<<NEWL>><<NEWL>>    def __init__(self, *args, **kwargs):<<NEWL>>        self._headers = self._repair_headers()<<NEWL>><<NEWL>>    # suppress spurious error from mypy<<NEWL>>    def __iter__(self):<<NEWL>>        return super().__iter__()<<NEWL>><<NEWL>>    def _repair_headers(self):<<NEWL>>        def redent(value):<<NEWL>>            ""Correct for RFC822 indentation""<<NEWL>>            if not value or '\n' not in value:<<NEWL>>                return value<<NEWL>>            return textwrap.dedent(' ' * 8 + value)<<NEWL>><<NEWL>>        headers = [(key, redent(value)) for key, value in vars(self)['_headers']]<<NEWL>>        if self._payload:<<NEWL>>            headers.append(('Description', self.get_payload()))<<NEWL>>        return headers<<NEWL>><<NEWL>>    @property<<NEWL>>    def json(self):<<NEWL>>        """"""<<NEWL>>        Convert PackageMetadata to a JSON-compatible format<<NEWL>>        per PEP 0566.<<NEWL>>        """"""<<NEWL>><<NEWL>>        def transform(key):<<NEWL>>            value = self.get_all(key) if key in self.multiple_use_keys else self[key]<<NEWL>>            if key == 'Keywords':<<NEWL>>                value = re.split(r'\s+', value)<<NEWL>>            tk = key.lower().replace('-', '_')<<NEWL>>            return tk, value<<NEWL>><<NEWL>>        return dict(map(transform, map(FoldedCase, self)))"
258	donghui	2	"import re<<NEWL>>import textwrap<<NEWL>>import email.message<<NEWL>><<NEWL>>from ._text import FoldedCase<<NEWL>><<NEWL>><<NEWL>>class Message(email.message.Message):<<NEWL>>    multiple_use_keys = set(<<NEWL>>        map(<<NEWL>>            FoldedCase,<<NEWL>>            [<<NEWL>>                'Classifier',<<NEWL>>                'Obsoletes-Dist',<<NEWL>>                'Platform',<<NEWL>>                'Project-URL',<<NEWL>>                'Provides-Dist',<<NEWL>>                'Provides-Extra',<<NEWL>>                'Requires-Dist',<<NEWL>>                'Requires-External',<<NEWL>>                'Supported-Platform',<<NEWL>>                'Dynamic',<<NEWL>>            ],<<NEWL>>        )<<NEWL>>    )<<NEWL>>    """"""<<NEWL>>    Keys that may be indicated multiple times per PEP 566.<<NEWL>>    """"""<<NEWL>><<NEWL>>    def __new__(cls, orig: email.message.Message):<<NEWL>>        res = super().__new__(cls)<<NEWL>>        vars(res).update(vars(orig))<<NEWL>>        return res<<NEWL>><<NEWL>>    def __init__(self, *args, **kwargs):<<NEWL>>        self._headers = self._repair_headers()<<NEWL>><<NEWL>>    # suppress spurious error from mypy<<NEWL>>    def __iter__(self):<<NEWL>>        return super().__iter__()<<NEWL>><<NEWL>>    def _repair_headers(self):<<NEWL>>        def redent(value):<<NEWL>>            ""Correct for RFC822 indentation""<<NEWL>>            if not value or '\n' not in value:<<NEWL>>                return value<<NEWL>>            return textwrap.dedent(' ' * 8 + value)<<NEWL>><<NEWL>>        headers = [(key, redent(value)) for key, value in vars(self)['_headers']]<<NEWL>>        if self._payload:<<NEWL>>            headers.append(('Description', self.get_payload()))<<NEWL>>        return headers<<NEWL>><<NEWL>>    @property<<NEWL>>    def json(self):<<NEWL>>        """"""<<NEWL>>        Convert PackageMetadata to a JSON-compatible format<<NEWL>>        per PEP 0566.<<NEWL>>        """"""<<NEWL>><<NEWL>>        def transform(key):<<NEWL>>            value = self.get_all(key) if key in self.multiple_use_keys else self[key]<<NEWL>>            if key == 'Keywords':<<NEWL>>                value = re.split(r'\s+', value)<<NEWL>>            tk = key.lower().replace('-', '_')<<NEWL>>            return tk, value<<NEWL>><<NEWL>>        return dict(map(transform, map(FoldedCase, self)))"
349	jackson	3	"import typing as t<<NEWL>>from threading import local<<NEWL>><<NEWL>>if t.TYPE_CHECKING:<<NEWL>>    import typing_extensions as te<<NEWL>>    from .core import Context<<NEWL>><<NEWL>>_local = local()<<NEWL>><<NEWL>><<NEWL>>@t.overload<<NEWL>>def get_current_context(silent: ""te.Literal[False]"" = False) -> ""Context"":<<NEWL>>    ...<<NEWL>><<NEWL>><<NEWL>>@t.overload<<NEWL>>def get_current_context(silent: bool = ...) -> t.Optional[""Context""]:<<NEWL>>    ...<<NEWL>><<NEWL>><<NEWL>>def get_current_context(silent: bool = False) -> t.Optional[""Context""]:<<NEWL>>    """"""Returns the current click context.  This can be used as a way to<<NEWL>>    access the current context object from anywhere.  This is a more implicit<<NEWL>>    alternative to the :func:`pass_context` decorator.  This function is<<NEWL>>    primarily useful for helpers such as :func:`echo` which might be<<NEWL>>    interested in changing its behavior based on the current context.<<NEWL>><<NEWL>>    To push the current context, :meth:`Context.scope` can be used.<<NEWL>><<NEWL>>    .. versionadded:: 5.0<<NEWL>><<NEWL>>    :param silent: if set to `True` the return value is `None` if no context<<NEWL>>                   is available.  The default behavior is to raise a<<NEWL>>                   :exc:`RuntimeError`.<<NEWL>>    """"""<<NEWL>>    try:<<NEWL>>        return t.cast(""Context"", _local.stack[-1])<<NEWL>>    except (AttributeError, IndexError) as e:<<NEWL>>        if not silent:<<NEWL>>            raise RuntimeError(""There is no active click context."") from e<<NEWL>><<NEWL>>    return None<<NEWL>><<NEWL>><<NEWL>>def push_context(ctx: ""Context"") -> None:<<NEWL>>    """"""Pushes a new context to the current stack.""""""<<NEWL>>    _local.__dict__.setdefault(""stack"", []).append(ctx)<<NEWL>><<NEWL>><<NEWL>>def pop_context() -> None:<<NEWL>>    """"""Removes the top level from the stack.""""""<<NEWL>>    _local.stack.pop()<<NEWL>><<NEWL>><<NEWL>>def resolve_color_default(color: t.Optional[bool] = None) -> t.Optional[bool]:<<NEWL>>    """"""Internal helper to get the default value of the color flag.  If a<<NEWL>>    value is passed it's returned unchanged, otherwise it's looked up from<<NEWL>>    the current context.<<NEWL>>    """"""<<NEWL>>    if color is not None:<<NEWL>>        return color<<NEWL>><<NEWL>>    ctx = get_current_context(silent=True)<<NEWL>><<NEWL>>    if ctx is not None:<<NEWL>>        return ctx.color<<NEWL>><<NEWL>>    return None"
349	donghui	4	"import typing as t<<NEWL>>from threading import local<<NEWL>><<NEWL>>if t.TYPE_CHECKING:<<NEWL>>    import typing_extensions as te<<NEWL>>    from .core import Context<<NEWL>><<NEWL>>_local = local()<<NEWL>><<NEWL>><<NEWL>>@t.overload<<NEWL>>def get_current_context(silent: ""te.Literal[False]"" = False) -> ""Context"":<<NEWL>>    ...<<NEWL>><<NEWL>><<NEWL>>@t.overload<<NEWL>>def get_current_context(silent: bool = ...) -> t.Optional[""Context""]:<<NEWL>>    ...<<NEWL>><<NEWL>><<NEWL>>def get_current_context(silent: bool = False) -> t.Optional[""Context""]:<<NEWL>>    """"""Returns the current click context.  This can be used as a way to<<NEWL>>    access the current context object from anywhere.  This is a more implicit<<NEWL>>    alternative to the :func:`pass_context` decorator.  This function is<<NEWL>>    primarily useful for helpers such as :func:`echo` which might be<<NEWL>>    interested in changing its behavior based on the current context.<<NEWL>><<NEWL>>    To push the current context, :meth:`Context.scope` can be used.<<NEWL>><<NEWL>>    .. versionadded:: 5.0<<NEWL>><<NEWL>>    :param silent: if set to `True` the return value is `None` if no context<<NEWL>>                   is available.  The default behavior is to raise a<<NEWL>>                   :exc:`RuntimeError`.<<NEWL>>    """"""<<NEWL>>    try:<<NEWL>>        return t.cast(""Context"", _local.stack[-1])<<NEWL>>    except (AttributeError, IndexError) as e:<<NEWL>>        if not silent:<<NEWL>>            raise RuntimeError(""There is no active click context."") from e<<NEWL>><<NEWL>>    return None<<NEWL>><<NEWL>><<NEWL>>def push_context(ctx: ""Context"") -> None:<<NEWL>>    """"""Pushes a new context to the current stack.""""""<<NEWL>>    _local.__dict__.setdefault(""stack"", []).append(ctx)<<NEWL>><<NEWL>><<NEWL>>def pop_context() -> None:<<NEWL>>    """"""Removes the top level from the stack.""""""<<NEWL>>    _local.stack.pop()<<NEWL>><<NEWL>><<NEWL>>def resolve_color_default(color: t.Optional[bool] = None) -> t.Optional[bool]:<<NEWL>>    """"""Internal helper to get the default value of the color flag.  If a<<NEWL>>    value is passed it's returned unchanged, otherwise it's looked up from<<NEWL>>    the current context.<<NEWL>>    """"""<<NEWL>>    if color is not None:<<NEWL>>        return color<<NEWL>><<NEWL>>    ctx = get_current_context(silent=True)<<NEWL>><<NEWL>>    if ctx is not None:<<NEWL>>        return ctx.color<<NEWL>><<NEWL>>    return None"
456	jackson	0	import json<<NEWL>><<NEWL>>from .oauth import OAuth2Test<<NEWL>><<NEWL>><<NEWL>>class GiteaOAuth2Test(OAuth2Test):<<NEWL>>    backend_path = 'social_core.backends.gitea.GiteaOAuth2'<<NEWL>>    user_data_url = 'https://gitea.com/api/v1/user'<<NEWL>>    expected_username = 'foobar'<<NEWL>>    access_token_body = json.dumps({<<NEWL>>        'access_token': 'foobar',<<NEWL>>        'token_type': 'bearer',<<NEWL>>        'expires_in': 7200,<<NEWL>>        'refresh_token': 'barfoo'<<NEWL>>    })<<NEWL>>    user_data_body = json.dumps({<<NEWL>>        'id': 123456,<<NEWL>>        'login': 'foobar',<<NEWL>>        'full_name': 'Foo Bar',<<NEWL>>        'email': 'foobar@example.com',<<NEWL>>        'avatar_url': 'https://gitea.com/user/avatar/foobar/-1',<<NEWL>>        'language': 'en-US',<<NEWL>>        'is_admin': False,<<NEWL>>        'last_login': '2016-12-28T12:26:19+01:00',<<NEWL>>        'created': '2016-12-28T12:26:19+01:00',<<NEWL>>        'restricted': False,<<NEWL>>        'username': 'foobar'<<NEWL>>    })<<NEWL>><<NEWL>>    def test_login(self):<<NEWL>>        self.do_login()<<NEWL>><<NEWL>>    def test_partial_pipeline(self):<<NEWL>>        self.do_partial_pipeline()<<NEWL>><<NEWL>><<NEWL>>class GiteaCustomDomainOAuth2Test(OAuth2Test):<<NEWL>>    backend_path = 'social_core.backends.gitea.GiteaOAuth2'<<NEWL>>    user_data_url = 'https://example.com/api/v1/user'<<NEWL>>    expected_username = 'foobar'<<NEWL>>    access_token_body = json.dumps({<<NEWL>>        'access_token': 'foobar',<<NEWL>>        'token_type': 'bearer',<<NEWL>>        'expires_in': 7200,<<NEWL>>        'refresh_token': 'barfoo'<<NEWL>>    })<<NEWL>>    user_data_body = json.dumps({<<NEWL>>        'id': 123456,<<NEWL>>        'login': 'foobar',<<NEWL>>        'full_name': 'Foo Bar',<<NEWL>>        'email': 'foobar@example.com',<<NEWL>>        'avatar_url': 'https://example.com/user/avatar/foobar/-1',<<NEWL>>        'language': 'en-US',<<NEWL>>        'is_admin': False,<<NEWL>>        'last_login': '2016-12-28T12:26:19+01:00',<<NEWL>>        'created': '2016-12-28T12:26:19+01:00',<<NEWL>>        'restricted': False,<<NEWL>>        'username': 'foobar'<<NEWL>>    })<<NEWL>><<NEWL>>    def test_login(self):<<NEWL>>        self.strategy.set_settings({<<NEWL>>            'SOCIAL_AUTH_GITEA_API_URL': 'https://example.com'<<NEWL>>        })<<NEWL>>        self.do_login()<<NEWL>><<NEWL>>    def test_partial_pipeline(self):<<NEWL>>        self.strategy.set_settings({<<NEWL>>            'SOCIAL_AUTH_GITEA_API_URL': 'https://example.com'<<NEWL>>        })<<NEWL>>        self.do_partial_pipeline()
456	donghui	0	import json<<NEWL>><<NEWL>>from .oauth import OAuth2Test<<NEWL>><<NEWL>><<NEWL>>class GiteaOAuth2Test(OAuth2Test):<<NEWL>>    backend_path = 'social_core.backends.gitea.GiteaOAuth2'<<NEWL>>    user_data_url = 'https://gitea.com/api/v1/user'<<NEWL>>    expected_username = 'foobar'<<NEWL>>    access_token_body = json.dumps({<<NEWL>>        'access_token': 'foobar',<<NEWL>>        'token_type': 'bearer',<<NEWL>>        'expires_in': 7200,<<NEWL>>        'refresh_token': 'barfoo'<<NEWL>>    })<<NEWL>>    user_data_body = json.dumps({<<NEWL>>        'id': 123456,<<NEWL>>        'login': 'foobar',<<NEWL>>        'full_name': 'Foo Bar',<<NEWL>>        'email': 'foobar@example.com',<<NEWL>>        'avatar_url': 'https://gitea.com/user/avatar/foobar/-1',<<NEWL>>        'language': 'en-US',<<NEWL>>        'is_admin': False,<<NEWL>>        'last_login': '2016-12-28T12:26:19+01:00',<<NEWL>>        'created': '2016-12-28T12:26:19+01:00',<<NEWL>>        'restricted': False,<<NEWL>>        'username': 'foobar'<<NEWL>>    })<<NEWL>><<NEWL>>    def test_login(self):<<NEWL>>        self.do_login()<<NEWL>><<NEWL>>    def test_partial_pipeline(self):<<NEWL>>        self.do_partial_pipeline()<<NEWL>><<NEWL>><<NEWL>>class GiteaCustomDomainOAuth2Test(OAuth2Test):<<NEWL>>    backend_path = 'social_core.backends.gitea.GiteaOAuth2'<<NEWL>>    user_data_url = 'https://example.com/api/v1/user'<<NEWL>>    expected_username = 'foobar'<<NEWL>>    access_token_body = json.dumps({<<NEWL>>        'access_token': 'foobar',<<NEWL>>        'token_type': 'bearer',<<NEWL>>        'expires_in': 7200,<<NEWL>>        'refresh_token': 'barfoo'<<NEWL>>    })<<NEWL>>    user_data_body = json.dumps({<<NEWL>>        'id': 123456,<<NEWL>>        'login': 'foobar',<<NEWL>>        'full_name': 'Foo Bar',<<NEWL>>        'email': 'foobar@example.com',<<NEWL>>        'avatar_url': 'https://example.com/user/avatar/foobar/-1',<<NEWL>>        'language': 'en-US',<<NEWL>>        'is_admin': False,<<NEWL>>        'last_login': '2016-12-28T12:26:19+01:00',<<NEWL>>        'created': '2016-12-28T12:26:19+01:00',<<NEWL>>        'restricted': False,<<NEWL>>        'username': 'foobar'<<NEWL>>    })<<NEWL>><<NEWL>>    def test_login(self):<<NEWL>>        self.strategy.set_settings({<<NEWL>>            'SOCIAL_AUTH_GITEA_API_URL': 'https://example.com'<<NEWL>>        })<<NEWL>>        self.do_login()<<NEWL>><<NEWL>>    def test_partial_pipeline(self):<<NEWL>>        self.strategy.set_settings({<<NEWL>>            'SOCIAL_AUTH_GITEA_API_URL': 'https://example.com'<<NEWL>>        })<<NEWL>>        self.do_partial_pipeline()
462	jackson	2	"""""""<<NEWL>>Dummy database backend for Django.<<NEWL>><<NEWL>>Django uses this if the database ENGINE setting is empty (None or empty string).<<NEWL>><<NEWL>>Each of these API functions, except connection.close(), raise<<NEWL>>ImproperlyConfigured.<<NEWL>>""""""<<NEWL>><<NEWL>>from django.core.exceptions import ImproperlyConfigured<<NEWL>>from django.db.backends.base.base import BaseDatabaseWrapper<<NEWL>>from django.db.backends.base.client import BaseDatabaseClient<<NEWL>>from django.db.backends.base.creation import BaseDatabaseCreation<<NEWL>>from django.db.backends.base.introspection import BaseDatabaseIntrospection<<NEWL>>from django.db.backends.base.operations import BaseDatabaseOperations<<NEWL>>from django.db.backends.dummy.features import DummyDatabaseFeatures<<NEWL>><<NEWL>><<NEWL>>def complain(*args, **kwargs):<<NEWL>>    raise ImproperlyConfigured(<<NEWL>>        ""settings.DATABASES is improperly configured. ""<<NEWL>>        ""Please supply the ENGINE value. Check ""<<NEWL>>        ""settings documentation for more details.""<<NEWL>>    )<<NEWL>><<NEWL>><<NEWL>>def ignore(*args, **kwargs):<<NEWL>>    pass<<NEWL>><<NEWL>><<NEWL>>class DatabaseOperations(BaseDatabaseOperations):<<NEWL>>    quote_name = complain<<NEWL>><<NEWL>><<NEWL>>class DatabaseClient(BaseDatabaseClient):<<NEWL>>    runshell = complain<<NEWL>><<NEWL>><<NEWL>>class DatabaseCreation(BaseDatabaseCreation):<<NEWL>>    create_test_db = ignore<<NEWL>>    destroy_test_db = ignore<<NEWL>><<NEWL>><<NEWL>>class DatabaseIntrospection(BaseDatabaseIntrospection):<<NEWL>>    get_table_list = complain<<NEWL>>    get_table_description = complain<<NEWL>>    get_relations = complain<<NEWL>>    get_indexes = complain<<NEWL>><<NEWL>><<NEWL>>class DatabaseWrapper(BaseDatabaseWrapper):<<NEWL>>    operators = {}<<NEWL>>    # Override the base class implementations with null<<NEWL>>    # implementations. Anything that tries to actually<<NEWL>>    # do something raises complain; anything that tries<<NEWL>>    # to rollback or undo something raises ignore.<<NEWL>>    _cursor = complain<<NEWL>>    ensure_connection = complain<<NEWL>>    _commit = complain<<NEWL>>    _rollback = ignore<<NEWL>>    _close = ignore<<NEWL>>    _savepoint = ignore<<NEWL>>    _savepoint_commit = complain<<NEWL>>    _savepoint_rollback = ignore<<NEWL>>    _set_autocommit = complain<<NEWL>>    # Classes instantiated in __init__().<<NEWL>>    client_class = DatabaseClient<<NEWL>>    creation_class = DatabaseCreation<<NEWL>>    features_class = DummyDatabaseFeatures<<NEWL>>    introspection_class = DatabaseIntrospection<<NEWL>>    ops_class = DatabaseOperations<<NEWL>><<NEWL>>    def is_usable(self):<<NEWL>>        return True"
462	donghui	2	"""""""<<NEWL>>Dummy database backend for Django.<<NEWL>><<NEWL>>Django uses this if the database ENGINE setting is empty (None or empty string).<<NEWL>><<NEWL>>Each of these API functions, except connection.close(), raise<<NEWL>>ImproperlyConfigured.<<NEWL>>""""""<<NEWL>><<NEWL>>from django.core.exceptions import ImproperlyConfigured<<NEWL>>from django.db.backends.base.base import BaseDatabaseWrapper<<NEWL>>from django.db.backends.base.client import BaseDatabaseClient<<NEWL>>from django.db.backends.base.creation import BaseDatabaseCreation<<NEWL>>from django.db.backends.base.introspection import BaseDatabaseIntrospection<<NEWL>>from django.db.backends.base.operations import BaseDatabaseOperations<<NEWL>>from django.db.backends.dummy.features import DummyDatabaseFeatures<<NEWL>><<NEWL>><<NEWL>>def complain(*args, **kwargs):<<NEWL>>    raise ImproperlyConfigured(<<NEWL>>        ""settings.DATABASES is improperly configured. ""<<NEWL>>        ""Please supply the ENGINE value. Check ""<<NEWL>>        ""settings documentation for more details.""<<NEWL>>    )<<NEWL>><<NEWL>><<NEWL>>def ignore(*args, **kwargs):<<NEWL>>    pass<<NEWL>><<NEWL>><<NEWL>>class DatabaseOperations(BaseDatabaseOperations):<<NEWL>>    quote_name = complain<<NEWL>><<NEWL>><<NEWL>>class DatabaseClient(BaseDatabaseClient):<<NEWL>>    runshell = complain<<NEWL>><<NEWL>><<NEWL>>class DatabaseCreation(BaseDatabaseCreation):<<NEWL>>    create_test_db = ignore<<NEWL>>    destroy_test_db = ignore<<NEWL>><<NEWL>><<NEWL>>class DatabaseIntrospection(BaseDatabaseIntrospection):<<NEWL>>    get_table_list = complain<<NEWL>>    get_table_description = complain<<NEWL>>    get_relations = complain<<NEWL>>    get_indexes = complain<<NEWL>><<NEWL>><<NEWL>>class DatabaseWrapper(BaseDatabaseWrapper):<<NEWL>>    operators = {}<<NEWL>>    # Override the base class implementations with null<<NEWL>>    # implementations. Anything that tries to actually<<NEWL>>    # do something raises complain; anything that tries<<NEWL>>    # to rollback or undo something raises ignore.<<NEWL>>    _cursor = complain<<NEWL>>    ensure_connection = complain<<NEWL>>    _commit = complain<<NEWL>>    _rollback = ignore<<NEWL>>    _close = ignore<<NEWL>>    _savepoint = ignore<<NEWL>>    _savepoint_commit = complain<<NEWL>>    _savepoint_rollback = ignore<<NEWL>>    _set_autocommit = complain<<NEWL>>    # Classes instantiated in __init__().<<NEWL>>    client_class = DatabaseClient<<NEWL>>    creation_class = DatabaseCreation<<NEWL>>    features_class = DummyDatabaseFeatures<<NEWL>>    introspection_class = DatabaseIntrospection<<NEWL>>    ops_class = DatabaseOperations<<NEWL>><<NEWL>>    def is_usable(self):<<NEWL>>        return True"
433	jackson	2	"# Copyright 2016 Julien Danjou<<NEWL>># Copyright 2016 Joshua Harlow<<NEWL>># Copyright 2013-2014 Ray Holder<<NEWL>>#<<NEWL>># Licensed under the Apache License, Version 2.0 (the ""License"");<<NEWL>># you may not use this file except in compliance with the License.<<NEWL>># You may obtain a copy of the License at<<NEWL>>#<<NEWL>># http://www.apache.org/licenses/LICENSE-2.0<<NEWL>>#<<NEWL>># Unless required by applicable law or agreed to in writing, software<<NEWL>># distributed under the License is distributed on an ""AS IS"" BASIS,<<NEWL>># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.<<NEWL>># See the License for the specific language governing permissions and<<NEWL>># limitations under the License.<<NEWL>><<NEWL>>import sys<<NEWL>>import typing<<NEWL>><<NEWL>><<NEWL>># sys.maxsize:<<NEWL>># An integer giving the maximum value a variable of type Py_ssize_t can take.<<NEWL>>MAX_WAIT = sys.maxsize / 2<<NEWL>><<NEWL>><<NEWL>>def find_ordinal(pos_num: int) -> str:<<NEWL>>    # See: https://en.wikipedia.org/wiki/English_numerals#Ordinal_numbers<<NEWL>>    if pos_num == 0:<<NEWL>>        return ""th""<<NEWL>>    elif pos_num == 1:<<NEWL>>        return ""st""<<NEWL>>    elif pos_num == 2:<<NEWL>>        return ""nd""<<NEWL>>    elif pos_num == 3:<<NEWL>>        return ""rd""<<NEWL>>    elif 4 <= pos_num <= 20:<<NEWL>>        return ""th""<<NEWL>>    else:<<NEWL>>        return find_ordinal(pos_num % 10)<<NEWL>><<NEWL>><<NEWL>>def to_ordinal(pos_num: int) -> str:<<NEWL>>    return f""{pos_num}{find_ordinal(pos_num)}""<<NEWL>><<NEWL>><<NEWL>>def get_callback_name(cb: typing.Callable[..., typing.Any]) -> str:<<NEWL>>    """"""Get a callback fully-qualified name.<<NEWL>><<NEWL>>    If no name can be produced ``repr(cb)`` is called and returned.<<NEWL>>    """"""<<NEWL>>    segments = []<<NEWL>>    try:<<NEWL>>        segments.append(cb.__qualname__)<<NEWL>>    except AttributeError:<<NEWL>>        try:<<NEWL>>            segments.append(cb.__name__)<<NEWL>>        except AttributeError:<<NEWL>>            pass<<NEWL>>    if not segments:<<NEWL>>        return repr(cb)<<NEWL>>    else:<<NEWL>>        try:<<NEWL>>            # When running under sphinx it appears this can be none?<<NEWL>>            if cb.__module__:<<NEWL>>                segments.insert(0, cb.__module__)<<NEWL>>        except AttributeError:<<NEWL>>            pass<<NEWL>>        return ""."".join(segments)"
433	donghui	2	"# Copyright 2016 Julien Danjou<<NEWL>># Copyright 2016 Joshua Harlow<<NEWL>># Copyright 2013-2014 Ray Holder<<NEWL>>#<<NEWL>># Licensed under the Apache License, Version 2.0 (the ""License"");<<NEWL>># you may not use this file except in compliance with the License.<<NEWL>># You may obtain a copy of the License at<<NEWL>>#<<NEWL>># http://www.apache.org/licenses/LICENSE-2.0<<NEWL>>#<<NEWL>># Unless required by applicable law or agreed to in writing, software<<NEWL>># distributed under the License is distributed on an ""AS IS"" BASIS,<<NEWL>># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.<<NEWL>># See the License for the specific language governing permissions and<<NEWL>># limitations under the License.<<NEWL>><<NEWL>>import sys<<NEWL>>import typing<<NEWL>><<NEWL>><<NEWL>># sys.maxsize:<<NEWL>># An integer giving the maximum value a variable of type Py_ssize_t can take.<<NEWL>>MAX_WAIT = sys.maxsize / 2<<NEWL>><<NEWL>><<NEWL>>def find_ordinal(pos_num: int) -> str:<<NEWL>>    # See: https://en.wikipedia.org/wiki/English_numerals#Ordinal_numbers<<NEWL>>    if pos_num == 0:<<NEWL>>        return ""th""<<NEWL>>    elif pos_num == 1:<<NEWL>>        return ""st""<<NEWL>>    elif pos_num == 2:<<NEWL>>        return ""nd""<<NEWL>>    elif pos_num == 3:<<NEWL>>        return ""rd""<<NEWL>>    elif 4 <= pos_num <= 20:<<NEWL>>        return ""th""<<NEWL>>    else:<<NEWL>>        return find_ordinal(pos_num % 10)<<NEWL>><<NEWL>><<NEWL>>def to_ordinal(pos_num: int) -> str:<<NEWL>>    return f""{pos_num}{find_ordinal(pos_num)}""<<NEWL>><<NEWL>><<NEWL>>def get_callback_name(cb: typing.Callable[..., typing.Any]) -> str:<<NEWL>>    """"""Get a callback fully-qualified name.<<NEWL>><<NEWL>>    If no name can be produced ``repr(cb)`` is called and returned.<<NEWL>>    """"""<<NEWL>>    segments = []<<NEWL>>    try:<<NEWL>>        segments.append(cb.__qualname__)<<NEWL>>    except AttributeError:<<NEWL>>        try:<<NEWL>>            segments.append(cb.__name__)<<NEWL>>        except AttributeError:<<NEWL>>            pass<<NEWL>>    if not segments:<<NEWL>>        return repr(cb)<<NEWL>>    else:<<NEWL>>        try:<<NEWL>>            # When running under sphinx it appears this can be none?<<NEWL>>            if cb.__module__:<<NEWL>>                segments.insert(0, cb.__module__)<<NEWL>>        except AttributeError:<<NEWL>>            pass<<NEWL>>        return ""."".join(segments)"
491	jackson	3	"# Copyright 2020 Google LLC<<NEWL>>#<<NEWL>># Licensed under the Apache License, Version 2.0 (the ""License"");<<NEWL>># you may not use this file except in compliance with the License.<<NEWL>># You may obtain a copy of the License at<<NEWL>>#<<NEWL>>#    http://www.apache.org/licenses/LICENSE-2.0<<NEWL>>#<<NEWL>># Unless required by applicable law or agreed to in writing, software<<NEWL>># distributed under the License is distributed on an ""AS IS"" BASIS,<<NEWL>># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.<<NEWL>># See the License for the specific language governing permissions and<<NEWL>># limitations under the License.<<NEWL>><<NEWL>><<NEWL>># [START automl_video_classification_list_datasets_beta]<<NEWL>># [START automl_video_object_tracking_list_datasets_beta]<<NEWL>>from google.cloud import automl_v1beta1 as automl<<NEWL>><<NEWL>><<NEWL>>def list_datasets(project_id=""YOUR_PROJECT_ID""):<<NEWL>>    """"""List datasets.""""""<<NEWL>>    client = automl.AutoMlClient()<<NEWL>>    # A resource that represents Google Cloud Platform location.<<NEWL>>    project_location = f""projects/{project_id}/locations/us-central1""<<NEWL>><<NEWL>>    # List all the datasets available in the region.<<NEWL>>    request = automl.ListDatasetsRequest(parent=project_location, filter="""")<<NEWL>>    response = client.list_datasets(request=request)<<NEWL>><<NEWL>>    print(""List of datasets:"")<<NEWL>>    for dataset in response:<<NEWL>>        print(""Dataset name: {}"".format(dataset.name))<<NEWL>>        print(""Dataset id: {}"".format(dataset.name.split(""/"")[-1]))<<NEWL>>        print(""Dataset display name: {}"".format(dataset.display_name))<<NEWL>>        print(""Dataset create time: {}"".format(dataset.create_time))<<NEWL>>        # [END automl_video_object_tracking_list_datasets_beta]<<NEWL>><<NEWL>>        print(<<NEWL>>            ""Video classification dataset metadata: {}"".format(<<NEWL>>                dataset.video_classification_dataset_metadata<<NEWL>>            )<<NEWL>>        )<<NEWL>>        # [END automl_video_classification_list_datasets_beta]<<NEWL>><<NEWL>>        # [START automl_video_object_tracking_list_datasets_beta]<<NEWL>>        print(<<NEWL>>            ""Video object tracking dataset metadata: {}"".format(<<NEWL>>                dataset.video_object_tracking_dataset_metadata<<NEWL>>            )<<NEWL>>        )<<NEWL>>        # [END automl_video_object_tracking_list_datasets_beta]"
491	donghui	2	"# Copyright 2020 Google LLC<<NEWL>>#<<NEWL>># Licensed under the Apache License, Version 2.0 (the ""License"");<<NEWL>># you may not use this file except in compliance with the License.<<NEWL>># You may obtain a copy of the License at<<NEWL>>#<<NEWL>>#    http://www.apache.org/licenses/LICENSE-2.0<<NEWL>>#<<NEWL>># Unless required by applicable law or agreed to in writing, software<<NEWL>># distributed under the License is distributed on an ""AS IS"" BASIS,<<NEWL>># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.<<NEWL>># See the License for the specific language governing permissions and<<NEWL>># limitations under the License.<<NEWL>><<NEWL>><<NEWL>># [START automl_video_classification_list_datasets_beta]<<NEWL>># [START automl_video_object_tracking_list_datasets_beta]<<NEWL>>from google.cloud import automl_v1beta1 as automl<<NEWL>><<NEWL>><<NEWL>>def list_datasets(project_id=""YOUR_PROJECT_ID""):<<NEWL>>    """"""List datasets.""""""<<NEWL>>    client = automl.AutoMlClient()<<NEWL>>    # A resource that represents Google Cloud Platform location.<<NEWL>>    project_location = f""projects/{project_id}/locations/us-central1""<<NEWL>><<NEWL>>    # List all the datasets available in the region.<<NEWL>>    request = automl.ListDatasetsRequest(parent=project_location, filter="""")<<NEWL>>    response = client.list_datasets(request=request)<<NEWL>><<NEWL>>    print(""List of datasets:"")<<NEWL>>    for dataset in response:<<NEWL>>        print(""Dataset name: {}"".format(dataset.name))<<NEWL>>        print(""Dataset id: {}"".format(dataset.name.split(""/"")[-1]))<<NEWL>>        print(""Dataset display name: {}"".format(dataset.display_name))<<NEWL>>        print(""Dataset create time: {}"".format(dataset.create_time))<<NEWL>>        # [END automl_video_object_tracking_list_datasets_beta]<<NEWL>><<NEWL>>        print(<<NEWL>>            ""Video classification dataset metadata: {}"".format(<<NEWL>>                dataset.video_classification_dataset_metadata<<NEWL>>            )<<NEWL>>        )<<NEWL>>        # [END automl_video_classification_list_datasets_beta]<<NEWL>><<NEWL>>        # [START automl_video_object_tracking_list_datasets_beta]<<NEWL>>        print(<<NEWL>>            ""Video object tracking dataset metadata: {}"".format(<<NEWL>>                dataset.video_object_tracking_dataset_metadata<<NEWL>>            )<<NEWL>>        )<<NEWL>>        # [END automl_video_object_tracking_list_datasets_beta]"
413	jackson	2	"# -*- coding: utf-8 -*-<<NEWL>>""""""<<NEWL>>set_fake_passwords.py<<NEWL>><<NEWL>>    Reset all user passwords to a common value. Useful for testing in a<<NEWL>>    development environment. As such, this command is only available when<<NEWL>>    setting.DEBUG is True.<<NEWL>><<NEWL>>""""""<<NEWL>>from typing import List<<NEWL>><<NEWL>>from django.conf import settings<<NEWL>>from django.contrib.auth import get_user_model<<NEWL>>from django.core.management.base import BaseCommand, CommandError<<NEWL>><<NEWL>>from django_extensions.management.utils import signalcommand<<NEWL>><<NEWL>>DEFAULT_FAKE_PASSWORD = 'password'<<NEWL>><<NEWL>><<NEWL>>class Command(BaseCommand):<<NEWL>>    help = 'DEBUG only: sets all user passwords to a common value (""%s"" by default)' % (DEFAULT_FAKE_PASSWORD, )<<NEWL>>    requires_system_checks: List[str] = []<<NEWL>><<NEWL>>    def add_arguments(self, parser):<<NEWL>>        super().add_arguments(parser)<<NEWL>>        parser.add_argument(<<NEWL>>            '--prompt', dest='prompt_passwd', default=False,<<NEWL>>            action='store_true',<<NEWL>>            help='Prompts for the new password to apply to all users'<<NEWL>>        )<<NEWL>>        parser.add_argument(<<NEWL>>            '--password', dest='default_passwd', default=DEFAULT_FAKE_PASSWORD,<<NEWL>>            help='Use this as default password.'<<NEWL>>        )<<NEWL>><<NEWL>>    @signalcommand<<NEWL>>    def handle(self, *args, **options):<<NEWL>>        if not settings.DEBUG:<<NEWL>>            raise CommandError('Only available in debug mode')<<NEWL>><<NEWL>>        if options['prompt_passwd']:<<NEWL>>            from getpass import getpass<<NEWL>>            passwd = getpass('Password: ')<<NEWL>>            if not passwd:<<NEWL>>                raise CommandError('You must enter a valid password')<<NEWL>>        else:<<NEWL>>            passwd = options['default_passwd']<<NEWL>><<NEWL>>        User = get_user_model()<<NEWL>>        user = User()<<NEWL>>        user.set_password(passwd)<<NEWL>>        count = User.objects.all().update(password=user.password)<<NEWL>><<NEWL>>        print('Reset %d passwords' % count)"
413	donghui	1	"# -*- coding: utf-8 -*-<<NEWL>>""""""<<NEWL>>set_fake_passwords.py<<NEWL>><<NEWL>>    Reset all user passwords to a common value. Useful for testing in a<<NEWL>>    development environment. As such, this command is only available when<<NEWL>>    setting.DEBUG is True.<<NEWL>><<NEWL>>""""""<<NEWL>>from typing import List<<NEWL>><<NEWL>>from django.conf import settings<<NEWL>>from django.contrib.auth import get_user_model<<NEWL>>from django.core.management.base import BaseCommand, CommandError<<NEWL>><<NEWL>>from django_extensions.management.utils import signalcommand<<NEWL>><<NEWL>>DEFAULT_FAKE_PASSWORD = 'password'<<NEWL>><<NEWL>><<NEWL>>class Command(BaseCommand):<<NEWL>>    help = 'DEBUG only: sets all user passwords to a common value (""%s"" by default)' % (DEFAULT_FAKE_PASSWORD, )<<NEWL>>    requires_system_checks: List[str] = []<<NEWL>><<NEWL>>    def add_arguments(self, parser):<<NEWL>>        super().add_arguments(parser)<<NEWL>>        parser.add_argument(<<NEWL>>            '--prompt', dest='prompt_passwd', default=False,<<NEWL>>            action='store_true',<<NEWL>>            help='Prompts for the new password to apply to all users'<<NEWL>>        )<<NEWL>>        parser.add_argument(<<NEWL>>            '--password', dest='default_passwd', default=DEFAULT_FAKE_PASSWORD,<<NEWL>>            help='Use this as default password.'<<NEWL>>        )<<NEWL>><<NEWL>>    @signalcommand<<NEWL>>    def handle(self, *args, **options):<<NEWL>>        if not settings.DEBUG:<<NEWL>>            raise CommandError('Only available in debug mode')<<NEWL>><<NEWL>>        if options['prompt_passwd']:<<NEWL>>            from getpass import getpass<<NEWL>>            passwd = getpass('Password: ')<<NEWL>>            if not passwd:<<NEWL>>                raise CommandError('You must enter a valid password')<<NEWL>>        else:<<NEWL>>            passwd = options['default_passwd']<<NEWL>><<NEWL>>        User = get_user_model()<<NEWL>>        user = User()<<NEWL>>        user.set_password(passwd)<<NEWL>>        count = User.objects.all().update(password=user.password)<<NEWL>><<NEWL>>        print('Reset %d passwords' % count)"
442	jackson	0	"import pytest<<NEWL>><<NEWL>>from pandas.util._decorators import deprecate_kwarg<<NEWL>><<NEWL>>import pandas._testing as tm<<NEWL>><<NEWL>><<NEWL>>@deprecate_kwarg(""old"", ""new"")<<NEWL>>def _f1(new=False):<<NEWL>>    return new<<NEWL>><<NEWL>><<NEWL>>_f2_mappings = {""yes"": True, ""no"": False}<<NEWL>><<NEWL>><<NEWL>>@deprecate_kwarg(""old"", ""new"", _f2_mappings)<<NEWL>>def _f2(new=False):<<NEWL>>    return new<<NEWL>><<NEWL>><<NEWL>>def _f3_mapping(x):<<NEWL>>    return x + 1<<NEWL>><<NEWL>><<NEWL>>@deprecate_kwarg(""old"", ""new"", _f3_mapping)<<NEWL>>def _f3(new=0):<<NEWL>>    return new<<NEWL>><<NEWL>><<NEWL>>@pytest.mark.parametrize(""key,klass"", [(""old"", FutureWarning), (""new"", None)])<<NEWL>>def test_deprecate_kwarg(key, klass):<<NEWL>>    x = 78<<NEWL>><<NEWL>>    with tm.assert_produces_warning(klass):<<NEWL>>        assert _f1(**{key: x}) == x<<NEWL>><<NEWL>><<NEWL>>@pytest.mark.parametrize(""key"", list(_f2_mappings.keys()))<<NEWL>>def test_dict_deprecate_kwarg(key):<<NEWL>>    with tm.assert_produces_warning(FutureWarning):<<NEWL>>        assert _f2(old=key) == _f2_mappings[key]<<NEWL>><<NEWL>><<NEWL>>@pytest.mark.parametrize(""key"", [""bogus"", 12345, -1.23])<<NEWL>>def test_missing_deprecate_kwarg(key):<<NEWL>>    with tm.assert_produces_warning(FutureWarning):<<NEWL>>        assert _f2(old=key) == key<<NEWL>><<NEWL>><<NEWL>>@pytest.mark.parametrize(""x"", [1, -1.4, 0])<<NEWL>>def test_callable_deprecate_kwarg(x):<<NEWL>>    with tm.assert_produces_warning(FutureWarning):<<NEWL>>        assert _f3(old=x) == _f3_mapping(x)<<NEWL>><<NEWL>><<NEWL>>def test_callable_deprecate_kwarg_fail():<<NEWL>>    msg = ""((can only|cannot) concatenate)|(must be str)|(Can't convert)""<<NEWL>><<NEWL>>    with pytest.raises(TypeError, match=msg):<<NEWL>>        _f3(old=""hello"")<<NEWL>><<NEWL>><<NEWL>>def test_bad_deprecate_kwarg():<<NEWL>>    msg = ""mapping from old to new argument values must be dict or callable!""<<NEWL>><<NEWL>>    with pytest.raises(TypeError, match=msg):<<NEWL>><<NEWL>>        @deprecate_kwarg(""old"", ""new"", 0)<<NEWL>>        def f4(new=None):<<NEWL>>            return new<<NEWL>><<NEWL>><<NEWL>>@deprecate_kwarg(""old"", None)<<NEWL>>def _f4(old=True, unchanged=True):<<NEWL>>    return old, unchanged<<NEWL>><<NEWL>><<NEWL>>@pytest.mark.parametrize(""key"", [""old"", ""unchanged""])<<NEWL>>def test_deprecate_keyword(key):<<NEWL>>    x = 9<<NEWL>><<NEWL>>    if key == ""old"":<<NEWL>>        klass = FutureWarning<<NEWL>>        expected = (x, True)<<NEWL>>    else:<<NEWL>>        klass = None<<NEWL>>        expected = (True, x)<<NEWL>><<NEWL>>    with tm.assert_produces_warning(klass):<<NEWL>>        assert _f4(**{key: x}) == expected"
442	donghui	0	"import pytest<<NEWL>><<NEWL>>from pandas.util._decorators import deprecate_kwarg<<NEWL>><<NEWL>>import pandas._testing as tm<<NEWL>><<NEWL>><<NEWL>>@deprecate_kwarg(""old"", ""new"")<<NEWL>>def _f1(new=False):<<NEWL>>    return new<<NEWL>><<NEWL>><<NEWL>>_f2_mappings = {""yes"": True, ""no"": False}<<NEWL>><<NEWL>><<NEWL>>@deprecate_kwarg(""old"", ""new"", _f2_mappings)<<NEWL>>def _f2(new=False):<<NEWL>>    return new<<NEWL>><<NEWL>><<NEWL>>def _f3_mapping(x):<<NEWL>>    return x + 1<<NEWL>><<NEWL>><<NEWL>>@deprecate_kwarg(""old"", ""new"", _f3_mapping)<<NEWL>>def _f3(new=0):<<NEWL>>    return new<<NEWL>><<NEWL>><<NEWL>>@pytest.mark.parametrize(""key,klass"", [(""old"", FutureWarning), (""new"", None)])<<NEWL>>def test_deprecate_kwarg(key, klass):<<NEWL>>    x = 78<<NEWL>><<NEWL>>    with tm.assert_produces_warning(klass):<<NEWL>>        assert _f1(**{key: x}) == x<<NEWL>><<NEWL>><<NEWL>>@pytest.mark.parametrize(""key"", list(_f2_mappings.keys()))<<NEWL>>def test_dict_deprecate_kwarg(key):<<NEWL>>    with tm.assert_produces_warning(FutureWarning):<<NEWL>>        assert _f2(old=key) == _f2_mappings[key]<<NEWL>><<NEWL>><<NEWL>>@pytest.mark.parametrize(""key"", [""bogus"", 12345, -1.23])<<NEWL>>def test_missing_deprecate_kwarg(key):<<NEWL>>    with tm.assert_produces_warning(FutureWarning):<<NEWL>>        assert _f2(old=key) == key<<NEWL>><<NEWL>><<NEWL>>@pytest.mark.parametrize(""x"", [1, -1.4, 0])<<NEWL>>def test_callable_deprecate_kwarg(x):<<NEWL>>    with tm.assert_produces_warning(FutureWarning):<<NEWL>>        assert _f3(old=x) == _f3_mapping(x)<<NEWL>><<NEWL>><<NEWL>>def test_callable_deprecate_kwarg_fail():<<NEWL>>    msg = ""((can only|cannot) concatenate)|(must be str)|(Can't convert)""<<NEWL>><<NEWL>>    with pytest.raises(TypeError, match=msg):<<NEWL>>        _f3(old=""hello"")<<NEWL>><<NEWL>><<NEWL>>def test_bad_deprecate_kwarg():<<NEWL>>    msg = ""mapping from old to new argument values must be dict or callable!""<<NEWL>><<NEWL>>    with pytest.raises(TypeError, match=msg):<<NEWL>><<NEWL>>        @deprecate_kwarg(""old"", ""new"", 0)<<NEWL>>        def f4(new=None):<<NEWL>>            return new<<NEWL>><<NEWL>><<NEWL>>@deprecate_kwarg(""old"", None)<<NEWL>>def _f4(old=True, unchanged=True):<<NEWL>>    return old, unchanged<<NEWL>><<NEWL>><<NEWL>>@pytest.mark.parametrize(""key"", [""old"", ""unchanged""])<<NEWL>>def test_deprecate_keyword(key):<<NEWL>>    x = 9<<NEWL>><<NEWL>>    if key == ""old"":<<NEWL>>        klass = FutureWarning<<NEWL>>        expected = (x, True)<<NEWL>>    else:<<NEWL>>        klass = None<<NEWL>>        expected = (True, x)<<NEWL>><<NEWL>>    with tm.assert_produces_warning(klass):<<NEWL>>        assert _f4(**{key: x}) == expected"
485	jackson	4	"""""""<<NEWL>>NGP VAN's `ActionID` Provider<<NEWL>><<NEWL>>http://developers.ngpvan.com/action-id<<NEWL>>""""""<<NEWL>>from openid.extensions import ax<<NEWL>><<NEWL>>from .open_id import OpenIdAuth<<NEWL>><<NEWL>><<NEWL>>class ActionIDOpenID(OpenIdAuth):<<NEWL>>    """"""<<NEWL>>    NGP VAN's ActionID OpenID 1.1 authentication backend<<NEWL>>    """"""<<NEWL>>    name = 'actionid-openid'<<NEWL>>    URL = 'https://accounts.ngpvan.com/Home/Xrds'<<NEWL>>    USERNAME_KEY = 'email'<<NEWL>><<NEWL>>    def get_ax_attributes(self):<<NEWL>>        """"""<<NEWL>>        Return the AX attributes that ActionID responds with, as well as the<<NEWL>>        user data result that it must map to.<<NEWL>>        """"""<<NEWL>>        return [<<NEWL>>            ('http://openid.net/schema/contact/internet/email', 'email'),<<NEWL>>            ('http://openid.net/schema/contact/phone/business', 'phone'),<<NEWL>>            ('http://openid.net/schema/namePerson/first', 'first_name'),<<NEWL>>            ('http://openid.net/schema/namePerson/last', 'last_name'),<<NEWL>>            ('http://openid.net/schema/namePerson', 'fullname'),<<NEWL>>        ]<<NEWL>><<NEWL>>    def setup_request(self, params=None):<<NEWL>>        """"""<<NEWL>>        Setup the OpenID request<<NEWL>><<NEWL>>        Because ActionID does not advertise the availiability of AX attributes<<NEWL>>        nor use standard attribute aliases, we need to setup the attributes<<NEWL>>        manually instead of rely on the parent OpenIdAuth.setup_request()<<NEWL>>        """"""<<NEWL>>        request = self.openid_request(params)<<NEWL>><<NEWL>>        fetch_request = ax.FetchRequest()<<NEWL>>        fetch_request.add(ax.AttrInfo(<<NEWL>>            'http://openid.net/schema/contact/internet/email',<<NEWL>>            alias='ngpvanemail',<<NEWL>>            required=True<<NEWL>>        ))<<NEWL>><<NEWL>>        fetch_request.add(ax.AttrInfo(<<NEWL>>            'http://openid.net/schema/contact/phone/business',<<NEWL>>            alias='ngpvanphone',<<NEWL>>            required=False<<NEWL>>        ))<<NEWL>>        fetch_request.add(ax.AttrInfo(<<NEWL>>            'http://openid.net/schema/namePerson/first',<<NEWL>>            alias='ngpvanfirstname',<<NEWL>>            required=False<<NEWL>>        ))<<NEWL>>        fetch_request.add(ax.AttrInfo(<<NEWL>>            'http://openid.net/schema/namePerson/last',<<NEWL>>            alias='ngpvanlastname',<<NEWL>>            required=False<<NEWL>>        ))<<NEWL>>        request.addExtension(fetch_request)<<NEWL>><<NEWL>>        return request"
485	donghui	3	"""""""<<NEWL>>NGP VAN's `ActionID` Provider<<NEWL>><<NEWL>>http://developers.ngpvan.com/action-id<<NEWL>>""""""<<NEWL>>from openid.extensions import ax<<NEWL>><<NEWL>>from .open_id import OpenIdAuth<<NEWL>><<NEWL>><<NEWL>>class ActionIDOpenID(OpenIdAuth):<<NEWL>>    """"""<<NEWL>>    NGP VAN's ActionID OpenID 1.1 authentication backend<<NEWL>>    """"""<<NEWL>>    name = 'actionid-openid'<<NEWL>>    URL = 'https://accounts.ngpvan.com/Home/Xrds'<<NEWL>>    USERNAME_KEY = 'email'<<NEWL>><<NEWL>>    def get_ax_attributes(self):<<NEWL>>        """"""<<NEWL>>        Return the AX attributes that ActionID responds with, as well as the<<NEWL>>        user data result that it must map to.<<NEWL>>        """"""<<NEWL>>        return [<<NEWL>>            ('http://openid.net/schema/contact/internet/email', 'email'),<<NEWL>>            ('http://openid.net/schema/contact/phone/business', 'phone'),<<NEWL>>            ('http://openid.net/schema/namePerson/first', 'first_name'),<<NEWL>>            ('http://openid.net/schema/namePerson/last', 'last_name'),<<NEWL>>            ('http://openid.net/schema/namePerson', 'fullname'),<<NEWL>>        ]<<NEWL>><<NEWL>>    def setup_request(self, params=None):<<NEWL>>        """"""<<NEWL>>        Setup the OpenID request<<NEWL>><<NEWL>>        Because ActionID does not advertise the availiability of AX attributes<<NEWL>>        nor use standard attribute aliases, we need to setup the attributes<<NEWL>>        manually instead of rely on the parent OpenIdAuth.setup_request()<<NEWL>>        """"""<<NEWL>>        request = self.openid_request(params)<<NEWL>><<NEWL>>        fetch_request = ax.FetchRequest()<<NEWL>>        fetch_request.add(ax.AttrInfo(<<NEWL>>            'http://openid.net/schema/contact/internet/email',<<NEWL>>            alias='ngpvanemail',<<NEWL>>            required=True<<NEWL>>        ))<<NEWL>><<NEWL>>        fetch_request.add(ax.AttrInfo(<<NEWL>>            'http://openid.net/schema/contact/phone/business',<<NEWL>>            alias='ngpvanphone',<<NEWL>>            required=False<<NEWL>>        ))<<NEWL>>        fetch_request.add(ax.AttrInfo(<<NEWL>>            'http://openid.net/schema/namePerson/first',<<NEWL>>            alias='ngpvanfirstname',<<NEWL>>            required=False<<NEWL>>        ))<<NEWL>>        fetch_request.add(ax.AttrInfo(<<NEWL>>            'http://openid.net/schema/namePerson/last',<<NEWL>>            alias='ngpvanlastname',<<NEWL>>            required=False<<NEWL>>        ))<<NEWL>>        request.addExtension(fetch_request)<<NEWL>><<NEWL>>        return request"
476	jackson	1	"import pytest<<NEWL>><<NEWL>>from pandas.util._validators import validate_args<<NEWL>><<NEWL>>_fname = ""func""<<NEWL>><<NEWL>><<NEWL>>def test_bad_min_fname_arg_count():<<NEWL>>    msg = ""'max_fname_arg_count' must be non-negative""<<NEWL>><<NEWL>>    with pytest.raises(ValueError, match=msg):<<NEWL>>        validate_args(_fname, (None,), -1, ""foo"")<<NEWL>><<NEWL>><<NEWL>>def test_bad_arg_length_max_value_single():<<NEWL>>    args = (None, None)<<NEWL>>    compat_args = (""foo"",)<<NEWL>><<NEWL>>    min_fname_arg_count = 0<<NEWL>>    max_length = len(compat_args) + min_fname_arg_count<<NEWL>>    actual_length = len(args) + min_fname_arg_count<<NEWL>>    msg = (<<NEWL>>        rf""{_fname}\(\) takes at most {max_length} ""<<NEWL>>        rf""argument \({actual_length} given\)""<<NEWL>>    )<<NEWL>><<NEWL>>    with pytest.raises(TypeError, match=msg):<<NEWL>>        validate_args(_fname, args, min_fname_arg_count, compat_args)<<NEWL>><<NEWL>><<NEWL>>def test_bad_arg_length_max_value_multiple():<<NEWL>>    args = (None, None)<<NEWL>>    compat_args = {""foo"": None}<<NEWL>><<NEWL>>    min_fname_arg_count = 2<<NEWL>>    max_length = len(compat_args) + min_fname_arg_count<<NEWL>>    actual_length = len(args) + min_fname_arg_count<<NEWL>>    msg = (<<NEWL>>        rf""{_fname}\(\) takes at most {max_length} ""<<NEWL>>        rf""arguments \({actual_length} given\)""<<NEWL>>    )<<NEWL>><<NEWL>>    with pytest.raises(TypeError, match=msg):<<NEWL>>        validate_args(_fname, args, min_fname_arg_count, compat_args)<<NEWL>><<NEWL>><<NEWL>>@pytest.mark.parametrize(""i"", range(1, 3))<<NEWL>>def test_not_all_defaults(i):<<NEWL>>    bad_arg = ""foo""<<NEWL>>    msg = (<<NEWL>>        f""the '{bad_arg}' parameter is not supported ""<<NEWL>>        rf""in the pandas implementation of {_fname}\(\)""<<NEWL>>    )<<NEWL>><<NEWL>>    compat_args = {""foo"": 2, ""bar"": -1, ""baz"": 3}<<NEWL>>    arg_vals = (1, -1, 3)<<NEWL>><<NEWL>>    with pytest.raises(ValueError, match=msg):<<NEWL>>        validate_args(_fname, arg_vals[:i], 2, compat_args)<<NEWL>><<NEWL>><<NEWL>>def test_validation():<<NEWL>>    # No exceptions should be raised.<<NEWL>>    validate_args(_fname, (None,), 2, {""out"": None})<<NEWL>><<NEWL>>    compat_args = {""axis"": 1, ""out"": None}<<NEWL>>    validate_args(_fname, (1, None), 2, compat_args)"
476	donghui	1	"import pytest<<NEWL>><<NEWL>>from pandas.util._validators import validate_args<<NEWL>><<NEWL>>_fname = ""func""<<NEWL>><<NEWL>><<NEWL>>def test_bad_min_fname_arg_count():<<NEWL>>    msg = ""'max_fname_arg_count' must be non-negative""<<NEWL>><<NEWL>>    with pytest.raises(ValueError, match=msg):<<NEWL>>        validate_args(_fname, (None,), -1, ""foo"")<<NEWL>><<NEWL>><<NEWL>>def test_bad_arg_length_max_value_single():<<NEWL>>    args = (None, None)<<NEWL>>    compat_args = (""foo"",)<<NEWL>><<NEWL>>    min_fname_arg_count = 0<<NEWL>>    max_length = len(compat_args) + min_fname_arg_count<<NEWL>>    actual_length = len(args) + min_fname_arg_count<<NEWL>>    msg = (<<NEWL>>        rf""{_fname}\(\) takes at most {max_length} ""<<NEWL>>        rf""argument \({actual_length} given\)""<<NEWL>>    )<<NEWL>><<NEWL>>    with pytest.raises(TypeError, match=msg):<<NEWL>>        validate_args(_fname, args, min_fname_arg_count, compat_args)<<NEWL>><<NEWL>><<NEWL>>def test_bad_arg_length_max_value_multiple():<<NEWL>>    args = (None, None)<<NEWL>>    compat_args = {""foo"": None}<<NEWL>><<NEWL>>    min_fname_arg_count = 2<<NEWL>>    max_length = len(compat_args) + min_fname_arg_count<<NEWL>>    actual_length = len(args) + min_fname_arg_count<<NEWL>>    msg = (<<NEWL>>        rf""{_fname}\(\) takes at most {max_length} ""<<NEWL>>        rf""arguments \({actual_length} given\)""<<NEWL>>    )<<NEWL>><<NEWL>>    with pytest.raises(TypeError, match=msg):<<NEWL>>        validate_args(_fname, args, min_fname_arg_count, compat_args)<<NEWL>><<NEWL>><<NEWL>>@pytest.mark.parametrize(""i"", range(1, 3))<<NEWL>>def test_not_all_defaults(i):<<NEWL>>    bad_arg = ""foo""<<NEWL>>    msg = (<<NEWL>>        f""the '{bad_arg}' parameter is not supported ""<<NEWL>>        rf""in the pandas implementation of {_fname}\(\)""<<NEWL>>    )<<NEWL>><<NEWL>>    compat_args = {""foo"": 2, ""bar"": -1, ""baz"": 3}<<NEWL>>    arg_vals = (1, -1, 3)<<NEWL>><<NEWL>>    with pytest.raises(ValueError, match=msg):<<NEWL>>        validate_args(_fname, arg_vals[:i], 2, compat_args)<<NEWL>><<NEWL>><<NEWL>>def test_validation():<<NEWL>>    # No exceptions should be raised.<<NEWL>>    validate_args(_fname, (None,), 2, {""out"": None})<<NEWL>><<NEWL>>    compat_args = {""axis"": 1, ""out"": None}<<NEWL>>    validate_args(_fname, (1, None), 2, compat_args)"
427	jackson	3	"""""""<<NEWL>> The GeometryColumns and SpatialRefSys models for the PostGIS backend.<<NEWL>>""""""<<NEWL>>from django.contrib.gis.db.backends.base.models import SpatialRefSysMixin<<NEWL>>from django.db import models<<NEWL>><<NEWL>><<NEWL>>class PostGISGeometryColumns(models.Model):<<NEWL>>    """"""<<NEWL>>    The 'geometry_columns' view from PostGIS. See the PostGIS<<NEWL>>    documentation at Ch. 4.3.2.<<NEWL>>    """"""<<NEWL>><<NEWL>>    f_table_catalog = models.CharField(max_length=256)<<NEWL>>    f_table_schema = models.CharField(max_length=256)<<NEWL>>    f_table_name = models.CharField(max_length=256)<<NEWL>>    f_geometry_column = models.CharField(max_length=256)<<NEWL>>    coord_dimension = models.IntegerField()<<NEWL>>    srid = models.IntegerField(primary_key=True)<<NEWL>>    type = models.CharField(max_length=30)<<NEWL>><<NEWL>>    class Meta:<<NEWL>>        app_label = ""gis""<<NEWL>>        db_table = ""geometry_columns""<<NEWL>>        managed = False<<NEWL>><<NEWL>>    def __str__(self):<<NEWL>>        return ""%s.%s - %dD %s field (SRID: %d)"" % (<<NEWL>>            self.f_table_name,<<NEWL>>            self.f_geometry_column,<<NEWL>>            self.coord_dimension,<<NEWL>>            self.type,<<NEWL>>            self.srid,<<NEWL>>        )<<NEWL>><<NEWL>>    @classmethod<<NEWL>>    def table_name_col(cls):<<NEWL>>        """"""<<NEWL>>        Return the name of the metadata column used to store the feature table<<NEWL>>        name.<<NEWL>>        """"""<<NEWL>>        return ""f_table_name""<<NEWL>><<NEWL>>    @classmethod<<NEWL>>    def geom_col_name(cls):<<NEWL>>        """"""<<NEWL>>        Return the name of the metadata column used to store the feature<<NEWL>>        geometry column.<<NEWL>>        """"""<<NEWL>>        return ""f_geometry_column""<<NEWL>><<NEWL>><<NEWL>>class PostGISSpatialRefSys(models.Model, SpatialRefSysMixin):<<NEWL>>    """"""<<NEWL>>    The 'spatial_ref_sys' table from PostGIS. See the PostGIS<<NEWL>>    documentation at Ch. 4.2.1.<<NEWL>>    """"""<<NEWL>><<NEWL>>    srid = models.IntegerField(primary_key=True)<<NEWL>>    auth_name = models.CharField(max_length=256)<<NEWL>>    auth_srid = models.IntegerField()<<NEWL>>    srtext = models.CharField(max_length=2048)<<NEWL>>    proj4text = models.CharField(max_length=2048)<<NEWL>><<NEWL>>    class Meta:<<NEWL>>        app_label = ""gis""<<NEWL>>        db_table = ""spatial_ref_sys""<<NEWL>>        managed = False<<NEWL>><<NEWL>>    @property<<NEWL>>    def wkt(self):<<NEWL>>        return self.srtext"
427	donghui	2	"""""""<<NEWL>> The GeometryColumns and SpatialRefSys models for the PostGIS backend.<<NEWL>>""""""<<NEWL>>from django.contrib.gis.db.backends.base.models import SpatialRefSysMixin<<NEWL>>from django.db import models<<NEWL>><<NEWL>><<NEWL>>class PostGISGeometryColumns(models.Model):<<NEWL>>    """"""<<NEWL>>    The 'geometry_columns' view from PostGIS. See the PostGIS<<NEWL>>    documentation at Ch. 4.3.2.<<NEWL>>    """"""<<NEWL>><<NEWL>>    f_table_catalog = models.CharField(max_length=256)<<NEWL>>    f_table_schema = models.CharField(max_length=256)<<NEWL>>    f_table_name = models.CharField(max_length=256)<<NEWL>>    f_geometry_column = models.CharField(max_length=256)<<NEWL>>    coord_dimension = models.IntegerField()<<NEWL>>    srid = models.IntegerField(primary_key=True)<<NEWL>>    type = models.CharField(max_length=30)<<NEWL>><<NEWL>>    class Meta:<<NEWL>>        app_label = ""gis""<<NEWL>>        db_table = ""geometry_columns""<<NEWL>>        managed = False<<NEWL>><<NEWL>>    def __str__(self):<<NEWL>>        return ""%s.%s - %dD %s field (SRID: %d)"" % (<<NEWL>>            self.f_table_name,<<NEWL>>            self.f_geometry_column,<<NEWL>>            self.coord_dimension,<<NEWL>>            self.type,<<NEWL>>            self.srid,<<NEWL>>        )<<NEWL>><<NEWL>>    @classmethod<<NEWL>>    def table_name_col(cls):<<NEWL>>        """"""<<NEWL>>        Return the name of the metadata column used to store the feature table<<NEWL>>        name.<<NEWL>>        """"""<<NEWL>>        return ""f_table_name""<<NEWL>><<NEWL>>    @classmethod<<NEWL>>    def geom_col_name(cls):<<NEWL>>        """"""<<NEWL>>        Return the name of the metadata column used to store the feature<<NEWL>>        geometry column.<<NEWL>>        """"""<<NEWL>>        return ""f_geometry_column""<<NEWL>><<NEWL>><<NEWL>>class PostGISSpatialRefSys(models.Model, SpatialRefSysMixin):<<NEWL>>    """"""<<NEWL>>    The 'spatial_ref_sys' table from PostGIS. See the PostGIS<<NEWL>>    documentation at Ch. 4.2.1.<<NEWL>>    """"""<<NEWL>><<NEWL>>    srid = models.IntegerField(primary_key=True)<<NEWL>>    auth_name = models.CharField(max_length=256)<<NEWL>>    auth_srid = models.IntegerField()<<NEWL>>    srtext = models.CharField(max_length=2048)<<NEWL>>    proj4text = models.CharField(max_length=2048)<<NEWL>><<NEWL>>    class Meta:<<NEWL>>        app_label = ""gis""<<NEWL>>        db_table = ""spatial_ref_sys""<<NEWL>>        managed = False<<NEWL>><<NEWL>>    @property<<NEWL>>    def wkt(self):<<NEWL>>        return self.srtext"
369	jackson	3	"""""""<<NEWL>>    pygments.filter<<NEWL>>    ~~~~~~~~~~~~~~~<<NEWL>><<NEWL>>    Module that implements the default filter.<<NEWL>><<NEWL>>    :copyright: Copyright 2006-2022 by the Pygments team, see AUTHORS.<<NEWL>>    :license: BSD, see LICENSE for details.<<NEWL>>""""""<<NEWL>><<NEWL>><<NEWL>>def apply_filters(stream, filters, lexer=None):<<NEWL>>    """"""<<NEWL>>    Use this method to apply an iterable of filters to<<NEWL>>    a stream. If lexer is given it's forwarded to the<<NEWL>>    filter, otherwise the filter receives `None`.<<NEWL>>    """"""<<NEWL>>    def _apply(filter_, stream):<<NEWL>>        yield from filter_.filter(lexer, stream)<<NEWL>>    for filter_ in filters:<<NEWL>>        stream = _apply(filter_, stream)<<NEWL>>    return stream<<NEWL>><<NEWL>><<NEWL>>def simplefilter(f):<<NEWL>>    """"""<<NEWL>>    Decorator that converts a function into a filter::<<NEWL>><<NEWL>>        @simplefilter<<NEWL>>        def lowercase(self, lexer, stream, options):<<NEWL>>            for ttype, value in stream:<<NEWL>>                yield ttype, value.lower()<<NEWL>>    """"""<<NEWL>>    return type(f.__name__, (FunctionFilter,), {<<NEWL>>        '__module__': getattr(f, '__module__'),<<NEWL>>        '__doc__': f.__doc__,<<NEWL>>        'function': f,<<NEWL>>    })<<NEWL>><<NEWL>><<NEWL>>class Filter:<<NEWL>>    """"""<<NEWL>>    Default filter. Subclass this class or use the `simplefilter`<<NEWL>>    decorator to create own filters.<<NEWL>>    """"""<<NEWL>><<NEWL>>    def __init__(self, **options):<<NEWL>>        self.options = options<<NEWL>><<NEWL>>    def filter(self, lexer, stream):<<NEWL>>        raise NotImplementedError()<<NEWL>><<NEWL>><<NEWL>>class FunctionFilter(Filter):<<NEWL>>    """"""<<NEWL>>    Abstract class used by `simplefilter` to create simple<<NEWL>>    function filters on the fly. The `simplefilter` decorator<<NEWL>>    automatically creates subclasses of this class for<<NEWL>>    functions passed to it.<<NEWL>>    """"""<<NEWL>>    function = None<<NEWL>><<NEWL>>    def __init__(self, **options):<<NEWL>>        if not hasattr(self, 'function'):<<NEWL>>            raise TypeError('%r used without bound function' %<<NEWL>>                            self.__class__.__name__)<<NEWL>>        Filter.__init__(self, **options)<<NEWL>><<NEWL>>    def filter(self, lexer, stream):<<NEWL>>        # pylint: disable=not-callable<<NEWL>>        yield from self.function(lexer, stream, self.options)"
369	donghui	2	"""""""<<NEWL>>    pygments.filter<<NEWL>>    ~~~~~~~~~~~~~~~<<NEWL>><<NEWL>>    Module that implements the default filter.<<NEWL>><<NEWL>>    :copyright: Copyright 2006-2022 by the Pygments team, see AUTHORS.<<NEWL>>    :license: BSD, see LICENSE for details.<<NEWL>>""""""<<NEWL>><<NEWL>><<NEWL>>def apply_filters(stream, filters, lexer=None):<<NEWL>>    """"""<<NEWL>>    Use this method to apply an iterable of filters to<<NEWL>>    a stream. If lexer is given it's forwarded to the<<NEWL>>    filter, otherwise the filter receives `None`.<<NEWL>>    """"""<<NEWL>>    def _apply(filter_, stream):<<NEWL>>        yield from filter_.filter(lexer, stream)<<NEWL>>    for filter_ in filters:<<NEWL>>        stream = _apply(filter_, stream)<<NEWL>>    return stream<<NEWL>><<NEWL>><<NEWL>>def simplefilter(f):<<NEWL>>    """"""<<NEWL>>    Decorator that converts a function into a filter::<<NEWL>><<NEWL>>        @simplefilter<<NEWL>>        def lowercase(self, lexer, stream, options):<<NEWL>>            for ttype, value in stream:<<NEWL>>                yield ttype, value.lower()<<NEWL>>    """"""<<NEWL>>    return type(f.__name__, (FunctionFilter,), {<<NEWL>>        '__module__': getattr(f, '__module__'),<<NEWL>>        '__doc__': f.__doc__,<<NEWL>>        'function': f,<<NEWL>>    })<<NEWL>><<NEWL>><<NEWL>>class Filter:<<NEWL>>    """"""<<NEWL>>    Default filter. Subclass this class or use the `simplefilter`<<NEWL>>    decorator to create own filters.<<NEWL>>    """"""<<NEWL>><<NEWL>>    def __init__(self, **options):<<NEWL>>        self.options = options<<NEWL>><<NEWL>>    def filter(self, lexer, stream):<<NEWL>>        raise NotImplementedError()<<NEWL>><<NEWL>><<NEWL>>class FunctionFilter(Filter):<<NEWL>>    """"""<<NEWL>>    Abstract class used by `simplefilter` to create simple<<NEWL>>    function filters on the fly. The `simplefilter` decorator<<NEWL>>    automatically creates subclasses of this class for<<NEWL>>    functions passed to it.<<NEWL>>    """"""<<NEWL>>    function = None<<NEWL>><<NEWL>>    def __init__(self, **options):<<NEWL>>        if not hasattr(self, 'function'):<<NEWL>>            raise TypeError('%r used without bound function' %<<NEWL>>                            self.__class__.__name__)<<NEWL>>        Filter.__init__(self, **options)<<NEWL>><<NEWL>>    def filter(self, lexer, stream):<<NEWL>>        # pylint: disable=not-callable<<NEWL>>        yield from self.function(lexer, stream, self.options)"
338	jackson	2	""""""" Test functions for linalg module using the matrix class.""""""<<NEWL>>import numpy as np<<NEWL>><<NEWL>>from numpy.linalg.tests.test_linalg import (<<NEWL>>    LinalgCase, apply_tag, TestQR as _TestQR, LinalgTestCase,<<NEWL>>    _TestNorm2D, _TestNormDoubleBase, _TestNormSingleBase, _TestNormInt64Base,<<NEWL>>    SolveCases, InvCases, EigvalsCases, EigCases, SVDCases, CondCases,<<NEWL>>    PinvCases, DetCases, LstsqCases)<<NEWL>><<NEWL>><<NEWL>>CASES = []<<NEWL>><<NEWL>># square test cases<<NEWL>>CASES += apply_tag('square', [<<NEWL>>    LinalgCase(""0x0_matrix"",<<NEWL>>               np.empty((0, 0), dtype=np.double).view(np.matrix),<<NEWL>>               np.empty((0, 1), dtype=np.double).view(np.matrix),<<NEWL>>               tags={'size-0'}),<<NEWL>>    LinalgCase(""matrix_b_only"",<<NEWL>>               np.array([[1., 2.], [3., 4.]]),<<NEWL>>               np.matrix([2., 1.]).T),<<NEWL>>    LinalgCase(""matrix_a_and_b"",<<NEWL>>               np.matrix([[1., 2.], [3., 4.]]),<<NEWL>>               np.matrix([2., 1.]).T),<<NEWL>>])<<NEWL>><<NEWL>># hermitian test-cases<<NEWL>>CASES += apply_tag('hermitian', [<<NEWL>>    LinalgCase(""hmatrix_a_and_b"",<<NEWL>>               np.matrix([[1., 2.], [2., 1.]]),<<NEWL>>               None),<<NEWL>>])<<NEWL>># No need to make generalized or strided cases for matrices.<<NEWL>><<NEWL>><<NEWL>>class MatrixTestCase(LinalgTestCase):<<NEWL>>    TEST_CASES = CASES<<NEWL>><<NEWL>><<NEWL>>class TestSolveMatrix(SolveCases, MatrixTestCase):<<NEWL>>    pass<<NEWL>><<NEWL>><<NEWL>>class TestInvMatrix(InvCases, MatrixTestCase):<<NEWL>>    pass<<NEWL>><<NEWL>><<NEWL>>class TestEigvalsMatrix(EigvalsCases, MatrixTestCase):<<NEWL>>    pass<<NEWL>><<NEWL>><<NEWL>>class TestEigMatrix(EigCases, MatrixTestCase):<<NEWL>>    pass<<NEWL>><<NEWL>><<NEWL>>class TestSVDMatrix(SVDCases, MatrixTestCase):<<NEWL>>    pass<<NEWL>><<NEWL>><<NEWL>>class TestCondMatrix(CondCases, MatrixTestCase):<<NEWL>>    pass<<NEWL>><<NEWL>><<NEWL>>class TestPinvMatrix(PinvCases, MatrixTestCase):<<NEWL>>    pass<<NEWL>><<NEWL>><<NEWL>>class TestDetMatrix(DetCases, MatrixTestCase):<<NEWL>>    pass<<NEWL>><<NEWL>><<NEWL>>class TestLstsqMatrix(LstsqCases, MatrixTestCase):<<NEWL>>    pass<<NEWL>><<NEWL>><<NEWL>>class _TestNorm2DMatrix(_TestNorm2D):<<NEWL>>    array = np.matrix<<NEWL>><<NEWL>><<NEWL>>class TestNormDoubleMatrix(_TestNorm2DMatrix, _TestNormDoubleBase):<<NEWL>>    pass<<NEWL>><<NEWL>><<NEWL>>class TestNormSingleMatrix(_TestNorm2DMatrix, _TestNormSingleBase):<<NEWL>>    pass<<NEWL>><<NEWL>><<NEWL>>class TestNormInt64Matrix(_TestNorm2DMatrix, _TestNormInt64Base):<<NEWL>>    pass<<NEWL>><<NEWL>><<NEWL>>class TestQRMatrix(_TestQR):<<NEWL>>    array = np.matrix"
338	donghui	1	""""""" Test functions for linalg module using the matrix class.""""""<<NEWL>>import numpy as np<<NEWL>><<NEWL>>from numpy.linalg.tests.test_linalg import (<<NEWL>>    LinalgCase, apply_tag, TestQR as _TestQR, LinalgTestCase,<<NEWL>>    _TestNorm2D, _TestNormDoubleBase, _TestNormSingleBase, _TestNormInt64Base,<<NEWL>>    SolveCases, InvCases, EigvalsCases, EigCases, SVDCases, CondCases,<<NEWL>>    PinvCases, DetCases, LstsqCases)<<NEWL>><<NEWL>><<NEWL>>CASES = []<<NEWL>><<NEWL>># square test cases<<NEWL>>CASES += apply_tag('square', [<<NEWL>>    LinalgCase(""0x0_matrix"",<<NEWL>>               np.empty((0, 0), dtype=np.double).view(np.matrix),<<NEWL>>               np.empty((0, 1), dtype=np.double).view(np.matrix),<<NEWL>>               tags={'size-0'}),<<NEWL>>    LinalgCase(""matrix_b_only"",<<NEWL>>               np.array([[1., 2.], [3., 4.]]),<<NEWL>>               np.matrix([2., 1.]).T),<<NEWL>>    LinalgCase(""matrix_a_and_b"",<<NEWL>>               np.matrix([[1., 2.], [3., 4.]]),<<NEWL>>               np.matrix([2., 1.]).T),<<NEWL>>])<<NEWL>><<NEWL>># hermitian test-cases<<NEWL>>CASES += apply_tag('hermitian', [<<NEWL>>    LinalgCase(""hmatrix_a_and_b"",<<NEWL>>               np.matrix([[1., 2.], [2., 1.]]),<<NEWL>>               None),<<NEWL>>])<<NEWL>># No need to make generalized or strided cases for matrices.<<NEWL>><<NEWL>><<NEWL>>class MatrixTestCase(LinalgTestCase):<<NEWL>>    TEST_CASES = CASES<<NEWL>><<NEWL>><<NEWL>>class TestSolveMatrix(SolveCases, MatrixTestCase):<<NEWL>>    pass<<NEWL>><<NEWL>><<NEWL>>class TestInvMatrix(InvCases, MatrixTestCase):<<NEWL>>    pass<<NEWL>><<NEWL>><<NEWL>>class TestEigvalsMatrix(EigvalsCases, MatrixTestCase):<<NEWL>>    pass<<NEWL>><<NEWL>><<NEWL>>class TestEigMatrix(EigCases, MatrixTestCase):<<NEWL>>    pass<<NEWL>><<NEWL>><<NEWL>>class TestSVDMatrix(SVDCases, MatrixTestCase):<<NEWL>>    pass<<NEWL>><<NEWL>><<NEWL>>class TestCondMatrix(CondCases, MatrixTestCase):<<NEWL>>    pass<<NEWL>><<NEWL>><<NEWL>>class TestPinvMatrix(PinvCases, MatrixTestCase):<<NEWL>>    pass<<NEWL>><<NEWL>><<NEWL>>class TestDetMatrix(DetCases, MatrixTestCase):<<NEWL>>    pass<<NEWL>><<NEWL>><<NEWL>>class TestLstsqMatrix(LstsqCases, MatrixTestCase):<<NEWL>>    pass<<NEWL>><<NEWL>><<NEWL>>class _TestNorm2DMatrix(_TestNorm2D):<<NEWL>>    array = np.matrix<<NEWL>><<NEWL>><<NEWL>>class TestNormDoubleMatrix(_TestNorm2DMatrix, _TestNormDoubleBase):<<NEWL>>    pass<<NEWL>><<NEWL>><<NEWL>>class TestNormSingleMatrix(_TestNorm2DMatrix, _TestNormSingleBase):<<NEWL>>    pass<<NEWL>><<NEWL>><<NEWL>>class TestNormInt64Matrix(_TestNorm2DMatrix, _TestNormInt64Base):<<NEWL>>    pass<<NEWL>><<NEWL>><<NEWL>>class TestQRMatrix(_TestQR):<<NEWL>>    array = np.matrix"
278	jackson	0	# Copyright (c) 2020, Oracle and/or its affiliates.<<NEWL>>#<<NEWL>># This program is free software; you can redistribute it and/or modify<<NEWL>># it under the terms of the GNU General Public License, version 2.0, as<<NEWL>># published by the Free Software Foundation.<<NEWL>>#<<NEWL>># This program is also distributed with certain software (including<<NEWL>># but not limited to OpenSSL) that is licensed under separate terms,<<NEWL>># as designated in a particular file or component or in included license<<NEWL>># documentation.  The authors of MySQL hereby grant you an<<NEWL>># additional permission to link the program and your derivative works<<NEWL>># with the separately licensed software that they have included with<<NEWL>># MySQL.<<NEWL>>#<<NEWL>># Without limiting anything contained in the foregoing, this file,<<NEWL>># which is part of MySQL Connector/Python, is also subject to the<<NEWL>># Universal FOSS Exception, version 1.0, a copy of which can be found at<<NEWL>># http://oss.oracle.com/licenses/universal-foss-exception.<<NEWL>>#<<NEWL>># This program is distributed in the hope that it will be useful, but<<NEWL>># WITHOUT ANY WARRANTY; without even the implied warranty of<<NEWL>># MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.<<NEWL>># See the GNU General Public License, version 2.0, for more details.<<NEWL>>#<<NEWL>># You should have received a copy of the GNU General Public License<<NEWL>># along with this program; if not, write to the Free Software Foundation, Inc.,<<NEWL>># 51 Franklin St, Fifth Floor, Boston, MA 02110-1301  USA<<NEWL>><<NEWL>>from django.db.backends.mysql.features import DatabaseFeatures as MySQLDatabaseFeatures<<NEWL>>from django.utils.functional import cached_property<<NEWL>><<NEWL>><<NEWL>>class DatabaseFeatures(MySQLDatabaseFeatures):<<NEWL>>    empty_fetchmany_value = []<<NEWL>><<NEWL>>    @cached_property<<NEWL>>    def can_introspect_check_constraints(self):<<NEWL>>        return self.connection.mysql_version >= (8, 0, 16)<<NEWL>><<NEWL>>    @cached_property<<NEWL>>    def supports_microsecond_precision(self):<<NEWL>>        if self.connection.mysql_version >= (5, 6, 3):<<NEWL>>            return True<<NEWL>>        return False
278	donghui	0	# Copyright (c) 2020, Oracle and/or its affiliates.<<NEWL>>#<<NEWL>># This program is free software; you can redistribute it and/or modify<<NEWL>># it under the terms of the GNU General Public License, version 2.0, as<<NEWL>># published by the Free Software Foundation.<<NEWL>>#<<NEWL>># This program is also distributed with certain software (including<<NEWL>># but not limited to OpenSSL) that is licensed under separate terms,<<NEWL>># as designated in a particular file or component or in included license<<NEWL>># documentation.  The authors of MySQL hereby grant you an<<NEWL>># additional permission to link the program and your derivative works<<NEWL>># with the separately licensed software that they have included with<<NEWL>># MySQL.<<NEWL>>#<<NEWL>># Without limiting anything contained in the foregoing, this file,<<NEWL>># which is part of MySQL Connector/Python, is also subject to the<<NEWL>># Universal FOSS Exception, version 1.0, a copy of which can be found at<<NEWL>># http://oss.oracle.com/licenses/universal-foss-exception.<<NEWL>>#<<NEWL>># This program is distributed in the hope that it will be useful, but<<NEWL>># WITHOUT ANY WARRANTY; without even the implied warranty of<<NEWL>># MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.<<NEWL>># See the GNU General Public License, version 2.0, for more details.<<NEWL>>#<<NEWL>># You should have received a copy of the GNU General Public License<<NEWL>># along with this program; if not, write to the Free Software Foundation, Inc.,<<NEWL>># 51 Franklin St, Fifth Floor, Boston, MA 02110-1301  USA<<NEWL>><<NEWL>>from django.db.backends.mysql.features import DatabaseFeatures as MySQLDatabaseFeatures<<NEWL>>from django.utils.functional import cached_property<<NEWL>><<NEWL>><<NEWL>>class DatabaseFeatures(MySQLDatabaseFeatures):<<NEWL>>    empty_fetchmany_value = []<<NEWL>><<NEWL>>    @cached_property<<NEWL>>    def can_introspect_check_constraints(self):<<NEWL>>        return self.connection.mysql_version >= (8, 0, 16)<<NEWL>><<NEWL>>    @cached_property<<NEWL>>    def supports_microsecond_precision(self):<<NEWL>>        if self.connection.mysql_version >= (5, 6, 3):<<NEWL>>            return True<<NEWL>>        return False
268	jackson	1	"from distutils.util import convert_path<<NEWL>>from distutils import log<<NEWL>>from distutils.errors import DistutilsOptionError<<NEWL>>import os<<NEWL>>import shutil<<NEWL>><<NEWL>>from setuptools.extern import six<<NEWL>><<NEWL>>from setuptools import Command<<NEWL>><<NEWL>><<NEWL>>class rotate(Command):<<NEWL>>    """"""Delete older distributions""""""<<NEWL>><<NEWL>>    description = ""delete older distributions, keeping N newest files""<<NEWL>>    user_options = [<<NEWL>>        ('match=', 'm', ""patterns to match (required)""),<<NEWL>>        ('dist-dir=', 'd', ""directory where the distributions are""),<<NEWL>>        ('keep=', 'k', ""number of matching distributions to keep""),<<NEWL>>    ]<<NEWL>><<NEWL>>    boolean_options = []<<NEWL>><<NEWL>>    def initialize_options(self):<<NEWL>>        self.match = None<<NEWL>>        self.dist_dir = None<<NEWL>>        self.keep = None<<NEWL>><<NEWL>>    def finalize_options(self):<<NEWL>>        if self.match is None:<<NEWL>>            raise DistutilsOptionError(<<NEWL>>                ""Must specify one or more (comma-separated) match patterns ""<<NEWL>>                ""(e.g. '.zip' or '.egg')""<<NEWL>>            )<<NEWL>>        if self.keep is None:<<NEWL>>            raise DistutilsOptionError(""Must specify number of files to keep"")<<NEWL>>        try:<<NEWL>>            self.keep = int(self.keep)<<NEWL>>        except ValueError:<<NEWL>>            raise DistutilsOptionError(""--keep must be an integer"")<<NEWL>>        if isinstance(self.match, six.string_types):<<NEWL>>            self.match = [<<NEWL>>                convert_path(p.strip()) for p in self.match.split(',')<<NEWL>>            ]<<NEWL>>        self.set_undefined_options('bdist', ('dist_dir', 'dist_dir'))<<NEWL>><<NEWL>>    def run(self):<<NEWL>>        self.run_command(""egg_info"")<<NEWL>>        from glob import glob<<NEWL>><<NEWL>>        for pattern in self.match:<<NEWL>>            pattern = self.distribution.get_name() + '*' + pattern<<NEWL>>            files = glob(os.path.join(self.dist_dir, pattern))<<NEWL>>            files = [(os.path.getmtime(f), f) for f in files]<<NEWL>>            files.sort()<<NEWL>>            files.reverse()<<NEWL>><<NEWL>>            log.info(""%d file(s) matching %s"", len(files), pattern)<<NEWL>>            files = files[self.keep:]<<NEWL>>            for (t, f) in files:<<NEWL>>                log.info(""Deleting %s"", f)<<NEWL>>                if not self.dry_run:<<NEWL>>                    if os.path.isdir(f):<<NEWL>>                        shutil.rmtree(f)<<NEWL>>                    else:<<NEWL>>                        os.unlink(f)"
268	donghui	1	"from distutils.util import convert_path<<NEWL>>from distutils import log<<NEWL>>from distutils.errors import DistutilsOptionError<<NEWL>>import os<<NEWL>>import shutil<<NEWL>><<NEWL>>from setuptools.extern import six<<NEWL>><<NEWL>>from setuptools import Command<<NEWL>><<NEWL>><<NEWL>>class rotate(Command):<<NEWL>>    """"""Delete older distributions""""""<<NEWL>><<NEWL>>    description = ""delete older distributions, keeping N newest files""<<NEWL>>    user_options = [<<NEWL>>        ('match=', 'm', ""patterns to match (required)""),<<NEWL>>        ('dist-dir=', 'd', ""directory where the distributions are""),<<NEWL>>        ('keep=', 'k', ""number of matching distributions to keep""),<<NEWL>>    ]<<NEWL>><<NEWL>>    boolean_options = []<<NEWL>><<NEWL>>    def initialize_options(self):<<NEWL>>        self.match = None<<NEWL>>        self.dist_dir = None<<NEWL>>        self.keep = None<<NEWL>><<NEWL>>    def finalize_options(self):<<NEWL>>        if self.match is None:<<NEWL>>            raise DistutilsOptionError(<<NEWL>>                ""Must specify one or more (comma-separated) match patterns ""<<NEWL>>                ""(e.g. '.zip' or '.egg')""<<NEWL>>            )<<NEWL>>        if self.keep is None:<<NEWL>>            raise DistutilsOptionError(""Must specify number of files to keep"")<<NEWL>>        try:<<NEWL>>            self.keep = int(self.keep)<<NEWL>>        except ValueError:<<NEWL>>            raise DistutilsOptionError(""--keep must be an integer"")<<NEWL>>        if isinstance(self.match, six.string_types):<<NEWL>>            self.match = [<<NEWL>>                convert_path(p.strip()) for p in self.match.split(',')<<NEWL>>            ]<<NEWL>>        self.set_undefined_options('bdist', ('dist_dir', 'dist_dir'))<<NEWL>><<NEWL>>    def run(self):<<NEWL>>        self.run_command(""egg_info"")<<NEWL>>        from glob import glob<<NEWL>><<NEWL>>        for pattern in self.match:<<NEWL>>            pattern = self.distribution.get_name() + '*' + pattern<<NEWL>>            files = glob(os.path.join(self.dist_dir, pattern))<<NEWL>>            files = [(os.path.getmtime(f), f) for f in files]<<NEWL>>            files.sort()<<NEWL>>            files.reverse()<<NEWL>><<NEWL>>            log.info(""%d file(s) matching %s"", len(files), pattern)<<NEWL>>            files = files[self.keep:]<<NEWL>>            for (t, f) in files:<<NEWL>>                log.info(""Deleting %s"", f)<<NEWL>>                if not self.dry_run:<<NEWL>>                    if os.path.isdir(f):<<NEWL>>                        shutil.rmtree(f)<<NEWL>>                    else:<<NEWL>>                        os.unlink(f)"
328	jackson	0	"from esphome.components import fan<<NEWL>>import esphome.config_validation as cv<<NEWL>>import esphome.codegen as cg<<NEWL>>from esphome.const import CONF_OUTPUT_ID, CONF_SPEED_COUNT, CONF_SWITCH_DATAPOINT<<NEWL>>from .. import tuya_ns, CONF_TUYA_ID, Tuya<<NEWL>><<NEWL>>DEPENDENCIES = [""tuya""]<<NEWL>><<NEWL>>CONF_SPEED_DATAPOINT = ""speed_datapoint""<<NEWL>>CONF_OSCILLATION_DATAPOINT = ""oscillation_datapoint""<<NEWL>>CONF_DIRECTION_DATAPOINT = ""direction_datapoint""<<NEWL>><<NEWL>>TuyaFan = tuya_ns.class_(""TuyaFan"", cg.Component, fan.Fan)<<NEWL>><<NEWL>>CONFIG_SCHEMA = cv.All(<<NEWL>>    fan.FAN_SCHEMA.extend(<<NEWL>>        {<<NEWL>>            cv.GenerateID(CONF_OUTPUT_ID): cv.declare_id(TuyaFan),<<NEWL>>            cv.GenerateID(CONF_TUYA_ID): cv.use_id(Tuya),<<NEWL>>            cv.Optional(CONF_OSCILLATION_DATAPOINT): cv.uint8_t,<<NEWL>>            cv.Optional(CONF_SPEED_DATAPOINT): cv.uint8_t,<<NEWL>>            cv.Optional(CONF_SWITCH_DATAPOINT): cv.uint8_t,<<NEWL>>            cv.Optional(CONF_DIRECTION_DATAPOINT): cv.uint8_t,<<NEWL>>            cv.Optional(CONF_SPEED_COUNT, default=3): cv.int_range(min=1, max=256),<<NEWL>>        }<<NEWL>>    ).extend(cv.COMPONENT_SCHEMA),<<NEWL>>    cv.has_at_least_one_key(CONF_SPEED_DATAPOINT, CONF_SWITCH_DATAPOINT),<<NEWL>>)<<NEWL>><<NEWL>><<NEWL>>async def to_code(config):<<NEWL>>    parent = await cg.get_variable(config[CONF_TUYA_ID])<<NEWL>><<NEWL>>    var = cg.new_Pvariable(config[CONF_OUTPUT_ID], parent, config[CONF_SPEED_COUNT])<<NEWL>>    await cg.register_component(var, config)<<NEWL>>    await fan.register_fan(var, config)<<NEWL>><<NEWL>>    if CONF_SPEED_DATAPOINT in config:<<NEWL>>        cg.add(var.set_speed_id(config[CONF_SPEED_DATAPOINT]))<<NEWL>>    if CONF_SWITCH_DATAPOINT in config:<<NEWL>>        cg.add(var.set_switch_id(config[CONF_SWITCH_DATAPOINT]))<<NEWL>>    if CONF_OSCILLATION_DATAPOINT in config:<<NEWL>>        cg.add(var.set_oscillation_id(config[CONF_OSCILLATION_DATAPOINT]))<<NEWL>>    if CONF_DIRECTION_DATAPOINT in config:<<NEWL>>        cg.add(var.set_direction_id(config[CONF_DIRECTION_DATAPOINT]))"
328	donghui	0	"from esphome.components import fan<<NEWL>>import esphome.config_validation as cv<<NEWL>>import esphome.codegen as cg<<NEWL>>from esphome.const import CONF_OUTPUT_ID, CONF_SPEED_COUNT, CONF_SWITCH_DATAPOINT<<NEWL>>from .. import tuya_ns, CONF_TUYA_ID, Tuya<<NEWL>><<NEWL>>DEPENDENCIES = [""tuya""]<<NEWL>><<NEWL>>CONF_SPEED_DATAPOINT = ""speed_datapoint""<<NEWL>>CONF_OSCILLATION_DATAPOINT = ""oscillation_datapoint""<<NEWL>>CONF_DIRECTION_DATAPOINT = ""direction_datapoint""<<NEWL>><<NEWL>>TuyaFan = tuya_ns.class_(""TuyaFan"", cg.Component, fan.Fan)<<NEWL>><<NEWL>>CONFIG_SCHEMA = cv.All(<<NEWL>>    fan.FAN_SCHEMA.extend(<<NEWL>>        {<<NEWL>>            cv.GenerateID(CONF_OUTPUT_ID): cv.declare_id(TuyaFan),<<NEWL>>            cv.GenerateID(CONF_TUYA_ID): cv.use_id(Tuya),<<NEWL>>            cv.Optional(CONF_OSCILLATION_DATAPOINT): cv.uint8_t,<<NEWL>>            cv.Optional(CONF_SPEED_DATAPOINT): cv.uint8_t,<<NEWL>>            cv.Optional(CONF_SWITCH_DATAPOINT): cv.uint8_t,<<NEWL>>            cv.Optional(CONF_DIRECTION_DATAPOINT): cv.uint8_t,<<NEWL>>            cv.Optional(CONF_SPEED_COUNT, default=3): cv.int_range(min=1, max=256),<<NEWL>>        }<<NEWL>>    ).extend(cv.COMPONENT_SCHEMA),<<NEWL>>    cv.has_at_least_one_key(CONF_SPEED_DATAPOINT, CONF_SWITCH_DATAPOINT),<<NEWL>>)<<NEWL>><<NEWL>><<NEWL>>async def to_code(config):<<NEWL>>    parent = await cg.get_variable(config[CONF_TUYA_ID])<<NEWL>><<NEWL>>    var = cg.new_Pvariable(config[CONF_OUTPUT_ID], parent, config[CONF_SPEED_COUNT])<<NEWL>>    await cg.register_component(var, config)<<NEWL>>    await fan.register_fan(var, config)<<NEWL>><<NEWL>>    if CONF_SPEED_DATAPOINT in config:<<NEWL>>        cg.add(var.set_speed_id(config[CONF_SPEED_DATAPOINT]))<<NEWL>>    if CONF_SWITCH_DATAPOINT in config:<<NEWL>>        cg.add(var.set_switch_id(config[CONF_SWITCH_DATAPOINT]))<<NEWL>>    if CONF_OSCILLATION_DATAPOINT in config:<<NEWL>>        cg.add(var.set_oscillation_id(config[CONF_OSCILLATION_DATAPOINT]))<<NEWL>>    if CONF_DIRECTION_DATAPOINT in config:<<NEWL>>        cg.add(var.set_direction_id(config[CONF_DIRECTION_DATAPOINT]))"
379	jackson	0	"from typing import Optional<<NEWL>><<NEWL>>from fastapi import Depends, Request<<NEWL>>from fastapi_users import BaseUserManager, FastAPIUsers, IntegerIDMixin<<NEWL>>from fastapi_users.authentication import AuthenticationBackend, BearerTransport, CookieTransport, JWTStrategy<<NEWL>>from fastapi_users.db import SQLAlchemyUserDatabase<<NEWL>><<NEWL>>import crud<<NEWL>>from app import models<<NEWL>>from app.deps import get_db<<NEWL>>from .db import User, get_user_db<<NEWL>>from .secrets import secrets<<NEWL>><<NEWL>>class UserManager(IntegerIDMixin, BaseUserManager[User, int]):<<NEWL>>    reset_password_token_secret = secrets['SECRET_KEY']<<NEWL>>    verification_token_secret = secrets['SECRET_KEY']<<NEWL>><<NEWL>>    async def on_after_register(self, user: User, request: Optional[Request] = None):<<NEWL>>        print(f""User {user.id} has registered."")<<NEWL>><<NEWL>>    async def on_after_forgot_password(<<NEWL>>        self, user: User, token: str, request: Optional[Request] = None<<NEWL>>    ):<<NEWL>>        print(f""User {user.id} has forgot their password. Reset token: {token}"")<<NEWL>><<NEWL>>    async def on_after_request_verify(<<NEWL>>        self, user: User, token: str, request: Optional[Request] = None<<NEWL>>    ):<<NEWL>>        print(f""Verification requested for user {user.id}. Verification token: {token}"")<<NEWL>><<NEWL>><<NEWL>>async def get_user_manager(user_db: SQLAlchemyUserDatabase = Depends(get_user_db)):<<NEWL>>    yield UserManager(user_db)<<NEWL>><<NEWL>><<NEWL>>bearer_transport = BearerTransport(tokenUrl=""auth/jwt/login"")<<NEWL>><<NEWL>><<NEWL>>def get_jwt_strategy() -> JWTStrategy:<<NEWL>>    return JWTStrategy(secret=secrets['SECRET_KEY'], lifetime_seconds=3600)<<NEWL>><<NEWL>><<NEWL>>jwt_auth_backend = AuthenticationBackend(<<NEWL>>    name=""jwt"",<<NEWL>>    transport=bearer_transport,<<NEWL>>    get_strategy=get_jwt_strategy,<<NEWL>>)<<NEWL>><<NEWL>>cookie_transport = CookieTransport(cookie_max_age=3600)<<NEWL>><<NEWL>>cookie_auth_backend = AuthenticationBackend(<<NEWL>>    name=""cookie"",<<NEWL>>    transport=cookie_transport,<<NEWL>>    get_strategy=get_jwt_strategy,<<NEWL>>)<<NEWL>><<NEWL>>fastapi_users = FastAPIUsers[User, int](get_user_manager, [jwt_auth_backend, cookie_auth_backend])<<NEWL>><<NEWL>>current_active_user = fastapi_users.current_user(active=True)<<NEWL>><<NEWL>>async def get_current_profile(user: User = Depends(current_active_user), db = Depends(get_db)):<<NEWL>>    profile = await crud.read(_id=user.id, db=db, model=models.Profile)<<NEWL>>    return profile"
379	donghui	0	"from typing import Optional<<NEWL>><<NEWL>>from fastapi import Depends, Request<<NEWL>>from fastapi_users import BaseUserManager, FastAPIUsers, IntegerIDMixin<<NEWL>>from fastapi_users.authentication import AuthenticationBackend, BearerTransport, CookieTransport, JWTStrategy<<NEWL>>from fastapi_users.db import SQLAlchemyUserDatabase<<NEWL>><<NEWL>>import crud<<NEWL>>from app import models<<NEWL>>from app.deps import get_db<<NEWL>>from .db import User, get_user_db<<NEWL>>from .secrets import secrets<<NEWL>><<NEWL>>class UserManager(IntegerIDMixin, BaseUserManager[User, int]):<<NEWL>>    reset_password_token_secret = secrets['SECRET_KEY']<<NEWL>>    verification_token_secret = secrets['SECRET_KEY']<<NEWL>><<NEWL>>    async def on_after_register(self, user: User, request: Optional[Request] = None):<<NEWL>>        print(f""User {user.id} has registered."")<<NEWL>><<NEWL>>    async def on_after_forgot_password(<<NEWL>>        self, user: User, token: str, request: Optional[Request] = None<<NEWL>>    ):<<NEWL>>        print(f""User {user.id} has forgot their password. Reset token: {token}"")<<NEWL>><<NEWL>>    async def on_after_request_verify(<<NEWL>>        self, user: User, token: str, request: Optional[Request] = None<<NEWL>>    ):<<NEWL>>        print(f""Verification requested for user {user.id}. Verification token: {token}"")<<NEWL>><<NEWL>><<NEWL>>async def get_user_manager(user_db: SQLAlchemyUserDatabase = Depends(get_user_db)):<<NEWL>>    yield UserManager(user_db)<<NEWL>><<NEWL>><<NEWL>>bearer_transport = BearerTransport(tokenUrl=""auth/jwt/login"")<<NEWL>><<NEWL>><<NEWL>>def get_jwt_strategy() -> JWTStrategy:<<NEWL>>    return JWTStrategy(secret=secrets['SECRET_KEY'], lifetime_seconds=3600)<<NEWL>><<NEWL>><<NEWL>>jwt_auth_backend = AuthenticationBackend(<<NEWL>>    name=""jwt"",<<NEWL>>    transport=bearer_transport,<<NEWL>>    get_strategy=get_jwt_strategy,<<NEWL>>)<<NEWL>><<NEWL>>cookie_transport = CookieTransport(cookie_max_age=3600)<<NEWL>><<NEWL>>cookie_auth_backend = AuthenticationBackend(<<NEWL>>    name=""cookie"",<<NEWL>>    transport=cookie_transport,<<NEWL>>    get_strategy=get_jwt_strategy,<<NEWL>>)<<NEWL>><<NEWL>>fastapi_users = FastAPIUsers[User, int](get_user_manager, [jwt_auth_backend, cookie_auth_backend])<<NEWL>><<NEWL>>current_active_user = fastapi_users.current_user(active=True)<<NEWL>><<NEWL>>async def get_current_profile(user: User = Depends(current_active_user), db = Depends(get_db)):<<NEWL>>    profile = await crud.read(_id=user.id, db=db, model=models.Profile)<<NEWL>>    return profile"
437	jackson	3	"import sys<<NEWL>>import platform<<NEWL>><<NEWL>><<NEWL>>__all__ = ['install', 'NullFinder', 'Protocol']<<NEWL>><<NEWL>><<NEWL>>try:<<NEWL>>    from typing import Protocol<<NEWL>>except ImportError:  # pragma: no cover<<NEWL>>    # Python 3.7 compatibility<<NEWL>>    from ..typing_extensions import Protocol  # type: ignore<<NEWL>><<NEWL>><<NEWL>>def install(cls):<<NEWL>>    """"""<<NEWL>>    Class decorator for installation on sys.meta_path.<<NEWL>><<NEWL>>    Adds the backport DistributionFinder to sys.meta_path and<<NEWL>>    attempts to disable the finder functionality of the stdlib<<NEWL>>    DistributionFinder.<<NEWL>>    """"""<<NEWL>>    sys.meta_path.append(cls())<<NEWL>>    disable_stdlib_finder()<<NEWL>>    return cls<<NEWL>><<NEWL>><<NEWL>>def disable_stdlib_finder():<<NEWL>>    """"""<<NEWL>>    Give the backport primacy for discovering path-based distributions<<NEWL>>    by monkey-patching the stdlib O_O.<<NEWL>><<NEWL>>    See #91 for more background for rationale on this sketchy<<NEWL>>    behavior.<<NEWL>>    """"""<<NEWL>><<NEWL>>    def matches(finder):<<NEWL>>        return getattr(<<NEWL>>            finder, '__module__', None<<NEWL>>        ) == '_frozen_importlib_external' and hasattr(finder, 'find_distributions')<<NEWL>><<NEWL>>    for finder in filter(matches, sys.meta_path):  # pragma: nocover<<NEWL>>        del finder.find_distributions<<NEWL>><<NEWL>><<NEWL>>class NullFinder:<<NEWL>>    """"""<<NEWL>>    A ""Finder"" (aka ""MetaClassFinder"") that never finds any modules,<<NEWL>>    but may find distributions.<<NEWL>>    """"""<<NEWL>><<NEWL>>    @staticmethod<<NEWL>>    def find_spec(*args, **kwargs):<<NEWL>>        return None<<NEWL>><<NEWL>>    # In Python 2, the import system requires finders<<NEWL>>    # to have a find_module() method, but this usage<<NEWL>>    # is deprecated in Python 3 in favor of find_spec().<<NEWL>>    # For the purposes of this finder (i.e. being present<<NEWL>>    # on sys.meta_path but having no other import<<NEWL>>    # system functionality), the two methods are identical.<<NEWL>>    find_module = find_spec<<NEWL>><<NEWL>><<NEWL>>def pypy_partial(val):<<NEWL>>    """"""<<NEWL>>    Adjust for variable stacklevel on partial under PyPy.<<NEWL>><<NEWL>>    Workaround for #327.<<NEWL>>    """"""<<NEWL>>    is_pypy = platform.python_implementation() == 'PyPy'<<NEWL>>    return val + is_pypy"
437	donghui	3	"import sys<<NEWL>>import platform<<NEWL>><<NEWL>><<NEWL>>__all__ = ['install', 'NullFinder', 'Protocol']<<NEWL>><<NEWL>><<NEWL>>try:<<NEWL>>    from typing import Protocol<<NEWL>>except ImportError:  # pragma: no cover<<NEWL>>    # Python 3.7 compatibility<<NEWL>>    from ..typing_extensions import Protocol  # type: ignore<<NEWL>><<NEWL>><<NEWL>>def install(cls):<<NEWL>>    """"""<<NEWL>>    Class decorator for installation on sys.meta_path.<<NEWL>><<NEWL>>    Adds the backport DistributionFinder to sys.meta_path and<<NEWL>>    attempts to disable the finder functionality of the stdlib<<NEWL>>    DistributionFinder.<<NEWL>>    """"""<<NEWL>>    sys.meta_path.append(cls())<<NEWL>>    disable_stdlib_finder()<<NEWL>>    return cls<<NEWL>><<NEWL>><<NEWL>>def disable_stdlib_finder():<<NEWL>>    """"""<<NEWL>>    Give the backport primacy for discovering path-based distributions<<NEWL>>    by monkey-patching the stdlib O_O.<<NEWL>><<NEWL>>    See #91 for more background for rationale on this sketchy<<NEWL>>    behavior.<<NEWL>>    """"""<<NEWL>><<NEWL>>    def matches(finder):<<NEWL>>        return getattr(<<NEWL>>            finder, '__module__', None<<NEWL>>        ) == '_frozen_importlib_external' and hasattr(finder, 'find_distributions')<<NEWL>><<NEWL>>    for finder in filter(matches, sys.meta_path):  # pragma: nocover<<NEWL>>        del finder.find_distributions<<NEWL>><<NEWL>><<NEWL>>class NullFinder:<<NEWL>>    """"""<<NEWL>>    A ""Finder"" (aka ""MetaClassFinder"") that never finds any modules,<<NEWL>>    but may find distributions.<<NEWL>>    """"""<<NEWL>><<NEWL>>    @staticmethod<<NEWL>>    def find_spec(*args, **kwargs):<<NEWL>>        return None<<NEWL>><<NEWL>>    # In Python 2, the import system requires finders<<NEWL>>    # to have a find_module() method, but this usage<<NEWL>>    # is deprecated in Python 3 in favor of find_spec().<<NEWL>>    # For the purposes of this finder (i.e. being present<<NEWL>>    # on sys.meta_path but having no other import<<NEWL>>    # system functionality), the two methods are identical.<<NEWL>>    find_module = find_spec<<NEWL>><<NEWL>><<NEWL>>def pypy_partial(val):<<NEWL>>    """"""<<NEWL>>    Adjust for variable stacklevel on partial under PyPy.<<NEWL>><<NEWL>>    Workaround for #327.<<NEWL>>    """"""<<NEWL>>    is_pypy = platform.python_implementation() == 'PyPy'<<NEWL>>    return val + is_pypy"
466	jackson	1	"import argparse<<NEWL>>import unittest<<NEWL>>from typing import Any, Dict, Sequence<<NEWL>><<NEWL>>import torch<<NEWL>>from fairseq.models import transformer<<NEWL>><<NEWL>>from tests.test_roberta import FakeTask<<NEWL>><<NEWL>><<NEWL>>def mk_sample(tok: Sequence[int] = None, batch_size: int = 2) -> Dict[str, Any]:<<NEWL>>    if not tok:<<NEWL>>        tok = [10, 11, 12, 13, 14, 15, 2]<<NEWL>><<NEWL>>    batch = torch.stack([torch.tensor(tok, dtype=torch.long)] * batch_size)<<NEWL>>    sample = {<<NEWL>>        ""net_input"": {<<NEWL>>            ""src_tokens"": batch,<<NEWL>>            ""prev_output_tokens"": batch,<<NEWL>>            ""src_lengths"": torch.tensor(<<NEWL>>                [len(tok)] * batch_size, dtype=torch.long, device=batch.device<<NEWL>>            ),<<NEWL>>        },<<NEWL>>        ""target"": batch[:, 1:],<<NEWL>>    }<<NEWL>>    return sample<<NEWL>><<NEWL>><<NEWL>>def mk_transformer(**extra_args: Any):<<NEWL>>    overrides = {<<NEWL>>        # Use characteristics dimensions<<NEWL>>        ""encoder_embed_dim"": 12,<<NEWL>>        ""encoder_ffn_embed_dim"": 14,<<NEWL>>        ""decoder_embed_dim"": 12,<<NEWL>>        ""decoder_ffn_embed_dim"": 14,<<NEWL>>        # Disable dropout so we have comparable tests.<<NEWL>>        ""dropout"": 0,<<NEWL>>        ""attention_dropout"": 0,<<NEWL>>        ""activation_dropout"": 0,<<NEWL>>        ""encoder_layerdrop"": 0,<<NEWL>>    }<<NEWL>>    overrides.update(extra_args)<<NEWL>>    # Overrides the defaults from the parser<<NEWL>>    args = argparse.Namespace(**overrides)<<NEWL>>    transformer.tiny_architecture(args)<<NEWL>><<NEWL>>    torch.manual_seed(0)<<NEWL>>    task = FakeTask(args)<<NEWL>>    return transformer.TransformerModel.build_model(args, task)<<NEWL>><<NEWL>><<NEWL>>class TransformerTestCase(unittest.TestCase):<<NEWL>>    def test_forward_backward(self):<<NEWL>>        model = mk_transformer(encoder_embed_dim=12, decoder_embed_dim=12)<<NEWL>>        sample = mk_sample()<<NEWL>>        o, _ = model.forward(**sample[""net_input""])<<NEWL>>        loss = o.sum()<<NEWL>>        loss.backward()<<NEWL>><<NEWL>>    def test_different_encoder_decoder_embed_dim(self):<<NEWL>>        model = mk_transformer(encoder_embed_dim=12, decoder_embed_dim=16)<<NEWL>>        sample = mk_sample()<<NEWL>>        o, _ = model.forward(**sample[""net_input""])<<NEWL>>        loss = o.sum()<<NEWL>>        loss.backward()"
466	donghui	1	"import argparse<<NEWL>>import unittest<<NEWL>>from typing import Any, Dict, Sequence<<NEWL>><<NEWL>>import torch<<NEWL>>from fairseq.models import transformer<<NEWL>><<NEWL>>from tests.test_roberta import FakeTask<<NEWL>><<NEWL>><<NEWL>>def mk_sample(tok: Sequence[int] = None, batch_size: int = 2) -> Dict[str, Any]:<<NEWL>>    if not tok:<<NEWL>>        tok = [10, 11, 12, 13, 14, 15, 2]<<NEWL>><<NEWL>>    batch = torch.stack([torch.tensor(tok, dtype=torch.long)] * batch_size)<<NEWL>>    sample = {<<NEWL>>        ""net_input"": {<<NEWL>>            ""src_tokens"": batch,<<NEWL>>            ""prev_output_tokens"": batch,<<NEWL>>            ""src_lengths"": torch.tensor(<<NEWL>>                [len(tok)] * batch_size, dtype=torch.long, device=batch.device<<NEWL>>            ),<<NEWL>>        },<<NEWL>>        ""target"": batch[:, 1:],<<NEWL>>    }<<NEWL>>    return sample<<NEWL>><<NEWL>><<NEWL>>def mk_transformer(**extra_args: Any):<<NEWL>>    overrides = {<<NEWL>>        # Use characteristics dimensions<<NEWL>>        ""encoder_embed_dim"": 12,<<NEWL>>        ""encoder_ffn_embed_dim"": 14,<<NEWL>>        ""decoder_embed_dim"": 12,<<NEWL>>        ""decoder_ffn_embed_dim"": 14,<<NEWL>>        # Disable dropout so we have comparable tests.<<NEWL>>        ""dropout"": 0,<<NEWL>>        ""attention_dropout"": 0,<<NEWL>>        ""activation_dropout"": 0,<<NEWL>>        ""encoder_layerdrop"": 0,<<NEWL>>    }<<NEWL>>    overrides.update(extra_args)<<NEWL>>    # Overrides the defaults from the parser<<NEWL>>    args = argparse.Namespace(**overrides)<<NEWL>>    transformer.tiny_architecture(args)<<NEWL>><<NEWL>>    torch.manual_seed(0)<<NEWL>>    task = FakeTask(args)<<NEWL>>    return transformer.TransformerModel.build_model(args, task)<<NEWL>><<NEWL>><<NEWL>>class TransformerTestCase(unittest.TestCase):<<NEWL>>    def test_forward_backward(self):<<NEWL>>        model = mk_transformer(encoder_embed_dim=12, decoder_embed_dim=12)<<NEWL>>        sample = mk_sample()<<NEWL>>        o, _ = model.forward(**sample[""net_input""])<<NEWL>>        loss = o.sum()<<NEWL>>        loss.backward()<<NEWL>><<NEWL>>    def test_different_encoder_decoder_embed_dim(self):<<NEWL>>        model = mk_transformer(encoder_embed_dim=12, decoder_embed_dim=16)<<NEWL>>        sample = mk_sample()<<NEWL>>        o, _ = model.forward(**sample[""net_input""])<<NEWL>>        loss = o.sum()<<NEWL>>        loss.backward()"
452	jackson	3	"import os<<NEWL>>import string<<NEWL>>import urllib.parse<<NEWL>>import urllib.request<<NEWL>>from typing import Optional<<NEWL>><<NEWL>>from .compat import WINDOWS<<NEWL>><<NEWL>><<NEWL>>def get_url_scheme(url):<<NEWL>>    # type: (str) -> Optional[str]<<NEWL>>    if "":"" not in url:<<NEWL>>        return None<<NEWL>>    return url.split("":"", 1)[0].lower()<<NEWL>><<NEWL>><<NEWL>>def path_to_url(path):<<NEWL>>    # type: (str) -> str<<NEWL>>    """"""<<NEWL>>    Convert a path to a file: URL.  The path will be made absolute and have<<NEWL>>    quoted path parts.<<NEWL>>    """"""<<NEWL>>    path = os.path.normpath(os.path.abspath(path))<<NEWL>>    url = urllib.parse.urljoin(""file:"", urllib.request.pathname2url(path))<<NEWL>>    return url<<NEWL>><<NEWL>><<NEWL>>def url_to_path(url):<<NEWL>>    # type: (str) -> str<<NEWL>>    """"""<<NEWL>>    Convert a file: URL to a path.<<NEWL>>    """"""<<NEWL>>    assert url.startswith(<<NEWL>>        ""file:""<<NEWL>>    ), f""You can only turn file: urls into filenames (not {url!r})""<<NEWL>><<NEWL>>    _, netloc, path, _, _ = urllib.parse.urlsplit(url)<<NEWL>><<NEWL>>    if not netloc or netloc == ""localhost"":<<NEWL>>        # According to RFC 8089, same as empty authority.<<NEWL>>        netloc = """"<<NEWL>>    elif WINDOWS:<<NEWL>>        # If we have a UNC path, prepend UNC share notation.<<NEWL>>        netloc = ""\\\\"" + netloc<<NEWL>>    else:<<NEWL>>        raise ValueError(<<NEWL>>            f""non-local file URIs are not supported on this platform: {url!r}""<<NEWL>>        )<<NEWL>><<NEWL>>    path = urllib.request.url2pathname(netloc + path)<<NEWL>><<NEWL>>    # On Windows, urlsplit parses the path as something like ""/C:/Users/foo"".<<NEWL>>    # This creates issues for path-related functions like io.open(), so we try<<NEWL>>    # to detect and strip the leading slash.<<NEWL>>    if (<<NEWL>>        WINDOWS<<NEWL>>        and not netloc  # Not UNC.<<NEWL>>        and len(path) >= 3<<NEWL>>        and path[0] == ""/""  # Leading slash to strip.<<NEWL>>        and path[1] in string.ascii_letters  # Drive letter.<<NEWL>>        and path[2:4] in ("":"", "":/"")  # Colon + end of string, or colon + absolute path.<<NEWL>>    ):<<NEWL>>        path = path[1:]<<NEWL>><<NEWL>>    return path"
452	donghui	3	"import os<<NEWL>>import string<<NEWL>>import urllib.parse<<NEWL>>import urllib.request<<NEWL>>from typing import Optional<<NEWL>><<NEWL>>from .compat import WINDOWS<<NEWL>><<NEWL>><<NEWL>>def get_url_scheme(url):<<NEWL>>    # type: (str) -> Optional[str]<<NEWL>>    if "":"" not in url:<<NEWL>>        return None<<NEWL>>    return url.split("":"", 1)[0].lower()<<NEWL>><<NEWL>><<NEWL>>def path_to_url(path):<<NEWL>>    # type: (str) -> str<<NEWL>>    """"""<<NEWL>>    Convert a path to a file: URL.  The path will be made absolute and have<<NEWL>>    quoted path parts.<<NEWL>>    """"""<<NEWL>>    path = os.path.normpath(os.path.abspath(path))<<NEWL>>    url = urllib.parse.urljoin(""file:"", urllib.request.pathname2url(path))<<NEWL>>    return url<<NEWL>><<NEWL>><<NEWL>>def url_to_path(url):<<NEWL>>    # type: (str) -> str<<NEWL>>    """"""<<NEWL>>    Convert a file: URL to a path.<<NEWL>>    """"""<<NEWL>>    assert url.startswith(<<NEWL>>        ""file:""<<NEWL>>    ), f""You can only turn file: urls into filenames (not {url!r})""<<NEWL>><<NEWL>>    _, netloc, path, _, _ = urllib.parse.urlsplit(url)<<NEWL>><<NEWL>>    if not netloc or netloc == ""localhost"":<<NEWL>>        # According to RFC 8089, same as empty authority.<<NEWL>>        netloc = """"<<NEWL>>    elif WINDOWS:<<NEWL>>        # If we have a UNC path, prepend UNC share notation.<<NEWL>>        netloc = ""\\\\"" + netloc<<NEWL>>    else:<<NEWL>>        raise ValueError(<<NEWL>>            f""non-local file URIs are not supported on this platform: {url!r}""<<NEWL>>        )<<NEWL>><<NEWL>>    path = urllib.request.url2pathname(netloc + path)<<NEWL>><<NEWL>>    # On Windows, urlsplit parses the path as something like ""/C:/Users/foo"".<<NEWL>>    # This creates issues for path-related functions like io.open(), so we try<<NEWL>>    # to detect and strip the leading slash.<<NEWL>>    if (<<NEWL>>        WINDOWS<<NEWL>>        and not netloc  # Not UNC.<<NEWL>>        and len(path) >= 3<<NEWL>>        and path[0] == ""/""  # Leading slash to strip.<<NEWL>>        and path[1] in string.ascii_letters  # Drive letter.<<NEWL>>        and path[2:4] in ("":"", "":/"")  # Colon + end of string, or colon + absolute path.<<NEWL>>    ):<<NEWL>>        path = path[1:]<<NEWL>><<NEWL>>    return path"
512	jackson	0	"# -*- coding: utf-8 -*-<<NEWL>><<NEWL>># Copyright 2010 Dirk Holtwick, holtwick.it<<NEWL>>#<<NEWL>># Licensed under the Apache License, Version 2.0 (the ""License"");<<NEWL>># you may not use this file except in compliance with the License.<<NEWL>># You may obtain a copy of the License at<<NEWL>>#<<NEWL>>#     http://www.apache.org/licenses/LICENSE-2.0<<NEWL>>#<<NEWL>># Unless required by applicable law or agreed to in writing, software<<NEWL>># distributed under the License is distributed on an ""AS IS"" BASIS,<<NEWL>># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.<<NEWL>># See the License for the specific language governing permissions and<<NEWL>># limitations under the License.<<NEWL>><<NEWL>>import six<<NEWL>>import logging<<NEWL>><<NEWL>><<NEWL>>from xhtml2pdf.util import pisaTempFile, getFile, PyPDF2<<NEWL>><<NEWL>><<NEWL>>log = logging.getLogger(""xhtml2pdf"")<<NEWL>><<NEWL>><<NEWL>>class pisaPDF:<<NEWL>>    def __init__(self, capacity=-1):<<NEWL>>        self.capacity = capacity<<NEWL>>        self.files = []<<NEWL>><<NEWL>>    def addFromURI(self, url, basepath=None):<<NEWL>>        obj = getFile(url, basepath)<<NEWL>>        if obj and (not obj.notFound()):<<NEWL>>            self.files.append(obj.getFile())<<NEWL>><<NEWL>>    addFromFileName = addFromURI<<NEWL>><<NEWL>>    def addFromFile(self, f):<<NEWL>>        if hasattr(f, ""read""):<<NEWL>>            self.files.append(f)<<NEWL>>        else:<<NEWL>>            self.addFromURI(f)<<NEWL>><<NEWL>>    def addFromString(self, data):<<NEWL>>        self.files.append(pisaTempFile(data, capacity=self.capacity))<<NEWL>><<NEWL>>    def addDocument(self, doc):<<NEWL>>        if hasattr(doc.dest, ""read""):<<NEWL>>            self.files.append(doc.dest)<<NEWL>><<NEWL>>    def join(self, file=None):<<NEWL>>        output = PyPDF2.PdfFileWriter()<<NEWL>>        for pdffile in self.files:<<NEWL>>            input = PyPDF2.PdfFileReader(pdffile)<<NEWL>>            for pageNumber in six.moves.range(input.getNumPages()):<<NEWL>>                output.addPage(input.getPage(pageNumber))<<NEWL>><<NEWL>>        if file is not None:<<NEWL>>            output.write(file)<<NEWL>>            return file<<NEWL>>        out = pisaTempFile(capacity=self.capacity)<<NEWL>>        output.write(out)<<NEWL>>        return out.getvalue()<<NEWL>><<NEWL>>    getvalue = join<<NEWL>>    __str__ = join"
512	donghui	0	"# -*- coding: utf-8 -*-<<NEWL>><<NEWL>># Copyright 2010 Dirk Holtwick, holtwick.it<<NEWL>>#<<NEWL>># Licensed under the Apache License, Version 2.0 (the ""License"");<<NEWL>># you may not use this file except in compliance with the License.<<NEWL>># You may obtain a copy of the License at<<NEWL>>#<<NEWL>>#     http://www.apache.org/licenses/LICENSE-2.0<<NEWL>>#<<NEWL>># Unless required by applicable law or agreed to in writing, software<<NEWL>># distributed under the License is distributed on an ""AS IS"" BASIS,<<NEWL>># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.<<NEWL>># See the License for the specific language governing permissions and<<NEWL>># limitations under the License.<<NEWL>><<NEWL>>import six<<NEWL>>import logging<<NEWL>><<NEWL>><<NEWL>>from xhtml2pdf.util import pisaTempFile, getFile, PyPDF2<<NEWL>><<NEWL>><<NEWL>>log = logging.getLogger(""xhtml2pdf"")<<NEWL>><<NEWL>><<NEWL>>class pisaPDF:<<NEWL>>    def __init__(self, capacity=-1):<<NEWL>>        self.capacity = capacity<<NEWL>>        self.files = []<<NEWL>><<NEWL>>    def addFromURI(self, url, basepath=None):<<NEWL>>        obj = getFile(url, basepath)<<NEWL>>        if obj and (not obj.notFound()):<<NEWL>>            self.files.append(obj.getFile())<<NEWL>><<NEWL>>    addFromFileName = addFromURI<<NEWL>><<NEWL>>    def addFromFile(self, f):<<NEWL>>        if hasattr(f, ""read""):<<NEWL>>            self.files.append(f)<<NEWL>>        else:<<NEWL>>            self.addFromURI(f)<<NEWL>><<NEWL>>    def addFromString(self, data):<<NEWL>>        self.files.append(pisaTempFile(data, capacity=self.capacity))<<NEWL>><<NEWL>>    def addDocument(self, doc):<<NEWL>>        if hasattr(doc.dest, ""read""):<<NEWL>>            self.files.append(doc.dest)<<NEWL>><<NEWL>>    def join(self, file=None):<<NEWL>>        output = PyPDF2.PdfFileWriter()<<NEWL>>        for pdffile in self.files:<<NEWL>>            input = PyPDF2.PdfFileReader(pdffile)<<NEWL>>            for pageNumber in six.moves.range(input.getNumPages()):<<NEWL>>                output.addPage(input.getPage(pageNumber))<<NEWL>><<NEWL>>        if file is not None:<<NEWL>>            output.write(file)<<NEWL>>            return file<<NEWL>>        out = pisaTempFile(capacity=self.capacity)<<NEWL>>        output.write(out)<<NEWL>>        return out.getvalue()<<NEWL>><<NEWL>>    getvalue = join<<NEWL>>    __str__ = join"
403	jackson	2	"import _plotly_utils.basevalidators<<NEWL>><<NEWL>><<NEWL>>class HoverlabelValidator(_plotly_utils.basevalidators.CompoundValidator):<<NEWL>>    def __init__(self, plotly_name=""hoverlabel"", parent_name=""box"", **kwargs):<<NEWL>>        super(HoverlabelValidator, self).__init__(<<NEWL>>            plotly_name=plotly_name,<<NEWL>>            parent_name=parent_name,<<NEWL>>            data_class_str=kwargs.pop(""data_class_str"", ""Hoverlabel""),<<NEWL>>            data_docs=kwargs.pop(<<NEWL>>                ""data_docs"",<<NEWL>>                """"""<<NEWL>>            align<<NEWL>>                Sets the horizontal alignment of the text<<NEWL>>                content within hover label box. Has an effect<<NEWL>>                only if the hover label text spans more two or<<NEWL>>                more lines<<NEWL>>            alignsrc<<NEWL>>                Sets the source reference on Chart Studio Cloud<<NEWL>>                for `align`.<<NEWL>>            bgcolor<<NEWL>>                Sets the background color of the hover labels<<NEWL>>                for this trace<<NEWL>>            bgcolorsrc<<NEWL>>                Sets the source reference on Chart Studio Cloud<<NEWL>>                for `bgcolor`.<<NEWL>>            bordercolor<<NEWL>>                Sets the border color of the hover labels for<<NEWL>>                this trace.<<NEWL>>            bordercolorsrc<<NEWL>>                Sets the source reference on Chart Studio Cloud<<NEWL>>                for `bordercolor`.<<NEWL>>            font<<NEWL>>                Sets the font used in hover labels.<<NEWL>>            namelength<<NEWL>>                Sets the default length (in number of<<NEWL>>                characters) of the trace name in the hover<<NEWL>>                labels for all traces. -1 shows the whole name<<NEWL>>                regardless of length. 0-3 shows the first 0-3<<NEWL>>                characters, and an integer >3 will show the<<NEWL>>                whole name if it is less than that many<<NEWL>>                characters, but if it is longer, will truncate<<NEWL>>                to `namelength - 3` characters and add an<<NEWL>>                ellipsis.<<NEWL>>            namelengthsrc<<NEWL>>                Sets the source reference on Chart Studio Cloud<<NEWL>>                for `namelength`.<<NEWL>>"""""",<<NEWL>>            ),<<NEWL>>            **kwargs,<<NEWL>>        )"
403	donghui	1	"import _plotly_utils.basevalidators<<NEWL>><<NEWL>><<NEWL>>class HoverlabelValidator(_plotly_utils.basevalidators.CompoundValidator):<<NEWL>>    def __init__(self, plotly_name=""hoverlabel"", parent_name=""box"", **kwargs):<<NEWL>>        super(HoverlabelValidator, self).__init__(<<NEWL>>            plotly_name=plotly_name,<<NEWL>>            parent_name=parent_name,<<NEWL>>            data_class_str=kwargs.pop(""data_class_str"", ""Hoverlabel""),<<NEWL>>            data_docs=kwargs.pop(<<NEWL>>                ""data_docs"",<<NEWL>>                """"""<<NEWL>>            align<<NEWL>>                Sets the horizontal alignment of the text<<NEWL>>                content within hover label box. Has an effect<<NEWL>>                only if the hover label text spans more two or<<NEWL>>                more lines<<NEWL>>            alignsrc<<NEWL>>                Sets the source reference on Chart Studio Cloud<<NEWL>>                for `align`.<<NEWL>>            bgcolor<<NEWL>>                Sets the background color of the hover labels<<NEWL>>                for this trace<<NEWL>>            bgcolorsrc<<NEWL>>                Sets the source reference on Chart Studio Cloud<<NEWL>>                for `bgcolor`.<<NEWL>>            bordercolor<<NEWL>>                Sets the border color of the hover labels for<<NEWL>>                this trace.<<NEWL>>            bordercolorsrc<<NEWL>>                Sets the source reference on Chart Studio Cloud<<NEWL>>                for `bordercolor`.<<NEWL>>            font<<NEWL>>                Sets the font used in hover labels.<<NEWL>>            namelength<<NEWL>>                Sets the default length (in number of<<NEWL>>                characters) of the trace name in the hover<<NEWL>>                labels for all traces. -1 shows the whole name<<NEWL>>                regardless of length. 0-3 shows the first 0-3<<NEWL>>                characters, and an integer >3 will show the<<NEWL>>                whole name if it is less than that many<<NEWL>>                characters, but if it is longer, will truncate<<NEWL>>                to `namelength - 3` characters and add an<<NEWL>>                ellipsis.<<NEWL>>            namelengthsrc<<NEWL>>                Sets the source reference on Chart Studio Cloud<<NEWL>>                for `namelength`.<<NEWL>>"""""",<<NEWL>>            ),<<NEWL>>            **kwargs,<<NEWL>>        )"
280	jackson	3	"from django.urls import get_script_prefix, resolve<<NEWL>><<NEWL>><<NEWL>>def get_breadcrumbs(url, request=None):<<NEWL>>    """"""<<NEWL>>    Given a url returns a list of breadcrumbs, which are each a<<NEWL>>    tuple of (name, url).<<NEWL>>    """"""<<NEWL>>    from rest_framework.reverse import preserve_builtin_query_params<<NEWL>>    from rest_framework.views import APIView<<NEWL>><<NEWL>>    def breadcrumbs_recursive(url, breadcrumbs_list, prefix, seen):<<NEWL>>        """"""<<NEWL>>        Add tuples of (name, url) to the breadcrumbs list,<<NEWL>>        progressively chomping off parts of the url.<<NEWL>>        """"""<<NEWL>>        try:<<NEWL>>            (view, unused_args, unused_kwargs) = resolve(url)<<NEWL>>        except Exception:<<NEWL>>            pass<<NEWL>>        else:<<NEWL>>            # Check if this is a REST framework view,<<NEWL>>            # and if so add it to the breadcrumbs<<NEWL>>            cls = getattr(view, ""cls"", None)<<NEWL>>            initkwargs = getattr(view, ""initkwargs"", {})<<NEWL>>            if cls is not None and issubclass(cls, APIView):<<NEWL>>                # Don't list the same view twice in a row.<<NEWL>>                # Probably an optional trailing slash.<<NEWL>>                if not seen or seen[-1] != view:<<NEWL>>                    c = cls(**initkwargs)<<NEWL>>                    name = c.get_view_name()<<NEWL>>                    insert_url = preserve_builtin_query_params(prefix + url, request)<<NEWL>>                    breadcrumbs_list.insert(0, (name, insert_url))<<NEWL>>                    seen.append(view)<<NEWL>><<NEWL>>        if url == """":<<NEWL>>            # All done<<NEWL>>            return breadcrumbs_list<<NEWL>><<NEWL>>        elif url.endswith(""/""):<<NEWL>>            # Drop trailing slash off the end and continue to try to<<NEWL>>            # resolve more breadcrumbs<<NEWL>>            url = url.rstrip(""/"")<<NEWL>>            return breadcrumbs_recursive(url, breadcrumbs_list, prefix, seen)<<NEWL>><<NEWL>>        # Drop trailing non-slash off the end and continue to try to<<NEWL>>        # resolve more breadcrumbs<<NEWL>>        url = url[: url.rfind(""/"") + 1]<<NEWL>>        return breadcrumbs_recursive(url, breadcrumbs_list, prefix, seen)<<NEWL>><<NEWL>>    prefix = get_script_prefix().rstrip(""/"")<<NEWL>>    url = url[len(prefix) :]<<NEWL>>    return breadcrumbs_recursive(url, [], prefix, [])"
280	donghui	3	"from django.urls import get_script_prefix, resolve<<NEWL>><<NEWL>><<NEWL>>def get_breadcrumbs(url, request=None):<<NEWL>>    """"""<<NEWL>>    Given a url returns a list of breadcrumbs, which are each a<<NEWL>>    tuple of (name, url).<<NEWL>>    """"""<<NEWL>>    from rest_framework.reverse import preserve_builtin_query_params<<NEWL>>    from rest_framework.views import APIView<<NEWL>><<NEWL>>    def breadcrumbs_recursive(url, breadcrumbs_list, prefix, seen):<<NEWL>>        """"""<<NEWL>>        Add tuples of (name, url) to the breadcrumbs list,<<NEWL>>        progressively chomping off parts of the url.<<NEWL>>        """"""<<NEWL>>        try:<<NEWL>>            (view, unused_args, unused_kwargs) = resolve(url)<<NEWL>>        except Exception:<<NEWL>>            pass<<NEWL>>        else:<<NEWL>>            # Check if this is a REST framework view,<<NEWL>>            # and if so add it to the breadcrumbs<<NEWL>>            cls = getattr(view, ""cls"", None)<<NEWL>>            initkwargs = getattr(view, ""initkwargs"", {})<<NEWL>>            if cls is not None and issubclass(cls, APIView):<<NEWL>>                # Don't list the same view twice in a row.<<NEWL>>                # Probably an optional trailing slash.<<NEWL>>                if not seen or seen[-1] != view:<<NEWL>>                    c = cls(**initkwargs)<<NEWL>>                    name = c.get_view_name()<<NEWL>>                    insert_url = preserve_builtin_query_params(prefix + url, request)<<NEWL>>                    breadcrumbs_list.insert(0, (name, insert_url))<<NEWL>>                    seen.append(view)<<NEWL>><<NEWL>>        if url == """":<<NEWL>>            # All done<<NEWL>>            return breadcrumbs_list<<NEWL>><<NEWL>>        elif url.endswith(""/""):<<NEWL>>            # Drop trailing slash off the end and continue to try to<<NEWL>>            # resolve more breadcrumbs<<NEWL>>            url = url.rstrip(""/"")<<NEWL>>            return breadcrumbs_recursive(url, breadcrumbs_list, prefix, seen)<<NEWL>><<NEWL>>        # Drop trailing non-slash off the end and continue to try to<<NEWL>>        # resolve more breadcrumbs<<NEWL>>        url = url[: url.rfind(""/"") + 1]<<NEWL>>        return breadcrumbs_recursive(url, breadcrumbs_list, prefix, seen)<<NEWL>><<NEWL>>    prefix = get_script_prefix().rstrip(""/"")<<NEWL>>    url = url[len(prefix) :]<<NEWL>>    return breadcrumbs_recursive(url, [], prefix, [])"
391	jackson	1	"#<<NEWL>># The Python Imaging Library.<<NEWL>># $Id$<<NEWL>>#<<NEWL>># XV Thumbnail file handler by Charles E. ""Gene"" Cash<<NEWL>># (gcash@magicnet.net)<<NEWL>>#<<NEWL>># see xvcolor.c and xvbrowse.c in the sources to John Bradley's XV,<<NEWL>># available from ftp://ftp.cis.upenn.edu/pub/xv/<<NEWL>>#<<NEWL>># history:<<NEWL>># 98-08-15 cec  created (b/w only)<<NEWL>># 98-12-09 cec  added color palette<<NEWL>># 98-12-28 fl   added to PIL (with only a few very minor modifications)<<NEWL>>#<<NEWL>># To do:<<NEWL>># FIXME: make save work (this requires quantization support)<<NEWL>>#<<NEWL>><<NEWL>>from . import Image, ImageFile, ImagePalette<<NEWL>>from ._binary import o8<<NEWL>><<NEWL>>_MAGIC = b""P7 332""<<NEWL>><<NEWL>># standard color palette for thumbnails (RGB332)<<NEWL>>PALETTE = b""""<<NEWL>>for r in range(8):<<NEWL>>    for g in range(8):<<NEWL>>        for b in range(4):<<NEWL>>            PALETTE = PALETTE + (<<NEWL>>                o8((r * 255) // 7) + o8((g * 255) // 7) + o8((b * 255) // 3)<<NEWL>>            )<<NEWL>><<NEWL>><<NEWL>>def _accept(prefix):<<NEWL>>    return prefix[:6] == _MAGIC<<NEWL>><<NEWL>><<NEWL>>##<<NEWL>># Image plugin for XV thumbnail images.<<NEWL>><<NEWL>><<NEWL>>class XVThumbImageFile(ImageFile.ImageFile):<<NEWL>><<NEWL>>    format = ""XVThumb""<<NEWL>>    format_description = ""XV thumbnail image""<<NEWL>><<NEWL>>    def _open(self):<<NEWL>><<NEWL>>        # check magic<<NEWL>>        if not _accept(self.fp.read(6)):<<NEWL>>            msg = ""not an XV thumbnail file""<<NEWL>>            raise SyntaxError(msg)<<NEWL>><<NEWL>>        # Skip to beginning of next line<<NEWL>>        self.fp.readline()<<NEWL>><<NEWL>>        # skip info comments<<NEWL>>        while True:<<NEWL>>            s = self.fp.readline()<<NEWL>>            if not s:<<NEWL>>                msg = ""Unexpected EOF reading XV thumbnail file""<<NEWL>>                raise SyntaxError(msg)<<NEWL>>            if s[0] != 35:  # ie. when not a comment: '#'<<NEWL>>                break<<NEWL>><<NEWL>>        # parse header line (already read)<<NEWL>>        s = s.strip().split()<<NEWL>><<NEWL>>        self.mode = ""P""<<NEWL>>        self._size = int(s[0]), int(s[1])<<NEWL>><<NEWL>>        self.palette = ImagePalette.raw(""RGB"", PALETTE)<<NEWL>><<NEWL>>        self.tile = [(""raw"", (0, 0) + self.size, self.fp.tell(), (self.mode, 0, 1))]<<NEWL>><<NEWL>><<NEWL>># --------------------------------------------------------------------<<NEWL>><<NEWL>>Image.register_open(XVThumbImageFile.format, XVThumbImageFile, _accept)"
391	donghui	1	"#<<NEWL>># The Python Imaging Library.<<NEWL>># $Id$<<NEWL>>#<<NEWL>># XV Thumbnail file handler by Charles E. ""Gene"" Cash<<NEWL>># (gcash@magicnet.net)<<NEWL>>#<<NEWL>># see xvcolor.c and xvbrowse.c in the sources to John Bradley's XV,<<NEWL>># available from ftp://ftp.cis.upenn.edu/pub/xv/<<NEWL>>#<<NEWL>># history:<<NEWL>># 98-08-15 cec  created (b/w only)<<NEWL>># 98-12-09 cec  added color palette<<NEWL>># 98-12-28 fl   added to PIL (with only a few very minor modifications)<<NEWL>>#<<NEWL>># To do:<<NEWL>># FIXME: make save work (this requires quantization support)<<NEWL>>#<<NEWL>><<NEWL>>from . import Image, ImageFile, ImagePalette<<NEWL>>from ._binary import o8<<NEWL>><<NEWL>>_MAGIC = b""P7 332""<<NEWL>><<NEWL>># standard color palette for thumbnails (RGB332)<<NEWL>>PALETTE = b""""<<NEWL>>for r in range(8):<<NEWL>>    for g in range(8):<<NEWL>>        for b in range(4):<<NEWL>>            PALETTE = PALETTE + (<<NEWL>>                o8((r * 255) // 7) + o8((g * 255) // 7) + o8((b * 255) // 3)<<NEWL>>            )<<NEWL>><<NEWL>><<NEWL>>def _accept(prefix):<<NEWL>>    return prefix[:6] == _MAGIC<<NEWL>><<NEWL>><<NEWL>>##<<NEWL>># Image plugin for XV thumbnail images.<<NEWL>><<NEWL>><<NEWL>>class XVThumbImageFile(ImageFile.ImageFile):<<NEWL>><<NEWL>>    format = ""XVThumb""<<NEWL>>    format_description = ""XV thumbnail image""<<NEWL>><<NEWL>>    def _open(self):<<NEWL>><<NEWL>>        # check magic<<NEWL>>        if not _accept(self.fp.read(6)):<<NEWL>>            msg = ""not an XV thumbnail file""<<NEWL>>            raise SyntaxError(msg)<<NEWL>><<NEWL>>        # Skip to beginning of next line<<NEWL>>        self.fp.readline()<<NEWL>><<NEWL>>        # skip info comments<<NEWL>>        while True:<<NEWL>>            s = self.fp.readline()<<NEWL>>            if not s:<<NEWL>>                msg = ""Unexpected EOF reading XV thumbnail file""<<NEWL>>                raise SyntaxError(msg)<<NEWL>>            if s[0] != 35:  # ie. when not a comment: '#'<<NEWL>>                break<<NEWL>><<NEWL>>        # parse header line (already read)<<NEWL>>        s = s.strip().split()<<NEWL>><<NEWL>>        self.mode = ""P""<<NEWL>>        self._size = int(s[0]), int(s[1])<<NEWL>><<NEWL>>        self.palette = ImagePalette.raw(""RGB"", PALETTE)<<NEWL>><<NEWL>>        self.tile = [(""raw"", (0, 0) + self.size, self.fp.tell(), (self.mode, 0, 1))]<<NEWL>><<NEWL>><<NEWL>># --------------------------------------------------------------------<<NEWL>><<NEWL>>Image.register_open(XVThumbImageFile.format, XVThumbImageFile, _accept)"
362	jackson	3	"import pytest<<NEWL>>import pytest_asyncio<<NEWL>><<NEWL>>from rtsu_students_bot.rtsu import RTSUApi<<NEWL>><<NEWL>>pytest_plugins = ('pytest_asyncio',)<<NEWL>><<NEWL>>TEST_DATA = {<<NEWL>>    ""login"": ""your login"",<<NEWL>>    ""password"": ""your pass"",<<NEWL>>}<<NEWL>><<NEWL>><<NEWL>>@pytest_asyncio.fixture()<<NEWL>>async def rtsu_client():<<NEWL>>    """"""<<NEWL>>    Initializes client<<NEWL>>    :return: Prepared `RTSUApi` client<<NEWL>>    """"""<<NEWL>><<NEWL>>    async with RTSUApi() as api:<<NEWL>>        yield api<<NEWL>><<NEWL>><<NEWL>>@pytest.mark.asyncio<<NEWL>>async def test_rtsu_login(rtsu_client: RTSUApi):<<NEWL>>    """"""<<NEWL>>    Tests rtsu login<<NEWL>>    :param rtsu_client: A RTSU API client<<NEWL>>    :return:<<NEWL>>    """"""<<NEWL>><<NEWL>>    resp = await rtsu_client.auth(TEST_DATA.get(""login""), TEST_DATA.get(""password""))<<NEWL>><<NEWL>>    assert resp.token is not None<<NEWL>><<NEWL>><<NEWL>>@pytest.mark.asyncio<<NEWL>>async def test_rtsu_profile_fetching(rtsu_client: RTSUApi):<<NEWL>>    """"""<<NEWL>>    Tests rtsu profile fetching<<NEWL>>    :param rtsu_client:<<NEWL>>    :return:<<NEWL>>    """"""<<NEWL>><<NEWL>>    await rtsu_client.auth(TEST_DATA.get(""login""), TEST_DATA.get(""password""))<<NEWL>><<NEWL>>    profile = await rtsu_client.get_profile()<<NEWL>><<NEWL>>    assert profile is not None<<NEWL>>    assert profile.full_name is not None<<NEWL>><<NEWL>><<NEWL>>@pytest.mark.asyncio<<NEWL>>async def test_rtsu_academic_years_fetching(rtsu_client: RTSUApi):<<NEWL>>    """"""<<NEWL>>    Tests rtsu academic years fetching<<NEWL>>    :param rtsu_client:<<NEWL>>    :return:<<NEWL>>    """"""<<NEWL>><<NEWL>>    await rtsu_client.auth(TEST_DATA.get(""login""), TEST_DATA.get(""password""))<<NEWL>><<NEWL>>    years = await rtsu_client.get_academic_years()<<NEWL>><<NEWL>>    assert type(years) == list<<NEWL>>    assert len(years) > 0<<NEWL>><<NEWL>><<NEWL>>@pytest.mark.asyncio<<NEWL>>async def test_rtsu_academic_year_subjects_fetching(rtsu_client: RTSUApi):<<NEWL>>    """"""<<NEWL>>    Tests rtsu academic year fetching<<NEWL>>    :param rtsu_client:<<NEWL>>    :return:<<NEWL>>    """"""<<NEWL>><<NEWL>>    await rtsu_client.auth(TEST_DATA.get(""login""), TEST_DATA.get(""password""))<<NEWL>><<NEWL>>    ac_years = await rtsu_client.get_academic_years()<<NEWL>>    year = ac_years[0].id<<NEWL>>    years = await rtsu_client.get_academic_year_subjects(year)<<NEWL>><<NEWL>>    assert type(years) == list<<NEWL>>    assert len(years) > 0"
362	donghui	2	"import pytest<<NEWL>>import pytest_asyncio<<NEWL>><<NEWL>>from rtsu_students_bot.rtsu import RTSUApi<<NEWL>><<NEWL>>pytest_plugins = ('pytest_asyncio',)<<NEWL>><<NEWL>>TEST_DATA = {<<NEWL>>    ""login"": ""your login"",<<NEWL>>    ""password"": ""your pass"",<<NEWL>>}<<NEWL>><<NEWL>><<NEWL>>@pytest_asyncio.fixture()<<NEWL>>async def rtsu_client():<<NEWL>>    """"""<<NEWL>>    Initializes client<<NEWL>>    :return: Prepared `RTSUApi` client<<NEWL>>    """"""<<NEWL>><<NEWL>>    async with RTSUApi() as api:<<NEWL>>        yield api<<NEWL>><<NEWL>><<NEWL>>@pytest.mark.asyncio<<NEWL>>async def test_rtsu_login(rtsu_client: RTSUApi):<<NEWL>>    """"""<<NEWL>>    Tests rtsu login<<NEWL>>    :param rtsu_client: A RTSU API client<<NEWL>>    :return:<<NEWL>>    """"""<<NEWL>><<NEWL>>    resp = await rtsu_client.auth(TEST_DATA.get(""login""), TEST_DATA.get(""password""))<<NEWL>><<NEWL>>    assert resp.token is not None<<NEWL>><<NEWL>><<NEWL>>@pytest.mark.asyncio<<NEWL>>async def test_rtsu_profile_fetching(rtsu_client: RTSUApi):<<NEWL>>    """"""<<NEWL>>    Tests rtsu profile fetching<<NEWL>>    :param rtsu_client:<<NEWL>>    :return:<<NEWL>>    """"""<<NEWL>><<NEWL>>    await rtsu_client.auth(TEST_DATA.get(""login""), TEST_DATA.get(""password""))<<NEWL>><<NEWL>>    profile = await rtsu_client.get_profile()<<NEWL>><<NEWL>>    assert profile is not None<<NEWL>>    assert profile.full_name is not None<<NEWL>><<NEWL>><<NEWL>>@pytest.mark.asyncio<<NEWL>>async def test_rtsu_academic_years_fetching(rtsu_client: RTSUApi):<<NEWL>>    """"""<<NEWL>>    Tests rtsu academic years fetching<<NEWL>>    :param rtsu_client:<<NEWL>>    :return:<<NEWL>>    """"""<<NEWL>><<NEWL>>    await rtsu_client.auth(TEST_DATA.get(""login""), TEST_DATA.get(""password""))<<NEWL>><<NEWL>>    years = await rtsu_client.get_academic_years()<<NEWL>><<NEWL>>    assert type(years) == list<<NEWL>>    assert len(years) > 0<<NEWL>><<NEWL>><<NEWL>>@pytest.mark.asyncio<<NEWL>>async def test_rtsu_academic_year_subjects_fetching(rtsu_client: RTSUApi):<<NEWL>>    """"""<<NEWL>>    Tests rtsu academic year fetching<<NEWL>>    :param rtsu_client:<<NEWL>>    :return:<<NEWL>>    """"""<<NEWL>><<NEWL>>    await rtsu_client.auth(TEST_DATA.get(""login""), TEST_DATA.get(""password""))<<NEWL>><<NEWL>>    ac_years = await rtsu_client.get_academic_years()<<NEWL>>    year = ac_years[0].id<<NEWL>>    years = await rtsu_client.get_academic_year_subjects(year)<<NEWL>><<NEWL>>    assert type(years) == list<<NEWL>>    assert len(years) > 0"
333	jackson	0	"# This file is dual licensed under the terms of the Apache License, Version<<NEWL>># 2.0, and the BSD License. See the LICENSE file in the root of this repository<<NEWL>># for complete details.<<NEWL>><<NEWL>><<NEWL>>import typing<<NEWL>><<NEWL>>from cryptography import utils<<NEWL>>from cryptography.exceptions import (<<NEWL>>    AlreadyFinalized,<<NEWL>>    InvalidKey,<<NEWL>>    UnsupportedAlgorithm,<<NEWL>>    _Reasons,<<NEWL>>)<<NEWL>>from cryptography.hazmat.primitives import constant_time, hashes<<NEWL>>from cryptography.hazmat.primitives.kdf import KeyDerivationFunction<<NEWL>><<NEWL>><<NEWL>>class PBKDF2HMAC(KeyDerivationFunction):<<NEWL>>    def __init__(<<NEWL>>        self,<<NEWL>>        algorithm: hashes.HashAlgorithm,<<NEWL>>        length: int,<<NEWL>>        salt: bytes,<<NEWL>>        iterations: int,<<NEWL>>        backend: typing.Any = None,<<NEWL>>    ):<<NEWL>>        from cryptography.hazmat.backends.openssl.backend import (<<NEWL>>            backend as ossl,<<NEWL>>        )<<NEWL>><<NEWL>>        if not ossl.pbkdf2_hmac_supported(algorithm):<<NEWL>>            raise UnsupportedAlgorithm(<<NEWL>>                ""{} is not supported for PBKDF2 by this backend."".format(<<NEWL>>                    algorithm.name<<NEWL>>                ),<<NEWL>>                _Reasons.UNSUPPORTED_HASH,<<NEWL>>            )<<NEWL>>        self._used = False<<NEWL>>        self._algorithm = algorithm<<NEWL>>        self._length = length<<NEWL>>        utils._check_bytes(""salt"", salt)<<NEWL>>        self._salt = salt<<NEWL>>        self._iterations = iterations<<NEWL>><<NEWL>>    def derive(self, key_material: bytes) -> bytes:<<NEWL>>        if self._used:<<NEWL>>            raise AlreadyFinalized(""PBKDF2 instances can only be used once."")<<NEWL>>        self._used = True<<NEWL>><<NEWL>>        utils._check_byteslike(""key_material"", key_material)<<NEWL>>        from cryptography.hazmat.backends.openssl.backend import backend<<NEWL>><<NEWL>>        return backend.derive_pbkdf2_hmac(<<NEWL>>            self._algorithm,<<NEWL>>            self._length,<<NEWL>>            self._salt,<<NEWL>>            self._iterations,<<NEWL>>            key_material,<<NEWL>>        )<<NEWL>><<NEWL>>    def verify(self, key_material: bytes, expected_key: bytes) -> None:<<NEWL>>        derived_key = self.derive(key_material)<<NEWL>>        if not constant_time.bytes_eq(derived_key, expected_key):<<NEWL>>            raise InvalidKey(""Keys do not match."")"
333	donghui	0	"# This file is dual licensed under the terms of the Apache License, Version<<NEWL>># 2.0, and the BSD License. See the LICENSE file in the root of this repository<<NEWL>># for complete details.<<NEWL>><<NEWL>><<NEWL>>import typing<<NEWL>><<NEWL>>from cryptography import utils<<NEWL>>from cryptography.exceptions import (<<NEWL>>    AlreadyFinalized,<<NEWL>>    InvalidKey,<<NEWL>>    UnsupportedAlgorithm,<<NEWL>>    _Reasons,<<NEWL>>)<<NEWL>>from cryptography.hazmat.primitives import constant_time, hashes<<NEWL>>from cryptography.hazmat.primitives.kdf import KeyDerivationFunction<<NEWL>><<NEWL>><<NEWL>>class PBKDF2HMAC(KeyDerivationFunction):<<NEWL>>    def __init__(<<NEWL>>        self,<<NEWL>>        algorithm: hashes.HashAlgorithm,<<NEWL>>        length: int,<<NEWL>>        salt: bytes,<<NEWL>>        iterations: int,<<NEWL>>        backend: typing.Any = None,<<NEWL>>    ):<<NEWL>>        from cryptography.hazmat.backends.openssl.backend import (<<NEWL>>            backend as ossl,<<NEWL>>        )<<NEWL>><<NEWL>>        if not ossl.pbkdf2_hmac_supported(algorithm):<<NEWL>>            raise UnsupportedAlgorithm(<<NEWL>>                ""{} is not supported for PBKDF2 by this backend."".format(<<NEWL>>                    algorithm.name<<NEWL>>                ),<<NEWL>>                _Reasons.UNSUPPORTED_HASH,<<NEWL>>            )<<NEWL>>        self._used = False<<NEWL>>        self._algorithm = algorithm<<NEWL>>        self._length = length<<NEWL>>        utils._check_bytes(""salt"", salt)<<NEWL>>        self._salt = salt<<NEWL>>        self._iterations = iterations<<NEWL>><<NEWL>>    def derive(self, key_material: bytes) -> bytes:<<NEWL>>        if self._used:<<NEWL>>            raise AlreadyFinalized(""PBKDF2 instances can only be used once."")<<NEWL>>        self._used = True<<NEWL>><<NEWL>>        utils._check_byteslike(""key_material"", key_material)<<NEWL>>        from cryptography.hazmat.backends.openssl.backend import backend<<NEWL>><<NEWL>>        return backend.derive_pbkdf2_hmac(<<NEWL>>            self._algorithm,<<NEWL>>            self._length,<<NEWL>>            self._salt,<<NEWL>>            self._iterations,<<NEWL>>            key_material,<<NEWL>>        )<<NEWL>><<NEWL>>    def verify(self, key_material: bytes, expected_key: bytes) -> None:<<NEWL>>        derived_key = self.derive(key_material)<<NEWL>>        if not constant_time.bytes_eq(derived_key, expected_key):<<NEWL>>            raise InvalidKey(""Keys do not match."")"
273	jackson	0	"# Copyright (C) 2017-2023 The Sipwise Team - http://sipwise.com<<NEWL>>#<<NEWL>># This program is free software: you can redistribute it and/or modify it<<NEWL>># under the terms of the GNU General Public License as published by the Free<<NEWL>># Software Foundation, either version 3 of the License, or (at your option)<<NEWL>># any later version.<<NEWL>>#<<NEWL>># This program is distributed in the hope that it will be useful, but WITHOUT<<NEWL>># ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or<<NEWL>># FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for<<NEWL>># more details.<<NEWL>>#<<NEWL>># You should have received a copy of the GNU General Public License along<<NEWL>># with this program.  If not, see <http://www.gnu.org/licenses/>.<<NEWL>>from django.contrib import admin<<NEWL>>from import_export import resources<<NEWL>>from import_export.admin import ExportActionModelAdmin<<NEWL>>from import_export.admin import ImportExportModelAdmin<<NEWL>><<NEWL>>from . import models<<NEWL>><<NEWL>><<NEWL>>class BuildReleaseResource(resources.ModelResource):<<NEWL>>    class Meta:<<NEWL>>        model = models.BuildRelease<<NEWL>><<NEWL>><<NEWL>>@admin.register(models.BuildRelease)<<NEWL>>class BuildReleaseAdmin(ImportExportModelAdmin, ExportActionModelAdmin):<<NEWL>>    resource_class = BuildReleaseResource<<NEWL>>    list_filter = (""release"",)<<NEWL>>    readonly_fields = (<<NEWL>>        ""projects"",<<NEWL>>        ""triggered_projects"",<<NEWL>>        ""built_projects"",<<NEWL>>        ""failed_projects"",<<NEWL>>        ""pool_size"",<<NEWL>>        ""triggered_jobs"",<<NEWL>>        ""build_deps"",<<NEWL>>    )<<NEWL>>    modify_readonly_fields = (<<NEWL>>        ""uuid"",<<NEWL>>        ""release"",<<NEWL>>    ) + readonly_fields<<NEWL>><<NEWL>>    def get_readonly_fields(self, request, obj=None):<<NEWL>>        if obj is None:<<NEWL>>            return self.readonly_fields<<NEWL>>        return self.modify_readonly_fields<<NEWL>><<NEWL>>    def save_model(self, request, obj, form, change):<<NEWL>>        if change:<<NEWL>>            super(BuildReleaseAdmin, self).save_model(<<NEWL>>                request, obj, form, change<<NEWL>>            )<<NEWL>>        else:<<NEWL>>            new_obj = models.BuildRelease.objects.create_build_release(<<NEWL>>                uuid=obj.uuid, release=obj.release<<NEWL>>            )<<NEWL>>            obj.pk = new_obj.pk"
273	donghui	0	"# Copyright (C) 2017-2023 The Sipwise Team - http://sipwise.com<<NEWL>>#<<NEWL>># This program is free software: you can redistribute it and/or modify it<<NEWL>># under the terms of the GNU General Public License as published by the Free<<NEWL>># Software Foundation, either version 3 of the License, or (at your option)<<NEWL>># any later version.<<NEWL>>#<<NEWL>># This program is distributed in the hope that it will be useful, but WITHOUT<<NEWL>># ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or<<NEWL>># FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for<<NEWL>># more details.<<NEWL>>#<<NEWL>># You should have received a copy of the GNU General Public License along<<NEWL>># with this program.  If not, see <http://www.gnu.org/licenses/>.<<NEWL>>from django.contrib import admin<<NEWL>>from import_export import resources<<NEWL>>from import_export.admin import ExportActionModelAdmin<<NEWL>>from import_export.admin import ImportExportModelAdmin<<NEWL>><<NEWL>>from . import models<<NEWL>><<NEWL>><<NEWL>>class BuildReleaseResource(resources.ModelResource):<<NEWL>>    class Meta:<<NEWL>>        model = models.BuildRelease<<NEWL>><<NEWL>><<NEWL>>@admin.register(models.BuildRelease)<<NEWL>>class BuildReleaseAdmin(ImportExportModelAdmin, ExportActionModelAdmin):<<NEWL>>    resource_class = BuildReleaseResource<<NEWL>>    list_filter = (""release"",)<<NEWL>>    readonly_fields = (<<NEWL>>        ""projects"",<<NEWL>>        ""triggered_projects"",<<NEWL>>        ""built_projects"",<<NEWL>>        ""failed_projects"",<<NEWL>>        ""pool_size"",<<NEWL>>        ""triggered_jobs"",<<NEWL>>        ""build_deps"",<<NEWL>>    )<<NEWL>>    modify_readonly_fields = (<<NEWL>>        ""uuid"",<<NEWL>>        ""release"",<<NEWL>>    ) + readonly_fields<<NEWL>><<NEWL>>    def get_readonly_fields(self, request, obj=None):<<NEWL>>        if obj is None:<<NEWL>>            return self.readonly_fields<<NEWL>>        return self.modify_readonly_fields<<NEWL>><<NEWL>>    def save_model(self, request, obj, form, change):<<NEWL>>        if change:<<NEWL>>            super(BuildReleaseAdmin, self).save_model(<<NEWL>>                request, obj, form, change<<NEWL>>            )<<NEWL>>        else:<<NEWL>>            new_obj = models.BuildRelease.objects.create_build_release(<<NEWL>>                uuid=obj.uuid, release=obj.release<<NEWL>>            )<<NEWL>>            obj.pk = new_obj.pk"
307	jackson	0	"CONSOLE_HTML_FORMAT = """"""\<<NEWL>><!DOCTYPE html><<NEWL>><head><<NEWL>><meta charset=""UTF-8""><<NEWL>><style><<NEWL>>{stylesheet}<<NEWL>>body {{<<NEWL>>    color: {foreground};<<NEWL>>    background-color: {background};<<NEWL>>}}<<NEWL>></style><<NEWL>></head><<NEWL>><html><<NEWL>><body><<NEWL>>    <pre style=""font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace""><code>{code}</code></pre><<NEWL>></body><<NEWL>></html><<NEWL>>""""""<<NEWL>><<NEWL>>CONSOLE_SVG_FORMAT = """"""\<<NEWL>><svg class=""rich-terminal"" viewBox=""0 0 {width} {height}"" xmlns=""http://www.w3.org/2000/svg""><<NEWL>>    <!-- Generated with Rich https://www.textualize.io --><<NEWL>>    <style><<NEWL>><<NEWL>>    @font-face {{<<NEWL>>        font-family: ""Fira Code"";<<NEWL>>        src: local(""FiraCode-Regular""),<<NEWL>>                url(""https://cdnjs.cloudflare.com/ajax/libs/firacode/6.2.0/woff2/FiraCode-Regular.woff2"") format(""woff2""),<<NEWL>>                url(""https://cdnjs.cloudflare.com/ajax/libs/firacode/6.2.0/woff/FiraCode-Regular.woff"") format(""woff"");<<NEWL>>        font-style: normal;<<NEWL>>        font-weight: 400;<<NEWL>>    }}<<NEWL>>    @font-face {{<<NEWL>>        font-family: ""Fira Code"";<<NEWL>>        src: local(""FiraCode-Bold""),<<NEWL>>                url(""https://cdnjs.cloudflare.com/ajax/libs/firacode/6.2.0/woff2/FiraCode-Bold.woff2"") format(""woff2""),<<NEWL>>                url(""https://cdnjs.cloudflare.com/ajax/libs/firacode/6.2.0/woff/FiraCode-Bold.woff"") format(""woff"");<<NEWL>>        font-style: bold;<<NEWL>>        font-weight: 700;<<NEWL>>    }}<<NEWL>><<NEWL>>    .{unique_id}-matrix {{<<NEWL>>        font-family: Fira Code, monospace;<<NEWL>>        font-size: {char_height}px;<<NEWL>>        line-height: {line_height}px;<<NEWL>>        font-variant-east-asian: full-width;<<NEWL>>    }}<<NEWL>><<NEWL>>    .{unique_id}-title {{<<NEWL>>        font-size: 18px;<<NEWL>>        font-weight: bold;<<NEWL>>        font-family: arial;<<NEWL>>    }}<<NEWL>><<NEWL>>    {styles}<<NEWL>>    </style><<NEWL>><<NEWL>>    <defs><<NEWL>>    <clipPath id=""{unique_id}-clip-terminal""><<NEWL>>      <rect x=""0"" y=""0"" width=""{terminal_width}"" height=""{terminal_height}"" /><<NEWL>>    </clipPath><<NEWL>>    {lines}<<NEWL>>    </defs><<NEWL>><<NEWL>>    {chrome}<<NEWL>>    <g transform=""translate({terminal_x}, {terminal_y})"" clip-path=""url(#{unique_id}-clip-terminal)""><<NEWL>>    {backgrounds}<<NEWL>>    <g class=""{unique_id}-matrix""><<NEWL>>    {matrix}<<NEWL>>    </g><<NEWL>>    </g><<NEWL>></svg><<NEWL>>""""""<<NEWL>><<NEWL>>_SVG_FONT_FAMILY = ""Rich Fira Code""<<NEWL>>_SVG_CLASSES_PREFIX = ""rich-svg"""
307	donghui	0	"CONSOLE_HTML_FORMAT = """"""\<<NEWL>><!DOCTYPE html><<NEWL>><head><<NEWL>><meta charset=""UTF-8""><<NEWL>><style><<NEWL>>{stylesheet}<<NEWL>>body {{<<NEWL>>    color: {foreground};<<NEWL>>    background-color: {background};<<NEWL>>}}<<NEWL>></style><<NEWL>></head><<NEWL>><html><<NEWL>><body><<NEWL>>    <pre style=""font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace""><code>{code}</code></pre><<NEWL>></body><<NEWL>></html><<NEWL>>""""""<<NEWL>><<NEWL>>CONSOLE_SVG_FORMAT = """"""\<<NEWL>><svg class=""rich-terminal"" viewBox=""0 0 {width} {height}"" xmlns=""http://www.w3.org/2000/svg""><<NEWL>>    <!-- Generated with Rich https://www.textualize.io --><<NEWL>>    <style><<NEWL>><<NEWL>>    @font-face {{<<NEWL>>        font-family: ""Fira Code"";<<NEWL>>        src: local(""FiraCode-Regular""),<<NEWL>>                url(""https://cdnjs.cloudflare.com/ajax/libs/firacode/6.2.0/woff2/FiraCode-Regular.woff2"") format(""woff2""),<<NEWL>>                url(""https://cdnjs.cloudflare.com/ajax/libs/firacode/6.2.0/woff/FiraCode-Regular.woff"") format(""woff"");<<NEWL>>        font-style: normal;<<NEWL>>        font-weight: 400;<<NEWL>>    }}<<NEWL>>    @font-face {{<<NEWL>>        font-family: ""Fira Code"";<<NEWL>>        src: local(""FiraCode-Bold""),<<NEWL>>                url(""https://cdnjs.cloudflare.com/ajax/libs/firacode/6.2.0/woff2/FiraCode-Bold.woff2"") format(""woff2""),<<NEWL>>                url(""https://cdnjs.cloudflare.com/ajax/libs/firacode/6.2.0/woff/FiraCode-Bold.woff"") format(""woff"");<<NEWL>>        font-style: bold;<<NEWL>>        font-weight: 700;<<NEWL>>    }}<<NEWL>><<NEWL>>    .{unique_id}-matrix {{<<NEWL>>        font-family: Fira Code, monospace;<<NEWL>>        font-size: {char_height}px;<<NEWL>>        line-height: {line_height}px;<<NEWL>>        font-variant-east-asian: full-width;<<NEWL>>    }}<<NEWL>><<NEWL>>    .{unique_id}-title {{<<NEWL>>        font-size: 18px;<<NEWL>>        font-weight: bold;<<NEWL>>        font-family: arial;<<NEWL>>    }}<<NEWL>><<NEWL>>    {styles}<<NEWL>>    </style><<NEWL>><<NEWL>>    <defs><<NEWL>>    <clipPath id=""{unique_id}-clip-terminal""><<NEWL>>      <rect x=""0"" y=""0"" width=""{terminal_width}"" height=""{terminal_height}"" /><<NEWL>>    </clipPath><<NEWL>>    {lines}<<NEWL>>    </defs><<NEWL>><<NEWL>>    {chrome}<<NEWL>>    <g transform=""translate({terminal_x}, {terminal_y})"" clip-path=""url(#{unique_id}-clip-terminal)""><<NEWL>>    {backgrounds}<<NEWL>>    <g class=""{unique_id}-matrix""><<NEWL>>    {matrix}<<NEWL>>    </g><<NEWL>>    </g><<NEWL>></svg><<NEWL>>""""""<<NEWL>><<NEWL>>_SVG_FONT_FAMILY = ""Rich Fira Code""<<NEWL>>_SVG_CLASSES_PREFIX = ""rich-svg"""
356	jackson	1	#<<NEWL>># This file is part of pyasn1-modules software.<<NEWL>>#<<NEWL>># Created by Russ Housley.<<NEWL>>#<<NEWL>># Copyright (c) 2019, Vigil Security, LLC<<NEWL>># License: http://snmplabs.com/pyasn1/license.html<<NEWL>>#<<NEWL>># RSAES-OAEP Key Transport Algorithm in CMS<<NEWL>>#<<NEWL>># Notice that all of the things needed in RFC 3560 are also defined<<NEWL>># in RFC 4055.  So, they are all pulled from the RFC 4055 module into<<NEWL>># this one so that people looking a RFC 3560 can easily find them.<<NEWL>>#<<NEWL>># ASN.1 source from:<<NEWL>># https://www.rfc-editor.org/rfc/rfc3560.txt<<NEWL>>#<<NEWL>><<NEWL>>from pyasn1_modules import rfc4055<<NEWL>><<NEWL>>id_sha1 = rfc4055.id_sha1<<NEWL>><<NEWL>>id_sha256 = rfc4055.id_sha256<<NEWL>><<NEWL>>id_sha384 = rfc4055.id_sha384<<NEWL>><<NEWL>>id_sha512 = rfc4055.id_sha512<<NEWL>><<NEWL>>id_mgf1 = rfc4055.id_mgf1<<NEWL>><<NEWL>>rsaEncryption = rfc4055.rsaEncryption<<NEWL>><<NEWL>>id_RSAES_OAEP = rfc4055.id_RSAES_OAEP<<NEWL>><<NEWL>>id_pSpecified = rfc4055.id_pSpecified<<NEWL>><<NEWL>>sha1Identifier = rfc4055.sha1Identifier<<NEWL>><<NEWL>>sha256Identifier = rfc4055.sha256Identifier<<NEWL>><<NEWL>>sha384Identifier = rfc4055.sha384Identifier<<NEWL>><<NEWL>>sha512Identifier = rfc4055.sha512Identifier<<NEWL>><<NEWL>>mgf1SHA1Identifier = rfc4055.mgf1SHA1Identifier<<NEWL>><<NEWL>>mgf1SHA256Identifier = rfc4055.mgf1SHA256Identifier<<NEWL>><<NEWL>>mgf1SHA384Identifier = rfc4055.mgf1SHA384Identifier<<NEWL>><<NEWL>>mgf1SHA512Identifier = rfc4055.mgf1SHA512Identifier<<NEWL>><<NEWL>>pSpecifiedEmptyIdentifier = rfc4055.pSpecifiedEmptyIdentifier<<NEWL>><<NEWL>><<NEWL>>class RSAES_OAEP_params(rfc4055.RSAES_OAEP_params):<<NEWL>>    pass<<NEWL>><<NEWL>><<NEWL>>rSAES_OAEP_Default_Params = RSAES_OAEP_params()<<NEWL>><<NEWL>>rSAES_OAEP_Default_Identifier = rfc4055.rSAES_OAEP_Default_Identifier<<NEWL>><<NEWL>>rSAES_OAEP_SHA256_Params = rfc4055.rSAES_OAEP_SHA256_Params<<NEWL>><<NEWL>>rSAES_OAEP_SHA256_Identifier = rfc4055.rSAES_OAEP_SHA256_Identifier<<NEWL>><<NEWL>>rSAES_OAEP_SHA384_Params = rfc4055.rSAES_OAEP_SHA384_Params<<NEWL>><<NEWL>>rSAES_OAEP_SHA384_Identifier = rfc4055.rSAES_OAEP_SHA384_Identifier<<NEWL>><<NEWL>>rSAES_OAEP_SHA512_Params = rfc4055.rSAES_OAEP_SHA512_Params<<NEWL>><<NEWL>>rSAES_OAEP_SHA512_Identifier = rfc4055.rSAES_OAEP_SHA512_Identifier
356	donghui	0	#<<NEWL>># This file is part of pyasn1-modules software.<<NEWL>>#<<NEWL>># Created by Russ Housley.<<NEWL>>#<<NEWL>># Copyright (c) 2019, Vigil Security, LLC<<NEWL>># License: http://snmplabs.com/pyasn1/license.html<<NEWL>>#<<NEWL>># RSAES-OAEP Key Transport Algorithm in CMS<<NEWL>>#<<NEWL>># Notice that all of the things needed in RFC 3560 are also defined<<NEWL>># in RFC 4055.  So, they are all pulled from the RFC 4055 module into<<NEWL>># this one so that people looking a RFC 3560 can easily find them.<<NEWL>>#<<NEWL>># ASN.1 source from:<<NEWL>># https://www.rfc-editor.org/rfc/rfc3560.txt<<NEWL>>#<<NEWL>><<NEWL>>from pyasn1_modules import rfc4055<<NEWL>><<NEWL>>id_sha1 = rfc4055.id_sha1<<NEWL>><<NEWL>>id_sha256 = rfc4055.id_sha256<<NEWL>><<NEWL>>id_sha384 = rfc4055.id_sha384<<NEWL>><<NEWL>>id_sha512 = rfc4055.id_sha512<<NEWL>><<NEWL>>id_mgf1 = rfc4055.id_mgf1<<NEWL>><<NEWL>>rsaEncryption = rfc4055.rsaEncryption<<NEWL>><<NEWL>>id_RSAES_OAEP = rfc4055.id_RSAES_OAEP<<NEWL>><<NEWL>>id_pSpecified = rfc4055.id_pSpecified<<NEWL>><<NEWL>>sha1Identifier = rfc4055.sha1Identifier<<NEWL>><<NEWL>>sha256Identifier = rfc4055.sha256Identifier<<NEWL>><<NEWL>>sha384Identifier = rfc4055.sha384Identifier<<NEWL>><<NEWL>>sha512Identifier = rfc4055.sha512Identifier<<NEWL>><<NEWL>>mgf1SHA1Identifier = rfc4055.mgf1SHA1Identifier<<NEWL>><<NEWL>>mgf1SHA256Identifier = rfc4055.mgf1SHA256Identifier<<NEWL>><<NEWL>>mgf1SHA384Identifier = rfc4055.mgf1SHA384Identifier<<NEWL>><<NEWL>>mgf1SHA512Identifier = rfc4055.mgf1SHA512Identifier<<NEWL>><<NEWL>>pSpecifiedEmptyIdentifier = rfc4055.pSpecifiedEmptyIdentifier<<NEWL>><<NEWL>><<NEWL>>class RSAES_OAEP_params(rfc4055.RSAES_OAEP_params):<<NEWL>>    pass<<NEWL>><<NEWL>><<NEWL>>rSAES_OAEP_Default_Params = RSAES_OAEP_params()<<NEWL>><<NEWL>>rSAES_OAEP_Default_Identifier = rfc4055.rSAES_OAEP_Default_Identifier<<NEWL>><<NEWL>>rSAES_OAEP_SHA256_Params = rfc4055.rSAES_OAEP_SHA256_Params<<NEWL>><<NEWL>>rSAES_OAEP_SHA256_Identifier = rfc4055.rSAES_OAEP_SHA256_Identifier<<NEWL>><<NEWL>>rSAES_OAEP_SHA384_Params = rfc4055.rSAES_OAEP_SHA384_Params<<NEWL>><<NEWL>>rSAES_OAEP_SHA384_Identifier = rfc4055.rSAES_OAEP_SHA384_Identifier<<NEWL>><<NEWL>>rSAES_OAEP_SHA512_Params = rfc4055.rSAES_OAEP_SHA512_Params<<NEWL>><<NEWL>>rSAES_OAEP_SHA512_Identifier = rfc4055.rSAES_OAEP_SHA512_Identifier
418	jackson	4	"#<<NEWL>># Copyright 2011 Facebook<<NEWL>>#<<NEWL>># Licensed under the Apache License, Version 2.0 (the ""License""); you may<<NEWL>># not use this file except in compliance with the License. You may obtain<<NEWL>># a copy of the License at<<NEWL>>#<<NEWL>>#     http://www.apache.org/licenses/LICENSE-2.0<<NEWL>>#<<NEWL>># Unless required by applicable law or agreed to in writing, software<<NEWL>># distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT<<NEWL>># WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the<<NEWL>># License for the specific language governing permissions and limitations<<NEWL>># under the License.<<NEWL>><<NEWL>>""""""Implementation of platform-specific functionality.<<NEWL>><<NEWL>>For each function or class described in `tornado.platform.interface`,<<NEWL>>the appropriate platform-specific implementation exists in this module.<<NEWL>>Most code that needs access to this functionality should do e.g.::<<NEWL>><<NEWL>>    from tornado.platform.auto import set_close_exec<<NEWL>>""""""<<NEWL>><<NEWL>>from __future__ import absolute_import, division, print_function<<NEWL>><<NEWL>>import os<<NEWL>><<NEWL>>if 'APPENGINE_RUNTIME' in os.environ:<<NEWL>>    from tornado.platform.common import Waker<<NEWL>><<NEWL>>    def set_close_exec(fd):<<NEWL>>        pass<<NEWL>>elif os.name == 'nt':<<NEWL>>    from tornado.platform.common import Waker<<NEWL>>    from tornado.platform.windows import set_close_exec<<NEWL>>else:<<NEWL>>    from tornado.platform.posix import set_close_exec, Waker<<NEWL>><<NEWL>>try:<<NEWL>>    # monotime monkey-patches the time module to have a monotonic function<<NEWL>>    # in versions of python before 3.3.<<NEWL>>    import monotime<<NEWL>>    # Silence pyflakes warning about this unused import<<NEWL>>    monotime<<NEWL>>except ImportError:<<NEWL>>    pass<<NEWL>>try:<<NEWL>>    # monotonic can provide a monotonic function in versions of python before<<NEWL>>    # 3.3, too.<<NEWL>>    from monotonic import monotonic as monotonic_time<<NEWL>>except ImportError:<<NEWL>>    try:<<NEWL>>        from time import monotonic as monotonic_time<<NEWL>>    except ImportError:<<NEWL>>        monotonic_time = None<<NEWL>><<NEWL>>__all__ = ['Waker', 'set_close_exec', 'monotonic_time']"
418	donghui	2	"#<<NEWL>># Copyright 2011 Facebook<<NEWL>>#<<NEWL>># Licensed under the Apache License, Version 2.0 (the ""License""); you may<<NEWL>># not use this file except in compliance with the License. You may obtain<<NEWL>># a copy of the License at<<NEWL>>#<<NEWL>>#     http://www.apache.org/licenses/LICENSE-2.0<<NEWL>>#<<NEWL>># Unless required by applicable law or agreed to in writing, software<<NEWL>># distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT<<NEWL>># WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the<<NEWL>># License for the specific language governing permissions and limitations<<NEWL>># under the License.<<NEWL>><<NEWL>>""""""Implementation of platform-specific functionality.<<NEWL>><<NEWL>>For each function or class described in `tornado.platform.interface`,<<NEWL>>the appropriate platform-specific implementation exists in this module.<<NEWL>>Most code that needs access to this functionality should do e.g.::<<NEWL>><<NEWL>>    from tornado.platform.auto import set_close_exec<<NEWL>>""""""<<NEWL>><<NEWL>>from __future__ import absolute_import, division, print_function<<NEWL>><<NEWL>>import os<<NEWL>><<NEWL>>if 'APPENGINE_RUNTIME' in os.environ:<<NEWL>>    from tornado.platform.common import Waker<<NEWL>><<NEWL>>    def set_close_exec(fd):<<NEWL>>        pass<<NEWL>>elif os.name == 'nt':<<NEWL>>    from tornado.platform.common import Waker<<NEWL>>    from tornado.platform.windows import set_close_exec<<NEWL>>else:<<NEWL>>    from tornado.platform.posix import set_close_exec, Waker<<NEWL>><<NEWL>>try:<<NEWL>>    # monotime monkey-patches the time module to have a monotonic function<<NEWL>>    # in versions of python before 3.3.<<NEWL>>    import monotime<<NEWL>>    # Silence pyflakes warning about this unused import<<NEWL>>    monotime<<NEWL>>except ImportError:<<NEWL>>    pass<<NEWL>>try:<<NEWL>>    # monotonic can provide a monotonic function in versions of python before<<NEWL>>    # 3.3, too.<<NEWL>>    from monotonic import monotonic as monotonic_time<<NEWL>>except ImportError:<<NEWL>>    try:<<NEWL>>        from time import monotonic as monotonic_time<<NEWL>>    except ImportError:<<NEWL>>        monotonic_time = None<<NEWL>><<NEWL>>__all__ = ['Waker', 'set_close_exec', 'monotonic_time']"
509	jackson	1	"#<<NEWL>># Licensed to the Apache Software Foundation (ASF) under one<<NEWL>># or more contributor license agreements.  See the NOTICE file<<NEWL>># distributed with this work for additional information<<NEWL>># regarding copyright ownership.  The ASF licenses this file<<NEWL>># to you under the Apache License, Version 2.0 (the<<NEWL>># ""License""); you may not use this file except in compliance<<NEWL>># with the License.  You may obtain a copy of the License at<<NEWL>>#<<NEWL>>#   http://www.apache.org/licenses/LICENSE-2.0<<NEWL>>#<<NEWL>># Unless required by applicable law or agreed to in writing,<<NEWL>># software distributed under the License is distributed on an<<NEWL>># ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY<<NEWL>># KIND, either express or implied.  See the License for the<<NEWL>># specific language governing permissions and limitations<<NEWL>># under the License.<<NEWL>>""""""Example DAG demonstrating the usage of the BranchPythonOperator.""""""<<NEWL>>from __future__ import annotations<<NEWL>><<NEWL>>import random<<NEWL>><<NEWL>>import pendulum<<NEWL>><<NEWL>>from airflow import DAG<<NEWL>>from airflow.operators.empty import EmptyOperator<<NEWL>>from airflow.operators.python import BranchPythonOperator<<NEWL>>from airflow.utils.edgemodifier import Label<<NEWL>>from airflow.utils.trigger_rule import TriggerRule<<NEWL>><<NEWL>>with DAG(<<NEWL>>    dag_id=""example_branch_operator"",<<NEWL>>    start_date=pendulum.datetime(2021, 1, 1, tz=""UTC""),<<NEWL>>    catchup=False,<<NEWL>>    schedule=""@daily"",<<NEWL>>    tags=[""example"", ""example2""],<<NEWL>>) as dag:<<NEWL>>    run_this_first = EmptyOperator(<<NEWL>>        task_id=""run_this_first"",<<NEWL>>    )<<NEWL>><<NEWL>>    options = [""branch_a"", ""branch_b"", ""branch_c"", ""branch_d""]<<NEWL>><<NEWL>>    branching = BranchPythonOperator(<<NEWL>>        task_id=""branching"",<<NEWL>>        python_callable=lambda: random.choice(options),<<NEWL>>    )<<NEWL>>    run_this_first >> branching<<NEWL>><<NEWL>>    join = EmptyOperator(<<NEWL>>        task_id=""join"",<<NEWL>>        trigger_rule=TriggerRule.NONE_FAILED_MIN_ONE_SUCCESS,<<NEWL>>    )<<NEWL>><<NEWL>>    for option in options:<<NEWL>>        t = EmptyOperator(<<NEWL>>            task_id=option,<<NEWL>>        )<<NEWL>><<NEWL>>        empty_follow = EmptyOperator(<<NEWL>>            task_id=""follow_"" + option,<<NEWL>>        )<<NEWL>><<NEWL>>        # Label is optional here, but it can help identify more complex branches<<NEWL>>        branching >> Label(option) >> t >> empty_follow >> join"
509	donghui	1	"#<<NEWL>># Licensed to the Apache Software Foundation (ASF) under one<<NEWL>># or more contributor license agreements.  See the NOTICE file<<NEWL>># distributed with this work for additional information<<NEWL>># regarding copyright ownership.  The ASF licenses this file<<NEWL>># to you under the Apache License, Version 2.0 (the<<NEWL>># ""License""); you may not use this file except in compliance<<NEWL>># with the License.  You may obtain a copy of the License at<<NEWL>>#<<NEWL>>#   http://www.apache.org/licenses/LICENSE-2.0<<NEWL>>#<<NEWL>># Unless required by applicable law or agreed to in writing,<<NEWL>># software distributed under the License is distributed on an<<NEWL>># ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY<<NEWL>># KIND, either express or implied.  See the License for the<<NEWL>># specific language governing permissions and limitations<<NEWL>># under the License.<<NEWL>>""""""Example DAG demonstrating the usage of the BranchPythonOperator.""""""<<NEWL>>from __future__ import annotations<<NEWL>><<NEWL>>import random<<NEWL>><<NEWL>>import pendulum<<NEWL>><<NEWL>>from airflow import DAG<<NEWL>>from airflow.operators.empty import EmptyOperator<<NEWL>>from airflow.operators.python import BranchPythonOperator<<NEWL>>from airflow.utils.edgemodifier import Label<<NEWL>>from airflow.utils.trigger_rule import TriggerRule<<NEWL>><<NEWL>>with DAG(<<NEWL>>    dag_id=""example_branch_operator"",<<NEWL>>    start_date=pendulum.datetime(2021, 1, 1, tz=""UTC""),<<NEWL>>    catchup=False,<<NEWL>>    schedule=""@daily"",<<NEWL>>    tags=[""example"", ""example2""],<<NEWL>>) as dag:<<NEWL>>    run_this_first = EmptyOperator(<<NEWL>>        task_id=""run_this_first"",<<NEWL>>    )<<NEWL>><<NEWL>>    options = [""branch_a"", ""branch_b"", ""branch_c"", ""branch_d""]<<NEWL>><<NEWL>>    branching = BranchPythonOperator(<<NEWL>>        task_id=""branching"",<<NEWL>>        python_callable=lambda: random.choice(options),<<NEWL>>    )<<NEWL>>    run_this_first >> branching<<NEWL>><<NEWL>>    join = EmptyOperator(<<NEWL>>        task_id=""join"",<<NEWL>>        trigger_rule=TriggerRule.NONE_FAILED_MIN_ONE_SUCCESS,<<NEWL>>    )<<NEWL>><<NEWL>>    for option in options:<<NEWL>>        t = EmptyOperator(<<NEWL>>            task_id=option,<<NEWL>>        )<<NEWL>><<NEWL>>        empty_follow = EmptyOperator(<<NEWL>>            task_id=""follow_"" + option,<<NEWL>>        )<<NEWL>><<NEWL>>        # Label is optional here, but it can help identify more complex branches<<NEWL>>        branching >> Label(option) >> t >> empty_follow >> join"
449	jackson	3	"""""""distutils.command.install_scripts<<NEWL>><<NEWL>>Implements the Distutils 'install_scripts' command, for installing<<NEWL>>Python scripts.""""""<<NEWL>><<NEWL>># contributed by Bastian Kleineidam<<NEWL>><<NEWL>>import os<<NEWL>>from distutils.core import Command<<NEWL>>from distutils import log<<NEWL>>from stat import ST_MODE<<NEWL>><<NEWL>><<NEWL>>class install_scripts(Command):<<NEWL>>    description = ""install scripts (Python or otherwise)""<<NEWL>><<NEWL>>    user_options = [<<NEWL>>        (""install-dir="", ""d"", ""directory to install scripts to""),<<NEWL>>        (""build-dir="", ""b"", ""build directory (where to install from)""),<<NEWL>>        (""force"", ""f"", ""force installation (overwrite existing files)""),<<NEWL>>        (""skip-build"", None, ""skip the build steps""),<<NEWL>>    ]<<NEWL>><<NEWL>>    boolean_options = [""force"", ""skip-build""]<<NEWL>><<NEWL>>    def initialize_options(self):<<NEWL>>        self.install_dir = None<<NEWL>>        self.force = 0<<NEWL>>        self.build_dir = None<<NEWL>>        self.skip_build = None<<NEWL>><<NEWL>>    def finalize_options(self):<<NEWL>>        self.set_undefined_options(""build"", (""build_scripts"", ""build_dir""))<<NEWL>>        self.set_undefined_options(<<NEWL>>            ""install"",<<NEWL>>            (""install_scripts"", ""install_dir""),<<NEWL>>            (""force"", ""force""),<<NEWL>>            (""skip_build"", ""skip_build""),<<NEWL>>        )<<NEWL>><<NEWL>>    def run(self):<<NEWL>>        if not self.skip_build:<<NEWL>>            self.run_command(""build_scripts"")<<NEWL>>        self.outfiles = self.copy_tree(self.build_dir, self.install_dir)<<NEWL>>        if os.name == ""posix"":<<NEWL>>            # Set the executable bits (owner, group, and world) on<<NEWL>>            # all the scripts we just installed.<<NEWL>>            for file in self.get_outputs():<<NEWL>>                if self.dry_run:<<NEWL>>                    log.info(""changing mode of %s"", file)<<NEWL>>                else:<<NEWL>>                    mode = ((os.stat(file)[ST_MODE]) | 0o555) & 0o7777<<NEWL>>                    log.info(""changing mode of %s to %o"", file, mode)<<NEWL>>                    os.chmod(file, mode)<<NEWL>><<NEWL>>    def get_inputs(self):<<NEWL>>        return self.distribution.scripts or []<<NEWL>><<NEWL>>    def get_outputs(self):<<NEWL>>        return self.outfiles or []"
449	donghui	1	"""""""distutils.command.install_scripts<<NEWL>><<NEWL>>Implements the Distutils 'install_scripts' command, for installing<<NEWL>>Python scripts.""""""<<NEWL>><<NEWL>># contributed by Bastian Kleineidam<<NEWL>><<NEWL>>import os<<NEWL>>from distutils.core import Command<<NEWL>>from distutils import log<<NEWL>>from stat import ST_MODE<<NEWL>><<NEWL>><<NEWL>>class install_scripts(Command):<<NEWL>>    description = ""install scripts (Python or otherwise)""<<NEWL>><<NEWL>>    user_options = [<<NEWL>>        (""install-dir="", ""d"", ""directory to install scripts to""),<<NEWL>>        (""build-dir="", ""b"", ""build directory (where to install from)""),<<NEWL>>        (""force"", ""f"", ""force installation (overwrite existing files)""),<<NEWL>>        (""skip-build"", None, ""skip the build steps""),<<NEWL>>    ]<<NEWL>><<NEWL>>    boolean_options = [""force"", ""skip-build""]<<NEWL>><<NEWL>>    def initialize_options(self):<<NEWL>>        self.install_dir = None<<NEWL>>        self.force = 0<<NEWL>>        self.build_dir = None<<NEWL>>        self.skip_build = None<<NEWL>><<NEWL>>    def finalize_options(self):<<NEWL>>        self.set_undefined_options(""build"", (""build_scripts"", ""build_dir""))<<NEWL>>        self.set_undefined_options(<<NEWL>>            ""install"",<<NEWL>>            (""install_scripts"", ""install_dir""),<<NEWL>>            (""force"", ""force""),<<NEWL>>            (""skip_build"", ""skip_build""),<<NEWL>>        )<<NEWL>><<NEWL>>    def run(self):<<NEWL>>        if not self.skip_build:<<NEWL>>            self.run_command(""build_scripts"")<<NEWL>>        self.outfiles = self.copy_tree(self.build_dir, self.install_dir)<<NEWL>>        if os.name == ""posix"":<<NEWL>>            # Set the executable bits (owner, group, and world) on<<NEWL>>            # all the scripts we just installed.<<NEWL>>            for file in self.get_outputs():<<NEWL>>                if self.dry_run:<<NEWL>>                    log.info(""changing mode of %s"", file)<<NEWL>>                else:<<NEWL>>                    mode = ((os.stat(file)[ST_MODE]) | 0o555) & 0o7777<<NEWL>>                    log.info(""changing mode of %s to %o"", file, mode)<<NEWL>>                    os.chmod(file, mode)<<NEWL>><<NEWL>>    def get_inputs(self):<<NEWL>>        return self.distribution.scripts or []<<NEWL>><<NEWL>>    def get_outputs(self):<<NEWL>>        return self.outfiles or []"
459	jackson	1	"# This source code is licensed under the MIT license found in the<<NEWL>># LICENSE file in the root directory of this source tree.<<NEWL>><<NEWL>>import os<<NEWL>>import shutil<<NEWL>>import sys<<NEWL>>import tempfile<<NEWL>>import unittest<<NEWL>>from typing import Optional<<NEWL>>from unittest.mock import MagicMock<<NEWL>><<NEWL>><<NEWL>>class TestFileIO(unittest.TestCase):<<NEWL>><<NEWL>>    _tmpdir: Optional[str] = None<<NEWL>>    _tmpfile: Optional[str] = None<<NEWL>>    _tmpfile_contents = ""Hello, World""<<NEWL>><<NEWL>>    @classmethod<<NEWL>>    def setUpClass(cls) -> None:<<NEWL>>        cls._tmpdir = tempfile.mkdtemp()<<NEWL>>        with open(os.path.join(cls._tmpdir, ""test.txt""), ""w"") as f:<<NEWL>>            cls._tmpfile = f.name<<NEWL>>            f.write(cls._tmpfile_contents)<<NEWL>>            f.flush()<<NEWL>><<NEWL>>    @classmethod<<NEWL>>    def tearDownClass(cls) -> None:<<NEWL>>        # Cleanup temp working dir.<<NEWL>>        if cls._tmpdir is not None:<<NEWL>>            shutil.rmtree(cls._tmpdir)  # type: ignore<<NEWL>><<NEWL>>    def test_file_io(self):<<NEWL>>        from fairseq.file_io import PathManager<<NEWL>><<NEWL>>        with PathManager.open(os.path.join(self._tmpdir, ""test.txt""), ""r"") as f:<<NEWL>>            s = f.read()<<NEWL>>        self.assertEqual(s, self._tmpfile_contents)<<NEWL>><<NEWL>>    def test_file_io_oss(self):<<NEWL>>        # Mock iopath to simulate oss environment.<<NEWL>>        sys.modules[""iopath""] = MagicMock()<<NEWL>>        from fairseq.file_io import PathManager<<NEWL>><<NEWL>>        with PathManager.open(os.path.join(self._tmpdir, ""test.txt""), ""r"") as f:<<NEWL>>            s = f.read()<<NEWL>>        self.assertEqual(s, self._tmpfile_contents)<<NEWL>><<NEWL>>    def test_file_io_async(self):<<NEWL>>        # ioPath `PathManager` is initialized after the first `opena` call.<<NEWL>>        try:<<NEWL>>            from fairseq.file_io import IOPathManager, PathManager<<NEWL>>            _asyncfile = os.path.join(self._tmpdir, ""async.txt"")<<NEWL>>            f = PathManager.opena(_asyncfile, ""wb"")<<NEWL>>            f.close()<<NEWL>><<NEWL>>        finally:<<NEWL>>            self.assertTrue(PathManager.async_close())"
459	donghui	1	"# This source code is licensed under the MIT license found in the<<NEWL>># LICENSE file in the root directory of this source tree.<<NEWL>><<NEWL>>import os<<NEWL>>import shutil<<NEWL>>import sys<<NEWL>>import tempfile<<NEWL>>import unittest<<NEWL>>from typing import Optional<<NEWL>>from unittest.mock import MagicMock<<NEWL>><<NEWL>><<NEWL>>class TestFileIO(unittest.TestCase):<<NEWL>><<NEWL>>    _tmpdir: Optional[str] = None<<NEWL>>    _tmpfile: Optional[str] = None<<NEWL>>    _tmpfile_contents = ""Hello, World""<<NEWL>><<NEWL>>    @classmethod<<NEWL>>    def setUpClass(cls) -> None:<<NEWL>>        cls._tmpdir = tempfile.mkdtemp()<<NEWL>>        with open(os.path.join(cls._tmpdir, ""test.txt""), ""w"") as f:<<NEWL>>            cls._tmpfile = f.name<<NEWL>>            f.write(cls._tmpfile_contents)<<NEWL>>            f.flush()<<NEWL>><<NEWL>>    @classmethod<<NEWL>>    def tearDownClass(cls) -> None:<<NEWL>>        # Cleanup temp working dir.<<NEWL>>        if cls._tmpdir is not None:<<NEWL>>            shutil.rmtree(cls._tmpdir)  # type: ignore<<NEWL>><<NEWL>>    def test_file_io(self):<<NEWL>>        from fairseq.file_io import PathManager<<NEWL>><<NEWL>>        with PathManager.open(os.path.join(self._tmpdir, ""test.txt""), ""r"") as f:<<NEWL>>            s = f.read()<<NEWL>>        self.assertEqual(s, self._tmpfile_contents)<<NEWL>><<NEWL>>    def test_file_io_oss(self):<<NEWL>>        # Mock iopath to simulate oss environment.<<NEWL>>        sys.modules[""iopath""] = MagicMock()<<NEWL>>        from fairseq.file_io import PathManager<<NEWL>><<NEWL>>        with PathManager.open(os.path.join(self._tmpdir, ""test.txt""), ""r"") as f:<<NEWL>>            s = f.read()<<NEWL>>        self.assertEqual(s, self._tmpfile_contents)<<NEWL>><<NEWL>>    def test_file_io_async(self):<<NEWL>>        # ioPath `PathManager` is initialized after the first `opena` call.<<NEWL>>        try:<<NEWL>>            from fairseq.file_io import IOPathManager, PathManager<<NEWL>>            _asyncfile = os.path.join(self._tmpdir, ""async.txt"")<<NEWL>>            f = PathManager.opena(_asyncfile, ""wb"")<<NEWL>>            f.close()<<NEWL>><<NEWL>>        finally:<<NEWL>>            self.assertTrue(PathManager.async_close())"
408	jackson	0	"# Copyright Jonathan Hartley 2013. BSD 3-Clause license, see LICENSE file.<<NEWL>>import atexit<<NEWL>>import contextlib<<NEWL>>import sys<<NEWL>><<NEWL>>from .ansitowin32 import AnsiToWin32<<NEWL>><<NEWL>><<NEWL>>orig_stdout = None<<NEWL>>orig_stderr = None<<NEWL>><<NEWL>>wrapped_stdout = None<<NEWL>>wrapped_stderr = None<<NEWL>><<NEWL>>atexit_done = False<<NEWL>><<NEWL>><<NEWL>>def reset_all():<<NEWL>>    if AnsiToWin32 is not None:  # Issue #74: objects might become None at exit<<NEWL>>        AnsiToWin32(orig_stdout).reset_all()<<NEWL>><<NEWL>><<NEWL>>def init(autoreset=False, convert=None, strip=None, wrap=True):<<NEWL>>    if not wrap and any([autoreset, convert, strip]):<<NEWL>>        raise ValueError(""wrap=False conflicts with any other arg=True"")<<NEWL>><<NEWL>>    global wrapped_stdout, wrapped_stderr<<NEWL>>    global orig_stdout, orig_stderr<<NEWL>><<NEWL>>    orig_stdout = sys.stdout<<NEWL>>    orig_stderr = sys.stderr<<NEWL>><<NEWL>>    if sys.stdout is None:<<NEWL>>        wrapped_stdout = None<<NEWL>>    else:<<NEWL>>        sys.stdout = wrapped_stdout = wrap_stream(<<NEWL>>            orig_stdout, convert, strip, autoreset, wrap<<NEWL>>        )<<NEWL>>    if sys.stderr is None:<<NEWL>>        wrapped_stderr = None<<NEWL>>    else:<<NEWL>>        sys.stderr = wrapped_stderr = wrap_stream(<<NEWL>>            orig_stderr, convert, strip, autoreset, wrap<<NEWL>>        )<<NEWL>><<NEWL>>    global atexit_done<<NEWL>>    if not atexit_done:<<NEWL>>        atexit.register(reset_all)<<NEWL>>        atexit_done = True<<NEWL>><<NEWL>><<NEWL>>def deinit():<<NEWL>>    if orig_stdout is not None:<<NEWL>>        sys.stdout = orig_stdout<<NEWL>>    if orig_stderr is not None:<<NEWL>>        sys.stderr = orig_stderr<<NEWL>><<NEWL>><<NEWL>>@contextlib.contextmanager<<NEWL>>def colorama_text(*args, **kwargs):<<NEWL>>    init(*args, **kwargs)<<NEWL>>    try:<<NEWL>>        yield<<NEWL>>    finally:<<NEWL>>        deinit()<<NEWL>><<NEWL>><<NEWL>>def reinit():<<NEWL>>    if wrapped_stdout is not None:<<NEWL>>        sys.stdout = wrapped_stdout<<NEWL>>    if wrapped_stderr is not None:<<NEWL>>        sys.stderr = wrapped_stderr<<NEWL>><<NEWL>><<NEWL>>def wrap_stream(stream, convert, strip, autoreset, wrap):<<NEWL>>    if wrap:<<NEWL>>        wrapper = AnsiToWin32(stream, convert=convert, strip=strip, autoreset=autoreset)<<NEWL>>        if wrapper.should_wrap():<<NEWL>>            stream = wrapper.stream<<NEWL>>    return stream"
408	donghui	1	"# Copyright Jonathan Hartley 2013. BSD 3-Clause license, see LICENSE file.<<NEWL>>import atexit<<NEWL>>import contextlib<<NEWL>>import sys<<NEWL>><<NEWL>>from .ansitowin32 import AnsiToWin32<<NEWL>><<NEWL>><<NEWL>>orig_stdout = None<<NEWL>>orig_stderr = None<<NEWL>><<NEWL>>wrapped_stdout = None<<NEWL>>wrapped_stderr = None<<NEWL>><<NEWL>>atexit_done = False<<NEWL>><<NEWL>><<NEWL>>def reset_all():<<NEWL>>    if AnsiToWin32 is not None:  # Issue #74: objects might become None at exit<<NEWL>>        AnsiToWin32(orig_stdout).reset_all()<<NEWL>><<NEWL>><<NEWL>>def init(autoreset=False, convert=None, strip=None, wrap=True):<<NEWL>>    if not wrap and any([autoreset, convert, strip]):<<NEWL>>        raise ValueError(""wrap=False conflicts with any other arg=True"")<<NEWL>><<NEWL>>    global wrapped_stdout, wrapped_stderr<<NEWL>>    global orig_stdout, orig_stderr<<NEWL>><<NEWL>>    orig_stdout = sys.stdout<<NEWL>>    orig_stderr = sys.stderr<<NEWL>><<NEWL>>    if sys.stdout is None:<<NEWL>>        wrapped_stdout = None<<NEWL>>    else:<<NEWL>>        sys.stdout = wrapped_stdout = wrap_stream(<<NEWL>>            orig_stdout, convert, strip, autoreset, wrap<<NEWL>>        )<<NEWL>>    if sys.stderr is None:<<NEWL>>        wrapped_stderr = None<<NEWL>>    else:<<NEWL>>        sys.stderr = wrapped_stderr = wrap_stream(<<NEWL>>            orig_stderr, convert, strip, autoreset, wrap<<NEWL>>        )<<NEWL>><<NEWL>>    global atexit_done<<NEWL>>    if not atexit_done:<<NEWL>>        atexit.register(reset_all)<<NEWL>>        atexit_done = True<<NEWL>><<NEWL>><<NEWL>>def deinit():<<NEWL>>    if orig_stdout is not None:<<NEWL>>        sys.stdout = orig_stdout<<NEWL>>    if orig_stderr is not None:<<NEWL>>        sys.stderr = orig_stderr<<NEWL>><<NEWL>><<NEWL>>@contextlib.contextmanager<<NEWL>>def colorama_text(*args, **kwargs):<<NEWL>>    init(*args, **kwargs)<<NEWL>>    try:<<NEWL>>        yield<<NEWL>>    finally:<<NEWL>>        deinit()<<NEWL>><<NEWL>><<NEWL>>def reinit():<<NEWL>>    if wrapped_stdout is not None:<<NEWL>>        sys.stdout = wrapped_stdout<<NEWL>>    if wrapped_stderr is not None:<<NEWL>>        sys.stderr = wrapped_stderr<<NEWL>><<NEWL>><<NEWL>>def wrap_stream(stream, convert, strip, autoreset, wrap):<<NEWL>>    if wrap:<<NEWL>>        wrapper = AnsiToWin32(stream, convert=convert, strip=strip, autoreset=autoreset)<<NEWL>>        if wrapper.should_wrap():<<NEWL>>            stream = wrapper.stream<<NEWL>>    return stream"
346	jackson	1	"from textwrap import dedent<<NEWL>><<NEWL>>from flaky import flaky<<NEWL>><<NEWL>>from .test_embed_kernel import setup_kernel<<NEWL>><<NEWL>>TIMEOUT = 15<<NEWL>><<NEWL>><<NEWL>>@flaky(max_runs=3)<<NEWL>>def test_ipython_start_kernel_userns():<<NEWL>>    cmd = dedent(<<NEWL>>        """"""<<NEWL>>        from ipykernel.kernelapp import launch_new_instance<<NEWL>>        ns = {""tre"": 123}<<NEWL>>        launch_new_instance(user_ns=ns)<<NEWL>>        """"""<<NEWL>>    )<<NEWL>><<NEWL>>    with setup_kernel(cmd) as client:<<NEWL>>        client.inspect(""tre"")<<NEWL>>        msg = client.get_shell_msg(timeout=TIMEOUT)<<NEWL>>        content = msg[""content""]<<NEWL>>        assert content[""found""]<<NEWL>>        text = content[""data""][""text/plain""]<<NEWL>>        assert ""123"" in text<<NEWL>><<NEWL>>        # user_module should be an instance of DummyMod<<NEWL>>        client.execute(""usermod = get_ipython().user_module"")<<NEWL>>        msg = client.get_shell_msg(timeout=TIMEOUT)<<NEWL>>        content = msg[""content""]<<NEWL>>        assert content[""status""] == ""ok""<<NEWL>>        client.inspect(""usermod"")<<NEWL>>        msg = client.get_shell_msg(timeout=TIMEOUT)<<NEWL>>        content = msg[""content""]<<NEWL>>        assert content[""found""]<<NEWL>>        text = content[""data""][""text/plain""]<<NEWL>>        assert ""DummyMod"" in text<<NEWL>><<NEWL>><<NEWL>>@flaky(max_runs=3)<<NEWL>>def test_ipython_start_kernel_no_userns():<<NEWL>>    # Issue #4188 - user_ns should be passed to shell as None, not {}<<NEWL>>    cmd = dedent(<<NEWL>>        """"""<<NEWL>>        from ipykernel.kernelapp import launch_new_instance<<NEWL>>        launch_new_instance()<<NEWL>>        """"""<<NEWL>>    )<<NEWL>><<NEWL>>    with setup_kernel(cmd) as client:<<NEWL>>        # user_module should not be an instance of DummyMod<<NEWL>>        client.execute(""usermod = get_ipython().user_module"")<<NEWL>>        msg = client.get_shell_msg(timeout=TIMEOUT)<<NEWL>>        content = msg[""content""]<<NEWL>>        assert content[""status""] == ""ok""<<NEWL>>        client.inspect(""usermod"")<<NEWL>>        msg = client.get_shell_msg(timeout=TIMEOUT)<<NEWL>>        content = msg[""content""]<<NEWL>>        assert content[""found""]<<NEWL>>        text = content[""data""][""text/plain""]<<NEWL>>        assert ""DummyMod"" not in text"
346	donghui	2	"from textwrap import dedent<<NEWL>><<NEWL>>from flaky import flaky<<NEWL>><<NEWL>>from .test_embed_kernel import setup_kernel<<NEWL>><<NEWL>>TIMEOUT = 15<<NEWL>><<NEWL>><<NEWL>>@flaky(max_runs=3)<<NEWL>>def test_ipython_start_kernel_userns():<<NEWL>>    cmd = dedent(<<NEWL>>        """"""<<NEWL>>        from ipykernel.kernelapp import launch_new_instance<<NEWL>>        ns = {""tre"": 123}<<NEWL>>        launch_new_instance(user_ns=ns)<<NEWL>>        """"""<<NEWL>>    )<<NEWL>><<NEWL>>    with setup_kernel(cmd) as client:<<NEWL>>        client.inspect(""tre"")<<NEWL>>        msg = client.get_shell_msg(timeout=TIMEOUT)<<NEWL>>        content = msg[""content""]<<NEWL>>        assert content[""found""]<<NEWL>>        text = content[""data""][""text/plain""]<<NEWL>>        assert ""123"" in text<<NEWL>><<NEWL>>        # user_module should be an instance of DummyMod<<NEWL>>        client.execute(""usermod = get_ipython().user_module"")<<NEWL>>        msg = client.get_shell_msg(timeout=TIMEOUT)<<NEWL>>        content = msg[""content""]<<NEWL>>        assert content[""status""] == ""ok""<<NEWL>>        client.inspect(""usermod"")<<NEWL>>        msg = client.get_shell_msg(timeout=TIMEOUT)<<NEWL>>        content = msg[""content""]<<NEWL>>        assert content[""found""]<<NEWL>>        text = content[""data""][""text/plain""]<<NEWL>>        assert ""DummyMod"" in text<<NEWL>><<NEWL>><<NEWL>>@flaky(max_runs=3)<<NEWL>>def test_ipython_start_kernel_no_userns():<<NEWL>>    # Issue #4188 - user_ns should be passed to shell as None, not {}<<NEWL>>    cmd = dedent(<<NEWL>>        """"""<<NEWL>>        from ipykernel.kernelapp import launch_new_instance<<NEWL>>        launch_new_instance()<<NEWL>>        """"""<<NEWL>>    )<<NEWL>><<NEWL>>    with setup_kernel(cmd) as client:<<NEWL>>        # user_module should not be an instance of DummyMod<<NEWL>>        client.execute(""usermod = get_ipython().user_module"")<<NEWL>>        msg = client.get_shell_msg(timeout=TIMEOUT)<<NEWL>>        content = msg[""content""]<<NEWL>>        assert content[""status""] == ""ok""<<NEWL>>        client.inspect(""usermod"")<<NEWL>>        msg = client.get_shell_msg(timeout=TIMEOUT)<<NEWL>>        content = msg[""content""]<<NEWL>>        assert content[""found""]<<NEWL>>        text = content[""data""][""text/plain""]<<NEWL>>        assert ""DummyMod"" not in text"
317	jackson	4	# This file is distributed under the same license as the Django package.<<NEWL>>#<<NEWL>># The *_FORMAT strings use the Django date format syntax,<<NEWL>># see https://docs.djangoproject.com/en/dev/ref/templates/builtins/#date<<NEWL>>DATE_FORMAT = r'j\-\a \d\e F Y'         # '26-a de julio 1887'<<NEWL>>TIME_FORMAT = 'H:i'                     # '18:59'<<NEWL>>DATETIME_FORMAT = r'j\-\a \d\e F Y\, \j\e H:i'  # '26-a de julio 1887, je 18:59'<<NEWL>>YEAR_MONTH_FORMAT = r'F \d\e Y'         # 'julio de 1887'<<NEWL>>MONTH_DAY_FORMAT = r'j\-\a \d\e F'      # '26-a de julio'<<NEWL>>SHORT_DATE_FORMAT = 'Y-m-d'             # '1887-07-26'<<NEWL>>SHORT_DATETIME_FORMAT = 'Y-m-d H:i'     # '1887-07-26 18:59'<<NEWL>>FIRST_DAY_OF_WEEK = 1  # Monday (lundo)<<NEWL>><<NEWL>># The *_INPUT_FORMATS strings use the Python strftime format syntax,<<NEWL>># see https://docs.python.org/library/datetime.html#strftime-strptime-behavior<<NEWL>>DATE_INPUT_FORMATS = [<<NEWL>>    '%Y-%m-%d',                         # '1887-07-26'<<NEWL>>    '%y-%m-%d',                         # '87-07-26'<<NEWL>>    '%Y %m %d',                         # '1887 07 26'<<NEWL>>    '%Y.%m.%d',                         # '1887.07.26'<<NEWL>>    '%d-a de %b %Y',                    # '26-a de jul 1887'<<NEWL>>    '%d %b %Y',                         # '26 jul 1887'<<NEWL>>    '%d-a de %B %Y',                    # '26-a de julio 1887'<<NEWL>>    '%d %B %Y',                         # '26 julio 1887'<<NEWL>>    '%d %m %Y',                         # '26 07 1887'<<NEWL>>    '%d/%m/%Y',                         # '26/07/1887'<<NEWL>>]<<NEWL>>TIME_INPUT_FORMATS = [<<NEWL>>    '%H:%M:%S',                         # '18:59:00'<<NEWL>>    '%H:%M',                            # '18:59'<<NEWL>>]<<NEWL>>DATETIME_INPUT_FORMATS = [<<NEWL>>    '%Y-%m-%d %H:%M:%S',                # '1887-07-26 18:59:00'<<NEWL>>    '%Y-%m-%d %H:%M',                   # '1887-07-26 18:59'<<NEWL>><<NEWL>>    '%Y.%m.%d %H:%M:%S',                # '1887.07.26 18:59:00'<<NEWL>>    '%Y.%m.%d %H:%M',                   # '1887.07.26 18:59'<<NEWL>><<NEWL>>    '%d/%m/%Y %H:%M:%S',                # '26/07/1887 18:59:00'<<NEWL>>    '%d/%m/%Y %H:%M',                   # '26/07/1887 18:59'<<NEWL>><<NEWL>>    '%y-%m-%d %H:%M:%S',                # '87-07-26 18:59:00'<<NEWL>>    '%y-%m-%d %H:%M',                   # '87-07-26 18:59'<<NEWL>>]<<NEWL>>DECIMAL_SEPARATOR = ','<<NEWL>>THOUSAND_SEPARATOR = '\xa0'  # non-breaking space<<NEWL>>NUMBER_GROUPING = 3
317	donghui	1	# This file is distributed under the same license as the Django package.<<NEWL>>#<<NEWL>># The *_FORMAT strings use the Django date format syntax,<<NEWL>># see https://docs.djangoproject.com/en/dev/ref/templates/builtins/#date<<NEWL>>DATE_FORMAT = r'j\-\a \d\e F Y'         # '26-a de julio 1887'<<NEWL>>TIME_FORMAT = 'H:i'                     # '18:59'<<NEWL>>DATETIME_FORMAT = r'j\-\a \d\e F Y\, \j\e H:i'  # '26-a de julio 1887, je 18:59'<<NEWL>>YEAR_MONTH_FORMAT = r'F \d\e Y'         # 'julio de 1887'<<NEWL>>MONTH_DAY_FORMAT = r'j\-\a \d\e F'      # '26-a de julio'<<NEWL>>SHORT_DATE_FORMAT = 'Y-m-d'             # '1887-07-26'<<NEWL>>SHORT_DATETIME_FORMAT = 'Y-m-d H:i'     # '1887-07-26 18:59'<<NEWL>>FIRST_DAY_OF_WEEK = 1  # Monday (lundo)<<NEWL>><<NEWL>># The *_INPUT_FORMATS strings use the Python strftime format syntax,<<NEWL>># see https://docs.python.org/library/datetime.html#strftime-strptime-behavior<<NEWL>>DATE_INPUT_FORMATS = [<<NEWL>>    '%Y-%m-%d',                         # '1887-07-26'<<NEWL>>    '%y-%m-%d',                         # '87-07-26'<<NEWL>>    '%Y %m %d',                         # '1887 07 26'<<NEWL>>    '%Y.%m.%d',                         # '1887.07.26'<<NEWL>>    '%d-a de %b %Y',                    # '26-a de jul 1887'<<NEWL>>    '%d %b %Y',                         # '26 jul 1887'<<NEWL>>    '%d-a de %B %Y',                    # '26-a de julio 1887'<<NEWL>>    '%d %B %Y',                         # '26 julio 1887'<<NEWL>>    '%d %m %Y',                         # '26 07 1887'<<NEWL>>    '%d/%m/%Y',                         # '26/07/1887'<<NEWL>>]<<NEWL>>TIME_INPUT_FORMATS = [<<NEWL>>    '%H:%M:%S',                         # '18:59:00'<<NEWL>>    '%H:%M',                            # '18:59'<<NEWL>>]<<NEWL>>DATETIME_INPUT_FORMATS = [<<NEWL>>    '%Y-%m-%d %H:%M:%S',                # '1887-07-26 18:59:00'<<NEWL>>    '%Y-%m-%d %H:%M',                   # '1887-07-26 18:59'<<NEWL>><<NEWL>>    '%Y.%m.%d %H:%M:%S',                # '1887.07.26 18:59:00'<<NEWL>>    '%Y.%m.%d %H:%M',                   # '1887.07.26 18:59'<<NEWL>><<NEWL>>    '%d/%m/%Y %H:%M:%S',                # '26/07/1887 18:59:00'<<NEWL>>    '%d/%m/%Y %H:%M',                   # '26/07/1887 18:59'<<NEWL>><<NEWL>>    '%y-%m-%d %H:%M:%S',                # '87-07-26 18:59:00'<<NEWL>>    '%y-%m-%d %H:%M',                   # '87-07-26 18:59'<<NEWL>>]<<NEWL>>DECIMAL_SEPARATOR = ','<<NEWL>>THOUSAND_SEPARATOR = '\xa0'  # non-breaking space<<NEWL>>NUMBER_GROUPING = 3
257	jackson	0	"import json<<NEWL>>import openpyxl<<NEWL>>from configparser import ConfigParser<<NEWL>>from core.infrastructure.constants.data import PROJECT_PATH, CONFIG_PATH<<NEWL>><<NEWL>><<NEWL>>def read_config(key: str, value: str) -> str:<<NEWL>>    config = ConfigParser()<<NEWL>>    config.read(CONFIG_PATH)<<NEWL>>    return config.get(key, value)<<NEWL>><<NEWL>><<NEWL>>def read_json(path: str) -> dict:<<NEWL>>    with open(path, 'r', encoding='utf-8') as json_file:<<NEWL>>        file = json.load(json_file)<<NEWL>>        return file<<NEWL>><<NEWL>><<NEWL>>def write_json(path: str, key: str, value: str) -> None:<<NEWL>>    data = read_json(path)<<NEWL>>    data[key] = value<<NEWL>>    with open(path, 'w', encoding='utf-8') as json_file:<<NEWL>>        json.dump(data, json_file)<<NEWL>><<NEWL>><<NEWL>>def read_excel(sheet_name: str, value: str) -> dict[str]:<<NEWL>>    path = fr""{PROJECT_PATH}\{read_config('path', 'page_base')}""<<NEWL>>    workbook = openpyxl.load_workbook(path)<<NEWL>>    sheet = workbook[sheet_name]<<NEWL>>    cache = {}<<NEWL>>    for row in sheet.iter_rows(min_row=2, values_only=True):<<NEWL>>        result = {<<NEWL>>            'name': row[0],<<NEWL>>            'locator': row[1],<<NEWL>>            'type': row[2],<<NEWL>>            'image': row[3]<<NEWL>>        }<<NEWL>>        cache[result['name']] = result<<NEWL>>    try:<<NEWL>>        match cache[value]['name']:<<NEWL>>            case _:<<NEWL>>                return {<<NEWL>>                    'name': cache[value]['name'],<<NEWL>>                    'locator': cache[value]['locator'],<<NEWL>>                    'type': cache[value]['type'],<<NEWL>>                    'image': cache[value]['image']<<NEWL>>                }<<NEWL>>    except ValueError:<<NEWL>>        raise Exception('no such type')<<NEWL>><<NEWL>><<NEWL>>def get_name(*args: str) -> str:<<NEWL>>    return read_excel(*args)['name']<<NEWL>><<NEWL>><<NEWL>>def get_locator(*args: str) -> str:<<NEWL>>    return read_excel(*args)['locator']<<NEWL>><<NEWL>><<NEWL>>def get_type(*args: str) -> str:<<NEWL>>    return read_excel(*args)['type']<<NEWL>><<NEWL>><<NEWL>>def get_image(*args: str) -> str:<<NEWL>>    return read_excel(*args)['image']"
257	donghui	0	"import json<<NEWL>>import openpyxl<<NEWL>>from configparser import ConfigParser<<NEWL>>from core.infrastructure.constants.data import PROJECT_PATH, CONFIG_PATH<<NEWL>><<NEWL>><<NEWL>>def read_config(key: str, value: str) -> str:<<NEWL>>    config = ConfigParser()<<NEWL>>    config.read(CONFIG_PATH)<<NEWL>>    return config.get(key, value)<<NEWL>><<NEWL>><<NEWL>>def read_json(path: str) -> dict:<<NEWL>>    with open(path, 'r', encoding='utf-8') as json_file:<<NEWL>>        file = json.load(json_file)<<NEWL>>        return file<<NEWL>><<NEWL>><<NEWL>>def write_json(path: str, key: str, value: str) -> None:<<NEWL>>    data = read_json(path)<<NEWL>>    data[key] = value<<NEWL>>    with open(path, 'w', encoding='utf-8') as json_file:<<NEWL>>        json.dump(data, json_file)<<NEWL>><<NEWL>><<NEWL>>def read_excel(sheet_name: str, value: str) -> dict[str]:<<NEWL>>    path = fr""{PROJECT_PATH}\{read_config('path', 'page_base')}""<<NEWL>>    workbook = openpyxl.load_workbook(path)<<NEWL>>    sheet = workbook[sheet_name]<<NEWL>>    cache = {}<<NEWL>>    for row in sheet.iter_rows(min_row=2, values_only=True):<<NEWL>>        result = {<<NEWL>>            'name': row[0],<<NEWL>>            'locator': row[1],<<NEWL>>            'type': row[2],<<NEWL>>            'image': row[3]<<NEWL>>        }<<NEWL>>        cache[result['name']] = result<<NEWL>>    try:<<NEWL>>        match cache[value]['name']:<<NEWL>>            case _:<<NEWL>>                return {<<NEWL>>                    'name': cache[value]['name'],<<NEWL>>                    'locator': cache[value]['locator'],<<NEWL>>                    'type': cache[value]['type'],<<NEWL>>                    'image': cache[value]['image']<<NEWL>>                }<<NEWL>>    except ValueError:<<NEWL>>        raise Exception('no such type')<<NEWL>><<NEWL>><<NEWL>>def get_name(*args: str) -> str:<<NEWL>>    return read_excel(*args)['name']<<NEWL>><<NEWL>><<NEWL>>def get_locator(*args: str) -> str:<<NEWL>>    return read_excel(*args)['locator']<<NEWL>><<NEWL>><<NEWL>>def get_type(*args: str) -> str:<<NEWL>>    return read_excel(*args)['type']<<NEWL>><<NEWL>><<NEWL>>def get_image(*args: str) -> str:<<NEWL>>    return read_excel(*args)['image']"
263	jackson	0	"from graphql.language.location import SourceLocation<<NEWL>>from graphql.validation.rules import LoneAnonymousOperation<<NEWL>><<NEWL>>from .utils import expect_fails_rule, expect_passes_rule<<NEWL>><<NEWL>><<NEWL>>def anon_not_alone(line, column):<<NEWL>>    return {<<NEWL>>        ""message"": LoneAnonymousOperation.anonymous_operation_not_alone_message(),<<NEWL>>        ""locations"": [SourceLocation(line, column)],<<NEWL>>    }<<NEWL>><<NEWL>><<NEWL>>def test_no_operations():<<NEWL>>    expect_passes_rule(<<NEWL>>        LoneAnonymousOperation,<<NEWL>>        """"""<<NEWL>>      fragment fragA on Type {<<NEWL>>        field<<NEWL>>      }<<NEWL>>    """""",<<NEWL>>    )<<NEWL>><<NEWL>><<NEWL>>def test_one_anon_operation():<<NEWL>>    expect_passes_rule(<<NEWL>>        LoneAnonymousOperation,<<NEWL>>        """"""<<NEWL>>      {<<NEWL>>        field<<NEWL>>      }<<NEWL>>    """""",<<NEWL>>    )<<NEWL>><<NEWL>><<NEWL>>def test_multiple_named_operation():<<NEWL>>    expect_passes_rule(<<NEWL>>        LoneAnonymousOperation,<<NEWL>>        """"""<<NEWL>>      query Foo {<<NEWL>>        field<<NEWL>>      }<<NEWL>><<NEWL>>      query Bar {<<NEWL>>        field<<NEWL>>      }<<NEWL>>    """""",<<NEWL>>    )<<NEWL>><<NEWL>><<NEWL>>def test_anon_operation_with_fragment():<<NEWL>>    expect_passes_rule(<<NEWL>>        LoneAnonymousOperation,<<NEWL>>        """"""<<NEWL>>      {<<NEWL>>        ...Foo<<NEWL>>      }<<NEWL>>      fragment Foo on Type {<<NEWL>>        field<<NEWL>>      }<<NEWL>>    """""",<<NEWL>>    )<<NEWL>><<NEWL>><<NEWL>>def test_multiple_anon_operations():<<NEWL>>    expect_fails_rule(<<NEWL>>        LoneAnonymousOperation,<<NEWL>>        """"""<<NEWL>>      {<<NEWL>>        fieldA<<NEWL>>      }<<NEWL>>      {<<NEWL>>        fieldB<<NEWL>>      }<<NEWL>>    """""",<<NEWL>>        [anon_not_alone(2, 7), anon_not_alone(5, 7)],<<NEWL>>    )<<NEWL>><<NEWL>><<NEWL>>def test_anon_operation_with_a_mutation():<<NEWL>>    expect_fails_rule(<<NEWL>>        LoneAnonymousOperation,<<NEWL>>        """"""<<NEWL>>      {<<NEWL>>        fieldA<<NEWL>>      }<<NEWL>>      mutation Foo {<<NEWL>>        fieldB<<NEWL>>      }<<NEWL>>    """""",<<NEWL>>        [anon_not_alone(2, 7)],<<NEWL>>    )<<NEWL>><<NEWL>><<NEWL>>def test_anon_operation_with_a_subscription():<<NEWL>>    expect_fails_rule(<<NEWL>>        LoneAnonymousOperation,<<NEWL>>        """"""<<NEWL>>      {<<NEWL>>        fieldA<<NEWL>>      }<<NEWL>>      subscription Foo {<<NEWL>>        fieldB<<NEWL>>      }<<NEWL>>    """""",<<NEWL>>        [anon_not_alone(2, 7)],<<NEWL>>    )"
263	donghui	0	"from graphql.language.location import SourceLocation<<NEWL>>from graphql.validation.rules import LoneAnonymousOperation<<NEWL>><<NEWL>>from .utils import expect_fails_rule, expect_passes_rule<<NEWL>><<NEWL>><<NEWL>>def anon_not_alone(line, column):<<NEWL>>    return {<<NEWL>>        ""message"": LoneAnonymousOperation.anonymous_operation_not_alone_message(),<<NEWL>>        ""locations"": [SourceLocation(line, column)],<<NEWL>>    }<<NEWL>><<NEWL>><<NEWL>>def test_no_operations():<<NEWL>>    expect_passes_rule(<<NEWL>>        LoneAnonymousOperation,<<NEWL>>        """"""<<NEWL>>      fragment fragA on Type {<<NEWL>>        field<<NEWL>>      }<<NEWL>>    """""",<<NEWL>>    )<<NEWL>><<NEWL>><<NEWL>>def test_one_anon_operation():<<NEWL>>    expect_passes_rule(<<NEWL>>        LoneAnonymousOperation,<<NEWL>>        """"""<<NEWL>>      {<<NEWL>>        field<<NEWL>>      }<<NEWL>>    """""",<<NEWL>>    )<<NEWL>><<NEWL>><<NEWL>>def test_multiple_named_operation():<<NEWL>>    expect_passes_rule(<<NEWL>>        LoneAnonymousOperation,<<NEWL>>        """"""<<NEWL>>      query Foo {<<NEWL>>        field<<NEWL>>      }<<NEWL>><<NEWL>>      query Bar {<<NEWL>>        field<<NEWL>>      }<<NEWL>>    """""",<<NEWL>>    )<<NEWL>><<NEWL>><<NEWL>>def test_anon_operation_with_fragment():<<NEWL>>    expect_passes_rule(<<NEWL>>        LoneAnonymousOperation,<<NEWL>>        """"""<<NEWL>>      {<<NEWL>>        ...Foo<<NEWL>>      }<<NEWL>>      fragment Foo on Type {<<NEWL>>        field<<NEWL>>      }<<NEWL>>    """""",<<NEWL>>    )<<NEWL>><<NEWL>><<NEWL>>def test_multiple_anon_operations():<<NEWL>>    expect_fails_rule(<<NEWL>>        LoneAnonymousOperation,<<NEWL>>        """"""<<NEWL>>      {<<NEWL>>        fieldA<<NEWL>>      }<<NEWL>>      {<<NEWL>>        fieldB<<NEWL>>      }<<NEWL>>    """""",<<NEWL>>        [anon_not_alone(2, 7), anon_not_alone(5, 7)],<<NEWL>>    )<<NEWL>><<NEWL>><<NEWL>>def test_anon_operation_with_a_mutation():<<NEWL>>    expect_fails_rule(<<NEWL>>        LoneAnonymousOperation,<<NEWL>>        """"""<<NEWL>>      {<<NEWL>>        fieldA<<NEWL>>      }<<NEWL>>      mutation Foo {<<NEWL>>        fieldB<<NEWL>>      }<<NEWL>>    """""",<<NEWL>>        [anon_not_alone(2, 7)],<<NEWL>>    )<<NEWL>><<NEWL>><<NEWL>>def test_anon_operation_with_a_subscription():<<NEWL>>    expect_fails_rule(<<NEWL>>        LoneAnonymousOperation,<<NEWL>>        """"""<<NEWL>>      {<<NEWL>>        fieldA<<NEWL>>      }<<NEWL>>      subscription Foo {<<NEWL>>        fieldB<<NEWL>>      }<<NEWL>>    """""",<<NEWL>>        [anon_not_alone(2, 7)],<<NEWL>>    )"
323	jackson	4	"# -*- coding: utf-8 -*-<<NEWL>>""""""<<NEWL>>Charset-Normalizer<<NEWL>>~~~~~~~~~~~~~~<<NEWL>>The Real First Universal Charset Detector.<<NEWL>>A library that helps you read text from an unknown charset encoding.<<NEWL>>Motivated by chardet, This package is trying to resolve the issue by taking a new approach.<<NEWL>>All IANA character set names for which the Python core library provides codecs are supported.<<NEWL>><<NEWL>>Basic usage:<<NEWL>>   >>> from charset_normalizer import from_bytes<<NEWL>>   >>> results = from_bytes('Bсеки човек има право на образование. Oбразованието!'.encode('utf_8'))<<NEWL>>   >>> best_guess = results.best()<<NEWL>>   >>> str(best_guess)<<NEWL>>   'Bсеки човек има право на образование. Oбразованието!'<<NEWL>><<NEWL>>Others methods and usages are available - see the full documentation<<NEWL>>at <https://github.com/Ousret/charset_normalizer>.<<NEWL>>:copyright: (c) 2021 by Ahmed TAHRI<<NEWL>>:license: MIT, see LICENSE for more details.<<NEWL>>""""""<<NEWL>>import logging<<NEWL>><<NEWL>>from .api import from_bytes, from_fp, from_path, normalize<<NEWL>>from .legacy import (<<NEWL>>    CharsetDetector,<<NEWL>>    CharsetDoctor,<<NEWL>>    CharsetNormalizerMatch,<<NEWL>>    CharsetNormalizerMatches,<<NEWL>>    detect,<<NEWL>>)<<NEWL>>from .models import CharsetMatch, CharsetMatches<<NEWL>>from .utils import set_logging_handler<<NEWL>>from .version import VERSION, __version__<<NEWL>><<NEWL>>__all__ = (<<NEWL>>    ""from_fp"",<<NEWL>>    ""from_path"",<<NEWL>>    ""from_bytes"",<<NEWL>>    ""normalize"",<<NEWL>>    ""detect"",<<NEWL>>    ""CharsetMatch"",<<NEWL>>    ""CharsetMatches"",<<NEWL>>    ""CharsetNormalizerMatch"",<<NEWL>>    ""CharsetNormalizerMatches"",<<NEWL>>    ""CharsetDetector"",<<NEWL>>    ""CharsetDoctor"",<<NEWL>>    ""__version__"",<<NEWL>>    ""VERSION"",<<NEWL>>    ""set_logging_handler"",<<NEWL>>)<<NEWL>><<NEWL>># Attach a NullHandler to the top level logger by default<<NEWL>># https://docs.python.org/3.3/howto/logging.html#configuring-logging-for-a-library<<NEWL>><<NEWL>>logging.getLogger(""charset_normalizer"").addHandler(logging.NullHandler())"
323	donghui	3	"# -*- coding: utf-8 -*-<<NEWL>>""""""<<NEWL>>Charset-Normalizer<<NEWL>>~~~~~~~~~~~~~~<<NEWL>>The Real First Universal Charset Detector.<<NEWL>>A library that helps you read text from an unknown charset encoding.<<NEWL>>Motivated by chardet, This package is trying to resolve the issue by taking a new approach.<<NEWL>>All IANA character set names for which the Python core library provides codecs are supported.<<NEWL>><<NEWL>>Basic usage:<<NEWL>>   >>> from charset_normalizer import from_bytes<<NEWL>>   >>> results = from_bytes('Bсеки човек има право на образование. Oбразованието!'.encode('utf_8'))<<NEWL>>   >>> best_guess = results.best()<<NEWL>>   >>> str(best_guess)<<NEWL>>   'Bсеки човек има право на образование. Oбразованието!'<<NEWL>><<NEWL>>Others methods and usages are available - see the full documentation<<NEWL>>at <https://github.com/Ousret/charset_normalizer>.<<NEWL>>:copyright: (c) 2021 by Ahmed TAHRI<<NEWL>>:license: MIT, see LICENSE for more details.<<NEWL>>""""""<<NEWL>>import logging<<NEWL>><<NEWL>>from .api import from_bytes, from_fp, from_path, normalize<<NEWL>>from .legacy import (<<NEWL>>    CharsetDetector,<<NEWL>>    CharsetDoctor,<<NEWL>>    CharsetNormalizerMatch,<<NEWL>>    CharsetNormalizerMatches,<<NEWL>>    detect,<<NEWL>>)<<NEWL>>from .models import CharsetMatch, CharsetMatches<<NEWL>>from .utils import set_logging_handler<<NEWL>>from .version import VERSION, __version__<<NEWL>><<NEWL>>__all__ = (<<NEWL>>    ""from_fp"",<<NEWL>>    ""from_path"",<<NEWL>>    ""from_bytes"",<<NEWL>>    ""normalize"",<<NEWL>>    ""detect"",<<NEWL>>    ""CharsetMatch"",<<NEWL>>    ""CharsetMatches"",<<NEWL>>    ""CharsetNormalizerMatch"",<<NEWL>>    ""CharsetNormalizerMatches"",<<NEWL>>    ""CharsetDetector"",<<NEWL>>    ""CharsetDoctor"",<<NEWL>>    ""__version__"",<<NEWL>>    ""VERSION"",<<NEWL>>    ""set_logging_handler"",<<NEWL>>)<<NEWL>><<NEWL>># Attach a NullHandler to the top level logger by default<<NEWL>># https://docs.python.org/3.3/howto/logging.html#configuring-logging-for-a-library<<NEWL>><<NEWL>>logging.getLogger(""charset_normalizer"").addHandler(logging.NullHandler())"
372	jackson	4	"""""""uestc URL Configuration<<NEWL>><<NEWL>>The `urlpatterns` list routes URLs to views. For more information please see:<<NEWL>>    https://docs.djangoproject.com/en/1.9/topics/http/urls/<<NEWL>>Examples:<<NEWL>>Function views<<NEWL>>    1. Add an import:  from my_app import views<<NEWL>>    2. Add a URL to urlpatterns:  url(r'^$', views.home, name='home')<<NEWL>>Class-based views<<NEWL>>    1. Add an import:  from other_app.views import Home<<NEWL>>    2. Add a URL to urlpatterns:  url(r'^$', Home.as_view(), name='home')<<NEWL>>Including another URLconf<<NEWL>>    1. Import the include() function: from django.conf.urls import url, include<<NEWL>>    2. Add a URL to urlpatterns:  url(r'^blog/', include('blog.urls'))<<NEWL>>""""""<<NEWL>>from django.conf.urls import url<<NEWL>>from django.contrib import admin<<NEWL>>from subject import views<<NEWL>><<NEWL>>urlpatterns = [<<NEWL>>    url(r'^admin/$', views.admin_login, name='admin_login'),<<NEWL>>    url(r'^$', views.login, name='login'),<<NEWL>>    url(r'^login/$', views.user_login, name='user_login'),<<NEWL>>    url(r'^index/$', views.index, name='index'),<<NEWL>>    url(r'^admin/index/$', views.admin_index, name='admin_index'),<<NEWL>>    url(r'^course/$', views.get_course, name='get_course'),<<NEWL>>    url(r'^log/$', views.get_log, name='get_log'),<<NEWL>>    url(r'^choose/$', views.get_already_choose, name='get_already_choose'),<<NEWL>>    url(r'^logout/$', views.logout, name='logout'),<<NEWL>>    url(r'^select/$', views.select_course, name='select_course'),<<NEWL>>    url(r'^cancel/$', views.cancel_course, name='cancel_course'),<<NEWL>>    url(r'^admin/get/course/$', views.list_course, name='list_course'),<<NEWL>>    url(r'^admin/get/student/$', views.list_student, name='list_student'),<<NEWL>>    url(r'^admin/get/teacher/$', views.list_teacher, name='list_teacher'),<<NEWL>>    url(r'^admin/delete/course/$', views.delete_course, name='delete_course'),<<NEWL>>    url(r'^admin/add/course/$', views.add_course, name='add_course'),<<NEWL>>    url(r'^admin/add/student/$', views.add_student, name='add_student'),<<NEWL>>    url(r'^admin/resetPassword/$', views.reset_passwd, name='reset_passwd'),<<NEWL>>    url(r'^admin/delete/teacher/$', views.delete_teacher, name='delete_teacher'),<<NEWL>>    url(r'^admin/add/teacher/$', views.add_teacher, name='add_teacher'),<<NEWL>>    url(r'^search/$', views.search, name='search'),<<NEWL>>    url(r'^password/$', views.change_passwd, name='change_passwd'),<<NEWL>>]"
372	donghui	4	"""""""uestc URL Configuration<<NEWL>><<NEWL>>The `urlpatterns` list routes URLs to views. For more information please see:<<NEWL>>    https://docs.djangoproject.com/en/1.9/topics/http/urls/<<NEWL>>Examples:<<NEWL>>Function views<<NEWL>>    1. Add an import:  from my_app import views<<NEWL>>    2. Add a URL to urlpatterns:  url(r'^$', views.home, name='home')<<NEWL>>Class-based views<<NEWL>>    1. Add an import:  from other_app.views import Home<<NEWL>>    2. Add a URL to urlpatterns:  url(r'^$', Home.as_view(), name='home')<<NEWL>>Including another URLconf<<NEWL>>    1. Import the include() function: from django.conf.urls import url, include<<NEWL>>    2. Add a URL to urlpatterns:  url(r'^blog/', include('blog.urls'))<<NEWL>>""""""<<NEWL>>from django.conf.urls import url<<NEWL>>from django.contrib import admin<<NEWL>>from subject import views<<NEWL>><<NEWL>>urlpatterns = [<<NEWL>>    url(r'^admin/$', views.admin_login, name='admin_login'),<<NEWL>>    url(r'^$', views.login, name='login'),<<NEWL>>    url(r'^login/$', views.user_login, name='user_login'),<<NEWL>>    url(r'^index/$', views.index, name='index'),<<NEWL>>    url(r'^admin/index/$', views.admin_index, name='admin_index'),<<NEWL>>    url(r'^course/$', views.get_course, name='get_course'),<<NEWL>>    url(r'^log/$', views.get_log, name='get_log'),<<NEWL>>    url(r'^choose/$', views.get_already_choose, name='get_already_choose'),<<NEWL>>    url(r'^logout/$', views.logout, name='logout'),<<NEWL>>    url(r'^select/$', views.select_course, name='select_course'),<<NEWL>>    url(r'^cancel/$', views.cancel_course, name='cancel_course'),<<NEWL>>    url(r'^admin/get/course/$', views.list_course, name='list_course'),<<NEWL>>    url(r'^admin/get/student/$', views.list_student, name='list_student'),<<NEWL>>    url(r'^admin/get/teacher/$', views.list_teacher, name='list_teacher'),<<NEWL>>    url(r'^admin/delete/course/$', views.delete_course, name='delete_course'),<<NEWL>>    url(r'^admin/add/course/$', views.add_course, name='add_course'),<<NEWL>>    url(r'^admin/add/student/$', views.add_student, name='add_student'),<<NEWL>>    url(r'^admin/resetPassword/$', views.reset_passwd, name='reset_passwd'),<<NEWL>>    url(r'^admin/delete/teacher/$', views.delete_teacher, name='delete_teacher'),<<NEWL>>    url(r'^admin/add/teacher/$', views.add_teacher, name='add_teacher'),<<NEWL>>    url(r'^search/$', views.search, name='search'),<<NEWL>>    url(r'^password/$', views.change_passwd, name='change_passwd'),<<NEWL>>]"
381	jackson	3	"""""""<<NEWL>> The GeometryColumns and SpatialRefSys models for the Oracle spatial<<NEWL>> backend.<<NEWL>><<NEWL>> It should be noted that Oracle Spatial does not have database tables<<NEWL>> named according to the OGC standard, so the closest analogs are used.<<NEWL>> For example, the `USER_SDO_GEOM_METADATA` is used for the GeometryColumns<<NEWL>> model and the `SDO_COORD_REF_SYS` is used for the SpatialRefSys model.<<NEWL>>""""""<<NEWL>>from django.contrib.gis.db import models<<NEWL>>from django.contrib.gis.db.backends.base.models import SpatialRefSysMixin<<NEWL>><<NEWL>><<NEWL>>class OracleGeometryColumns(models.Model):<<NEWL>>    ""Maps to the Oracle USER_SDO_GEOM_METADATA table.""<<NEWL>>    table_name = models.CharField(max_length=32)<<NEWL>>    column_name = models.CharField(max_length=1024)<<NEWL>>    srid = models.IntegerField(primary_key=True)<<NEWL>>    # TODO: Add support for `diminfo` column (type MDSYS.SDO_DIM_ARRAY).<<NEWL>><<NEWL>>    class Meta:<<NEWL>>        app_label = ""gis""<<NEWL>>        db_table = ""USER_SDO_GEOM_METADATA""<<NEWL>>        managed = False<<NEWL>><<NEWL>>    def __str__(self):<<NEWL>>        return ""%s - %s (SRID: %s)"" % (self.table_name, self.column_name, self.srid)<<NEWL>><<NEWL>>    @classmethod<<NEWL>>    def table_name_col(cls):<<NEWL>>        """"""<<NEWL>>        Return the name of the metadata column used to store the feature table<<NEWL>>        name.<<NEWL>>        """"""<<NEWL>>        return ""table_name""<<NEWL>><<NEWL>>    @classmethod<<NEWL>>    def geom_col_name(cls):<<NEWL>>        """"""<<NEWL>>        Return the name of the metadata column used to store the feature<<NEWL>>        geometry column.<<NEWL>>        """"""<<NEWL>>        return ""column_name""<<NEWL>><<NEWL>><<NEWL>>class OracleSpatialRefSys(models.Model, SpatialRefSysMixin):<<NEWL>>    ""Maps to the Oracle MDSYS.CS_SRS table.""<<NEWL>>    cs_name = models.CharField(max_length=68)<<NEWL>>    srid = models.IntegerField(primary_key=True)<<NEWL>>    auth_srid = models.IntegerField()<<NEWL>>    auth_name = models.CharField(max_length=256)<<NEWL>>    wktext = models.CharField(max_length=2046)<<NEWL>>    # Optional geometry representing the bounds of this coordinate<<NEWL>>    # system.  By default, all are NULL in the table.<<NEWL>>    cs_bounds = models.PolygonField(null=True)<<NEWL>><<NEWL>>    class Meta:<<NEWL>>        app_label = ""gis""<<NEWL>>        db_table = ""CS_SRS""<<NEWL>>        managed = False<<NEWL>><<NEWL>>    @property<<NEWL>>    def wkt(self):<<NEWL>>        return self.wktext"
381	donghui	2	"""""""<<NEWL>> The GeometryColumns and SpatialRefSys models for the Oracle spatial<<NEWL>> backend.<<NEWL>><<NEWL>> It should be noted that Oracle Spatial does not have database tables<<NEWL>> named according to the OGC standard, so the closest analogs are used.<<NEWL>> For example, the `USER_SDO_GEOM_METADATA` is used for the GeometryColumns<<NEWL>> model and the `SDO_COORD_REF_SYS` is used for the SpatialRefSys model.<<NEWL>>""""""<<NEWL>>from django.contrib.gis.db import models<<NEWL>>from django.contrib.gis.db.backends.base.models import SpatialRefSysMixin<<NEWL>><<NEWL>><<NEWL>>class OracleGeometryColumns(models.Model):<<NEWL>>    ""Maps to the Oracle USER_SDO_GEOM_METADATA table.""<<NEWL>>    table_name = models.CharField(max_length=32)<<NEWL>>    column_name = models.CharField(max_length=1024)<<NEWL>>    srid = models.IntegerField(primary_key=True)<<NEWL>>    # TODO: Add support for `diminfo` column (type MDSYS.SDO_DIM_ARRAY).<<NEWL>><<NEWL>>    class Meta:<<NEWL>>        app_label = ""gis""<<NEWL>>        db_table = ""USER_SDO_GEOM_METADATA""<<NEWL>>        managed = False<<NEWL>><<NEWL>>    def __str__(self):<<NEWL>>        return ""%s - %s (SRID: %s)"" % (self.table_name, self.column_name, self.srid)<<NEWL>><<NEWL>>    @classmethod<<NEWL>>    def table_name_col(cls):<<NEWL>>        """"""<<NEWL>>        Return the name of the metadata column used to store the feature table<<NEWL>>        name.<<NEWL>>        """"""<<NEWL>>        return ""table_name""<<NEWL>><<NEWL>>    @classmethod<<NEWL>>    def geom_col_name(cls):<<NEWL>>        """"""<<NEWL>>        Return the name of the metadata column used to store the feature<<NEWL>>        geometry column.<<NEWL>>        """"""<<NEWL>>        return ""column_name""<<NEWL>><<NEWL>><<NEWL>>class OracleSpatialRefSys(models.Model, SpatialRefSysMixin):<<NEWL>>    ""Maps to the Oracle MDSYS.CS_SRS table.""<<NEWL>>    cs_name = models.CharField(max_length=68)<<NEWL>>    srid = models.IntegerField(primary_key=True)<<NEWL>>    auth_srid = models.IntegerField()<<NEWL>>    auth_name = models.CharField(max_length=256)<<NEWL>>    wktext = models.CharField(max_length=2046)<<NEWL>>    # Optional geometry representing the bounds of this coordinate<<NEWL>>    # system.  By default, all are NULL in the table.<<NEWL>>    cs_bounds = models.PolygonField(null=True)<<NEWL>><<NEWL>>    class Meta:<<NEWL>>        app_label = ""gis""<<NEWL>>        db_table = ""CS_SRS""<<NEWL>>        managed = False<<NEWL>><<NEWL>>    @property<<NEWL>>    def wkt(self):<<NEWL>>        return self.wktext"
290	jackson	0	# Copyright (c) 2020, Oracle and/or its affiliates.<<NEWL>>#<<NEWL>># This program is free software; you can redistribute it and/or modify<<NEWL>># it under the terms of the GNU General Public License, version 2.0, as<<NEWL>># published by the Free Software Foundation.<<NEWL>>#<<NEWL>># This program is also distributed with certain software (including<<NEWL>># but not limited to OpenSSL) that is licensed under separate terms,<<NEWL>># as designated in a particular file or component or in included license<<NEWL>># documentation.  The authors of MySQL hereby grant you an<<NEWL>># additional permission to link the program and your derivative works<<NEWL>># with the separately licensed software that they have included with<<NEWL>># MySQL.<<NEWL>>#<<NEWL>># Without limiting anything contained in the foregoing, this file,<<NEWL>># which is part of MySQL Connector/Python, is also subject to the<<NEWL>># Universal FOSS Exception, version 1.0, a copy of which can be found at<<NEWL>># http://oss.oracle.com/licenses/universal-foss-exception.<<NEWL>>#<<NEWL>># This program is distributed in the hope that it will be useful, but<<NEWL>># WITHOUT ANY WARRANTY; without even the implied warranty of<<NEWL>># MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.<<NEWL>># See the GNU General Public License, version 2.0, for more details.<<NEWL>>#<<NEWL>># You should have received a copy of the GNU General Public License<<NEWL>># along with this program; if not, write to the Free Software Foundation, Inc.,<<NEWL>># 51 Franklin St, Fifth Floor, Boston, MA 02110-1301  USA<<NEWL>><<NEWL>>from django.db.backends.mysql.schema import DatabaseSchemaEditor as MySQLDatabaseSchemaEditor<<NEWL>><<NEWL>><<NEWL>>class DatabaseSchemaEditor(MySQLDatabaseSchemaEditor):<<NEWL>><<NEWL>>    def quote_value(self, value):<<NEWL>>        self.connection.ensure_connection()<<NEWL>>        if isinstance(value, str):<<NEWL>>            value = value.replace('%', '%%')<<NEWL>>        quoted = self.connection.connection.converter.escape(value)<<NEWL>>        if isinstance(value, str) and isinstance(quoted, bytes):<<NEWL>>            quoted = quoted.decode()<<NEWL>>        return quoted
290	donghui	0	# Copyright (c) 2020, Oracle and/or its affiliates.<<NEWL>>#<<NEWL>># This program is free software; you can redistribute it and/or modify<<NEWL>># it under the terms of the GNU General Public License, version 2.0, as<<NEWL>># published by the Free Software Foundation.<<NEWL>>#<<NEWL>># This program is also distributed with certain software (including<<NEWL>># but not limited to OpenSSL) that is licensed under separate terms,<<NEWL>># as designated in a particular file or component or in included license<<NEWL>># documentation.  The authors of MySQL hereby grant you an<<NEWL>># additional permission to link the program and your derivative works<<NEWL>># with the separately licensed software that they have included with<<NEWL>># MySQL.<<NEWL>>#<<NEWL>># Without limiting anything contained in the foregoing, this file,<<NEWL>># which is part of MySQL Connector/Python, is also subject to the<<NEWL>># Universal FOSS Exception, version 1.0, a copy of which can be found at<<NEWL>># http://oss.oracle.com/licenses/universal-foss-exception.<<NEWL>>#<<NEWL>># This program is distributed in the hope that it will be useful, but<<NEWL>># WITHOUT ANY WARRANTY; without even the implied warranty of<<NEWL>># MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.<<NEWL>># See the GNU General Public License, version 2.0, for more details.<<NEWL>>#<<NEWL>># You should have received a copy of the GNU General Public License<<NEWL>># along with this program; if not, write to the Free Software Foundation, Inc.,<<NEWL>># 51 Franklin St, Fifth Floor, Boston, MA 02110-1301  USA<<NEWL>><<NEWL>>from django.db.backends.mysql.schema import DatabaseSchemaEditor as MySQLDatabaseSchemaEditor<<NEWL>><<NEWL>><<NEWL>>class DatabaseSchemaEditor(MySQLDatabaseSchemaEditor):<<NEWL>><<NEWL>>    def quote_value(self, value):<<NEWL>>        self.connection.ensure_connection()<<NEWL>>        if isinstance(value, str):<<NEWL>>            value = value.replace('%', '%%')<<NEWL>>        quoted = self.connection.connection.converter.escape(value)<<NEWL>>        if isinstance(value, str) and isinstance(quoted, bytes):<<NEWL>>            quoted = quoted.decode()<<NEWL>>        return quoted
352	jackson	0	"from time import time<<NEWL>><<NEWL>>from bot import DOWNLOAD_DIR, LOGGER<<NEWL>>from bot.helper.ext_utils.bot_utils import get_readable_file_size, MirrorStatus, EngineStatus, get_readable_time<<NEWL>>from bot.helper.ext_utils.fs_utils import get_path_size<<NEWL>><<NEWL>>class ZipStatus:<<NEWL>>    def __init__(self, name, size, gid, listener):<<NEWL>>        self.__name = name<<NEWL>>        self.__size = size<<NEWL>>        self.__gid = gid<<NEWL>>        self.__listener = listener<<NEWL>>        self.__uid = listener.uid<<NEWL>>        self.__start_time = time()<<NEWL>>        self.message = listener.message<<NEWL>><<NEWL>>    def gid(self):<<NEWL>>        return self.__gid<<NEWL>><<NEWL>>    def speed_raw(self):<<NEWL>>        return self.processed_bytes() / (time() - self.__start_time)<<NEWL>><<NEWL>>    def progress_raw(self):<<NEWL>>        try:<<NEWL>>            return self.processed_bytes() / self.__size * 100<<NEWL>>        except:<<NEWL>>            return 0<<NEWL>><<NEWL>>    def progress(self):<<NEWL>>        return f'{round(self.progress_raw(), 2)}%'<<NEWL>><<NEWL>>    def speed(self):<<NEWL>>        return f'{get_readable_file_size(self.speed_raw())}/s'<<NEWL>><<NEWL>>    def name(self):<<NEWL>>        return self.__name<<NEWL>><<NEWL>>    def size_raw(self):<<NEWL>>        return self.__size<<NEWL>><<NEWL>>    def size(self):<<NEWL>>        return get_readable_file_size(self.__size)<<NEWL>><<NEWL>>    def eta(self):<<NEWL>>        try:<<NEWL>>            seconds = (self.size_raw() - self.processed_bytes()) / self.speed_raw()<<NEWL>>            return f'{get_readable_time(seconds)}'<<NEWL>>        except:<<NEWL>>            return '-'<<NEWL>><<NEWL>>    def status(self):<<NEWL>>        return MirrorStatus.STATUS_ARCHIVING<<NEWL>><<NEWL>>    def processed_bytes(self):<<NEWL>>        if self.__listener.newDir:<<NEWL>>            return get_path_size(f""{DOWNLOAD_DIR}{self.__uid}10000"")<<NEWL>>        else:<<NEWL>>            return get_path_size(f""{DOWNLOAD_DIR}{self.__uid}"") - self.__size<<NEWL>><<NEWL>>    def download(self):<<NEWL>>        return self<<NEWL>><<NEWL>>    def cancel_download(self):<<NEWL>>        LOGGER.info(f'Cancelling Archive: {self.__name}')<<NEWL>>        if self.__listener.suproc is not None:<<NEWL>>            self.__listener.suproc.kill()<<NEWL>>        self.__listener.onUploadError('archiving stopped by user!')<<NEWL>><<NEWL>>    def eng(self):<<NEWL>>        return EngineStatus.STATUS_ZIP"
352	donghui	0	"from time import time<<NEWL>><<NEWL>>from bot import DOWNLOAD_DIR, LOGGER<<NEWL>>from bot.helper.ext_utils.bot_utils import get_readable_file_size, MirrorStatus, EngineStatus, get_readable_time<<NEWL>>from bot.helper.ext_utils.fs_utils import get_path_size<<NEWL>><<NEWL>>class ZipStatus:<<NEWL>>    def __init__(self, name, size, gid, listener):<<NEWL>>        self.__name = name<<NEWL>>        self.__size = size<<NEWL>>        self.__gid = gid<<NEWL>>        self.__listener = listener<<NEWL>>        self.__uid = listener.uid<<NEWL>>        self.__start_time = time()<<NEWL>>        self.message = listener.message<<NEWL>><<NEWL>>    def gid(self):<<NEWL>>        return self.__gid<<NEWL>><<NEWL>>    def speed_raw(self):<<NEWL>>        return self.processed_bytes() / (time() - self.__start_time)<<NEWL>><<NEWL>>    def progress_raw(self):<<NEWL>>        try:<<NEWL>>            return self.processed_bytes() / self.__size * 100<<NEWL>>        except:<<NEWL>>            return 0<<NEWL>><<NEWL>>    def progress(self):<<NEWL>>        return f'{round(self.progress_raw(), 2)}%'<<NEWL>><<NEWL>>    def speed(self):<<NEWL>>        return f'{get_readable_file_size(self.speed_raw())}/s'<<NEWL>><<NEWL>>    def name(self):<<NEWL>>        return self.__name<<NEWL>><<NEWL>>    def size_raw(self):<<NEWL>>        return self.__size<<NEWL>><<NEWL>>    def size(self):<<NEWL>>        return get_readable_file_size(self.__size)<<NEWL>><<NEWL>>    def eta(self):<<NEWL>>        try:<<NEWL>>            seconds = (self.size_raw() - self.processed_bytes()) / self.speed_raw()<<NEWL>>            return f'{get_readable_time(seconds)}'<<NEWL>>        except:<<NEWL>>            return '-'<<NEWL>><<NEWL>>    def status(self):<<NEWL>>        return MirrorStatus.STATUS_ARCHIVING<<NEWL>><<NEWL>>    def processed_bytes(self):<<NEWL>>        if self.__listener.newDir:<<NEWL>>            return get_path_size(f""{DOWNLOAD_DIR}{self.__uid}10000"")<<NEWL>>        else:<<NEWL>>            return get_path_size(f""{DOWNLOAD_DIR}{self.__uid}"") - self.__size<<NEWL>><<NEWL>>    def download(self):<<NEWL>>        return self<<NEWL>><<NEWL>>    def cancel_download(self):<<NEWL>>        LOGGER.info(f'Cancelling Archive: {self.__name}')<<NEWL>>        if self.__listener.suproc is not None:<<NEWL>>            self.__listener.suproc.kill()<<NEWL>>        self.__listener.onUploadError('archiving stopped by user!')<<NEWL>><<NEWL>>    def eng(self):<<NEWL>>        return EngineStatus.STATUS_ZIP"
303	jackson	4	"#  Copyright 2022 Google LLC<<NEWL>>#<<NEWL>>#  Licensed under the Apache License, Version 2.0 (the ""License"");<<NEWL>>#  you may not use this file except in compliance with the License.<<NEWL>>#  You may obtain a copy of the License at<<NEWL>>#<<NEWL>>#      http://www.apache.org/licenses/LICENSE-2.0<<NEWL>>#<<NEWL>>#  Unless required by applicable law or agreed to in writing, software<<NEWL>>#  distributed under the License is distributed on an ""AS IS"" BASIS,<<NEWL>>#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.<<NEWL>>#  See the License for the specific language governing permissions and<<NEWL>>#  limitations under the License.<<NEWL>><<NEWL>><<NEWL>># This is an ingredient file. It is not meant to be run directly. Check the samples/snippets<<NEWL>># folder for complete code samples that are ready to be used.<<NEWL>># Disabling flake8 for the ingredients file, as it would fail F821 - undefined name check.<<NEWL>># flake8: noqa<<NEWL>>from google.cloud import compute_v1<<NEWL>><<NEWL>><<NEWL>># <INGREDIENT set_deprecation_status><<NEWL>>def set_deprecation_status(project_id: str, image_name: str, status: compute_v1.DeprecationStatus.State) -> None:<<NEWL>>    """"""<<NEWL>>    Modify the deprecation status of an image.<<NEWL>><<NEWL>>    Note: Image objects by default don't have the `deprecated` attribute at all unless it's set.<<NEWL>><<NEWL>>    Args:<<NEWL>>        project_id: project ID or project number of the Cloud project that hosts the image.<<NEWL>>        image_name: name of the image you want to modify<<NEWL>>        status: the status you want to set for the image. Available values are available in<<NEWL>>            `compute_v1.DeprecationStatus.State` enum. Learn more about image deprecation statuses:<<NEWL>>            https://cloud.google.com/compute/docs/images/create-delete-deprecate-private-images#deprecation-states<<NEWL>>    """"""<<NEWL>>    image_client = compute_v1.ImagesClient()<<NEWL>>    deprecation_status = compute_v1.DeprecationStatus()<<NEWL>>    deprecation_status.state = status.name<<NEWL>>    operation = image_client.deprecate(project=project_id, image=image_name,<<NEWL>>                                       deprecation_status_resource=deprecation_status)<<NEWL>><<NEWL>>    wait_for_extended_operation(operation, ""changing deprecation state of an image"")<<NEWL>># </INGREDIENT>"
303	donghui	4	"#  Copyright 2022 Google LLC<<NEWL>>#<<NEWL>>#  Licensed under the Apache License, Version 2.0 (the ""License"");<<NEWL>>#  you may not use this file except in compliance with the License.<<NEWL>>#  You may obtain a copy of the License at<<NEWL>>#<<NEWL>>#      http://www.apache.org/licenses/LICENSE-2.0<<NEWL>>#<<NEWL>>#  Unless required by applicable law or agreed to in writing, software<<NEWL>>#  distributed under the License is distributed on an ""AS IS"" BASIS,<<NEWL>>#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.<<NEWL>>#  See the License for the specific language governing permissions and<<NEWL>>#  limitations under the License.<<NEWL>><<NEWL>><<NEWL>># This is an ingredient file. It is not meant to be run directly. Check the samples/snippets<<NEWL>># folder for complete code samples that are ready to be used.<<NEWL>># Disabling flake8 for the ingredients file, as it would fail F821 - undefined name check.<<NEWL>># flake8: noqa<<NEWL>>from google.cloud import compute_v1<<NEWL>><<NEWL>><<NEWL>># <INGREDIENT set_deprecation_status><<NEWL>>def set_deprecation_status(project_id: str, image_name: str, status: compute_v1.DeprecationStatus.State) -> None:<<NEWL>>    """"""<<NEWL>>    Modify the deprecation status of an image.<<NEWL>><<NEWL>>    Note: Image objects by default don't have the `deprecated` attribute at all unless it's set.<<NEWL>><<NEWL>>    Args:<<NEWL>>        project_id: project ID or project number of the Cloud project that hosts the image.<<NEWL>>        image_name: name of the image you want to modify<<NEWL>>        status: the status you want to set for the image. Available values are available in<<NEWL>>            `compute_v1.DeprecationStatus.State` enum. Learn more about image deprecation statuses:<<NEWL>>            https://cloud.google.com/compute/docs/images/create-delete-deprecate-private-images#deprecation-states<<NEWL>>    """"""<<NEWL>>    image_client = compute_v1.ImagesClient()<<NEWL>>    deprecation_status = compute_v1.DeprecationStatus()<<NEWL>>    deprecation_status.state = status.name<<NEWL>>    operation = image_client.deprecate(project=project_id, image=image_name,<<NEWL>>                                       deprecation_status_resource=deprecation_status)<<NEWL>><<NEWL>>    wait_for_extended_operation(operation, ""changing deprecation state of an image"")<<NEWL>># </INGREDIENT>"
395	jackson	0	# -*- coding: utf-8 -*-<<NEWL>># Copyright (C) 2006-2007 Søren Roug, European Environment Agency<<NEWL>>#<<NEWL>># This library is free software; you can redistribute it and/or<<NEWL>># modify it under the terms of the GNU Lesser General Public<<NEWL>># License as published by the Free Software Foundation; either<<NEWL>># version 2.1 of the License, or (at your option) any later version.<<NEWL>>#<<NEWL>># This library is distributed in the hope that it will be useful,<<NEWL>># but WITHOUT ANY WARRANTY; without even the implied warranty of<<NEWL>># MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU<<NEWL>># Lesser General Public License for more details.<<NEWL>>#<<NEWL>># You should have received a copy of the GNU Lesser General Public<<NEWL>># License along with this library; if not, write to the Free Software<<NEWL>># Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301  USA<<NEWL>>#<<NEWL>># Contributor(s):<<NEWL>>#<<NEWL>><<NEWL>>from odf.namespaces import ANIMNS<<NEWL>>from odf.element import Element<<NEWL>><<NEWL>><<NEWL>># Autogenerated<<NEWL>>def Animate(**args):<<NEWL>>    return Element(qname = (ANIMNS,'animate'), **args)<<NEWL>><<NEWL>>def Animatecolor(**args):<<NEWL>>    return Element(qname = (ANIMNS,'animateColor'), **args)<<NEWL>><<NEWL>>def Animatemotion(**args):<<NEWL>>    return Element(qname = (ANIMNS,'animateMotion'), **args)<<NEWL>><<NEWL>>def Animatetransform(**args):<<NEWL>>    return Element(qname = (ANIMNS,'animateTransform'), **args)<<NEWL>><<NEWL>>def Audio(**args):<<NEWL>>    return Element(qname = (ANIMNS,'audio'), **args)<<NEWL>><<NEWL>>def Command(**args):<<NEWL>>    return Element(qname = (ANIMNS,'command'), **args)<<NEWL>><<NEWL>>def Iterate(**args):<<NEWL>>    return Element(qname = (ANIMNS,'iterate'), **args)<<NEWL>><<NEWL>>def Par(**args):<<NEWL>>    return Element(qname = (ANIMNS,'par'), **args)<<NEWL>><<NEWL>>def Param(**args):<<NEWL>>    return Element(qname = (ANIMNS,'param'), **args)<<NEWL>><<NEWL>>def Seq(**args):<<NEWL>>    return Element(qname = (ANIMNS,'seq'), **args)<<NEWL>><<NEWL>>def Set(**args):<<NEWL>>    return Element(qname = (ANIMNS,'set'), **args)<<NEWL>><<NEWL>>def Transitionfilter(**args):<<NEWL>>    return Element(qname = (ANIMNS,'transitionFilter'), **args)<<NEWL>>
395	donghui	0	# -*- coding: utf-8 -*-<<NEWL>># Copyright (C) 2006-2007 Søren Roug, European Environment Agency<<NEWL>>#<<NEWL>># This library is free software; you can redistribute it and/or<<NEWL>># modify it under the terms of the GNU Lesser General Public<<NEWL>># License as published by the Free Software Foundation; either<<NEWL>># version 2.1 of the License, or (at your option) any later version.<<NEWL>>#<<NEWL>># This library is distributed in the hope that it will be useful,<<NEWL>># but WITHOUT ANY WARRANTY; without even the implied warranty of<<NEWL>># MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU<<NEWL>># Lesser General Public License for more details.<<NEWL>>#<<NEWL>># You should have received a copy of the GNU Lesser General Public<<NEWL>># License along with this library; if not, write to the Free Software<<NEWL>># Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301  USA<<NEWL>>#<<NEWL>># Contributor(s):<<NEWL>>#<<NEWL>><<NEWL>>from odf.namespaces import ANIMNS<<NEWL>>from odf.element import Element<<NEWL>><<NEWL>><<NEWL>># Autogenerated<<NEWL>>def Animate(**args):<<NEWL>>    return Element(qname = (ANIMNS,'animate'), **args)<<NEWL>><<NEWL>>def Animatecolor(**args):<<NEWL>>    return Element(qname = (ANIMNS,'animateColor'), **args)<<NEWL>><<NEWL>>def Animatemotion(**args):<<NEWL>>    return Element(qname = (ANIMNS,'animateMotion'), **args)<<NEWL>><<NEWL>>def Animatetransform(**args):<<NEWL>>    return Element(qname = (ANIMNS,'animateTransform'), **args)<<NEWL>><<NEWL>>def Audio(**args):<<NEWL>>    return Element(qname = (ANIMNS,'audio'), **args)<<NEWL>><<NEWL>>def Command(**args):<<NEWL>>    return Element(qname = (ANIMNS,'command'), **args)<<NEWL>><<NEWL>>def Iterate(**args):<<NEWL>>    return Element(qname = (ANIMNS,'iterate'), **args)<<NEWL>><<NEWL>>def Par(**args):<<NEWL>>    return Element(qname = (ANIMNS,'par'), **args)<<NEWL>><<NEWL>>def Param(**args):<<NEWL>>    return Element(qname = (ANIMNS,'param'), **args)<<NEWL>><<NEWL>>def Seq(**args):<<NEWL>>    return Element(qname = (ANIMNS,'seq'), **args)<<NEWL>><<NEWL>>def Set(**args):<<NEWL>>    return Element(qname = (ANIMNS,'set'), **args)<<NEWL>><<NEWL>>def Transitionfilter(**args):<<NEWL>>    return Element(qname = (ANIMNS,'transitionFilter'), **args)<<NEWL>>
284	jackson	3	"""""""<<NEWL>> The GeometryColumns and SpatialRefSys models for the PostGIS backend.<<NEWL>>""""""<<NEWL>>from django.contrib.gis.db.backends.base.models import SpatialRefSysMixin<<NEWL>>from django.db import models<<NEWL>><<NEWL>><<NEWL>>class PostGISGeometryColumns(models.Model):<<NEWL>>    """"""<<NEWL>>    The 'geometry_columns' view from PostGIS. See the PostGIS<<NEWL>>    documentation at Ch. 4.3.2.<<NEWL>>    """"""<<NEWL>>    f_table_catalog = models.CharField(max_length=256)<<NEWL>>    f_table_schema = models.CharField(max_length=256)<<NEWL>>    f_table_name = models.CharField(max_length=256)<<NEWL>>    f_geometry_column = models.CharField(max_length=256)<<NEWL>>    coord_dimension = models.IntegerField()<<NEWL>>    srid = models.IntegerField(primary_key=True)<<NEWL>>    type = models.CharField(max_length=30)<<NEWL>><<NEWL>>    class Meta:<<NEWL>>        app_label = 'gis'<<NEWL>>        db_table = 'geometry_columns'<<NEWL>>        managed = False<<NEWL>><<NEWL>>    def __str__(self):<<NEWL>>        return '%s.%s - %dD %s field (SRID: %d)' % (<<NEWL>>            self.f_table_name,<<NEWL>>            self.f_geometry_column,<<NEWL>>            self.coord_dimension,<<NEWL>>            self.type,<<NEWL>>            self.srid,<<NEWL>>        )<<NEWL>><<NEWL>>    @classmethod<<NEWL>>    def table_name_col(cls):<<NEWL>>        """"""<<NEWL>>        Return the name of the metadata column used to store the feature table<<NEWL>>        name.<<NEWL>>        """"""<<NEWL>>        return 'f_table_name'<<NEWL>><<NEWL>>    @classmethod<<NEWL>>    def geom_col_name(cls):<<NEWL>>        """"""<<NEWL>>        Return the name of the metadata column used to store the feature<<NEWL>>        geometry column.<<NEWL>>        """"""<<NEWL>>        return 'f_geometry_column'<<NEWL>><<NEWL>><<NEWL>>class PostGISSpatialRefSys(models.Model, SpatialRefSysMixin):<<NEWL>>    """"""<<NEWL>>    The 'spatial_ref_sys' table from PostGIS. See the PostGIS<<NEWL>>    documentation at Ch. 4.2.1.<<NEWL>>    """"""<<NEWL>>    srid = models.IntegerField(primary_key=True)<<NEWL>>    auth_name = models.CharField(max_length=256)<<NEWL>>    auth_srid = models.IntegerField()<<NEWL>>    srtext = models.CharField(max_length=2048)<<NEWL>>    proj4text = models.CharField(max_length=2048)<<NEWL>><<NEWL>>    class Meta:<<NEWL>>        app_label = 'gis'<<NEWL>>        db_table = 'spatial_ref_sys'<<NEWL>>        managed = False<<NEWL>><<NEWL>>    @property<<NEWL>>    def wkt(self):<<NEWL>>        return self.srtext"
284	donghui	2	"""""""<<NEWL>> The GeometryColumns and SpatialRefSys models for the PostGIS backend.<<NEWL>>""""""<<NEWL>>from django.contrib.gis.db.backends.base.models import SpatialRefSysMixin<<NEWL>>from django.db import models<<NEWL>><<NEWL>><<NEWL>>class PostGISGeometryColumns(models.Model):<<NEWL>>    """"""<<NEWL>>    The 'geometry_columns' view from PostGIS. See the PostGIS<<NEWL>>    documentation at Ch. 4.3.2.<<NEWL>>    """"""<<NEWL>>    f_table_catalog = models.CharField(max_length=256)<<NEWL>>    f_table_schema = models.CharField(max_length=256)<<NEWL>>    f_table_name = models.CharField(max_length=256)<<NEWL>>    f_geometry_column = models.CharField(max_length=256)<<NEWL>>    coord_dimension = models.IntegerField()<<NEWL>>    srid = models.IntegerField(primary_key=True)<<NEWL>>    type = models.CharField(max_length=30)<<NEWL>><<NEWL>>    class Meta:<<NEWL>>        app_label = 'gis'<<NEWL>>        db_table = 'geometry_columns'<<NEWL>>        managed = False<<NEWL>><<NEWL>>    def __str__(self):<<NEWL>>        return '%s.%s - %dD %s field (SRID: %d)' % (<<NEWL>>            self.f_table_name,<<NEWL>>            self.f_geometry_column,<<NEWL>>            self.coord_dimension,<<NEWL>>            self.type,<<NEWL>>            self.srid,<<NEWL>>        )<<NEWL>><<NEWL>>    @classmethod<<NEWL>>    def table_name_col(cls):<<NEWL>>        """"""<<NEWL>>        Return the name of the metadata column used to store the feature table<<NEWL>>        name.<<NEWL>>        """"""<<NEWL>>        return 'f_table_name'<<NEWL>><<NEWL>>    @classmethod<<NEWL>>    def geom_col_name(cls):<<NEWL>>        """"""<<NEWL>>        Return the name of the metadata column used to store the feature<<NEWL>>        geometry column.<<NEWL>>        """"""<<NEWL>>        return 'f_geometry_column'<<NEWL>><<NEWL>><<NEWL>>class PostGISSpatialRefSys(models.Model, SpatialRefSysMixin):<<NEWL>>    """"""<<NEWL>>    The 'spatial_ref_sys' table from PostGIS. See the PostGIS<<NEWL>>    documentation at Ch. 4.2.1.<<NEWL>>    """"""<<NEWL>>    srid = models.IntegerField(primary_key=True)<<NEWL>>    auth_name = models.CharField(max_length=256)<<NEWL>>    auth_srid = models.IntegerField()<<NEWL>>    srtext = models.CharField(max_length=2048)<<NEWL>>    proj4text = models.CharField(max_length=2048)<<NEWL>><<NEWL>>    class Meta:<<NEWL>>        app_label = 'gis'<<NEWL>>        db_table = 'spatial_ref_sys'<<NEWL>>        managed = False<<NEWL>><<NEWL>>    @property<<NEWL>>    def wkt(self):<<NEWL>>        return self.srtext"
337	jackson	0	######################## BEGIN LICENSE BLOCK ########################<<NEWL>># The Original Code is Mozilla Universal charset detector code.<<NEWL>>#<<NEWL>># The Initial Developer of the Original Code is<<NEWL>># Netscape Communications Corporation.<<NEWL>># Portions created by the Initial Developer are Copyright (C) 2001<<NEWL>># the Initial Developer. All Rights Reserved.<<NEWL>>#<<NEWL>># Contributor(s):<<NEWL>>#   Mark Pilgrim - port to Python<<NEWL>>#   Shy Shalom - original C code<<NEWL>>#   Proofpoint, Inc.<<NEWL>>#<<NEWL>># This library is free software; you can redistribute it and/or<<NEWL>># modify it under the terms of the GNU Lesser General Public<<NEWL>># License as published by the Free Software Foundation; either<<NEWL>># version 2.1 of the License, or (at your option) any later version.<<NEWL>>#<<NEWL>># This library is distributed in the hope that it will be useful,<<NEWL>># but WITHOUT ANY WARRANTY; without even the implied warranty of<<NEWL>># MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU<<NEWL>># Lesser General Public License for more details.<<NEWL>>#<<NEWL>># You should have received a copy of the GNU Lesser General Public<<NEWL>># License along with this library; if not, write to the Free Software<<NEWL>># Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA<<NEWL>># 02110-1301  USA<<NEWL>>######################### END LICENSE BLOCK #########################<<NEWL>><<NEWL>>from .charsetgroupprober import CharSetGroupProber<<NEWL>>from .utf8prober import UTF8Prober<<NEWL>>from .sjisprober import SJISProber<<NEWL>>from .eucjpprober import EUCJPProber<<NEWL>>from .gb2312prober import GB2312Prober<<NEWL>>from .euckrprober import EUCKRProber<<NEWL>>from .cp949prober import CP949Prober<<NEWL>>from .big5prober import Big5Prober<<NEWL>>from .euctwprober import EUCTWProber<<NEWL>><<NEWL>><<NEWL>>class MBCSGroupProber(CharSetGroupProber):<<NEWL>>    def __init__(self, lang_filter=None):<<NEWL>>        super(MBCSGroupProber, self).__init__(lang_filter=lang_filter)<<NEWL>>        self.probers = [<<NEWL>>            UTF8Prober(),<<NEWL>>            SJISProber(),<<NEWL>>            EUCJPProber(),<<NEWL>>            GB2312Prober(),<<NEWL>>            EUCKRProber(),<<NEWL>>            CP949Prober(),<<NEWL>>            Big5Prober(),<<NEWL>>            EUCTWProber()<<NEWL>>        ]<<NEWL>>        self.reset()
337	donghui	0	######################## BEGIN LICENSE BLOCK ########################<<NEWL>># The Original Code is Mozilla Universal charset detector code.<<NEWL>>#<<NEWL>># The Initial Developer of the Original Code is<<NEWL>># Netscape Communications Corporation.<<NEWL>># Portions created by the Initial Developer are Copyright (C) 2001<<NEWL>># the Initial Developer. All Rights Reserved.<<NEWL>>#<<NEWL>># Contributor(s):<<NEWL>>#   Mark Pilgrim - port to Python<<NEWL>>#   Shy Shalom - original C code<<NEWL>>#   Proofpoint, Inc.<<NEWL>>#<<NEWL>># This library is free software; you can redistribute it and/or<<NEWL>># modify it under the terms of the GNU Lesser General Public<<NEWL>># License as published by the Free Software Foundation; either<<NEWL>># version 2.1 of the License, or (at your option) any later version.<<NEWL>>#<<NEWL>># This library is distributed in the hope that it will be useful,<<NEWL>># but WITHOUT ANY WARRANTY; without even the implied warranty of<<NEWL>># MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU<<NEWL>># Lesser General Public License for more details.<<NEWL>>#<<NEWL>># You should have received a copy of the GNU Lesser General Public<<NEWL>># License along with this library; if not, write to the Free Software<<NEWL>># Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA<<NEWL>># 02110-1301  USA<<NEWL>>######################### END LICENSE BLOCK #########################<<NEWL>><<NEWL>>from .charsetgroupprober import CharSetGroupProber<<NEWL>>from .utf8prober import UTF8Prober<<NEWL>>from .sjisprober import SJISProber<<NEWL>>from .eucjpprober import EUCJPProber<<NEWL>>from .gb2312prober import GB2312Prober<<NEWL>>from .euckrprober import EUCKRProber<<NEWL>>from .cp949prober import CP949Prober<<NEWL>>from .big5prober import Big5Prober<<NEWL>>from .euctwprober import EUCTWProber<<NEWL>><<NEWL>><<NEWL>>class MBCSGroupProber(CharSetGroupProber):<<NEWL>>    def __init__(self, lang_filter=None):<<NEWL>>        super(MBCSGroupProber, self).__init__(lang_filter=lang_filter)<<NEWL>>        self.probers = [<<NEWL>>            UTF8Prober(),<<NEWL>>            SJISProber(),<<NEWL>>            EUCJPProber(),<<NEWL>>            GB2312Prober(),<<NEWL>>            EUCKRProber(),<<NEWL>>            CP949Prober(),<<NEWL>>            Big5Prober(),<<NEWL>>            EUCTWProber()<<NEWL>>        ]<<NEWL>>        self.reset()
277	jackson	4	"from __future__ import annotations<<NEWL>><<NEWL>>import numpy as np<<NEWL>><<NEWL>>from pandas._typing import NumpyIndexT<<NEWL>><<NEWL>>from pandas.core.dtypes.common import is_list_like<<NEWL>><<NEWL>><<NEWL>>def cartesian_product(X) -> list[np.ndarray]:<<NEWL>>    """"""<<NEWL>>    Numpy version of itertools.product.<<NEWL>>    Sometimes faster (for large inputs)...<<NEWL>><<NEWL>>    Parameters<<NEWL>>    ----------<<NEWL>>    X : list-like of list-likes<<NEWL>><<NEWL>>    Returns<<NEWL>>    -------<<NEWL>>    product : list of ndarrays<<NEWL>><<NEWL>>    Examples<<NEWL>>    --------<<NEWL>>    >>> cartesian_product([list('ABC'), [1, 2]])<<NEWL>>    [array(['A', 'A', 'B', 'B', 'C', 'C'], dtype='<U1'), array([1, 2, 1, 2, 1, 2])]<<NEWL>><<NEWL>>    See Also<<NEWL>>    --------<<NEWL>>    itertools.product : Cartesian product of input iterables.  Equivalent to<<NEWL>>        nested for-loops.<<NEWL>>    """"""<<NEWL>>    msg = ""Input must be a list-like of list-likes""<<NEWL>>    if not is_list_like(X):<<NEWL>>        raise TypeError(msg)<<NEWL>>    for x in X:<<NEWL>>        if not is_list_like(x):<<NEWL>>            raise TypeError(msg)<<NEWL>><<NEWL>>    if len(X) == 0:<<NEWL>>        return []<<NEWL>><<NEWL>>    lenX = np.fromiter((len(x) for x in X), dtype=np.intp)<<NEWL>>    cumprodX = np.cumproduct(lenX)<<NEWL>><<NEWL>>    if np.any(cumprodX < 0):<<NEWL>>        raise ValueError(""Product space too large to allocate arrays!"")<<NEWL>><<NEWL>>    a = np.roll(cumprodX, 1)<<NEWL>>    a[0] = 1<<NEWL>><<NEWL>>    if cumprodX[-1] != 0:<<NEWL>>        b = cumprodX[-1] / cumprodX<<NEWL>>    else:<<NEWL>>        # if any factor is empty, the cartesian product is empty<<NEWL>>        b = np.zeros_like(cumprodX)<<NEWL>><<NEWL>>    # error: Argument of type ""int_"" cannot be assigned to parameter ""num"" of<<NEWL>>    # type ""int"" in function ""tile_compat""<<NEWL>>    return [<<NEWL>>        tile_compat(<<NEWL>>            np.repeat(x, b[i]),<<NEWL>>            np.product(a[i]),  # pyright: ignore[reportGeneralTypeIssues]<<NEWL>>        )<<NEWL>>        for i, x in enumerate(X)<<NEWL>>    ]<<NEWL>><<NEWL>><<NEWL>>def tile_compat(arr: NumpyIndexT, num: int) -> NumpyIndexT:<<NEWL>>    """"""<<NEWL>>    Index compat for np.tile.<<NEWL>><<NEWL>>    Notes<<NEWL>>    -----<<NEWL>>    Does not support multi-dimensional `num`.<<NEWL>>    """"""<<NEWL>>    if isinstance(arr, np.ndarray):<<NEWL>>        return np.tile(arr, num)<<NEWL>><<NEWL>>    # Otherwise we have an Index<<NEWL>>    taker = np.tile(np.arange(len(arr)), num)<<NEWL>>    return arr.take(taker)"
277	donghui	4	"from __future__ import annotations<<NEWL>><<NEWL>>import numpy as np<<NEWL>><<NEWL>>from pandas._typing import NumpyIndexT<<NEWL>><<NEWL>>from pandas.core.dtypes.common import is_list_like<<NEWL>><<NEWL>><<NEWL>>def cartesian_product(X) -> list[np.ndarray]:<<NEWL>>    """"""<<NEWL>>    Numpy version of itertools.product.<<NEWL>>    Sometimes faster (for large inputs)...<<NEWL>><<NEWL>>    Parameters<<NEWL>>    ----------<<NEWL>>    X : list-like of list-likes<<NEWL>><<NEWL>>    Returns<<NEWL>>    -------<<NEWL>>    product : list of ndarrays<<NEWL>><<NEWL>>    Examples<<NEWL>>    --------<<NEWL>>    >>> cartesian_product([list('ABC'), [1, 2]])<<NEWL>>    [array(['A', 'A', 'B', 'B', 'C', 'C'], dtype='<U1'), array([1, 2, 1, 2, 1, 2])]<<NEWL>><<NEWL>>    See Also<<NEWL>>    --------<<NEWL>>    itertools.product : Cartesian product of input iterables.  Equivalent to<<NEWL>>        nested for-loops.<<NEWL>>    """"""<<NEWL>>    msg = ""Input must be a list-like of list-likes""<<NEWL>>    if not is_list_like(X):<<NEWL>>        raise TypeError(msg)<<NEWL>>    for x in X:<<NEWL>>        if not is_list_like(x):<<NEWL>>            raise TypeError(msg)<<NEWL>><<NEWL>>    if len(X) == 0:<<NEWL>>        return []<<NEWL>><<NEWL>>    lenX = np.fromiter((len(x) for x in X), dtype=np.intp)<<NEWL>>    cumprodX = np.cumproduct(lenX)<<NEWL>><<NEWL>>    if np.any(cumprodX < 0):<<NEWL>>        raise ValueError(""Product space too large to allocate arrays!"")<<NEWL>><<NEWL>>    a = np.roll(cumprodX, 1)<<NEWL>>    a[0] = 1<<NEWL>><<NEWL>>    if cumprodX[-1] != 0:<<NEWL>>        b = cumprodX[-1] / cumprodX<<NEWL>>    else:<<NEWL>>        # if any factor is empty, the cartesian product is empty<<NEWL>>        b = np.zeros_like(cumprodX)<<NEWL>><<NEWL>>    # error: Argument of type ""int_"" cannot be assigned to parameter ""num"" of<<NEWL>>    # type ""int"" in function ""tile_compat""<<NEWL>>    return [<<NEWL>>        tile_compat(<<NEWL>>            np.repeat(x, b[i]),<<NEWL>>            np.product(a[i]),  # pyright: ignore[reportGeneralTypeIssues]<<NEWL>>        )<<NEWL>>        for i, x in enumerate(X)<<NEWL>>    ]<<NEWL>><<NEWL>><<NEWL>>def tile_compat(arr: NumpyIndexT, num: int) -> NumpyIndexT:<<NEWL>>    """"""<<NEWL>>    Index compat for np.tile.<<NEWL>><<NEWL>>    Notes<<NEWL>>    -----<<NEWL>>    Does not support multi-dimensional `num`.<<NEWL>>    """"""<<NEWL>>    if isinstance(arr, np.ndarray):<<NEWL>>        return np.tile(arr, num)<<NEWL>><<NEWL>>    # Otherwise we have an Index<<NEWL>>    taker = np.tile(np.arange(len(arr)), num)<<NEWL>>    return arr.take(taker)"
366	jackson	3	"from six import string_types<<NEWL>><<NEWL>>from .base import GraphQLDocument<<NEWL>><<NEWL>># Necessary for static type checking<<NEWL>>if False:  # flake8: noqa<<NEWL>>    from ..type.schema import GraphQLSchema<<NEWL>>    from typing import Any, Optional, Dict, Callable, Union<<NEWL>><<NEWL>><<NEWL>>class GraphQLCompiledDocument(GraphQLDocument):<<NEWL>>    @classmethod<<NEWL>>    def from_code(<<NEWL>>        cls,<<NEWL>>        schema,  # type: GraphQLSchema<<NEWL>>        code,  # type: Union[str, Any]<<NEWL>>        uptodate=None,  # type: Optional[bool]<<NEWL>>        extra_namespace=None,  # type: Optional[Dict[str, Any]]<<NEWL>>    ):<<NEWL>>        # type: (...) -> GraphQLCompiledDocument<<NEWL>>        """"""Creates a GraphQLDocument object from compiled code and the globals.  This<<NEWL>>        is used by the loaders and schema to create a document object.<<NEWL>>        """"""<<NEWL>>        if isinstance(code, string_types):<<NEWL>>            filename = ""<document>""<<NEWL>>            code = compile(code, filename, ""exec"")<<NEWL>>        namespace = {""__file__"": code.co_filename}<<NEWL>>        exec(code, namespace)<<NEWL>>        if extra_namespace:<<NEWL>>            namespace.update(extra_namespace)<<NEWL>>        rv = cls._from_namespace(schema, namespace)<<NEWL>>        # rv._uptodate = uptodate<<NEWL>>        return rv<<NEWL>><<NEWL>>    @classmethod<<NEWL>>    def from_module_dict(cls, schema, module_dict):<<NEWL>>        # type: (GraphQLSchema, Dict[str, Any]) -> GraphQLCompiledDocument<<NEWL>>        """"""Creates a template object from a module.  This is used by the<<NEWL>>        module loader to create a document object.<<NEWL>>        """"""<<NEWL>>        return cls._from_namespace(schema, module_dict)<<NEWL>><<NEWL>>    @classmethod<<NEWL>>    def _from_namespace(cls, schema, namespace):<<NEWL>>        # type: (GraphQLSchema, Dict[str, Any]) -> GraphQLCompiledDocument<<NEWL>>        document_string = namespace.get(""document_string"", """")  # type: str<<NEWL>>        document_ast = namespace.get(""document_ast"")  # type: ignore<<NEWL>>        execute = namespace[""execute""]  # type: Callable<<NEWL>><<NEWL>>        namespace[""schema""] = schema<<NEWL>>        return cls(<<NEWL>>            schema=schema,<<NEWL>>            document_string=document_string,<<NEWL>>            document_ast=document_ast,  # type: ignore<<NEWL>>            execute=execute,<<NEWL>>        )"
366	donghui	3	"from six import string_types<<NEWL>><<NEWL>>from .base import GraphQLDocument<<NEWL>><<NEWL>># Necessary for static type checking<<NEWL>>if False:  # flake8: noqa<<NEWL>>    from ..type.schema import GraphQLSchema<<NEWL>>    from typing import Any, Optional, Dict, Callable, Union<<NEWL>><<NEWL>><<NEWL>>class GraphQLCompiledDocument(GraphQLDocument):<<NEWL>>    @classmethod<<NEWL>>    def from_code(<<NEWL>>        cls,<<NEWL>>        schema,  # type: GraphQLSchema<<NEWL>>        code,  # type: Union[str, Any]<<NEWL>>        uptodate=None,  # type: Optional[bool]<<NEWL>>        extra_namespace=None,  # type: Optional[Dict[str, Any]]<<NEWL>>    ):<<NEWL>>        # type: (...) -> GraphQLCompiledDocument<<NEWL>>        """"""Creates a GraphQLDocument object from compiled code and the globals.  This<<NEWL>>        is used by the loaders and schema to create a document object.<<NEWL>>        """"""<<NEWL>>        if isinstance(code, string_types):<<NEWL>>            filename = ""<document>""<<NEWL>>            code = compile(code, filename, ""exec"")<<NEWL>>        namespace = {""__file__"": code.co_filename}<<NEWL>>        exec(code, namespace)<<NEWL>>        if extra_namespace:<<NEWL>>            namespace.update(extra_namespace)<<NEWL>>        rv = cls._from_namespace(schema, namespace)<<NEWL>>        # rv._uptodate = uptodate<<NEWL>>        return rv<<NEWL>><<NEWL>>    @classmethod<<NEWL>>    def from_module_dict(cls, schema, module_dict):<<NEWL>>        # type: (GraphQLSchema, Dict[str, Any]) -> GraphQLCompiledDocument<<NEWL>>        """"""Creates a template object from a module.  This is used by the<<NEWL>>        module loader to create a document object.<<NEWL>>        """"""<<NEWL>>        return cls._from_namespace(schema, module_dict)<<NEWL>><<NEWL>>    @classmethod<<NEWL>>    def _from_namespace(cls, schema, namespace):<<NEWL>>        # type: (GraphQLSchema, Dict[str, Any]) -> GraphQLCompiledDocument<<NEWL>>        document_string = namespace.get(""document_string"", """")  # type: str<<NEWL>>        document_ast = namespace.get(""document_ast"")  # type: ignore<<NEWL>>        execute = namespace[""execute""]  # type: Callable<<NEWL>><<NEWL>>        namespace[""schema""] = schema<<NEWL>>        return cls(<<NEWL>>            schema=schema,<<NEWL>>            document_string=document_string,<<NEWL>>            document_ast=document_ast,  # type: ignore<<NEWL>>            execute=execute,<<NEWL>>        )"
428	jackson	1	"# Copyright 2017-present Open Networking Foundation<<NEWL>>#<<NEWL>># Licensed under the Apache License, Version 2.0 (the ""License"");<<NEWL>># you may not use this file except in compliance with the License.<<NEWL>># You may obtain a copy of the License at<<NEWL>>#<<NEWL>># http://www.apache.org/licenses/LICENSE-2.0<<NEWL>>#<<NEWL>># Unless required by applicable law or agreed to in writing, software<<NEWL>># distributed under the License is distributed on an ""AS IS"" BASIS,<<NEWL>># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.<<NEWL>># See the License for the specific language governing permissions and<<NEWL>># limitations under the License.<<NEWL>>import structlog<<NEWL>>from enum import Enum<<NEWL>>from google.protobuf.json_format import MessageToDict<<NEWL>>from google.protobuf.message import Message<<NEWL>>from simplejson import dumps<<NEWL>><<NEWL>>from common.event_bus import EventBusClient<<NEWL>>from voltha.core.config.config_proxy import CallbackType<<NEWL>>from voltha.protos import third_party<<NEWL>>from voltha.protos.events_pb2 import ConfigEvent, ConfigEventType<<NEWL>><<NEWL>>IGNORED_CALLBACKS = [CallbackType.PRE_ADD, CallbackType.GET,<<NEWL>>                     CallbackType.POST_LISTCHANGE, CallbackType.PRE_REMOVE,<<NEWL>>                     CallbackType.PRE_UPDATE]<<NEWL>><<NEWL>>log = structlog.get_logger()<<NEWL>><<NEWL>>class ConfigEventBus(object):<<NEWL>><<NEWL>>    __slots__ = (<<NEWL>>        '_event_bus_client',  # The event bus client used to publish events.<<NEWL>>        '_topic'  # the topic to publish to<<NEWL>>    )<<NEWL>><<NEWL>>    def __init__(self):<<NEWL>>        self._event_bus_client = EventBusClient()<<NEWL>>        self._topic = 'model-change-events'<<NEWL>><<NEWL>>    def advertise(self, type, data, hash=None):<<NEWL>>        if type in IGNORED_CALLBACKS:<<NEWL>>            log.info('Ignoring event {} with data {}'.format(type, data))<<NEWL>>            return<<NEWL>><<NEWL>>        if type is CallbackType.POST_ADD:<<NEWL>>            kind = ConfigEventType.add<<NEWL>>        elif type is CallbackType.POST_REMOVE:<<NEWL>>            kind = ConfigEventType.remove<<NEWL>>        else:<<NEWL>>            kind = ConfigEventType.update<<NEWL>><<NEWL>>        if isinstance(data, Message):<<NEWL>>            msg = dumps(MessageToDict(data, True, True))<<NEWL>>        else:<<NEWL>>            msg = data<<NEWL>><<NEWL>>        event = ConfigEvent(<<NEWL>>            type=kind,<<NEWL>>            hash=hash,<<NEWL>>            data=msg<<NEWL>>        )<<NEWL>><<NEWL>>        self._event_bus_client.publish(self._topic, event)<<NEWL>>"
428	donghui	1	"# Copyright 2017-present Open Networking Foundation<<NEWL>>#<<NEWL>># Licensed under the Apache License, Version 2.0 (the ""License"");<<NEWL>># you may not use this file except in compliance with the License.<<NEWL>># You may obtain a copy of the License at<<NEWL>>#<<NEWL>># http://www.apache.org/licenses/LICENSE-2.0<<NEWL>>#<<NEWL>># Unless required by applicable law or agreed to in writing, software<<NEWL>># distributed under the License is distributed on an ""AS IS"" BASIS,<<NEWL>># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.<<NEWL>># See the License for the specific language governing permissions and<<NEWL>># limitations under the License.<<NEWL>>import structlog<<NEWL>>from enum import Enum<<NEWL>>from google.protobuf.json_format import MessageToDict<<NEWL>>from google.protobuf.message import Message<<NEWL>>from simplejson import dumps<<NEWL>><<NEWL>>from common.event_bus import EventBusClient<<NEWL>>from voltha.core.config.config_proxy import CallbackType<<NEWL>>from voltha.protos import third_party<<NEWL>>from voltha.protos.events_pb2 import ConfigEvent, ConfigEventType<<NEWL>><<NEWL>>IGNORED_CALLBACKS = [CallbackType.PRE_ADD, CallbackType.GET,<<NEWL>>                     CallbackType.POST_LISTCHANGE, CallbackType.PRE_REMOVE,<<NEWL>>                     CallbackType.PRE_UPDATE]<<NEWL>><<NEWL>>log = structlog.get_logger()<<NEWL>><<NEWL>>class ConfigEventBus(object):<<NEWL>><<NEWL>>    __slots__ = (<<NEWL>>        '_event_bus_client',  # The event bus client used to publish events.<<NEWL>>        '_topic'  # the topic to publish to<<NEWL>>    )<<NEWL>><<NEWL>>    def __init__(self):<<NEWL>>        self._event_bus_client = EventBusClient()<<NEWL>>        self._topic = 'model-change-events'<<NEWL>><<NEWL>>    def advertise(self, type, data, hash=None):<<NEWL>>        if type in IGNORED_CALLBACKS:<<NEWL>>            log.info('Ignoring event {} with data {}'.format(type, data))<<NEWL>>            return<<NEWL>><<NEWL>>        if type is CallbackType.POST_ADD:<<NEWL>>            kind = ConfigEventType.add<<NEWL>>        elif type is CallbackType.POST_REMOVE:<<NEWL>>            kind = ConfigEventType.remove<<NEWL>>        else:<<NEWL>>            kind = ConfigEventType.update<<NEWL>><<NEWL>>        if isinstance(data, Message):<<NEWL>>            msg = dumps(MessageToDict(data, True, True))<<NEWL>>        else:<<NEWL>>            msg = data<<NEWL>><<NEWL>>        event = ConfigEvent(<<NEWL>>            type=kind,<<NEWL>>            hash=hash,<<NEWL>>            data=msg<<NEWL>>        )<<NEWL>><<NEWL>>        self._event_bus_client.publish(self._topic, event)<<NEWL>>"
479	jackson	3	"# A demo for the IDsObjectPicker interface.<<NEWL>>import win32clipboard<<NEWL>>import pythoncom<<NEWL>>from win32com.adsi import adsi<<NEWL>>from win32com.adsi.adsicon import *<<NEWL>><<NEWL>>cf_objectpicker = win32clipboard.RegisterClipboardFormat(CFSTR_DSOP_DS_SELECTION_LIST)<<NEWL>><<NEWL>><<NEWL>>def main():<<NEWL>>    hwnd = 0<<NEWL>><<NEWL>>    # Create an instance of the object picker.<<NEWL>>    picker = pythoncom.CoCreateInstance(<<NEWL>>        adsi.CLSID_DsObjectPicker,<<NEWL>>        None,<<NEWL>>        pythoncom.CLSCTX_INPROC_SERVER,<<NEWL>>        adsi.IID_IDsObjectPicker,<<NEWL>>    )<<NEWL>><<NEWL>>    # Create our scope init info.<<NEWL>>    siis = adsi.DSOP_SCOPE_INIT_INFOs(1)<<NEWL>>    sii = siis[0]<<NEWL>><<NEWL>>    # Combine multiple scope types in a single array entry.<<NEWL>><<NEWL>>    sii.type = (<<NEWL>>        DSOP_SCOPE_TYPE_UPLEVEL_JOINED_DOMAIN | DSOP_SCOPE_TYPE_DOWNLEVEL_JOINED_DOMAIN<<NEWL>>    )<<NEWL>><<NEWL>>    # Set uplevel and downlevel filters to include only computer objects.<<NEWL>>    # Uplevel filters apply to both mixed and native modes.<<NEWL>>    # Notice that the uplevel and downlevel flags are different.<<NEWL>><<NEWL>>    sii.filterFlags.uplevel.bothModes = DSOP_FILTER_COMPUTERS<<NEWL>>    sii.filterFlags.downlevel = DSOP_DOWNLEVEL_FILTER_COMPUTERS<<NEWL>><<NEWL>>    # Initialize the interface.<<NEWL>>    picker.Initialize(<<NEWL>>        None,  # Target is the local computer.<<NEWL>>        siis,  # scope infos<<NEWL>>        DSOP_FLAG_MULTISELECT,  # options<<NEWL>>        (""objectGUID"", ""displayName""),<<NEWL>>    )  # attributes to fetch<<NEWL>><<NEWL>>    do = picker.InvokeDialog(hwnd)<<NEWL>>    # Extract the data from the IDataObject.<<NEWL>>    format_etc = (<<NEWL>>        cf_objectpicker,<<NEWL>>        None,<<NEWL>>        pythoncom.DVASPECT_CONTENT,<<NEWL>>        -1,<<NEWL>>        pythoncom.TYMED_HGLOBAL,<<NEWL>>    )<<NEWL>>    medium = do.GetData(format_etc)<<NEWL>>    data = adsi.StringAsDS_SELECTION_LIST(medium.data)<<NEWL>>    for item in data:<<NEWL>>        name, klass, adspath, upn, attrs, flags = item<<NEWL>>        print(""Item"", name)<<NEWL>>        print("" Class:"", klass)<<NEWL>>        print("" AdsPath:"", adspath)<<NEWL>>        print("" UPN:"", upn)<<NEWL>>        print("" Attrs:"", attrs)<<NEWL>>        print("" Flags:"", flags)<<NEWL>><<NEWL>><<NEWL>>if __name__ == ""__main__"":<<NEWL>>    main()"
479	donghui	2	"# A demo for the IDsObjectPicker interface.<<NEWL>>import win32clipboard<<NEWL>>import pythoncom<<NEWL>>from win32com.adsi import adsi<<NEWL>>from win32com.adsi.adsicon import *<<NEWL>><<NEWL>>cf_objectpicker = win32clipboard.RegisterClipboardFormat(CFSTR_DSOP_DS_SELECTION_LIST)<<NEWL>><<NEWL>><<NEWL>>def main():<<NEWL>>    hwnd = 0<<NEWL>><<NEWL>>    # Create an instance of the object picker.<<NEWL>>    picker = pythoncom.CoCreateInstance(<<NEWL>>        adsi.CLSID_DsObjectPicker,<<NEWL>>        None,<<NEWL>>        pythoncom.CLSCTX_INPROC_SERVER,<<NEWL>>        adsi.IID_IDsObjectPicker,<<NEWL>>    )<<NEWL>><<NEWL>>    # Create our scope init info.<<NEWL>>    siis = adsi.DSOP_SCOPE_INIT_INFOs(1)<<NEWL>>    sii = siis[0]<<NEWL>><<NEWL>>    # Combine multiple scope types in a single array entry.<<NEWL>><<NEWL>>    sii.type = (<<NEWL>>        DSOP_SCOPE_TYPE_UPLEVEL_JOINED_DOMAIN | DSOP_SCOPE_TYPE_DOWNLEVEL_JOINED_DOMAIN<<NEWL>>    )<<NEWL>><<NEWL>>    # Set uplevel and downlevel filters to include only computer objects.<<NEWL>>    # Uplevel filters apply to both mixed and native modes.<<NEWL>>    # Notice that the uplevel and downlevel flags are different.<<NEWL>><<NEWL>>    sii.filterFlags.uplevel.bothModes = DSOP_FILTER_COMPUTERS<<NEWL>>    sii.filterFlags.downlevel = DSOP_DOWNLEVEL_FILTER_COMPUTERS<<NEWL>><<NEWL>>    # Initialize the interface.<<NEWL>>    picker.Initialize(<<NEWL>>        None,  # Target is the local computer.<<NEWL>>        siis,  # scope infos<<NEWL>>        DSOP_FLAG_MULTISELECT,  # options<<NEWL>>        (""objectGUID"", ""displayName""),<<NEWL>>    )  # attributes to fetch<<NEWL>><<NEWL>>    do = picker.InvokeDialog(hwnd)<<NEWL>>    # Extract the data from the IDataObject.<<NEWL>>    format_etc = (<<NEWL>>        cf_objectpicker,<<NEWL>>        None,<<NEWL>>        pythoncom.DVASPECT_CONTENT,<<NEWL>>        -1,<<NEWL>>        pythoncom.TYMED_HGLOBAL,<<NEWL>>    )<<NEWL>>    medium = do.GetData(format_etc)<<NEWL>>    data = adsi.StringAsDS_SELECTION_LIST(medium.data)<<NEWL>>    for item in data:<<NEWL>>        name, klass, adspath, upn, attrs, flags = item<<NEWL>>        print(""Item"", name)<<NEWL>>        print("" Class:"", klass)<<NEWL>>        print("" AdsPath:"", adspath)<<NEWL>>        print("" UPN:"", upn)<<NEWL>>        print("" Attrs:"", attrs)<<NEWL>>        print("" Flags:"", flags)<<NEWL>><<NEWL>><<NEWL>>if __name__ == ""__main__"":<<NEWL>>    main()"
469	jackson	2	"import _plotly_utils.basevalidators<<NEWL>><<NEWL>><<NEWL>>class HoverlabelValidator(_plotly_utils.basevalidators.CompoundValidator):<<NEWL>>    def __init__(<<NEWL>>        self, plotly_name=""hoverlabel"", parent_name=""choroplethmapbox"", **kwargs<<NEWL>>    ):<<NEWL>>        super(HoverlabelValidator, self).__init__(<<NEWL>>            plotly_name=plotly_name,<<NEWL>>            parent_name=parent_name,<<NEWL>>            data_class_str=kwargs.pop(""data_class_str"", ""Hoverlabel""),<<NEWL>>            data_docs=kwargs.pop(<<NEWL>>                ""data_docs"",<<NEWL>>                """"""<<NEWL>>            align<<NEWL>>                Sets the horizontal alignment of the text<<NEWL>>                content within hover label box. Has an effect<<NEWL>>                only if the hover label text spans more two or<<NEWL>>                more lines<<NEWL>>            alignsrc<<NEWL>>                Sets the source reference on Chart Studio Cloud<<NEWL>>                for `align`.<<NEWL>>            bgcolor<<NEWL>>                Sets the background color of the hover labels<<NEWL>>                for this trace<<NEWL>>            bgcolorsrc<<NEWL>>                Sets the source reference on Chart Studio Cloud<<NEWL>>                for `bgcolor`.<<NEWL>>            bordercolor<<NEWL>>                Sets the border color of the hover labels for<<NEWL>>                this trace.<<NEWL>>            bordercolorsrc<<NEWL>>                Sets the source reference on Chart Studio Cloud<<NEWL>>                for `bordercolor`.<<NEWL>>            font<<NEWL>>                Sets the font used in hover labels.<<NEWL>>            namelength<<NEWL>>                Sets the default length (in number of<<NEWL>>                characters) of the trace name in the hover<<NEWL>>                labels for all traces. -1 shows the whole name<<NEWL>>                regardless of length. 0-3 shows the first 0-3<<NEWL>>                characters, and an integer >3 will show the<<NEWL>>                whole name if it is less than that many<<NEWL>>                characters, but if it is longer, will truncate<<NEWL>>                to `namelength - 3` characters and add an<<NEWL>>                ellipsis.<<NEWL>>            namelengthsrc<<NEWL>>                Sets the source reference on Chart Studio Cloud<<NEWL>>                for `namelength`.<<NEWL>>"""""",<<NEWL>>            ),<<NEWL>>            **kwargs,<<NEWL>>        )"
469	donghui	1	"import _plotly_utils.basevalidators<<NEWL>><<NEWL>><<NEWL>>class HoverlabelValidator(_plotly_utils.basevalidators.CompoundValidator):<<NEWL>>    def __init__(<<NEWL>>        self, plotly_name=""hoverlabel"", parent_name=""choroplethmapbox"", **kwargs<<NEWL>>    ):<<NEWL>>        super(HoverlabelValidator, self).__init__(<<NEWL>>            plotly_name=plotly_name,<<NEWL>>            parent_name=parent_name,<<NEWL>>            data_class_str=kwargs.pop(""data_class_str"", ""Hoverlabel""),<<NEWL>>            data_docs=kwargs.pop(<<NEWL>>                ""data_docs"",<<NEWL>>                """"""<<NEWL>>            align<<NEWL>>                Sets the horizontal alignment of the text<<NEWL>>                content within hover label box. Has an effect<<NEWL>>                only if the hover label text spans more two or<<NEWL>>                more lines<<NEWL>>            alignsrc<<NEWL>>                Sets the source reference on Chart Studio Cloud<<NEWL>>                for `align`.<<NEWL>>            bgcolor<<NEWL>>                Sets the background color of the hover labels<<NEWL>>                for this trace<<NEWL>>            bgcolorsrc<<NEWL>>                Sets the source reference on Chart Studio Cloud<<NEWL>>                for `bgcolor`.<<NEWL>>            bordercolor<<NEWL>>                Sets the border color of the hover labels for<<NEWL>>                this trace.<<NEWL>>            bordercolorsrc<<NEWL>>                Sets the source reference on Chart Studio Cloud<<NEWL>>                for `bordercolor`.<<NEWL>>            font<<NEWL>>                Sets the font used in hover labels.<<NEWL>>            namelength<<NEWL>>                Sets the default length (in number of<<NEWL>>                characters) of the trace name in the hover<<NEWL>>                labels for all traces. -1 shows the whole name<<NEWL>>                regardless of length. 0-3 shows the first 0-3<<NEWL>>                characters, and an integer >3 will show the<<NEWL>>                whole name if it is less than that many<<NEWL>>                characters, but if it is longer, will truncate<<NEWL>>                to `namelength - 3` characters and add an<<NEWL>>                ellipsis.<<NEWL>>            namelengthsrc<<NEWL>>                Sets the source reference on Chart Studio Cloud<<NEWL>>                for `namelength`.<<NEWL>>"""""",<<NEWL>>            ),<<NEWL>>            **kwargs,<<NEWL>>        )"
438	jackson	0	import time<<NEWL>><<NEWL>>from jet_bridge_base.settings import set_settings<<NEWL>><<NEWL>><<NEWL>>class Configuration(object):<<NEWL>><<NEWL>>    def __init__(self):<<NEWL>>        self.init_time = time.time()<<NEWL>><<NEWL>>    def get_type(self):<<NEWL>>        pass<<NEWL>><<NEWL>>    def get_version(self):<<NEWL>>        pass<<NEWL>><<NEWL>>    def get_model_description(self, db_table):<<NEWL>>        pass<<NEWL>><<NEWL>>    def get_hidden_model_description(self):<<NEWL>>        return []<<NEWL>><<NEWL>>    def get_settings(self):<<NEWL>>        pass<<NEWL>><<NEWL>>    def on_model_pre_create(self, model, pk):<<NEWL>>        pass<<NEWL>><<NEWL>>    def on_model_post_create(self, model, instance):<<NEWL>>        pass<<NEWL>><<NEWL>>    def on_model_pre_update(self, model, instance):<<NEWL>>        pass<<NEWL>><<NEWL>>    def on_model_post_update(self, model, instance):<<NEWL>>        pass<<NEWL>><<NEWL>>    def on_model_pre_delete(self, model, instance):<<NEWL>>        pass<<NEWL>><<NEWL>>    def on_model_post_delete(self, model, instance):<<NEWL>>        pass<<NEWL>><<NEWL>>    def media_get_available_name(self, path):<<NEWL>>        pass<<NEWL>><<NEWL>>    def media_exists(self, path):<<NEWL>>        pass<<NEWL>><<NEWL>>    def media_listdir(self, path):<<NEWL>>        pass<<NEWL>><<NEWL>>    def media_get_modified_time(self, path):<<NEWL>>        pass<<NEWL>><<NEWL>>    def media_open(self, path, mode='rb'):<<NEWL>>        pass<<NEWL>><<NEWL>>    def media_save(self, path, content):<<NEWL>>        pass<<NEWL>><<NEWL>>    def media_delete(self, path):<<NEWL>>        pass<<NEWL>><<NEWL>>    def media_url(self, path, request):<<NEWL>>        pass<<NEWL>><<NEWL>>    def session_set(self, request, name, value, secure=True):<<NEWL>>        pass<<NEWL>><<NEWL>>    def session_get(self, request, name, default=None, decode=True, secure=True):<<NEWL>>        pass<<NEWL>><<NEWL>>    def session_clear(self, request, name):<<NEWL>>        pass<<NEWL>><<NEWL>>    def clean_sso_application_name(self, name):<<NEWL>>        return name.lower().replace('-', '')<<NEWL>><<NEWL>>    def clean_sso_applications(self, applications):<<NEWL>>        return dict(map(lambda x: (self.clean_sso_application_name(x[0]), x[1]), applications.items()))<<NEWL>><<NEWL>><<NEWL>>configuration = Configuration()<<NEWL>><<NEWL>><<NEWL>>def set_configuration(new_configuration):<<NEWL>>    global configuration<<NEWL>>    configuration = new_configuration<<NEWL>>    set_settings(configuration.get_settings())
438	donghui	0	import time<<NEWL>><<NEWL>>from jet_bridge_base.settings import set_settings<<NEWL>><<NEWL>><<NEWL>>class Configuration(object):<<NEWL>><<NEWL>>    def __init__(self):<<NEWL>>        self.init_time = time.time()<<NEWL>><<NEWL>>    def get_type(self):<<NEWL>>        pass<<NEWL>><<NEWL>>    def get_version(self):<<NEWL>>        pass<<NEWL>><<NEWL>>    def get_model_description(self, db_table):<<NEWL>>        pass<<NEWL>><<NEWL>>    def get_hidden_model_description(self):<<NEWL>>        return []<<NEWL>><<NEWL>>    def get_settings(self):<<NEWL>>        pass<<NEWL>><<NEWL>>    def on_model_pre_create(self, model, pk):<<NEWL>>        pass<<NEWL>><<NEWL>>    def on_model_post_create(self, model, instance):<<NEWL>>        pass<<NEWL>><<NEWL>>    def on_model_pre_update(self, model, instance):<<NEWL>>        pass<<NEWL>><<NEWL>>    def on_model_post_update(self, model, instance):<<NEWL>>        pass<<NEWL>><<NEWL>>    def on_model_pre_delete(self, model, instance):<<NEWL>>        pass<<NEWL>><<NEWL>>    def on_model_post_delete(self, model, instance):<<NEWL>>        pass<<NEWL>><<NEWL>>    def media_get_available_name(self, path):<<NEWL>>        pass<<NEWL>><<NEWL>>    def media_exists(self, path):<<NEWL>>        pass<<NEWL>><<NEWL>>    def media_listdir(self, path):<<NEWL>>        pass<<NEWL>><<NEWL>>    def media_get_modified_time(self, path):<<NEWL>>        pass<<NEWL>><<NEWL>>    def media_open(self, path, mode='rb'):<<NEWL>>        pass<<NEWL>><<NEWL>>    def media_save(self, path, content):<<NEWL>>        pass<<NEWL>><<NEWL>>    def media_delete(self, path):<<NEWL>>        pass<<NEWL>><<NEWL>>    def media_url(self, path, request):<<NEWL>>        pass<<NEWL>><<NEWL>>    def session_set(self, request, name, value, secure=True):<<NEWL>>        pass<<NEWL>><<NEWL>>    def session_get(self, request, name, default=None, decode=True, secure=True):<<NEWL>>        pass<<NEWL>><<NEWL>>    def session_clear(self, request, name):<<NEWL>>        pass<<NEWL>><<NEWL>>    def clean_sso_application_name(self, name):<<NEWL>>        return name.lower().replace('-', '')<<NEWL>><<NEWL>>    def clean_sso_applications(self, applications):<<NEWL>>        return dict(map(lambda x: (self.clean_sso_application_name(x[0]), x[1]), applications.items()))<<NEWL>><<NEWL>><<NEWL>>configuration = Configuration()<<NEWL>><<NEWL>><<NEWL>>def set_configuration(new_configuration):<<NEWL>>    global configuration<<NEWL>>    configuration = new_configuration<<NEWL>>    set_settings(configuration.get_settings())
376	jackson	2	"import _plotly_utils.basevalidators<<NEWL>><<NEWL>><<NEWL>>class FontValidator(_plotly_utils.basevalidators.CompoundValidator):<<NEWL>>    def __init__(<<NEWL>>        self, plotly_name=""font"", parent_name=""scattersmith.hoverlabel"", **kwargs<<NEWL>>    ):<<NEWL>>        super(FontValidator, self).__init__(<<NEWL>>            plotly_name=plotly_name,<<NEWL>>            parent_name=parent_name,<<NEWL>>            data_class_str=kwargs.pop(""data_class_str"", ""Font""),<<NEWL>>            data_docs=kwargs.pop(<<NEWL>>                ""data_docs"",<<NEWL>>                """"""<<NEWL>>            color<<NEWL>><<NEWL>>            colorsrc<<NEWL>>                Sets the source reference on Chart Studio Cloud<<NEWL>>                for `color`.<<NEWL>>            family<<NEWL>>                HTML font family - the typeface that will be<<NEWL>>                applied by the web browser. The web browser<<NEWL>>                will only be able to apply a font if it is<<NEWL>>                available on the system which it operates.<<NEWL>>                Provide multiple font families, separated by<<NEWL>>                commas, to indicate the preference in which to<<NEWL>>                apply fonts if they aren't available on the<<NEWL>>                system. The Chart Studio Cloud (at<<NEWL>>                https://chart-studio.plotly.com or on-premise)<<NEWL>>                generates images on a server, where only a<<NEWL>>                select number of fonts are installed and<<NEWL>>                supported. These include ""Arial"", ""Balto"",<<NEWL>>                ""Courier New"", ""Droid Sans"",, ""Droid Serif"",<<NEWL>>                ""Droid Sans Mono"", ""Gravitas One"", ""Old<<NEWL>>                Standard TT"", ""Open Sans"", ""Overpass"", ""PT Sans<<NEWL>>                Narrow"", ""Raleway"", ""Times New Roman"".<<NEWL>>            familysrc<<NEWL>>                Sets the source reference on Chart Studio Cloud<<NEWL>>                for `family`.<<NEWL>>            size<<NEWL>><<NEWL>>            sizesrc<<NEWL>>                Sets the source reference on Chart Studio Cloud<<NEWL>>                for `size`.<<NEWL>>"""""",<<NEWL>>            ),<<NEWL>>            **kwargs,<<NEWL>>        )"
376	donghui	1	"import _plotly_utils.basevalidators<<NEWL>><<NEWL>><<NEWL>>class FontValidator(_plotly_utils.basevalidators.CompoundValidator):<<NEWL>>    def __init__(<<NEWL>>        self, plotly_name=""font"", parent_name=""scattersmith.hoverlabel"", **kwargs<<NEWL>>    ):<<NEWL>>        super(FontValidator, self).__init__(<<NEWL>>            plotly_name=plotly_name,<<NEWL>>            parent_name=parent_name,<<NEWL>>            data_class_str=kwargs.pop(""data_class_str"", ""Font""),<<NEWL>>            data_docs=kwargs.pop(<<NEWL>>                ""data_docs"",<<NEWL>>                """"""<<NEWL>>            color<<NEWL>><<NEWL>>            colorsrc<<NEWL>>                Sets the source reference on Chart Studio Cloud<<NEWL>>                for `color`.<<NEWL>>            family<<NEWL>>                HTML font family - the typeface that will be<<NEWL>>                applied by the web browser. The web browser<<NEWL>>                will only be able to apply a font if it is<<NEWL>>                available on the system which it operates.<<NEWL>>                Provide multiple font families, separated by<<NEWL>>                commas, to indicate the preference in which to<<NEWL>>                apply fonts if they aren't available on the<<NEWL>>                system. The Chart Studio Cloud (at<<NEWL>>                https://chart-studio.plotly.com or on-premise)<<NEWL>>                generates images on a server, where only a<<NEWL>>                select number of fonts are installed and<<NEWL>>                supported. These include ""Arial"", ""Balto"",<<NEWL>>                ""Courier New"", ""Droid Sans"",, ""Droid Serif"",<<NEWL>>                ""Droid Sans Mono"", ""Gravitas One"", ""Old<<NEWL>>                Standard TT"", ""Open Sans"", ""Overpass"", ""PT Sans<<NEWL>>                Narrow"", ""Raleway"", ""Times New Roman"".<<NEWL>>            familysrc<<NEWL>>                Sets the source reference on Chart Studio Cloud<<NEWL>>                for `family`.<<NEWL>>            size<<NEWL>><<NEWL>>            sizesrc<<NEWL>>                Sets the source reference on Chart Studio Cloud<<NEWL>>                for `size`.<<NEWL>>"""""",<<NEWL>>            ),<<NEWL>>            **kwargs,<<NEWL>>        )"
267	jackson	2	"""""""Basic implementation to support SOAP-Attachments<<NEWL>><<NEWL>>See https://www.w3.org/TR/SOAP-attachments<<NEWL>><<NEWL>>""""""<<NEWL>><<NEWL>>import base64<<NEWL>><<NEWL>>from cached_property import cached_property<<NEWL>>from requests.structures import CaseInsensitiveDict<<NEWL>><<NEWL>><<NEWL>>class MessagePack:<<NEWL>>    def __init__(self, parts):<<NEWL>>        self._parts = parts<<NEWL>><<NEWL>>    def __repr__(self):<<NEWL>>        return ""<MessagePack(attachments=[%s])>"" % (<<NEWL>>            "", "".join(repr(a) for a in self.attachments)<<NEWL>>        )<<NEWL>><<NEWL>>    @property<<NEWL>>    def root(self):<<NEWL>>        return self._root<<NEWL>><<NEWL>>    def _set_root(self, root):<<NEWL>>        self._root = root<<NEWL>><<NEWL>>    @cached_property<<NEWL>>    def attachments(self):<<NEWL>>        """"""Return a list of attachments.<<NEWL>><<NEWL>>        :rtype: list of Attachment<<NEWL>><<NEWL>>        """"""<<NEWL>>        return [Attachment(part) for part in self._parts]<<NEWL>><<NEWL>>    def get_by_content_id(self, content_id):<<NEWL>>        """"""get_by_content_id<<NEWL>><<NEWL>>        :param content_id: The content-id to return<<NEWL>>        :type content_id: str<<NEWL>>        :rtype: Attachment<<NEWL>><<NEWL>>        """"""<<NEWL>>        for attachment in self.attachments:<<NEWL>>            if attachment.content_id == content_id:<<NEWL>>                return attachment<<NEWL>><<NEWL>><<NEWL>>class Attachment:<<NEWL>>    def __init__(self, part):<<NEWL>>        encoding = part.encoding or ""utf-8""<<NEWL>>        self.headers = CaseInsensitiveDict(<<NEWL>>            {k.decode(encoding): v.decode(encoding) for k, v in part.headers.items()}<<NEWL>>        )<<NEWL>>        self.content_type = self.headers.get(""Content-Type"", None)<<NEWL>>        self.content_id = self.headers.get(""Content-ID"", None)<<NEWL>>        self.content_location = self.headers.get(""Content-Location"", None)<<NEWL>>        self._part = part<<NEWL>><<NEWL>>    def __repr__(self):<<NEWL>>        return ""<Attachment(%r, %r)>"" % (self.content_id, self.content_type)<<NEWL>><<NEWL>>    @cached_property<<NEWL>>    def content(self):<<NEWL>>        """"""Return the content of the attachment<<NEWL>><<NEWL>>        :rtype: bytes or str<<NEWL>><<NEWL>>        """"""<<NEWL>>        encoding = self.headers.get(""Content-Transfer-Encoding"", None)<<NEWL>>        content = self._part.content<<NEWL>><<NEWL>>        if encoding == ""base64"":<<NEWL>>            return base64.b64decode(content)<<NEWL>>        elif encoding == ""binary"":<<NEWL>>            return content.strip(b""\r\n"")<<NEWL>>        else:<<NEWL>>            return content"
267	donghui	2	"""""""Basic implementation to support SOAP-Attachments<<NEWL>><<NEWL>>See https://www.w3.org/TR/SOAP-attachments<<NEWL>><<NEWL>>""""""<<NEWL>><<NEWL>>import base64<<NEWL>><<NEWL>>from cached_property import cached_property<<NEWL>>from requests.structures import CaseInsensitiveDict<<NEWL>><<NEWL>><<NEWL>>class MessagePack:<<NEWL>>    def __init__(self, parts):<<NEWL>>        self._parts = parts<<NEWL>><<NEWL>>    def __repr__(self):<<NEWL>>        return ""<MessagePack(attachments=[%s])>"" % (<<NEWL>>            "", "".join(repr(a) for a in self.attachments)<<NEWL>>        )<<NEWL>><<NEWL>>    @property<<NEWL>>    def root(self):<<NEWL>>        return self._root<<NEWL>><<NEWL>>    def _set_root(self, root):<<NEWL>>        self._root = root<<NEWL>><<NEWL>>    @cached_property<<NEWL>>    def attachments(self):<<NEWL>>        """"""Return a list of attachments.<<NEWL>><<NEWL>>        :rtype: list of Attachment<<NEWL>><<NEWL>>        """"""<<NEWL>>        return [Attachment(part) for part in self._parts]<<NEWL>><<NEWL>>    def get_by_content_id(self, content_id):<<NEWL>>        """"""get_by_content_id<<NEWL>><<NEWL>>        :param content_id: The content-id to return<<NEWL>>        :type content_id: str<<NEWL>>        :rtype: Attachment<<NEWL>><<NEWL>>        """"""<<NEWL>>        for attachment in self.attachments:<<NEWL>>            if attachment.content_id == content_id:<<NEWL>>                return attachment<<NEWL>><<NEWL>><<NEWL>>class Attachment:<<NEWL>>    def __init__(self, part):<<NEWL>>        encoding = part.encoding or ""utf-8""<<NEWL>>        self.headers = CaseInsensitiveDict(<<NEWL>>            {k.decode(encoding): v.decode(encoding) for k, v in part.headers.items()}<<NEWL>>        )<<NEWL>>        self.content_type = self.headers.get(""Content-Type"", None)<<NEWL>>        self.content_id = self.headers.get(""Content-ID"", None)<<NEWL>>        self.content_location = self.headers.get(""Content-Location"", None)<<NEWL>>        self._part = part<<NEWL>><<NEWL>>    def __repr__(self):<<NEWL>>        return ""<Attachment(%r, %r)>"" % (self.content_id, self.content_type)<<NEWL>><<NEWL>>    @cached_property<<NEWL>>    def content(self):<<NEWL>>        """"""Return the content of the attachment<<NEWL>><<NEWL>>        :rtype: bytes or str<<NEWL>><<NEWL>>        """"""<<NEWL>>        encoding = self.headers.get(""Content-Transfer-Encoding"", None)<<NEWL>>        content = self._part.content<<NEWL>><<NEWL>>        if encoding == ""base64"":<<NEWL>>            return base64.b64decode(content)<<NEWL>>        elif encoding == ""binary"":<<NEWL>>            return content.strip(b""\r\n"")<<NEWL>>        else:<<NEWL>>            return content"
327	jackson	1	"######################## BEGIN LICENSE BLOCK ########################<<NEWL>># The Original Code is mozilla.org code.<<NEWL>>#<<NEWL>># The Initial Developer of the Original Code is<<NEWL>># Netscape Communications Corporation.<<NEWL>># Portions created by the Initial Developer are Copyright (C) 1998<<NEWL>># the Initial Developer. All Rights Reserved.<<NEWL>>#<<NEWL>># Contributor(s):<<NEWL>>#   Mark Pilgrim - port to Python<<NEWL>>#<<NEWL>># This library is free software; you can redistribute it and/or<<NEWL>># modify it under the terms of the GNU Lesser General Public<<NEWL>># License as published by the Free Software Foundation; either<<NEWL>># version 2.1 of the License, or (at your option) any later version.<<NEWL>>#<<NEWL>># This library is distributed in the hope that it will be useful,<<NEWL>># but WITHOUT ANY WARRANTY; without even the implied warranty of<<NEWL>># MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU<<NEWL>># Lesser General Public License for more details.<<NEWL>>#<<NEWL>># You should have received a copy of the GNU Lesser General Public<<NEWL>># License along with this library; if not, write to the Free Software<<NEWL>># Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA<<NEWL>># 02110-1301  USA<<NEWL>>######################### END LICENSE BLOCK #########################<<NEWL>><<NEWL>>from .chardistribution import EUCKRDistributionAnalysis<<NEWL>>from .codingstatemachine import CodingStateMachine<<NEWL>>from .mbcharsetprober import MultiByteCharSetProber<<NEWL>>from .mbcssm import CP949_SM_MODEL<<NEWL>><<NEWL>><<NEWL>>class CP949Prober(MultiByteCharSetProber):<<NEWL>>    def __init__(self) -> None:<<NEWL>>        super().__init__()<<NEWL>>        self.coding_sm = CodingStateMachine(CP949_SM_MODEL)<<NEWL>>        # NOTE: CP949 is a superset of EUC-KR, so the distribution should be<<NEWL>>        #       not different.<<NEWL>>        self.distribution_analyzer = EUCKRDistributionAnalysis()<<NEWL>>        self.reset()<<NEWL>><<NEWL>>    @property<<NEWL>>    def charset_name(self) -> str:<<NEWL>>        return ""CP949""<<NEWL>><<NEWL>>    @property<<NEWL>>    def language(self) -> str:<<NEWL>>        return ""Korean"""
327	donghui	1	"######################## BEGIN LICENSE BLOCK ########################<<NEWL>># The Original Code is mozilla.org code.<<NEWL>>#<<NEWL>># The Initial Developer of the Original Code is<<NEWL>># Netscape Communications Corporation.<<NEWL>># Portions created by the Initial Developer are Copyright (C) 1998<<NEWL>># the Initial Developer. All Rights Reserved.<<NEWL>>#<<NEWL>># Contributor(s):<<NEWL>>#   Mark Pilgrim - port to Python<<NEWL>>#<<NEWL>># This library is free software; you can redistribute it and/or<<NEWL>># modify it under the terms of the GNU Lesser General Public<<NEWL>># License as published by the Free Software Foundation; either<<NEWL>># version 2.1 of the License, or (at your option) any later version.<<NEWL>>#<<NEWL>># This library is distributed in the hope that it will be useful,<<NEWL>># but WITHOUT ANY WARRANTY; without even the implied warranty of<<NEWL>># MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU<<NEWL>># Lesser General Public License for more details.<<NEWL>>#<<NEWL>># You should have received a copy of the GNU Lesser General Public<<NEWL>># License along with this library; if not, write to the Free Software<<NEWL>># Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA<<NEWL>># 02110-1301  USA<<NEWL>>######################### END LICENSE BLOCK #########################<<NEWL>><<NEWL>>from .chardistribution import EUCKRDistributionAnalysis<<NEWL>>from .codingstatemachine import CodingStateMachine<<NEWL>>from .mbcharsetprober import MultiByteCharSetProber<<NEWL>>from .mbcssm import CP949_SM_MODEL<<NEWL>><<NEWL>><<NEWL>>class CP949Prober(MultiByteCharSetProber):<<NEWL>>    def __init__(self) -> None:<<NEWL>>        super().__init__()<<NEWL>>        self.coding_sm = CodingStateMachine(CP949_SM_MODEL)<<NEWL>>        # NOTE: CP949 is a superset of EUC-KR, so the distribution should be<<NEWL>>        #       not different.<<NEWL>>        self.distribution_analyzer = EUCKRDistributionAnalysis()<<NEWL>>        self.reset()<<NEWL>><<NEWL>>    @property<<NEWL>>    def charset_name(self) -> str:<<NEWL>>        return ""CP949""<<NEWL>><<NEWL>>    @property<<NEWL>>    def language(self) -> str:<<NEWL>>        return ""Korean"""
294	jackson	2	"# Copyright 2021 Google LLC<<NEWL>>#<<NEWL>># Licensed under the Apache License, Version 2.0 (the ""License"");<<NEWL>># you may not use this file except in compliance with the License.<<NEWL>># You may obtain a copy of the License at<<NEWL>>#<<NEWL>>#      http://www.apache.org/licenses/LICENSE-2.0<<NEWL>>#<<NEWL>># Unless required by applicable law or agreed to in writing, software<<NEWL>># distributed under the License is distributed on an ""AS IS"" BASIS,<<NEWL>># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.<<NEWL>># See the License for the specific language governing permissions and<<NEWL>># limitations under the License.<<NEWL>><<NEWL>>import os<<NEWL>><<NEWL>>from google.cloud import workflows_v1beta<<NEWL>><<NEWL>>import main<<NEWL>><<NEWL>>PROJECT = os.environ[""GOOGLE_CLOUD_PROJECT""]<<NEWL>>LOCATION = ""us-central1""<<NEWL>>WORKFLOW_ID = ""myFirstWorkflow""<<NEWL>><<NEWL>><<NEWL>>def test_workflow_execution():<<NEWL>>    assert PROJECT != """"<<NEWL>><<NEWL>>    if not workflow_exists():<<NEWL>>        workflow_file = open(""myFirstWorkflow.workflows.yaml"", ""r"").read()<<NEWL>><<NEWL>>        workflows_client = workflows_v1beta.WorkflowsClient()<<NEWL>>        workflows_client.create_workflow(request={<<NEWL>>            # Manually construct the location<<NEWL>>            # https://github.com/googleapis/python-workflows/issues/21<<NEWL>>            ""parent"": f'projects/{PROJECT}/locations/{LOCATION}',<<NEWL>>            ""workflow_id"": WORKFLOW_ID,<<NEWL>>            ""workflow"": {<<NEWL>>                ""name"": WORKFLOW_ID,<<NEWL>>                ""source_contents"": workflow_file<<NEWL>>            }<<NEWL>>        })<<NEWL>><<NEWL>>    result = main.execute_workflow(PROJECT)<<NEWL>>    assert len(result) > 0<<NEWL>><<NEWL>><<NEWL>>def workflow_exists():<<NEWL>>    """"""Returns True if the workflow exists in this project<<NEWL>>    """"""<<NEWL>>    try:<<NEWL>>        workflows_client = workflows_v1beta.WorkflowsClient()<<NEWL>>        workflow_name = workflows_client.workflow_path(PROJECT, LOCATION, WORKFLOW_ID)<<NEWL>>        workflows_client.get_workflow(request={""name"": workflow_name})<<NEWL>>        return True<<NEWL>>    except Exception as e:<<NEWL>>        print(f""Workflow doesn't exist: {e}"")<<NEWL>>        return False"
294	donghui	1	"# Copyright 2021 Google LLC<<NEWL>>#<<NEWL>># Licensed under the Apache License, Version 2.0 (the ""License"");<<NEWL>># you may not use this file except in compliance with the License.<<NEWL>># You may obtain a copy of the License at<<NEWL>>#<<NEWL>>#      http://www.apache.org/licenses/LICENSE-2.0<<NEWL>>#<<NEWL>># Unless required by applicable law or agreed to in writing, software<<NEWL>># distributed under the License is distributed on an ""AS IS"" BASIS,<<NEWL>># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.<<NEWL>># See the License for the specific language governing permissions and<<NEWL>># limitations under the License.<<NEWL>><<NEWL>>import os<<NEWL>><<NEWL>>from google.cloud import workflows_v1beta<<NEWL>><<NEWL>>import main<<NEWL>><<NEWL>>PROJECT = os.environ[""GOOGLE_CLOUD_PROJECT""]<<NEWL>>LOCATION = ""us-central1""<<NEWL>>WORKFLOW_ID = ""myFirstWorkflow""<<NEWL>><<NEWL>><<NEWL>>def test_workflow_execution():<<NEWL>>    assert PROJECT != """"<<NEWL>><<NEWL>>    if not workflow_exists():<<NEWL>>        workflow_file = open(""myFirstWorkflow.workflows.yaml"", ""r"").read()<<NEWL>><<NEWL>>        workflows_client = workflows_v1beta.WorkflowsClient()<<NEWL>>        workflows_client.create_workflow(request={<<NEWL>>            # Manually construct the location<<NEWL>>            # https://github.com/googleapis/python-workflows/issues/21<<NEWL>>            ""parent"": f'projects/{PROJECT}/locations/{LOCATION}',<<NEWL>>            ""workflow_id"": WORKFLOW_ID,<<NEWL>>            ""workflow"": {<<NEWL>>                ""name"": WORKFLOW_ID,<<NEWL>>                ""source_contents"": workflow_file<<NEWL>>            }<<NEWL>>        })<<NEWL>><<NEWL>>    result = main.execute_workflow(PROJECT)<<NEWL>>    assert len(result) > 0<<NEWL>><<NEWL>><<NEWL>>def workflow_exists():<<NEWL>>    """"""Returns True if the workflow exists in this project<<NEWL>>    """"""<<NEWL>>    try:<<NEWL>>        workflows_client = workflows_v1beta.WorkflowsClient()<<NEWL>>        workflow_name = workflows_client.workflow_path(PROJECT, LOCATION, WORKFLOW_ID)<<NEWL>>        workflows_client.get_workflow(request={""name"": workflow_name})<<NEWL>>        return True<<NEWL>>    except Exception as e:<<NEWL>>        print(f""Workflow doesn't exist: {e}"")<<NEWL>>        return False"
385	jackson	2	"import _plotly_utils.basevalidators<<NEWL>><<NEWL>><<NEWL>>class FontValidator(_plotly_utils.basevalidators.CompoundValidator):<<NEWL>>    def __init__(<<NEWL>>        self, plotly_name=""font"", parent_name=""histogram2dcontour.hoverlabel"", **kwargs<<NEWL>>    ):<<NEWL>>        super(FontValidator, self).__init__(<<NEWL>>            plotly_name=plotly_name,<<NEWL>>            parent_name=parent_name,<<NEWL>>            data_class_str=kwargs.pop(""data_class_str"", ""Font""),<<NEWL>>            data_docs=kwargs.pop(<<NEWL>>                ""data_docs"",<<NEWL>>                """"""<<NEWL>>            color<<NEWL>><<NEWL>>            colorsrc<<NEWL>>                Sets the source reference on Chart Studio Cloud<<NEWL>>                for `color`.<<NEWL>>            family<<NEWL>>                HTML font family - the typeface that will be<<NEWL>>                applied by the web browser. The web browser<<NEWL>>                will only be able to apply a font if it is<<NEWL>>                available on the system which it operates.<<NEWL>>                Provide multiple font families, separated by<<NEWL>>                commas, to indicate the preference in which to<<NEWL>>                apply fonts if they aren't available on the<<NEWL>>                system. The Chart Studio Cloud (at<<NEWL>>                https://chart-studio.plotly.com or on-premise)<<NEWL>>                generates images on a server, where only a<<NEWL>>                select number of fonts are installed and<<NEWL>>                supported. These include ""Arial"", ""Balto"",<<NEWL>>                ""Courier New"", ""Droid Sans"",, ""Droid Serif"",<<NEWL>>                ""Droid Sans Mono"", ""Gravitas One"", ""Old<<NEWL>>                Standard TT"", ""Open Sans"", ""Overpass"", ""PT Sans<<NEWL>>                Narrow"", ""Raleway"", ""Times New Roman"".<<NEWL>>            familysrc<<NEWL>>                Sets the source reference on Chart Studio Cloud<<NEWL>>                for `family`.<<NEWL>>            size<<NEWL>><<NEWL>>            sizesrc<<NEWL>>                Sets the source reference on Chart Studio Cloud<<NEWL>>                for `size`.<<NEWL>>"""""",<<NEWL>>            ),<<NEWL>>            **kwargs,<<NEWL>>        )"
385	donghui	1	"import _plotly_utils.basevalidators<<NEWL>><<NEWL>><<NEWL>>class FontValidator(_plotly_utils.basevalidators.CompoundValidator):<<NEWL>>    def __init__(<<NEWL>>        self, plotly_name=""font"", parent_name=""histogram2dcontour.hoverlabel"", **kwargs<<NEWL>>    ):<<NEWL>>        super(FontValidator, self).__init__(<<NEWL>>            plotly_name=plotly_name,<<NEWL>>            parent_name=parent_name,<<NEWL>>            data_class_str=kwargs.pop(""data_class_str"", ""Font""),<<NEWL>>            data_docs=kwargs.pop(<<NEWL>>                ""data_docs"",<<NEWL>>                """"""<<NEWL>>            color<<NEWL>><<NEWL>>            colorsrc<<NEWL>>                Sets the source reference on Chart Studio Cloud<<NEWL>>                for `color`.<<NEWL>>            family<<NEWL>>                HTML font family - the typeface that will be<<NEWL>>                applied by the web browser. The web browser<<NEWL>>                will only be able to apply a font if it is<<NEWL>>                available on the system which it operates.<<NEWL>>                Provide multiple font families, separated by<<NEWL>>                commas, to indicate the preference in which to<<NEWL>>                apply fonts if they aren't available on the<<NEWL>>                system. The Chart Studio Cloud (at<<NEWL>>                https://chart-studio.plotly.com or on-premise)<<NEWL>>                generates images on a server, where only a<<NEWL>>                select number of fonts are installed and<<NEWL>>                supported. These include ""Arial"", ""Balto"",<<NEWL>>                ""Courier New"", ""Droid Sans"",, ""Droid Serif"",<<NEWL>>                ""Droid Sans Mono"", ""Gravitas One"", ""Old<<NEWL>>                Standard TT"", ""Open Sans"", ""Overpass"", ""PT Sans<<NEWL>>                Narrow"", ""Raleway"", ""Times New Roman"".<<NEWL>>            familysrc<<NEWL>>                Sets the source reference on Chart Studio Cloud<<NEWL>>                for `family`.<<NEWL>>            size<<NEWL>><<NEWL>>            sizesrc<<NEWL>>                Sets the source reference on Chart Studio Cloud<<NEWL>>                for `size`.<<NEWL>>"""""",<<NEWL>>            ),<<NEWL>>            **kwargs,<<NEWL>>        )"
313	jackson	2	"import _plotly_utils.basevalidators<<NEWL>><<NEWL>><<NEWL>>class FontValidator(_plotly_utils.basevalidators.CompoundValidator):<<NEWL>>    def __init__(<<NEWL>>        self, plotly_name=""font"", parent_name=""scatterpolar.hoverlabel"", **kwargs<<NEWL>>    ):<<NEWL>>        super(FontValidator, self).__init__(<<NEWL>>            plotly_name=plotly_name,<<NEWL>>            parent_name=parent_name,<<NEWL>>            data_class_str=kwargs.pop(""data_class_str"", ""Font""),<<NEWL>>            data_docs=kwargs.pop(<<NEWL>>                ""data_docs"",<<NEWL>>                """"""<<NEWL>>            color<<NEWL>><<NEWL>>            colorsrc<<NEWL>>                Sets the source reference on Chart Studio Cloud<<NEWL>>                for `color`.<<NEWL>>            family<<NEWL>>                HTML font family - the typeface that will be<<NEWL>>                applied by the web browser. The web browser<<NEWL>>                will only be able to apply a font if it is<<NEWL>>                available on the system which it operates.<<NEWL>>                Provide multiple font families, separated by<<NEWL>>                commas, to indicate the preference in which to<<NEWL>>                apply fonts if they aren't available on the<<NEWL>>                system. The Chart Studio Cloud (at<<NEWL>>                https://chart-studio.plotly.com or on-premise)<<NEWL>>                generates images on a server, where only a<<NEWL>>                select number of fonts are installed and<<NEWL>>                supported. These include ""Arial"", ""Balto"",<<NEWL>>                ""Courier New"", ""Droid Sans"",, ""Droid Serif"",<<NEWL>>                ""Droid Sans Mono"", ""Gravitas One"", ""Old<<NEWL>>                Standard TT"", ""Open Sans"", ""Overpass"", ""PT Sans<<NEWL>>                Narrow"", ""Raleway"", ""Times New Roman"".<<NEWL>>            familysrc<<NEWL>>                Sets the source reference on Chart Studio Cloud<<NEWL>>                for `family`.<<NEWL>>            size<<NEWL>><<NEWL>>            sizesrc<<NEWL>>                Sets the source reference on Chart Studio Cloud<<NEWL>>                for `size`.<<NEWL>>"""""",<<NEWL>>            ),<<NEWL>>            **kwargs,<<NEWL>>        )"
313	donghui	1	"import _plotly_utils.basevalidators<<NEWL>><<NEWL>><<NEWL>>class FontValidator(_plotly_utils.basevalidators.CompoundValidator):<<NEWL>>    def __init__(<<NEWL>>        self, plotly_name=""font"", parent_name=""scatterpolar.hoverlabel"", **kwargs<<NEWL>>    ):<<NEWL>>        super(FontValidator, self).__init__(<<NEWL>>            plotly_name=plotly_name,<<NEWL>>            parent_name=parent_name,<<NEWL>>            data_class_str=kwargs.pop(""data_class_str"", ""Font""),<<NEWL>>            data_docs=kwargs.pop(<<NEWL>>                ""data_docs"",<<NEWL>>                """"""<<NEWL>>            color<<NEWL>><<NEWL>>            colorsrc<<NEWL>>                Sets the source reference on Chart Studio Cloud<<NEWL>>                for `color`.<<NEWL>>            family<<NEWL>>                HTML font family - the typeface that will be<<NEWL>>                applied by the web browser. The web browser<<NEWL>>                will only be able to apply a font if it is<<NEWL>>                available on the system which it operates.<<NEWL>>                Provide multiple font families, separated by<<NEWL>>                commas, to indicate the preference in which to<<NEWL>>                apply fonts if they aren't available on the<<NEWL>>                system. The Chart Studio Cloud (at<<NEWL>>                https://chart-studio.plotly.com or on-premise)<<NEWL>>                generates images on a server, where only a<<NEWL>>                select number of fonts are installed and<<NEWL>>                supported. These include ""Arial"", ""Balto"",<<NEWL>>                ""Courier New"", ""Droid Sans"",, ""Droid Serif"",<<NEWL>>                ""Droid Sans Mono"", ""Gravitas One"", ""Old<<NEWL>>                Standard TT"", ""Open Sans"", ""Overpass"", ""PT Sans<<NEWL>>                Narrow"", ""Raleway"", ""Times New Roman"".<<NEWL>>            familysrc<<NEWL>>                Sets the source reference on Chart Studio Cloud<<NEWL>>                for `family`.<<NEWL>>            size<<NEWL>><<NEWL>>            sizesrc<<NEWL>>                Sets the source reference on Chart Studio Cloud<<NEWL>>                for `size`.<<NEWL>>"""""",<<NEWL>>            ),<<NEWL>>            **kwargs,<<NEWL>>        )"
342	jackson	0	"from playwright.sync_api import sync_playwright<<NEWL>>import pandas as pd<<NEWL>>import numpy as np<<NEWL>><<NEWL>>def pagina_min_fazenda(cidade, estado, pagina):<<NEWL>>    pagina.goto(""https://www.airbnb.com.br/"")<<NEWL>><<NEWL>><<NEWL>>    #FAZENDO A PESQUISA <<NEWL>>    pagina.locator('''xpath = /html/body/div[5]/div/div/div[1]/div/div[2]/div/div/div/div/div/div[2]/div/div/div/<<NEWL>>    header/div/div[2]/div[1]/div/button[1]''').click()<<NEWL>>    pagina.locator('''xpath = /html/body/div[5]/div/div/div[1]/div/div[2]/div/div/div/div/div/div[2]/div/div/div/<<NEWL>>    header/div/div[2]/div[2]/div/div/div/form/div[2]/div/div[1]/div/label/div/input''').fill(cidade + ', ' + estado)<<NEWL>>    if cidade in pagina.locator('''/html/body/div[5]/div/div/div[1]/div/div[2]/div/div/div/div/div/div[2]/div/div/div/header/div/div[2]/div[2]/div/div/div/form/div[2]/div/div[1]/div/div[2]/div''').inner_text():<<NEWL>>        pagina.locator('''xpath = /html/body/div[5]/div/div/div[1]/div/div[2]/div/div/div/div/div/div[2]/div/div/div/header/div/div[2]/div[2]/div/div/div/form/div[2]/div/div[1]/div/div[2]/div''').locator('nth = 0').click()<<NEWL>>        pagina.locator('''xpath = /html/body/div[5]/div/div/div[1]/div/div[2]/div/div/div/div/div/div[2]/div/div/div/header/div/div[2]/div[2]/div/div/div/form/div[2]/div/div[3]/div[2]/div/div/section/div/div/div/div/div[1]/div/div[1]/div/button[2]''').click()<<NEWL>>        pagina.locator('''xpath = /html/body/div[5]/div/div/div[1]/div/div[2]/div/div/div/div/div/div[2]/div/div/div/header/div/div[2]/div[2]/div/div/div/form/div[2]/div/div[3]/div[2]/div/div/section/div/div/div/div/div[2]/div[2]/div/div[1]/div[2]/div[2]/label''').click()<<NEWL>>        pagina.locator('''xpath = /html/body/div[5]/div/div/div[1]/div/div[2]/div/div/div/div/div/div[2]/div/div/div/header/div/div[2]/div[2]/div/div/div/form/div[2]/div/div[5]/div[1]/div[2]/button/div/div[1]/svg''').click()<<NEWL>><<NEWL>><<NEWL>>    return <<NEWL>>with sync_playwright() as p:<<NEWL>>    <<NEWL>>    navegador = p.chromium.launch(headless = False)<<NEWL>>    pagina = navegador.new_page(viewport = {'width': 1200, 'height': 800})"
342	donghui	0	"from playwright.sync_api import sync_playwright<<NEWL>>import pandas as pd<<NEWL>>import numpy as np<<NEWL>><<NEWL>>def pagina_min_fazenda(cidade, estado, pagina):<<NEWL>>    pagina.goto(""https://www.airbnb.com.br/"")<<NEWL>><<NEWL>><<NEWL>>    #FAZENDO A PESQUISA <<NEWL>>    pagina.locator('''xpath = /html/body/div[5]/div/div/div[1]/div/div[2]/div/div/div/div/div/div[2]/div/div/div/<<NEWL>>    header/div/div[2]/div[1]/div/button[1]''').click()<<NEWL>>    pagina.locator('''xpath = /html/body/div[5]/div/div/div[1]/div/div[2]/div/div/div/div/div/div[2]/div/div/div/<<NEWL>>    header/div/div[2]/div[2]/div/div/div/form/div[2]/div/div[1]/div/label/div/input''').fill(cidade + ', ' + estado)<<NEWL>>    if cidade in pagina.locator('''/html/body/div[5]/div/div/div[1]/div/div[2]/div/div/div/div/div/div[2]/div/div/div/header/div/div[2]/div[2]/div/div/div/form/div[2]/div/div[1]/div/div[2]/div''').inner_text():<<NEWL>>        pagina.locator('''xpath = /html/body/div[5]/div/div/div[1]/div/div[2]/div/div/div/div/div/div[2]/div/div/div/header/div/div[2]/div[2]/div/div/div/form/div[2]/div/div[1]/div/div[2]/div''').locator('nth = 0').click()<<NEWL>>        pagina.locator('''xpath = /html/body/div[5]/div/div/div[1]/div/div[2]/div/div/div/div/div/div[2]/div/div/div/header/div/div[2]/div[2]/div/div/div/form/div[2]/div/div[3]/div[2]/div/div/section/div/div/div/div/div[1]/div/div[1]/div/button[2]''').click()<<NEWL>>        pagina.locator('''xpath = /html/body/div[5]/div/div/div[1]/div/div[2]/div/div/div/div/div/div[2]/div/div/div/header/div/div[2]/div[2]/div/div/div/form/div[2]/div/div[3]/div[2]/div/div/section/div/div/div/div/div[2]/div[2]/div/div[1]/div[2]/div[2]/label''').click()<<NEWL>>        pagina.locator('''xpath = /html/body/div[5]/div/div/div[1]/div/div[2]/div/div/div/div/div/div[2]/div/div/div/header/div/div[2]/div[2]/div/div/div/form/div[2]/div/div[5]/div[1]/div[2]/button/div/div[1]/svg''').click()<<NEWL>><<NEWL>><<NEWL>>    return <<NEWL>>with sync_playwright() as p:<<NEWL>>    <<NEWL>>    navegador = p.chromium.launch(headless = False)<<NEWL>>    pagina = navegador.new_page(viewport = {'width': 1200, 'height': 800})"
353	jackson	0	"import json<<NEWL>>import os<<NEWL>>import time<<NEWL>>import random<<NEWL>>from linkedin_api import Linkedin<<NEWL>>from dotenv import load_dotenv<<NEWL>>import sys<<NEWL>><<NEWL>>class Scrape():<<NEWL>>    def __init__(self, api):<<NEWL>>        self.api = api<<NEWL>>        <<NEWL>>    def read_json(self, filename='jobs.json'):<<NEWL>>        with open(filename, 'r') as file:<<NEWL>>            return json.load(file)<<NEWL>><<NEWL>>    def write_json(self, newData):<<NEWL>>        with open('jobs.json', 'r+') as file:<<NEWL>>            data = self.read_json()<<NEWL>>            data['job-list'].append(newData)<<NEWL>>            json.dump(data, file)<<NEWL>><<NEWL>>    def searchJobs(self, apiChosen, numberOfSearches, keywordChosen, offsetNumber):<<NEWL>>        jobs = apiChosen.search_jobs(keywordChosen, remote = 1, limit = \<<NEWL>>                            numberOfSearches, offset = offsetNumber)<<NEWL>>        for job in jobs:<<NEWL>>            title = job['title']<<NEWL>>            jobID = job['dashEntityUrn'].split(':')[-1] <<NEWL>>            location = job['formattedLocation']<<NEWL>>            #jobDetails = api.get_job(jobID)<<NEWL>>            jobLink = f'https://www.linkedin.com/jobs/view/{jobID}/'<<NEWL>>            job = {<<NEWL>>                ""Job title"":title,<<NEWL>>                ""Job link"":jobLink,<<NEWL>>                ""Location"":location<<NEWL>>            }<<NEWL>>            print(f""{title} : {jobID} : {location}"")<<NEWL>>            self.write_json(job)<<NEWL>>        <<NEWL>><<NEWL>>    def findSWEJobs(self, apiChosen):<<NEWL>>        listOfJobs = [""Software Developer"",""Software Engineer"", ""Software Intern"",""SDET"",""Developer Intern"",""Software co-op"",""Junior Developer""] <<NEWL>>        for i in range(11,101):<<NEWL>>            <<NEWL>>            for element in listOfJobs:<<NEWL>>                self.searchJobs(apiChosen, 1, element, i)<<NEWL>>                time.sleep(1*random.randint(1,5)+2)<<NEWL>>    <<NEWL>>if(__name__ == ""__main__""):<<NEWL>>    load_dotenv()<<NEWL>>    Password = os.getenv('PASSWORD')<<NEWL>>    Email = os.getenv('EMAIL')<<NEWL>>    api = Linkedin(Email, Password)<<NEWL>>    data = {""job-list"": [{}]}<<NEWL>>    with open('jobs.json', 'w') as file:<<NEWL>>        json.dump(data, file)<<NEWL>><<NEWL>>    scraper = Scrape(api)<<NEWL>>    scraper.findSWEJobs(api)<<NEWL>>    <<NEWL>>    "
353	donghui	0	"import json<<NEWL>>import os<<NEWL>>import time<<NEWL>>import random<<NEWL>>from linkedin_api import Linkedin<<NEWL>>from dotenv import load_dotenv<<NEWL>>import sys<<NEWL>><<NEWL>>class Scrape():<<NEWL>>    def __init__(self, api):<<NEWL>>        self.api = api<<NEWL>>        <<NEWL>>    def read_json(self, filename='jobs.json'):<<NEWL>>        with open(filename, 'r') as file:<<NEWL>>            return json.load(file)<<NEWL>><<NEWL>>    def write_json(self, newData):<<NEWL>>        with open('jobs.json', 'r+') as file:<<NEWL>>            data = self.read_json()<<NEWL>>            data['job-list'].append(newData)<<NEWL>>            json.dump(data, file)<<NEWL>><<NEWL>>    def searchJobs(self, apiChosen, numberOfSearches, keywordChosen, offsetNumber):<<NEWL>>        jobs = apiChosen.search_jobs(keywordChosen, remote = 1, limit = \<<NEWL>>                            numberOfSearches, offset = offsetNumber)<<NEWL>>        for job in jobs:<<NEWL>>            title = job['title']<<NEWL>>            jobID = job['dashEntityUrn'].split(':')[-1] <<NEWL>>            location = job['formattedLocation']<<NEWL>>            #jobDetails = api.get_job(jobID)<<NEWL>>            jobLink = f'https://www.linkedin.com/jobs/view/{jobID}/'<<NEWL>>            job = {<<NEWL>>                ""Job title"":title,<<NEWL>>                ""Job link"":jobLink,<<NEWL>>                ""Location"":location<<NEWL>>            }<<NEWL>>            print(f""{title} : {jobID} : {location}"")<<NEWL>>            self.write_json(job)<<NEWL>>        <<NEWL>><<NEWL>>    def findSWEJobs(self, apiChosen):<<NEWL>>        listOfJobs = [""Software Developer"",""Software Engineer"", ""Software Intern"",""SDET"",""Developer Intern"",""Software co-op"",""Junior Developer""] <<NEWL>>        for i in range(11,101):<<NEWL>>            <<NEWL>>            for element in listOfJobs:<<NEWL>>                self.searchJobs(apiChosen, 1, element, i)<<NEWL>>                time.sleep(1*random.randint(1,5)+2)<<NEWL>>    <<NEWL>>if(__name__ == ""__main__""):<<NEWL>>    load_dotenv()<<NEWL>>    Password = os.getenv('PASSWORD')<<NEWL>>    Email = os.getenv('EMAIL')<<NEWL>>    api = Linkedin(Email, Password)<<NEWL>>    data = {""job-list"": [{}]}<<NEWL>>    with open('jobs.json', 'w') as file:<<NEWL>>        json.dump(data, file)<<NEWL>><<NEWL>>    scraper = Scrape(api)<<NEWL>>    scraper.findSWEJobs(api)<<NEWL>>    <<NEWL>>    "
302	jackson	3	"from rx.core import Observable<<NEWL>>from rx.internal import extensionmethod<<NEWL>>import math<<NEWL>><<NEWL>><<NEWL>>def determine_median(sorted_list):<<NEWL>>    if len(sorted_list) == 0:<<NEWL>>        raise Exception(""The input sequence was empty"")<<NEWL>><<NEWL>>    if len(sorted_list) % 2 == 1:<<NEWL>>        return sorted_list[int((len(sorted_list) + 1) / 2) - 1]<<NEWL>>    else:<<NEWL>>        median_1 = sorted_list[int((len(sorted_list) + 1) / 2) - 1]<<NEWL>>        median_2 = sorted_list[int((len(sorted_list) + 1) / 2)]<<NEWL>>        return float(median_1 + median_2) / 2.0<<NEWL>><<NEWL>><<NEWL>>@extensionmethod(Observable)<<NEWL>>def median(self):<<NEWL>>    """"""<<NEWL>>    Calculates the statistical median on numerical emissions. The sequence must be finite.<<NEWL>>    """"""<<NEWL>>    return self.to_sorted_list().map(lambda l: determine_median(l))<<NEWL>><<NEWL>><<NEWL>>@extensionmethod(Observable)<<NEWL>>def mode(self):<<NEWL>>    """"""<<NEWL>>    Returns the most frequently emitted value (or ""values"" if they have the same number of occurrences).<<NEWL>>    The sequence must be finite.<<NEWL>>    """"""<<NEWL>>    return self.group_by(lambda v: v) \<<NEWL>>        .flat_map(lambda grp: grp.count().map(lambda ct: (grp.key, ct))) \<<NEWL>>        .to_sorted_list(lambda t: t[1], reverse=True) \<<NEWL>>        .flat_map(lambda l: Observable.from_(l).take_while(lambda t: t[1] == l[0][1])) \<<NEWL>>        .map(lambda t: t[0])<<NEWL>><<NEWL>><<NEWL>>@extensionmethod(Observable)<<NEWL>>def variance(self):<<NEWL>>    """"""<<NEWL>>    Returns the statistical variance of the numerical emissions.<<NEWL>>    The sequence must be finite.<<NEWL>>    """"""<<NEWL>>    squared_values = self.to_list() \<<NEWL>>        .flat_map(lambda l: Observable.from_(l).average().flat_map(lambda avg: Observable.from_(l).map(lambda i: i - avg))) \<<NEWL>>        .map(lambda i: i * i) \<<NEWL>>        .publish() \<<NEWL>>        .auto_connect(2)<<NEWL>><<NEWL>>    return Observable.zip(squared_values.sum(), squared_values.count(), lambda sum, ct: sum / (ct - 1))<<NEWL>><<NEWL>><<NEWL>>@extensionmethod(Observable)<<NEWL>>def standard_deviation(self):<<NEWL>>    """"""<<NEWL>>    Returns the standard deviation of the numerical emissions:<<NEWL>>    The sequence must be finite.<<NEWL>>    """"""<<NEWL>>    return self.variance().map(lambda i: math.sqrt(i))<<NEWL>><<NEWL>>"
302	donghui	2	"from rx.core import Observable<<NEWL>>from rx.internal import extensionmethod<<NEWL>>import math<<NEWL>><<NEWL>><<NEWL>>def determine_median(sorted_list):<<NEWL>>    if len(sorted_list) == 0:<<NEWL>>        raise Exception(""The input sequence was empty"")<<NEWL>><<NEWL>>    if len(sorted_list) % 2 == 1:<<NEWL>>        return sorted_list[int((len(sorted_list) + 1) / 2) - 1]<<NEWL>>    else:<<NEWL>>        median_1 = sorted_list[int((len(sorted_list) + 1) / 2) - 1]<<NEWL>>        median_2 = sorted_list[int((len(sorted_list) + 1) / 2)]<<NEWL>>        return float(median_1 + median_2) / 2.0<<NEWL>><<NEWL>><<NEWL>>@extensionmethod(Observable)<<NEWL>>def median(self):<<NEWL>>    """"""<<NEWL>>    Calculates the statistical median on numerical emissions. The sequence must be finite.<<NEWL>>    """"""<<NEWL>>    return self.to_sorted_list().map(lambda l: determine_median(l))<<NEWL>><<NEWL>><<NEWL>>@extensionmethod(Observable)<<NEWL>>def mode(self):<<NEWL>>    """"""<<NEWL>>    Returns the most frequently emitted value (or ""values"" if they have the same number of occurrences).<<NEWL>>    The sequence must be finite.<<NEWL>>    """"""<<NEWL>>    return self.group_by(lambda v: v) \<<NEWL>>        .flat_map(lambda grp: grp.count().map(lambda ct: (grp.key, ct))) \<<NEWL>>        .to_sorted_list(lambda t: t[1], reverse=True) \<<NEWL>>        .flat_map(lambda l: Observable.from_(l).take_while(lambda t: t[1] == l[0][1])) \<<NEWL>>        .map(lambda t: t[0])<<NEWL>><<NEWL>><<NEWL>>@extensionmethod(Observable)<<NEWL>>def variance(self):<<NEWL>>    """"""<<NEWL>>    Returns the statistical variance of the numerical emissions.<<NEWL>>    The sequence must be finite.<<NEWL>>    """"""<<NEWL>>    squared_values = self.to_list() \<<NEWL>>        .flat_map(lambda l: Observable.from_(l).average().flat_map(lambda avg: Observable.from_(l).map(lambda i: i - avg))) \<<NEWL>>        .map(lambda i: i * i) \<<NEWL>>        .publish() \<<NEWL>>        .auto_connect(2)<<NEWL>><<NEWL>>    return Observable.zip(squared_values.sum(), squared_values.count(), lambda sum, ct: sum / (ct - 1))<<NEWL>><<NEWL>><<NEWL>>@extensionmethod(Observable)<<NEWL>>def standard_deviation(self):<<NEWL>>    """"""<<NEWL>>    Returns the standard deviation of the numerical emissions:<<NEWL>>    The sequence must be finite.<<NEWL>>    """"""<<NEWL>>    return self.variance().map(lambda i: math.sqrt(i))<<NEWL>><<NEWL>>"
394	jackson	2	"import sys<<NEWL>>from dataclasses import dataclass<<NEWL>><<NEWL>><<NEWL>>@dataclass<<NEWL>>class WindowsConsoleFeatures:<<NEWL>>    """"""Windows features available.""""""<<NEWL>><<NEWL>>    vt: bool = False<<NEWL>>    """"""The console supports VT codes.""""""<<NEWL>>    truecolor: bool = False<<NEWL>>    """"""The console supports truecolor.""""""<<NEWL>><<NEWL>><<NEWL>>try:<<NEWL>>    import ctypes<<NEWL>>    from ctypes import LibraryLoader<<NEWL>><<NEWL>>    if sys.platform == ""win32"":<<NEWL>>        windll = LibraryLoader(ctypes.WinDLL)<<NEWL>>    else:<<NEWL>>        windll = None<<NEWL>>        raise ImportError(""Not windows"")<<NEWL>><<NEWL>>    from pip._vendor.rich._win32_console import (<<NEWL>>        ENABLE_VIRTUAL_TERMINAL_PROCESSING,<<NEWL>>        GetConsoleMode,<<NEWL>>        GetStdHandle,<<NEWL>>        LegacyWindowsError,<<NEWL>>    )<<NEWL>><<NEWL>>except (AttributeError, ImportError, ValueError):<<NEWL>><<NEWL>>    # Fallback if we can't load the Windows DLL<<NEWL>>    def get_windows_console_features() -> WindowsConsoleFeatures:<<NEWL>>        features = WindowsConsoleFeatures()<<NEWL>>        return features<<NEWL>><<NEWL>>else:<<NEWL>><<NEWL>>    def get_windows_console_features() -> WindowsConsoleFeatures:<<NEWL>>        """"""Get windows console features.<<NEWL>><<NEWL>>        Returns:<<NEWL>>            WindowsConsoleFeatures: An instance of WindowsConsoleFeatures.<<NEWL>>        """"""<<NEWL>>        handle = GetStdHandle()<<NEWL>>        try:<<NEWL>>            console_mode = GetConsoleMode(handle)<<NEWL>>            success = True<<NEWL>>        except LegacyWindowsError:<<NEWL>>            console_mode = 0<<NEWL>>            success = False<<NEWL>>        vt = bool(success and console_mode & ENABLE_VIRTUAL_TERMINAL_PROCESSING)<<NEWL>>        truecolor = False<<NEWL>>        if vt:<<NEWL>>            win_version = sys.getwindowsversion()<<NEWL>>            truecolor = win_version.major > 10 or (<<NEWL>>                win_version.major == 10 and win_version.build >= 15063<<NEWL>>            )<<NEWL>>        features = WindowsConsoleFeatures(vt=vt, truecolor=truecolor)<<NEWL>>        return features<<NEWL>><<NEWL>><<NEWL>>if __name__ == ""__main__"":<<NEWL>>    import platform<<NEWL>><<NEWL>>    features = get_windows_console_features()<<NEWL>>    from pip._vendor.rich import print<<NEWL>><<NEWL>>    print(f'platform=""{platform.system()}""')<<NEWL>>    print(repr(features))"
394	donghui	2	"import sys<<NEWL>>from dataclasses import dataclass<<NEWL>><<NEWL>><<NEWL>>@dataclass<<NEWL>>class WindowsConsoleFeatures:<<NEWL>>    """"""Windows features available.""""""<<NEWL>><<NEWL>>    vt: bool = False<<NEWL>>    """"""The console supports VT codes.""""""<<NEWL>>    truecolor: bool = False<<NEWL>>    """"""The console supports truecolor.""""""<<NEWL>><<NEWL>><<NEWL>>try:<<NEWL>>    import ctypes<<NEWL>>    from ctypes import LibraryLoader<<NEWL>><<NEWL>>    if sys.platform == ""win32"":<<NEWL>>        windll = LibraryLoader(ctypes.WinDLL)<<NEWL>>    else:<<NEWL>>        windll = None<<NEWL>>        raise ImportError(""Not windows"")<<NEWL>><<NEWL>>    from pip._vendor.rich._win32_console import (<<NEWL>>        ENABLE_VIRTUAL_TERMINAL_PROCESSING,<<NEWL>>        GetConsoleMode,<<NEWL>>        GetStdHandle,<<NEWL>>        LegacyWindowsError,<<NEWL>>    )<<NEWL>><<NEWL>>except (AttributeError, ImportError, ValueError):<<NEWL>><<NEWL>>    # Fallback if we can't load the Windows DLL<<NEWL>>    def get_windows_console_features() -> WindowsConsoleFeatures:<<NEWL>>        features = WindowsConsoleFeatures()<<NEWL>>        return features<<NEWL>><<NEWL>>else:<<NEWL>><<NEWL>>    def get_windows_console_features() -> WindowsConsoleFeatures:<<NEWL>>        """"""Get windows console features.<<NEWL>><<NEWL>>        Returns:<<NEWL>>            WindowsConsoleFeatures: An instance of WindowsConsoleFeatures.<<NEWL>>        """"""<<NEWL>>        handle = GetStdHandle()<<NEWL>>        try:<<NEWL>>            console_mode = GetConsoleMode(handle)<<NEWL>>            success = True<<NEWL>>        except LegacyWindowsError:<<NEWL>>            console_mode = 0<<NEWL>>            success = False<<NEWL>>        vt = bool(success and console_mode & ENABLE_VIRTUAL_TERMINAL_PROCESSING)<<NEWL>>        truecolor = False<<NEWL>>        if vt:<<NEWL>>            win_version = sys.getwindowsversion()<<NEWL>>            truecolor = win_version.major > 10 or (<<NEWL>>                win_version.major == 10 and win_version.build >= 15063<<NEWL>>            )<<NEWL>>        features = WindowsConsoleFeatures(vt=vt, truecolor=truecolor)<<NEWL>>        return features<<NEWL>><<NEWL>><<NEWL>>if __name__ == ""__main__"":<<NEWL>>    import platform<<NEWL>><<NEWL>>    features = get_windows_console_features()<<NEWL>>    from pip._vendor.rich import print<<NEWL>><<NEWL>>    print(f'platform=""{platform.system()}""')<<NEWL>>    print(repr(features))"
285	jackson	1	"import sys<<NEWL>><<NEWL>><<NEWL>>def patch_sys_module():<<NEWL>><<NEWL>>    def patched_exc_info(fun):<<NEWL>><<NEWL>>        def pydev_debugger_exc_info():<<NEWL>>            type, value, traceback = fun()<<NEWL>>            if type == ImportError:<<NEWL>>                # we should not show frame added by plugin_import call<<NEWL>>                if traceback and hasattr(traceback, ""tb_next""):<<NEWL>>                    return type, value, traceback.tb_next<<NEWL>>            return type, value, traceback<<NEWL>><<NEWL>>        return pydev_debugger_exc_info<<NEWL>><<NEWL>>    system_exc_info = sys.exc_info<<NEWL>>    sys.exc_info = patched_exc_info(system_exc_info)<<NEWL>>    if not hasattr(sys, ""system_exc_info""):<<NEWL>>        sys.system_exc_info = system_exc_info<<NEWL>><<NEWL>><<NEWL>>def patched_reload(orig_reload):<<NEWL>><<NEWL>>    def pydev_debugger_reload(module):<<NEWL>>        orig_reload(module)<<NEWL>>        if module.__name__ == ""sys"":<<NEWL>>            # if sys module was reloaded we should patch it again<<NEWL>>            patch_sys_module()<<NEWL>><<NEWL>>    return pydev_debugger_reload<<NEWL>><<NEWL>><<NEWL>>def patch_reload():<<NEWL>>    import builtins  # Py3<<NEWL>><<NEWL>>    if hasattr(builtins, ""reload""):<<NEWL>>        sys.builtin_orig_reload = builtins.reload<<NEWL>>        builtins.reload = patched_reload(sys.builtin_orig_reload)  # @UndefinedVariable<<NEWL>>        try:<<NEWL>>            import imp<<NEWL>>            sys.imp_orig_reload = imp.reload<<NEWL>>            imp.reload = patched_reload(sys.imp_orig_reload)  # @UndefinedVariable<<NEWL>>        except:<<NEWL>>            pass<<NEWL>>    else:<<NEWL>>        try:<<NEWL>>            import importlib<<NEWL>>            sys.importlib_orig_reload = importlib.reload  # @UndefinedVariable<<NEWL>>            importlib.reload = patched_reload(sys.importlib_orig_reload)  # @UndefinedVariable<<NEWL>>        except:<<NEWL>>            pass<<NEWL>><<NEWL>>    del builtins<<NEWL>><<NEWL>><<NEWL>>def cancel_patches_in_sys_module():<<NEWL>>    sys.exc_info = sys.system_exc_info  # @UndefinedVariable<<NEWL>>    import builtins  # Py3<<NEWL>><<NEWL>>    if hasattr(sys, ""builtin_orig_reload""):<<NEWL>>        builtins.reload = sys.builtin_orig_reload<<NEWL>><<NEWL>>    if hasattr(sys, ""imp_orig_reload""):<<NEWL>>        import imp<<NEWL>>        imp.reload = sys.imp_orig_reload<<NEWL>><<NEWL>>    if hasattr(sys, ""importlib_orig_reload""):<<NEWL>>        import importlib<<NEWL>>        importlib.reload = sys.importlib_orig_reload<<NEWL>><<NEWL>>    del builtins"
285	donghui	1	"import sys<<NEWL>><<NEWL>><<NEWL>>def patch_sys_module():<<NEWL>><<NEWL>>    def patched_exc_info(fun):<<NEWL>><<NEWL>>        def pydev_debugger_exc_info():<<NEWL>>            type, value, traceback = fun()<<NEWL>>            if type == ImportError:<<NEWL>>                # we should not show frame added by plugin_import call<<NEWL>>                if traceback and hasattr(traceback, ""tb_next""):<<NEWL>>                    return type, value, traceback.tb_next<<NEWL>>            return type, value, traceback<<NEWL>><<NEWL>>        return pydev_debugger_exc_info<<NEWL>><<NEWL>>    system_exc_info = sys.exc_info<<NEWL>>    sys.exc_info = patched_exc_info(system_exc_info)<<NEWL>>    if not hasattr(sys, ""system_exc_info""):<<NEWL>>        sys.system_exc_info = system_exc_info<<NEWL>><<NEWL>><<NEWL>>def patched_reload(orig_reload):<<NEWL>><<NEWL>>    def pydev_debugger_reload(module):<<NEWL>>        orig_reload(module)<<NEWL>>        if module.__name__ == ""sys"":<<NEWL>>            # if sys module was reloaded we should patch it again<<NEWL>>            patch_sys_module()<<NEWL>><<NEWL>>    return pydev_debugger_reload<<NEWL>><<NEWL>><<NEWL>>def patch_reload():<<NEWL>>    import builtins  # Py3<<NEWL>><<NEWL>>    if hasattr(builtins, ""reload""):<<NEWL>>        sys.builtin_orig_reload = builtins.reload<<NEWL>>        builtins.reload = patched_reload(sys.builtin_orig_reload)  # @UndefinedVariable<<NEWL>>        try:<<NEWL>>            import imp<<NEWL>>            sys.imp_orig_reload = imp.reload<<NEWL>>            imp.reload = patched_reload(sys.imp_orig_reload)  # @UndefinedVariable<<NEWL>>        except:<<NEWL>>            pass<<NEWL>>    else:<<NEWL>>        try:<<NEWL>>            import importlib<<NEWL>>            sys.importlib_orig_reload = importlib.reload  # @UndefinedVariable<<NEWL>>            importlib.reload = patched_reload(sys.importlib_orig_reload)  # @UndefinedVariable<<NEWL>>        except:<<NEWL>>            pass<<NEWL>><<NEWL>>    del builtins<<NEWL>><<NEWL>><<NEWL>>def cancel_patches_in_sys_module():<<NEWL>>    sys.exc_info = sys.system_exc_info  # @UndefinedVariable<<NEWL>>    import builtins  # Py3<<NEWL>><<NEWL>>    if hasattr(sys, ""builtin_orig_reload""):<<NEWL>>        builtins.reload = sys.builtin_orig_reload<<NEWL>><<NEWL>>    if hasattr(sys, ""imp_orig_reload""):<<NEWL>>        import imp<<NEWL>>        imp.reload = sys.imp_orig_reload<<NEWL>><<NEWL>>    if hasattr(sys, ""importlib_orig_reload""):<<NEWL>>        import importlib<<NEWL>>        importlib.reload = sys.importlib_orig_reload<<NEWL>><<NEWL>>    del builtins"
336	jackson	2	"from fontTools.pens.basePen import BasePen<<NEWL>>from reportlab.graphics.shapes import Path<<NEWL>><<NEWL>><<NEWL>>__all__ = [""ReportLabPen""]<<NEWL>><<NEWL>><<NEWL>>class ReportLabPen(BasePen):<<NEWL>><<NEWL>>    """"""A pen for drawing onto a ``reportlab.graphics.shapes.Path`` object.""""""<<NEWL>><<NEWL>>    def __init__(self, glyphSet, path=None):<<NEWL>>        BasePen.__init__(self, glyphSet)<<NEWL>>        if path is None:<<NEWL>>            path = Path()<<NEWL>>        self.path = path<<NEWL>><<NEWL>>    def _moveTo(self, p):<<NEWL>>        (x, y) = p<<NEWL>>        self.path.moveTo(x, y)<<NEWL>><<NEWL>>    def _lineTo(self, p):<<NEWL>>        (x, y) = p<<NEWL>>        self.path.lineTo(x, y)<<NEWL>><<NEWL>>    def _curveToOne(self, p1, p2, p3):<<NEWL>>        (x1, y1) = p1<<NEWL>>        (x2, y2) = p2<<NEWL>>        (x3, y3) = p3<<NEWL>>        self.path.curveTo(x1, y1, x2, y2, x3, y3)<<NEWL>><<NEWL>>    def _closePath(self):<<NEWL>>        self.path.closePath()<<NEWL>><<NEWL>><<NEWL>>if __name__ == ""__main__"":<<NEWL>>    import sys<<NEWL>><<NEWL>>    if len(sys.argv) < 3:<<NEWL>>        print(<<NEWL>>            ""Usage: reportLabPen.py <OTF/TTF font> <glyphname> [<image file to create>]""<<NEWL>>        )<<NEWL>>        print(<<NEWL>>            ""  If no image file name is created, by default <glyphname>.png is created.""<<NEWL>>        )<<NEWL>>        print(""  example: reportLabPen.py Arial.TTF R test.png"")<<NEWL>>        print(<<NEWL>>            ""  (The file format will be PNG, regardless of the image file name supplied)""<<NEWL>>        )<<NEWL>>        sys.exit(0)<<NEWL>><<NEWL>>    from fontTools.ttLib import TTFont<<NEWL>>    from reportlab.lib import colors<<NEWL>><<NEWL>>    path = sys.argv[1]<<NEWL>>    glyphName = sys.argv[2]<<NEWL>>    if len(sys.argv) > 3:<<NEWL>>        imageFile = sys.argv[3]<<NEWL>>    else:<<NEWL>>        imageFile = ""%s.png"" % glyphName<<NEWL>><<NEWL>>    font = TTFont(path)  # it would work just as well with fontTools.t1Lib.T1Font<<NEWL>>    gs = font.getGlyphSet()<<NEWL>>    pen = ReportLabPen(gs, Path(fillColor=colors.red, strokeWidth=5))<<NEWL>>    g = gs[glyphName]<<NEWL>>    g.draw(pen)<<NEWL>><<NEWL>>    w, h = g.width, 1000<<NEWL>>    from reportlab.graphics import renderPM<<NEWL>>    from reportlab.graphics.shapes import Group, Drawing, scale<<NEWL>><<NEWL>>    # Everything is wrapped in a group to allow transformations.<<NEWL>>    g = Group(pen.path)<<NEWL>>    g.translate(0, 200)<<NEWL>>    g.scale(0.3, 0.3)<<NEWL>><<NEWL>>    d = Drawing(w, h)<<NEWL>>    d.add(g)<<NEWL>><<NEWL>>    renderPM.drawToFile(d, imageFile, fmt=""PNG"")"
336	donghui	2	"from fontTools.pens.basePen import BasePen<<NEWL>>from reportlab.graphics.shapes import Path<<NEWL>><<NEWL>><<NEWL>>__all__ = [""ReportLabPen""]<<NEWL>><<NEWL>><<NEWL>>class ReportLabPen(BasePen):<<NEWL>><<NEWL>>    """"""A pen for drawing onto a ``reportlab.graphics.shapes.Path`` object.""""""<<NEWL>><<NEWL>>    def __init__(self, glyphSet, path=None):<<NEWL>>        BasePen.__init__(self, glyphSet)<<NEWL>>        if path is None:<<NEWL>>            path = Path()<<NEWL>>        self.path = path<<NEWL>><<NEWL>>    def _moveTo(self, p):<<NEWL>>        (x, y) = p<<NEWL>>        self.path.moveTo(x, y)<<NEWL>><<NEWL>>    def _lineTo(self, p):<<NEWL>>        (x, y) = p<<NEWL>>        self.path.lineTo(x, y)<<NEWL>><<NEWL>>    def _curveToOne(self, p1, p2, p3):<<NEWL>>        (x1, y1) = p1<<NEWL>>        (x2, y2) = p2<<NEWL>>        (x3, y3) = p3<<NEWL>>        self.path.curveTo(x1, y1, x2, y2, x3, y3)<<NEWL>><<NEWL>>    def _closePath(self):<<NEWL>>        self.path.closePath()<<NEWL>><<NEWL>><<NEWL>>if __name__ == ""__main__"":<<NEWL>>    import sys<<NEWL>><<NEWL>>    if len(sys.argv) < 3:<<NEWL>>        print(<<NEWL>>            ""Usage: reportLabPen.py <OTF/TTF font> <glyphname> [<image file to create>]""<<NEWL>>        )<<NEWL>>        print(<<NEWL>>            ""  If no image file name is created, by default <glyphname>.png is created.""<<NEWL>>        )<<NEWL>>        print(""  example: reportLabPen.py Arial.TTF R test.png"")<<NEWL>>        print(<<NEWL>>            ""  (The file format will be PNG, regardless of the image file name supplied)""<<NEWL>>        )<<NEWL>>        sys.exit(0)<<NEWL>><<NEWL>>    from fontTools.ttLib import TTFont<<NEWL>>    from reportlab.lib import colors<<NEWL>><<NEWL>>    path = sys.argv[1]<<NEWL>>    glyphName = sys.argv[2]<<NEWL>>    if len(sys.argv) > 3:<<NEWL>>        imageFile = sys.argv[3]<<NEWL>>    else:<<NEWL>>        imageFile = ""%s.png"" % glyphName<<NEWL>><<NEWL>>    font = TTFont(path)  # it would work just as well with fontTools.t1Lib.T1Font<<NEWL>>    gs = font.getGlyphSet()<<NEWL>>    pen = ReportLabPen(gs, Path(fillColor=colors.red, strokeWidth=5))<<NEWL>>    g = gs[glyphName]<<NEWL>>    g.draw(pen)<<NEWL>><<NEWL>>    w, h = g.width, 1000<<NEWL>>    from reportlab.graphics import renderPM<<NEWL>>    from reportlab.graphics.shapes import Group, Drawing, scale<<NEWL>><<NEWL>>    # Everything is wrapped in a group to allow transformations.<<NEWL>>    g = Group(pen.path)<<NEWL>>    g.translate(0, 200)<<NEWL>>    g.scale(0.3, 0.3)<<NEWL>><<NEWL>>    d = Drawing(w, h)<<NEWL>>    d.add(g)<<NEWL>><<NEWL>>    renderPM.drawToFile(d, imageFile, fmt=""PNG"")"
276	jackson	4	"# Kills a process by process name<<NEWL>>#<<NEWL>># Uses the Performance Data Helper to locate the PID, then kills it.<<NEWL>># Will only kill the process if there is only one process of that name<<NEWL>># (eg, attempting to kill ""Python.exe"" will only work if there is only<<NEWL>># one Python.exe running.  (Note that the current process does not<<NEWL>># count - ie, if Python.exe is hosting this script, you can still kill<<NEWL>># another Python.exe (as long as there is only one other Python.exe)<<NEWL>><<NEWL>># Really just a demo for the win32pdh(util) module, which allows you<<NEWL>># to get all sorts of information about a running process and many<<NEWL>># other aspects of your system.<<NEWL>><<NEWL>>import win32api, win32pdhutil, win32con, sys<<NEWL>><<NEWL>><<NEWL>>def killProcName(procname):<<NEWL>>    # Change suggested by Dan Knierim, who found that this performed a<<NEWL>>    # ""refresh"", allowing us to kill processes created since this was run<<NEWL>>    # for the first time.<<NEWL>>    try:<<NEWL>>        win32pdhutil.GetPerformanceAttributes(""Process"", ""ID Process"", procname)<<NEWL>>    except:<<NEWL>>        pass<<NEWL>><<NEWL>>    pids = win32pdhutil.FindPerformanceAttributesByName(procname)<<NEWL>><<NEWL>>    # If _my_ pid in there, remove it!<<NEWL>>    try:<<NEWL>>        pids.remove(win32api.GetCurrentProcessId())<<NEWL>>    except ValueError:<<NEWL>>        pass<<NEWL>><<NEWL>>    if len(pids) == 0:<<NEWL>>        result = ""Can't find %s"" % procname<<NEWL>>    elif len(pids) > 1:<<NEWL>>        result = ""Found too many %s's - pids=`%s`"" % (procname, pids)<<NEWL>>    else:<<NEWL>>        handle = win32api.OpenProcess(win32con.PROCESS_TERMINATE, 0, pids[0])<<NEWL>>        win32api.TerminateProcess(handle, 0)<<NEWL>>        win32api.CloseHandle(handle)<<NEWL>>        result = """"<<NEWL>><<NEWL>>    return result<<NEWL>><<NEWL>><<NEWL>>if __name__ == ""__main__"":<<NEWL>>    if len(sys.argv) > 1:<<NEWL>>        for procname in sys.argv[1:]:<<NEWL>>            result = killProcName(procname)<<NEWL>>            if result:<<NEWL>>                print(result)<<NEWL>>                print(""Dumping all processes..."")<<NEWL>>                win32pdhutil.ShowAllProcesses()<<NEWL>>            else:<<NEWL>>                print(""Killed %s"" % procname)<<NEWL>>    else:<<NEWL>>        print(""Usage: killProcName.py procname ..."")"
276	donghui	4	"# Kills a process by process name<<NEWL>>#<<NEWL>># Uses the Performance Data Helper to locate the PID, then kills it.<<NEWL>># Will only kill the process if there is only one process of that name<<NEWL>># (eg, attempting to kill ""Python.exe"" will only work if there is only<<NEWL>># one Python.exe running.  (Note that the current process does not<<NEWL>># count - ie, if Python.exe is hosting this script, you can still kill<<NEWL>># another Python.exe (as long as there is only one other Python.exe)<<NEWL>><<NEWL>># Really just a demo for the win32pdh(util) module, which allows you<<NEWL>># to get all sorts of information about a running process and many<<NEWL>># other aspects of your system.<<NEWL>><<NEWL>>import win32api, win32pdhutil, win32con, sys<<NEWL>><<NEWL>><<NEWL>>def killProcName(procname):<<NEWL>>    # Change suggested by Dan Knierim, who found that this performed a<<NEWL>>    # ""refresh"", allowing us to kill processes created since this was run<<NEWL>>    # for the first time.<<NEWL>>    try:<<NEWL>>        win32pdhutil.GetPerformanceAttributes(""Process"", ""ID Process"", procname)<<NEWL>>    except:<<NEWL>>        pass<<NEWL>><<NEWL>>    pids = win32pdhutil.FindPerformanceAttributesByName(procname)<<NEWL>><<NEWL>>    # If _my_ pid in there, remove it!<<NEWL>>    try:<<NEWL>>        pids.remove(win32api.GetCurrentProcessId())<<NEWL>>    except ValueError:<<NEWL>>        pass<<NEWL>><<NEWL>>    if len(pids) == 0:<<NEWL>>        result = ""Can't find %s"" % procname<<NEWL>>    elif len(pids) > 1:<<NEWL>>        result = ""Found too many %s's - pids=`%s`"" % (procname, pids)<<NEWL>>    else:<<NEWL>>        handle = win32api.OpenProcess(win32con.PROCESS_TERMINATE, 0, pids[0])<<NEWL>>        win32api.TerminateProcess(handle, 0)<<NEWL>>        win32api.CloseHandle(handle)<<NEWL>>        result = """"<<NEWL>><<NEWL>>    return result<<NEWL>><<NEWL>><<NEWL>>if __name__ == ""__main__"":<<NEWL>>    if len(sys.argv) > 1:<<NEWL>>        for procname in sys.argv[1:]:<<NEWL>>            result = killProcName(procname)<<NEWL>>            if result:<<NEWL>>                print(result)<<NEWL>>                print(""Dumping all processes..."")<<NEWL>>                win32pdhutil.ShowAllProcesses()<<NEWL>>            else:<<NEWL>>                print(""Killed %s"" % procname)<<NEWL>>    else:<<NEWL>>        print(""Usage: killProcName.py procname ..."")"
429	jackson	1	"from django.apps import apps<<NEWL>>from django.conf import settings<<NEWL>>from django.contrib.redirects.models import Redirect<<NEWL>>from django.contrib.sites.shortcuts import get_current_site<<NEWL>>from django.core.exceptions import ImproperlyConfigured<<NEWL>>from django.http import HttpResponseGone, HttpResponsePermanentRedirect<<NEWL>>from django.utils.deprecation import MiddlewareMixin<<NEWL>><<NEWL>><<NEWL>>class RedirectFallbackMiddleware(MiddlewareMixin):<<NEWL>>    # Defined as class-level attributes to be subclassing-friendly.<<NEWL>>    response_gone_class = HttpResponseGone<<NEWL>>    response_redirect_class = HttpResponsePermanentRedirect<<NEWL>><<NEWL>>    def __init__(self, get_response):<<NEWL>>        if not apps.is_installed(""django.contrib.sites""):<<NEWL>>            raise ImproperlyConfigured(<<NEWL>>                ""You cannot use RedirectFallbackMiddleware when ""<<NEWL>>                ""django.contrib.sites is not installed.""<<NEWL>>            )<<NEWL>>        super().__init__(get_response)<<NEWL>><<NEWL>>    def process_response(self, request, response):<<NEWL>>        # No need to check for a redirect for non-404 responses.<<NEWL>>        if response.status_code != 404:<<NEWL>>            return response<<NEWL>><<NEWL>>        full_path = request.get_full_path()<<NEWL>>        current_site = get_current_site(request)<<NEWL>><<NEWL>>        r = None<<NEWL>>        try:<<NEWL>>            r = Redirect.objects.get(site=current_site, old_path=full_path)<<NEWL>>        except Redirect.DoesNotExist:<<NEWL>>            pass<<NEWL>>        if r is None and settings.APPEND_SLASH and not request.path.endswith(""/""):<<NEWL>>            try:<<NEWL>>                r = Redirect.objects.get(<<NEWL>>                    site=current_site,<<NEWL>>                    old_path=request.get_full_path(force_append_slash=True),<<NEWL>>                )<<NEWL>>            except Redirect.DoesNotExist:<<NEWL>>                pass<<NEWL>>        if r is not None:<<NEWL>>            if r.new_path == """":<<NEWL>>                return self.response_gone_class()<<NEWL>>            return self.response_redirect_class(r.new_path)<<NEWL>><<NEWL>>        # No redirect was found. Return the response.<<NEWL>>        return response"
429	donghui	1	"from django.apps import apps<<NEWL>>from django.conf import settings<<NEWL>>from django.contrib.redirects.models import Redirect<<NEWL>>from django.contrib.sites.shortcuts import get_current_site<<NEWL>>from django.core.exceptions import ImproperlyConfigured<<NEWL>>from django.http import HttpResponseGone, HttpResponsePermanentRedirect<<NEWL>>from django.utils.deprecation import MiddlewareMixin<<NEWL>><<NEWL>><<NEWL>>class RedirectFallbackMiddleware(MiddlewareMixin):<<NEWL>>    # Defined as class-level attributes to be subclassing-friendly.<<NEWL>>    response_gone_class = HttpResponseGone<<NEWL>>    response_redirect_class = HttpResponsePermanentRedirect<<NEWL>><<NEWL>>    def __init__(self, get_response):<<NEWL>>        if not apps.is_installed(""django.contrib.sites""):<<NEWL>>            raise ImproperlyConfigured(<<NEWL>>                ""You cannot use RedirectFallbackMiddleware when ""<<NEWL>>                ""django.contrib.sites is not installed.""<<NEWL>>            )<<NEWL>>        super().__init__(get_response)<<NEWL>><<NEWL>>    def process_response(self, request, response):<<NEWL>>        # No need to check for a redirect for non-404 responses.<<NEWL>>        if response.status_code != 404:<<NEWL>>            return response<<NEWL>><<NEWL>>        full_path = request.get_full_path()<<NEWL>>        current_site = get_current_site(request)<<NEWL>><<NEWL>>        r = None<<NEWL>>        try:<<NEWL>>            r = Redirect.objects.get(site=current_site, old_path=full_path)<<NEWL>>        except Redirect.DoesNotExist:<<NEWL>>            pass<<NEWL>>        if r is None and settings.APPEND_SLASH and not request.path.endswith(""/""):<<NEWL>>            try:<<NEWL>>                r = Redirect.objects.get(<<NEWL>>                    site=current_site,<<NEWL>>                    old_path=request.get_full_path(force_append_slash=True),<<NEWL>>                )<<NEWL>>            except Redirect.DoesNotExist:<<NEWL>>                pass<<NEWL>>        if r is not None:<<NEWL>>            if r.new_path == """":<<NEWL>>                return self.response_gone_class()<<NEWL>>            return self.response_redirect_class(r.new_path)<<NEWL>><<NEWL>>        # No redirect was found. Return the response.<<NEWL>>        return response"
478	jackson	2	"import numpy as np<<NEWL>>import pytest<<NEWL>><<NEWL>>import pandas as pd<<NEWL>>import pandas._testing as tm<<NEWL>>from pandas.arrays import BooleanArray<<NEWL>>from pandas.tests.arrays.masked_shared import ComparisonOps<<NEWL>><<NEWL>><<NEWL>>@pytest.fixture<<NEWL>>def data():<<NEWL>>    """"""Fixture returning boolean array with valid and missing data""""""<<NEWL>>    return pd.array(<<NEWL>>        [True, False] * 4 + [np.nan] + [True, False] * 44 + [np.nan] + [True, False],<<NEWL>>        dtype=""boolean"",<<NEWL>>    )<<NEWL>><<NEWL>><<NEWL>>@pytest.fixture<<NEWL>>def dtype():<<NEWL>>    """"""Fixture returning BooleanDtype""""""<<NEWL>>    return pd.BooleanDtype()<<NEWL>><<NEWL>><<NEWL>>class TestComparisonOps(ComparisonOps):<<NEWL>>    def test_compare_scalar(self, data, comparison_op):<<NEWL>>        self._compare_other(data, comparison_op, True)<<NEWL>><<NEWL>>    def test_compare_array(self, data, comparison_op):<<NEWL>>        other = pd.array([True] * len(data), dtype=""boolean"")<<NEWL>>        self._compare_other(data, comparison_op, other)<<NEWL>>        other = np.array([True] * len(data))<<NEWL>>        self._compare_other(data, comparison_op, other)<<NEWL>>        other = pd.Series([True] * len(data))<<NEWL>>        self._compare_other(data, comparison_op, other)<<NEWL>><<NEWL>>    @pytest.mark.parametrize(""other"", [True, False, pd.NA])<<NEWL>>    def test_scalar(self, other, comparison_op, dtype):<<NEWL>>        ComparisonOps.test_scalar(self, other, comparison_op, dtype)<<NEWL>><<NEWL>>    def test_array(self, comparison_op):<<NEWL>>        op = comparison_op<<NEWL>>        a = pd.array([True] * 3 + [False] * 3 + [None] * 3, dtype=""boolean"")<<NEWL>>        b = pd.array([True, False, None] * 3, dtype=""boolean"")<<NEWL>><<NEWL>>        result = op(a, b)<<NEWL>><<NEWL>>        values = op(a._data, b._data)<<NEWL>>        mask = a._mask | b._mask<<NEWL>>        expected = BooleanArray(values, mask)<<NEWL>>        tm.assert_extension_array_equal(result, expected)<<NEWL>><<NEWL>>        # ensure we haven't mutated anything inplace<<NEWL>>        result[0] = None<<NEWL>>        tm.assert_extension_array_equal(<<NEWL>>            a, pd.array([True] * 3 + [False] * 3 + [None] * 3, dtype=""boolean"")<<NEWL>>        )<<NEWL>>        tm.assert_extension_array_equal(<<NEWL>>            b, pd.array([True, False, None] * 3, dtype=""boolean"")<<NEWL>>        )"
478	donghui	2	"import numpy as np<<NEWL>>import pytest<<NEWL>><<NEWL>>import pandas as pd<<NEWL>>import pandas._testing as tm<<NEWL>>from pandas.arrays import BooleanArray<<NEWL>>from pandas.tests.arrays.masked_shared import ComparisonOps<<NEWL>><<NEWL>><<NEWL>>@pytest.fixture<<NEWL>>def data():<<NEWL>>    """"""Fixture returning boolean array with valid and missing data""""""<<NEWL>>    return pd.array(<<NEWL>>        [True, False] * 4 + [np.nan] + [True, False] * 44 + [np.nan] + [True, False],<<NEWL>>        dtype=""boolean"",<<NEWL>>    )<<NEWL>><<NEWL>><<NEWL>>@pytest.fixture<<NEWL>>def dtype():<<NEWL>>    """"""Fixture returning BooleanDtype""""""<<NEWL>>    return pd.BooleanDtype()<<NEWL>><<NEWL>><<NEWL>>class TestComparisonOps(ComparisonOps):<<NEWL>>    def test_compare_scalar(self, data, comparison_op):<<NEWL>>        self._compare_other(data, comparison_op, True)<<NEWL>><<NEWL>>    def test_compare_array(self, data, comparison_op):<<NEWL>>        other = pd.array([True] * len(data), dtype=""boolean"")<<NEWL>>        self._compare_other(data, comparison_op, other)<<NEWL>>        other = np.array([True] * len(data))<<NEWL>>        self._compare_other(data, comparison_op, other)<<NEWL>>        other = pd.Series([True] * len(data))<<NEWL>>        self._compare_other(data, comparison_op, other)<<NEWL>><<NEWL>>    @pytest.mark.parametrize(""other"", [True, False, pd.NA])<<NEWL>>    def test_scalar(self, other, comparison_op, dtype):<<NEWL>>        ComparisonOps.test_scalar(self, other, comparison_op, dtype)<<NEWL>><<NEWL>>    def test_array(self, comparison_op):<<NEWL>>        op = comparison_op<<NEWL>>        a = pd.array([True] * 3 + [False] * 3 + [None] * 3, dtype=""boolean"")<<NEWL>>        b = pd.array([True, False, None] * 3, dtype=""boolean"")<<NEWL>><<NEWL>>        result = op(a, b)<<NEWL>><<NEWL>>        values = op(a._data, b._data)<<NEWL>>        mask = a._mask | b._mask<<NEWL>>        expected = BooleanArray(values, mask)<<NEWL>>        tm.assert_extension_array_equal(result, expected)<<NEWL>><<NEWL>>        # ensure we haven't mutated anything inplace<<NEWL>>        result[0] = None<<NEWL>>        tm.assert_extension_array_equal(<<NEWL>>            a, pd.array([True] * 3 + [False] * 3 + [None] * 3, dtype=""boolean"")<<NEWL>>        )<<NEWL>>        tm.assert_extension_array_equal(<<NEWL>>            b, pd.array([True, False, None] * 3, dtype=""boolean"")<<NEWL>>        )"
468	jackson	2	"import _plotly_utils.basevalidators<<NEWL>><<NEWL>><<NEWL>>class HoverlabelValidator(_plotly_utils.basevalidators.CompoundValidator):<<NEWL>>    def __init__(self, plotly_name=""hoverlabel"", parent_name=""scatter3d"", **kwargs):<<NEWL>>        super(HoverlabelValidator, self).__init__(<<NEWL>>            plotly_name=plotly_name,<<NEWL>>            parent_name=parent_name,<<NEWL>>            data_class_str=kwargs.pop(""data_class_str"", ""Hoverlabel""),<<NEWL>>            data_docs=kwargs.pop(<<NEWL>>                ""data_docs"",<<NEWL>>                """"""<<NEWL>>            align<<NEWL>>                Sets the horizontal alignment of the text<<NEWL>>                content within hover label box. Has an effect<<NEWL>>                only if the hover label text spans more two or<<NEWL>>                more lines<<NEWL>>            alignsrc<<NEWL>>                Sets the source reference on Chart Studio Cloud<<NEWL>>                for `align`.<<NEWL>>            bgcolor<<NEWL>>                Sets the background color of the hover labels<<NEWL>>                for this trace<<NEWL>>            bgcolorsrc<<NEWL>>                Sets the source reference on Chart Studio Cloud<<NEWL>>                for `bgcolor`.<<NEWL>>            bordercolor<<NEWL>>                Sets the border color of the hover labels for<<NEWL>>                this trace.<<NEWL>>            bordercolorsrc<<NEWL>>                Sets the source reference on Chart Studio Cloud<<NEWL>>                for `bordercolor`.<<NEWL>>            font<<NEWL>>                Sets the font used in hover labels.<<NEWL>>            namelength<<NEWL>>                Sets the default length (in number of<<NEWL>>                characters) of the trace name in the hover<<NEWL>>                labels for all traces. -1 shows the whole name<<NEWL>>                regardless of length. 0-3 shows the first 0-3<<NEWL>>                characters, and an integer >3 will show the<<NEWL>>                whole name if it is less than that many<<NEWL>>                characters, but if it is longer, will truncate<<NEWL>>                to `namelength - 3` characters and add an<<NEWL>>                ellipsis.<<NEWL>>            namelengthsrc<<NEWL>>                Sets the source reference on Chart Studio Cloud<<NEWL>>                for `namelength`.<<NEWL>>"""""",<<NEWL>>            ),<<NEWL>>            **kwargs,<<NEWL>>        )"
468	donghui	1	"import _plotly_utils.basevalidators<<NEWL>><<NEWL>><<NEWL>>class HoverlabelValidator(_plotly_utils.basevalidators.CompoundValidator):<<NEWL>>    def __init__(self, plotly_name=""hoverlabel"", parent_name=""scatter3d"", **kwargs):<<NEWL>>        super(HoverlabelValidator, self).__init__(<<NEWL>>            plotly_name=plotly_name,<<NEWL>>            parent_name=parent_name,<<NEWL>>            data_class_str=kwargs.pop(""data_class_str"", ""Hoverlabel""),<<NEWL>>            data_docs=kwargs.pop(<<NEWL>>                ""data_docs"",<<NEWL>>                """"""<<NEWL>>            align<<NEWL>>                Sets the horizontal alignment of the text<<NEWL>>                content within hover label box. Has an effect<<NEWL>>                only if the hover label text spans more two or<<NEWL>>                more lines<<NEWL>>            alignsrc<<NEWL>>                Sets the source reference on Chart Studio Cloud<<NEWL>>                for `align`.<<NEWL>>            bgcolor<<NEWL>>                Sets the background color of the hover labels<<NEWL>>                for this trace<<NEWL>>            bgcolorsrc<<NEWL>>                Sets the source reference on Chart Studio Cloud<<NEWL>>                for `bgcolor`.<<NEWL>>            bordercolor<<NEWL>>                Sets the border color of the hover labels for<<NEWL>>                this trace.<<NEWL>>            bordercolorsrc<<NEWL>>                Sets the source reference on Chart Studio Cloud<<NEWL>>                for `bordercolor`.<<NEWL>>            font<<NEWL>>                Sets the font used in hover labels.<<NEWL>>            namelength<<NEWL>>                Sets the default length (in number of<<NEWL>>                characters) of the trace name in the hover<<NEWL>>                labels for all traces. -1 shows the whole name<<NEWL>>                regardless of length. 0-3 shows the first 0-3<<NEWL>>                characters, and an integer >3 will show the<<NEWL>>                whole name if it is less than that many<<NEWL>>                characters, but if it is longer, will truncate<<NEWL>>                to `namelength - 3` characters and add an<<NEWL>>                ellipsis.<<NEWL>>            namelengthsrc<<NEWL>>                Sets the source reference on Chart Studio Cloud<<NEWL>>                for `namelength`.<<NEWL>>"""""",<<NEWL>>            ),<<NEWL>>            **kwargs,<<NEWL>>        )"
439	jackson	3	from __future__ import unicode_literals<<NEWL>><<NEWL>># For backwards-compatibility. keep this file.<<NEWL>># (Many people are going to have key bindings that rely on this file.)<<NEWL>>from .app import *<<NEWL>><<NEWL>>__all__ = [<<NEWL>>    # Old names.<<NEWL>>    'HasArg',<<NEWL>>    'HasCompletions',<<NEWL>>    'HasFocus',<<NEWL>>    'HasSelection',<<NEWL>>    'HasValidationError',<<NEWL>>    'IsDone',<<NEWL>>    'IsReadOnly',<<NEWL>>    'IsMultiline',<<NEWL>>    'RendererHeightIsKnown',<<NEWL>>    'InEditingMode',<<NEWL>>    'InPasteMode',<<NEWL>><<NEWL>>    'ViMode',<<NEWL>>    'ViNavigationMode',<<NEWL>>    'ViInsertMode',<<NEWL>>    'ViInsertMultipleMode',<<NEWL>>    'ViReplaceMode',<<NEWL>>    'ViSelectionMode',<<NEWL>>    'ViWaitingForTextObjectMode',<<NEWL>>    'ViDigraphMode',<<NEWL>><<NEWL>>    'EmacsMode',<<NEWL>>    'EmacsInsertMode',<<NEWL>>    'EmacsSelectionMode',<<NEWL>><<NEWL>>    'IsSearching',<<NEWL>>    'HasSearch',<<NEWL>>    'ControlIsSearchable',<<NEWL>>]<<NEWL>><<NEWL>># Keep the original classnames for backwards compatibility.<<NEWL>>HasValidationError = lambda: has_validation_error<<NEWL>>HasArg = lambda: has_arg<<NEWL>>IsDone = lambda: is_done<<NEWL>>RendererHeightIsKnown = lambda: renderer_height_is_known<<NEWL>>ViNavigationMode = lambda: vi_navigation_mode<<NEWL>>InPasteMode = lambda: in_paste_mode<<NEWL>>EmacsMode = lambda: emacs_mode<<NEWL>>EmacsInsertMode = lambda: emacs_insert_mode<<NEWL>>ViMode = lambda: vi_mode<<NEWL>>IsSearching = lambda: is_searching<<NEWL>>HasSearch = lambda: is_searching<<NEWL>>ControlIsSearchable = lambda: control_is_searchable<<NEWL>>EmacsSelectionMode = lambda: emacs_selection_mode<<NEWL>>ViDigraphMode = lambda: vi_digraph_mode<<NEWL>>ViWaitingForTextObjectMode = lambda: vi_waiting_for_text_object_mode<<NEWL>>ViSelectionMode = lambda: vi_selection_mode<<NEWL>>ViReplaceMode = lambda: vi_replace_mode<<NEWL>>ViInsertMultipleMode = lambda: vi_insert_multiple_mode<<NEWL>>ViInsertMode = lambda: vi_insert_mode<<NEWL>>HasSelection = lambda: has_selection<<NEWL>>HasCompletions = lambda: has_completions<<NEWL>>IsReadOnly = lambda: is_read_only<<NEWL>>IsMultiline = lambda: is_multiline<<NEWL>><<NEWL>>HasFocus = has_focus  # No lambda here! (Has_focus is callable that returns a callable.)<<NEWL>>InEditingMode = in_editing_mode
439	donghui	1	from __future__ import unicode_literals<<NEWL>><<NEWL>># For backwards-compatibility. keep this file.<<NEWL>># (Many people are going to have key bindings that rely on this file.)<<NEWL>>from .app import *<<NEWL>><<NEWL>>__all__ = [<<NEWL>>    # Old names.<<NEWL>>    'HasArg',<<NEWL>>    'HasCompletions',<<NEWL>>    'HasFocus',<<NEWL>>    'HasSelection',<<NEWL>>    'HasValidationError',<<NEWL>>    'IsDone',<<NEWL>>    'IsReadOnly',<<NEWL>>    'IsMultiline',<<NEWL>>    'RendererHeightIsKnown',<<NEWL>>    'InEditingMode',<<NEWL>>    'InPasteMode',<<NEWL>><<NEWL>>    'ViMode',<<NEWL>>    'ViNavigationMode',<<NEWL>>    'ViInsertMode',<<NEWL>>    'ViInsertMultipleMode',<<NEWL>>    'ViReplaceMode',<<NEWL>>    'ViSelectionMode',<<NEWL>>    'ViWaitingForTextObjectMode',<<NEWL>>    'ViDigraphMode',<<NEWL>><<NEWL>>    'EmacsMode',<<NEWL>>    'EmacsInsertMode',<<NEWL>>    'EmacsSelectionMode',<<NEWL>><<NEWL>>    'IsSearching',<<NEWL>>    'HasSearch',<<NEWL>>    'ControlIsSearchable',<<NEWL>>]<<NEWL>><<NEWL>># Keep the original classnames for backwards compatibility.<<NEWL>>HasValidationError = lambda: has_validation_error<<NEWL>>HasArg = lambda: has_arg<<NEWL>>IsDone = lambda: is_done<<NEWL>>RendererHeightIsKnown = lambda: renderer_height_is_known<<NEWL>>ViNavigationMode = lambda: vi_navigation_mode<<NEWL>>InPasteMode = lambda: in_paste_mode<<NEWL>>EmacsMode = lambda: emacs_mode<<NEWL>>EmacsInsertMode = lambda: emacs_insert_mode<<NEWL>>ViMode = lambda: vi_mode<<NEWL>>IsSearching = lambda: is_searching<<NEWL>>HasSearch = lambda: is_searching<<NEWL>>ControlIsSearchable = lambda: control_is_searchable<<NEWL>>EmacsSelectionMode = lambda: emacs_selection_mode<<NEWL>>ViDigraphMode = lambda: vi_digraph_mode<<NEWL>>ViWaitingForTextObjectMode = lambda: vi_waiting_for_text_object_mode<<NEWL>>ViSelectionMode = lambda: vi_selection_mode<<NEWL>>ViReplaceMode = lambda: vi_replace_mode<<NEWL>>ViInsertMultipleMode = lambda: vi_insert_multiple_mode<<NEWL>>ViInsertMode = lambda: vi_insert_mode<<NEWL>>HasSelection = lambda: has_selection<<NEWL>>HasCompletions = lambda: has_completions<<NEWL>>IsReadOnly = lambda: is_read_only<<NEWL>>IsMultiline = lambda: is_multiline<<NEWL>><<NEWL>>HasFocus = has_focus  # No lambda here! (Has_focus is callable that returns a callable.)<<NEWL>>InEditingMode = in_editing_mode
377	jackson	1	"######################## BEGIN LICENSE BLOCK ########################<<NEWL>># The Original Code is mozilla.org code.<<NEWL>>#<<NEWL>># The Initial Developer of the Original Code is<<NEWL>># Netscape Communications Corporation.<<NEWL>># Portions created by the Initial Developer are Copyright (C) 1998<<NEWL>># the Initial Developer. All Rights Reserved.<<NEWL>>#<<NEWL>># Contributor(s):<<NEWL>>#   Mark Pilgrim - port to Python<<NEWL>>#<<NEWL>># This library is free software; you can redistribute it and/or<<NEWL>># modify it under the terms of the GNU Lesser General Public<<NEWL>># License as published by the Free Software Foundation; either<<NEWL>># version 2.1 of the License, or (at your option) any later version.<<NEWL>>#<<NEWL>># This library is distributed in the hope that it will be useful,<<NEWL>># but WITHOUT ANY WARRANTY; without even the implied warranty of<<NEWL>># MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU<<NEWL>># Lesser General Public License for more details.<<NEWL>>#<<NEWL>># You should have received a copy of the GNU Lesser General Public<<NEWL>># License along with this library; if not, write to the Free Software<<NEWL>># Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA<<NEWL>># 02110-1301  USA<<NEWL>>######################### END LICENSE BLOCK #########################<<NEWL>><<NEWL>>from .chardistribution import EUCKRDistributionAnalysis<<NEWL>>from .codingstatemachine import CodingStateMachine<<NEWL>>from .mbcharsetprober import MultiByteCharSetProber<<NEWL>>from .mbcssm import CP949_SM_MODEL<<NEWL>><<NEWL>><<NEWL>>class CP949Prober(MultiByteCharSetProber):<<NEWL>>    def __init__(self):<<NEWL>>        super().__init__()<<NEWL>>        self.coding_sm = CodingStateMachine(CP949_SM_MODEL)<<NEWL>>        # NOTE: CP949 is a superset of EUC-KR, so the distribution should be<<NEWL>>        #       not different.<<NEWL>>        self.distribution_analyzer = EUCKRDistributionAnalysis()<<NEWL>>        self.reset()<<NEWL>><<NEWL>>    @property<<NEWL>>    def charset_name(self):<<NEWL>>        return ""CP949""<<NEWL>><<NEWL>>    @property<<NEWL>>    def language(self):<<NEWL>>        return ""Korean"""
377	donghui	1	"######################## BEGIN LICENSE BLOCK ########################<<NEWL>># The Original Code is mozilla.org code.<<NEWL>>#<<NEWL>># The Initial Developer of the Original Code is<<NEWL>># Netscape Communications Corporation.<<NEWL>># Portions created by the Initial Developer are Copyright (C) 1998<<NEWL>># the Initial Developer. All Rights Reserved.<<NEWL>>#<<NEWL>># Contributor(s):<<NEWL>>#   Mark Pilgrim - port to Python<<NEWL>>#<<NEWL>># This library is free software; you can redistribute it and/or<<NEWL>># modify it under the terms of the GNU Lesser General Public<<NEWL>># License as published by the Free Software Foundation; either<<NEWL>># version 2.1 of the License, or (at your option) any later version.<<NEWL>>#<<NEWL>># This library is distributed in the hope that it will be useful,<<NEWL>># but WITHOUT ANY WARRANTY; without even the implied warranty of<<NEWL>># MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU<<NEWL>># Lesser General Public License for more details.<<NEWL>>#<<NEWL>># You should have received a copy of the GNU Lesser General Public<<NEWL>># License along with this library; if not, write to the Free Software<<NEWL>># Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA<<NEWL>># 02110-1301  USA<<NEWL>>######################### END LICENSE BLOCK #########################<<NEWL>><<NEWL>>from .chardistribution import EUCKRDistributionAnalysis<<NEWL>>from .codingstatemachine import CodingStateMachine<<NEWL>>from .mbcharsetprober import MultiByteCharSetProber<<NEWL>>from .mbcssm import CP949_SM_MODEL<<NEWL>><<NEWL>><<NEWL>>class CP949Prober(MultiByteCharSetProber):<<NEWL>>    def __init__(self):<<NEWL>>        super().__init__()<<NEWL>>        self.coding_sm = CodingStateMachine(CP949_SM_MODEL)<<NEWL>>        # NOTE: CP949 is a superset of EUC-KR, so the distribution should be<<NEWL>>        #       not different.<<NEWL>>        self.distribution_analyzer = EUCKRDistributionAnalysis()<<NEWL>>        self.reset()<<NEWL>><<NEWL>>    @property<<NEWL>>    def charset_name(self):<<NEWL>>        return ""CP949""<<NEWL>><<NEWL>>    @property<<NEWL>>    def language(self):<<NEWL>>        return ""Korean"""
266	jackson	1	"# Copyright (c) Meta Platforms, Inc. and affiliates.<<NEWL>>#<<NEWL>># This source code is licensed under the MIT license found in the<<NEWL>># LICENSE file in the root directory of this source tree.<<NEWL>><<NEWL>>from viktor._vendor.libcst._parser.grammar import _should_include<<NEWL>>from viktor._vendor.libcst._parser.parso.utils import PythonVersionInfo<<NEWL>>from viktor._vendor.libcst.testing.utils import data_provider, UnitTest<<NEWL>><<NEWL>><<NEWL>>class VersionCompareTest(UnitTest):<<NEWL>>    @data_provider(<<NEWL>>        (<<NEWL>>            # Simple equality<<NEWL>>            (""==3.6"", PythonVersionInfo(3, 6), True),<<NEWL>>            (""!=3.6"", PythonVersionInfo(3, 6), False),<<NEWL>>            # Equal or GT/LT<<NEWL>>            ("">=3.6"", PythonVersionInfo(3, 5), False),<<NEWL>>            ("">=3.6"", PythonVersionInfo(3, 6), True),<<NEWL>>            ("">=3.6"", PythonVersionInfo(3, 7), True),<<NEWL>>            (""<=3.6"", PythonVersionInfo(3, 5), True),<<NEWL>>            (""<=3.6"", PythonVersionInfo(3, 6), True),<<NEWL>>            (""<=3.6"", PythonVersionInfo(3, 7), False),<<NEWL>>            # GT/LT<<NEWL>>            ("">3.6"", PythonVersionInfo(3, 5), False),<<NEWL>>            ("">3.6"", PythonVersionInfo(3, 6), False),<<NEWL>>            ("">3.6"", PythonVersionInfo(3, 7), True),<<NEWL>>            (""<3.6"", PythonVersionInfo(3, 5), True),<<NEWL>>            (""<3.6"", PythonVersionInfo(3, 6), False),<<NEWL>>            (""<3.6"", PythonVersionInfo(3, 7), False),<<NEWL>>            # Multiple checks<<NEWL>>            ("">3.6,<3.8"", PythonVersionInfo(3, 6), False),<<NEWL>>            ("">3.6,<3.8"", PythonVersionInfo(3, 7), True),<<NEWL>>            ("">3.6,<3.8"", PythonVersionInfo(3, 8), False),<<NEWL>>        )<<NEWL>>    )<<NEWL>>    def test_tokenize(<<NEWL>>        self,<<NEWL>>        requested_version: str,<<NEWL>>        actual_version: PythonVersionInfo,<<NEWL>>        expected_result: bool,<<NEWL>>    ) -> None:<<NEWL>>        self.assertEqual(<<NEWL>>            _should_include(requested_version, actual_version), expected_result<<NEWL>>        )"
266	donghui	1	"# Copyright (c) Meta Platforms, Inc. and affiliates.<<NEWL>>#<<NEWL>># This source code is licensed under the MIT license found in the<<NEWL>># LICENSE file in the root directory of this source tree.<<NEWL>><<NEWL>>from viktor._vendor.libcst._parser.grammar import _should_include<<NEWL>>from viktor._vendor.libcst._parser.parso.utils import PythonVersionInfo<<NEWL>>from viktor._vendor.libcst.testing.utils import data_provider, UnitTest<<NEWL>><<NEWL>><<NEWL>>class VersionCompareTest(UnitTest):<<NEWL>>    @data_provider(<<NEWL>>        (<<NEWL>>            # Simple equality<<NEWL>>            (""==3.6"", PythonVersionInfo(3, 6), True),<<NEWL>>            (""!=3.6"", PythonVersionInfo(3, 6), False),<<NEWL>>            # Equal or GT/LT<<NEWL>>            ("">=3.6"", PythonVersionInfo(3, 5), False),<<NEWL>>            ("">=3.6"", PythonVersionInfo(3, 6), True),<<NEWL>>            ("">=3.6"", PythonVersionInfo(3, 7), True),<<NEWL>>            (""<=3.6"", PythonVersionInfo(3, 5), True),<<NEWL>>            (""<=3.6"", PythonVersionInfo(3, 6), True),<<NEWL>>            (""<=3.6"", PythonVersionInfo(3, 7), False),<<NEWL>>            # GT/LT<<NEWL>>            ("">3.6"", PythonVersionInfo(3, 5), False),<<NEWL>>            ("">3.6"", PythonVersionInfo(3, 6), False),<<NEWL>>            ("">3.6"", PythonVersionInfo(3, 7), True),<<NEWL>>            (""<3.6"", PythonVersionInfo(3, 5), True),<<NEWL>>            (""<3.6"", PythonVersionInfo(3, 6), False),<<NEWL>>            (""<3.6"", PythonVersionInfo(3, 7), False),<<NEWL>>            # Multiple checks<<NEWL>>            ("">3.6,<3.8"", PythonVersionInfo(3, 6), False),<<NEWL>>            ("">3.6,<3.8"", PythonVersionInfo(3, 7), True),<<NEWL>>            ("">3.6,<3.8"", PythonVersionInfo(3, 8), False),<<NEWL>>        )<<NEWL>>    )<<NEWL>>    def test_tokenize(<<NEWL>>        self,<<NEWL>>        requested_version: str,<<NEWL>>        actual_version: PythonVersionInfo,<<NEWL>>        expected_result: bool,<<NEWL>>    ) -> None:<<NEWL>>        self.assertEqual(<<NEWL>>            _should_include(requested_version, actual_version), expected_result<<NEWL>>        )"
326	jackson	2	"""""""<<NEWL>>    pygments.lexers.pointless<<NEWL>>    ~~~~~~~~~~~~~~~~~~~~~~~~~<<NEWL>><<NEWL>>    Lexers for Pointless.<<NEWL>><<NEWL>>    :copyright: Copyright 2006-2022 by the Pygments team, see AUTHORS.<<NEWL>>    :license: BSD, see LICENSE for details.<<NEWL>>""""""<<NEWL>><<NEWL>>from pygments.lexer import RegexLexer, words<<NEWL>>from pygments.token import Comment, Error, Keyword, Name, Number, Operator, \<<NEWL>>    Punctuation, String, Text<<NEWL>><<NEWL>>__all__ = ['PointlessLexer']<<NEWL>><<NEWL>><<NEWL>>class PointlessLexer(RegexLexer):<<NEWL>>    """"""<<NEWL>>    For Pointless source code.<<NEWL>><<NEWL>>    .. versionadded:: 2.7<<NEWL>>    """"""<<NEWL>><<NEWL>>    name = 'Pointless'<<NEWL>>    url = 'https://ptls.dev'<<NEWL>>    aliases = ['pointless']<<NEWL>>    filenames = ['*.ptls']<<NEWL>><<NEWL>>    ops = words([<<NEWL>>        ""+"", ""-"", ""*"", ""/"", ""**"", ""%"", ""+="", ""-="", ""*="",<<NEWL>>        ""/="", ""**="", ""%="", ""|>"", ""="", ""=="", ""!="", ""<"", "">"",<<NEWL>>        ""<="", "">="", ""=>"", ""$"", ""++"",<<NEWL>>    ])<<NEWL>><<NEWL>>    keywords = words([<<NEWL>>        ""if"", ""then"", ""else"", ""where"", ""with"", ""cond"",<<NEWL>>        ""case"", ""and"", ""or"", ""not"", ""in"", ""as"", ""for"",<<NEWL>>        ""requires"", ""throw"", ""try"", ""catch"", ""when"",<<NEWL>>        ""yield"", ""upval"",<<NEWL>>    ], suffix=r'\b')<<NEWL>><<NEWL>>    tokens = {<<NEWL>>        'root': [<<NEWL>>            (r'[ \n\r]+', Text),<<NEWL>>            (r'--.*$', Comment.Single),<<NEWL>>            (r'""""""', String, 'multiString'),<<NEWL>>            (r'""', String, 'string'),<<NEWL>>            (r'[\[\](){}:;,.]', Punctuation),<<NEWL>>            (ops, Operator),<<NEWL>>            (keywords, Keyword),<<NEWL>>            (r'\d+|\d*\.\d+', Number),<<NEWL>>            (r'(true|false)\b', Name.Builtin),<<NEWL>>            (r'[A-Z][a-zA-Z0-9]*\b', String.Symbol),<<NEWL>>            (r'output\b', Name.Variable.Magic),<<NEWL>>            (r'(export|import)\b', Keyword.Namespace),<<NEWL>>            (r'[a-z][a-zA-Z0-9]*\b', Name.Variable)<<NEWL>>        ],<<NEWL>>        'multiString': [<<NEWL>>            (r'\\.', String.Escape),<<NEWL>>            (r'""""""', String, '#pop'),<<NEWL>>            (r'""', String),<<NEWL>>            (r'[^\\""]+', String),<<NEWL>>        ],<<NEWL>>        'string': [<<NEWL>>            (r'\\.', String.Escape),<<NEWL>>            (r'""', String, '#pop'),<<NEWL>>            (r'\n', Error),<<NEWL>>            (r'[^\\""]+', String),<<NEWL>>        ],<<NEWL>>    }"
326	donghui	1	"""""""<<NEWL>>    pygments.lexers.pointless<<NEWL>>    ~~~~~~~~~~~~~~~~~~~~~~~~~<<NEWL>><<NEWL>>    Lexers for Pointless.<<NEWL>><<NEWL>>    :copyright: Copyright 2006-2022 by the Pygments team, see AUTHORS.<<NEWL>>    :license: BSD, see LICENSE for details.<<NEWL>>""""""<<NEWL>><<NEWL>>from pygments.lexer import RegexLexer, words<<NEWL>>from pygments.token import Comment, Error, Keyword, Name, Number, Operator, \<<NEWL>>    Punctuation, String, Text<<NEWL>><<NEWL>>__all__ = ['PointlessLexer']<<NEWL>><<NEWL>><<NEWL>>class PointlessLexer(RegexLexer):<<NEWL>>    """"""<<NEWL>>    For Pointless source code.<<NEWL>><<NEWL>>    .. versionadded:: 2.7<<NEWL>>    """"""<<NEWL>><<NEWL>>    name = 'Pointless'<<NEWL>>    url = 'https://ptls.dev'<<NEWL>>    aliases = ['pointless']<<NEWL>>    filenames = ['*.ptls']<<NEWL>><<NEWL>>    ops = words([<<NEWL>>        ""+"", ""-"", ""*"", ""/"", ""**"", ""%"", ""+="", ""-="", ""*="",<<NEWL>>        ""/="", ""**="", ""%="", ""|>"", ""="", ""=="", ""!="", ""<"", "">"",<<NEWL>>        ""<="", "">="", ""=>"", ""$"", ""++"",<<NEWL>>    ])<<NEWL>><<NEWL>>    keywords = words([<<NEWL>>        ""if"", ""then"", ""else"", ""where"", ""with"", ""cond"",<<NEWL>>        ""case"", ""and"", ""or"", ""not"", ""in"", ""as"", ""for"",<<NEWL>>        ""requires"", ""throw"", ""try"", ""catch"", ""when"",<<NEWL>>        ""yield"", ""upval"",<<NEWL>>    ], suffix=r'\b')<<NEWL>><<NEWL>>    tokens = {<<NEWL>>        'root': [<<NEWL>>            (r'[ \n\r]+', Text),<<NEWL>>            (r'--.*$', Comment.Single),<<NEWL>>            (r'""""""', String, 'multiString'),<<NEWL>>            (r'""', String, 'string'),<<NEWL>>            (r'[\[\](){}:;,.]', Punctuation),<<NEWL>>            (ops, Operator),<<NEWL>>            (keywords, Keyword),<<NEWL>>            (r'\d+|\d*\.\d+', Number),<<NEWL>>            (r'(true|false)\b', Name.Builtin),<<NEWL>>            (r'[A-Z][a-zA-Z0-9]*\b', String.Symbol),<<NEWL>>            (r'output\b', Name.Variable.Magic),<<NEWL>>            (r'(export|import)\b', Keyword.Namespace),<<NEWL>>            (r'[a-z][a-zA-Z0-9]*\b', Name.Variable)<<NEWL>>        ],<<NEWL>>        'multiString': [<<NEWL>>            (r'\\.', String.Escape),<<NEWL>>            (r'""""""', String, '#pop'),<<NEWL>>            (r'""', String),<<NEWL>>            (r'[^\\""]+', String),<<NEWL>>        ],<<NEWL>>        'string': [<<NEWL>>            (r'\\.', String.Escape),<<NEWL>>            (r'""', String, '#pop'),<<NEWL>>            (r'\n', Error),<<NEWL>>            (r'[^\\""]+', String),<<NEWL>>        ],<<NEWL>>    }"
295	jackson	3	"from typing import Any, Dict, Optional, Union<<NEWL>>from warnings import warn<<NEWL>><<NEWL>>from .api import from_bytes<<NEWL>>from .constant import CHARDET_CORRESPONDENCE<<NEWL>><<NEWL>><<NEWL>>def detect(<<NEWL>>    byte_str: bytes, should_rename_legacy: bool = False, **kwargs: Any<<NEWL>>) -> Dict[str, Optional[Union[str, float]]]:<<NEWL>>    """"""<<NEWL>>    chardet legacy method<<NEWL>>    Detect the encoding of the given byte string. It should be mostly backward-compatible.<<NEWL>>    Encoding name will match Chardet own writing whenever possible. (Not on encoding name unsupported by it)<<NEWL>>    This function is deprecated and should be used to migrate your project easily, consult the documentation for<<NEWL>>    further information. Not planned for removal.<<NEWL>><<NEWL>>    :param byte_str:     The byte sequence to examine.<<NEWL>>    :param should_rename_legacy:  Should we rename legacy encodings<<NEWL>>                                  to their more modern equivalents?<<NEWL>>    """"""<<NEWL>>    if len(kwargs):<<NEWL>>        warn(<<NEWL>>            f""charset-normalizer disregard arguments '{','.join(list(kwargs.keys()))}' in legacy function detect()""<<NEWL>>        )<<NEWL>><<NEWL>>    if not isinstance(byte_str, (bytearray, bytes)):<<NEWL>>        raise TypeError(  # pragma: nocover<<NEWL>>            ""Expected object of type bytes or bytearray, got: ""<<NEWL>>            ""{0}"".format(type(byte_str))<<NEWL>>        )<<NEWL>><<NEWL>>    if isinstance(byte_str, bytearray):<<NEWL>>        byte_str = bytes(byte_str)<<NEWL>><<NEWL>>    r = from_bytes(byte_str).best()<<NEWL>><<NEWL>>    encoding = r.encoding if r is not None else None<<NEWL>>    language = r.language if r is not None and r.language != ""Unknown"" else """"<<NEWL>>    confidence = 1.0 - r.chaos if r is not None else None<<NEWL>><<NEWL>>    # Note: CharsetNormalizer does not return 'UTF-8-SIG' as the sig get stripped in the detection/normalization process<<NEWL>>    # but chardet does return 'utf-8-sig' and it is a valid codec name.<<NEWL>>    if r is not None and encoding == ""utf_8"" and r.bom:<<NEWL>>        encoding += ""_sig""<<NEWL>><<NEWL>>    if should_rename_legacy is False and encoding in CHARDET_CORRESPONDENCE:<<NEWL>>        encoding = CHARDET_CORRESPONDENCE[encoding]<<NEWL>><<NEWL>>    return {<<NEWL>>        ""encoding"": encoding,<<NEWL>>        ""language"": language,<<NEWL>>        ""confidence"": confidence,<<NEWL>>    }"
295	donghui	3	"from typing import Any, Dict, Optional, Union<<NEWL>>from warnings import warn<<NEWL>><<NEWL>>from .api import from_bytes<<NEWL>>from .constant import CHARDET_CORRESPONDENCE<<NEWL>><<NEWL>><<NEWL>>def detect(<<NEWL>>    byte_str: bytes, should_rename_legacy: bool = False, **kwargs: Any<<NEWL>>) -> Dict[str, Optional[Union[str, float]]]:<<NEWL>>    """"""<<NEWL>>    chardet legacy method<<NEWL>>    Detect the encoding of the given byte string. It should be mostly backward-compatible.<<NEWL>>    Encoding name will match Chardet own writing whenever possible. (Not on encoding name unsupported by it)<<NEWL>>    This function is deprecated and should be used to migrate your project easily, consult the documentation for<<NEWL>>    further information. Not planned for removal.<<NEWL>><<NEWL>>    :param byte_str:     The byte sequence to examine.<<NEWL>>    :param should_rename_legacy:  Should we rename legacy encodings<<NEWL>>                                  to their more modern equivalents?<<NEWL>>    """"""<<NEWL>>    if len(kwargs):<<NEWL>>        warn(<<NEWL>>            f""charset-normalizer disregard arguments '{','.join(list(kwargs.keys()))}' in legacy function detect()""<<NEWL>>        )<<NEWL>><<NEWL>>    if not isinstance(byte_str, (bytearray, bytes)):<<NEWL>>        raise TypeError(  # pragma: nocover<<NEWL>>            ""Expected object of type bytes or bytearray, got: ""<<NEWL>>            ""{0}"".format(type(byte_str))<<NEWL>>        )<<NEWL>><<NEWL>>    if isinstance(byte_str, bytearray):<<NEWL>>        byte_str = bytes(byte_str)<<NEWL>><<NEWL>>    r = from_bytes(byte_str).best()<<NEWL>><<NEWL>>    encoding = r.encoding if r is not None else None<<NEWL>>    language = r.language if r is not None and r.language != ""Unknown"" else """"<<NEWL>>    confidence = 1.0 - r.chaos if r is not None else None<<NEWL>><<NEWL>>    # Note: CharsetNormalizer does not return 'UTF-8-SIG' as the sig get stripped in the detection/normalization process<<NEWL>>    # but chardet does return 'utf-8-sig' and it is a valid codec name.<<NEWL>>    if r is not None and encoding == ""utf_8"" and r.bom:<<NEWL>>        encoding += ""_sig""<<NEWL>><<NEWL>>    if should_rename_legacy is False and encoding in CHARDET_CORRESPONDENCE:<<NEWL>>        encoding = CHARDET_CORRESPONDENCE[encoding]<<NEWL>><<NEWL>>    return {<<NEWL>>        ""encoding"": encoding,<<NEWL>>        ""language"": language,<<NEWL>>        ""confidence"": confidence,<<NEWL>>    }"
384	jackson	0	"import esphome.codegen as cg<<NEWL>>import esphome.config_validation as cv<<NEWL>>from esphome import pins<<NEWL>>from esphome.components import display<<NEWL>>from esphome.const import (<<NEWL>>    CONF_BRIGHTNESS,<<NEWL>>    CONF_EXTERNAL_VCC,<<NEWL>>    CONF_LAMBDA,<<NEWL>>    CONF_MODEL,<<NEWL>>    CONF_RESET_PIN,<<NEWL>>)<<NEWL>><<NEWL>>CODEOWNERS = [""@kbx81""]<<NEWL>><<NEWL>>ssd1325_base_ns = cg.esphome_ns.namespace(""ssd1325_base"")<<NEWL>>SSD1325 = ssd1325_base_ns.class_(""SSD1325"", cg.PollingComponent, display.DisplayBuffer)<<NEWL>>SSD1325Model = ssd1325_base_ns.enum(""SSD1325Model"")<<NEWL>><<NEWL>>MODELS = {<<NEWL>>    ""SSD1325_128X32"": SSD1325Model.SSD1325_MODEL_128_32,<<NEWL>>    ""SSD1325_128X64"": SSD1325Model.SSD1325_MODEL_128_64,<<NEWL>>    ""SSD1325_96X16"": SSD1325Model.SSD1325_MODEL_96_16,<<NEWL>>    ""SSD1325_64X48"": SSD1325Model.SSD1325_MODEL_64_48,<<NEWL>>    ""SSD1327_128X128"": SSD1325Model.SSD1327_MODEL_128_128,<<NEWL>>}<<NEWL>><<NEWL>>SSD1325_MODEL = cv.enum(MODELS, upper=True, space=""_"")<<NEWL>><<NEWL>>SSD1325_SCHEMA = display.FULL_DISPLAY_SCHEMA.extend(<<NEWL>>    {<<NEWL>>        cv.Required(CONF_MODEL): SSD1325_MODEL,<<NEWL>>        cv.Optional(CONF_RESET_PIN): pins.gpio_output_pin_schema,<<NEWL>>        cv.Optional(CONF_BRIGHTNESS, default=1.0): cv.percentage,<<NEWL>>        cv.Optional(CONF_EXTERNAL_VCC): cv.boolean,<<NEWL>>    }<<NEWL>>).extend(cv.polling_component_schema(""1s""))<<NEWL>><<NEWL>><<NEWL>>async def setup_ssd1325(var, config):<<NEWL>>    await cg.register_component(var, config)<<NEWL>>    await display.register_display(var, config)<<NEWL>><<NEWL>>    cg.add(var.set_model(config[CONF_MODEL]))<<NEWL>>    if CONF_RESET_PIN in config:<<NEWL>>        reset = await cg.gpio_pin_expression(config[CONF_RESET_PIN])<<NEWL>>        cg.add(var.set_reset_pin(reset))<<NEWL>>    if CONF_BRIGHTNESS in config:<<NEWL>>        cg.add(var.init_brightness(config[CONF_BRIGHTNESS]))<<NEWL>>    if CONF_EXTERNAL_VCC in config:<<NEWL>>        cg.add(var.set_external_vcc(config[CONF_EXTERNAL_VCC]))<<NEWL>>    if CONF_LAMBDA in config:<<NEWL>>        lambda_ = await cg.process_lambda(<<NEWL>>            config[CONF_LAMBDA], [(display.DisplayBufferRef, ""it"")], return_type=cg.void<<NEWL>>        )<<NEWL>>        cg.add(var.set_writer(lambda_))"
384	donghui	0	"import esphome.codegen as cg<<NEWL>>import esphome.config_validation as cv<<NEWL>>from esphome import pins<<NEWL>>from esphome.components import display<<NEWL>>from esphome.const import (<<NEWL>>    CONF_BRIGHTNESS,<<NEWL>>    CONF_EXTERNAL_VCC,<<NEWL>>    CONF_LAMBDA,<<NEWL>>    CONF_MODEL,<<NEWL>>    CONF_RESET_PIN,<<NEWL>>)<<NEWL>><<NEWL>>CODEOWNERS = [""@kbx81""]<<NEWL>><<NEWL>>ssd1325_base_ns = cg.esphome_ns.namespace(""ssd1325_base"")<<NEWL>>SSD1325 = ssd1325_base_ns.class_(""SSD1325"", cg.PollingComponent, display.DisplayBuffer)<<NEWL>>SSD1325Model = ssd1325_base_ns.enum(""SSD1325Model"")<<NEWL>><<NEWL>>MODELS = {<<NEWL>>    ""SSD1325_128X32"": SSD1325Model.SSD1325_MODEL_128_32,<<NEWL>>    ""SSD1325_128X64"": SSD1325Model.SSD1325_MODEL_128_64,<<NEWL>>    ""SSD1325_96X16"": SSD1325Model.SSD1325_MODEL_96_16,<<NEWL>>    ""SSD1325_64X48"": SSD1325Model.SSD1325_MODEL_64_48,<<NEWL>>    ""SSD1327_128X128"": SSD1325Model.SSD1327_MODEL_128_128,<<NEWL>>}<<NEWL>><<NEWL>>SSD1325_MODEL = cv.enum(MODELS, upper=True, space=""_"")<<NEWL>><<NEWL>>SSD1325_SCHEMA = display.FULL_DISPLAY_SCHEMA.extend(<<NEWL>>    {<<NEWL>>        cv.Required(CONF_MODEL): SSD1325_MODEL,<<NEWL>>        cv.Optional(CONF_RESET_PIN): pins.gpio_output_pin_schema,<<NEWL>>        cv.Optional(CONF_BRIGHTNESS, default=1.0): cv.percentage,<<NEWL>>        cv.Optional(CONF_EXTERNAL_VCC): cv.boolean,<<NEWL>>    }<<NEWL>>).extend(cv.polling_component_schema(""1s""))<<NEWL>><<NEWL>><<NEWL>>async def setup_ssd1325(var, config):<<NEWL>>    await cg.register_component(var, config)<<NEWL>>    await display.register_display(var, config)<<NEWL>><<NEWL>>    cg.add(var.set_model(config[CONF_MODEL]))<<NEWL>>    if CONF_RESET_PIN in config:<<NEWL>>        reset = await cg.gpio_pin_expression(config[CONF_RESET_PIN])<<NEWL>>        cg.add(var.set_reset_pin(reset))<<NEWL>>    if CONF_BRIGHTNESS in config:<<NEWL>>        cg.add(var.init_brightness(config[CONF_BRIGHTNESS]))<<NEWL>>    if CONF_EXTERNAL_VCC in config:<<NEWL>>        cg.add(var.set_external_vcc(config[CONF_EXTERNAL_VCC]))<<NEWL>>    if CONF_LAMBDA in config:<<NEWL>>        lambda_ = await cg.process_lambda(<<NEWL>>            config[CONF_LAMBDA], [(display.DisplayBufferRef, ""it"")], return_type=cg.void<<NEWL>>        )<<NEWL>>        cg.add(var.set_writer(lambda_))"
312	jackson	4	"#!../env.py<<NEWL>>#<<NEWL>># SPDX-License-Identifier: BSD-3-Clause<<NEWL>># Copyright 2020, Intel Corporation<<NEWL>><<NEWL>>import testframework as t<<NEWL>>from testframework import granularity as g<<NEWL>>import futils<<NEWL>>import os<<NEWL>><<NEWL>><<NEWL>># All test cases in pmem2_persist_valgrind use Valgrind, which is not available<<NEWL>># on Windows systems.<<NEWL>>@t.windows_exclude<<NEWL>>@t.require_valgrind_enabled('pmemcheck')<<NEWL>># XXX In the match file, there are two possible numbers of errors. It varies<<NEWL>># from compiler to compiler. There should be only one number when pmemcheck<<NEWL>># will be fixed. Please also remove the below requirement after pmemcheck fix.<<NEWL>># https://github.com/pmem/valgrind/pull/76<<NEWL>>@g.require_granularity(g.CL_OR_LESS)<<NEWL>>class PMEM2_PERSIST(t.Test):<<NEWL>>    test_type = t.Medium<<NEWL>>    available_granularity = None<<NEWL>><<NEWL>>    def run(self, ctx):<<NEWL>>        filepath = ctx.create_holey_file(2 * t.MiB, 'testfile')<<NEWL>>        ctx.exec('pmem2_persist_valgrind', self.test_case, filepath)<<NEWL>><<NEWL>><<NEWL>>class TEST0(PMEM2_PERSIST):<<NEWL>>    """"""persist continuous data in a range of pmem""""""<<NEWL>>    test_case = ""test_persist_continuous_range""<<NEWL>><<NEWL>><<NEWL>>class TEST1(PMEM2_PERSIST):<<NEWL>>    """"""persist discontinuous data in a range of pmem""""""<<NEWL>>    test_case = ""test_persist_discontinuous_range""<<NEWL>><<NEWL>><<NEWL>>class TEST2(PMEM2_PERSIST):<<NEWL>>    """"""persist part of discontinuous data in a range of pmem""""""<<NEWL>>    test_case = ""test_persist_discontinuous_range_partially""<<NEWL>><<NEWL>>    def run(self, ctx):<<NEWL>>        filepath = ctx.create_holey_file(16 * t.KiB, 'testfile')<<NEWL>>        ctx.exec('pmem2_persist_valgrind', self.test_case, filepath)<<NEWL>>        pmemecheck_log = os.path.join(<<NEWL>>            os.getcwd(), 'pmem2_persist_valgrind', 'pmemcheck2.log')<<NEWL>>        futils.tail(pmemecheck_log, 2)<<NEWL>><<NEWL>><<NEWL>>class TEST3(PMEM2_PERSIST):<<NEWL>>    """"""persist data in a range of the memory mapped by mmap()""""""<<NEWL>>    test_case = ""test_persist_nonpmem_data"""
312	donghui	3	"#!../env.py<<NEWL>>#<<NEWL>># SPDX-License-Identifier: BSD-3-Clause<<NEWL>># Copyright 2020, Intel Corporation<<NEWL>><<NEWL>>import testframework as t<<NEWL>>from testframework import granularity as g<<NEWL>>import futils<<NEWL>>import os<<NEWL>><<NEWL>><<NEWL>># All test cases in pmem2_persist_valgrind use Valgrind, which is not available<<NEWL>># on Windows systems.<<NEWL>>@t.windows_exclude<<NEWL>>@t.require_valgrind_enabled('pmemcheck')<<NEWL>># XXX In the match file, there are two possible numbers of errors. It varies<<NEWL>># from compiler to compiler. There should be only one number when pmemcheck<<NEWL>># will be fixed. Please also remove the below requirement after pmemcheck fix.<<NEWL>># https://github.com/pmem/valgrind/pull/76<<NEWL>>@g.require_granularity(g.CL_OR_LESS)<<NEWL>>class PMEM2_PERSIST(t.Test):<<NEWL>>    test_type = t.Medium<<NEWL>>    available_granularity = None<<NEWL>><<NEWL>>    def run(self, ctx):<<NEWL>>        filepath = ctx.create_holey_file(2 * t.MiB, 'testfile')<<NEWL>>        ctx.exec('pmem2_persist_valgrind', self.test_case, filepath)<<NEWL>><<NEWL>><<NEWL>>class TEST0(PMEM2_PERSIST):<<NEWL>>    """"""persist continuous data in a range of pmem""""""<<NEWL>>    test_case = ""test_persist_continuous_range""<<NEWL>><<NEWL>><<NEWL>>class TEST1(PMEM2_PERSIST):<<NEWL>>    """"""persist discontinuous data in a range of pmem""""""<<NEWL>>    test_case = ""test_persist_discontinuous_range""<<NEWL>><<NEWL>><<NEWL>>class TEST2(PMEM2_PERSIST):<<NEWL>>    """"""persist part of discontinuous data in a range of pmem""""""<<NEWL>>    test_case = ""test_persist_discontinuous_range_partially""<<NEWL>><<NEWL>>    def run(self, ctx):<<NEWL>>        filepath = ctx.create_holey_file(16 * t.KiB, 'testfile')<<NEWL>>        ctx.exec('pmem2_persist_valgrind', self.test_case, filepath)<<NEWL>>        pmemecheck_log = os.path.join(<<NEWL>>            os.getcwd(), 'pmem2_persist_valgrind', 'pmemcheck2.log')<<NEWL>>        futils.tail(pmemecheck_log, 2)<<NEWL>><<NEWL>><<NEWL>>class TEST3(PMEM2_PERSIST):<<NEWL>>    """"""persist data in a range of the memory mapped by mmap()""""""<<NEWL>>    test_case = ""test_persist_nonpmem_data"""
252	jackson	0	"import datetime, json, os, sys<<NEWL>>from lib.crawling import Crawling<<NEWL>>from lib.database import DB<<NEWL>>from lib.telegram import TeleGram<<NEWL>>from lib.make_data import Make_Data<<NEWL>>from lib.settings import logger, make_folder, args_check, json_check, now, photo_path<<NEWL>><<NEWL>>def run(hscode_dict, crawling, db, tele, make):<<NEWL>>    menus = ''<<NEWL>>    menus += '#수출입데이터 ' + now.strftime('#%Y년%m월%d일') + '\n'<<NEWL>><<NEWL>>    for title in hscode_dict:<<NEWL>>        tag = '#수출입데이터' + ' ' + '#' + title + ' ' + now.strftime('#%Y년%m월%d일')    <<NEWL>>        photo_name = photo_path + str(hscode_dict[title]) + '_' + str(now.year) + str(now.month) + str(now.day) + '.png'<<NEWL>>        crawling_dict = crawling.get_search(hscode_dict[title])<<NEWL>>        df, template, month = make.data_remodel(crawling_dict, hscode_dict[title], tag, db)<<NEWL>>        make.make_photo(df, title, hscode_dict[title], photo_name, month)<<NEWL>>        tele.send_photo(photo_name, template)<<NEWL>>        db.insert_update_db(title, hscode_dict[title], crawling_dict)<<NEWL>>        menus += '#' + title + '\n'<<NEWL>>    tele.send_message(menus)<<NEWL>><<NEWL>>def main():<<NEWL>>    crawling = Crawling()<<NEWL>>    db = DB()<<NEWL>>    tele = TeleGram()<<NEWL>>    make = Make_Data()<<NEWL>><<NEWL>>    for json_file in json_list:<<NEWL>>        with open(json_path + json_file, 'r', encoding='utf-8') as f:<<NEWL>>            hscode_dict = json.load(f)<<NEWL>>        run(hscode_dict, crawling, db, tele, make)<<NEWL>>        f.close()    <<NEWL>><<NEWL>>    db.cs.close()<<NEWL>>    crawling.driver.close()<<NEWL>><<NEWL>>if __name__ == '__main__':<<NEWL>>    args = sys.argv<<NEWL>>    json_path = args_check(args)<<NEWL>>    json_list = json_check(json_path)<<NEWL>><<NEWL>>    logger(f""Main started at {now}"")<<NEWL>>    make_folder()<<NEWL>>    main()<<NEWL>>    et = datetime.datetime.now()<<NEWL>>    logger(f""Main finished at {et}"")<<NEWL>>    logger(f""Main time for task: {et-now}"")<<NEWL>>    os.system(""sudo rm -rf {photo_path}*.png"".format(photo_path=photo_path))"
252	donghui	0	"import datetime, json, os, sys<<NEWL>>from lib.crawling import Crawling<<NEWL>>from lib.database import DB<<NEWL>>from lib.telegram import TeleGram<<NEWL>>from lib.make_data import Make_Data<<NEWL>>from lib.settings import logger, make_folder, args_check, json_check, now, photo_path<<NEWL>><<NEWL>>def run(hscode_dict, crawling, db, tele, make):<<NEWL>>    menus = ''<<NEWL>>    menus += '#수출입데이터 ' + now.strftime('#%Y년%m월%d일') + '\n'<<NEWL>><<NEWL>>    for title in hscode_dict:<<NEWL>>        tag = '#수출입데이터' + ' ' + '#' + title + ' ' + now.strftime('#%Y년%m월%d일')    <<NEWL>>        photo_name = photo_path + str(hscode_dict[title]) + '_' + str(now.year) + str(now.month) + str(now.day) + '.png'<<NEWL>>        crawling_dict = crawling.get_search(hscode_dict[title])<<NEWL>>        df, template, month = make.data_remodel(crawling_dict, hscode_dict[title], tag, db)<<NEWL>>        make.make_photo(df, title, hscode_dict[title], photo_name, month)<<NEWL>>        tele.send_photo(photo_name, template)<<NEWL>>        db.insert_update_db(title, hscode_dict[title], crawling_dict)<<NEWL>>        menus += '#' + title + '\n'<<NEWL>>    tele.send_message(menus)<<NEWL>><<NEWL>>def main():<<NEWL>>    crawling = Crawling()<<NEWL>>    db = DB()<<NEWL>>    tele = TeleGram()<<NEWL>>    make = Make_Data()<<NEWL>><<NEWL>>    for json_file in json_list:<<NEWL>>        with open(json_path + json_file, 'r', encoding='utf-8') as f:<<NEWL>>            hscode_dict = json.load(f)<<NEWL>>        run(hscode_dict, crawling, db, tele, make)<<NEWL>>        f.close()    <<NEWL>><<NEWL>>    db.cs.close()<<NEWL>>    crawling.driver.close()<<NEWL>><<NEWL>>if __name__ == '__main__':<<NEWL>>    args = sys.argv<<NEWL>>    json_path = args_check(args)<<NEWL>>    json_list = json_check(json_path)<<NEWL>><<NEWL>>    logger(f""Main started at {now}"")<<NEWL>>    make_folder()<<NEWL>>    main()<<NEWL>>    et = datetime.datetime.now()<<NEWL>>    logger(f""Main finished at {et}"")<<NEWL>>    logger(f""Main time for task: {et-now}"")<<NEWL>>    os.system(""sudo rm -rf {photo_path}*.png"".format(photo_path=photo_path))"
343	jackson	4	"""""""<<NEWL>>PostGIS to GDAL conversion constant definitions<<NEWL>>""""""<<NEWL>># Lookup to convert pixel type values from GDAL to PostGIS<<NEWL>>GDAL_TO_POSTGIS = [None, 4, 6, 5, 8, 7, 10, 11, None, None, None, None]<<NEWL>><<NEWL>># Lookup to convert pixel type values from PostGIS to GDAL<<NEWL>>POSTGIS_TO_GDAL = [1, 1, 1, 3, 1, 3, 2, 5, 4, None, 6, 7, None, None]<<NEWL>><<NEWL>># Struct pack structure for raster header, the raster header has the<<NEWL>># following structure:<<NEWL>>#<<NEWL>># Endianness, PostGIS raster version, number of bands, scale, origin,<<NEWL>># skew, srid, width, and height.<<NEWL>>#<<NEWL>># Scale, origin, and skew have x and y values. PostGIS currently uses<<NEWL>># a fixed endianness (1) and there is only one version (0).<<NEWL>>POSTGIS_HEADER_STRUCTURE = 'B H H d d d d d d i H H'<<NEWL>><<NEWL>># Lookup values to convert GDAL pixel types to struct characters. This is<<NEWL>># used to pack and unpack the pixel values of PostGIS raster bands.<<NEWL>>GDAL_TO_STRUCT = [<<NEWL>>    None, 'B', 'H', 'h', 'L', 'l', 'f', 'd',<<NEWL>>    None, None, None, None,<<NEWL>>]<<NEWL>><<NEWL>># Size of the packed value in bytes for different numerical types.<<NEWL>># This is needed to cut chunks of band data out of PostGIS raster strings<<NEWL>># when decomposing them into GDALRasters.<<NEWL>># See https://docs.python.org/library/struct.html#format-characters<<NEWL>>STRUCT_SIZE = {<<NEWL>>    'b': 1,  # Signed char<<NEWL>>    'B': 1,  # Unsigned char<<NEWL>>    '?': 1,  # _Bool<<NEWL>>    'h': 2,  # Short<<NEWL>>    'H': 2,  # Unsigned short<<NEWL>>    'i': 4,  # Integer<<NEWL>>    'I': 4,  # Unsigned Integer<<NEWL>>    'l': 4,  # Long<<NEWL>>    'L': 4,  # Unsigned Long<<NEWL>>    'f': 4,  # Float<<NEWL>>    'd': 8,  # Double<<NEWL>>}<<NEWL>><<NEWL>># Pixel type specifies type of pixel values in a band. Storage flag specifies<<NEWL>># whether the band data is stored as part of the datum or is to be found on the<<NEWL>># server's filesystem. There are currently 11 supported pixel value types, so 4<<NEWL>># bits are enough to account for all. Reserve the upper 4 bits for generic<<NEWL>># flags.<<NEWL>># See https://trac.osgeo.org/postgis/wiki/WKTRaster/RFC/RFC1_V0SerialFormat#Pixeltypeandstorageflag<<NEWL>>BANDTYPE_PIXTYPE_MASK = 0x0F<<NEWL>>BANDTYPE_FLAG_HASNODATA = 1 << 6"
343	donghui	2	"""""""<<NEWL>>PostGIS to GDAL conversion constant definitions<<NEWL>>""""""<<NEWL>># Lookup to convert pixel type values from GDAL to PostGIS<<NEWL>>GDAL_TO_POSTGIS = [None, 4, 6, 5, 8, 7, 10, 11, None, None, None, None]<<NEWL>><<NEWL>># Lookup to convert pixel type values from PostGIS to GDAL<<NEWL>>POSTGIS_TO_GDAL = [1, 1, 1, 3, 1, 3, 2, 5, 4, None, 6, 7, None, None]<<NEWL>><<NEWL>># Struct pack structure for raster header, the raster header has the<<NEWL>># following structure:<<NEWL>>#<<NEWL>># Endianness, PostGIS raster version, number of bands, scale, origin,<<NEWL>># skew, srid, width, and height.<<NEWL>>#<<NEWL>># Scale, origin, and skew have x and y values. PostGIS currently uses<<NEWL>># a fixed endianness (1) and there is only one version (0).<<NEWL>>POSTGIS_HEADER_STRUCTURE = 'B H H d d d d d d i H H'<<NEWL>><<NEWL>># Lookup values to convert GDAL pixel types to struct characters. This is<<NEWL>># used to pack and unpack the pixel values of PostGIS raster bands.<<NEWL>>GDAL_TO_STRUCT = [<<NEWL>>    None, 'B', 'H', 'h', 'L', 'l', 'f', 'd',<<NEWL>>    None, None, None, None,<<NEWL>>]<<NEWL>><<NEWL>># Size of the packed value in bytes for different numerical types.<<NEWL>># This is needed to cut chunks of band data out of PostGIS raster strings<<NEWL>># when decomposing them into GDALRasters.<<NEWL>># See https://docs.python.org/library/struct.html#format-characters<<NEWL>>STRUCT_SIZE = {<<NEWL>>    'b': 1,  # Signed char<<NEWL>>    'B': 1,  # Unsigned char<<NEWL>>    '?': 1,  # _Bool<<NEWL>>    'h': 2,  # Short<<NEWL>>    'H': 2,  # Unsigned short<<NEWL>>    'i': 4,  # Integer<<NEWL>>    'I': 4,  # Unsigned Integer<<NEWL>>    'l': 4,  # Long<<NEWL>>    'L': 4,  # Unsigned Long<<NEWL>>    'f': 4,  # Float<<NEWL>>    'd': 8,  # Double<<NEWL>>}<<NEWL>><<NEWL>># Pixel type specifies type of pixel values in a band. Storage flag specifies<<NEWL>># whether the band data is stored as part of the datum or is to be found on the<<NEWL>># server's filesystem. There are currently 11 supported pixel value types, so 4<<NEWL>># bits are enough to account for all. Reserve the upper 4 bits for generic<<NEWL>># flags.<<NEWL>># See https://trac.osgeo.org/postgis/wiki/WKTRaster/RFC/RFC1_V0SerialFormat#Pixeltypeandstorageflag<<NEWL>>BANDTYPE_PIXTYPE_MASK = 0x0F<<NEWL>>BANDTYPE_FLAG_HASNODATA = 1 << 6"
390	jackson	0	"######################## BEGIN LICENSE BLOCK ########################<<NEWL>># The Original Code is mozilla.org code.<<NEWL>>#<<NEWL>># The Initial Developer of the Original Code is<<NEWL>># Netscape Communications Corporation.<<NEWL>># Portions created by the Initial Developer are Copyright (C) 1998<<NEWL>># the Initial Developer. All Rights Reserved.<<NEWL>>#<<NEWL>># Contributor(s):<<NEWL>>#   Mark Pilgrim - port to Python<<NEWL>>#<<NEWL>># This library is free software; you can redistribute it and/or<<NEWL>># modify it under the terms of the GNU Lesser General Public<<NEWL>># License as published by the Free Software Foundation; either<<NEWL>># version 2.1 of the License, or (at your option) any later version.<<NEWL>>#<<NEWL>># This library is distributed in the hope that it will be useful,<<NEWL>># but WITHOUT ANY WARRANTY; without even the implied warranty of<<NEWL>># MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU<<NEWL>># Lesser General Public License for more details.<<NEWL>>#<<NEWL>># You should have received a copy of the GNU Lesser General Public<<NEWL>># License along with this library; if not, write to the Free Software<<NEWL>># Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA<<NEWL>># 02110-1301  USA<<NEWL>>######################### END LICENSE BLOCK #########################<<NEWL>><<NEWL>>from .chardistribution import EUCKRDistributionAnalysis<<NEWL>>from .codingstatemachine import CodingStateMachine<<NEWL>>from .mbcharsetprober import MultiByteCharSetProber<<NEWL>>from .mbcssm import EUCKR_SM_MODEL<<NEWL>><<NEWL>><<NEWL>>class EUCKRProber(MultiByteCharSetProber):<<NEWL>>    def __init__(self) -> None:<<NEWL>>        super().__init__()<<NEWL>>        self.coding_sm = CodingStateMachine(EUCKR_SM_MODEL)<<NEWL>>        self.distribution_analyzer = EUCKRDistributionAnalysis()<<NEWL>>        self.reset()<<NEWL>><<NEWL>>    @property<<NEWL>>    def charset_name(self) -> str:<<NEWL>>        return ""EUC-KR""<<NEWL>><<NEWL>>    @property<<NEWL>>    def language(self) -> str:<<NEWL>>        return ""Korean"""
390	donghui	0	"######################## BEGIN LICENSE BLOCK ########################<<NEWL>># The Original Code is mozilla.org code.<<NEWL>>#<<NEWL>># The Initial Developer of the Original Code is<<NEWL>># Netscape Communications Corporation.<<NEWL>># Portions created by the Initial Developer are Copyright (C) 1998<<NEWL>># the Initial Developer. All Rights Reserved.<<NEWL>>#<<NEWL>># Contributor(s):<<NEWL>>#   Mark Pilgrim - port to Python<<NEWL>>#<<NEWL>># This library is free software; you can redistribute it and/or<<NEWL>># modify it under the terms of the GNU Lesser General Public<<NEWL>># License as published by the Free Software Foundation; either<<NEWL>># version 2.1 of the License, or (at your option) any later version.<<NEWL>>#<<NEWL>># This library is distributed in the hope that it will be useful,<<NEWL>># but WITHOUT ANY WARRANTY; without even the implied warranty of<<NEWL>># MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU<<NEWL>># Lesser General Public License for more details.<<NEWL>>#<<NEWL>># You should have received a copy of the GNU Lesser General Public<<NEWL>># License along with this library; if not, write to the Free Software<<NEWL>># Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA<<NEWL>># 02110-1301  USA<<NEWL>>######################### END LICENSE BLOCK #########################<<NEWL>><<NEWL>>from .chardistribution import EUCKRDistributionAnalysis<<NEWL>>from .codingstatemachine import CodingStateMachine<<NEWL>>from .mbcharsetprober import MultiByteCharSetProber<<NEWL>>from .mbcssm import EUCKR_SM_MODEL<<NEWL>><<NEWL>><<NEWL>>class EUCKRProber(MultiByteCharSetProber):<<NEWL>>    def __init__(self) -> None:<<NEWL>>        super().__init__()<<NEWL>>        self.coding_sm = CodingStateMachine(EUCKR_SM_MODEL)<<NEWL>>        self.distribution_analyzer = EUCKRDistributionAnalysis()<<NEWL>>        self.reset()<<NEWL>><<NEWL>>    @property<<NEWL>>    def charset_name(self) -> str:<<NEWL>>        return ""EUC-KR""<<NEWL>><<NEWL>>    @property<<NEWL>>    def language(self) -> str:<<NEWL>>        return ""Korean"""
363	jackson	0	"import esphome.codegen as cg<<NEWL>>import esphome.config_validation as cv<<NEWL>>from esphome.components import switch<<NEWL>>from esphome.const import ICON_POWER<<NEWL>>from .. import CONF_PIPSOLAR_ID, PIPSOLAR_COMPONENT_SCHEMA, pipsolar_ns<<NEWL>><<NEWL>>DEPENDENCIES = [""uart""]<<NEWL>><<NEWL>>CONF_OUTPUT_SOURCE_PRIORITY_UTILITY = ""output_source_priority_utility""<<NEWL>>CONF_OUTPUT_SOURCE_PRIORITY_SOLAR = ""output_source_priority_solar""<<NEWL>>CONF_OUTPUT_SOURCE_PRIORITY_BATTERY = ""output_source_priority_battery""<<NEWL>>CONF_INPUT_VOLTAGE_RANGE = ""input_voltage_range""<<NEWL>>CONF_PV_OK_CONDITION_FOR_PARALLEL = ""pv_ok_condition_for_parallel""<<NEWL>>CONF_PV_POWER_BALANCE = ""pv_power_balance""<<NEWL>><<NEWL>>TYPES = {<<NEWL>>    CONF_OUTPUT_SOURCE_PRIORITY_UTILITY: (""POP00"", None),<<NEWL>>    CONF_OUTPUT_SOURCE_PRIORITY_SOLAR: (""POP01"", None),<<NEWL>>    CONF_OUTPUT_SOURCE_PRIORITY_BATTERY: (""POP02"", None),<<NEWL>>    CONF_INPUT_VOLTAGE_RANGE: (""PGR01"", ""PGR00""),<<NEWL>>    CONF_PV_OK_CONDITION_FOR_PARALLEL: (""PPVOKC1"", ""PPVOKC0""),<<NEWL>>    CONF_PV_POWER_BALANCE: (""PSPB1"", ""PSPB0""),<<NEWL>>}<<NEWL>><<NEWL>>PipsolarSwitch = pipsolar_ns.class_(""PipsolarSwitch"", switch.Switch, cg.Component)<<NEWL>><<NEWL>>PIPSWITCH_SCHEMA = switch.switch_schema(<<NEWL>>    PipsolarSwitch, icon=ICON_POWER, block_inverted=True<<NEWL>>).extend(cv.COMPONENT_SCHEMA)<<NEWL>><<NEWL>>CONFIG_SCHEMA = PIPSOLAR_COMPONENT_SCHEMA.extend(<<NEWL>>    {cv.Optional(type): PIPSWITCH_SCHEMA for type in TYPES}<<NEWL>>)<<NEWL>><<NEWL>><<NEWL>>async def to_code(config):<<NEWL>>    paren = await cg.get_variable(config[CONF_PIPSOLAR_ID])<<NEWL>><<NEWL>>    for type, (on, off) in TYPES.items():<<NEWL>>        if type in config:<<NEWL>>            conf = config[type]<<NEWL>>            var = await switch.new_switch(conf)<<NEWL>>            await cg.register_component(var, conf)<<NEWL>>            cg.add(getattr(paren, f""set_{type}_switch"")(var))<<NEWL>>            cg.add(var.set_parent(paren))<<NEWL>>            cg.add(var.set_on_command(on))<<NEWL>>            if off is not None:<<NEWL>>                cg.add(var.set_off_command(off))"
363	donghui	0	"import esphome.codegen as cg<<NEWL>>import esphome.config_validation as cv<<NEWL>>from esphome.components import switch<<NEWL>>from esphome.const import ICON_POWER<<NEWL>>from .. import CONF_PIPSOLAR_ID, PIPSOLAR_COMPONENT_SCHEMA, pipsolar_ns<<NEWL>><<NEWL>>DEPENDENCIES = [""uart""]<<NEWL>><<NEWL>>CONF_OUTPUT_SOURCE_PRIORITY_UTILITY = ""output_source_priority_utility""<<NEWL>>CONF_OUTPUT_SOURCE_PRIORITY_SOLAR = ""output_source_priority_solar""<<NEWL>>CONF_OUTPUT_SOURCE_PRIORITY_BATTERY = ""output_source_priority_battery""<<NEWL>>CONF_INPUT_VOLTAGE_RANGE = ""input_voltage_range""<<NEWL>>CONF_PV_OK_CONDITION_FOR_PARALLEL = ""pv_ok_condition_for_parallel""<<NEWL>>CONF_PV_POWER_BALANCE = ""pv_power_balance""<<NEWL>><<NEWL>>TYPES = {<<NEWL>>    CONF_OUTPUT_SOURCE_PRIORITY_UTILITY: (""POP00"", None),<<NEWL>>    CONF_OUTPUT_SOURCE_PRIORITY_SOLAR: (""POP01"", None),<<NEWL>>    CONF_OUTPUT_SOURCE_PRIORITY_BATTERY: (""POP02"", None),<<NEWL>>    CONF_INPUT_VOLTAGE_RANGE: (""PGR01"", ""PGR00""),<<NEWL>>    CONF_PV_OK_CONDITION_FOR_PARALLEL: (""PPVOKC1"", ""PPVOKC0""),<<NEWL>>    CONF_PV_POWER_BALANCE: (""PSPB1"", ""PSPB0""),<<NEWL>>}<<NEWL>><<NEWL>>PipsolarSwitch = pipsolar_ns.class_(""PipsolarSwitch"", switch.Switch, cg.Component)<<NEWL>><<NEWL>>PIPSWITCH_SCHEMA = switch.switch_schema(<<NEWL>>    PipsolarSwitch, icon=ICON_POWER, block_inverted=True<<NEWL>>).extend(cv.COMPONENT_SCHEMA)<<NEWL>><<NEWL>>CONFIG_SCHEMA = PIPSOLAR_COMPONENT_SCHEMA.extend(<<NEWL>>    {cv.Optional(type): PIPSWITCH_SCHEMA for type in TYPES}<<NEWL>>)<<NEWL>><<NEWL>><<NEWL>>async def to_code(config):<<NEWL>>    paren = await cg.get_variable(config[CONF_PIPSOLAR_ID])<<NEWL>><<NEWL>>    for type, (on, off) in TYPES.items():<<NEWL>>        if type in config:<<NEWL>>            conf = config[type]<<NEWL>>            var = await switch.new_switch(conf)<<NEWL>>            await cg.register_component(var, conf)<<NEWL>>            cg.add(getattr(paren, f""set_{type}_switch"")(var))<<NEWL>>            cg.add(var.set_parent(paren))<<NEWL>>            cg.add(var.set_on_command(on))<<NEWL>>            if off is not None:<<NEWL>>                cg.add(var.set_off_command(off))"
332	jackson	4	"from __future__ import annotations<<NEWL>><<NEWL>>import numpy as np<<NEWL>><<NEWL>>from pandas._typing import NumpyIndexT<<NEWL>><<NEWL>>from pandas.core.dtypes.common import is_list_like<<NEWL>><<NEWL>><<NEWL>>def cartesian_product(X) -> list[np.ndarray]:<<NEWL>>    """"""<<NEWL>>    Numpy version of itertools.product.<<NEWL>>    Sometimes faster (for large inputs)...<<NEWL>><<NEWL>>    Parameters<<NEWL>>    ----------<<NEWL>>    X : list-like of list-likes<<NEWL>><<NEWL>>    Returns<<NEWL>>    -------<<NEWL>>    product : list of ndarrays<<NEWL>><<NEWL>>    Examples<<NEWL>>    --------<<NEWL>>    >>> cartesian_product([list('ABC'), [1, 2]])<<NEWL>>    [array(['A', 'A', 'B', 'B', 'C', 'C'], dtype='<U1'), array([1, 2, 1, 2, 1, 2])]<<NEWL>><<NEWL>>    See Also<<NEWL>>    --------<<NEWL>>    itertools.product : Cartesian product of input iterables.  Equivalent to<<NEWL>>        nested for-loops.<<NEWL>>    """"""<<NEWL>>    msg = ""Input must be a list-like of list-likes""<<NEWL>>    if not is_list_like(X):<<NEWL>>        raise TypeError(msg)<<NEWL>>    for x in X:<<NEWL>>        if not is_list_like(x):<<NEWL>>            raise TypeError(msg)<<NEWL>><<NEWL>>    if len(X) == 0:<<NEWL>>        return []<<NEWL>><<NEWL>>    lenX = np.fromiter((len(x) for x in X), dtype=np.intp)<<NEWL>>    cumprodX = np.cumproduct(lenX)<<NEWL>><<NEWL>>    if np.any(cumprodX < 0):<<NEWL>>        raise ValueError(""Product space too large to allocate arrays!"")<<NEWL>><<NEWL>>    a = np.roll(cumprodX, 1)<<NEWL>>    a[0] = 1<<NEWL>><<NEWL>>    if cumprodX[-1] != 0:<<NEWL>>        b = cumprodX[-1] / cumprodX<<NEWL>>    else:<<NEWL>>        # if any factor is empty, the cartesian product is empty<<NEWL>>        b = np.zeros_like(cumprodX)<<NEWL>><<NEWL>>    # error: Argument of type ""int_"" cannot be assigned to parameter ""num"" of<<NEWL>>    # type ""int"" in function ""tile_compat""<<NEWL>>    return [<<NEWL>>        tile_compat(<<NEWL>>            np.repeat(x, b[i]),<<NEWL>>            np.product(a[i]),  # pyright: ignore[reportGeneralTypeIssues]<<NEWL>>        )<<NEWL>>        for i, x in enumerate(X)<<NEWL>>    ]<<NEWL>><<NEWL>><<NEWL>>def tile_compat(arr: NumpyIndexT, num: int) -> NumpyIndexT:<<NEWL>>    """"""<<NEWL>>    Index compat for np.tile.<<NEWL>><<NEWL>>    Notes<<NEWL>>    -----<<NEWL>>    Does not support multi-dimensional `num`.<<NEWL>>    """"""<<NEWL>>    if isinstance(arr, np.ndarray):<<NEWL>>        return np.tile(arr, num)<<NEWL>><<NEWL>>    # Otherwise we have an Index<<NEWL>>    taker = np.tile(np.arange(len(arr)), num)<<NEWL>>    return arr.take(taker)"
332	donghui	4	"from __future__ import annotations<<NEWL>><<NEWL>>import numpy as np<<NEWL>><<NEWL>>from pandas._typing import NumpyIndexT<<NEWL>><<NEWL>>from pandas.core.dtypes.common import is_list_like<<NEWL>><<NEWL>><<NEWL>>def cartesian_product(X) -> list[np.ndarray]:<<NEWL>>    """"""<<NEWL>>    Numpy version of itertools.product.<<NEWL>>    Sometimes faster (for large inputs)...<<NEWL>><<NEWL>>    Parameters<<NEWL>>    ----------<<NEWL>>    X : list-like of list-likes<<NEWL>><<NEWL>>    Returns<<NEWL>>    -------<<NEWL>>    product : list of ndarrays<<NEWL>><<NEWL>>    Examples<<NEWL>>    --------<<NEWL>>    >>> cartesian_product([list('ABC'), [1, 2]])<<NEWL>>    [array(['A', 'A', 'B', 'B', 'C', 'C'], dtype='<U1'), array([1, 2, 1, 2, 1, 2])]<<NEWL>><<NEWL>>    See Also<<NEWL>>    --------<<NEWL>>    itertools.product : Cartesian product of input iterables.  Equivalent to<<NEWL>>        nested for-loops.<<NEWL>>    """"""<<NEWL>>    msg = ""Input must be a list-like of list-likes""<<NEWL>>    if not is_list_like(X):<<NEWL>>        raise TypeError(msg)<<NEWL>>    for x in X:<<NEWL>>        if not is_list_like(x):<<NEWL>>            raise TypeError(msg)<<NEWL>><<NEWL>>    if len(X) == 0:<<NEWL>>        return []<<NEWL>><<NEWL>>    lenX = np.fromiter((len(x) for x in X), dtype=np.intp)<<NEWL>>    cumprodX = np.cumproduct(lenX)<<NEWL>><<NEWL>>    if np.any(cumprodX < 0):<<NEWL>>        raise ValueError(""Product space too large to allocate arrays!"")<<NEWL>><<NEWL>>    a = np.roll(cumprodX, 1)<<NEWL>>    a[0] = 1<<NEWL>><<NEWL>>    if cumprodX[-1] != 0:<<NEWL>>        b = cumprodX[-1] / cumprodX<<NEWL>>    else:<<NEWL>>        # if any factor is empty, the cartesian product is empty<<NEWL>>        b = np.zeros_like(cumprodX)<<NEWL>><<NEWL>>    # error: Argument of type ""int_"" cannot be assigned to parameter ""num"" of<<NEWL>>    # type ""int"" in function ""tile_compat""<<NEWL>>    return [<<NEWL>>        tile_compat(<<NEWL>>            np.repeat(x, b[i]),<<NEWL>>            np.product(a[i]),  # pyright: ignore[reportGeneralTypeIssues]<<NEWL>>        )<<NEWL>>        for i, x in enumerate(X)<<NEWL>>    ]<<NEWL>><<NEWL>><<NEWL>>def tile_compat(arr: NumpyIndexT, num: int) -> NumpyIndexT:<<NEWL>>    """"""<<NEWL>>    Index compat for np.tile.<<NEWL>><<NEWL>>    Notes<<NEWL>>    -----<<NEWL>>    Does not support multi-dimensional `num`.<<NEWL>>    """"""<<NEWL>>    if isinstance(arr, np.ndarray):<<NEWL>>        return np.tile(arr, num)<<NEWL>><<NEWL>>    # Otherwise we have an Index<<NEWL>>    taker = np.tile(np.arange(len(arr)), num)<<NEWL>>    return arr.take(taker)"
272	jackson	3	"#!/usr/bin/env python<<NEWL>>#<<NEWL>># Copyright 2020 Google LLC<<NEWL>>#<<NEWL>># Licensed under the Apache License, Version 2.0 (the ""License"");<<NEWL>># you may not use this file except in compliance with the License.<<NEWL>># You may obtain a copy of the License at<<NEWL>>#<<NEWL>>#     https://www.apache.org/licenses/LICENSE-2.0<<NEWL>>#<<NEWL>># Unless required by applicable law or agreed to in writing, software<<NEWL>># distributed under the License is distributed on an ""AS IS"" BASIS,<<NEWL>># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.<<NEWL>># See the License for the specific language governing permissions and<<NEWL>># limitations under the License.<<NEWL>>""""""Demo for receiving notifications.""""""<<NEWL>><<NEWL>><<NEWL>>def receive_notifications(project_id, subscription_name):<<NEWL>>    # [START securitycenter_receive_notifications]<<NEWL>>    # Requires https://cloud.google.com/pubsub/docs/quickstart-client-libraries#pubsub-client-libraries-python<<NEWL>>    import concurrent<<NEWL>><<NEWL>>    from google.cloud import pubsub_v1<<NEWL>>    from google.cloud.securitycenter_v1 import NotificationMessage<<NEWL>><<NEWL>>    # TODO: project_id = ""your-project-id""<<NEWL>>    # TODO: subscription_name = ""your-subscription-name""<<NEWL>><<NEWL>>    def callback(message):<<NEWL>><<NEWL>>        # Print the data received for debugging purpose if needed<<NEWL>>        print(f""Received message: {message.data}"")<<NEWL>><<NEWL>>        notification_msg = NotificationMessage.from_json(message.data)<<NEWL>><<NEWL>>        print(<<NEWL>>            ""Notification config name: {}"".format(<<NEWL>>                notification_msg.notification_config_name<<NEWL>>            )<<NEWL>>        )<<NEWL>>        print(""Finding: {}"".format(notification_msg.finding))<<NEWL>><<NEWL>>        # Ack the message to prevent it from being pulled again<<NEWL>>        message.ack()<<NEWL>><<NEWL>>    subscriber = pubsub_v1.SubscriberClient()<<NEWL>>    subscription_path = subscriber.subscription_path(project_id, subscription_name)<<NEWL>><<NEWL>>    streaming_pull_future = subscriber.subscribe(subscription_path, callback=callback)<<NEWL>><<NEWL>>    print(""Listening for messages on {}...\n"".format(subscription_path))<<NEWL>>    try:<<NEWL>>        streaming_pull_future.result(timeout=1)  # Block for 1 second<<NEWL>>    except concurrent.futures.TimeoutError:<<NEWL>>        streaming_pull_future.cancel()<<NEWL>>    # [END securitycenter_receive_notifications]<<NEWL>>    return True"
272	donghui	1	"#!/usr/bin/env python<<NEWL>>#<<NEWL>># Copyright 2020 Google LLC<<NEWL>>#<<NEWL>># Licensed under the Apache License, Version 2.0 (the ""License"");<<NEWL>># you may not use this file except in compliance with the License.<<NEWL>># You may obtain a copy of the License at<<NEWL>>#<<NEWL>>#     https://www.apache.org/licenses/LICENSE-2.0<<NEWL>>#<<NEWL>># Unless required by applicable law or agreed to in writing, software<<NEWL>># distributed under the License is distributed on an ""AS IS"" BASIS,<<NEWL>># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.<<NEWL>># See the License for the specific language governing permissions and<<NEWL>># limitations under the License.<<NEWL>>""""""Demo for receiving notifications.""""""<<NEWL>><<NEWL>><<NEWL>>def receive_notifications(project_id, subscription_name):<<NEWL>>    # [START securitycenter_receive_notifications]<<NEWL>>    # Requires https://cloud.google.com/pubsub/docs/quickstart-client-libraries#pubsub-client-libraries-python<<NEWL>>    import concurrent<<NEWL>><<NEWL>>    from google.cloud import pubsub_v1<<NEWL>>    from google.cloud.securitycenter_v1 import NotificationMessage<<NEWL>><<NEWL>>    # TODO: project_id = ""your-project-id""<<NEWL>>    # TODO: subscription_name = ""your-subscription-name""<<NEWL>><<NEWL>>    def callback(message):<<NEWL>><<NEWL>>        # Print the data received for debugging purpose if needed<<NEWL>>        print(f""Received message: {message.data}"")<<NEWL>><<NEWL>>        notification_msg = NotificationMessage.from_json(message.data)<<NEWL>><<NEWL>>        print(<<NEWL>>            ""Notification config name: {}"".format(<<NEWL>>                notification_msg.notification_config_name<<NEWL>>            )<<NEWL>>        )<<NEWL>>        print(""Finding: {}"".format(notification_msg.finding))<<NEWL>><<NEWL>>        # Ack the message to prevent it from being pulled again<<NEWL>>        message.ack()<<NEWL>><<NEWL>>    subscriber = pubsub_v1.SubscriberClient()<<NEWL>>    subscription_path = subscriber.subscription_path(project_id, subscription_name)<<NEWL>><<NEWL>>    streaming_pull_future = subscriber.subscribe(subscription_path, callback=callback)<<NEWL>><<NEWL>>    print(""Listening for messages on {}...\n"".format(subscription_path))<<NEWL>>    try:<<NEWL>>        streaming_pull_future.result(timeout=1)  # Block for 1 second<<NEWL>>    except concurrent.futures.TimeoutError:<<NEWL>>        streaming_pull_future.cancel()<<NEWL>>    # [END securitycenter_receive_notifications]<<NEWL>>    return True"
306	jackson	3	"from . import engines<<NEWL>>from .exceptions import TemplateDoesNotExist<<NEWL>><<NEWL>><<NEWL>>def get_template(template_name, using=None):<<NEWL>>    """"""<<NEWL>>    Load and return a template for the given name.<<NEWL>><<NEWL>>    Raise TemplateDoesNotExist if no such template exists.<<NEWL>>    """"""<<NEWL>>    chain = []<<NEWL>>    engines = _engine_list(using)<<NEWL>>    for engine in engines:<<NEWL>>        try:<<NEWL>>            return engine.get_template(template_name)<<NEWL>>        except TemplateDoesNotExist as e:<<NEWL>>            chain.append(e)<<NEWL>><<NEWL>>    raise TemplateDoesNotExist(template_name, chain=chain)<<NEWL>><<NEWL>><<NEWL>>def select_template(template_name_list, using=None):<<NEWL>>    """"""<<NEWL>>    Load and return a template for one of the given names.<<NEWL>><<NEWL>>    Try names in order and return the first template found.<<NEWL>><<NEWL>>    Raise TemplateDoesNotExist if no such template exists.<<NEWL>>    """"""<<NEWL>>    if isinstance(template_name_list, str):<<NEWL>>        raise TypeError(<<NEWL>>            ""select_template() takes an iterable of template names but got a ""<<NEWL>>            ""string: %r. Use get_template() if you want to load a single ""<<NEWL>>            ""template by name."" % template_name_list<<NEWL>>        )<<NEWL>><<NEWL>>    chain = []<<NEWL>>    engines = _engine_list(using)<<NEWL>>    for template_name in template_name_list:<<NEWL>>        for engine in engines:<<NEWL>>            try:<<NEWL>>                return engine.get_template(template_name)<<NEWL>>            except TemplateDoesNotExist as e:<<NEWL>>                chain.append(e)<<NEWL>><<NEWL>>    if template_name_list:<<NEWL>>        raise TemplateDoesNotExist("", "".join(template_name_list), chain=chain)<<NEWL>>    else:<<NEWL>>        raise TemplateDoesNotExist(""No template names provided"")<<NEWL>><<NEWL>><<NEWL>>def render_to_string(template_name, context=None, request=None, using=None):<<NEWL>>    """"""<<NEWL>>    Load a template and render it with a context. Return a string.<<NEWL>><<NEWL>>    template_name may be a string or a list of strings.<<NEWL>>    """"""<<NEWL>>    if isinstance(template_name, (list, tuple)):<<NEWL>>        template = select_template(template_name, using=using)<<NEWL>>    else:<<NEWL>>        template = get_template(template_name, using=using)<<NEWL>>    return template.render(context, request)<<NEWL>><<NEWL>><<NEWL>>def _engine_list(using=None):<<NEWL>>    return engines.all() if using is None else [engines[using]]"
306	donghui	2	"from . import engines<<NEWL>>from .exceptions import TemplateDoesNotExist<<NEWL>><<NEWL>><<NEWL>>def get_template(template_name, using=None):<<NEWL>>    """"""<<NEWL>>    Load and return a template for the given name.<<NEWL>><<NEWL>>    Raise TemplateDoesNotExist if no such template exists.<<NEWL>>    """"""<<NEWL>>    chain = []<<NEWL>>    engines = _engine_list(using)<<NEWL>>    for engine in engines:<<NEWL>>        try:<<NEWL>>            return engine.get_template(template_name)<<NEWL>>        except TemplateDoesNotExist as e:<<NEWL>>            chain.append(e)<<NEWL>><<NEWL>>    raise TemplateDoesNotExist(template_name, chain=chain)<<NEWL>><<NEWL>><<NEWL>>def select_template(template_name_list, using=None):<<NEWL>>    """"""<<NEWL>>    Load and return a template for one of the given names.<<NEWL>><<NEWL>>    Try names in order and return the first template found.<<NEWL>><<NEWL>>    Raise TemplateDoesNotExist if no such template exists.<<NEWL>>    """"""<<NEWL>>    if isinstance(template_name_list, str):<<NEWL>>        raise TypeError(<<NEWL>>            ""select_template() takes an iterable of template names but got a ""<<NEWL>>            ""string: %r. Use get_template() if you want to load a single ""<<NEWL>>            ""template by name."" % template_name_list<<NEWL>>        )<<NEWL>><<NEWL>>    chain = []<<NEWL>>    engines = _engine_list(using)<<NEWL>>    for template_name in template_name_list:<<NEWL>>        for engine in engines:<<NEWL>>            try:<<NEWL>>                return engine.get_template(template_name)<<NEWL>>            except TemplateDoesNotExist as e:<<NEWL>>                chain.append(e)<<NEWL>><<NEWL>>    if template_name_list:<<NEWL>>        raise TemplateDoesNotExist("", "".join(template_name_list), chain=chain)<<NEWL>>    else:<<NEWL>>        raise TemplateDoesNotExist(""No template names provided"")<<NEWL>><<NEWL>><<NEWL>>def render_to_string(template_name, context=None, request=None, using=None):<<NEWL>>    """"""<<NEWL>>    Load a template and render it with a context. Return a string.<<NEWL>><<NEWL>>    template_name may be a string or a list of strings.<<NEWL>>    """"""<<NEWL>>    if isinstance(template_name, (list, tuple)):<<NEWL>>        template = select_template(template_name, using=using)<<NEWL>>    else:<<NEWL>>        template = get_template(template_name, using=using)<<NEWL>>    return template.render(context, request)<<NEWL>><<NEWL>><<NEWL>>def _engine_list(using=None):<<NEWL>>    return engines.all() if using is None else [engines[using]]"
357	jackson	0	from typing import List<<NEWL>>from collections import deque<<NEWL>><<NEWL>>class Solution:<<NEWL>>    def maxAreaofIsland(self, grid: List[List[int]]) -> int:<<NEWL>>        row = len(grid)<<NEWL>>        col = len(grid[0])<<NEWL>>        biggest_island = 0<<NEWL>>        visited = [[False for i in range(col)] for j in range(row)]<<NEWL>>        for i in range(row):<<NEWL>>            for j in range(col):<<NEWL>>                if grid[i][j] == 1 and visited[i][j] is False:<<NEWL>>                    island_area = self.visitIsland(grid, visited, i,j)<<NEWL>>                    biggest_island = max(island_area,biggest_island)<<NEWL>>        return biggest_island<<NEWL>><<NEWL>>    def visitIsland(self, grid: List[List[int]], visited: List[List[int]], i: int, j: int) -> int:<<NEWL>>        neighbours = deque([(i,j)])<<NEWL>>        area = 0<<NEWL>>        while neighbours:<<NEWL>>            row,col = neighbours.popleft()<<NEWL>>            if row < 0 or row >= len(grid) or col < 0 or col >= len(grid[0]):<<NEWL>>                continue<<NEWL>>            if visited[row][col] is False and grid[row][col] == 1:<<NEWL>>                visited[row][col] = True<<NEWL>>                area += 1<<NEWL>>                neighbours.extend([(row + 1,col)])<<NEWL>>                neighbours.extend([(row - 1, col)])<<NEWL>>                neighbours.extend([(row, col + 1)])<<NEWL>>                neighbours.extend([(row, col - 1)])<<NEWL>>        return area<<NEWL>><<NEWL>>if __name__ == '__main__':<<NEWL>>    solution = Solution()<<NEWL>>    case1 = [[0,0,0,0,0,0,0,0]]<<NEWL>>    assert solution.maxAreaofIsland(case1) == 0<<NEWL>><<NEWL>>    case2 = [[0,0,1,0,0,0,0,1,0,0,0,0,0],[0,0,0,0,0,0,0,1,1,1,0,0,0],[0,1,1,0,1,0,0,0,0,0,0,0,0],[0,1,0,0,1,1,0,0,1,0,1,0,0],<<NEWL>>             [0,1,0,0,1,1,0,0,1,1,1,0,0],[0,0,0,0,0,0,0,0,0,0,1,0,0],[0,0,0,0,0,0,0,1,1,1,0,0,0],[0,0,0,0,0,0,0,1,1,0,0,0,0]]<<NEWL>>    assert solution.maxAreaofIsland(case2) == 6<<NEWL>><<NEWL>>    case3 = [[1, 1, 1, 0, 0], [0, 1, 0, 0, 1], [0, 0, 1, 1, 0], [0, 1, 1, 0, 0], [0, 0, 1, 0, 0]]<<NEWL>><<NEWL>>    assert solution.maxAreaofIsland(case3) == 5
357	donghui	0	from typing import List<<NEWL>>from collections import deque<<NEWL>><<NEWL>>class Solution:<<NEWL>>    def maxAreaofIsland(self, grid: List[List[int]]) -> int:<<NEWL>>        row = len(grid)<<NEWL>>        col = len(grid[0])<<NEWL>>        biggest_island = 0<<NEWL>>        visited = [[False for i in range(col)] for j in range(row)]<<NEWL>>        for i in range(row):<<NEWL>>            for j in range(col):<<NEWL>>                if grid[i][j] == 1 and visited[i][j] is False:<<NEWL>>                    island_area = self.visitIsland(grid, visited, i,j)<<NEWL>>                    biggest_island = max(island_area,biggest_island)<<NEWL>>        return biggest_island<<NEWL>><<NEWL>>    def visitIsland(self, grid: List[List[int]], visited: List[List[int]], i: int, j: int) -> int:<<NEWL>>        neighbours = deque([(i,j)])<<NEWL>>        area = 0<<NEWL>>        while neighbours:<<NEWL>>            row,col = neighbours.popleft()<<NEWL>>            if row < 0 or row >= len(grid) or col < 0 or col >= len(grid[0]):<<NEWL>>                continue<<NEWL>>            if visited[row][col] is False and grid[row][col] == 1:<<NEWL>>                visited[row][col] = True<<NEWL>>                area += 1<<NEWL>>                neighbours.extend([(row + 1,col)])<<NEWL>>                neighbours.extend([(row - 1, col)])<<NEWL>>                neighbours.extend([(row, col + 1)])<<NEWL>>                neighbours.extend([(row, col - 1)])<<NEWL>>        return area<<NEWL>><<NEWL>>if __name__ == '__main__':<<NEWL>>    solution = Solution()<<NEWL>>    case1 = [[0,0,0,0,0,0,0,0]]<<NEWL>>    assert solution.maxAreaofIsland(case1) == 0<<NEWL>><<NEWL>>    case2 = [[0,0,1,0,0,0,0,1,0,0,0,0,0],[0,0,0,0,0,0,0,1,1,1,0,0,0],[0,1,1,0,1,0,0,0,0,0,0,0,0],[0,1,0,0,1,1,0,0,1,0,1,0,0],<<NEWL>>             [0,1,0,0,1,1,0,0,1,1,1,0,0],[0,0,0,0,0,0,0,0,0,0,1,0,0],[0,0,0,0,0,0,0,1,1,1,0,0,0],[0,0,0,0,0,0,0,1,1,0,0,0,0]]<<NEWL>>    assert solution.maxAreaofIsland(case2) == 6<<NEWL>><<NEWL>>    case3 = [[1, 1, 1, 0, 0], [0, 1, 0, 0, 1], [0, 0, 1, 1, 0], [0, 1, 1, 0, 0], [0, 0, 1, 0, 0]]<<NEWL>><<NEWL>>    assert solution.maxAreaofIsland(case3) == 5
419	jackson	2	"import _plotly_utils.basevalidators<<NEWL>><<NEWL>><<NEWL>>class InsidetextfontValidator(_plotly_utils.basevalidators.CompoundValidator):<<NEWL>>    def __init__(self, plotly_name=""insidetextfont"", parent_name=""pie"", **kwargs):<<NEWL>>        super(InsidetextfontValidator, self).__init__(<<NEWL>>            plotly_name=plotly_name,<<NEWL>>            parent_name=parent_name,<<NEWL>>            data_class_str=kwargs.pop(""data_class_str"", ""Insidetextfont""),<<NEWL>>            data_docs=kwargs.pop(<<NEWL>>                ""data_docs"",<<NEWL>>                """"""<<NEWL>>            color<<NEWL>><<NEWL>>            colorsrc<<NEWL>>                Sets the source reference on Chart Studio Cloud<<NEWL>>                for `color`.<<NEWL>>            family<<NEWL>>                HTML font family - the typeface that will be<<NEWL>>                applied by the web browser. The web browser<<NEWL>>                will only be able to apply a font if it is<<NEWL>>                available on the system which it operates.<<NEWL>>                Provide multiple font families, separated by<<NEWL>>                commas, to indicate the preference in which to<<NEWL>>                apply fonts if they aren't available on the<<NEWL>>                system. The Chart Studio Cloud (at<<NEWL>>                https://chart-studio.plotly.com or on-premise)<<NEWL>>                generates images on a server, where only a<<NEWL>>                select number of fonts are installed and<<NEWL>>                supported. These include ""Arial"", ""Balto"",<<NEWL>>                ""Courier New"", ""Droid Sans"",, ""Droid Serif"",<<NEWL>>                ""Droid Sans Mono"", ""Gravitas One"", ""Old<<NEWL>>                Standard TT"", ""Open Sans"", ""Overpass"", ""PT Sans<<NEWL>>                Narrow"", ""Raleway"", ""Times New Roman"".<<NEWL>>            familysrc<<NEWL>>                Sets the source reference on Chart Studio Cloud<<NEWL>>                for `family`.<<NEWL>>            size<<NEWL>><<NEWL>>            sizesrc<<NEWL>>                Sets the source reference on Chart Studio Cloud<<NEWL>>                for `size`.<<NEWL>>"""""",<<NEWL>>            ),<<NEWL>>            **kwargs,<<NEWL>>        )"
419	donghui	1	"import _plotly_utils.basevalidators<<NEWL>><<NEWL>><<NEWL>>class InsidetextfontValidator(_plotly_utils.basevalidators.CompoundValidator):<<NEWL>>    def __init__(self, plotly_name=""insidetextfont"", parent_name=""pie"", **kwargs):<<NEWL>>        super(InsidetextfontValidator, self).__init__(<<NEWL>>            plotly_name=plotly_name,<<NEWL>>            parent_name=parent_name,<<NEWL>>            data_class_str=kwargs.pop(""data_class_str"", ""Insidetextfont""),<<NEWL>>            data_docs=kwargs.pop(<<NEWL>>                ""data_docs"",<<NEWL>>                """"""<<NEWL>>            color<<NEWL>><<NEWL>>            colorsrc<<NEWL>>                Sets the source reference on Chart Studio Cloud<<NEWL>>                for `color`.<<NEWL>>            family<<NEWL>>                HTML font family - the typeface that will be<<NEWL>>                applied by the web browser. The web browser<<NEWL>>                will only be able to apply a font if it is<<NEWL>>                available on the system which it operates.<<NEWL>>                Provide multiple font families, separated by<<NEWL>>                commas, to indicate the preference in which to<<NEWL>>                apply fonts if they aren't available on the<<NEWL>>                system. The Chart Studio Cloud (at<<NEWL>>                https://chart-studio.plotly.com or on-premise)<<NEWL>>                generates images on a server, where only a<<NEWL>>                select number of fonts are installed and<<NEWL>>                supported. These include ""Arial"", ""Balto"",<<NEWL>>                ""Courier New"", ""Droid Sans"",, ""Droid Serif"",<<NEWL>>                ""Droid Sans Mono"", ""Gravitas One"", ""Old<<NEWL>>                Standard TT"", ""Open Sans"", ""Overpass"", ""PT Sans<<NEWL>>                Narrow"", ""Raleway"", ""Times New Roman"".<<NEWL>>            familysrc<<NEWL>>                Sets the source reference on Chart Studio Cloud<<NEWL>>                for `family`.<<NEWL>>            size<<NEWL>><<NEWL>>            sizesrc<<NEWL>>                Sets the source reference on Chart Studio Cloud<<NEWL>>                for `size`.<<NEWL>>"""""",<<NEWL>>            ),<<NEWL>>            **kwargs,<<NEWL>>        )"
508	jackson	1	"#<<NEWL>># Copyright 2018 the original author or authors.<<NEWL>>#<<NEWL>># Licensed under the Apache License, Version 2.0 (the ""License"");<<NEWL>># you may not use this file except in compliance with the License.<<NEWL>># You may obtain a copy of the License at<<NEWL>>#<<NEWL>>#      http://www.apache.org/licenses/LICENSE-2.0<<NEWL>>#<<NEWL>># Unless required by applicable law or agreed to in writing, software<<NEWL>># distributed under the License is distributed on an ""AS IS"" BASIS,<<NEWL>># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.<<NEWL>># See the License for the specific language governing permissions and<<NEWL>># limitations under the License.<<NEWL>>#<<NEWL>>from google.protobuf.json_format import MessageToDict<<NEWL>>from google.protobuf.message import Message<<NEWL>>from simplejson import dumps<<NEWL>>from common.event_bus import EventBusClient<<NEWL>>from voltha.protos.omci_mib_db_pb2 import OpenOmciEvent<<NEWL>>from voltha.protos.omci_alarm_db_pb2 import AlarmOpenOmciEvent<<NEWL>>from common.utils.json_format import MessageToDict<<NEWL>><<NEWL>><<NEWL>>class OpenOmciEventBus(object):<<NEWL>>    """""" Event bus for publishing OpenOMCI related events. """"""<<NEWL>>    __slots__ = (<<NEWL>>        '_event_bus_client',  # The event bus client used to publish events.<<NEWL>>        '_topic'              # the topic to publish to<<NEWL>>    )<<NEWL>><<NEWL>>    def __init__(self):<<NEWL>>        self._event_bus_client = EventBusClient()<<NEWL>>        self._topic = 'openomci-events'<<NEWL>><<NEWL>>    def message_to_dict(m):<<NEWL>>        return MessageToDict(m, True, True, False)<<NEWL>><<NEWL>>    def advertise(self, event_type, data):<<NEWL>>        if isinstance(data, Message):<<NEWL>>            msg = dumps(MessageToDict(data, True, True))<<NEWL>>        elif isinstance(data, dict):<<NEWL>>            msg = dumps(data)<<NEWL>>        else:<<NEWL>>            msg = str(data)<<NEWL>><<NEWL>>        event_func = AlarmOpenOmciEvent if 'AlarmSynchronizer' in msg \<<NEWL>>                                  else OpenOmciEvent<<NEWL>>        event = event_func(<<NEWL>>                type=event_type,<<NEWL>>                data=msg<<NEWL>>        )<<NEWL>><<NEWL>>        self._event_bus_client.publish(self._topic, event)"
508	donghui	1	"#<<NEWL>># Copyright 2018 the original author or authors.<<NEWL>>#<<NEWL>># Licensed under the Apache License, Version 2.0 (the ""License"");<<NEWL>># you may not use this file except in compliance with the License.<<NEWL>># You may obtain a copy of the License at<<NEWL>>#<<NEWL>>#      http://www.apache.org/licenses/LICENSE-2.0<<NEWL>>#<<NEWL>># Unless required by applicable law or agreed to in writing, software<<NEWL>># distributed under the License is distributed on an ""AS IS"" BASIS,<<NEWL>># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.<<NEWL>># See the License for the specific language governing permissions and<<NEWL>># limitations under the License.<<NEWL>>#<<NEWL>>from google.protobuf.json_format import MessageToDict<<NEWL>>from google.protobuf.message import Message<<NEWL>>from simplejson import dumps<<NEWL>>from common.event_bus import EventBusClient<<NEWL>>from voltha.protos.omci_mib_db_pb2 import OpenOmciEvent<<NEWL>>from voltha.protos.omci_alarm_db_pb2 import AlarmOpenOmciEvent<<NEWL>>from common.utils.json_format import MessageToDict<<NEWL>><<NEWL>><<NEWL>>class OpenOmciEventBus(object):<<NEWL>>    """""" Event bus for publishing OpenOMCI related events. """"""<<NEWL>>    __slots__ = (<<NEWL>>        '_event_bus_client',  # The event bus client used to publish events.<<NEWL>>        '_topic'              # the topic to publish to<<NEWL>>    )<<NEWL>><<NEWL>>    def __init__(self):<<NEWL>>        self._event_bus_client = EventBusClient()<<NEWL>>        self._topic = 'openomci-events'<<NEWL>><<NEWL>>    def message_to_dict(m):<<NEWL>>        return MessageToDict(m, True, True, False)<<NEWL>><<NEWL>>    def advertise(self, event_type, data):<<NEWL>>        if isinstance(data, Message):<<NEWL>>            msg = dumps(MessageToDict(data, True, True))<<NEWL>>        elif isinstance(data, dict):<<NEWL>>            msg = dumps(data)<<NEWL>>        else:<<NEWL>>            msg = str(data)<<NEWL>><<NEWL>>        event_func = AlarmOpenOmciEvent if 'AlarmSynchronizer' in msg \<<NEWL>>                                  else OpenOmciEvent<<NEWL>>        event = event_func(<<NEWL>>                type=event_type,<<NEWL>>                data=msg<<NEWL>>        )<<NEWL>><<NEWL>>        self._event_bus_client.publish(self._topic, event)"
448	jackson	1	"# -*- coding: utf-8 -<<NEWL>>#<<NEWL>># This file is part of gunicorn released under the MIT license.<<NEWL>># See the NOTICE for more information.<<NEWL>><<NEWL>>import os<<NEWL>><<NEWL>>from gunicorn.errors import ConfigError<<NEWL>>from gunicorn.app.base import Application<<NEWL>>from gunicorn import util<<NEWL>><<NEWL>><<NEWL>>class WSGIApplication(Application):<<NEWL>>    def init(self, parser, opts, args):<<NEWL>>        self.app_uri = None<<NEWL>><<NEWL>>        if opts.paste:<<NEWL>>            from .pasterapp import has_logging_config<<NEWL>><<NEWL>>            config_uri = os.path.abspath(opts.paste)<<NEWL>>            config_file = config_uri.split('#')[0]<<NEWL>><<NEWL>>            if not os.path.exists(config_file):<<NEWL>>                raise ConfigError(""%r not found"" % config_file)<<NEWL>><<NEWL>>            self.cfg.set(""default_proc_name"", config_file)<<NEWL>>            self.app_uri = config_uri<<NEWL>><<NEWL>>            if has_logging_config(config_file):<<NEWL>>                self.cfg.set(""logconfig"", config_file)<<NEWL>><<NEWL>>            return<<NEWL>><<NEWL>>        if len(args) > 0:<<NEWL>>            self.cfg.set(""default_proc_name"", args[0])<<NEWL>>            self.app_uri = args[0]<<NEWL>><<NEWL>>    def load_config(self):<<NEWL>>        super().load_config()<<NEWL>><<NEWL>>        if self.app_uri is None:<<NEWL>>            if self.cfg.wsgi_app is not None:<<NEWL>>                self.app_uri = self.cfg.wsgi_app<<NEWL>>            else:<<NEWL>>                raise ConfigError(""No application module specified."")<<NEWL>><<NEWL>>    def load_wsgiapp(self):<<NEWL>>        return util.import_app(self.app_uri)<<NEWL>><<NEWL>>    def load_pasteapp(self):<<NEWL>>        from .pasterapp import get_wsgi_app<<NEWL>>        return get_wsgi_app(self.app_uri, defaults=self.cfg.paste_global_conf)<<NEWL>><<NEWL>>    def load(self):<<NEWL>>        if self.cfg.paste is not None:<<NEWL>>            return self.load_pasteapp()<<NEWL>>        else:<<NEWL>>            return self.load_wsgiapp()<<NEWL>><<NEWL>><<NEWL>>def run():<<NEWL>>    """"""\<<NEWL>>    The ``gunicorn`` command line runner for launching Gunicorn with<<NEWL>>    generic WSGI applications.<<NEWL>>    """"""<<NEWL>>    from gunicorn.app.wsgiapp import WSGIApplication<<NEWL>>    WSGIApplication(""%(prog)s [OPTIONS] [APP_MODULE]"").run()<<NEWL>><<NEWL>><<NEWL>>if __name__ == '__main__':<<NEWL>>    run()"
448	donghui	1	"# -*- coding: utf-8 -<<NEWL>>#<<NEWL>># This file is part of gunicorn released under the MIT license.<<NEWL>># See the NOTICE for more information.<<NEWL>><<NEWL>>import os<<NEWL>><<NEWL>>from gunicorn.errors import ConfigError<<NEWL>>from gunicorn.app.base import Application<<NEWL>>from gunicorn import util<<NEWL>><<NEWL>><<NEWL>>class WSGIApplication(Application):<<NEWL>>    def init(self, parser, opts, args):<<NEWL>>        self.app_uri = None<<NEWL>><<NEWL>>        if opts.paste:<<NEWL>>            from .pasterapp import has_logging_config<<NEWL>><<NEWL>>            config_uri = os.path.abspath(opts.paste)<<NEWL>>            config_file = config_uri.split('#')[0]<<NEWL>><<NEWL>>            if not os.path.exists(config_file):<<NEWL>>                raise ConfigError(""%r not found"" % config_file)<<NEWL>><<NEWL>>            self.cfg.set(""default_proc_name"", config_file)<<NEWL>>            self.app_uri = config_uri<<NEWL>><<NEWL>>            if has_logging_config(config_file):<<NEWL>>                self.cfg.set(""logconfig"", config_file)<<NEWL>><<NEWL>>            return<<NEWL>><<NEWL>>        if len(args) > 0:<<NEWL>>            self.cfg.set(""default_proc_name"", args[0])<<NEWL>>            self.app_uri = args[0]<<NEWL>><<NEWL>>    def load_config(self):<<NEWL>>        super().load_config()<<NEWL>><<NEWL>>        if self.app_uri is None:<<NEWL>>            if self.cfg.wsgi_app is not None:<<NEWL>>                self.app_uri = self.cfg.wsgi_app<<NEWL>>            else:<<NEWL>>                raise ConfigError(""No application module specified."")<<NEWL>><<NEWL>>    def load_wsgiapp(self):<<NEWL>>        return util.import_app(self.app_uri)<<NEWL>><<NEWL>>    def load_pasteapp(self):<<NEWL>>        from .pasterapp import get_wsgi_app<<NEWL>>        return get_wsgi_app(self.app_uri, defaults=self.cfg.paste_global_conf)<<NEWL>><<NEWL>>    def load(self):<<NEWL>>        if self.cfg.paste is not None:<<NEWL>>            return self.load_pasteapp()<<NEWL>>        else:<<NEWL>>            return self.load_wsgiapp()<<NEWL>><<NEWL>><<NEWL>>def run():<<NEWL>>    """"""\<<NEWL>>    The ``gunicorn`` command line runner for launching Gunicorn with<<NEWL>>    generic WSGI applications.<<NEWL>>    """"""<<NEWL>>    from gunicorn.app.wsgiapp import WSGIApplication<<NEWL>>    WSGIApplication(""%(prog)s [OPTIONS] [APP_MODULE]"").run()<<NEWL>><<NEWL>><<NEWL>>if __name__ == '__main__':<<NEWL>>    run()"
458	jackson	1	"# This file is part of Scapy<<NEWL>># See http://www.secdev.org/projects/scapy for more information<<NEWL>># Copyright (C) Philippe Biondi <phil@secdev.org><<NEWL>># This program is published under a GPLv2 license<<NEWL>><<NEWL>>""""""<<NEWL>>External link to programs<<NEWL>>""""""<<NEWL>><<NEWL>>import os<<NEWL>>import subprocess<<NEWL>>from scapy.error import log_loading<<NEWL>><<NEWL>># Notice: this file must not be called before main.py, if started<<NEWL>># in interactive mode, because it needs to be called after the<<NEWL>># logger has been setup, to be able to print the warning messages<<NEWL>><<NEWL>># MATPLOTLIB<<NEWL>><<NEWL>>try:<<NEWL>>    from matplotlib import get_backend as matplotlib_get_backend<<NEWL>>    from matplotlib import pyplot as plt<<NEWL>>    from matplotlib.lines import Line2D<<NEWL>>    MATPLOTLIB = 1<<NEWL>>    if ""inline"" in matplotlib_get_backend():<<NEWL>>        MATPLOTLIB_INLINED = 1<<NEWL>>    else:<<NEWL>>        MATPLOTLIB_INLINED = 0<<NEWL>>    MATPLOTLIB_DEFAULT_PLOT_KARGS = {""marker"": ""+""}<<NEWL>># RuntimeError to catch gtk ""Cannot open display"" error<<NEWL>>except (ImportError, RuntimeError):<<NEWL>>    plt = None<<NEWL>>    Line2D = None<<NEWL>>    MATPLOTLIB = 0<<NEWL>>    MATPLOTLIB_INLINED = 0<<NEWL>>    MATPLOTLIB_DEFAULT_PLOT_KARGS = dict()<<NEWL>>    log_loading.info(""Can't import matplotlib. Won't be able to plot."")<<NEWL>><<NEWL>># PYX<<NEWL>><<NEWL>><<NEWL>>def _test_pyx():<<NEWL>>    # type: () -> bool<<NEWL>>    """"""Returns if PyX is correctly installed or not""""""<<NEWL>>    try:<<NEWL>>        with open(os.devnull, 'wb') as devnull:<<NEWL>>            r = subprocess.check_call([""pdflatex"", ""--version""],<<NEWL>>                                      stdout=devnull, stderr=subprocess.STDOUT)<<NEWL>>    except (subprocess.CalledProcessError, OSError):<<NEWL>>        return False<<NEWL>>    else:<<NEWL>>        return r == 0<<NEWL>><<NEWL>><<NEWL>>try:<<NEWL>>    import pyx  # noqa: F401<<NEWL>>    if _test_pyx():<<NEWL>>        PYX = 1<<NEWL>>    else:<<NEWL>>        log_loading.info(""PyX dependencies are not installed ! Please install TexLive or MikTeX."")  # noqa: E501<<NEWL>>        PYX = 0<<NEWL>>except ImportError:<<NEWL>>    log_loading.info(""Can't import PyX. Won't be able to use psdump() or pdfdump()."")  # noqa: E501<<NEWL>>    PYX = 0"
458	donghui	2	"# This file is part of Scapy<<NEWL>># See http://www.secdev.org/projects/scapy for more information<<NEWL>># Copyright (C) Philippe Biondi <phil@secdev.org><<NEWL>># This program is published under a GPLv2 license<<NEWL>><<NEWL>>""""""<<NEWL>>External link to programs<<NEWL>>""""""<<NEWL>><<NEWL>>import os<<NEWL>>import subprocess<<NEWL>>from scapy.error import log_loading<<NEWL>><<NEWL>># Notice: this file must not be called before main.py, if started<<NEWL>># in interactive mode, because it needs to be called after the<<NEWL>># logger has been setup, to be able to print the warning messages<<NEWL>><<NEWL>># MATPLOTLIB<<NEWL>><<NEWL>>try:<<NEWL>>    from matplotlib import get_backend as matplotlib_get_backend<<NEWL>>    from matplotlib import pyplot as plt<<NEWL>>    from matplotlib.lines import Line2D<<NEWL>>    MATPLOTLIB = 1<<NEWL>>    if ""inline"" in matplotlib_get_backend():<<NEWL>>        MATPLOTLIB_INLINED = 1<<NEWL>>    else:<<NEWL>>        MATPLOTLIB_INLINED = 0<<NEWL>>    MATPLOTLIB_DEFAULT_PLOT_KARGS = {""marker"": ""+""}<<NEWL>># RuntimeError to catch gtk ""Cannot open display"" error<<NEWL>>except (ImportError, RuntimeError):<<NEWL>>    plt = None<<NEWL>>    Line2D = None<<NEWL>>    MATPLOTLIB = 0<<NEWL>>    MATPLOTLIB_INLINED = 0<<NEWL>>    MATPLOTLIB_DEFAULT_PLOT_KARGS = dict()<<NEWL>>    log_loading.info(""Can't import matplotlib. Won't be able to plot."")<<NEWL>><<NEWL>># PYX<<NEWL>><<NEWL>><<NEWL>>def _test_pyx():<<NEWL>>    # type: () -> bool<<NEWL>>    """"""Returns if PyX is correctly installed or not""""""<<NEWL>>    try:<<NEWL>>        with open(os.devnull, 'wb') as devnull:<<NEWL>>            r = subprocess.check_call([""pdflatex"", ""--version""],<<NEWL>>                                      stdout=devnull, stderr=subprocess.STDOUT)<<NEWL>>    except (subprocess.CalledProcessError, OSError):<<NEWL>>        return False<<NEWL>>    else:<<NEWL>>        return r == 0<<NEWL>><<NEWL>><<NEWL>>try:<<NEWL>>    import pyx  # noqa: F401<<NEWL>>    if _test_pyx():<<NEWL>>        PYX = 1<<NEWL>>    else:<<NEWL>>        log_loading.info(""PyX dependencies are not installed ! Please install TexLive or MikTeX."")  # noqa: E501<<NEWL>>        PYX = 0<<NEWL>>except ImportError:<<NEWL>>    log_loading.info(""Can't import PyX. Won't be able to use psdump() or pdfdump()."")  # noqa: E501<<NEWL>>    PYX = 0"
409	jackson	2	"import os<<NEWL>>import json<<NEWL>><<NEWL>>import torch<<NEWL>>from PIL import Image<<NEWL>>from torchvision import transforms<<NEWL>>import matplotlib.pyplot as plt<<NEWL>><<NEWL>>from vit_model import vit_base_patch16_224_in21k as create_model<<NEWL>><<NEWL>><<NEWL>>def main():<<NEWL>>    device = torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu"")<<NEWL>><<NEWL>>    data_transform = transforms.Compose(<<NEWL>>        [transforms.Resize(256),<<NEWL>>         transforms.CenterCrop(224),<<NEWL>>         transforms.ToTensor(),<<NEWL>>         transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])<<NEWL>><<NEWL>>    # load image<<NEWL>>    img_path = ""../tulip.jpg""<<NEWL>>    assert os.path.exists(img_path), ""file: '{}' dose not exist."".format(img_path)<<NEWL>>    img = Image.open(img_path)<<NEWL>>    plt.imshow(img)<<NEWL>>    # [N, C, H, W]<<NEWL>>    img = data_transform(img)<<NEWL>>    # expand batch dimension<<NEWL>>    img = torch.unsqueeze(img, dim=0)<<NEWL>><<NEWL>>    # read class_indict<<NEWL>>    json_path = './class_indices.json'<<NEWL>>    assert os.path.exists(json_path), ""file: '{}' dose not exist."".format(json_path)<<NEWL>><<NEWL>>    with open(json_path, ""r"") as f:<<NEWL>>        class_indict = json.load(f)<<NEWL>><<NEWL>>    # create model<<NEWL>>    model = create_model(num_classes=5, has_logits=False).to(device)<<NEWL>>    # load model weights<<NEWL>>    model_weight_path = ""./weights/model-9.pth""<<NEWL>>    model.load_state_dict(torch.load(model_weight_path, map_location=device))<<NEWL>>    model.eval()<<NEWL>>    with torch.no_grad():<<NEWL>>        # predict class<<NEWL>>        output = torch.squeeze(model(img.to(device))).cpu()<<NEWL>>        predict = torch.softmax(output, dim=0)<<NEWL>>        predict_cla = torch.argmax(predict).numpy()<<NEWL>><<NEWL>>    print_res = ""class: {}   prob: {:.3}"".format(class_indict[str(predict_cla)],<<NEWL>>                                                 predict[predict_cla].numpy())<<NEWL>>    plt.title(print_res)<<NEWL>>    for i in range(len(predict)):<<NEWL>>        print(""class: {:10}   prob: {:.3}"".format(class_indict[str(i)],<<NEWL>>                                                  predict[i].numpy()))<<NEWL>>    plt.show()<<NEWL>><<NEWL>><<NEWL>>if __name__ == '__main__':<<NEWL>>    main()"
409	donghui	2	"import os<<NEWL>>import json<<NEWL>><<NEWL>>import torch<<NEWL>>from PIL import Image<<NEWL>>from torchvision import transforms<<NEWL>>import matplotlib.pyplot as plt<<NEWL>><<NEWL>>from vit_model import vit_base_patch16_224_in21k as create_model<<NEWL>><<NEWL>><<NEWL>>def main():<<NEWL>>    device = torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu"")<<NEWL>><<NEWL>>    data_transform = transforms.Compose(<<NEWL>>        [transforms.Resize(256),<<NEWL>>         transforms.CenterCrop(224),<<NEWL>>         transforms.ToTensor(),<<NEWL>>         transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])<<NEWL>><<NEWL>>    # load image<<NEWL>>    img_path = ""../tulip.jpg""<<NEWL>>    assert os.path.exists(img_path), ""file: '{}' dose not exist."".format(img_path)<<NEWL>>    img = Image.open(img_path)<<NEWL>>    plt.imshow(img)<<NEWL>>    # [N, C, H, W]<<NEWL>>    img = data_transform(img)<<NEWL>>    # expand batch dimension<<NEWL>>    img = torch.unsqueeze(img, dim=0)<<NEWL>><<NEWL>>    # read class_indict<<NEWL>>    json_path = './class_indices.json'<<NEWL>>    assert os.path.exists(json_path), ""file: '{}' dose not exist."".format(json_path)<<NEWL>><<NEWL>>    with open(json_path, ""r"") as f:<<NEWL>>        class_indict = json.load(f)<<NEWL>><<NEWL>>    # create model<<NEWL>>    model = create_model(num_classes=5, has_logits=False).to(device)<<NEWL>>    # load model weights<<NEWL>>    model_weight_path = ""./weights/model-9.pth""<<NEWL>>    model.load_state_dict(torch.load(model_weight_path, map_location=device))<<NEWL>>    model.eval()<<NEWL>>    with torch.no_grad():<<NEWL>>        # predict class<<NEWL>>        output = torch.squeeze(model(img.to(device))).cpu()<<NEWL>>        predict = torch.softmax(output, dim=0)<<NEWL>>        predict_cla = torch.argmax(predict).numpy()<<NEWL>><<NEWL>>    print_res = ""class: {}   prob: {:.3}"".format(class_indict[str(predict_cla)],<<NEWL>>                                                 predict[predict_cla].numpy())<<NEWL>>    plt.title(print_res)<<NEWL>>    for i in range(len(predict)):<<NEWL>>        print(""class: {:10}   prob: {:.3}"".format(class_indict[str(i)],<<NEWL>>                                                  predict[i].numpy()))<<NEWL>>    plt.show()<<NEWL>><<NEWL>><<NEWL>>if __name__ == '__main__':<<NEWL>>    main()"
347	jackson	0	"from functools import partial<<NEWL>><<NEWL>>import pytest<<NEWL>><<NEWL>>from ..argument import Argument, to_arguments<<NEWL>>from ..field import Field<<NEWL>>from ..inputfield import InputField<<NEWL>>from ..scalars import String<<NEWL>>from ..structures import NonNull<<NEWL>><<NEWL>><<NEWL>>def test_argument():<<NEWL>>    arg = Argument(String, default_value=""a"", description=""desc"", name=""b"")<<NEWL>>    assert arg.type == String<<NEWL>>    assert arg.default_value == ""a""<<NEWL>>    assert arg.description == ""desc""<<NEWL>>    assert arg.name == ""b""<<NEWL>><<NEWL>><<NEWL>>def test_argument_comparasion():<<NEWL>>    arg1 = Argument(String, name=""Hey"", description=""Desc"", default_value=""default"")<<NEWL>>    arg2 = Argument(String, name=""Hey"", description=""Desc"", default_value=""default"")<<NEWL>><<NEWL>>    assert arg1 == arg2<<NEWL>>    assert arg1 != String()<<NEWL>><<NEWL>><<NEWL>>def test_argument_required():<<NEWL>>    arg = Argument(String, required=True)<<NEWL>>    assert arg.type == NonNull(String)<<NEWL>><<NEWL>><<NEWL>>def test_to_arguments():<<NEWL>>    args = {""arg_string"": Argument(String), ""unmounted_arg"": String(required=True)}<<NEWL>><<NEWL>>    my_args = to_arguments(args)<<NEWL>>    assert my_args == {<<NEWL>>        ""arg_string"": Argument(String),<<NEWL>>        ""unmounted_arg"": Argument(String, required=True),<<NEWL>>    }<<NEWL>><<NEWL>><<NEWL>>def test_to_arguments_raises_if_field():<<NEWL>>    args = {""arg_string"": Field(String)}<<NEWL>><<NEWL>>    with pytest.raises(ValueError) as exc_info:<<NEWL>>        to_arguments(args)<<NEWL>><<NEWL>>    assert str(exc_info.value) == (<<NEWL>>        ""Expected arg_string to be Argument, but received Field. Try using ""<<NEWL>>        ""Argument(String).""<<NEWL>>    )<<NEWL>><<NEWL>><<NEWL>>def test_to_arguments_raises_if_inputfield():<<NEWL>>    args = {""arg_string"": InputField(String)}<<NEWL>><<NEWL>>    with pytest.raises(ValueError) as exc_info:<<NEWL>>        to_arguments(args)<<NEWL>><<NEWL>>    assert str(exc_info.value) == (<<NEWL>>        ""Expected arg_string to be Argument, but received InputField. Try ""<<NEWL>>        ""using Argument(String).""<<NEWL>>    )<<NEWL>><<NEWL>><<NEWL>>def test_argument_with_lazy_type():<<NEWL>>    MyType = object()<<NEWL>>    arg = Argument(lambda: MyType)<<NEWL>>    assert arg.type == MyType<<NEWL>><<NEWL>><<NEWL>>def test_argument_with_lazy_partial_type():<<NEWL>>    MyType = object()<<NEWL>>    arg = Argument(partial(lambda: MyType))<<NEWL>>    assert arg.type == MyType"
347	donghui	0	"from functools import partial<<NEWL>><<NEWL>>import pytest<<NEWL>><<NEWL>>from ..argument import Argument, to_arguments<<NEWL>>from ..field import Field<<NEWL>>from ..inputfield import InputField<<NEWL>>from ..scalars import String<<NEWL>>from ..structures import NonNull<<NEWL>><<NEWL>><<NEWL>>def test_argument():<<NEWL>>    arg = Argument(String, default_value=""a"", description=""desc"", name=""b"")<<NEWL>>    assert arg.type == String<<NEWL>>    assert arg.default_value == ""a""<<NEWL>>    assert arg.description == ""desc""<<NEWL>>    assert arg.name == ""b""<<NEWL>><<NEWL>><<NEWL>>def test_argument_comparasion():<<NEWL>>    arg1 = Argument(String, name=""Hey"", description=""Desc"", default_value=""default"")<<NEWL>>    arg2 = Argument(String, name=""Hey"", description=""Desc"", default_value=""default"")<<NEWL>><<NEWL>>    assert arg1 == arg2<<NEWL>>    assert arg1 != String()<<NEWL>><<NEWL>><<NEWL>>def test_argument_required():<<NEWL>>    arg = Argument(String, required=True)<<NEWL>>    assert arg.type == NonNull(String)<<NEWL>><<NEWL>><<NEWL>>def test_to_arguments():<<NEWL>>    args = {""arg_string"": Argument(String), ""unmounted_arg"": String(required=True)}<<NEWL>><<NEWL>>    my_args = to_arguments(args)<<NEWL>>    assert my_args == {<<NEWL>>        ""arg_string"": Argument(String),<<NEWL>>        ""unmounted_arg"": Argument(String, required=True),<<NEWL>>    }<<NEWL>><<NEWL>><<NEWL>>def test_to_arguments_raises_if_field():<<NEWL>>    args = {""arg_string"": Field(String)}<<NEWL>><<NEWL>>    with pytest.raises(ValueError) as exc_info:<<NEWL>>        to_arguments(args)<<NEWL>><<NEWL>>    assert str(exc_info.value) == (<<NEWL>>        ""Expected arg_string to be Argument, but received Field. Try using ""<<NEWL>>        ""Argument(String).""<<NEWL>>    )<<NEWL>><<NEWL>><<NEWL>>def test_to_arguments_raises_if_inputfield():<<NEWL>>    args = {""arg_string"": InputField(String)}<<NEWL>><<NEWL>>    with pytest.raises(ValueError) as exc_info:<<NEWL>>        to_arguments(args)<<NEWL>><<NEWL>>    assert str(exc_info.value) == (<<NEWL>>        ""Expected arg_string to be Argument, but received InputField. Try ""<<NEWL>>        ""using Argument(String).""<<NEWL>>    )<<NEWL>><<NEWL>><<NEWL>>def test_argument_with_lazy_type():<<NEWL>>    MyType = object()<<NEWL>>    arg = Argument(lambda: MyType)<<NEWL>>    assert arg.type == MyType<<NEWL>><<NEWL>><<NEWL>>def test_argument_with_lazy_partial_type():<<NEWL>>    MyType = object()<<NEWL>>    arg = Argument(partial(lambda: MyType))<<NEWL>>    assert arg.type == MyType"
316	jackson	3	import numpy as np<<NEWL>>import numba<<NEWL>><<NEWL>># - plotStratigraphy takes 1) XorY_StratiOverTime (time and either x or y dimensions): strati__elevation selected for only the basin area and either averaged or selected for one across (y)/down(x) basin distance <<NEWL>>#     2) XorY_GrainSizeOverTime (time and either x or y dimensions) the grain size or erosion rate or other desired variable that will be used to fill the stratigraphy. This also needs to be selected or averaged for one x/y distance. <<NEWL>># - stratigraphy as it is written assumes that channels are draining either in the x or y direction (mountain along one axis) and stratigraphy is generated along one axis.     <<NEWL>># - plotStratigraphy averages the nearby nodes (grain size or erosion rate or other desired value passed) to fill a given cell of stratigraphy.<<NEWL>># -plotStraigraphy2 does not average the nearest nodes and takes the first closest value to fill the stratigraphy. <<NEWL>><<NEWL>>@numba.njit<<NEWL>>def plotStratigraphy(XorY_StratiOverTime,XorY_GrainSizeOverTime):<<NEWL>>    i=0<<NEWL>>    j=0<<NEWL>>    C=np.zeros(((XorY_StratiOverTime.shape[0]),(XorY_StratiOverTime.shape[1])))<<NEWL>>    for i in range(0,(XorY_StratiOverTime.shape[1])):<<NEWL>>        for j in range(0,(XorY_StratiOverTime.shape[0])):<<NEWL>>            tryff=np.array([XorY_GrainSizeOverTime[j,i],XorY_GrainSizeOverTime[j,i+1],XorY_GrainSizeOverTime[j+1,i],XorY_GrainSizeOverTime[j+1,i+1]])<<NEWL>>            C[j,i]=np.nanmean(tryff)<<NEWL>>    return C<<NEWL>><<NEWL>>@numba.njit<<NEWL>>def plotStratigraphy2(XorY_StratiOverTime,XorY_GrainSizeOverTime):<<NEWL>>    i=0<<NEWL>>    j=0<<NEWL>>    C=np.zeros(((XorY_StratiOverTime.shape[0]),(XorY_StratiOverTime.shape[1])))<<NEWL>>    for i in range(0,(XorY_StratiOverTime.shape[1])):<<NEWL>>        for j in range(0,(XorY_StratiOverTime.shape[0])):<<NEWL>>            #tryff=np.array([XorY_GrainSizeOverTime[j,i],XorY_GrainSizeOverTime[j,i+1],XorY_GrainSizeOverTime[j+1,i],XorY_GrainSizeOverTime[j+1,i+1]])<<NEWL>>            C[j,i]=(XorY_GrainSizeOverTime[j,i])<<NEWL>>    return C
316	donghui	1	import numpy as np<<NEWL>>import numba<<NEWL>><<NEWL>># - plotStratigraphy takes 1) XorY_StratiOverTime (time and either x or y dimensions): strati__elevation selected for only the basin area and either averaged or selected for one across (y)/down(x) basin distance <<NEWL>>#     2) XorY_GrainSizeOverTime (time and either x or y dimensions) the grain size or erosion rate or other desired variable that will be used to fill the stratigraphy. This also needs to be selected or averaged for one x/y distance. <<NEWL>># - stratigraphy as it is written assumes that channels are draining either in the x or y direction (mountain along one axis) and stratigraphy is generated along one axis.     <<NEWL>># - plotStratigraphy averages the nearby nodes (grain size or erosion rate or other desired value passed) to fill a given cell of stratigraphy.<<NEWL>># -plotStraigraphy2 does not average the nearest nodes and takes the first closest value to fill the stratigraphy. <<NEWL>><<NEWL>>@numba.njit<<NEWL>>def plotStratigraphy(XorY_StratiOverTime,XorY_GrainSizeOverTime):<<NEWL>>    i=0<<NEWL>>    j=0<<NEWL>>    C=np.zeros(((XorY_StratiOverTime.shape[0]),(XorY_StratiOverTime.shape[1])))<<NEWL>>    for i in range(0,(XorY_StratiOverTime.shape[1])):<<NEWL>>        for j in range(0,(XorY_StratiOverTime.shape[0])):<<NEWL>>            tryff=np.array([XorY_GrainSizeOverTime[j,i],XorY_GrainSizeOverTime[j,i+1],XorY_GrainSizeOverTime[j+1,i],XorY_GrainSizeOverTime[j+1,i+1]])<<NEWL>>            C[j,i]=np.nanmean(tryff)<<NEWL>>    return C<<NEWL>><<NEWL>>@numba.njit<<NEWL>>def plotStratigraphy2(XorY_StratiOverTime,XorY_GrainSizeOverTime):<<NEWL>>    i=0<<NEWL>>    j=0<<NEWL>>    C=np.zeros(((XorY_StratiOverTime.shape[0]),(XorY_StratiOverTime.shape[1])))<<NEWL>>    for i in range(0,(XorY_StratiOverTime.shape[1])):<<NEWL>>        for j in range(0,(XorY_StratiOverTime.shape[0])):<<NEWL>>            #tryff=np.array([XorY_GrainSizeOverTime[j,i],XorY_GrainSizeOverTime[j,i+1],XorY_GrainSizeOverTime[j+1,i],XorY_GrainSizeOverTime[j+1,i+1]])<<NEWL>>            C[j,i]=(XorY_GrainSizeOverTime[j,i])<<NEWL>>    return C
256	jackson	1	"import win32api, win32security<<NEWL>>import win32con, ntsecuritycon, winnt<<NEWL>>import os<<NEWL>><<NEWL>>temp_dir = win32api.GetTempPath()<<NEWL>>fname = win32api.GetTempFileName(temp_dir, ""rsk"")[0]<<NEWL>>print(fname)<<NEWL>>## file can't exist<<NEWL>>os.remove(fname)<<NEWL>><<NEWL>>## enable backup and restore privs<<NEWL>>required_privs = (<<NEWL>>    (<<NEWL>>        win32security.LookupPrivilegeValue("""", ntsecuritycon.SE_BACKUP_NAME),<<NEWL>>        win32con.SE_PRIVILEGE_ENABLED,<<NEWL>>    ),<<NEWL>>    (<<NEWL>>        win32security.LookupPrivilegeValue("""", ntsecuritycon.SE_RESTORE_NAME),<<NEWL>>        win32con.SE_PRIVILEGE_ENABLED,<<NEWL>>    ),<<NEWL>>)<<NEWL>>ph = win32api.GetCurrentProcess()<<NEWL>>th = win32security.OpenProcessToken(<<NEWL>>    ph, win32con.TOKEN_READ | win32con.TOKEN_ADJUST_PRIVILEGES<<NEWL>>)<<NEWL>>adjusted_privs = win32security.AdjustTokenPrivileges(th, 0, required_privs)<<NEWL>><<NEWL>>try:<<NEWL>>    sa = win32security.SECURITY_ATTRIBUTES()<<NEWL>>    my_sid = win32security.GetTokenInformation(th, ntsecuritycon.TokenUser)[0]<<NEWL>>    sa.SECURITY_DESCRIPTOR.SetSecurityDescriptorOwner(my_sid, 0)<<NEWL>><<NEWL>>    k, disp = win32api.RegCreateKeyEx(<<NEWL>>        win32con.HKEY_CURRENT_USER,<<NEWL>>        ""Python test key"",<<NEWL>>        SecurityAttributes=sa,<<NEWL>>        samDesired=win32con.KEY_ALL_ACCESS,<<NEWL>>        Class=""some class"",<<NEWL>>        Options=0,<<NEWL>>    )<<NEWL>>    win32api.RegSetValue(k, None, win32con.REG_SZ, ""Default value for python test key"")<<NEWL>><<NEWL>>    subk, disp = win32api.RegCreateKeyEx(<<NEWL>>        k,<<NEWL>>        ""python test subkey"",<<NEWL>>        SecurityAttributes=sa,<<NEWL>>        samDesired=win32con.KEY_ALL_ACCESS,<<NEWL>>        Class=""some other class"",<<NEWL>>        Options=0,<<NEWL>>    )<<NEWL>>    win32api.RegSetValue(subk, None, win32con.REG_SZ, ""Default value for subkey"")<<NEWL>><<NEWL>>    win32api.RegSaveKeyEx(<<NEWL>>        k, fname, Flags=winnt.REG_STANDARD_FORMAT, SecurityAttributes=sa<<NEWL>>    )<<NEWL>><<NEWL>>    restored_key, disp = win32api.RegCreateKeyEx(<<NEWL>>        win32con.HKEY_CURRENT_USER,<<NEWL>>        ""Python test key(restored)"",<<NEWL>>        SecurityAttributes=sa,<<NEWL>>        samDesired=win32con.KEY_ALL_ACCESS,<<NEWL>>        Class=""restored class"",<<NEWL>>        Options=0,<<NEWL>>    )<<NEWL>>    win32api.RegRestoreKey(restored_key, fname)<<NEWL>>finally:<<NEWL>>    win32security.AdjustTokenPrivileges(th, 0, adjusted_privs)"
256	donghui	1	"import win32api, win32security<<NEWL>>import win32con, ntsecuritycon, winnt<<NEWL>>import os<<NEWL>><<NEWL>>temp_dir = win32api.GetTempPath()<<NEWL>>fname = win32api.GetTempFileName(temp_dir, ""rsk"")[0]<<NEWL>>print(fname)<<NEWL>>## file can't exist<<NEWL>>os.remove(fname)<<NEWL>><<NEWL>>## enable backup and restore privs<<NEWL>>required_privs = (<<NEWL>>    (<<NEWL>>        win32security.LookupPrivilegeValue("""", ntsecuritycon.SE_BACKUP_NAME),<<NEWL>>        win32con.SE_PRIVILEGE_ENABLED,<<NEWL>>    ),<<NEWL>>    (<<NEWL>>        win32security.LookupPrivilegeValue("""", ntsecuritycon.SE_RESTORE_NAME),<<NEWL>>        win32con.SE_PRIVILEGE_ENABLED,<<NEWL>>    ),<<NEWL>>)<<NEWL>>ph = win32api.GetCurrentProcess()<<NEWL>>th = win32security.OpenProcessToken(<<NEWL>>    ph, win32con.TOKEN_READ | win32con.TOKEN_ADJUST_PRIVILEGES<<NEWL>>)<<NEWL>>adjusted_privs = win32security.AdjustTokenPrivileges(th, 0, required_privs)<<NEWL>><<NEWL>>try:<<NEWL>>    sa = win32security.SECURITY_ATTRIBUTES()<<NEWL>>    my_sid = win32security.GetTokenInformation(th, ntsecuritycon.TokenUser)[0]<<NEWL>>    sa.SECURITY_DESCRIPTOR.SetSecurityDescriptorOwner(my_sid, 0)<<NEWL>><<NEWL>>    k, disp = win32api.RegCreateKeyEx(<<NEWL>>        win32con.HKEY_CURRENT_USER,<<NEWL>>        ""Python test key"",<<NEWL>>        SecurityAttributes=sa,<<NEWL>>        samDesired=win32con.KEY_ALL_ACCESS,<<NEWL>>        Class=""some class"",<<NEWL>>        Options=0,<<NEWL>>    )<<NEWL>>    win32api.RegSetValue(k, None, win32con.REG_SZ, ""Default value for python test key"")<<NEWL>><<NEWL>>    subk, disp = win32api.RegCreateKeyEx(<<NEWL>>        k,<<NEWL>>        ""python test subkey"",<<NEWL>>        SecurityAttributes=sa,<<NEWL>>        samDesired=win32con.KEY_ALL_ACCESS,<<NEWL>>        Class=""some other class"",<<NEWL>>        Options=0,<<NEWL>>    )<<NEWL>>    win32api.RegSetValue(subk, None, win32con.REG_SZ, ""Default value for subkey"")<<NEWL>><<NEWL>>    win32api.RegSaveKeyEx(<<NEWL>>        k, fname, Flags=winnt.REG_STANDARD_FORMAT, SecurityAttributes=sa<<NEWL>>    )<<NEWL>><<NEWL>>    restored_key, disp = win32api.RegCreateKeyEx(<<NEWL>>        win32con.HKEY_CURRENT_USER,<<NEWL>>        ""Python test key(restored)"",<<NEWL>>        SecurityAttributes=sa,<<NEWL>>        samDesired=win32con.KEY_ALL_ACCESS,<<NEWL>>        Class=""restored class"",<<NEWL>>        Options=0,<<NEWL>>    )<<NEWL>>    win32api.RegRestoreKey(restored_key, fname)<<NEWL>>finally:<<NEWL>>    win32security.AdjustTokenPrivileges(th, 0, adjusted_privs)"
262	jackson	1	"import hashlib<<NEWL>>import hmac<<NEWL>>import re<<NEWL>>import time<<NEWL>>from binascii import a2b_hex<<NEWL>><<NEWL>><<NEWL>>AUTH_TOKEN_NAME = ""__cld_token__""<<NEWL>>AUTH_TOKEN_SEPARATOR = ""~""<<NEWL>>AUTH_TOKEN_UNSAFE_RE = r'([ ""#%&\'\/:;<=>?@\[\\\]^`{\|}~]+)'<<NEWL>><<NEWL>><<NEWL>>def generate(url=None, acl=None, start_time=None, duration=None,<<NEWL>>             expiration=None, ip=None, key=None, token_name=AUTH_TOKEN_NAME):<<NEWL>><<NEWL>>    if expiration is None:<<NEWL>>        if duration is not None:<<NEWL>>            start = start_time if start_time is not None else int(time.time())<<NEWL>>            expiration = start + duration<<NEWL>>        else:<<NEWL>>            raise Exception(""Must provide either expiration or duration"")<<NEWL>><<NEWL>>    if url is None and acl is None:<<NEWL>>        raise Exception(""Must provide either acl or url"")<<NEWL>><<NEWL>>    token_parts = []<<NEWL>>    if ip is not None:<<NEWL>>        token_parts.append(""ip="" + ip)<<NEWL>>    if start_time is not None:<<NEWL>>        token_parts.append(""st=%d"" % start_time)<<NEWL>>    token_parts.append(""exp=%d"" % expiration)<<NEWL>>    if acl is not None:<<NEWL>>        acl_list = acl if type(acl) is list else [acl]<<NEWL>>        acl_list = [_escape_to_lower(a) for a in acl_list] <<NEWL>>        token_parts.append(""acl=%s"" % ""!"".join(acl_list))<<NEWL>>    to_sign = list(token_parts)<<NEWL>>    if url is not None and acl is None:<<NEWL>>        to_sign.append(""url=%s"" % _escape_to_lower(url))<<NEWL>>    auth = _digest(AUTH_TOKEN_SEPARATOR.join(to_sign), key)<<NEWL>>    token_parts.append(""hmac=%s"" % auth)<<NEWL>>    return ""%(token_name)s=%(token)s"" % {""token_name"": token_name, ""token"": AUTH_TOKEN_SEPARATOR.join(token_parts)}<<NEWL>><<NEWL>><<NEWL>>def _digest(message, key):<<NEWL>>    bin_key = a2b_hex(key)<<NEWL>>    return hmac.new(bin_key, message.encode('utf-8'), hashlib.sha256).hexdigest()<<NEWL>><<NEWL>><<NEWL>>def _escape_to_lower(url):<<NEWL>>    # There is a circular import issue in this file, need to resolve it in the next major release<<NEWL>>    from cloudinary.utils import smart_escape<<NEWL>>    escaped_url = smart_escape(url, unsafe=AUTH_TOKEN_UNSAFE_RE)<<NEWL>>    escaped_url = re.sub(r""%[0-9A-F]{2}"", lambda x: x.group(0).lower(), escaped_url)<<NEWL>>    return escaped_url"
262	donghui	1	"import hashlib<<NEWL>>import hmac<<NEWL>>import re<<NEWL>>import time<<NEWL>>from binascii import a2b_hex<<NEWL>><<NEWL>><<NEWL>>AUTH_TOKEN_NAME = ""__cld_token__""<<NEWL>>AUTH_TOKEN_SEPARATOR = ""~""<<NEWL>>AUTH_TOKEN_UNSAFE_RE = r'([ ""#%&\'\/:;<=>?@\[\\\]^`{\|}~]+)'<<NEWL>><<NEWL>><<NEWL>>def generate(url=None, acl=None, start_time=None, duration=None,<<NEWL>>             expiration=None, ip=None, key=None, token_name=AUTH_TOKEN_NAME):<<NEWL>><<NEWL>>    if expiration is None:<<NEWL>>        if duration is not None:<<NEWL>>            start = start_time if start_time is not None else int(time.time())<<NEWL>>            expiration = start + duration<<NEWL>>        else:<<NEWL>>            raise Exception(""Must provide either expiration or duration"")<<NEWL>><<NEWL>>    if url is None and acl is None:<<NEWL>>        raise Exception(""Must provide either acl or url"")<<NEWL>><<NEWL>>    token_parts = []<<NEWL>>    if ip is not None:<<NEWL>>        token_parts.append(""ip="" + ip)<<NEWL>>    if start_time is not None:<<NEWL>>        token_parts.append(""st=%d"" % start_time)<<NEWL>>    token_parts.append(""exp=%d"" % expiration)<<NEWL>>    if acl is not None:<<NEWL>>        acl_list = acl if type(acl) is list else [acl]<<NEWL>>        acl_list = [_escape_to_lower(a) for a in acl_list] <<NEWL>>        token_parts.append(""acl=%s"" % ""!"".join(acl_list))<<NEWL>>    to_sign = list(token_parts)<<NEWL>>    if url is not None and acl is None:<<NEWL>>        to_sign.append(""url=%s"" % _escape_to_lower(url))<<NEWL>>    auth = _digest(AUTH_TOKEN_SEPARATOR.join(to_sign), key)<<NEWL>>    token_parts.append(""hmac=%s"" % auth)<<NEWL>>    return ""%(token_name)s=%(token)s"" % {""token_name"": token_name, ""token"": AUTH_TOKEN_SEPARATOR.join(token_parts)}<<NEWL>><<NEWL>><<NEWL>>def _digest(message, key):<<NEWL>>    bin_key = a2b_hex(key)<<NEWL>>    return hmac.new(bin_key, message.encode('utf-8'), hashlib.sha256).hexdigest()<<NEWL>><<NEWL>><<NEWL>>def _escape_to_lower(url):<<NEWL>>    # There is a circular import issue in this file, need to resolve it in the next major release<<NEWL>>    from cloudinary.utils import smart_escape<<NEWL>>    escaped_url = smart_escape(url, unsafe=AUTH_TOKEN_UNSAFE_RE)<<NEWL>>    escaped_url = re.sub(r""%[0-9A-F]{2}"", lambda x: x.group(0).lower(), escaped_url)<<NEWL>>    return escaped_url"
322	jackson	4	"""""""<<NEWL>>Mozilla Persona authentication backend, docs at:<<NEWL>>    https://python-social-auth.readthedocs.io/en/latest/backends/persona.html<<NEWL>>""""""<<NEWL>>from ..exceptions import AuthFailed, AuthMissingParameter<<NEWL>>from ..utils import handle_http_errors<<NEWL>>from .base import BaseAuth<<NEWL>><<NEWL>><<NEWL>>class PersonaAuth(BaseAuth):<<NEWL>>    """"""BrowserID authentication backend""""""<<NEWL>>    name = 'persona'<<NEWL>><<NEWL>>    def get_user_id(self, details, response):<<NEWL>>        """"""Use BrowserID email as ID""""""<<NEWL>>        return details['email']<<NEWL>><<NEWL>>    def get_user_details(self, response):<<NEWL>>        """"""Return user details, BrowserID only provides Email.""""""<<NEWL>>        # {'status': 'okay',<<NEWL>>        #  'audience': 'localhost:8000',<<NEWL>>        #  'expires': 1328983575529,<<NEWL>>        #  'email': 'name@server.com',<<NEWL>>        #  'issuer': 'browserid.org'}<<NEWL>>        email = response['email']<<NEWL>>        return {'username': email.split('@', 1)[0],<<NEWL>>                'email': email,<<NEWL>>                'fullname': '',<<NEWL>>                'first_name': '',<<NEWL>>                'last_name': ''}<<NEWL>><<NEWL>>    def extra_data(self, user, uid, response, details=None, *args, **kwargs):<<NEWL>>        """"""Return users extra data""""""<<NEWL>>        return {'audience': response['audience'],<<NEWL>>                'issuer': response['issuer']}<<NEWL>><<NEWL>>    @handle_http_errors<<NEWL>>    def auth_complete(self, *args, **kwargs):<<NEWL>>        """"""Completes login process, must return user instance""""""<<NEWL>>        if 'assertion' not in self.data:<<NEWL>>            raise AuthMissingParameter(self, 'assertion')<<NEWL>><<NEWL>>        response = self.get_json('https://browserid.org/verify', data={<<NEWL>>            'assertion': self.data['assertion'],<<NEWL>>            'audience': self.strategy.request_host()<<NEWL>>        }, method='POST')<<NEWL>>        if response.get('status') == 'failure':<<NEWL>>            raise AuthFailed(self)<<NEWL>>        kwargs.update({'response': response, 'backend': self})<<NEWL>>        return self.strategy.authenticate(*args, **kwargs)"
322	donghui	2	"""""""<<NEWL>>Mozilla Persona authentication backend, docs at:<<NEWL>>    https://python-social-auth.readthedocs.io/en/latest/backends/persona.html<<NEWL>>""""""<<NEWL>>from ..exceptions import AuthFailed, AuthMissingParameter<<NEWL>>from ..utils import handle_http_errors<<NEWL>>from .base import BaseAuth<<NEWL>><<NEWL>><<NEWL>>class PersonaAuth(BaseAuth):<<NEWL>>    """"""BrowserID authentication backend""""""<<NEWL>>    name = 'persona'<<NEWL>><<NEWL>>    def get_user_id(self, details, response):<<NEWL>>        """"""Use BrowserID email as ID""""""<<NEWL>>        return details['email']<<NEWL>><<NEWL>>    def get_user_details(self, response):<<NEWL>>        """"""Return user details, BrowserID only provides Email.""""""<<NEWL>>        # {'status': 'okay',<<NEWL>>        #  'audience': 'localhost:8000',<<NEWL>>        #  'expires': 1328983575529,<<NEWL>>        #  'email': 'name@server.com',<<NEWL>>        #  'issuer': 'browserid.org'}<<NEWL>>        email = response['email']<<NEWL>>        return {'username': email.split('@', 1)[0],<<NEWL>>                'email': email,<<NEWL>>                'fullname': '',<<NEWL>>                'first_name': '',<<NEWL>>                'last_name': ''}<<NEWL>><<NEWL>>    def extra_data(self, user, uid, response, details=None, *args, **kwargs):<<NEWL>>        """"""Return users extra data""""""<<NEWL>>        return {'audience': response['audience'],<<NEWL>>                'issuer': response['issuer']}<<NEWL>><<NEWL>>    @handle_http_errors<<NEWL>>    def auth_complete(self, *args, **kwargs):<<NEWL>>        """"""Completes login process, must return user instance""""""<<NEWL>>        if 'assertion' not in self.data:<<NEWL>>            raise AuthMissingParameter(self, 'assertion')<<NEWL>><<NEWL>>        response = self.get_json('https://browserid.org/verify', data={<<NEWL>>            'assertion': self.data['assertion'],<<NEWL>>            'audience': self.strategy.request_host()<<NEWL>>        }, method='POST')<<NEWL>>        if response.get('status') == 'failure':<<NEWL>>            raise AuthFailed(self)<<NEWL>>        kwargs.update({'response': response, 'backend': self})<<NEWL>>        return self.strategy.authenticate(*args, **kwargs)"
380	jackson	0	######################## BEGIN LICENSE BLOCK ########################<<NEWL>># The Original Code is Mozilla Universal charset detector code.<<NEWL>>#<<NEWL>># The Initial Developer of the Original Code is<<NEWL>># Netscape Communications Corporation.<<NEWL>># Portions created by the Initial Developer are Copyright (C) 2001<<NEWL>># the Initial Developer. All Rights Reserved.<<NEWL>>#<<NEWL>># Contributor(s):<<NEWL>>#   Mark Pilgrim - port to Python<<NEWL>>#   Shy Shalom - original C code<<NEWL>>#   Proofpoint, Inc.<<NEWL>>#<<NEWL>># This library is free software; you can redistribute it and/or<<NEWL>># modify it under the terms of the GNU Lesser General Public<<NEWL>># License as published by the Free Software Foundation; either<<NEWL>># version 2.1 of the License, or (at your option) any later version.<<NEWL>>#<<NEWL>># This library is distributed in the hope that it will be useful,<<NEWL>># but WITHOUT ANY WARRANTY; without even the implied warranty of<<NEWL>># MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU<<NEWL>># Lesser General Public License for more details.<<NEWL>>#<<NEWL>># You should have received a copy of the GNU Lesser General Public<<NEWL>># License along with this library; if not, write to the Free Software<<NEWL>># Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA<<NEWL>># 02110-1301  USA<<NEWL>>######################### END LICENSE BLOCK #########################<<NEWL>><<NEWL>>from .big5prober import Big5Prober<<NEWL>>from .charsetgroupprober import CharSetGroupProber<<NEWL>>from .cp949prober import CP949Prober<<NEWL>>from .enums import LanguageFilter<<NEWL>>from .eucjpprober import EUCJPProber<<NEWL>>from .euckrprober import EUCKRProber<<NEWL>>from .euctwprober import EUCTWProber<<NEWL>>from .gb2312prober import GB2312Prober<<NEWL>>from .johabprober import JOHABProber<<NEWL>>from .sjisprober import SJISProber<<NEWL>>from .utf8prober import UTF8Prober<<NEWL>><<NEWL>><<NEWL>>class MBCSGroupProber(CharSetGroupProber):<<NEWL>>    def __init__(self, lang_filter: LanguageFilter = LanguageFilter.NONE) -> None:<<NEWL>>        super().__init__(lang_filter=lang_filter)<<NEWL>>        self.probers = [<<NEWL>>            UTF8Prober(),<<NEWL>>            SJISProber(),<<NEWL>>            EUCJPProber(),<<NEWL>>            GB2312Prober(),<<NEWL>>            EUCKRProber(),<<NEWL>>            CP949Prober(),<<NEWL>>            Big5Prober(),<<NEWL>>            EUCTWProber(),<<NEWL>>            JOHABProber(),<<NEWL>>        ]<<NEWL>>        self.reset()
380	donghui	0	######################## BEGIN LICENSE BLOCK ########################<<NEWL>># The Original Code is Mozilla Universal charset detector code.<<NEWL>>#<<NEWL>># The Initial Developer of the Original Code is<<NEWL>># Netscape Communications Corporation.<<NEWL>># Portions created by the Initial Developer are Copyright (C) 2001<<NEWL>># the Initial Developer. All Rights Reserved.<<NEWL>>#<<NEWL>># Contributor(s):<<NEWL>>#   Mark Pilgrim - port to Python<<NEWL>>#   Shy Shalom - original C code<<NEWL>>#   Proofpoint, Inc.<<NEWL>>#<<NEWL>># This library is free software; you can redistribute it and/or<<NEWL>># modify it under the terms of the GNU Lesser General Public<<NEWL>># License as published by the Free Software Foundation; either<<NEWL>># version 2.1 of the License, or (at your option) any later version.<<NEWL>>#<<NEWL>># This library is distributed in the hope that it will be useful,<<NEWL>># but WITHOUT ANY WARRANTY; without even the implied warranty of<<NEWL>># MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU<<NEWL>># Lesser General Public License for more details.<<NEWL>>#<<NEWL>># You should have received a copy of the GNU Lesser General Public<<NEWL>># License along with this library; if not, write to the Free Software<<NEWL>># Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA<<NEWL>># 02110-1301  USA<<NEWL>>######################### END LICENSE BLOCK #########################<<NEWL>><<NEWL>>from .big5prober import Big5Prober<<NEWL>>from .charsetgroupprober import CharSetGroupProber<<NEWL>>from .cp949prober import CP949Prober<<NEWL>>from .enums import LanguageFilter<<NEWL>>from .eucjpprober import EUCJPProber<<NEWL>>from .euckrprober import EUCKRProber<<NEWL>>from .euctwprober import EUCTWProber<<NEWL>>from .gb2312prober import GB2312Prober<<NEWL>>from .johabprober import JOHABProber<<NEWL>>from .sjisprober import SJISProber<<NEWL>>from .utf8prober import UTF8Prober<<NEWL>><<NEWL>><<NEWL>>class MBCSGroupProber(CharSetGroupProber):<<NEWL>>    def __init__(self, lang_filter: LanguageFilter = LanguageFilter.NONE) -> None:<<NEWL>>        super().__init__(lang_filter=lang_filter)<<NEWL>>        self.probers = [<<NEWL>>            UTF8Prober(),<<NEWL>>            SJISProber(),<<NEWL>>            EUCJPProber(),<<NEWL>>            GB2312Prober(),<<NEWL>>            EUCKRProber(),<<NEWL>>            CP949Prober(),<<NEWL>>            Big5Prober(),<<NEWL>>            EUCTWProber(),<<NEWL>>            JOHABProber(),<<NEWL>>        ]<<NEWL>>        self.reset()
291	jackson	4	"from django import template<<NEWL>>from django.contrib.admin.models import LogEntry<<NEWL>><<NEWL>>register = template.Library()<<NEWL>><<NEWL>><<NEWL>>class AdminLogNode(template.Node):<<NEWL>>    def __init__(self, limit, varname, user):<<NEWL>>        self.limit, self.varname, self.user = limit, varname, user<<NEWL>><<NEWL>>    def __repr__(self):<<NEWL>>        return ""<GetAdminLog Node>""<<NEWL>><<NEWL>>    def render(self, context):<<NEWL>>        if self.user is None:<<NEWL>>            entries = LogEntry.objects.all()<<NEWL>>        else:<<NEWL>>            user_id = self.user<<NEWL>>            if not user_id.isdigit():<<NEWL>>                user_id = context[self.user].pk<<NEWL>>            entries = LogEntry.objects.filter(user__pk=user_id)<<NEWL>>        context[self.varname] = entries.select_related('content_type', 'user')[:int(self.limit)]<<NEWL>>        return ''<<NEWL>><<NEWL>><<NEWL>>@register.tag<<NEWL>>def get_admin_log(parser, token):<<NEWL>>    """"""<<NEWL>>    Populate a template variable with the admin log for the given criteria.<<NEWL>><<NEWL>>    Usage::<<NEWL>><<NEWL>>        {% get_admin_log [limit] as [varname] for_user [context_var_containing_user_obj] %}<<NEWL>><<NEWL>>    Examples::<<NEWL>><<NEWL>>        {% get_admin_log 10 as admin_log for_user 23 %}<<NEWL>>        {% get_admin_log 10 as admin_log for_user user %}<<NEWL>>        {% get_admin_log 10 as admin_log %}<<NEWL>><<NEWL>>    Note that ``context_var_containing_user_obj`` can be a hard-coded integer<<NEWL>>    (user ID) or the name of a template context variable containing the user<<NEWL>>    object whose ID you want.<<NEWL>>    """"""<<NEWL>>    tokens = token.contents.split()<<NEWL>>    if len(tokens) < 4:<<NEWL>>        raise template.TemplateSyntaxError(<<NEWL>>            ""'get_admin_log' statements require two arguments"")<<NEWL>>    if not tokens[1].isdigit():<<NEWL>>        raise template.TemplateSyntaxError(<<NEWL>>            ""First argument to 'get_admin_log' must be an integer"")<<NEWL>>    if tokens[2] != 'as':<<NEWL>>        raise template.TemplateSyntaxError(<<NEWL>>            ""Second argument to 'get_admin_log' must be 'as'"")<<NEWL>>    if len(tokens) > 4:<<NEWL>>        if tokens[4] != 'for_user':<<NEWL>>            raise template.TemplateSyntaxError(<<NEWL>>                ""Fourth argument to 'get_admin_log' must be 'for_user'"")<<NEWL>>    return AdminLogNode(limit=tokens[1], varname=tokens[3], user=(tokens[5] if len(tokens) > 5 else None))"
291	donghui	4	"from django import template<<NEWL>>from django.contrib.admin.models import LogEntry<<NEWL>><<NEWL>>register = template.Library()<<NEWL>><<NEWL>><<NEWL>>class AdminLogNode(template.Node):<<NEWL>>    def __init__(self, limit, varname, user):<<NEWL>>        self.limit, self.varname, self.user = limit, varname, user<<NEWL>><<NEWL>>    def __repr__(self):<<NEWL>>        return ""<GetAdminLog Node>""<<NEWL>><<NEWL>>    def render(self, context):<<NEWL>>        if self.user is None:<<NEWL>>            entries = LogEntry.objects.all()<<NEWL>>        else:<<NEWL>>            user_id = self.user<<NEWL>>            if not user_id.isdigit():<<NEWL>>                user_id = context[self.user].pk<<NEWL>>            entries = LogEntry.objects.filter(user__pk=user_id)<<NEWL>>        context[self.varname] = entries.select_related('content_type', 'user')[:int(self.limit)]<<NEWL>>        return ''<<NEWL>><<NEWL>><<NEWL>>@register.tag<<NEWL>>def get_admin_log(parser, token):<<NEWL>>    """"""<<NEWL>>    Populate a template variable with the admin log for the given criteria.<<NEWL>><<NEWL>>    Usage::<<NEWL>><<NEWL>>        {% get_admin_log [limit] as [varname] for_user [context_var_containing_user_obj] %}<<NEWL>><<NEWL>>    Examples::<<NEWL>><<NEWL>>        {% get_admin_log 10 as admin_log for_user 23 %}<<NEWL>>        {% get_admin_log 10 as admin_log for_user user %}<<NEWL>>        {% get_admin_log 10 as admin_log %}<<NEWL>><<NEWL>>    Note that ``context_var_containing_user_obj`` can be a hard-coded integer<<NEWL>>    (user ID) or the name of a template context variable containing the user<<NEWL>>    object whose ID you want.<<NEWL>>    """"""<<NEWL>>    tokens = token.contents.split()<<NEWL>>    if len(tokens) < 4:<<NEWL>>        raise template.TemplateSyntaxError(<<NEWL>>            ""'get_admin_log' statements require two arguments"")<<NEWL>>    if not tokens[1].isdigit():<<NEWL>>        raise template.TemplateSyntaxError(<<NEWL>>            ""First argument to 'get_admin_log' must be an integer"")<<NEWL>>    if tokens[2] != 'as':<<NEWL>>        raise template.TemplateSyntaxError(<<NEWL>>            ""Second argument to 'get_admin_log' must be 'as'"")<<NEWL>>    if len(tokens) > 4:<<NEWL>>        if tokens[4] != 'for_user':<<NEWL>>            raise template.TemplateSyntaxError(<<NEWL>>                ""Fourth argument to 'get_admin_log' must be 'for_user'"")<<NEWL>>    return AdminLogNode(limit=tokens[1], varname=tokens[3], user=(tokens[5] if len(tokens) > 5 else None))"
